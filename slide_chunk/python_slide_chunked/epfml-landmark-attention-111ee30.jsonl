{"filename": "lm_benchmark/eval_cmd_generator.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport dataclasses\n\n@dataclasses.dataclass\nclass Setting(object):\n    exp_dir: str\n    eval_len: int\n    topk: int\n    mem_size: int\n    mid_length: int\n    use_cache: bool = True\n    selection_method: str = \"per_token_and_head\"\n    mem_cache_freq: int = 50\n    eval_sample_size: int = 4000000\n    lm_cache: str = \"mem\"", "@dataclasses.dataclass\nclass Setting(object):\n    exp_dir: str\n    eval_len: int\n    topk: int\n    mem_size: int\n    mid_length: int\n    use_cache: bool = True\n    selection_method: str = \"per_token_and_head\"\n    mem_cache_freq: int = 50\n    eval_sample_size: int = 4000000\n    lm_cache: str = \"mem\"", "    \n\nexp_dirs = {\n    \"arxiv_landmark\": \"./exps/arxiv_landmark\",\n    \"arxiv_baseline\": \"./exps/arxiv_baseline\",\n\n    \"pg19_landmark\": \"./exps/pg19_landmark\",\n    \"pg19_baseline\": \"./exps/pg19_baseline\",\n    \"pg19_xl\": \"./exps/pg19_xl\",\n}", "    \"pg19_xl\": \"./exps/pg19_xl\",\n}\nsettings = [\n    dict(exp_dir=exp_dirs[\"pg19_baseline\"], eval_len=360, mid_length=360, \n         lm_cache=\"none\", mem_cache_freq=None, mem_size=None, topk=None, use_cache=False,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_baseline\"], eval_len=512, mid_length=512, \n         lm_cache=\"none\", mem_cache_freq=None, mem_size=None, topk=None, use_cache=False,eval_sample_size=None),\n        \n    dict(exp_dir=exp_dirs[\"pg19_xl\"], eval_len=2048, mid_length=256, \n         lm_cache=\"kv\", mem_cache_freq=None, mem_size=256, topk=None,eval_sample_size=None),", "    dict(exp_dir=exp_dirs[\"pg19_xl\"], eval_len=2048, mid_length=256, \n         lm_cache=\"kv\", mem_cache_freq=None, mem_size=256, topk=None,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_xl\"], eval_len=4096, mid_length=256, \n         lm_cache=\"kv\", mem_cache_freq=None, mem_size=256, topk=None,eval_sample_size=None),\n\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=512, mid_length=250, mem_size=10, topk=2,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=2,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=350, mem_size=40, topk=2,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=300, mem_size=40, topk=3,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=20, topk=4,eval_sample_size=None),", "    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=300, mem_size=40, topk=3,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=20, topk=4,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=2,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=4,eval_sample_size=None),\n\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=2,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),", "    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=40, topk=4,eval_sample_size=None),\n\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=2,\n         selection_method=\"max_over_heads\",eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4,\n         selection_method=\"max_over_heads\",eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=4,\n         selection_method=\"max_over_heads\",eval_sample_size=None),\n", "         selection_method=\"max_over_heads\",eval_sample_size=None),\n\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=2,\n         selection_method=\"max_over_tokens\",eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4,\n         selection_method=\"max_over_tokens\",eval_sample_size=None),\n    dict(exp_dir=exp_dirs[\"pg19_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=4,\n         selection_method=\"max_over_tokens\",eval_sample_size=None),\n\n    dict(exp_dir=exp_dirs[\"arxiv_baseline\"], eval_len=360, mid_length=360, ", "\n    dict(exp_dir=exp_dirs[\"arxiv_baseline\"], eval_len=360, mid_length=360, \n         lm_cache=None, mem_cache_freq=None, mem_size=None, topk=None, use_cache=False),\n    dict(exp_dir=exp_dirs[\"arxiv_baseline\"], eval_len=512, mid_length=512, \n         lm_cache=None, mem_cache_freq=None, mem_size=None, topk=None, use_cache=False),\n\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=512, mid_length=250, mem_size=10, topk=2),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=2),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=350, mem_size=40, topk=2),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=300, mem_size=40, topk=3),", "    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=350, mem_size=40, topk=2),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=300, mem_size=40, topk=3),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=250, mem_size=20, topk=4),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=2048, mid_length=250, mem_size=40, topk=4),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=4096, mid_length=250, mem_size=40, topk=4),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=2),\n    dict(exp_dir=exp_dirs[\"arxiv_landmark\"], eval_len=4096, mid_length=250, mem_size=80, topk=4),\n]\n\nimport itertools\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    for instance in itertools.product(*kwargs.values()):\n        yield dict(zip(keys, instance))", "\nimport itertools\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    for instance in itertools.product(*kwargs.values()):\n        yield dict(zip(keys, instance))\n\nflat_settings = []\nfor setting in settings:\n    flat_settings.extend(product_dict(**{x: y if isinstance(y, list) else [y] for x, y in setting.items()}))", "for setting in settings:\n    flat_settings.extend(product_dict(**{x: y if isinstance(y, list) else [y] for x, y in setting.items()}))\n\nsettings = [Setting(**d) for d in flat_settings]\nlast_exp_dir = None\n\nprint (\"#!/bin/bash\")\nfor setting in settings:\n     s_lines = []\n     if last_exp_dir != setting.exp_dir:\n          s_lines.append(\"\"\"EXP_DIR=\"{exp_dir}\";\"\"\".format(**dataclasses.asdict(setting)))\n     last_exp_dir = setting.exp_dir\n     use_cache_str = \"--use_cache\" if setting.use_cache else \"\"\n     mem_size_flag = \"\"\n     s_lines += [\"\"\"\n     filename=\"$EXP_DIR/eval-{eval_len}-{selection_method}-{topk}-memsize{mem_size}-midlength{mid_length}-memcachefreq{mem_cache_freq}\"; \n     grep val_acc $filename /dev/null; \n     if [[ $? -ne 0 ]]; then \n          script -c \\\\\n          \"python eval.py \\\\\n               --checkpoint  $EXP_DIR \\\\\n               --distributed_backend None  \\\\\n               --lm_cache {lm_cache} \\\\\"\"\",\"\"\"\n               --mem_cache_size {mem_size} \\\\\"\"\" if setting.mem_size is not None else \"\",\"\"\"\n               --mem_cache_freq {mem_cache_freq} \\\\\"\"\" if setting.mem_cache_freq is not None else \"\", \"\"\"\n               --mem_freq None \\\\\n               --eval_seq_length {eval_len} \\\\\n               --cache_selection_method {selection_method} \\\\\"\"\",\"\"\"\n               --cache_topk {topk} \\\\\"\"\" if setting.topk is not None else \"\", \"\"\"\n               --no_compile \\\\\n               --batch_size 16 \\\\\n               --mid_length {mid_length} \\\\\n               --positional_encoder rotary \\\\\n               --pos_jump_on_mem 0   \\\\\n               {use_cache_str} \\\\\"\"\", \"\"\"\n               --eval_sample_size {eval_sample_size}\"\"\" if setting.eval_sample_size is not None else \"\", \"\"\"\n               \" $filename; \n     fi;\"\"\"]\n     print (\"\".join(s_lines).format(**dataclasses.asdict(setting), use_cache_str=use_cache_str))", ""]}
{"filename": "lm_benchmark/main.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport inspect\nimport json\nimport copy\nimport argparse\nimport random\nimport wandb\n\nimport config", "\nimport config\nimport models\nfrom data import get_dataset, prepare_dataset\nfrom optim.base import train_base\nfrom optim.transformer_xl import train_xl\nimport distributed\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(allow_abbrev=False)\n    parser.add_argument('--config_format', default='base', choices=config.registered_formats())\n\n    args, rem_args = parser.parse_known_args()\n\n    return config.parse_args_with_format(format=args.config_format, base_parser=parser, args=rem_args, namespace=args)", "\ndef get_args():\n    parser = argparse.ArgumentParser(allow_abbrev=False)\n    parser.add_argument('--config_format', default='base', choices=config.registered_formats())\n\n    args, rem_args = parser.parse_known_args()\n\n    return config.parse_args_with_format(format=args.config_format, base_parser=parser, args=rem_args, namespace=args)\n\n\ndef main(args): \n\n\n    torch.backends.cuda.matmul.allow_tf32 = True # allows us to make sure we're able to use tensorfloat32 during training\n    torch.backends.cudnn.allow_tf32 = True\n\n    distributed_backend = distributed.make_backend_from_args(args)\n    args = distributed_backend.get_adjusted_args_for_process(args)\n\n    args.device = torch.device(args.device)\n    torch.cuda.set_device(args.device)\n    device_type = 'cuda' if 'cuda' in str(args.device) else 'cpu'\n    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    \n    print(f\"Loading dataset '{args.dataset}'\")\n\n    if distributed_backend.is_master_process():\n        prepare_dataset(args)\n    distributed_backend.sync()\n    \n    data = get_dataset(args) # data is a dict: {'train': train_tokenized, 'val': eval_tokenized}\n        \n    print(f\"Num training tokens: {len(data['train'])}\")\n    print(f\"Num validation tokens: {len(data['val'])}\")\n    \n    model = models.make_model_from_args(args).to(args.device)\n\n    model = distributed_backend.transform_model(model)\n    \n    group_specs = distributed_backend.get_raw_model(model).get_parameter_group_specs()\n    param_name_mapping = {p_name: p for p_name, p in model.named_parameters()}\n    optimized_params_cnt = 0\n    for g in group_specs:\n        params = []\n        for p_name in g[\"params\"]:\n            translated_p_names = distributed_backend.translate_model_parameter_name_for_node(p_name)\n            params += [param_name_mapping[p_name] for p_name in translated_p_names]\n        g[\"params\"] = params\n        optimized_params_cnt += sum([p.numel() for p in g[\"params\"]])\n    print(\"number of optimized parameters: %.2fM\" % (optimized_params_cnt/1e6,))\n    if args.opt == 'adamw':\n        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n        print(f\"using fused AdamW: {use_fused}\")\n        extra_args = dict(fused=True) if use_fused else dict()\n        opt = torch.optim.AdamW(group_specs, lr=args.lr, betas=(args.beta1, args.beta2),\n                                weight_decay=args.weight_decay, **extra_args)\n    elif args.opt == 'adafactor':\n        from optim.adafactor import Adafactor\n        opt = Adafactor(group_specs, lr=args.lr)\n    else:\n        opt = torch.optim.SGD(group_specs, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)\n    \n    if args.scheduler != 'none':\n        if args.scheduler in ['cos', 'linear']:\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, total_steps=args.iterations, \n                                                            pct_start=args.warmup_percent, anneal_strategy=args.scheduler, \n                                                            cycle_momentum=False, div_factor=1e2, final_div_factor=.05)\n        else:\n            raise NotImplementedError(f\"Unknown scheduler type: {args.scheduler}.\")\n    else:\n        scheduler = None\n\n    args.world_size = distributed_backend.get_world_size()\n    exp_name = args.exp_name\n    if distributed_backend.is_master_process() and args.wandb:\n        params_copy = copy.deepcopy(vars(args))\n        del params_copy['device']\n        wandb.init(project=args.wandb_project, name=exp_name, config=params_copy)\n    \n    ckpt_path = f\"{args.results_base_folder}/{args.dataset}/{args.model}/{exp_name}\"\n    if not os.path.exists(ckpt_path):\n        if distributed_backend.is_master_process():\n            os.makedirs(ckpt_path)\n    else:\n        if os.path.isfile(f\"{ckpt_path}/summary.json\"): # the experiment was already completed\n            print(f\"Already found experiment '{ckpt_path}'.\\nSkipping.\")\n            sys.exit(0)\n\n    if args.optimization_process == 'transformer_xl':\n        train = train_xl\n    else: \n        train = train_base\n\n    print(f\"\\nTraining model={args.model} \\n{vars(args)}\\n\")\n\n    stats = train(model, opt, data, scheduler, args.iterations, args.acc_steps, args.batch_size, args.sequence_length, \n                  eval_freq=args.eval_freq, \n                  distributed_backend=distributed_backend,\n                  ckpt_path=ckpt_path, extra_args=args)\n    \n    args.device = None\n    args.dtype = None\n    stats['args'] = vars(args)\n    if distributed_backend.is_master_process():\n        with open(f\"{ckpt_path}/summary.json\", \"w\") as fs:\n            json.dump(stats, fs)\n    distributed_backend.finalize()", "\n\ndef main(args): \n\n\n    torch.backends.cuda.matmul.allow_tf32 = True # allows us to make sure we're able to use tensorfloat32 during training\n    torch.backends.cudnn.allow_tf32 = True\n\n    distributed_backend = distributed.make_backend_from_args(args)\n    args = distributed_backend.get_adjusted_args_for_process(args)\n\n    args.device = torch.device(args.device)\n    torch.cuda.set_device(args.device)\n    device_type = 'cuda' if 'cuda' in str(args.device) else 'cpu'\n    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    \n    print(f\"Loading dataset '{args.dataset}'\")\n\n    if distributed_backend.is_master_process():\n        prepare_dataset(args)\n    distributed_backend.sync()\n    \n    data = get_dataset(args) # data is a dict: {'train': train_tokenized, 'val': eval_tokenized}\n        \n    print(f\"Num training tokens: {len(data['train'])}\")\n    print(f\"Num validation tokens: {len(data['val'])}\")\n    \n    model = models.make_model_from_args(args).to(args.device)\n\n    model = distributed_backend.transform_model(model)\n    \n    group_specs = distributed_backend.get_raw_model(model).get_parameter_group_specs()\n    param_name_mapping = {p_name: p for p_name, p in model.named_parameters()}\n    optimized_params_cnt = 0\n    for g in group_specs:\n        params = []\n        for p_name in g[\"params\"]:\n            translated_p_names = distributed_backend.translate_model_parameter_name_for_node(p_name)\n            params += [param_name_mapping[p_name] for p_name in translated_p_names]\n        g[\"params\"] = params\n        optimized_params_cnt += sum([p.numel() for p in g[\"params\"]])\n    print(\"number of optimized parameters: %.2fM\" % (optimized_params_cnt/1e6,))\n    if args.opt == 'adamw':\n        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n        print(f\"using fused AdamW: {use_fused}\")\n        extra_args = dict(fused=True) if use_fused else dict()\n        opt = torch.optim.AdamW(group_specs, lr=args.lr, betas=(args.beta1, args.beta2),\n                                weight_decay=args.weight_decay, **extra_args)\n    elif args.opt == 'adafactor':\n        from optim.adafactor import Adafactor\n        opt = Adafactor(group_specs, lr=args.lr)\n    else:\n        opt = torch.optim.SGD(group_specs, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)\n    \n    if args.scheduler != 'none':\n        if args.scheduler in ['cos', 'linear']:\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, total_steps=args.iterations, \n                                                            pct_start=args.warmup_percent, anneal_strategy=args.scheduler, \n                                                            cycle_momentum=False, div_factor=1e2, final_div_factor=.05)\n        else:\n            raise NotImplementedError(f\"Unknown scheduler type: {args.scheduler}.\")\n    else:\n        scheduler = None\n\n    args.world_size = distributed_backend.get_world_size()\n    exp_name = args.exp_name\n    if distributed_backend.is_master_process() and args.wandb:\n        params_copy = copy.deepcopy(vars(args))\n        del params_copy['device']\n        wandb.init(project=args.wandb_project, name=exp_name, config=params_copy)\n    \n    ckpt_path = f\"{args.results_base_folder}/{args.dataset}/{args.model}/{exp_name}\"\n    if not os.path.exists(ckpt_path):\n        if distributed_backend.is_master_process():\n            os.makedirs(ckpt_path)\n    else:\n        if os.path.isfile(f\"{ckpt_path}/summary.json\"): # the experiment was already completed\n            print(f\"Already found experiment '{ckpt_path}'.\\nSkipping.\")\n            sys.exit(0)\n\n    if args.optimization_process == 'transformer_xl':\n        train = train_xl\n    else: \n        train = train_base\n\n    print(f\"\\nTraining model={args.model} \\n{vars(args)}\\n\")\n\n    stats = train(model, opt, data, scheduler, args.iterations, args.acc_steps, args.batch_size, args.sequence_length, \n                  eval_freq=args.eval_freq, \n                  distributed_backend=distributed_backend,\n                  ckpt_path=ckpt_path, extra_args=args)\n    \n    args.device = None\n    args.dtype = None\n    stats['args'] = vars(args)\n    if distributed_backend.is_master_process():\n        with open(f\"{ckpt_path}/summary.json\", \"w\") as fs:\n            json.dump(stats, fs)\n    distributed_backend.finalize()", "\n\nif __name__ == \"__main__\":\n    args = get_args()\n    main(args)\n"]}
{"filename": "lm_benchmark/eval.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport inspect\nimport json\nimport copy\nimport argparse\nimport random\nimport wandb\nimport logging\n", "import logging\n\nfrom tqdm import tqdm\n\nimport config\nimport models\nfrom data import get_dataset, prepare_dataset\nfrom optim.base import train_base\nimport distributed\nfrom optim.utils import get_batch", "import distributed\nfrom optim.utils import get_batch\n\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(allow_abbrev=False)\n    parser.add_argument('--checkpoint', type=str, required=True)\n    \n    args, rem_args = parser.parse_known_args()\n\n    if os.path.isfile(args.checkpoint):\n        args.checkpoint, args.checkpoint_filename = os.path.split(args.checkpoint)\n    else:\n        args.checkpoint_filename = \"ckpt.pt\"\n\n    with open(os.path.join(args.checkpoint, \"summary.json\")) as f:\n        summary = json.load(f)\n\n    for k, v in summary['args'].items():\n        if k not in [\"device\", \"dtype\"]:\n            setattr(args, k, v)\n\n    return config.parse_args_with_format(format=args.config_format, base_parser=argparse.ArgumentParser(allow_abbrev=False), args=rem_args, namespace=args)", "\n\ndef get_as_batch(data, seq_length, batch_size, device='cpu', sample_size=None):\n    all_ix = list(range(0, len(data), seq_length))\n    assert all_ix[-1] + seq_length + 1 > len(data)\n    all_ix.pop()\n    if sample_size is not None:\n        all_ix = np.random.choice(all_ix, size=sample_size // seq_length, replace=False).tolist()\n    \n    idx = 0\n    for idx in range(0, len(all_ix), batch_size):\n        ix = all_ix[idx:idx+batch_size]\n        assert all([idx + seq_length + 1 <= len(data) for idx in ix])\n        x = torch.stack([torch.from_numpy((data[i:i+seq_length]).astype(np.int64)) for i in ix])\n        y = torch.stack([torch.from_numpy((data[i+1:i+1+seq_length]).astype(np.int64)) for i in ix])\n        if device != 'cpu':\n            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n        yield x, y", "\ndef iceildiv(x, y):\n    return (x + y - 1) // y\n\ndef evaluate(model, data, iterations, acc_steps, batch_size, sequence_length, distributed_backend, extra_args):\n    device_type = 'cuda' if 'cuda' in str(extra_args.device) else 'cpu'\n    type_ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n        device_type=device_type, dtype=extra_args.dtype)  # extra_args.dtype)\n    itr, substep, best_val_loss, text_table = 0, 0, float('inf'), None # best_val_loss not used atm, early stopping not recommended but possible \n\n    stats = {}\n\n    num_substeps_per_epoch = len(data['val']) // (batch_size * sequence_length)\n    \n    if not extra_args.no_compile:\n        print(f\"Compiling model ...\")\n        import torch._dynamo as torchdynamo\n        torchdynamo.config.guard_nn_modules = True\n        # torchdynamo.config.log_level = logging.DEBUG\n        model = torch.compile(model) # requires pytorch 2.0+\n\n    model.eval()\n\n    loss_list_val, acc_list = [], []\n    loss_step_list_val = []\n\n    max_num_batches = 400\n    with torch.no_grad():\n        mid_length = extra_args.mid_length\n        print(f\"Sending sub-sequences of length at most {mid_length}\")\n        seq_length = extra_args.eval_seq_length \n        print(f\"Using seq length {seq_length}\")\n        torch.set_printoptions(sci_mode=False)\n        for idx, (x, y) in tqdm(\n            enumerate(\n                get_as_batch(\n                    data['val'], \n                    seq_length, \n                    batch_size, \n                    device=extra_args.device, \n                    sample_size=extra_args.eval_sample_size\n                )\n            ),\n            total=iceildiv(\n                extra_args.eval_sample_size // seq_length if extra_args.eval_sample_size is not None else \n                iceildiv(len(data['val']), seq_length), \n                batch_size\n            )\n        ):\n            val_loss = 0.\n            acc = 0.\n            cnt = 0\n            model.clear_state()\n            for part_idx, i in enumerate(range(0, x.shape[1], mid_length)):\n                part_len = x[:, i:i + mid_length].shape[1]\n                with type_ctx:\n                    outputs = model(x[:, i:i + mid_length], targets=y[:, i:i+mid_length].contiguous(), get_logits=True, use_cache=extra_args.use_cache)\n                val_loss = outputs['loss'] * part_len + val_loss \n                acc = ((outputs['logits'].argmax(-1) == y[:, i:i+mid_length]).float().sum()) + acc \n                cnt += part_len\n                while len(loss_step_list_val) <= part_idx:\n                    loss_step_list_val.append([])\n                loss_step_list_val[part_idx].append(outputs['loss'].item())\n            val_loss /= cnt\n            acc /= cnt\n            \n            loss_list_val.append(val_loss.item())\n            acc_list.append(acc.item())\n        \n\n    stats['val_acc'] = torch.as_tensor(acc_list).mean().item()\n    stats['val_loss'] = torch.as_tensor(loss_list_val).mean().item()\n    stats['val_perplexity'] = 2.71828 ** stats['val_loss']\n    stats['val_perplexity_per_chunk'] = torch.exp(torch.as_tensor(loss_step_list_val).mean(dim=1))\n\n    return stats", "\ndef main(args): \n\n\n    torch.backends.cuda.matmul.allow_tf32 = True # allows us to make sure we're able to use tensorfloat32 during training\n    torch.backends.cudnn.allow_tf32 = True\n\n    distributed_backend = distributed.make_backend_from_args(args)\n    args = distributed_backend.get_adjusted_args_for_process(args)\n\n    args.device = torch.device(args.device)\n    torch.cuda.set_device(args.device)\n    device_type = 'cuda' if 'cuda' in str(args.device) else 'cpu'\n    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    \n    print(f\"Loading dataset '{args.dataset}'\")\n\n    if distributed_backend.is_master_process():\n        prepare_dataset(args)\n    distributed_backend.sync()\n    \n    data = get_dataset(args) # data is a dict: {'train': train_tokenized, 'val': eval_tokenized}\n        \n    print(f\"Num training tokens: {len(data['train'])}\")\n    print(f\"Num validation tokens: {len(data['val'])}\")\n    \n    model = models.make_model_from_args(args).to(args.device)\n\n    checkpoint = torch.load(os.path.join(args.checkpoint, args.checkpoint_filename))\n    model.load_state_dict({x: y for x, y in checkpoint['model'].items() if \"attn.bias\" not in x and \"wpe\" not in x}, strict=False)\n\n    model = distributed_backend.transform_model(model)\n    \n    print(f\"\\Evaluating model={args.model} \\n{vars(args)}\\n\")\n\n    stats = evaluate(model, data, args.iterations, args.acc_steps, args.batch_size, args.sequence_length, \n                  distributed_backend=distributed_backend,\n                  extra_args=args)\n\n    print(stats)\n    \n    distributed_backend.finalize()", "\n\nif __name__ == \"__main__\":\n    args = get_args()\n    main(args)\n"]}
{"filename": "lm_benchmark/distributed/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom . import ddp\nfrom . import single\n", "from . import single\n\nBACKEND_TYPE_TO_MODULE_MAP = {\n    \"nccl\": ddp.DataParallelDistributedBackend,\n    None: single.SinlgeNodeBackend,\n}\n\n\ndef make_backend_from_args(args):\n    return BACKEND_TYPE_TO_MODULE_MAP[args.distributed_backend](args)", "def make_backend_from_args(args):\n    return BACKEND_TYPE_TO_MODULE_MAP[args.distributed_backend](args)\n\n\ndef registered_backends():\n    return BACKEND_TYPE_TO_MODULE_MAP.keys()\n"]}
{"filename": "lm_benchmark/distributed/backend.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import List\n\n\nclass DistributedBackend(object):\n\n    def __init__(self, args):\n        pass\n\n    def transform_model(self, model):\n        raise NotImplementedError\n\n    def get_context_for_microstep_forward(self, model, microstep_idx, gradient_accumulation_steps):\n        raise NotImplementedError\n\n    def is_master_process(self) -> bool:\n        raise NotImplementedError\n\n    def get_adjusted_args_for_process(self, args):\n        raise NotImplementedError\n\n    def get_raw_model(self, model):\n        raise NotImplementedError\n\n    def translate_model_parameter_name_for_node(self, parameter_name) -> List[str]:\n        raise NotImplementedError\n\n    def get_world_size(self):\n        raise NotImplementedError\n\n    def sync(self):\n        raise NotImplementedError\n\n    def finalize(self):\n        pass", "\n\nclass DistributedBackend(object):\n\n    def __init__(self, args):\n        pass\n\n    def transform_model(self, model):\n        raise NotImplementedError\n\n    def get_context_for_microstep_forward(self, model, microstep_idx, gradient_accumulation_steps):\n        raise NotImplementedError\n\n    def is_master_process(self) -> bool:\n        raise NotImplementedError\n\n    def get_adjusted_args_for_process(self, args):\n        raise NotImplementedError\n\n    def get_raw_model(self, model):\n        raise NotImplementedError\n\n    def translate_model_parameter_name_for_node(self, parameter_name) -> List[str]:\n        raise NotImplementedError\n\n    def get_world_size(self):\n        raise NotImplementedError\n\n    def sync(self):\n        raise NotImplementedError\n\n    def finalize(self):\n        pass", ""]}
{"filename": "lm_benchmark/distributed/single.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom contextlib import nullcontext\n\nfrom .backend import DistributedBackend\n", "from .backend import DistributedBackend\n\n\nclass SinlgeNodeBackend(DistributedBackend):\n\n    def transform_model(self, model):\n        return model\n\n    def get_context_for_microstep_forward(self, *args, **kwargs):\n        return nullcontext()\n\n    def get_adjusted_args_for_process(self, args):\n        return args\n\n    def is_master_process(self) -> bool:\n        return True\n\n    def get_raw_model(self, model):\n        return model\n\n    def get_world_size(self):\n        return 1\n\n    def sync(self):\n        pass\n\n    def translate_model_parameter_name_for_node(self, parameter_name):\n        return [parameter_name]", ""]}
{"filename": "lm_benchmark/distributed/ddp.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport math\nfrom contextlib import contextmanager\n", "from contextlib import contextmanager\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group, get_world_size, barrier\n\nfrom .backend import DistributedBackend\n\n\nclass DataParallelDistributedBackend(DistributedBackend):\n\n    def __init__(self, args):\n        self.rank = int(os.environ.get('RANK', -1))\n        assert self.rank != -1, \"DDP backend can not be used without rank\"\n        assert \"cuda\" in args.device, \"DDP backend can not be used on non-CUDA devices\"\n        init_process_group(backend=args.distributed_backend)\n        self.local_rank = int(os.environ['LOCAL_RANK'])\n\n    def get_adjusted_args_for_process(self, args):\n        effective_batch_size = args.batch_size * args.acc_steps\n        world_size = self.get_world_size()\n        if effective_batch_size % world_size != 0:\n            raise ValueError(f\"Effective batch size \"\n                             \"{effective_batch_size} is not divisible \"\n                             \"by the world size {world_size}.\")\n        acc_steps_div = math.gcd(args.acc_steps, world_size)\n        args.acc_steps = args.acc_steps // acc_steps_div\n        args.batch_size = args.batch_size // (world_size // acc_steps_div)\n        args.device = f'cuda:{self.local_rank}'\n        args.seed = args.seed + self.local_rank\n        return args\n\n    def transform_model(self, model):\n        return DDP(model, device_ids=[self.local_rank], find_unused_parameters=True)\n\n    @contextmanager\n    def get_context_for_microstep_forward(self, model, microstep_idx, gradient_accumulation_steps):\n        model.require_backward_grad_sync = (\n            microstep_idx == gradient_accumulation_steps - 1)\n        yield\n\n    def is_master_process(self) -> bool:\n        return self.rank == 0\n\n    def get_raw_model(self, model):\n        return model.module\n\n    def translate_model_parameter_name_for_node(self, parameter_name):\n        return [f'module.{parameter_name}']\n\n    def get_world_size(self):\n        return get_world_size()\n    \n    def sync(self):\n        barrier()\n\n    def finalize(self):\n        destroy_process_group()", "class DataParallelDistributedBackend(DistributedBackend):\n\n    def __init__(self, args):\n        self.rank = int(os.environ.get('RANK', -1))\n        assert self.rank != -1, \"DDP backend can not be used without rank\"\n        assert \"cuda\" in args.device, \"DDP backend can not be used on non-CUDA devices\"\n        init_process_group(backend=args.distributed_backend)\n        self.local_rank = int(os.environ['LOCAL_RANK'])\n\n    def get_adjusted_args_for_process(self, args):\n        effective_batch_size = args.batch_size * args.acc_steps\n        world_size = self.get_world_size()\n        if effective_batch_size % world_size != 0:\n            raise ValueError(f\"Effective batch size \"\n                             \"{effective_batch_size} is not divisible \"\n                             \"by the world size {world_size}.\")\n        acc_steps_div = math.gcd(args.acc_steps, world_size)\n        args.acc_steps = args.acc_steps // acc_steps_div\n        args.batch_size = args.batch_size // (world_size // acc_steps_div)\n        args.device = f'cuda:{self.local_rank}'\n        args.seed = args.seed + self.local_rank\n        return args\n\n    def transform_model(self, model):\n        return DDP(model, device_ids=[self.local_rank], find_unused_parameters=True)\n\n    @contextmanager\n    def get_context_for_microstep_forward(self, model, microstep_idx, gradient_accumulation_steps):\n        model.require_backward_grad_sync = (\n            microstep_idx == gradient_accumulation_steps - 1)\n        yield\n\n    def is_master_process(self) -> bool:\n        return self.rank == 0\n\n    def get_raw_model(self, model):\n        return model.module\n\n    def translate_model_parameter_name_for_node(self, parameter_name):\n        return [f'module.{parameter_name}']\n\n    def get_world_size(self):\n        return get_world_size()\n    \n    def sync(self):\n        barrier()\n\n    def finalize(self):\n        destroy_process_group()", ""]}
{"filename": "lm_benchmark/config/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom . import rotary\n\nCONFIG_FORMAT_TO_MODULE_MAP = {\n    \"rotary\": rotary,", "CONFIG_FORMAT_TO_MODULE_MAP = {\n    \"rotary\": rotary,\n}\n\n\ndef parse_args_with_format(format, base_parser, args, namespace):\n    return CONFIG_FORMAT_TO_MODULE_MAP[format].parse_args(base_parser, args, namespace)\n\n\ndef registered_formats():\n    return CONFIG_FORMAT_TO_MODULE_MAP.keys()", "\ndef registered_formats():\n    return CONFIG_FORMAT_TO_MODULE_MAP.keys()\n"]}
{"filename": "lm_benchmark/config/rotary.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport torch\nimport distributed\nimport models", "import distributed\nimport models\n\ndef none_or_str(value):\n    if value == 'None':\n        return None\n    return value\n\ndef none_or_int(value):\n    if value == 'None':\n        return None\n    return int(value)", "def none_or_int(value):\n    if value == 'None':\n        return None\n    return int(value)\n\ndef none_or_float(value):\n    if value == 'None':\n        return None\n    return float(value)\n\ndef parse_args(base_parser, args, namespace):\n    parser = base_parser\n    # General training params\n    parser.add_argument('--batch_size', default=50, type=int)\n    parser.add_argument('--acc_steps', default=4, type=int)\n    parser.add_argument('--seed', default=2, type=int)\n    parser.add_argument('--device', default='cuda:0', type=str)\n    parser.add_argument('--iterations', default=15000, type=int)\n    parser.add_argument('--lr', default=2e-3, type=float)\n    parser.add_argument('--warmup_percent', default=0.02, type=float)\n    parser.add_argument('--weight_decay', default=1e-3, type=float)\n    parser.add_argument('--beta1', default=0.9, type=float)\n    parser.add_argument('--beta2', default=0.95, type=float)\n    parser.add_argument('--scheduler', default='cos', choices=['linear', 'cos', 'none'])\n    parser.add_argument('--opt', default='adamw', choices=['adamw', 'sgd', 'adafactor'])\n    parser.add_argument('--eval_freq', default=200, type=int) # in iterations\n    parser.add_argument('--results_base_folder', default=\"./exps\", type=str) \n    parser.add_argument('--save_checkpoint_freq', default=None, type=int, required=False)\n\n    # Dataset params\n    parser.add_argument('--dataset', choices=['pg19', 'arxivmath'])\n    parser.add_argument('--vocab_size', default=50304, type=int)\n    parser.add_argument('--mem_freq', default=50, type=none_or_int, required=False, help=\"Frequency of landmark tokens\")\n\n    # Model params\n    parser.add_argument('--model', default='base_rotary', choices=models.registered_models())\n    parser.add_argument('--dropout', default=0.0, type=float)\n    parser.add_argument('--group_dropout', default=None, type=float, required=False)\n    parser.add_argument('--n_head', default=8, type=int)\n    parser.add_argument('--n_layer', default=12, type=int) # depths in att + ff blocks\n    parser.add_argument('--n_embd', default=1024, type=int) # embedding size / hidden size ... \n    parser.add_argument('--sequence_length', default=512, type=int)\n    parser.add_argument('--dtype', default=\"torch.bfloat16\", type=str)\n    parser.add_argument('--bias', default=False, type=bool)\n    parser.add_argument('--no_compile', action='store_true') # if true then model is not compiled \n    parser.add_argument('--run_prefix', default=None, type=str, required=False) # is added before the autogenerated experiment name\n    parser.add_argument('--exp_name', default=None, type=str, required=False) # is added before the autogenerated experiment name\n    parser.add_argument('--softmax_func', default=\"mem_opt\", type=str, required=False,\n                        choices=[\"mem_opt\", \"nomem\", \"mem\", \"ignore_mem\"])  # distributed backend type\n    parser.add_argument('--positional_encoder', default=\"rotary\", type=str, required=False,\n                        choices=models.positional_encoders.registered_encoders())  # distributed backend type\n    # logging params (WandB)\n    parser.add_argument('--wandb', action='store_true') # whether to use wandb or not\n    parser.add_argument('--wandb_project', default=\"my-project\", type=str)\n    # Distributed args\n    parser.add_argument('--distributed_backend', default=None, type=none_or_str, required=False,\n                        choices=distributed.registered_backends())  # distributed backend type\n    # Landmark tokens\n    parser.add_argument('--max_groups_for_softmax', default=16, type=int, required=False, help=\"Should be at least 2 + max. number of landmark tokens in one chunk.\")\n    # Inference\n    parser.add_argument('--use_cache', action='store_true')\n    parser.add_argument('--lm_cache', default=\"none\", type=str, required=False,\n                        choices=models.caches.registered_caches())\n    parser.add_argument('--mem_cache_size', default=None, type=int, required=False)\n    parser.add_argument('--mem_cache_freq', default=None, type=int, required=False, help=\"Frequency to add landmark tokens in the input (block size at inference)\")\n    parser.add_argument('--cache_topk', default=1, type=int, required=False)\n    parser.add_argument('--cache_selection_method', default=\"per_token_and_head\", type=str, required=False,)  \n    parser.add_argument('--eval_seq_length', default=512, type=int, required=False, help=\"Evaluation Length\")\n    parser.add_argument('--eval_sample_size', default=None, type=none_or_int, required=False, help=\"Size of the random subset of validation set used for evaluation\")\n    parser.add_argument('--mid_length', default=250, type=int, required=False, help=\"Size of chunks to break the input into\")\n    parser.add_argument('--allow_cache_during_training', action='store_true') \n    parser.add_argument('--postpone_lm_cache', action='store_true') \n    parser.add_argument('--optimization_process', default=\"landmark\", type=str, required=False,\n                        choices=[\"transformer_xl\", \"landmark\"])  # distributed backend type \n\n    # CMT Token\n    parser.add_argument('--under_rem_score_prob', default=0., type=none_or_float, required=False)\n    parser.add_argument('--rem_cutoff', default=None, type=none_or_float, required=False)\n    parser.add_argument('--enable_rem_score', default=False, action='store_true', required=False)\n\n    # Positional Augmentation\n    parser.add_argument('--pos_jump_on_mem', default=None, type=none_or_int, required=False)\n\n    # Transformer XL\n    parser.add_argument('--total_sequence_length', default=None, type=int, required=False)\n    \n\n    args = parser.parse_args(args, namespace)\n    \n    if args.exp_name is None:\n        special_name_handle_fields = {\"model\", \"lr\", \"batch_size\", \n                                      \"acc_steps\", \"seed\", \"exp_name\", \n                                      \"wandb\", \"wandb_project\",\n                                      \"run_prefix\", \"distributed_backend\", \"config_format\",\n                                      \"sequence_length\", \"mem_freq\"}\n        overriden_values = []\n        for key in vars(args):\n            if key in special_name_handle_fields:\n                continue\n            if getattr(args, key) != parser.get_default(key):\n                overriden_values.append((key, getattr(args, key)))\n        chunk_len = 10\n        overriden_values_str_parts = []\n        for chunk_id in range(0, len(overriden_values), chunk_len):\n            overriden_values_str = \"_\".join([\"{}={}\".format(key, value) for key, value in overriden_values[chunk_id:chunk_id+chunk_len]])\n            overriden_values_str_parts.append(overriden_values_str)\n        overriden_values_str = \"/\".join(overriden_values_str_parts)\n        exp_name = \"\"\n        if args.run_prefix is not None:\n            exp_name += f\"{args.run_prefix}_\"\n        exp_name += f\"{args.model}_lr{args.lr}_memfreq{args.mem_freq}_bs{args.batch_size}x{args.acc_steps}_seqlen{args.sequence_length}/{overriden_values_str}_seed={args.seed}\"\n        args.exp_name = exp_name\n\n    args.landmark_id = 50260\n    if args.dtype == \"torch.bfloat16\":\n        args.dtype = torch.bfloat16\n    elif args.dtype == \"torch.float16\":\n        args.dtype = torch.float16\n\n    landmark_freq  = max(args.mem_cache_freq or 0,  args.mem_freq or 0)\n    if landmark_freq != 0 and args.max_groups_for_softmax < args.sequence_length // landmark_freq + 1 + 2:\n        print(\"CRITICAL WARNING: Maximum number of groups for softmax is too low. Adjust with --max_groups_for_softmax.\")\n\n    \n    return args", "\ndef parse_args(base_parser, args, namespace):\n    parser = base_parser\n    # General training params\n    parser.add_argument('--batch_size', default=50, type=int)\n    parser.add_argument('--acc_steps', default=4, type=int)\n    parser.add_argument('--seed', default=2, type=int)\n    parser.add_argument('--device', default='cuda:0', type=str)\n    parser.add_argument('--iterations', default=15000, type=int)\n    parser.add_argument('--lr', default=2e-3, type=float)\n    parser.add_argument('--warmup_percent', default=0.02, type=float)\n    parser.add_argument('--weight_decay', default=1e-3, type=float)\n    parser.add_argument('--beta1', default=0.9, type=float)\n    parser.add_argument('--beta2', default=0.95, type=float)\n    parser.add_argument('--scheduler', default='cos', choices=['linear', 'cos', 'none'])\n    parser.add_argument('--opt', default='adamw', choices=['adamw', 'sgd', 'adafactor'])\n    parser.add_argument('--eval_freq', default=200, type=int) # in iterations\n    parser.add_argument('--results_base_folder', default=\"./exps\", type=str) \n    parser.add_argument('--save_checkpoint_freq', default=None, type=int, required=False)\n\n    # Dataset params\n    parser.add_argument('--dataset', choices=['pg19', 'arxivmath'])\n    parser.add_argument('--vocab_size', default=50304, type=int)\n    parser.add_argument('--mem_freq', default=50, type=none_or_int, required=False, help=\"Frequency of landmark tokens\")\n\n    # Model params\n    parser.add_argument('--model', default='base_rotary', choices=models.registered_models())\n    parser.add_argument('--dropout', default=0.0, type=float)\n    parser.add_argument('--group_dropout', default=None, type=float, required=False)\n    parser.add_argument('--n_head', default=8, type=int)\n    parser.add_argument('--n_layer', default=12, type=int) # depths in att + ff blocks\n    parser.add_argument('--n_embd', default=1024, type=int) # embedding size / hidden size ... \n    parser.add_argument('--sequence_length', default=512, type=int)\n    parser.add_argument('--dtype', default=\"torch.bfloat16\", type=str)\n    parser.add_argument('--bias', default=False, type=bool)\n    parser.add_argument('--no_compile', action='store_true') # if true then model is not compiled \n    parser.add_argument('--run_prefix', default=None, type=str, required=False) # is added before the autogenerated experiment name\n    parser.add_argument('--exp_name', default=None, type=str, required=False) # is added before the autogenerated experiment name\n    parser.add_argument('--softmax_func', default=\"mem_opt\", type=str, required=False,\n                        choices=[\"mem_opt\", \"nomem\", \"mem\", \"ignore_mem\"])  # distributed backend type\n    parser.add_argument('--positional_encoder', default=\"rotary\", type=str, required=False,\n                        choices=models.positional_encoders.registered_encoders())  # distributed backend type\n    # logging params (WandB)\n    parser.add_argument('--wandb', action='store_true') # whether to use wandb or not\n    parser.add_argument('--wandb_project', default=\"my-project\", type=str)\n    # Distributed args\n    parser.add_argument('--distributed_backend', default=None, type=none_or_str, required=False,\n                        choices=distributed.registered_backends())  # distributed backend type\n    # Landmark tokens\n    parser.add_argument('--max_groups_for_softmax', default=16, type=int, required=False, help=\"Should be at least 2 + max. number of landmark tokens in one chunk.\")\n    # Inference\n    parser.add_argument('--use_cache', action='store_true')\n    parser.add_argument('--lm_cache', default=\"none\", type=str, required=False,\n                        choices=models.caches.registered_caches())\n    parser.add_argument('--mem_cache_size', default=None, type=int, required=False)\n    parser.add_argument('--mem_cache_freq', default=None, type=int, required=False, help=\"Frequency to add landmark tokens in the input (block size at inference)\")\n    parser.add_argument('--cache_topk', default=1, type=int, required=False)\n    parser.add_argument('--cache_selection_method', default=\"per_token_and_head\", type=str, required=False,)  \n    parser.add_argument('--eval_seq_length', default=512, type=int, required=False, help=\"Evaluation Length\")\n    parser.add_argument('--eval_sample_size', default=None, type=none_or_int, required=False, help=\"Size of the random subset of validation set used for evaluation\")\n    parser.add_argument('--mid_length', default=250, type=int, required=False, help=\"Size of chunks to break the input into\")\n    parser.add_argument('--allow_cache_during_training', action='store_true') \n    parser.add_argument('--postpone_lm_cache', action='store_true') \n    parser.add_argument('--optimization_process', default=\"landmark\", type=str, required=False,\n                        choices=[\"transformer_xl\", \"landmark\"])  # distributed backend type \n\n    # CMT Token\n    parser.add_argument('--under_rem_score_prob', default=0., type=none_or_float, required=False)\n    parser.add_argument('--rem_cutoff', default=None, type=none_or_float, required=False)\n    parser.add_argument('--enable_rem_score', default=False, action='store_true', required=False)\n\n    # Positional Augmentation\n    parser.add_argument('--pos_jump_on_mem', default=None, type=none_or_int, required=False)\n\n    # Transformer XL\n    parser.add_argument('--total_sequence_length', default=None, type=int, required=False)\n    \n\n    args = parser.parse_args(args, namespace)\n    \n    if args.exp_name is None:\n        special_name_handle_fields = {\"model\", \"lr\", \"batch_size\", \n                                      \"acc_steps\", \"seed\", \"exp_name\", \n                                      \"wandb\", \"wandb_project\",\n                                      \"run_prefix\", \"distributed_backend\", \"config_format\",\n                                      \"sequence_length\", \"mem_freq\"}\n        overriden_values = []\n        for key in vars(args):\n            if key in special_name_handle_fields:\n                continue\n            if getattr(args, key) != parser.get_default(key):\n                overriden_values.append((key, getattr(args, key)))\n        chunk_len = 10\n        overriden_values_str_parts = []\n        for chunk_id in range(0, len(overriden_values), chunk_len):\n            overriden_values_str = \"_\".join([\"{}={}\".format(key, value) for key, value in overriden_values[chunk_id:chunk_id+chunk_len]])\n            overriden_values_str_parts.append(overriden_values_str)\n        overriden_values_str = \"/\".join(overriden_values_str_parts)\n        exp_name = \"\"\n        if args.run_prefix is not None:\n            exp_name += f\"{args.run_prefix}_\"\n        exp_name += f\"{args.model}_lr{args.lr}_memfreq{args.mem_freq}_bs{args.batch_size}x{args.acc_steps}_seqlen{args.sequence_length}/{overriden_values_str}_seed={args.seed}\"\n        args.exp_name = exp_name\n\n    args.landmark_id = 50260\n    if args.dtype == \"torch.bfloat16\":\n        args.dtype = torch.bfloat16\n    elif args.dtype == \"torch.float16\":\n        args.dtype = torch.float16\n\n    landmark_freq  = max(args.mem_cache_freq or 0,  args.mem_freq or 0)\n    if landmark_freq != 0 and args.max_groups_for_softmax < args.sequence_length // landmark_freq + 1 + 2:\n        print(\"CRITICAL WARNING: Maximum number of groups for softmax is too low. Adjust with --max_groups_for_softmax.\")\n\n    \n    return args", ""]}
{"filename": "lm_benchmark/data/arxiv_math.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport zipfile\nimport urllib\nimport numpy as np", "import urllib\nimport numpy as np\nimport tiktoken\nimport torch\nimport regex\nimport multiprocessing\nimport itertools\nimport functools\n\nfrom .utils import add_mem_tokens", "\nfrom .utils import add_mem_tokens\n\nARXIVMATH_ORIGINAL_PATH = \"./data/proof-pile/\"\n\n\ndef get_path(config):\n    dataset_name = f\"arxiv_mem={config.mem_freq}\"\n    return os.path.join(os.path.dirname(__file__), f\"datasets/{dataset_name}/\")\n    \ndef prepare_arxivmath_data(config):\n    DATA_PATH = get_path(config)\n    print(DATA_PATH)\n    os.makedirs(DATA_PATH, exist_ok=True)\n    if not os.path.exists(os.path.join(DATA_PATH, 'train.bin')):\n        train_data = np.memmap(os.path.join(ARXIVMATH_ORIGINAL_PATH, 'train.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_train = add_mem_tokens(config.landmark_id, train_data, config.mem_freq)\n        train_tokenized = np.array(raw_tokenized_train, dtype=np.uint16) \n        train_tokenized.tofile(os.path.join(DATA_PATH, 'train.bin'))\n    \n    if not os.path.exists(os.path.join(DATA_PATH, 'val.bin')):\n        val_data = np.memmap(os.path.join(ARXIVMATH_ORIGINAL_PATH, 'validation.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_eval = add_mem_tokens(config.landmark_id, val_data, config.mem_freq)\n        eval_tokenized = np.array(raw_tokenized_eval, dtype=np.uint16)\n        eval_tokenized.tofile(os.path.join(DATA_PATH, 'val.bin'))\n    print(\"completed the tokenization process!\")", "    \ndef prepare_arxivmath_data(config):\n    DATA_PATH = get_path(config)\n    print(DATA_PATH)\n    os.makedirs(DATA_PATH, exist_ok=True)\n    if not os.path.exists(os.path.join(DATA_PATH, 'train.bin')):\n        train_data = np.memmap(os.path.join(ARXIVMATH_ORIGINAL_PATH, 'train.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_train = add_mem_tokens(config.landmark_id, train_data, config.mem_freq)\n        train_tokenized = np.array(raw_tokenized_train, dtype=np.uint16) \n        train_tokenized.tofile(os.path.join(DATA_PATH, 'train.bin'))\n    \n    if not os.path.exists(os.path.join(DATA_PATH, 'val.bin')):\n        val_data = np.memmap(os.path.join(ARXIVMATH_ORIGINAL_PATH, 'validation.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_eval = add_mem_tokens(config.landmark_id, val_data, config.mem_freq)\n        eval_tokenized = np.array(raw_tokenized_eval, dtype=np.uint16)\n        eval_tokenized.tofile(os.path.join(DATA_PATH, 'val.bin'))\n    print(\"completed the tokenization process!\")", "\n\ndef get_arxivmath_data(config):\n    DATA_PATH = get_path(config)\n    \n    train_data = np.memmap(os.path.join(DATA_PATH, 'train.bin'), dtype=np.uint16, mode='r')\n    val_data = np.memmap(os.path.join(DATA_PATH, 'val.bin'), dtype=np.uint16, mode='r')\n\n    return {'train': train_data, 'val': val_data}\n", ""]}
{"filename": "lm_benchmark/data/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom . import pg19, arxiv_math\n\nPREPARE_GET_DATASET_MAP = {\n    \"pg19\": (pg19.prepare_pg19_data, pg19.get_pg19_data),", "PREPARE_GET_DATASET_MAP = {\n    \"pg19\": (pg19.prepare_pg19_data, pg19.get_pg19_data),\n    \"arxivmath\": (arxiv_math.prepare_arxivmath_data, arxiv_math.get_arxivmath_data)\n}\n\n\ndef prepare_dataset(args):\n    \"\"\" Fetch the right dataset given by the args.dataset parameter. The logic for each dataset is\n     contained in its own pythin file. The expected format at the moment is a disctionary of np.memmap\n     containing two keys: 'train' and 'val', corresponding to the tokenized training and validation data. \"\"\"\n    return PREPARE_GET_DATASET_MAP[args.dataset][0](args)", "\ndef get_dataset(args):\n    \"\"\" Fetch the right dataset given by the args.dataset parameter. The logic for each dataset is\n     contained in its own pythin file. The expected format at the moment is a disctionary of np.memmap\n     containing two keys: 'train' and 'val', corresponding to the tokenized training and validation data. \"\"\"\n    return PREPARE_GET_DATASET_MAP[args.dataset][1](args)\n"]}
{"filename": "lm_benchmark/data/utils.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\nimport multiprocessing\nimport itertools", "import multiprocessing\nimport itertools\nimport functools\n\n\ndef apply_add_mem_tokens(mem_id, tokens_filename, freq, start_idx, end_idx):\n    tokens = np.memmap(tokens_filename, dtype=np.uint16, mode='r')\n    print(f\"Processing {start_idx}-{end_idx}\")\n    tokens_with_mem = []\n    for t_idx in range(start_idx, end_idx):\n        t =  tokens[t_idx]\n        tokens_with_mem.append(t)\n        if freq is not None and t_idx % freq == freq - 1:\n            tokens_with_mem.append(mem_id)\n    return tokens_with_mem", "\ndef add_mem_tokens(mem_id, tokens, freq, n_workers=32):\n    print(len(tokens))\n    with multiprocessing.Pool(n_workers) as pool:\n        ids = list(range(0, len(tokens), 10 * 1000 * 1000))\n        pair_ids = zip(ids, ids[1:] + [len(tokens)])\n        apply = functools.partial(apply_add_mem_tokens, mem_id, tokens.filename, freq)\n        return list(itertools.chain.from_iterable(pool.starmap(apply, pair_ids)))\n", ""]}
{"filename": "lm_benchmark/data/pg19.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport zipfile\nimport urllib\nimport numpy as np", "import urllib\nimport numpy as np\nimport tiktoken\nimport torch\nimport regex\nimport multiprocessing\nimport itertools\nimport functools\n\nfrom .utils import add_mem_tokens", "\nfrom .utils import add_mem_tokens\n\n\nPG19_ORIGINAL_PATH = \"./data/pg19\"\n\n\ndef get_path(config):\n    dataset_name = f\"pg19_mem={config.mem_freq}\"\n    return os.path.join(os.path.dirname(__file__), f\"datasets/{dataset_name}/\")", "    \ndef prepare_pg19_data(config):\n    DATA_PATH = get_path(config)\n    print(DATA_PATH)\n    os.makedirs(DATA_PATH, exist_ok=True)\n    if not os.path.exists(os.path.join(DATA_PATH, 'train.bin')):\n        train_data = np.memmap(os.path.join(PG19_ORIGINAL_PATH, 'train.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_train = add_mem_tokens(config.landmark_id, train_data, config.mem_freq)\n        train_tokenized = np.array(raw_tokenized_train, dtype=np.uint16) \n        train_tokenized.tofile(os.path.join(DATA_PATH, 'train.bin'))\n    \n    if not os.path.exists(os.path.join(DATA_PATH, 'val.bin')):\n        val_data = np.memmap(os.path.join(PG19_ORIGINAL_PATH, 'validation.bin'), dtype=np.uint16, mode='r')\n        raw_tokenized_eval = add_mem_tokens(config.landmark_id, val_data, config.mem_freq)\n        eval_tokenized = np.array(raw_tokenized_eval, dtype=np.uint16)\n        eval_tokenized.tofile(os.path.join(DATA_PATH, 'val.bin'))\n    print(\"completed the tokenization process!\")", "\n\ndef get_pg19_data(config):\n    DATA_PATH = get_path(config)\n    \n    train_data = np.memmap(os.path.join(DATA_PATH, 'train.bin'), dtype=np.uint16, mode='r')\n    val_data = np.memmap(os.path.join(DATA_PATH, 'val.bin'), dtype=np.uint16, mode='r')\n\n    return {'train': train_data, 'val': val_data}\n", ""]}
{"filename": "lm_benchmark/data/proof-pile/prepare.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\n\nfrom datasets import load_dataset", "\nfrom datasets import load_dataset\nimport numpy as np\nimport tiktoken\nfrom tqdm import tqdm\n\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\n\n", "\n\ndataset = load_dataset(\"hoskinson-center/proof-pile\", cache_dir=os.path.join(dir_path, \"cache\"))\n\n\nnum_proc = 16\narxiv = dataset.filter(lambda x: json.loads(x['meta']).get('config', None) == \"arxiv\", num_proc=num_proc)\n\n\nenc = tiktoken.get_encoding(\"gpt2\")\ndef process(example):\n    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n    out = {'ids': ids, 'len': len(ids)}\n    return out", "\nenc = tiktoken.get_encoding(\"gpt2\")\ndef process(example):\n    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n    out = {'ids': ids, 'len': len(ids)}\n    return out\n\n# tokenize the dataset", "\n# tokenize the dataset\ntokenized = arxiv.map(\n    process,\n    remove_columns=['text'],\n    desc=\"tokenizing the splits\",\n    num_proc=num_proc,\n)\n\n\nfor split, dset in tokenized.items():\n    arr_len = np.sum(dset['len'])\n    filename = os.path.join(dir_path, f'{split}.bin')\n    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n    total_batches = 1024\n\n    idx = 0\n    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n        # Batch together samples for faster write\n        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n        arr_batch = np.concatenate(batch['ids'])\n        # Write into mmap\n        arr[idx : idx + len(arr_batch)] = arr_batch\n        idx += len(arr_batch)\n    arr.flush()", "\n\nfor split, dset in tokenized.items():\n    arr_len = np.sum(dset['len'])\n    filename = os.path.join(dir_path, f'{split}.bin')\n    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n    total_batches = 1024\n\n    idx = 0\n    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n        # Batch together samples for faster write\n        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n        arr_batch = np.concatenate(batch['ids'])\n        # Write into mmap\n        arr[idx : idx + len(arr_batch)] = arr_batch\n        idx += len(arr_batch)\n    arr.flush()", ""]}
{"filename": "lm_benchmark/data/pg19/prepare.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tiktoken\n\nimport numpy as np", "\nimport numpy as np\n\n\n\ngpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\ndef _read_directory(path):\n    texts = []\n    for filename in os.listdir(path):\n        if filename.endswith(\".txt\") and filename[:-4].isnumeric():\n            print(filename)\n            with open(os.path.join(path, filename), 'r') as f:\n                texts += gpt2_tokenizer.encode_ordinary(f.read())\n                texts.append(gpt2_tokenizer.eot_token)\n    return np.array(texts, dtype=np.uint16)", "\n\nraw_eval_data = _read_directory(\"validation\")\nraw_eval_data.tofile('validation.bin')\nraw_train_data = _read_directory(\"train\")\nraw_train_data.tofile('train.bin')\n"]}
{"filename": "lm_benchmark/optim/transformer_xl.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport wandb\nimport time \nimport copy\nimport traceback\n\nfrom .utils import get_batch, save_checkpoint\n\n", "\n\n@torch.no_grad()\ndef eval(model, data_tensor, sequence_length, total_sequence_length, batch_size, device='cpu', max_num_batches=24, ctx=nullcontext()):\n    assert model.training == False\n\n    loss_list_val, acc_list = [], []\n\n    for _ in range(max_num_batches): \n        x, y = get_batch(data_tensor, total_sequence_length, batch_size, device=device)\n        model.clear_state()\n        total_loss = None\n        for idx in range(0, x.shape[1], sequence_length):\n            x_part = x[:, idx:idx+sequence_length]\n            y_part = y[:, idx:idx+sequence_length].contiguous()\n            with ctx:\n                outputs = model(x_part, targets=y_part, get_logits=True, use_cache=True)\n            val_loss = outputs['loss']\n            if idx == 0:\n                total_loss = val_loss\n            else:\n                total_loss += val_loss\n        loss_list_val.append(total_loss)\n        acc_list.append((outputs['logits'].argmax(-1) == y_part).float().mean())\n\n    val_acc = torch.stack(acc_list).mean().item()\n    val_loss = torch.stack(loss_list_val).mean().item()\n    val_perplexity = 2.71828 ** val_loss\n\n    return val_acc, val_loss, val_perplexity", "\ndef train_xl(model, opt, data, scheduler, iterations, acc_steps, batch_size, sequence_length, eval_freq, ckpt_path, distributed_backend, extra_args):\n    device_type = 'cuda' if 'cuda' in str(extra_args.device) else 'cpu'\n    type_ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n        device_type=device_type, dtype=extra_args.dtype)  # extra_args.dtype)\n    itr, substep, best_val_loss, text_table = 0, 0, float('inf'), None # best_val_loss not used atm, early stopping not recommended but possible \n\n    stats = {'train_loss': [], 'val_loss': [], 'val_pp': [], 'val_acc': []}\n\n    num_substeps_per_epoch = len(data['train']) // (batch_size * sequence_length)\n    \n    if not extra_args.no_compile:\n        print(f\"Compiling model ...\")\n        import torch._dynamo as torchdynamo\n        torchdynamo.config.guard_nn_modules = True\n        model = torch.compile(model) # requires pytorch 2.0+\n\n    model.train()\n\n    if extra_args.postpone_lm_cache:\n        distributed_backend.get_raw_model(model).init_cache()\n\n    t0 = time.time()\n    while itr < iterations:\n        for microstep_idx in range(acc_steps):  # gradient accumulation\n            x, y = get_batch(data['train'], extra_args.total_sequence_length, batch_size, device=extra_args.device)\n            distributed_backend.get_raw_model(model).clear_state()\n            total_loss = None\n            for idx in range(0, x.shape[1], extra_args.sequence_length):\n                with type_ctx:\n                    with distributed_backend.get_context_for_microstep_forward(model=model, microstep_idx=microstep_idx, gradient_accumulation_steps=acc_steps):\n                        outputs = model(x[:, idx:idx+extra_args.sequence_length], targets=y[:, idx:idx+extra_args.sequence_length].contiguous(), use_cache=True)\n                \n                loss = outputs['loss']\n                loss.backward()\n                if idx == 0:\n                    total_loss = loss\n                else:\n                    total_loss += loss\n            substep += 1\n\n        opt.step()\n        scheduler.step()\n        opt.zero_grad(set_to_none=True)\n        itr += 1\n\n        if itr % eval_freq == 0 or itr == iterations: # from here it's only evaluation code, all the training is above\n            if distributed_backend.is_master_process():\n                t1 = time.time()\n                dt = t1 - t0\n                epoch = substep//num_substeps_per_epoch\n\n                model.eval()\n                train_loss = loss.detach().cpu().item()\n                current_lr = scheduler.get_last_lr()[0] if scheduler is not None else extra_args.lr\n                val_acc, val_loss, val_perplexity = eval(distributed_backend.get_raw_model(model), data['val'], sequence_length, extra_args.total_sequence_length, \n                                                         batch_size, extra_args.device, max_num_batches=24, ctx=type_ctx)\n\n                print_string = f\"{epoch}/{itr} [train] loss={train_loss:.3f} [val] loss={val_loss:.3f}, pp={val_perplexity:.2f}, acc={val_acc:3f}\"\n                print_string += f\" [time per itr] {dt*1000/eval_freq:.2f}ms\"\n                if scheduler is not None:\n                    print_string += f\" [lr] {current_lr:.5f}\"\n                print(print_string)\n\n                if extra_args.wandb:\n                    wandb.log({\n                        \"iter\": itr,\n                        \"train/loss\": train_loss,\n                        \"val/loss\": val_loss,\n                        \"val/perplexity\": val_perplexity,\n                        \"val/acc\": val_acc,\n                        \"lr\": current_lr,\n                    })\n\n                model.train()\n                t0 = time.time()\n\n    if distributed_backend.is_master_process():\n        print(f\"saving checkpoint to {ckpt_path}\")\n        save_checkpoint(distributed_backend=distributed_backend,\n                        model=model,\n                        opt=opt,\n                        scheduler=scheduler,\n                        itr=itr,\n                        ckpt_path=ckpt_path)\n\n    return stats", ""]}
{"filename": "lm_benchmark/optim/base.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport wandb\nimport time \nimport copy\nimport traceback\n\nfrom .utils import get_batch, save_checkpoint\n\n@torch.no_grad()\ndef eval(model, data_tensor, sequence_length, batch_size, device='cpu', max_num_batches=24, ctx=nullcontext()):\n    assert model.training == False\n\n    loss_list_val, acc_list = [], []\n\n    for _ in range(max_num_batches): \n        x, y = get_batch(data_tensor, sequence_length, batch_size, device=device)\n        with ctx:\n            outputs = model(x, targets=y, get_logits=True)\n        val_loss = outputs['loss']\n        loss_list_val.append(val_loss)\n        acc_list.append((outputs['logits'].argmax(-1) == y).float().mean())\n\n    val_acc = torch.stack(acc_list).mean().item()\n    val_loss = torch.stack(loss_list_val).mean().item()\n    val_perplexity = 2.71828 ** val_loss\n\n    return val_acc, val_loss, val_perplexity", "\n@torch.no_grad()\ndef eval(model, data_tensor, sequence_length, batch_size, device='cpu', max_num_batches=24, ctx=nullcontext()):\n    assert model.training == False\n\n    loss_list_val, acc_list = [], []\n\n    for _ in range(max_num_batches): \n        x, y = get_batch(data_tensor, sequence_length, batch_size, device=device)\n        with ctx:\n            outputs = model(x, targets=y, get_logits=True)\n        val_loss = outputs['loss']\n        loss_list_val.append(val_loss)\n        acc_list.append((outputs['logits'].argmax(-1) == y).float().mean())\n\n    val_acc = torch.stack(acc_list).mean().item()\n    val_loss = torch.stack(loss_list_val).mean().item()\n    val_perplexity = 2.71828 ** val_loss\n\n    return val_acc, val_loss, val_perplexity", "\n\ndef train_base(model, opt, data, scheduler, iterations, acc_steps, batch_size, sequence_length, eval_freq, ckpt_path, distributed_backend, extra_args):\n    device_type = 'cuda' if 'cuda' in str(extra_args.device) else 'cpu'\n    type_ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n        device_type=device_type, dtype=extra_args.dtype)  # extra_args.dtype)\n    itr, substep, best_val_loss, text_table = 0, 0, float('inf'), None # best_val_loss not used atm, early stopping not recommended but possible \n\n    stats = {'train_loss': [], 'val_loss': [], 'val_pp': [], 'val_acc': []}\n\n    num_substeps_per_epoch = len(data['train']) // (batch_size * sequence_length)\n    \n    if not extra_args.no_compile:\n        print(f\"Compiling model ...\")\n        import torch._dynamo as torchdynamo\n        torchdynamo.config.guard_nn_modules = True\n        model = torch.compile(model) # requires pytorch 2.0+\n\n    model.train()\n\n    t0 = time.time()\n\n    while itr < iterations:\n                \n        for microstep_idx in range(acc_steps):  # gradient accumulation\n            x, y = get_batch(data['train'], sequence_length, batch_size, device=extra_args.device)\n            with type_ctx:\n                with distributed_backend.get_context_for_microstep_forward(model=model, microstep_idx=microstep_idx, gradient_accumulation_steps=acc_steps):\n                    if getattr(distributed_backend.get_raw_model(model), \"needs_iter\", False):\n                        outputs = model(x, targets=y, iter=itr)\n                    else:\n                        outputs = model(x, targets=y)\n\n            loss = outputs['loss']\n            loss.backward()\n            substep += 1\n\n        opt.step()\n        scheduler.step()\n        opt.zero_grad(set_to_none=True)\n        itr += 1\n\n        if itr % eval_freq == 0 or itr == iterations: # from here it's only evaluation code, all the training is above\n            if distributed_backend.is_master_process():\n                t1 = time.time()\n                dt = t1 - t0\n                epoch = substep//num_substeps_per_epoch\n\n                model.eval()\n                train_loss = loss.detach().cpu().item()\n                current_lr = scheduler.get_last_lr()[0] if scheduler is not None else extra_args.lr\n                val_acc, val_loss, val_perplexity = eval(model, data['val'], sequence_length, batch_size,\n                                                         extra_args.device, max_num_batches=24, ctx=type_ctx)\n\n                print_string = f\"{epoch}/{itr} [train] loss={train_loss:.3f} [val] loss={val_loss:.3f}, pp={val_perplexity:.2f}, acc={val_acc:3f}\"\n                print_string += f\" [time per itr] {dt*1000/eval_freq:.2f}ms\"\n                if scheduler is not None:\n                    print_string += f\" [lr] {current_lr:.5f}\"\n                print(print_string)\n\n                if extra_args.wandb:\n                    wandb.log({\n                        \"iter\": itr,\n                        \"train/loss\": train_loss,\n                        \"val/loss\": val_loss,\n                        \"val/perplexity\": val_perplexity,\n                        \"val/acc\": val_acc,\n                        \"lr\": current_lr,\n                    })\n\n                model.train()\n                t0 = time.time()\n        if distributed_backend.is_master_process():\n            if extra_args.save_checkpoint_freq is not None and itr % extra_args.save_checkpoint_freq == 0:\n                print(f\"saving checkpoint to {ckpt_path}/ckpt_{itr}.pt\")\n                save_checkpoint(distributed_backend=distributed_backend,\n                                model=model,\n                                opt=opt,\n                                scheduler=scheduler,\n                                itr=itr,\n                                ckpt_path=f\"{ckpt_path}/ckpt_{itr}.pt\")\n\n    if distributed_backend.is_master_process():\n        print(f\"saving checkpoint to {ckpt_path}\")\n        save_checkpoint(distributed_backend=distributed_backend,\n                        model=model,\n                        opt=opt,\n                        scheduler=scheduler,\n                        itr=itr,\n                        ckpt_path=f\"{ckpt_path}/ckpt.pt\")\n\n    return stats", ""]}
{"filename": "lm_benchmark/optim/utils.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom contextlib import nullcontext, contextmanager, ExitStack", "import torch.nn.functional as F\nfrom contextlib import nullcontext, contextmanager, ExitStack\n\n\ndef get_batch(data, seq_length, batch_size, device='cpu'):\n    ix = torch.randint(len(data) - seq_length - 1, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+seq_length]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+seq_length+1]).astype(np.int64)) for i in ix])\n    y = torch.where(y[:, :-1] == 50260, y[:, 1:], y[:, :-1])\n    y = torch.where((x == 50260) | (x == 50256) , -1, y)\n    if device != 'cpu':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n        #x, y = x.to(device), y.to(device)\n    return x, y", "\n\ndef save_checkpoint(distributed_backend, model, opt, scheduler, itr, ckpt_path, **extra_args):\n\n    checkpoint = dict({\n        'model': distributed_backend.get_raw_model(model).state_dict(),\n        'optimizer': opt.state_dict(),\n        'scheduler': scheduler.state_dict(),\n        'itr': itr,\n    }, **extra_args)\n\n    torch.save(checkpoint, ckpt_path)", ""]}
{"filename": "lm_benchmark/models/base_new.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport inspect\n\nimport tiktoken", "\nimport tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom . import positional_encoders, caches\n\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        self.cache_storage = lm_cache.get_storage_for_layer(self)\n        self.allow_cache_during_training = getattr(config, \"allow_cache_during_training\", False)\n\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        bias = torch.tril(torch.ones(config.sequence_length, config.sequence_length))\n        self.register_buffer(\"bias\", bias.view(1, 1, config.sequence_length, config.sequence_length))\n    \n    def forward(self, x, pos_emb_closure, cache_context, start_index):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        q = pos_emb_closure.adapt_queries(q, start_index=start_index)\n        if cache_context is not None:\n            att_prefix, cache_values_dict = \\\n                self.cache_storage.retrieve_for_query(q, cache_context, pos_emb_closure, start_index)\n            self.cache_storage.store_in_cache(k, {'v': v})\n            if self.training and att_prefix and not self.allow_cache_during_training:\n                raise ValueError(\"Cache is not allowed during training\")\n        else:\n            att_prefix = None\n        \n        k = pos_emb_closure.adapt_keys(k, start_index=start_index)\n        \n\n        # manual implementation of attention\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = pos_emb_closure.adapt_attention_before_softmax(att, start_query_index=start_index, start_key_index=start_index)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        if att_prefix is not None:\n            prefix_size = att_prefix.shape[-1]\n            current_size = att.shape[-1]\n            att = torch.cat((att_prefix, att), dim=-1)\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        if att_prefix is not None:\n            att_prefix, att = torch.split(att, (prefix_size, current_size), dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        if att_prefix is not None:\n            cache_v = cache_values_dict['v']\n            if cache_v.ndim == v.ndim:\n                y += att_prefix @ cache_v\n            elif cache_v.ndim == v.ndim + 1:\n                y += (att_prefix.unsqueeze(3) @ cache_v).squeeze(3)\n            else:\n                raise NotImplementedError\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y", "\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n        self.activation = nn.GELU()\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.activation(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config, lm_cache)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x, pos_emb_closure, cache_context, start_index):\n        x = x + self.attn(self.ln_1(x), pos_emb_closure, cache_context, start_index)\n        x = x + self.mlp(self.ln_2(x))\n        return x", "    \n\nclass GPTBase(nn.Module):\n\n    needs_iter = False\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n        self.lm_cache = caches.get_cache(config.lm_cache)(config)\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = positional_encoders.get_encoder(config.positional_encoder)(config),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config, self.lm_cache) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= sum(p.numel() for p in self.transformer.wpe.parameters()) # TODO: Why do we need this?\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None, get_logits=False, use_cache=False, iter=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.sequence_length, f\"Cannot forward sequence of length {t}, block size is only {self.config.sequence_length}\"\n        \n        \n        # forward the GPT model itself\n        if use_cache:\n            idx, index_shift, cache_context = self.lm_cache(idx)\n        else:\n            index_shift = 0\n            cache_context = None\n        if getattr(self.transformer.wpe, \"needs_iter\", False):\n            idx, pos_emb_closure = self.transformer.wpe(idx, iter=iter) # position embeddings of shape (1, t, n_embd)\n        else:\n            idx, pos_emb_closure = self.transformer.wpe(idx) # position embeddings of shape (1, t, n_embd)\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        x = pos_emb_closure.adapt_model_input(tok_emb, start_index=index_shift)\n        x = self.transformer.drop(x)\n        for block in self.transformer.h:\n            x = block(x, pos_emb_closure, cache_context, start_index=index_shift)\n        x = self.transformer.ln_f(x)\n\n        if use_cache:\n            x = self.lm_cache.get_final_logits(x)\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n        logits = logits if get_logits else None\n        return {'logits': logits, 'loss': loss}\n\n    def clear_state(self):\n        self.lm_cache.clear_state()\n\n    def crop_sequence_length(self, sequence_length):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert sequence_length <= self.config.sequence_length\n        self.config.sequence_length = sequence_length\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:sequence_length])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:sequence_length,:sequence_length]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        # TODO\n        pass\n\n    def get_parameter_group_specs(self):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n        # will appear in the no_decay and decay sets respectively after the above.\n        # In addition, because named_parameters() doesn't return duplicates, it\n        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n        # so let's manually remove 'lm_head.weight' from decay set. This will include\n        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n        decay.remove('lm_head.weight')\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        return [\n            {\"params\": sorted(list(decay))},\n            {\"params\": sorted(list(no_decay)), \"weight_decay\": 0.0},\n        ]\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at sequence_length\n            idx_cond = idx if idx.size(1) <= self.config.sequence_length else idx[:, -self.config.sequence_length:]\n            # forward the model to get the logits for the index in the sequence\n            logits = self(idx_cond, get_logits=True)['logits']\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n    \n    @torch.no_grad()\n    def generate_from_string(self, in_str, max_new_tokens, temperature=1.0, top_k=None):\n        idx = torch.tensor(self.tokenizer.encode(in_str, allowed_special={\"<|endoftext|>\"})).view(1,-1).to(self.lm_head.weight.device)\n        out_idx = self.generate(idx, max_new_tokens, temperature, top_k).view(-1).to('cpu').numpy()\n        return self.tokenizer.decode(out_idx)", ""]}
{"filename": "lm_benchmark/models/landmark.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport inspect\n\nimport tiktoken", "\nimport tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom . import positional_encoders, caches\n\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx, rem_score=None):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False)\n          \n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(dim, resp_mem_idx, x_exp, )\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        \n        \n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs)\n\n        return probs\n\n    @staticmethod\n    def backward(ctx, grad_probs):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n\n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx", "\ndef landmark_grouped_softmax(x, dim, is_mem, attn_mask,  last_section_mask, return_group_prob,\n                             max_mem_cnt, p_group_dropout=None):\n    \n    mask = attn_mask\n\n    last_and_rest_mask = last_section_mask | mask\n\n    full_access_mask =  is_mem | last_and_rest_mask\n\n    # assert max_mem_cnt >= (is_mem.sum(dim=dim).max().view((1,)) + 2 + 1).item()\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    mem_bucket_id = max_mem_cnt - 1\n\n    resp_mem_idx = torch.where(last_and_rest_mask, \n                                max_mem_cnt - 1,\n                                torch.where(is_mem, mem_bucket_id, mem_group_idx))\n    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\n\n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape, ))\n    group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs)\n    if p_group_dropout is not None:\n        group_prob = torch.nn.functional.dropout(group_prob, p=p_group_dropout, inplace=True)\n        group_prob.select(dim, max_mem_cnt - 1).copy_((probs * last_section_mask).sum(dim=dim, )).div_(1 - p_group_dropout)\n    else:\n        group_prob.select(dim, max_mem_cnt - 1).copy_((probs * last_section_mask).sum(dim=dim, ))\n    probs = probs.mul(torch.where(full_access_mask, last_section_mask, torch.gather(group_prob, dim, resp_mem_idx)))\n\n\n    return probs, (group_prob if return_group_prob else None)", "\ndef softmax_ignore_mem(x, dim, is_mem, *args, **kwargs):\n    x = x.masked_fill(is_mem, float('-inf'))\n    return torch.nn.functional.softmax(x, dim=dim), None\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        self.config = config\n        \n        self.allow_cache_during_training = getattr(config, \"allow_cache_during_training\", False)\n\n        self.cache_storage = None\n        if not config.postpone_lm_cache:\n            self.init_cache_storage(lm_cache)\n\n        SOFTMAX_FUNC = {\n            \"nomem\": lambda x, dim, *args, **kwargs: (nn.functional.softmax(x, dim=dim), None),\n            \"mem_opt\": landmark_grouped_softmax,\n            \"ignore_mem\": softmax_ignore_mem,\n        }\n\n        self.softmax_func = SOFTMAX_FUNC[config.softmax_func]\n\n        if self.config.enable_rem_score:\n            self.rem_token_embedding = nn.Linear(config.n_embd, 1, bias=config.bias)\n        else:\n            self.rem_token_embedding = None\n\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.sequence_length, config.sequence_length))\n                                    .view(1, 1, config.sequence_length, config.sequence_length))\n\n    def init_cache_storage(self, lm_cache):\n        if self.cache_storage is not None:\n            return\n        print(\"Init Storage\")\n        self.cache_storage = lm_cache.get_storage_for_layer(self)\n\n    def forward(self, x, is_mem, pos_emb_closure, cache_context, start_index):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        q = pos_emb_closure.adapt_queries(q, start_index=start_index)\n        if cache_context is not None and self.cache_storage is not None:\n            att_prefix, cache_values_dict = \\\n                self.cache_storage.retrieve_for_query(q, cache_context, pos_emb_closure, start_index)\n            if self.training and att_prefix is not None and not self.allow_cache_during_training:\n                raise ValueError(\"Cache is not allowed during training\")\n        else:\n            att_prefix = None\n        \n        k_before_pos = k\n        is_mem_orig = is_mem\n        k = pos_emb_closure.adapt_keys(k, start_index=start_index)\n        \n\n        # manual implementation of attention\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = pos_emb_closure.adapt_attention_before_softmax(att, start_query_index=start_index, start_key_index=start_index)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn_mask = self.bias[:,:,:T,:T] == 0\n        if att_prefix is not None:\n            prefix_size = att_prefix.shape[-1]\n            current_size = att.shape[-1]\n            att = torch.cat((att_prefix, att), dim=-1)\n            if 'is_mem' in cache_values_dict:\n                is_mem = torch.cat((cache_values_dict['is_mem'], is_mem), dim=-1)\n            else:\n                is_mem = torch.cat((is_mem.new_zeros((B, prefix_size)), is_mem), dim=-1)\n            attn_mask = torch.cat((attn_mask.new_zeros((1, 1, T, prefix_size)), attn_mask), dim=-1)\n\n        is_mem_mat = is_mem.unsqueeze(1).unsqueeze(2).repeat((1, self.n_head, 1, 1))\n\n        # The following block should be inside landmark_grouped_softmax\n        # however, moving it there causes problems with torch.compile.\n        is_mem = is_mem_mat\n        dim = -1\n        with torch.no_grad():\n            mem_ids = torch.where(attn_mask, 0., torch.cumsum(is_mem, dim) - is_mem.int())\n            last_section_mask = torch.amax(mem_ids, dim, keepdim=True) == mem_ids\n            # the landmark token for the current block should be ignored, \n            # even if we are processing the landmark token itself.\n            mask = attn_mask.logical_or(is_mem & last_section_mask) \n            last_section_mask.logical_and_(~mask)\n            is_mem = is_mem.logical_and(~mask)\n        att = att.masked_fill_(mask, float('-inf')) \n        # End of block\n        \n        att, group_prob = self.softmax_func(att, dim=-1, \n                                            is_mem=is_mem, attn_mask=mask,\n                                            last_section_mask=last_section_mask,\n                                            return_group_prob=False,\n                                            max_mem_cnt=self.config.max_groups_for_softmax,\n                                            p_group_dropout=self.config.group_dropout if self.training else None)\n        att = self.attn_dropout(att)\n        if att_prefix is not None:\n            att_prefix, att = torch.split(att, (prefix_size, current_size), dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        if att_prefix is not None:\n            cache_v = cache_values_dict['v']\n            if cache_v.ndim == v.ndim:\n                y += att_prefix @ cache_v\n            elif cache_v.ndim == v.ndim + 1:\n                y += (att_prefix.unsqueeze(3) @ cache_v).squeeze(3)\n            else:\n                raise NotImplementedError\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n\n        if cache_context is not None and self.cache_storage is not None:\n            with torch.no_grad():\n                self.cache_storage.store_in_cache(k_before_pos, {'v': v, 'is_mem': is_mem_orig})\n            \n\n        return y, group_prob", "\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n        self.activation = nn.GELU()\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.activation(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config, lm_cache)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x, is_mem, pos_emb_closure, cache_context, start_index):\n        x_attn, group_prob = self.attn(self.ln_1(x), is_mem, pos_emb_closure, cache_context, start_index)\n        x = x + x_attn\n        x = x + self.mlp(self.ln_2(x))\n        return x, group_prob", "    \n\nclass GPTBase(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n        self.lm_cache = None\n        if not config.postpone_lm_cache:\n            self.init_cache()\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = positional_encoders.get_encoder(config.positional_encoder)(config),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(\n                config, self.lm_cache) for idx in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def init_cache(self):\n        self.lm_cache = caches.get_cache(self.config.lm_cache)(self.config)\n        for m in self.modules():\n            if hasattr(m, \"init_cache_storage\"):\n                m.init_cache_storage(self.lm_cache)\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= sum(p.numel() for p in self.transformer.wpe.parameters()) # TODO: Why do we need this?\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None, get_logits=False, use_cache=False):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.sequence_length, f\"Cannot forward sequence of length {t}, block size is only {self.config.sequence_length}\"\n        \n        # forward the GPT model itself\n        if use_cache:\n            idx, index_shift, cache_context = self.lm_cache(idx)\n        else:\n            index_shift = 0\n            cache_context = None\n        idx, pos_emb_closure = self.transformer.wpe(idx) # position embeddings of shape (1, t, n_embd)\n        is_mem = idx == self.config.landmark_id\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        x = pos_emb_closure.adapt_model_input(tok_emb, start_index=index_shift)\n        # print(\"Index shift: \", index_shift)\n        x = self.transformer.drop(x)\n        group_prob = None\n        for block in self.transformer.h:\n            x, group_prob = block(x, is_mem, pos_emb_closure, cache_context, start_index=index_shift)\n        x = self.transformer.ln_f(x)\n\n        if use_cache:\n            x = self.lm_cache.get_final_logits(x)\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n        logits = logits if get_logits else None\n        return {'logits': logits, 'loss': loss}\n\n    def clear_state(self):\n        self.lm_cache.clear_state()\n    \n    def crop_sequence_length(self, sequence_length):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert sequence_length <= self.config.sequence_length\n        self.config.sequence_length = sequence_length\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:sequence_length])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:sequence_length,:sequence_length]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        # TODO\n        pass\n\n    def get_parameter_group_specs(self):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n        # will appear in the no_decay and decay sets respectively after the above.\n        # In addition, because named_parameters() doesn't return duplicates, it\n        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n        # so let's manually remove 'lm_head.weight' from decay set. This will include\n        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n        decay.remove('lm_head.weight')\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        return [\n            {\"params\": sorted(list(decay))},\n            {\"params\": sorted(list(no_decay)), \"weight_decay\": 0.0},\n        ]\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at sequence_length\n            idx_cond = idx if idx.size(1) <= self.config.sequence_length else idx[:, -self.config.sequence_length:]\n            # forward the model to get the logits for the index in the sequence\n            logits = self(idx_cond, get_logits=True)['logits']\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n    \n    @torch.no_grad()\n    def generate_from_string(self, in_str, max_new_tokens, temperature=1.0, top_k=None):\n        idx = torch.tensor(self.tokenizer.encode(in_str, allowed_special={\"<|endoftext|>\"})).view(1,-1).to(self.lm_head.weight.device)\n        out_idx = self.generate(idx, max_new_tokens, temperature, top_k).view(-1).to('cpu').numpy()\n        return self.tokenizer.decode(out_idx)", ""]}
{"filename": "lm_benchmark/models/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom . import base_new, landmark, landmark_with_cmt\n\nMODELS = {\n    \"base\": base_new.GPTBase,", "MODELS = {\n    \"base\": base_new.GPTBase,\n    \"landmark\": landmark.GPTBase,\n    \"landmark_with_cmt\": landmark_with_cmt.GPTBase,\n}\n\n\ndef make_model_from_args(args):\n    return MODELS[args.model](args)\n", "\n\ndef registered_models():\n    return MODELS.keys()\n"]}
{"filename": "lm_benchmark/models/landmark_with_cmt.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport inspect\n\nimport tiktoken", "\nimport tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom . import positional_encoders, caches\n\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx, rem_score=None):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False)\n        if rem_score is not None:\n            t = max_by_group.select(dim=dim, index=mem_cnt - 1).unsqueeze(dim)\n            t.copy_(t.maximum(rem_score))\n            \n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        if rem_score is not None:\n            rem_score_max = max_by_group.select(dim=dim, index=mem_cnt - 1).unsqueeze(dim)\n            rem_score_exp = torch.exp(rem_score - torch.where(torch.isinf(rem_score_max), 0, rem_score_max))\n\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(dim, resp_mem_idx, x_exp, )\n        if rem_score is not None:\n            cumsum_by_group.select(dim=dim, index=mem_cnt - 1).unsqueeze(dim).add_(rem_score_exp)\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        \n        if rem_score is not None:\n            rem_denom = cumsum_by_group.select(dim=dim, index=mem_cnt-1).unsqueeze(dim)\n            final_rem_score = torch.where(rem_denom < 0.5, 0, rem_score_exp / rem_denom)\n        else:\n            final_rem_score = None\n        \n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs, final_rem_score)\n\n        return probs, final_rem_score\n\n    @staticmethod\n    def backward(ctx, grad_probs, grad_final_rem_score):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs, final_rem_score = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = grad_rem_score = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n            if final_rem_score is not None:\n                rem_pair = final_rem_score * grad_final_rem_score\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n            if final_rem_score is not None:\n                cumsum_by_group.select(dim=dim, index=mem_cnt - 1).unsqueeze(dim).add_(rem_pair)\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n        if final_rem_score is not None and ctx.needs_input_grad[4]:\n            grad_rem_score = rem_pair - final_rem_score * cumsum_by_group.select(dim=dim, index=mem_cnt-1).unsqueeze(dim)\n\n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx, grad_rem_score", "\ndef landmark_grouped_softmax(x, dim, is_mem, attn_mask, last_section_mask,\n                             rem_score, mask_rem_score, return_group_prob,\n                             max_mem_cnt, p_group_dropout=None, rem_cutoff=None,):\n\n    mask = attn_mask\n    \n    last_and_rest_mask = last_section_mask | mask\n\n    full_access_mask =  is_mem | last_and_rest_mask\n\n    # assert max_mem_cnt >= (is_mem.sum(dim=dim).max().view((1,)) + 2 + 1).item()\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    if rem_score is None:\n        mem_bucket_id = max_mem_cnt - 1\n    else:\n        mem_bucket_id = torch.where(mask_rem_score, max_mem_cnt - 1, max_mem_cnt - 2)\n    resp_mem_idx = torch.where(last_and_rest_mask, \n                                max_mem_cnt - 1,\n                                torch.where(is_mem, mem_bucket_id, mem_group_idx))\n    probs, final_rem_score = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx, rem_score)\n\n    if rem_cutoff is not None:\n        final_rem_score = torch.where(final_rem_score < rem_cutoff, 0., final_rem_score)\n    \n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape, ))\n    if rem_score is None:\n        group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs)\n    else:\n        group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), \n            torch.where(mask_rem_score, 1., final_rem_score) * probs)\n    if p_group_dropout is not None:\n        group_prob = torch.nn.functional.dropout(group_prob, p=p_group_dropout, inplace=True)\n        group_prob.select(dim, max_mem_cnt - 1).copy_((probs * last_section_mask).sum(dim=dim, )).div_(1 - p_group_dropout)\n    else:\n        group_prob.select(dim, max_mem_cnt - 1).copy_((probs * last_section_mask).sum(dim=dim, ))\n    probs = probs.mul(torch.where(full_access_mask, last_section_mask, torch.gather(group_prob, dim, resp_mem_idx)))\n\n    return probs, (group_prob if return_group_prob else None)", "\ndef softmax_ignore_mem(x, dim, is_mem, *args, **kwargs):\n    x = x.masked_fill(is_mem, float('-inf'))\n    return torch.nn.functional.softmax(x, dim=dim), None\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        self.config = config\n        \n        self.allow_cache_during_training = getattr(config, \"allow_cache_during_training\", False)\n\n        self.cache_storage = None\n        if not config.postpone_lm_cache:\n            self.init_cache_storage(lm_cache)\n\n        SOFTMAX_FUNC = {\n            \"nomem\": lambda x, dim, *args, **kwargs: (nn.functional.softmax(x, dim=dim), None),\n            \"mem_opt\": landmark_grouped_softmax,\n            \"ignore_mem\": softmax_ignore_mem,\n        }\n\n        self.softmax_func = SOFTMAX_FUNC[config.softmax_func]\n\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.sequence_length, config.sequence_length))\n                                    .view(1, 1, config.sequence_length, config.sequence_length))\n\n    def init_cache_storage(self, lm_cache):\n        if self.cache_storage is not None:\n            return\n        print(\"Init Storage\")\n        self.cache_storage = lm_cache.get_storage_for_layer(self)\n\n    def forward(self, x, is_mem, pos_emb_closure, cache_context, start_index):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        rem_token_embedding = k[:, :, 0].unsqueeze(2)\n        q = q[:, :, 1:]\n        k = k[:, :, 1:]\n        v = v[:, :, 1:]\n        T -= 1\n\n        q = pos_emb_closure.adapt_queries(q, start_index=start_index)\n\n        rem_token_embedding = pos_emb_closure.adapt_keys(rem_token_embedding, start_index=-1)\n        rem_score = (q @ rem_token_embedding.transpose(-2, -1))  * (1.0 / math.sqrt(k.size(-1)))\n        if self.config.under_rem_score_prob is not None:\n            mask_rem_score = ~torch.where(\n                torch.rand_like(is_mem, dtype=q.dtype) <= self.config.under_rem_score_prob, is_mem, False)\n        else:\n            mask_rem_score = torch.ones_like(is_mem) # nothing is controlled\n\n        if cache_context is not None and self.cache_storage is not None:\n            att_prefix, cache_values_dict = \\\n                self.cache_storage.retrieve_for_query(q, cache_context, pos_emb_closure, start_index)\n            # self.cache_storage.store_in_cache(k, {'v': v, 'is_mem': is_mem})\n            if self.training and att_prefix is not None and not self.allow_cache_during_training:\n                raise ValueError(\"Cache is not allowed during training\")\n            mask_rem_score = torch.cat((~cache_values_dict['is_mem'], mask_rem_score), dim=-1)\n        else:\n            att_prefix = None\n\n        if self.config.rem_cutoff is not None:\n            assert not self.training\n\n        mask_rem_score = mask_rem_score.view(B, 1, 1, mask_rem_score.shape[1]).expand(B, self.n_head, T, mask_rem_score.shape[1])\n        \n        k_before_pos = k\n        is_mem_orig = is_mem\n        #k2 = pos_emb_closure.adapt_keys(k, start_index=0)\n        k = pos_emb_closure.adapt_keys(k, start_index=start_index)\n        \n\n        # manual implementation of attention\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        #att2 = (q2 @ k2.transpose(-2, -1)) * (1.0 / math.sqrt(k2.size(-1)))\n        #print(start_index, (att - att2).max())\n        att = pos_emb_closure.adapt_attention_before_softmax(att, start_query_index=start_index, start_key_index=start_index)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        attn_mask = self.bias[:,:,:T,:T] == 0\n        if att_prefix is not None:\n            prefix_size = att_prefix.shape[-1]\n            current_size = att.shape[-1]\n            att = torch.cat((att_prefix, att), dim=-1)\n            if 'is_mem' in cache_values_dict:\n                is_mem = torch.cat((cache_values_dict['is_mem'], is_mem), dim=-1)\n            else:\n                is_mem = torch.cat((is_mem.new_zeros((B, prefix_size)), is_mem), dim=-1)\n            attn_mask = torch.cat((attn_mask.new_zeros((1, 1, T, prefix_size)), attn_mask), dim=-1)\n\n        #print(attn_mask.shape, attn_mask[..., 40:80])\n        is_mem_mat = is_mem.unsqueeze(1).unsqueeze(2).repeat((1, self.n_head, 1, 1))\n\n        # The following block should be inside landmark_grouped_softmax\n        # however, moving it there causes problems with torch.compile.\n        is_mem = is_mem_mat\n        dim = -1\n        with torch.no_grad():\n            mem_ids = torch.where(attn_mask, 0., torch.cumsum(is_mem, dim) - is_mem.int())\n            last_section_mask = torch.amax(mem_ids, dim, keepdim=True) == mem_ids\n            mask = attn_mask.logical_or(is_mem & last_section_mask) \n            last_section_mask.logical_and_(~mask)\n            is_mem = is_mem.logical_and(~mask)\n        att = att.masked_fill_(mask, float('-inf'))  \n        # End of block\n\n        att, group_prob = self.softmax_func(att, dim=-1, \n                                            is_mem=is_mem, attn_mask=mask,\n                                            last_section_mask=last_section_mask,\n                                            rem_score=rem_score, mask_rem_score=mask_rem_score, \n                                            return_group_prob=False, max_mem_cnt=self.config.max_groups_for_softmax,\n                                            p_group_dropout=self.config.group_dropout if self.training else None,\n                                            rem_cutoff=self.config.rem_cutoff)\n        att = self.attn_dropout(att)\n        if att_prefix is not None:\n            att_prefix, att = torch.split(att, (prefix_size, current_size), dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        if att_prefix is not None:\n            cache_v = cache_values_dict['v']\n            if cache_v.ndim == v.ndim:\n                y += att_prefix @ cache_v\n            elif cache_v.ndim == v.ndim + 1:\n                y += (att_prefix.unsqueeze(3) @ cache_v).squeeze(3)\n            else:\n                raise NotImplementedError\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(torch.cat((y.new_zeros((B, 1, C)), y), dim=1)))\n\n        if cache_context is not None and self.cache_storage is not None:\n            with torch.no_grad():\n                self.cache_storage.store_in_cache(k_before_pos, {'v': v, 'is_mem': is_mem_orig})\n            \n\n        return y, group_prob", "\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n        self.activation = nn.GELU()\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.activation(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self, config, lm_cache):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config, lm_cache)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x, is_mem, pos_emb_closure, cache_context, start_index):\n        x_attn, group_prob = self.attn(self.ln_1(x), is_mem, pos_emb_closure, cache_context, start_index)\n        x = x + x_attn\n        x = x + self.mlp(self.ln_2(x))\n        return x, group_prob", "    \n\nclass GPTBase(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.sequence_length is not None\n        self.config = config\n        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n        assert self.config.enable_rem_score\n\n        self.lm_cache = None\n        if not config.postpone_lm_cache:\n            self.init_cache()\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = positional_encoders.get_encoder(config.positional_encoder)(config),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config, self.lm_cache) for idx in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n\n        self.rem_token_embedding = nn.Embedding(1, config.n_embd)\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def init_cache(self):\n        self.lm_cache = caches.get_cache(self.config.lm_cache)(self.config)\n        for m in self.modules():\n            if hasattr(m, \"init_cache_storage\"):\n                m.init_cache_storage(self.lm_cache)\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= sum(p.numel() for p in self.transformer.wpe.parameters()) # TODO: Why do we need this?\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None, get_logits=False, use_cache=False):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.sequence_length, f\"Cannot forward sequence of length {t}, block size is only {self.config.sequence_length}\"\n        \n        # forward the GPT model itself\n        if use_cache:\n            idx, index_shift, cache_context = self.lm_cache(idx)\n        else:\n            index_shift = 0\n            cache_context = None\n        idx, pos_emb_closure = self.transformer.wpe(idx) # position embeddings of shape (1, t, n_embd)\n        is_mem = idx == self.config.landmark_id\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        rem_tok_emb = self.rem_token_embedding(idx.new_zeros((b, 1)))\n        x = pos_emb_closure.adapt_model_input(tok_emb, start_index=index_shift)\n        rem_tok_emb = pos_emb_closure.adapt_model_input(rem_tok_emb, start_index=-1)\n        # print(\"Index shift: \", index_shift)\n        x = self.transformer.drop(x)\n        x = torch.cat((rem_tok_emb, x), dim=1)\n        group_prob = None\n        for block in self.transformer.h:\n            x, group_prob = block(x, is_mem, pos_emb_closure, cache_context, start_index=index_shift)\n        x = x[:, 1:]\n        x = self.transformer.ln_f(x)\n\n        if use_cache:\n            x = self.lm_cache.get_final_logits(x)\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n        logits = logits if get_logits else None\n        return {'logits': logits, 'loss': loss}\n\n    def clear_state(self):\n        self.lm_cache.clear_state()\n    \n    def crop_sequence_length(self, sequence_length):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert sequence_length <= self.config.sequence_length\n        self.config.sequence_length = sequence_length\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:sequence_length])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:sequence_length,:sequence_length]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        # TODO\n        pass\n\n    def get_parameter_group_specs(self):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n        # will appear in the no_decay and decay sets respectively after the above.\n        # In addition, because named_parameters() doesn't return duplicates, it\n        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n        # so let's manually remove 'lm_head.weight' from decay set. This will include\n        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n        decay.remove('lm_head.weight')\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        return [\n            {\"params\": sorted(list(decay))},\n            {\"params\": sorted(list(no_decay)), \"weight_decay\": 0.0},\n        ]\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at sequence_length\n            idx_cond = idx if idx.size(1) <= self.config.sequence_length else idx[:, -self.config.sequence_length:]\n            # forward the model to get the logits for the index in the sequence\n            logits = self(idx_cond, get_logits=True)['logits']\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n    \n    @torch.no_grad()\n    def generate_from_string(self, in_str, max_new_tokens, temperature=1.0, top_k=None):\n        idx = torch.tensor(self.tokenizer.encode(in_str, allowed_special={\"<|endoftext|>\"})).view(1,-1).to(self.lm_head.weight.device)\n        out_idx = self.generate(idx, max_new_tokens, temperature, top_k).view(-1).to('cpu').numpy()\n        return self.tokenizer.decode(out_idx)", ""]}
{"filename": "lm_benchmark/models/positional_encoders/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom . import encoder, rotary, rotary_mem_jump\n\nPOS_ENCS = {\n    \"rotary\": rotary.RotaryPositionalEncoder,", "POS_ENCS = {\n    \"rotary\": rotary.RotaryPositionalEncoder,\n    \"rotary_mem_jump\": rotary_mem_jump.RotaryJumpMemPositionalEncoder\n}\n\n\ndef get_encoder(encoder_name):\n    return POS_ENCS[encoder_name]\n\n\ndef registered_encoders():\n    return POS_ENCS.keys()", "\n\ndef registered_encoders():\n    return POS_ENCS.keys()\n"]}
{"filename": "lm_benchmark/models/positional_encoders/rotary_utils.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\ndef rotate_half(x):\n    x = x.view(*x.shape[:-1], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.view(*x.shape[:-2], -1)", "def rotate_half(x):\n    x = x.view(*x.shape[:-1], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.view(*x.shape[:-2], -1)\n\ndef apply_rotary_emb(freqs, t, start_index = 0, scale = 1.):\n    #freqs = freqs.to(t)\n    rot_dim = freqs.shape[-1]\n    end_index = start_index + rot_dim\n    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n    #t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n    t = (t * freqs.cos().to(t) * scale) + (rotate_half(t) * freqs.sin().to(t) * scale)\n    #return torch.cat((t_left, t, t_right), dim = -1)\n    return t", ""]}
{"filename": "lm_benchmark/models/positional_encoders/rotary_mem_jump.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch import nn\n\nfrom .encoder import PositionalEncoder, PositionalEncoderClosure", "\nfrom .encoder import PositionalEncoder, PositionalEncoderClosure\nfrom .rotary_utils import apply_rotary_emb\n\n\nclass JumpingRotaryPositionalEncoderClosure(PositionalEncoderClosure):\n\n    def __init__(self, encoder, jumps):\n        super().__init__(encoder)\n        self.jumps = jumps\n\n    def adapt_vector_for_indices(self, v, indices):\n        #changer = torch.zeros_like(indices)\n        #changer[50::51] = 1\n        #indices -= torch.cumsum(changer, dim=-1)\n\n\n        *other_dims, T, hs = v.shape\n        if T == 0:\n            return v\n        other_dims_prefix = other_dims[:len(other_dims) - len(indices.shape) + 1]\n        if self.jumps is not None:\n            indices = indices.view([1] * len(other_dims_prefix) + list(indices.shape)).repeat(other_dims_prefix + [1] * len(indices.shape))\n            indices[..., 1:] = indices[..., 1:] + self.jumps\n            other_dims_prefix = []\n            # print(indices)\n        freqs = (indices.unsqueeze(-1) * self.encoder.freqs.view(1, -1)).unsqueeze(-1).expand(*indices.shape, -1, 2).reshape(*indices.shape, hs)\n        freqs = freqs.view([1] * len(other_dims_prefix) + list(indices.shape) + [hs]).expand(*v.shape)\n        v = apply_rotary_emb(freqs, v)\n        return v\n\n    def _adapt_keys_for_indices(self, k, indices):\n        return self.adapt_vector_for_indices(k, indices)\n\n    def adapt_queries(self, q, start_index):\n        T = q.shape[-2]\n        indices = torch.arange(start_index, T + start_index, device=q.device)\n        return self.adapt_vector_for_indices(q, indices)", "\n\nclass RotaryJumpMemPositionalEncoder(PositionalEncoder):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.max_pos_log = 4\n        self.max_pos_base = 10  \n        n_embd_per_head = config.n_embd // config.n_head\n        freqs =  (self.max_pos_base ** (-self.max_pos_log * torch.arange(0, n_embd_per_head, 2)[:(n_embd_per_head // 2)].float() / n_embd_per_head))\n        self.register_buffer(\"freqs\", freqs)\n\n    def forward(self, x):\n        if self.config.pos_jump_on_mem is not None and self.config.pos_jump_on_mem > 0:\n            #assert self.config.mem_freq is not None\n            is_mem = (x == self.config.landmark_id)\n            jumps = torch.cumsum((is_mem * torch.randint_like(x, self.config.pos_jump_on_mem))[:, :-1], dim=-1)\n            return x, self.closure_model(self, jumps.unsqueeze(1)) # (B, 1, T)\n        else:\n            return x, self.closure_model(self, None)\n\n\n    closure_model = JumpingRotaryPositionalEncoderClosure", ""]}
{"filename": "lm_benchmark/models/positional_encoders/rotary.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch import nn\n\nfrom .encoder import PositionalEncoder, PositionalEncoderClosure", "\nfrom .encoder import PositionalEncoder, PositionalEncoderClosure\nfrom .rotary_utils import apply_rotary_emb\n\n\nclass RotaryPositionalEncoderClosure(PositionalEncoderClosure):\n\n    def adapt_vector_for_indices(self, v, indices):\n        #changer = torch.zeros_like(indices)\n        #changer[50::51] = 1\n        #indices -= torch.cumsum(changer, dim=-1)\n\n        *other_dims, T, hs = v.shape\n        if T == 0:\n            return v\n        other_dims_prefix = other_dims[:len(other_dims) - len(indices.shape) + 1]\n        freqs = (indices.unsqueeze(-1) * self.encoder.freqs.view(1, -1)).unsqueeze(-1).expand(*indices.shape, -1, 2).reshape(*indices.shape, hs)\n        freqs = freqs.view([1] * len(other_dims_prefix) + list(indices.shape) + [hs]).expand(*v.shape)\n        v = apply_rotary_emb(freqs, v)\n        return v\n\n    def _adapt_keys_for_indices(self, k, indices):\n        return self.adapt_vector_for_indices(k, indices)\n\n    def adapt_queries(self, q, start_index):\n        T = q.shape[-2]\n        indices = torch.arange(start_index, T + start_index, device=q.device)\n        return self.adapt_vector_for_indices(q, indices)", "\n\nclass RotaryPositionalEncoder(PositionalEncoder):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.max_pos_log = 4\n        self.max_pos_base = 10  \n        n_embd_per_head = config.n_embd // config.n_head\n        freqs =  (self.max_pos_base ** (-self.max_pos_log * torch.arange(0, n_embd_per_head, 2)[:(n_embd_per_head // 2)].float() / n_embd_per_head))\n        self.register_buffer(\"freqs\", freqs)\n\n    closure_model = RotaryPositionalEncoderClosure", ""]}
{"filename": "lm_benchmark/models/positional_encoders/encoder.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch import nn\n\n\nclass PositionalEncoderClosure(object):\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n\n    def adapt_model_input(self, x, start_index):\n        return x\n\n    def adapt_keys(self, k, start_index=None, indices=None):\n        if indices is None:\n            T = k.shape[-2]\n            indices = torch.arange(start_index, T + start_index, device=k.device)\n        return self._adapt_keys_for_indices(k, indices)\n\n    def _adapt_keys_for_indices(self, k, indices):\n        return k\n\n    def adapt_queries(self, q, start_index):\n        return q\n\n    def adapt_attention_before_softmax(self, att, start_query_index=None, start_key_index=None, q_indices=None, k_indices=None):\n        if q_indices is None:\n            qT = att.shape[-2]\n            q_indices = torch.arange(start_query_index, qT + start_query_index, device=att.device)\n        if k_indices is None:\n            kT = att.shape[-1]\n            k_indices = torch.arange(start_key_index, kT + start_key_index, device=att.device)\n        return self._adapt_attention_before_softmax_for_indices(att, q_indices, k_indices)\n\n    def _adapt_attention_before_softmax_for_indices(self, att, query_indices, key_indices):\n        return att", "\n\nclass PositionalEncoderClosure(object):\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n\n    def adapt_model_input(self, x, start_index):\n        return x\n\n    def adapt_keys(self, k, start_index=None, indices=None):\n        if indices is None:\n            T = k.shape[-2]\n            indices = torch.arange(start_index, T + start_index, device=k.device)\n        return self._adapt_keys_for_indices(k, indices)\n\n    def _adapt_keys_for_indices(self, k, indices):\n        return k\n\n    def adapt_queries(self, q, start_index):\n        return q\n\n    def adapt_attention_before_softmax(self, att, start_query_index=None, start_key_index=None, q_indices=None, k_indices=None):\n        if q_indices is None:\n            qT = att.shape[-2]\n            q_indices = torch.arange(start_query_index, qT + start_query_index, device=att.device)\n        if k_indices is None:\n            kT = att.shape[-1]\n            k_indices = torch.arange(start_key_index, kT + start_key_index, device=att.device)\n        return self._adapt_attention_before_softmax_for_indices(att, q_indices, k_indices)\n\n    def _adapt_attention_before_softmax_for_indices(self, att, query_indices, key_indices):\n        return att", "    \n\nclass PositionalEncoder(nn.Module):\n\n    closure_model = PositionalEncoderClosure\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n    def forward(self, x):\n        return x, self.closure_model(self)", ""]}
{"filename": "lm_benchmark/models/caches/cache.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom torch import nn\n\nclass LMCacheStorage(nn.Module):\n\n    def __init__(self, config, layer):\n        super().__init__()\n        self.config = config\n        #self._layer = [layer]\n\n    #@property\n    #def layer(self):\n    #    return self._layer[0]\n\n    def store_in_cache(self, keys, values_dict):\n        pass\n\n    def retrieve_for_query(self, q, cache_context, pos_emb_closure, start_index):\n        return None, {}\n\n    def clear_state(self):\n        pass", "class LMCacheStorage(nn.Module):\n\n    def __init__(self, config, layer):\n        super().__init__()\n        self.config = config\n        #self._layer = [layer]\n\n    #@property\n    #def layer(self):\n    #    return self._layer[0]\n\n    def store_in_cache(self, keys, values_dict):\n        pass\n\n    def retrieve_for_query(self, q, cache_context, pos_emb_closure, start_index):\n        return None, {}\n\n    def clear_state(self):\n        pass", "\n\nclass LMCacheContext(object):\n    pass\n\n\nclass LMCache(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer_storages_map = dict()\n        self.layer_storages = nn.ModuleList()\n        self.cache_storage = self.get_cache_storage()\n        self.context_class = self.get_context_class()\n\n    def get_cache_storage(self):\n        return LMCacheStorage\n\n    def get_context_class(self):\n        return LMCacheContext\n\n    def forward(self, x):\n        return x, 0, self.get_context_class()\n\n    def get_final_logits(self, logits):\n        return logits\n\n    def get_storage_for_layer(self, l):\n        if l not in self.layer_storages_map:\n            self.layer_storages_map[l] = len(self.layer_storages)\n            self.layer_storages.append(self.cache_storage(self.config, l))\n        return self.layer_storages[self.layer_storages_map[l]]\n\n    def clear_state(self):\n        for storage in self.layer_storages:\n            storage.clear_state()", ""]}
{"filename": "lm_benchmark/models/caches/mem_cache.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\n", "import torch\n\nfrom .cache import LMCache, LMCacheStorage\n\nclass MemLMCacheStorage(LMCacheStorage):\n\n    def __init__(self, config, layer):\n        super().__init__(config, layer)\n        n_embd_per_head = config.n_embd // config.n_head\n        self.register_buffer(\"cache_mem_k\", torch.empty((config.batch_size, config.mem_cache_size, config.mem_cache_freq + 1, config.n_head, n_embd_per_head)), persistent=False)\n        self.register_buffer(\"cache_mem_v\", torch.empty((config.batch_size, config.mem_cache_size, config.mem_cache_freq + 1, config.n_head, n_embd_per_head)), persistent=False)\n        self.register_buffer(\"last_incomplete_k\", torch.empty((config.batch_size, config.n_head, config.mem_cache_freq + 1, n_embd_per_head)), persistent=False)\n        self.register_buffer(\"last_incomplete_v\", torch.empty((config.batch_size, config.n_head, config.mem_cache_freq + 1, n_embd_per_head)), persistent=False)\n        self.register_buffer(\"last_incomplete_ismem\", torch.empty((config.batch_size, config.mem_cache_freq + 1), dtype=torch.bool), persistent=False)\n        self.last_incomplete_len = 0\n        self.cache_iter = 0\n        self.cache_size = 0\n        self.clear_state()\n        \n    def clear_state(self):\n        self.last_incomplete_len = 0\n        self.cache_iter = 0\n        self.cache_size = 0\n\n    def retrieve_for_query(self, q, cache_context, pos_emb_closure, start_index):\n        B, nh, T, hs = q.size() \n        last_incomplete_k = pos_emb_closure.adapt_keys(self.last_incomplete_k[:B, :, :self.last_incomplete_len], start_index=start_index - self.last_incomplete_len)\n        att_incomplete = (q @ last_incomplete_k.transpose(-2, -1)) * (1.0 / math.sqrt(last_incomplete_k.size(-1)))\n        last_incomplete_v = self.last_incomplete_v[:B, :, :self.last_incomplete_len].unsqueeze(2).expand(B, nh, T, self.last_incomplete_len, hs)\n        last_incomplete_mem = self.last_incomplete_ismem[:B, :self.last_incomplete_len]\n\n        if self.cache_size == 0 or self.config.cache_topk == 0:\n            return att_incomplete, {'v': last_incomplete_v.clone(), 'is_mem': last_incomplete_mem.clone()}\n        \n        top_k = self.config.cache_topk\n        k_with_cached_mem = self.cache_mem_k[:B, :self.cache_size, -1].view(B, -1, nh, hs).transpose(1, 2) # (B, nh, T, hs)\n        mem_indices = torch.cat((\n            torch.arange(k_with_cached_mem.shape[2] - self.cache_iter, k_with_cached_mem.shape[2], device=q.device),\n            torch.arange(0, k_with_cached_mem.shape[2] - self.cache_iter, device=q.device),\n        )).unsqueeze(0).expand(B, -1) \n\n        mem_indices = torch.where(mem_indices >= (k_with_cached_mem.shape[2] - top_k), mem_indices - (k_with_cached_mem.shape[2] - top_k) + 1, 0)\n        mem_token_indices = mem_indices * self.cache_mem_k.shape[2] + self.cache_mem_k.shape[2] - 1\n        k_with_cached_mem = pos_emb_closure.adapt_keys(k_with_cached_mem, indices=mem_token_indices.unsqueeze(1).expand(B, nh, -1))\n        mem_att = (q @ k_with_cached_mem.transpose(-2, -1)) * (1.0 / math.sqrt(k_with_cached_mem.size(-1)))\n        mem_att = torch.nn.functional.softmax(mem_att, dim=-1) # (B, nh, T, mem_count)\n        if self.config.cache_selection_method == \"max_over_heads\":\n            mem_att = mem_att.amax(dim=1).unsqueeze(1).expand(B, nh, T, -1)\n        elif self.config.cache_selection_method == \"per_token_and_head\":\n            pass\n        elif self.config.cache_selection_method == \"max_over_tokens\":\n            mem_att = mem_att.amax(dim=2, keepdim=True).expand(B, nh, T, -1)\n        else:\n            raise NotImplementedError\n\n        mem_att = mem_att.topk(dim=-1,k=top_k)[1]\n        if top_k > 1:\n            # sort\n            mem_att = torch.where(\n                mem_att < self.cache_iter, \n                (k_with_cached_mem.shape[2] - self.cache_iter) + mem_att, \n                mem_att - self.cache_iter)\n            mem_att = mem_att.sort(dim=-1)[0] # (B, nh, T, top_k)\n            mem_att = torch.where(\n                mem_att >= (k_with_cached_mem.shape[2] - self.cache_iter), \n                mem_att - (k_with_cached_mem.shape[2] - self.cache_iter), \n                mem_att + self.cache_iter) # (B, nh, T, top_k)\n        \n        mem_att_or = mem_att\n        mem_att = mem_att.permute(0, 2, 3, 1).view(B, T, top_k, 1, nh, 1).expand(B, T, top_k, self.cache_mem_k.shape[2], nh, hs)\n        mem_k = self.cache_mem_k[:B].unsqueeze(1).expand(B, T, -1, -1, -1, -1).take_along_dim(mem_att, dim=2).view(B, T, -1, nh, hs)\n        mem_k = mem_k.permute((0, 3, 1, 2, 4))\n        mem_v = self.cache_mem_v[:B].unsqueeze(1).expand(B, T, -1, -1, -1, -1).take_along_dim(mem_att, dim=2).view(B, T, -1, nh, hs) # (B, T, top_k * mem_block, nh, hs)\n        mem_v = mem_v.permute((0, 3, 1, 2, 4)) # (B, nh, T, mem_block, hs)\n\n        k_indices = torch.arange(0, self.cache_mem_k.shape[2] * top_k, device=q.device)\n        chosen_indices = mem_indices.view(B, 1, 1, -1).expand(B, nh, T, -1).take_along_dim(mem_att_or, dim=-1)\n        k_indices = (((chosen_indices > 0) * self.cache_mem_k.shape[2]).unsqueeze(-1) + k_indices.view(1, 1, 1, top_k, -1)).view(B, nh, T, -1) # (B, nh, T, top_k * mem_block)\n       \n        is_mem = torch.cat((q.new_zeros((B, top_k, self.cache_mem_k.shape[2] - 1), dtype=torch.bool), q.new_ones((B, top_k, 1), dtype=torch.bool)), dim=-1).view(B, -1)\n       \n        mem_k = pos_emb_closure.adapt_keys(mem_k.reshape(B, nh, -1, hs), indices=k_indices.reshape(B, nh, -1)).view(B, nh, T, -1, hs)\n        att_k = (q.unsqueeze(3) @ mem_k.transpose(-2, -1)).squeeze(3) * (1.0 / math.sqrt(mem_k.size(-1)))\n        att_k = pos_emb_closure.adapt_attention_before_softmax(att_k, start_query_index=start_index, k_indices=k_indices)\n\n        \n        att_prefix = torch.cat((att_k, att_incomplete), dim=-1)\n\n        v_prefix = torch.cat((mem_v, last_incomplete_v), dim=-2)\n\n        is_mem_prefix = torch.cat((is_mem, last_incomplete_mem), dim=-1)\n\n        return att_prefix, {'v': v_prefix, 'is_mem': is_mem_prefix}\n\n    def store_in_cache(self, keys, values_dict):\n\n        B, nh, T, hs = keys.size()\n        k = torch.cat((self.last_incomplete_k[:B, :, :self.last_incomplete_len], keys), dim=-2)\n        v = torch.cat((self.last_incomplete_v[:B, :, :self.last_incomplete_len], values_dict['v']), dim=-2)\n        is_mem = torch.cat((self.last_incomplete_ismem[:B, :self.last_incomplete_len], values_dict['is_mem']), dim=-1)\n        B, nh, T, hs = k.size()\n\n        incomplete_len = T % self.cache_mem_k.shape[2]\n        full_len = T - incomplete_len\n        k, incomplete_k = torch.split(k, (full_len, incomplete_len), dim=-2)\n        v, incomplete_v = torch.split(v, (full_len, incomplete_len), dim=-2)\n        is_mem, incomplete_ismem = torch.split(is_mem, (full_len, incomplete_len), dim=-1)\n        self.last_incomplete_k[:B, :, :incomplete_len].copy_(incomplete_k)\n        self.last_incomplete_v[:B, :, :incomplete_len].copy_(incomplete_v)\n        self.last_incomplete_ismem[:B, :incomplete_len].copy_(incomplete_ismem)\n        self.last_incomplete_len = incomplete_len\n        T = full_len\n        assert T % self.cache_mem_k.shape[2] == 0\n        is_mem_for_cache = is_mem.view(B, -1, self.cache_mem_k.shape[2])\n\n        assert is_mem_for_cache[..., -1].all()\n        assert not is_mem_for_cache[..., :-1].any()\n        added_size = is_mem_for_cache.shape[1]\n        k_for_cache = k.transpose(1, 2).view(B, added_size, self.cache_mem_k.shape[2], nh, hs)\n        v_for_cache = v.transpose(1, 2).view(B, added_size, self.cache_mem_v.shape[2], nh, hs)\n        is_mem_for_cache = is_mem_for_cache[:, -self.cache_mem_k.shape[1]:]\n        k_for_cache = k_for_cache[:, -self.cache_mem_k.shape[1]:]\n        v_for_cache = v_for_cache[:, -self.cache_mem_k.shape[1]:]\n        self.cache_iter = (self.cache_iter + added_size - is_mem_for_cache.shape[1]) % self.cache_mem_k.shape[1]\n        self.cache_size += added_size - is_mem_for_cache.shape[1]\n        added_size = is_mem_for_cache.shape[1]\n        # torch._assert(added_size <= self.cache_mem_k.shape[1], \"Should fit. Sanity check\")\n\n        if self.cache_iter + added_size >= self.cache_mem_k.shape[1]:\n            next_iter = (self.cache_iter + added_size) - self.cache_mem_k.shape[1]\n            rem = (self.cache_mem_k.shape[1] - self.cache_iter)\n            self.cache_mem_k[:B, :next_iter].copy_(k_for_cache[:, rem:])\n            self.cache_mem_k[:B, self.cache_iter:].copy_(k_for_cache[:, :rem])\n            self.cache_mem_v[:B, :next_iter].copy_(v_for_cache[:, rem:])\n            self.cache_mem_v[:B, self.cache_iter:].copy_(v_for_cache[:, :rem])\n        else:\n            next_iter = self.cache_iter + added_size\n            self.cache_mem_k[:B, self.cache_iter:next_iter].copy_(k_for_cache)\n            self.cache_mem_v[:B, self.cache_iter:next_iter].copy_(v_for_cache)\n        \n        self.cache_iter = next_iter\n        self.cache_size += added_size", "\nclass MemLMCacheContext(object):\n    def __init__(self):\n        self.group_prob = None\n\n\nclass MemLMCache(LMCache):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.last_incomplete_len = 0\n        self.total_len = 0\n\n    def get_cache_storage(self):\n        return MemLMCacheStorage\n\n    def get_context_class(self):\n        return MemLMCacheContext\n\n    def forward(self, x):\n        \n        previous_incomplete_len = self.last_incomplete_len\n        #print(\"Concatenating with {}\".format(previous_incomplete_len))\n        #print(\"Being forward\", x.shape)\n        B, T = x.size()\n        \n        incomplete_placeholder = x.new_full((B, previous_incomplete_len), -1)\n        x = torch.cat((incomplete_placeholder, x), dim=-1)\n        B, T = x.size()\n        incomplete_len = T % self.config.mem_cache_freq\n        full_len = T - incomplete_len\n        mem_x, incomplete_x = torch.split(x, (full_len, incomplete_len), dim=-1)\n        mem_x = mem_x.view(B, -1, self.config.mem_cache_freq)\n        mem_x = torch.cat((mem_x, mem_x.new_full((mem_x.shape[0], mem_x.shape[1], 1), self.config.landmark_id)), dim=-1)\n        x = torch.cat((mem_x.view(B, -1), incomplete_x), dim=-1)[:, previous_incomplete_len:]\n        self.last_incomplete_len = incomplete_len\n        #print(\"End forward\", x.shape)\n        #print(x)\n        prev_total_len = self.total_len\n        self.total_len += x.shape[1]\n        start_index = min(prev_total_len // (self.config.mem_cache_freq + 1), (self.config.cache_topk + 1)) * (self.config.mem_cache_freq + 1) + previous_incomplete_len\n        return x, start_index, self.context_class()\n\n    def get_final_logits(self, x):\n        B, T, C = x.size()\n        incomplete_len = self.last_incomplete_len\n        T_with_mem = T - incomplete_len\n        if T_with_mem <= 0: \n            incomplete_len = T\n            T_with_mem = 0\n            x, incomplete = torch.split(x, (0, T), dim=1)\n            previous_incomplete_len = -T_with_mem\n        else:\n            x, incomplete = torch.split(x, (T_with_mem, incomplete_len), dim=1)\n            previous_incomplete_len = (self.config.mem_cache_freq + 1 - T_with_mem % (self.config.mem_cache_freq + 1)) % (self.config.mem_cache_freq + 1)\n        incomplete_placeholder = x.new_full((B, previous_incomplete_len, C), -1)\n        x = torch.cat((incomplete_placeholder, x), dim=1).view(B, -1, self.config.mem_cache_freq + 1, C)\n        x = x[:, :, :-1].reshape(B, -1, C)[:, previous_incomplete_len:]\n        return torch.cat((x, incomplete), dim=1)\n\n    def clear_state(self):\n        super().clear_state()\n        self.last_incomplete_len = 0\n        self.total_len = 0", "        \n"]}
{"filename": "lm_benchmark/models/caches/kv_cache.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\n", "import torch\n\nfrom .cache import LMCache, LMCacheStorage\n\nclass KVLMCacheStorage(LMCacheStorage):\n\n    def __init__(self, config, layer):\n        super().__init__(config, layer)\n        n_embd_per_head = config.n_embd // config.n_head\n        self.max_cache_size = config.mem_cache_size\n        self.register_buffer(\"cache_k\", torch.empty((config.batch_size, config.n_head, config.mem_cache_size, n_embd_per_head)), persistent=False)\n        self.register_buffer(\"cache_v\", torch.empty((config.batch_size, config.n_head, config.mem_cache_size, n_embd_per_head)), persistent=False)\n        self.cache_iter = 0\n        self.cache_size = 0\n        self.clear_state()\n\n    def clear_state(self):\n        self.cache_iter = 0\n        self.cache_size = 0\n\n    def retrieve_for_query(self, q, cache_context, pos_emb_closure, start_index):\n        if self.cache_size == 0:\n            return None, {}\n        B, nh, T, hs = q.size() # batch size, num_heads, sequence length, per-head embedding dimensionality (n_embd)\n        cached_keys = self.cache_k[:B, :, :self.cache_size]\n        k_indices = torch.cat((\n            torch.arange(self.cache_size - self.cache_iter, self.cache_size, device=q.device),\n            torch.arange(self.cache_size - cached_keys.shape[2], self.cache_size - self.cache_iter, device=q.device),\n        ))\n        assert self.cache_size == start_index\n        last_incomplete_k = pos_emb_closure.adapt_keys(cached_keys, indices=k_indices)\n        att_incomplete = (q @ last_incomplete_k.transpose(-2, -1)) * (1.0 / math.sqrt(last_incomplete_k.size(-1)))\n        last_incomplete_v = self.cache_v[:B, :, :self.cache_size].unsqueeze(2).expand(B, nh, T, -1, hs)\n        return att_incomplete, {'v': last_incomplete_v.clone()}\n       \n\n    def store_in_cache(self, keys, values_dict):\n        if self.max_cache_size == 0:\n            return\n        B, nh, T, hs = keys.size()\n        k_for_cache = keys[:, :, -self.max_cache_size:]\n        v_for_cache = values_dict['v'][:, :, -self.max_cache_size:]\n        self.cache_iter = (self.cache_iter + T - k_for_cache.shape[2]) % self.cache_k.shape[2]\n        self.cache_size += T - k_for_cache.shape[2]\n        T = k_for_cache.shape[2]\n        \n        if self.cache_iter + T >= self.max_cache_size:\n            next_iter = (self.cache_iter + T) - self.max_cache_size\n            rem = (self.max_cache_size - self.cache_iter)\n            self.cache_k[:B, :, :next_iter].copy_(k_for_cache[:, :, rem:])\n            self.cache_k[:B, :, self.cache_iter:].copy_(k_for_cache[:,:, :rem])\n            self.cache_v[:B, :, :next_iter].copy_(v_for_cache[:,:, rem:])\n            self.cache_v[:B, :, self.cache_iter:].copy_(v_for_cache[:,:, :rem])\n        else:\n            next_iter = self.cache_iter + T\n            self.cache_k[:B, :, self.cache_iter:next_iter].copy_(k_for_cache)\n            self.cache_v[:B, :, self.cache_iter:next_iter].copy_(v_for_cache)\n        self.cache_iter = next_iter\n        self.cache_size += T", "\n\nclass KVLMCache(LMCache):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.total_len = 0\n\n    def get_cache_storage(self):\n        return KVLMCacheStorage\n\n    def forward(self, x):\n        B, T = x.size()\n        prev_total_len = self.total_len\n        self.total_len = self.total_len + x.shape[1]\n        return x, prev_total_len, self.context_class()\n\n    def clear_state(self):\n        super().clear_state()\n        self.total_len = 0", "        \n"]}
{"filename": "lm_benchmark/models/caches/__init__.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom . import cache, mem_cache, kv_cache, kv_cache_train\n\nCACHES = {\n    \"none\": cache.LMCache,", "CACHES = {\n    \"none\": cache.LMCache,\n    \"mem\": mem_cache.MemLMCache,\n    \"kv\": kv_cache.KVLMCache,\n    \"kv_train\": kv_cache_train.KVLMCache\n}\n\n\ndef get_cache(cache_name):\n    return CACHES[cache_name]", "def get_cache(cache_name):\n    return CACHES[cache_name]\n\n\ndef registered_caches():\n    return CACHES.keys()\n"]}
{"filename": "lm_benchmark/models/caches/kv_cache_train.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\n", "import torch\n\nfrom .cache import LMCache, LMCacheStorage\n\nclass KVLMCacheStorage(LMCacheStorage):\n\n    def __init__(self, config, layer):\n        super().__init__(config, layer)\n        n_embd_per_head = config.n_embd // config.n_head\n        self.max_cache_size = config.mem_cache_size\n        self._cache_k = [torch.empty((config.batch_size, config.n_head, config.mem_cache_size, n_embd_per_head), device=torch.device('cuda'))]\n        self._cache_v = [torch.empty((config.batch_size, config.n_head, config.mem_cache_size, n_embd_per_head), device=torch.device('cuda'))]\n        self.cache_iter = 0\n        self.cache_size = 0\n        self.clear_state()\n\n    @property\n    def cache_k(self):\n        return self._cache_k[0]\n\n    @property\n    def cache_v(self):\n        return self._cache_v[0]\n\n    def clear_state(self):\n        self.cache_iter = 0\n        self.cache_size = 0\n\n    def retrieve_for_query(self, q, cache_context, pos_emb_closure, start_index):\n        if self.cache_size == 0:\n            return None, {}\n        B, nh, T, hs = q.size() # batch size, num_heads, sequence length, per-head embedding dimensionality (n_embd)\n        cached_keys = self.cache_k[:B, :, :self.cache_size]\n        k_indices = torch.cat((\n            torch.arange(self.cache_size - self.cache_iter, self.cache_size, device=q.device),\n            torch.arange(self.cache_size - cached_keys.shape[2], self.cache_size - self.cache_iter, device=q.device),\n        ))\n        assert self.cache_size == start_index\n        last_incomplete_k = pos_emb_closure.adapt_keys(cached_keys, indices=k_indices)\n        att_incomplete = (q @ last_incomplete_k.transpose(-2, -1)) * (1.0 / math.sqrt(last_incomplete_k.size(-1)))\n        last_incomplete_v = self.cache_v[:B, :, :self.cache_size].unsqueeze(2).expand(B, nh, T, -1, hs)\n        return att_incomplete, {'v': last_incomplete_v.clone()}\n       \n\n    def store_in_cache(self, keys, values_dict):\n        if self.max_cache_size == 0:\n            return\n        B, nh, T, hs = keys.size()\n        k_for_cache = keys[:, :, -self.max_cache_size:]\n        v_for_cache = values_dict['v'][:, :, -self.max_cache_size:]\n        self.cache_iter = (self.cache_iter + T - k_for_cache.shape[2]) % self.cache_k.shape[2]\n        self.cache_size += T - k_for_cache.shape[2]\n        T = k_for_cache.shape[2]\n        \n        if self.cache_iter + T >= self.max_cache_size:\n            next_iter = (self.cache_iter + T) - self.max_cache_size\n            rem = (self.max_cache_size - self.cache_iter)\n            self.cache_k[:B, :, :next_iter].copy_(k_for_cache[:, :, rem:])\n            self.cache_k[:B, :, self.cache_iter:].copy_(k_for_cache[:,:, :rem])\n            self.cache_v[:B, :, :next_iter].copy_(v_for_cache[:,:, rem:])\n            self.cache_v[:B, :, self.cache_iter:].copy_(v_for_cache[:,:, :rem])\n        else:\n            next_iter = self.cache_iter + T\n            self.cache_k[:B, :, self.cache_iter:next_iter].copy_(k_for_cache)\n            self.cache_v[:B, :, self.cache_iter:next_iter].copy_(v_for_cache)\n        self.cache_iter = next_iter\n        self.cache_size += T", "\n\nclass KVLMCache(LMCache):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.total_len = 0\n\n    def get_cache_storage(self):\n        return KVLMCacheStorage\n\n    def forward(self, x):\n        B, T = x.size()\n        prev_total_len = self.total_len\n        self.total_len = self.total_len + x.shape[1]\n        return x, prev_total_len, self.context_class()\n\n    def clear_state(self):\n        super().clear_state()\n        self.total_len = 0", "        \n"]}
{"filename": "llama_legacy/train.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nfrom dataclasses import dataclass, field\nfrom functools import partial", "from dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import Dict, Optional, Sequence\n\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, DataCollatorForLanguageModeling, get_cosine_schedule_with_warmup\nfrom llama_mem import LlamaForCausalLM\n", "from llama_mem import LlamaForCausalLM\n\nfrom torch.distributed import barrier\nimport os\n\n\nfrom datasets import load_dataset\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"", "IGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")", "class ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n    )", "\n    \nclass TrainerCosine(Trainer):\n    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n        passed as an argument.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.args.lr_scheduler_type != \"cosine\":\n            return super().create_scheduler(num_training_steps, optimizer)\n        if self.lr_scheduler is None:\n            self.lr_scheduler = get_cosine_schedule_with_warmup(\n                optimizer=self.optimizer if optimizer is None else optimizer,\n                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps,\n                num_cycles=0.4 # ~10% of the init lr\n            )\n        return self.lr_scheduler", "\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\ndef tokenize_fn(tokenizer, example):\n    context_length = tokenizer.model_max_length\n    outputs = tokenizer(\n        tokenizer.eos_token.join(example[\"text\"]),\n        truncation=False,\n        return_tensors=\"pt\",\n        pad_to_multiple_of=context_length,\n        padding=True,\n    )\n    return {\"input_ids\": outputs[\"input_ids\"].view(-1, context_length)}", "\ndef add_mem_tokens(example, mem_freq, mem_id):\n    x = example[\"input_ids\"]\n    ret = []\n    prev_idx = 0\n    for t_idx in range(mem_freq, len(x), mem_freq):\n        ret.extend(x[prev_idx:t_idx])\n        ret.append(mem_id)\n        prev_idx = t_idx\n    ret.extend(x[prev_idx:])\n    # drop attention_mask\n    return {\"input_ids\": ret}", "\ndef train():\n    parser = transformers.HfArgumentParser((ModelArguments, TrainingArguments))\n    model_args, training_args = parser.parse_args_into_dataclasses()\n\n    model = LlamaForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n    )\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    special_tokens_dict = dict()\n    if tokenizer.pad_token is None:\n        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n    if tokenizer.eos_token is None:\n        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n    if tokenizer.bos_token is None:\n        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n    if tokenizer.unk_token is None:\n        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n    mem_token = \"<landmark>\"\n    special_tokens_dict[\"additional_special_tokens\"] = [mem_token]\n\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=special_tokens_dict,\n        tokenizer=tokenizer,\n        model=model,\n    )\n\n    mem_id = tokenizer.convert_tokens_to_ids(mem_token)\n    model.set_mem_id(mem_id)\n    rank = int(os.environ.get('RANK', -1))\n    if rank > 0:\n        barrier()\n    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir)\n\n    dataset = dataset.map(partial(tokenize_fn,tokenizer),batched=True, num_proc=32, remove_columns=[\"text\", \"meta\"])\n\n    dataset = dataset.map(\n        partial(\n            add_mem_tokens, \n            mem_freq=50, \n            mem_id=mem_id\n        ), batched=False, num_proc=32)\n    if rank == 0:\n        barrier()\n    print(dataset)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    trainer = TrainerCosine(\n        model=model, tokenizer=tokenizer, args=training_args, \n        train_dataset=dataset[\"train\"],\n        eval_dataset=None,\n        data_collator=data_collator)\n    trainer.train()\n    trainer.save_state()\n    trainer.save_model(output_dir=training_args.output_dir)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "llama_legacy/redpajama.py", "chunked_list": ["# Copyright 2023 Together Computer\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"RedPajama: An Open-Source, Clean-Room 1.2 Trillion Token Dataset.\"\"\"\n\n", "\n\nimport json\n\nimport datasets\nimport traceback\nimport numpy as np\nimport math\n\nlogger = datasets.logging.get_logger(__name__)", "\nlogger = datasets.logging.get_logger(__name__)\n\n\n_DESCRIPTION = \"\"\"\\\nRedPajama is a clean-room, fully open-source implementation of the LLaMa dataset.\n\"\"\"\n\n_URL_LISTS = {\n    \"arxiv\": \"urls/arxiv.txt\",", "_URL_LISTS = {\n    \"arxiv\": \"urls/arxiv.txt\",\n    \"book\": \"urls/book.txt\",\n    \"c4\": \"urls/c4.txt\",\n    \"common_crawl\": \"urls/common_crawl.txt\",\n    \"github\": \"urls/github.txt\",\n    \"stackexchange\": \"urls/stackexchange.txt\",\n    \"wikipedia\": \"urls/wikipedia.txt\",\n}\n", "}\n\n\nclass RedPajama1TConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for RedPajama sample.\"\"\"\n\n    def __init__(self, *args, subsets, p_sample=None, **kwargs):\n        \"\"\"BuilderConfig for RedPajama.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        super(RedPajama1TConfig, self).__init__(**kwargs)\n\n        self.subsets = subsets\n        self.p_sample = p_sample", "\n\nclass RedPajama1T(datasets.GeneratorBasedBuilder):\n    \"\"\"RedPajama: Reproducing the LLaMA training dataset of over 1.2 trillion tokens. Version 1.0.0.\"\"\"\n    BUILDER_CONFIG_CLASS = RedPajama1TConfig\n    BUILDER_CONFIGS = [\n        RedPajama1TConfig(\n            subsets = list(_URL_LISTS.keys()),\n            name=\"plain_text\",\n            version=datasets.Version(\"1.0.0\", \"\"),\n            description=\"Plain text\",\n        ),\n        RedPajama1TConfig(\n            subsets = list(_URL_LISTS.keys()),\n            name=\"plain_text_tenpercent\",\n            version=datasets.Version(\"1.0.0\", \"\"),\n            description=\"Plain text\",\n            p_sample=0.1\n        ),\n    ]\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {\n                    \"text\": datasets.Value(\"string\"),\n                    \"meta\": datasets.Value(\"string\"),\n                    \"red_pajama_subset\": datasets.Value(\"string\"),\n                }\n            ),\n            supervised_keys=None,\n        )\n\n    def _split_generators(self, dl_manager):\n        url_lists = dl_manager.download_and_extract({\n            subset: _URL_LISTS[subset] for subset in self.config.subsets\n        })\n\n        urls = {}\n        rng = np.random.default_rng(seed=2)\n\n        for subset, url_list in url_lists.items():\n            with open(url_list, encoding=\"utf-8\") as f:\n                urls[subset] = [line.strip() for line in f]\n            if self.config.p_sample is not None:\n                urls[subset] = rng.choice(\n                    urls[subset], \n                    size=int(math.ceil(len(urls[subset]) * self.config.p_sample)), replace=False).tolist()\n\n        downloaded_files = dl_manager.download(urls)\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs = {\n                    \"files\": {\n                        subset: downloaded_files[subset]\n                        for subset in self.config.subsets\n                    }\n                }\n            )\n        ]\n\n    def _generate_examples(self, files):\n        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n        key = 0\n        for subset in files:\n            if subset == \"common_crawl\":\n                import zstandard as zstd\n\n                for path in files[subset]:\n                    with zstd.open(open(path, \"rb\"), \"rt\", encoding=\"utf-8\") as f:\n                        for i, row in enumerate(f):\n                            try:\n                                data = json.loads(row)\n                                text = data[\"text\"]\n                                del data[\"text\"]\n                                yield key, {\n                                    \"text\": text,\n                                    \"meta\": json.dumps(data),\n                                    \"red_pajama_subset\": subset,\n                                }\n                                key += 1\n                            except Exception as e:\n                                print(f'Subset: {subset}')\n                                print(f'Path: {path}')\n                                print(f'Row: {row}')\n                                traceback.print_exc()\n\n                                raise e\n            else:\n                for path in files[subset]:\n                    with open(path, encoding=\"utf-8\") as f:\n                        for i, row in enumerate(f):\n                            try:\n                                data = json.loads(row)\n                                if \"meta\" not in data:\n                                    text = data[\"text\"]\n                                    del data[\"text\"]\n                                    yield key, {\n                                        \"text\": text,\n                                        \"meta\": json.dumps(data),\n                                        \"red_pajama_subset\": subset,\n                                    }\n                                else:\n                                    yield key, {\n                                        \"text\": data[\"text\"],\n                                        \"meta\": data[\"meta\"],\n                                        \"red_pajama_subset\": subset,\n                                    }\n                                key += 1\n                            except Exception as e:\n                                print(f'Subset: {subset}')\n                                print(f'Path: {path}')\n                                print(f'Row: {row}')\n                                traceback.print_exc()\n\n                                raise e", ""]}
{"filename": "llama_legacy/run_test.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nllama_weights_7b_base = \"/llama_weights/7B_hf/\"\nllama_weights_7b_tuned = \"/llama-redpajama-mem-15000-with-mem/\"\ncache_path = \"/hf-cache/\"\n\ndef make_llama_base_pipe():\n\n    from transformers import pipeline\n\n    from transformers.models.llama import LlamaForCausalLM\n\n    llama_base = LlamaForCausalLM.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n    )\n\n    llama_base = llama_base.to('cuda:0')\n\n    import transformers\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n        model_max_length=1024,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    llama_base_pipe = pipeline(\"text-generation\", model=llama_base, tokenizer=tokenizer, device=llama_base.device)\n    return llama_base_pipe", "cache_path = \"/hf-cache/\"\n\ndef make_llama_base_pipe():\n\n    from transformers import pipeline\n\n    from transformers.models.llama import LlamaForCausalLM\n\n    llama_base = LlamaForCausalLM.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n    )\n\n    llama_base = llama_base.to('cuda:0')\n\n    import transformers\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n        model_max_length=1024,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    llama_base_pipe = pipeline(\"text-generation\", model=llama_base, tokenizer=tokenizer, device=llama_base.device)\n    return llama_base_pipe", "\n\n\nllama_base_pipe = make_llama_base_pipe()\n\ndef make_llama_mem_pipe():\n    from llama_mem import LlamaForCausalLM\n\n    model = LlamaForCausalLM.from_pretrained(\n        llama_weights_7b_tuned,\n        cache_dir=cache_path,\n    )\n\n    model.to('cuda:1')\n\n    import transformers\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n            llama_weights_7b_tuned,\n            cache_dir=cache_path,\n            model_max_length=512,\n            padding_side=\"right\",\n            use_fast=False,\n        )\n    from transformers import pipeline\n    llama_mem_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=model.device)\n    return llama_mem_pipe", "\n\nllama_mem_pipe = make_llama_mem_pipe()\n\nmem_id = llama_mem_pipe.tokenizer.convert_tokens_to_ids(\"<landmark>\")\nllama_mem_pipe.model.set_mem_id(mem_id)\nllama_mem_pipe.model.set_mem_cache_args(max_seq_len=255, mem_freq=50, top_k=5, max_cache_size=None)\n\n\npipes = {\"base\": llama_base_pipe, \"mem\": llama_mem_pipe}", "\npipes = {\"base\": llama_base_pipe, \"mem\": llama_mem_pipe}\n\nimport torch\n\nimport os\nimport random\nimport re\nimport requests\n\ndef generate_prompt(n_garbage):\n    \"\"\"Generates a text file and inserts an execute line at a random position.\"\"\"\n    n_garbage_prefix = random.randint(0, n_garbage)\n    n_garbage_suffix = n_garbage - n_garbage_prefix\n    \n    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n    garbage_inf = \" \".join([garbage] * 2000)\n    assert len(garbage_inf) >= n_garbage\n    garbage_prefix = garbage_inf[:n_garbage_prefix]\n    garbage_suffix = garbage_inf[:n_garbage_suffix]\n    pass_key = random.randint(1, 50000)\n    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n    final_question = \"What is the pass key? The pass key is\"\n    lines = [\n        task_description,\n        garbage_prefix,\n        information_line,\n        garbage_suffix,\n        final_question\n    ]\n    return \"\\n\".join(lines), pass_key", "import requests\n\ndef generate_prompt(n_garbage):\n    \"\"\"Generates a text file and inserts an execute line at a random position.\"\"\"\n    n_garbage_prefix = random.randint(0, n_garbage)\n    n_garbage_suffix = n_garbage - n_garbage_prefix\n    \n    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n    garbage_inf = \" \".join([garbage] * 2000)\n    assert len(garbage_inf) >= n_garbage\n    garbage_prefix = garbage_inf[:n_garbage_prefix]\n    garbage_suffix = garbage_inf[:n_garbage_suffix]\n    pass_key = random.randint(1, 50000)\n    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n    final_question = \"What is the pass key? The pass key is\"\n    lines = [\n        task_description,\n        garbage_prefix,\n        information_line,\n        garbage_suffix,\n        final_question\n    ]\n    return \"\\n\".join(lines), pass_key", "            \n\n\ndef test_model(prompt_text, pass_key, model_name):\n    response = pipes[model_name](prompt_text,num_return_sequences=1, max_new_tokens=10)[0][\"generated_text\"][len(prompt_text):]\n    assert f\"The pass key is {pass_key}\" in prompt_text\n\n    try:\n        pass_key = int(re.search(r'\\d+', response).group())\n    except:\n        pass_key = response[:20]\n    \n    return pass_key", "\n\nn_values = [0, 100, 500, 1000, 5000, 8000, 10000, 12000, 14000, 18000, 20000, 25000, 38000]\nnum_tests = 50\nmodels = [\"base\", \"mem\"]\naccuracies = {x: [] for x in models}\nindividual_results = {x: [] for x in models}\n\nfor n in n_values:\n    \n    correct_count = {x: 0 for x in models}\n    \n    n_results = {x: [] for x in models}\n    for i in range(num_tests):\n        print(f\"\\nRunning test {i + 1}/{num_tests} for n = {n}...\")\n        prompt_text, pass_key = generate_prompt(n)\n        \n        \n        \n        for model_name in models:\n            num_tokens = len(pipes[model_name].tokenizer.encode(prompt_text))\n\n            print(\"Number of tokens in this prompt: \", num_tokens)\n            model_output = test_model(prompt_text, pass_key, model_name)\n            print(f\"Expected number in the prompt: {pass_key}, {model_name} output: {model_output}\")\n\n            if pass_key == model_output:\n                correct_count[model_name] += 1\n                n_results[model_name].append(1)\n                print(\"Success!\")\n            else:\n                n_results[model_name].append(0)\n                print(\"Fail.\")\n    \n    for model in models:\n        accuracy = (correct_count[model] / num_tests) * 100\n        print(f\"Accuracy {model} for n = {n}: {accuracy}%\")\n        accuracies[model].append(accuracy)\n        individual_results[model].append(n_results)", "for n in n_values:\n    \n    correct_count = {x: 0 for x in models}\n    \n    n_results = {x: [] for x in models}\n    for i in range(num_tests):\n        print(f\"\\nRunning test {i + 1}/{num_tests} for n = {n}...\")\n        prompt_text, pass_key = generate_prompt(n)\n        \n        \n        \n        for model_name in models:\n            num_tokens = len(pipes[model_name].tokenizer.encode(prompt_text))\n\n            print(\"Number of tokens in this prompt: \", num_tokens)\n            model_output = test_model(prompt_text, pass_key, model_name)\n            print(f\"Expected number in the prompt: {pass_key}, {model_name} output: {model_output}\")\n\n            if pass_key == model_output:\n                correct_count[model_name] += 1\n                n_results[model_name].append(1)\n                print(\"Success!\")\n            else:\n                n_results[model_name].append(0)\n                print(\"Fail.\")\n    \n    for model in models:\n        accuracy = (correct_count[model] / num_tests) * 100\n        print(f\"Accuracy {model} for n = {n}: {accuracy}%\")\n        accuracies[model].append(accuracy)\n        individual_results[model].append(n_results)", ""]}
{"filename": "llama_legacy/weight_diff.py", "chunked_list": ["#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,", "#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n# This file has been changed by Amirkeivan Mohtashami\n# to take into account the new token in the embedding layer\n\nfrom typing import Optional", "\nfrom typing import Optional\n\nimport fire\nimport torch\nimport tqdm\nimport transformers\nfrom train import smart_tokenizer_and_embedding_resize\n\n", "\n\n@torch.inference_mode()\ndef make_diff(\n    path_raw: str, path_tuned: str, path_diff: str, device=\"cpu\",  # \"cuda\" or \"cpu\"\n):\n    \"\"\"Make the weight diff.\n\n    This function is given to present full transparency of how the weight diff was created.\n\n    Run:\n        python weight_diff.py make_diff --path_raw <your_path_raw> --path_tuned <your_path_tuned> --path_diff <your_path_diff>\n    \"\"\"\n    model_tuned: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_tuned,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_tuned: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_tuned\n    )\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=\"[PAD]\", additional_special_tokens=[\"<landmark>\"]),\n        model=model_raw,\n        tokenizer=tokenizer_raw,\n    )\n\n    state_dict_tuned = model_tuned.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        state_dict_tuned[key].add_(-state_dict_raw[key])\n\n    model_tuned.save_pretrained(path_diff)\n    tokenizer_tuned.save_pretrained(path_diff)", "\n\n@torch.inference_mode()\ndef recover(\n    path_raw,\n    path_diff,\n    path_tuned: Optional[str] = None,\n    device=\"cpu\",\n    test_inference=True,\n    check_integrity_naively=True,\n):\n    \"\"\"Recover the original weights from the released weight diff.\n\n    This function is given for you to run.\n\n    Things to do before running this:\n        1. Convert Meta's released weights into huggingface format. Follow this guide:\n            https://huggingface.co/docs/transformers/main/model_doc/llama\n        2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n            https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n        3. Run this function with the correct paths. E.g.,\n            python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir>\n\n    Additional notes:\n        - If things run too slowly, and you have an 80G GPU lying around, let GPU go brrr by setting `--device \"cuda\"`.\n        - If you want to save the recovered weights, set `--path_tuned <your_path_tuned>`.\n            Next time you can load the recovered weights directly from `<your_path_tuned>`.\n    \"\"\"\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_recovered: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_diff,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=\"[PAD]\", additional_special_tokens=[\"<landmark>\"]),\n        model=model_raw,\n        tokenizer=tokenizer_raw,\n    )\n    tokenizer_recovered: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_diff\n    )\n\n    state_dict_recovered = model_recovered.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_recovered):\n        state_dict_recovered[key].add_(state_dict_raw[key])\n\n    if check_integrity_naively:\n        # This is not a rigorous, cryptographically strong integrity check :)\n        allsum = sum(state_dict_recovered[key].sum() for key in state_dict_recovered)\n        assert torch.allclose(\n            allsum, torch.full_like(allsum, fill_value=49798.7656), atol=1e-2, rtol=0\n        ), \"Naive integrity check failed. This could imply that some of the checkpoint files are corrupted.\"\n\n    if path_tuned is not None:\n        model_recovered.save_pretrained(path_tuned)\n        tokenizer_recovered.save_pretrained(path_tuned)\n\n    return model_recovered, tokenizer_recovered", "\n\ndef main(task, **kwargs):\n    globals()[task](**kwargs)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n", ""]}
{"filename": "llama_legacy/llama_mem.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn", "import torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n", "from transformers.models.llama.configuration_llama import LlamaConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)", "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)", "\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)", "\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states", "\n\nclass LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )", "\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    if q is None:\n        q_embed = None\n    else:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    if q is None:\n        q_embed = None\n    else:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "\n\nclass LlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))", "\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False)\n            \n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        #x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        x_exp = torch.exp((x - maxes).to(torch.float32))\n\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(dim, resp_mem_idx, x_exp, )\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        #probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        probs = x_exp / denom\n        \n        \n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs)\n\n        return probs\n\n    @staticmethod\n    def backward(ctx, grad_probs):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n        \n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx", "\ndef landmark_grouped_softmax(x, dim, is_mem, last_section_mask):\n    \n    last_and_rest_mask = last_section_mask # | mask\n\n    full_access_mask =  is_mem | last_and_rest_mask\n\n    max_mem_cnt = 16\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    mem_bucket_id = max_mem_cnt - 1\n    resp_mem_idx = torch.where(last_and_rest_mask, \n                                max_mem_cnt - 1,\n                                torch.where(is_mem, mem_bucket_id, mem_group_idx))\n    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape, ))\n    group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs)\n    probs = probs.mul(torch.where(full_access_mask, last_section_mask, torch.gather(group_prob, dim, resp_mem_idx)))\n\n\n    return probs", "\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n        self.mem_freq = None\n        self.top_k = None\n        self.max_cache_size = None\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        self.mem_freq = mem_freq\n        self.top_k = top_k\n        self.max_cache_size = max_cache_size\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n            if len(past_key_value) > 2:\n                kv_seq_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        key_states_before_pos = key_states\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # [bsz, nh, t, hd]\n\n        attn_prefix = None\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            if self.mem_freq is None:\n                cache_len = past_key_value[0].shape[2]\n                if self.max_cache_size is not None:\n                    cache_len = min(cache_len, self.max_cache_size)\n                if is_mem is not None:\n                    is_mem = torch.cat((is_mem.new_zeros((1, 1, q_len, cache_len)), is_mem), dim=-1)\n                    last_section_mask = torch.cat((last_section_mask.new_ones((1, 1, q_len, cache_len)), last_section_mask), dim=-1)\n\n                past_key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                past_value_states = torch.cat([past_key_value[1], value_states], dim=2)\n                key_states = past_key_states[:, :, -(q_len + cache_len):]\n                value_states = past_value_states[:, :, -(q_len + cache_len):]\n                expected_att_size = (bsz, self.num_heads, q_len, cache_len + q_len)\n            else:\n                orig_value_states = value_states\n\n                incomplete_len = past_key_value[0].shape[2] % (self.mem_freq + 1)\n                full_len = past_key_value[0].shape[2] - incomplete_len\n                past_key_mem, past_key_incomplete = torch.split(past_key_value[0], (full_len, incomplete_len), dim=2)\n                past_value_mem, past_value_incomplete = torch.split(past_key_value[1], (full_len, incomplete_len), dim=2)\n\n                if offload_cache_to_cpu:\n                    past_key_value = (past_key_incomplete, past_value_incomplete, *past_key_value[2:])\n                \n                if incomplete_len > 0:\n                    assert q_len + incomplete_len <= (self.mem_freq + 1)\n                is_mem = torch.cat((is_mem.new_zeros((1, 1, q_len, incomplete_len)), is_mem), dim=-1)\n                last_section_mask = torch.cat((last_section_mask.new_ones((1, 1, q_len, incomplete_len)), last_section_mask), dim=-1)\n\n                if len(past_key_value) > 2:\n                    full_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n                past_key_incomplete_pos = torch.arange(full_len, full_len + incomplete_len, dtype=torch.long, device=position_ids.device).unsqueeze(0)\n                _, past_key_incomplete = apply_rotary_pos_emb(None, past_key_incomplete, cos, sin, past_key_incomplete_pos)\n                key_states = torch.cat((past_key_incomplete, key_states), dim=2)\n                value_states = torch.cat((past_value_incomplete, value_states), dim=2)\n\n                past_key_mem = past_key_mem.view(bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim)\n                past_value_mem = past_value_mem.view(bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim)\n\n                if len(past_key_value) > 2:\n                    mem_key_nopos = torch.cat((\n                        past_key_value[2], \n                        past_key_mem.select(dim=3, index=self.mem_freq)), dim=2)\n                    past_key_mem_offload = past_key_value[3]\n                    past_key_mem = torch.cat((\n                        past_key_mem_offload, \n                        past_key_mem.to(past_key_mem_offload.device)), dim=2)\n                    past_value_mem = torch.cat((past_key_value[4], past_value_mem.to(past_key_mem_offload.device)), dim=2)\n                else:\n                    mem_key_nopos = past_key_mem.select(dim=3, index=self.mem_freq)    \n                \n                num_mems = past_key_mem.shape[2]\n                top_k = min(self.top_k, num_mems)\n                prefix_len = full_len - (top_k + 1) * (self.mem_freq + 1)\n                mem_indices = torch.cat(\n                    (position_ids.new_zeros((max(0, num_mems - top_k), )),\n                    torch.arange(1, top_k + 1, device=query_states.device, dtype=position_ids.dtype)), dim=0)\n                mem_pos = (mem_indices * (self.mem_freq + 1) + self.mem_freq).unsqueeze(0).expand(bsz, -1) + prefix_len\n                _, mem_key = apply_rotary_pos_emb(None, mem_key_nopos, cos, sin, mem_pos)\n                mem_attn_weights = torch.matmul(query_states, mem_key.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n                if offload_cache_to_cpu:\n                    aggregate = \"max_over_tokens\"\n                else:\n                    aggregate = None\n                if aggregate == \"max_over_tokens\":\n                    token_retrievers = 1\n                    head_retrievers = self.num_heads\n                    mem_attn_weights = torch.nn.functional.softmax(mem_attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n                    mem_attn_weights = mem_attn_weights.amax(dim=2, keepdim=True)\n                elif aggregate is None:\n                    token_retrievers = q_len\n                    head_retrievers = self.num_heads\n                else:\n                    raise NotImplementedError()\n\n                mem_selected_idx = mem_attn_weights.topk(dim=-1,k=top_k)[1].sort(dim=-1)[0].view(bsz, head_retrievers, token_retrievers, top_k)\n                \n                selected_indices = torch.arange(0, top_k * (self.mem_freq + 1), device=query_states.device, dtype=position_ids.dtype)\n                selected_indices = torch.where(mem_selected_idx >= num_mems - top_k, self.mem_freq + 1, 0).unsqueeze(-1) + selected_indices.view(1, 1, 1, top_k, self.mem_freq + 1)\n                selected_indices = selected_indices.view(bsz, head_retrievers, token_retrievers, -1).expand(bsz, self.num_heads, q_len, -1) + prefix_len\n\n                \n                \n                \n                mem_selected_idx = mem_selected_idx.to(past_key_mem.device)\n                \n                mem_selected_idx = mem_selected_idx.view(bsz, self.num_heads, token_retrievers, top_k, 1, 1).expand(bsz, self.num_heads, token_retrievers, top_k, self.mem_freq + 1, self.head_dim) \n                selected_keys = past_key_mem.unsqueeze(2).expand(bsz, self.num_heads, token_retrievers, -1, self.mem_freq + 1, self.head_dim)\n                selected_keys = selected_keys.take_along_dim(mem_selected_idx, dim=3).to(query_states.device)\n                selected_values = past_value_mem.unsqueeze(2).expand(bsz, self.num_heads, token_retrievers, -1, self.mem_freq + 1, self.head_dim).take_along_dim(mem_selected_idx, dim=3).to(query_states.device)\n            \n                selected_keys = selected_keys.view(bsz, self.num_heads, token_retrievers, -1, self.head_dim).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                selected_keys = apply_rotary_pos_emb(None, selected_keys.unsqueeze(1), cos, sin, selected_indices)[1].squeeze(1)    \n                selected_values = selected_values.view(bsz, self.num_heads, token_retrievers, -1, self.head_dim).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                attn_prefix = torch.matmul(query_states.unsqueeze(3), selected_keys.transpose(3, 4)).squeeze(3) / math.sqrt(self.head_dim)\n                is_mem_prefix = torch.cat((is_mem.new_zeros((self.mem_freq, )), is_mem.new_ones((1, )))).unsqueeze(0).repeat((top_k, 1))\n                is_mem_prefix = is_mem_prefix.view(1, 1, 1, -1).expand(1, 1, q_len, -1)\n                is_mem = torch.cat((is_mem_prefix, is_mem), dim=-1)\n                last_section_mask = torch.cat((last_section_mask.new_zeros((1, 1, q_len, top_k * (self.mem_freq + 1))), last_section_mask), dim=-1)\n                expected_att_size = (bsz, self.num_heads, q_len, q_len + incomplete_len)\n\n                past_key_states = torch.cat([past_key_value[0], key_states_before_pos], dim=2)\n                past_value_states = torch.cat([past_key_value[1], orig_value_states], dim=2)\n\n                if offload_cache_to_cpu:\n                    past_key_value = (past_key_states, past_value_states, mem_key_nopos, past_key_mem.to(\"cpu\"), past_value_mem.to(\"cpu\"), *past_key_value[5:]) if use_cache else None\n                else:\n                    past_key_value = (past_key_states, past_value_states) if use_cache else None\n\n        else:\n            if self.mem_freq is None:\n                past_key_states = key_states\n            else:\n                past_key_states = key_states_before_pos\n            past_value_states = value_states\n            expected_att_size = (bsz, self.num_heads, q_len, kv_seq_len)\n            past_key_value = (past_key_states, past_value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n        if attn_weights.size() != expected_att_size:\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask[...,-attn_weights.shape[-1]:]\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n        if attn_prefix is not None:\n            attn_weights = torch.cat((attn_prefix, attn_weights), dim=-1)\n        # upcast attention to fp32\n        if is_mem is None:\n            raise ValueError(\"Don't use this without landmarks\")\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        else:\n            attn_weights = landmark_grouped_softmax(attn_weights, dim=-1, is_mem=is_mem.expand(-1, self.num_heads, -1, -1), last_section_mask=last_section_mask).to(query_states.dtype)\n        if attn_prefix is not None:\n            attn_prefix, attn_weights = torch.split(attn_weights, (attn_prefix.shape[-1], attn_weights.shape[-1] - attn_prefix.shape[-1]), dim=-1)\n        attn_output = torch.matmul(attn_weights, value_states)\n        if attn_prefix is not None:\n            attn_output += torch.matmul(attn_prefix.unsqueeze(3), selected_values).squeeze(3)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value", "\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        self.self_attn.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            is_mem=is_mem,\n            last_section_mask=last_section_mask,\n            offload_cache_to_cpu=offload_cache_to_cpu\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs", "\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.", "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n", "\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value", "\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.", "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n", "            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n", "            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,", "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n", "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This", "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.", "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n", "\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.mem_id = None\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def set_mem_id(self, mem_id):\n        self.mem_id = mem_id\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        for l in self.layers:\n            l.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        is_mem = None\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n            if self.mem_id is not None:\n                with torch.no_grad():\n                    is_mem = input_ids == self.mem_id\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n            if self.mem_id is not None:\n                raise NotImplementedError\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            if is_mem is not None:\n                pass\n                #raise NotImplementedError\n            past_key_values_length = past_key_values[0][0].shape[2]\n            if len(past_key_values[0]) > 2:\n                past_key_values_length += past_key_values[0][3].shape[2] * past_key_values[0][3].shape[3]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n        \n        last_section_mask = None\n        if is_mem is not None:\n            is_mem = is_mem.unsqueeze(1).unsqueeze(2)\n            current_len = input_ids.shape[1]\n            mem_ids = torch.where(attention_mask[..., -current_len:] < -1, 0, torch.cumsum(is_mem, -1) - is_mem.int())\n            last_section_mask = torch.amax(mem_ids, -1, keepdim=True) == mem_ids\n            attention_mask[..., -current_len:].masked_fill_(last_section_mask & is_mem, torch.tensor(torch.finfo(inputs_embeds.dtype).min, device=inputs_embeds.device))\n            last_section_mask.logical_and_(attention_mask[..., -current_len:] > -1)\n            is_mem = is_mem.logical_and(attention_mask[..., -current_len:] > -1)\n            \n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                    is_mem,\n                    last_section_mask\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    is_mem=is_mem,\n                    last_section_mask=last_section_mask,\n                    offload_cache_to_cpu=offload_cache_to_cpu\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )", "\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.mem_id = None\n        self.mem_freq = None\n        self.top_k = None\n        self.max_seq_len = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        window_len = self.max_seq_len or input_ids.shape[1]\n        last_logits = None\n        for step, idx in enumerate(range(0, input_ids.shape[1], window_len)):\n            if idx >= 1:\n                if output_attentions or output_hidden_states:\n                    raise NotImplementedError\n                if not use_cache:\n                    raise NotImplementedError\n            outputs = self.model(\n                input_ids=input_ids[:, idx:idx + window_len],\n                attention_mask=attention_mask[:, :idx + window_len + attention_mask.shape[1] - input_ids.shape[1]] if attention_mask is not None else None,\n                position_ids=position_ids[:, idx:idx + window_len] if position_ids is not None else None,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds[:, idx:idx + window_len] if inputs_embeds is not None else None,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                offload_cache_to_cpu=offload_cache_to_cpu,\n            )\n            past_key_values = outputs[1]\n            if last_logits is not None:\n                last_logits = torch.cat((last_logits, outputs[0]), dim=-2)\n            last_logits = outputs[0]\n\n        hidden_states = last_logits\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_mem_id(self, mem_id):\n        self.mem_id = mem_id\n        self.model.set_mem_id(mem_id)\n    \n    def set_mem_cache_args(self, max_seq_len, mem_freq, top_k, max_cache_size):\n        self.mem_freq = mem_freq\n        self.top_k = top_k\n        self.max_seq_len = max_seq_len\n        if self.max_seq_len is not None:\n            assert self.max_seq_len % (self.mem_freq + 1) == 0\n        self.model.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        total_len = input_ids.shape[1]\n        if past_key_values:\n            prev_len = input_ids.shape[1] - 1\n        else:\n            prev_len = 0\n\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        if self.mem_freq is not None:\n            if position_ids is not None:\n                raise NotImplementedError\n            T = input_ids.shape[1]\n\n            prev_incomplete_len = prev_len % self.mem_freq\n            prev_complete_len = prev_len - prev_incomplete_len\n            incomplete_len = total_len % self.mem_freq\n            new_full_len = total_len - prev_complete_len - incomplete_len\n\n            prev_input, input_ids_with_mem, input_ids_without_mem = torch.split(input_ids, (prev_complete_len, new_full_len, incomplete_len), dim=-1)\n            \n            bsz, q_len = input_ids.size()\n            input_ids_with_mem = input_ids_with_mem.view(bsz, -1, self.mem_freq)            \n            input_ids_with_mem = torch.cat(\n                (\n                    input_ids_with_mem, \n                    input_ids_with_mem.new_full((bsz, input_ids_with_mem.shape[1], 1), self.mem_id)\n                ), \n                dim=-1\n            ).view(bsz, -1)\n            input_ids = torch.cat((prev_input, input_ids_with_mem, input_ids_without_mem), dim=-1)\n            if attention_mask is not None:\n                attention_mask_with_mem, attention_mask_without_mem = torch.split(attention_mask, (prev_complete_len + new_full_len, incomplete_len), dim=-1)\n                attention_mask_with_mem = attention_mask_with_mem.view(bsz, -1, self.mem_freq)\n                attention_mask_with_mem = torch.cat(\n                    (\n                        attention_mask_with_mem, \n                        attention_mask_with_mem.new_ones((bsz, attention_mask_with_mem.shape[1], 1))\n                    ), \n                    dim=-1\n                ).view(bsz, -1)\n                attention_mask = torch.cat((attention_mask_with_mem, attention_mask_without_mem), dim=-1)\n            \n            \n        input_ids = input_ids[:, prev_len:]\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            position_ids = position_ids[:, -input_ids.shape[1]:].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None and self.mem_freq is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"offload_cache_to_cpu\": kwargs.get(\"offload_cache_to_cpu\")\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past", "\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a", "\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )", ")\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )", ""]}
{"filename": "llama/train.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nfrom dataclasses import dataclass, field\nfrom functools import partial", "from dataclasses import dataclass, field\nfrom functools import partial\nfrom typing import Dict, Optional, Sequence\n\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, DataCollatorForLanguageModeling, get_cosine_schedule_with_warmup\nfrom llama_mem import LlamaForCausalLM\n", "from llama_mem import LlamaForCausalLM\n\nfrom torch.distributed import barrier\nimport os\n\n\nfrom datasets import load_dataset\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"", "IGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")", "class ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n    )\n    use_flash: bool = field(default=False)\n    mem_freq: int = field(default=63)", "\n    \nclass TrainerCosine(Trainer):\n    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n        passed as an argument.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.args.lr_scheduler_type != \"cosine\":\n            return super().create_scheduler(num_training_steps, optimizer)\n        if self.lr_scheduler is None:\n            self.lr_scheduler = get_cosine_schedule_with_warmup(\n                optimizer=self.optimizer if optimizer is None else optimizer,\n                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps,\n                num_cycles=0.4 # ~10% of the init lr\n            )\n        return self.lr_scheduler", "\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\ndef tokenize_fn(tokenizer, example):\n    context_length = tokenizer.model_max_length\n    outputs = tokenizer(\n        tokenizer.eos_token.join(example[\"text\"]),\n        truncation=False,\n        return_tensors=\"pt\",\n        pad_to_multiple_of=context_length,\n        padding=True,\n    )\n    return {\"input_ids\": outputs[\"input_ids\"].view(-1, context_length)}", "\ndef add_mem_tokens(example, mem_freq, mem_id):\n    x = example[\"input_ids\"]\n    ret = []\n    prev_idx = 0\n    for t_idx in range(mem_freq, len(x), mem_freq):\n        ret.extend(x[prev_idx:t_idx])\n        ret.append(mem_id)\n        prev_idx = t_idx\n    ret.extend(x[prev_idx:])\n    # drop attention_mask\n    return {\"input_ids\": ret}", "\ndef train():\n    parser = transformers.HfArgumentParser((ModelArguments, TrainingArguments))\n    model_args, training_args = parser.parse_args_into_dataclasses()\n\n    model = LlamaForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        mem_freq=training_args.mem_freq,\n        include_landmark_in_loss=not training_args.use_flash\n    )\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    special_tokens_dict = dict()\n    if tokenizer.pad_token is None:\n        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n    if tokenizer.eos_token is None:\n        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n    if tokenizer.bos_token is None:\n        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n    if tokenizer.unk_token is None:\n        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n    mem_token = \"<landmark>\"\n    special_tokens_dict[\"additional_special_tokens\"] = [mem_token]\n\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=special_tokens_dict,\n        tokenizer=tokenizer,\n        model=model,\n    )\n\n    mem_id = tokenizer.convert_tokens_to_ids(mem_token)\n    model.set_mem_id(mem_id)\n    \n    rank = int(os.environ.get('RANK', -1))\n    if rank > 0:\n        barrier()\n    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir)\n\n    dataset = dataset.map(partial(tokenize_fn,tokenizer),batched=True, num_proc=32, remove_columns=[\"text\", \"meta\"])\n\n    if training_args.use_flash:\n        model.enable_landmark_insertion()\n        model.enable_flash()\n    else:\n        dataset = dataset.map(\n            partial(\n                add_mem_tokens, \n                mem_freq=training_args.mem_freq, \n                mem_id=mem_id\n            ), batched=False, num_proc=32)\n\n    if rank == 0:\n        barrier()\n    print(dataset)\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    trainer = TrainerCosine(\n        model=model, tokenizer=tokenizer, args=training_args, \n        train_dataset=dataset[\"train\"],\n        eval_dataset=None,\n        data_collator=data_collator)\n    trainer.train()\n    trainer.save_state()\n    trainer.save_model(output_dir=training_args.output_dir)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "llama/redpajama.py", "chunked_list": ["# Copyright 2023 Together Computer\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"RedPajama: An Open-Source, Clean-Room 1.2 Trillion Token Dataset.\"\"\"\n\n", "\n\nimport json\n\nimport datasets\nimport traceback\nimport numpy as np\nimport math\n\nlogger = datasets.logging.get_logger(__name__)", "\nlogger = datasets.logging.get_logger(__name__)\n\n\n_DESCRIPTION = \"\"\"\\\nRedPajama is a clean-room, fully open-source implementation of the LLaMa dataset.\n\"\"\"\n\n_URL_LISTS = {\n    \"arxiv\": \"urls/arxiv.txt\",", "_URL_LISTS = {\n    \"arxiv\": \"urls/arxiv.txt\",\n    \"book\": \"urls/book.txt\",\n    \"c4\": \"urls/c4.txt\",\n    \"common_crawl\": \"urls/common_crawl.txt\",\n    \"github\": \"urls/github.txt\",\n    \"stackexchange\": \"urls/stackexchange.txt\",\n    \"wikipedia\": \"urls/wikipedia.txt\",\n}\n", "}\n\n\nclass RedPajama1TConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for RedPajama sample.\"\"\"\n\n    def __init__(self, *args, subsets, p_sample=None, **kwargs):\n        \"\"\"BuilderConfig for RedPajama.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        super(RedPajama1TConfig, self).__init__(**kwargs)\n\n        self.subsets = subsets\n        self.p_sample = p_sample", "\n\nclass RedPajama1T(datasets.GeneratorBasedBuilder):\n    \"\"\"RedPajama: Reproducing the LLaMA training dataset of over 1.2 trillion tokens. Version 1.0.0.\"\"\"\n    BUILDER_CONFIG_CLASS = RedPajama1TConfig\n    BUILDER_CONFIGS = [\n        RedPajama1TConfig(\n            subsets = list(_URL_LISTS.keys()),\n            name=\"plain_text\",\n            version=datasets.Version(\"1.0.0\", \"\"),\n            description=\"Plain text\",\n        ),\n        RedPajama1TConfig(\n            subsets = list(_URL_LISTS.keys()),\n            name=\"plain_text_tenpercent\",\n            version=datasets.Version(\"1.0.0\", \"\"),\n            description=\"Plain text\",\n            p_sample=0.1\n        ),\n    ]\n\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {\n                    \"text\": datasets.Value(\"string\"),\n                    \"meta\": datasets.Value(\"string\"),\n                    \"red_pajama_subset\": datasets.Value(\"string\"),\n                }\n            ),\n            supervised_keys=None,\n        )\n\n    def _split_generators(self, dl_manager):\n        url_lists = dl_manager.download_and_extract({\n            subset: _URL_LISTS[subset] for subset in self.config.subsets\n        })\n\n        urls = {}\n        rng = np.random.default_rng(seed=2)\n\n        for subset, url_list in url_lists.items():\n            with open(url_list, encoding=\"utf-8\") as f:\n                urls[subset] = [line.strip() for line in f]\n            if self.config.p_sample is not None:\n                urls[subset] = rng.choice(\n                    urls[subset], \n                    size=int(math.ceil(len(urls[subset]) * self.config.p_sample)), replace=False).tolist()\n\n        downloaded_files = dl_manager.download(urls)\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs = {\n                    \"files\": {\n                        subset: downloaded_files[subset]\n                        for subset in self.config.subsets\n                    }\n                }\n            )\n        ]\n\n    def _generate_examples(self, files):\n        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n        key = 0\n        for subset in files:\n            if subset == \"common_crawl\":\n                import zstandard as zstd\n\n                for path in files[subset]:\n                    with zstd.open(open(path, \"rb\"), \"rt\", encoding=\"utf-8\") as f:\n                        for i, row in enumerate(f):\n                            try:\n                                data = json.loads(row)\n                                text = data[\"text\"]\n                                del data[\"text\"]\n                                yield key, {\n                                    \"text\": text,\n                                    \"meta\": json.dumps(data),\n                                    \"red_pajama_subset\": subset,\n                                }\n                                key += 1\n                            except Exception as e:\n                                print(f'Subset: {subset}')\n                                print(f'Path: {path}')\n                                print(f'Row: {row}')\n                                traceback.print_exc()\n\n                                raise e\n            else:\n                for path in files[subset]:\n                    with open(path, encoding=\"utf-8\") as f:\n                        for i, row in enumerate(f):\n                            try:\n                                data = json.loads(row)\n                                if \"meta\" not in data:\n                                    text = data[\"text\"]\n                                    del data[\"text\"]\n                                    yield key, {\n                                        \"text\": text,\n                                        \"meta\": json.dumps(data),\n                                        \"red_pajama_subset\": subset,\n                                    }\n                                else:\n                                    yield key, {\n                                        \"text\": data[\"text\"],\n                                        \"meta\": data[\"meta\"],\n                                        \"red_pajama_subset\": subset,\n                                    }\n                                key += 1\n                            except Exception as e:\n                                print(f'Subset: {subset}')\n                                print(f'Path: {path}')\n                                print(f'Row: {row}')\n                                traceback.print_exc()\n\n                                raise e", ""]}
{"filename": "llama/llama_landmark_config.py", "chunked_list": ["from transformers.models.llama.configuration_llama import LlamaConfig\n\nclass LlamaLandmarkConfig(LlamaConfig):\n    model_type = \"llama_with_landmark\"\n\n    def __init__(\n        self,\n        mem_id=32001,\n        mem_freq=50,\n        train_context_length=512,\n        include_landmark_in_loss=True,\n        **kwargs,\n    ):\n        self.mem_id = mem_id\n        self.mem_freq = mem_freq\n        self.train_context_length = train_context_length\n        self.include_landmark_in_loss = include_landmark_in_loss\n        super().__init__(**kwargs)", ""]}
{"filename": "llama/run_test.py", "chunked_list": ["# Copyright 2023 Amirkeivan Mohtashami, Martin Jaggi\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\nimport os\nimport random", "import os\nimport random\nimport re\nimport requests\n\n\nllama_weights_7b_base = \"/llama_weights/7B_hf/\"\nllama_weights_7b_tuned = \"/llama-redpajama-mem-15000-with-mem/\"\ncache_path = \"/hf-cache/\"\nuse_flash = False # using flash for inference is only implemented for when offloading kv to cpu", "cache_path = \"/hf-cache/\"\nuse_flash = False # using flash for inference is only implemented for when offloading kv to cpu\ntop_k = 5\ndtype = torch.bfloat16\n\ndef make_llama_base_pipe():\n\n    from transformers import pipeline\n\n    from transformers.models.llama import LlamaForCausalLM\n\n    llama_base = LlamaForCausalLM.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n    )\n\n    llama_base = llama_base.to('cuda:0')\n\n    import transformers\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        llama_weights_7b_base,\n        cache_dir=cache_path,\n        model_max_length=2048,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    llama_base_pipe = pipeline(\"text-generation\", model=llama_base, tokenizer=tokenizer, device=llama_base.device)\n    return llama_base_pipe", "\n\n\nllama_base_pipe = make_llama_base_pipe()\n\ndef make_llama_mem_pipe():\n    from llama_mem import LlamaForCausalLM\n\n    model = LlamaForCausalLM.from_pretrained(\n        llama_weights_7b_tuned,\n        cache_dir=cache_path,\n        torch_dtype=dtype\n    )\n\n    model.to('cuda:1')\n\n    import transformers\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n            llama_weights_7b_tuned,\n            cache_dir=cache_path,\n            model_max_length=model.config.train_context_length,\n            padding_side=\"right\",\n            use_fast=False,\n        )\n    mem_id = tokenizer.convert_tokens_to_ids(\"<landmark>\")\n    model.set_mem_id(mem_id)\n    from transformers import pipeline\n    llama_mem_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=model.device,\n                              offload_cache_to_cpu=use_flash, use_flash=use_flash, \n                              cache_top_k=top_k)\n    return llama_mem_pipe", "\n\nllama_mem_pipe = make_llama_mem_pipe()\n\n\n\npipes = {\"base\": llama_base_pipe, \"mem\": llama_mem_pipe}\n\n\ndef generate_prompt(n_garbage):\n    \"\"\"Generates a text file and inserts an execute line at a random position.\"\"\"\n    n_garbage_prefix = random.randint(0, n_garbage)\n    n_garbage_suffix = n_garbage - n_garbage_prefix\n    \n    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n    garbage_inf = \" \".join([garbage] * 2000)\n    assert len(garbage_inf) >= n_garbage\n    garbage_prefix = garbage_inf[:n_garbage_prefix]\n    garbage_suffix = garbage_inf[:n_garbage_suffix]\n    pass_key = random.randint(1, 50000)\n    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n    final_question = \"What is the pass key? The pass key is\"\n    lines = [\n        task_description,\n        garbage_prefix,\n        information_line,\n        garbage_suffix,\n        final_question\n    ]\n    return \"\\n\".join(lines), pass_key", "\ndef generate_prompt(n_garbage):\n    \"\"\"Generates a text file and inserts an execute line at a random position.\"\"\"\n    n_garbage_prefix = random.randint(0, n_garbage)\n    n_garbage_suffix = n_garbage - n_garbage_prefix\n    \n    task_description = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\"\n    garbage = \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\"\n    garbage_inf = \" \".join([garbage] * 2000)\n    assert len(garbage_inf) >= n_garbage\n    garbage_prefix = garbage_inf[:n_garbage_prefix]\n    garbage_suffix = garbage_inf[:n_garbage_suffix]\n    pass_key = random.randint(1, 50000)\n    information_line = f\"The pass key is {pass_key}. Remember it. {pass_key} is the pass key.\"\n    final_question = \"What is the pass key? The pass key is\"\n    lines = [\n        task_description,\n        garbage_prefix,\n        information_line,\n        garbage_suffix,\n        final_question\n    ]\n    return \"\\n\".join(lines), pass_key", "            \n\n\ndef test_model(prompt_text, pass_key, model_name):\n    response = pipes[model_name](prompt_text,num_return_sequences=1, max_new_tokens=10)[0][\"generated_text\"][len(prompt_text):]\n    assert f\"The pass key is {pass_key}\" in prompt_text\n\n    try:\n        pass_key = int(re.search(r'\\d+', response).group())\n    except:\n        pass_key = response[:20]\n    \n    return pass_key", "\n\nn_values = [0, 100, 500, 1000, 5000, 8000, 10000, 12000, 14000, 18000, 20000, 25000, 38000]\nnum_tests = 50\nmodels = [\"base\", \"mem\"]\naccuracies = {x: [] for x in models}\nindividual_results = {x: [] for x in models}\n\nfor n in n_values:\n    \n    correct_count = {x: 0 for x in models}\n    \n    n_results = {x: [] for x in models}\n    for i in range(num_tests):\n        print(f\"\\nRunning test {i + 1}/{num_tests} for n = {n}...\")\n        prompt_text, pass_key = generate_prompt(n)\n        \n        \n        \n        for model_name in models:\n            if pipes[model_name] is None:\n                continue\n            num_tokens = len(pipes[model_name].tokenizer.encode(prompt_text))\n\n            print(\"Number of tokens in this prompt: \", num_tokens)\n            model_output = test_model(prompt_text, pass_key, model_name)\n            print(f\"Expected number in the prompt: {pass_key}, {model_name} output: {model_output}\")\n\n            if pass_key == model_output:\n                correct_count[model_name] += 1\n                n_results[model_name].append(1)\n                print(\"Success!\")\n            else:\n                n_results[model_name].append(0)\n                print(\"Fail.\")\n    \n    for model in models:\n        accuracy = (correct_count[model] / num_tests) * 100\n        print(f\"Accuracy {model} for n = {n}: {accuracy}%\")\n        accuracies[model].append(accuracy)\n        individual_results[model].append(n_results)", "for n in n_values:\n    \n    correct_count = {x: 0 for x in models}\n    \n    n_results = {x: [] for x in models}\n    for i in range(num_tests):\n        print(f\"\\nRunning test {i + 1}/{num_tests} for n = {n}...\")\n        prompt_text, pass_key = generate_prompt(n)\n        \n        \n        \n        for model_name in models:\n            if pipes[model_name] is None:\n                continue\n            num_tokens = len(pipes[model_name].tokenizer.encode(prompt_text))\n\n            print(\"Number of tokens in this prompt: \", num_tokens)\n            model_output = test_model(prompt_text, pass_key, model_name)\n            print(f\"Expected number in the prompt: {pass_key}, {model_name} output: {model_output}\")\n\n            if pass_key == model_output:\n                correct_count[model_name] += 1\n                n_results[model_name].append(1)\n                print(\"Success!\")\n            else:\n                n_results[model_name].append(0)\n                print(\"Fail.\")\n    \n    for model in models:\n        accuracy = (correct_count[model] / num_tests) * 100\n        print(f\"Accuracy {model} for n = {n}: {accuracy}%\")\n        accuracies[model].append(accuracy)\n        individual_results[model].append(n_results)", ""]}
{"filename": "llama/weight_diff.py", "chunked_list": ["#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,", "#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n# This file has been changed by Amirkeivan Mohtashami\n# to take into account the new token in the embedding layer\n\nimport os", "\nimport os\nfrom typing import Optional\n\nimport fire\nimport torch\nimport tqdm\nimport transformers\nfrom train import smart_tokenizer_and_embedding_resize\nimport llama_mem", "from train import smart_tokenizer_and_embedding_resize\nimport llama_mem\n\n@torch.inference_mode()\ndef make_diff(\n    path_raw: str, path_tuned: str, path_diff: str, device=\"cpu\",  # \"cuda\" or \"cpu\"\n):\n    \"\"\"Make the weight diff.\n\n    This function is given to present full transparency of how the weight diff was created.\n\n    Run:\n        python weight_diff.py make_diff --path_raw <your_path_raw> --path_tuned <your_path_tuned> --path_diff <your_path_diff>\n    \"\"\"\n    model_tuned: transformers.PreTrainedModel = llama_mem.LlamaForCausalLM.from_pretrained(\n        path_tuned,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_tuned: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_tuned\n    )\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=\"[PAD]\", additional_special_tokens=[\"<landmark>\"]),\n        model=model_raw,\n        tokenizer=tokenizer_raw,\n    )\n\n\n\n    state_dict_tuned = model_tuned.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    with open(os.path.join(path_diff, \"checksum_psum.txt\"), \"w\") as f:\n        f.write(str(sum(state_dict_tuned[key].sum().item() for key in state_dict_tuned)))\n\n    for key in tqdm.tqdm(state_dict_tuned):\n        state_dict_tuned[key].add_(-state_dict_raw[key])\n\n    model_tuned.save_pretrained(path_diff)\n    tokenizer_tuned.save_pretrained(path_diff)", "\n\n@torch.inference_mode()\ndef recover(\n    path_raw,\n    path_diff,\n    path_tuned: Optional[str] = None,\n    device=\"cpu\",\n    test_inference=True,\n    check_integrity_naively=True,\n):\n    \"\"\"Recover the original weights from the released weight diff.\n\n    This function is given for you to run.\n\n    Things to do before running this:\n        1. Convert Meta's released weights into huggingface format. Follow this guide:\n            https://huggingface.co/docs/transformers/main/model_doc/llama\n        2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n            https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n        3. Run this function with the correct paths. E.g.,\n            python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir>\n\n    Additional notes:\n        - If things run too slowly, and you have an 80G GPU lying around, let GPU go brrr by setting `--device \"cuda\"`.\n        - If you want to save the recovered weights, set `--path_tuned <your_path_tuned>`.\n            Next time you can load the recovered weights directly from `<your_path_tuned>`.\n    \"\"\"\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_recovered: transformers.PreTrainedModel = llama_mem.LlamaForCausalLM.from_pretrained(\n        path_diff,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    smart_tokenizer_and_embedding_resize(\n        special_tokens_dict=dict(pad_token=\"[PAD]\", additional_special_tokens=[\"<landmark>\"]),\n        model=model_raw,\n        tokenizer=tokenizer_raw,\n    )\n    tokenizer_recovered: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_diff\n    )\n\n    state_dict_recovered = model_recovered.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_recovered):\n        state_dict_recovered[key].add_(state_dict_raw[key])\n\n    if check_integrity_naively:\n        # This is not a rigorous, cryptographically strong integrity check :)\n        allsum = sum(state_dict_recovered[key].sum() for key in state_dict_recovered)\n        if os.path.exists(os.path.join(path_diff, \"checksum_psum.txt\")):\n            with open(os.path.join(path_diff, \"checksum_psum.txt\")) as f:\n                expected_sum = float(f.read())\n        else:\n            expected_sum = 49798.7656 # backward compatibility with the first released weights\n        assert torch.allclose(\n            allsum, torch.full_like(allsum, fill_value=expected_sum), atol=1e-2, rtol=0\n        ), \"Naive integrity check failed. This could imply that some of the checkpoint files are corrupted.\"\n\n    if path_tuned is not None:\n        model_recovered.save_pretrained(path_tuned)\n        tokenizer_recovered.save_pretrained(path_tuned)\n\n    return model_recovered, tokenizer_recovered", "\n\ndef main(task, **kwargs):\n    globals()[task](**kwargs)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n", ""]}
{"filename": "llama/llama_mem.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn", "import torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\nfrom llama_landmark_config import LlamaLandmarkConfig\nfrom ltriton.flash_landmark_attention import fused_landmark_attention", "from llama_landmark_config import LlamaLandmarkConfig\nfrom ltriton.flash_landmark_attention import fused_landmark_attention\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaLandmarkConfig\"\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)", "\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)", "\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)", "\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states", "\n\nclass LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )", "\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    if position_ids.ndim == 2:\n        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    else:\n        cos = cos[position_ids]\n        sin = sin[position_ids]\n    if q is None:\n        q_embed = None\n    else:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    if position_ids.ndim == 2:\n        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    else:\n        cos = cos[position_ids]\n        sin = sin[position_ids]\n    if q is None:\n        q_embed = None\n    else:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "\n\nclass LlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))", "\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False)\n            \n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        #x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        x_exp = torch.exp((x - maxes).to(torch.float32))\n\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(dim, resp_mem_idx, x_exp, )\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        #probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        probs = x_exp / denom\n        \n        \n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs)\n\n        return probs\n\n    @staticmethod\n    def backward(ctx, grad_probs):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n        \n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx", "\ndef landmark_grouped_softmax(x, dim, is_mem, last_section_mask):\n    \n    last_and_rest_mask = last_section_mask # | mask\n\n    full_access_mask =  is_mem | last_and_rest_mask\n\n    max_mem_cnt = 64\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    mem_bucket_id = max_mem_cnt - 1\n    resp_mem_idx = torch.where(last_and_rest_mask, \n                                max_mem_cnt - 1,\n                                torch.where(is_mem, mem_bucket_id, mem_group_idx))\n    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape, ))\n    group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs)\n    probs = probs.mul(torch.where(full_access_mask, last_section_mask, torch.gather(group_prob, dim, resp_mem_idx)))\n\n\n    return probs", "\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaLandmarkConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False,\n        use_flash: bool = False,\n        cache_top_k: Optional[int] = None,\n        mem_freq: Optional[int] = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n            if len(past_key_value) > 2:\n                kv_seq_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        key_states_before_pos = key_states\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # [bsz, nh, t, hd]\n\n        attn_prefix = None\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            if mem_freq is None:\n                cache_len = past_key_value[0].shape[2]\n                if is_mem is not None:\n                    if use_flash:\n                        is_mem = torch.cat((is_mem.new_zeros((1, cache_len)), is_mem), dim=-1)\n                    else:\n                        is_mem = torch.cat((is_mem.new_zeros((1, 1, q_len, cache_len)), is_mem), dim=-1)\n                        last_section_mask = torch.cat((last_section_mask.new_ones((1, 1, q_len, cache_len)), last_section_mask), dim=-1)\n\n                past_key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                past_value_states = torch.cat([past_key_value[1], value_states], dim=2)\n                key_states = past_key_states[:, :, -(q_len + cache_len):]\n                value_states = past_value_states[:, :, -(q_len + cache_len):]\n                expected_att_size = (bsz, self.num_heads, q_len, cache_len + q_len)\n            else:\n                orig_value_states = value_states\n\n                incomplete_len = past_key_value[0].shape[2] % (mem_freq + 1)\n                full_len = past_key_value[0].shape[2] - incomplete_len\n                past_key_mem, past_key_incomplete = torch.split(past_key_value[0], (full_len, incomplete_len), dim=2)\n                past_value_mem, past_value_incomplete = torch.split(past_key_value[1], (full_len, incomplete_len), dim=2)\n\n                if offload_cache_to_cpu:\n                    past_key_value = (past_key_incomplete, past_value_incomplete, *past_key_value[2:])\n                \n                if incomplete_len > 0:\n                    assert q_len + incomplete_len <= (mem_freq + 1)\n                if use_flash:\n                    is_mem = torch.cat((is_mem.new_zeros((1, incomplete_len)), is_mem), dim=-1)\n                else:\n                    is_mem = torch.cat((is_mem.new_zeros((1, 1, q_len, incomplete_len)), is_mem), dim=-1)\n                    last_section_mask = torch.cat((last_section_mask.new_ones((1, 1, q_len, incomplete_len)), last_section_mask), dim=-1)\n\n                if len(past_key_value) > 2:\n                    full_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n                past_key_incomplete_pos = torch.arange(full_len, full_len + incomplete_len, dtype=torch.long, device=position_ids.device).unsqueeze(0)\n                _, past_key_incomplete = apply_rotary_pos_emb(None, past_key_incomplete, cos, sin, past_key_incomplete_pos)\n                key_states = torch.cat((past_key_incomplete, key_states), dim=2)\n                value_states = torch.cat((past_value_incomplete, value_states), dim=2)\n\n                past_key_mem = past_key_mem.view(bsz, self.num_heads, -1, mem_freq + 1, self.head_dim)\n                past_value_mem = past_value_mem.view(bsz, self.num_heads, -1, mem_freq + 1, self.head_dim)\n\n                if len(past_key_value) > 2:\n                    mem_key_nopos = torch.cat((\n                        past_key_value[2], \n                        past_key_mem.select(dim=3, index=mem_freq)), dim=2)\n                    past_key_mem_offload = past_key_value[3]\n                    past_key_mem = torch.cat((\n                        past_key_mem_offload, \n                        past_key_mem.to(past_key_mem_offload.device)), dim=2)\n                    past_value_mem = torch.cat((past_key_value[4], past_value_mem.to(past_key_mem_offload.device)), dim=2)\n                else:\n                    mem_key_nopos = past_key_mem.select(dim=3, index=mem_freq)    \n                \n                num_mems = past_key_mem.shape[2]\n                top_k = min(cache_top_k, num_mems)\n                prefix_len = full_len - (top_k + 1) * (mem_freq + 1)\n                mem_indices = torch.cat(\n                    (position_ids.new_zeros((max(0, num_mems - top_k), )),\n                    torch.arange(1, top_k + 1, device=query_states.device, dtype=position_ids.dtype)), dim=0)\n                mem_pos = (mem_indices * (mem_freq + 1) + mem_freq).unsqueeze(0).expand(bsz, -1) + prefix_len\n                _, mem_key = apply_rotary_pos_emb(None, mem_key_nopos, cos, sin, mem_pos)\n                mem_attn_weights = torch.matmul(query_states, mem_key.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n                if offload_cache_to_cpu:\n                    aggregate = \"max_over_tokens\"\n                else:\n                    aggregate = None\n                if aggregate == \"max_over_tokens\":\n                    token_retrievers = 1\n                    head_retrievers = self.num_heads\n                    mem_attn_weights = torch.nn.functional.softmax(mem_attn_weights, dim=-1,dtype=torch.float32).to(query_states.dtype)\n                    mem_attn_weights = mem_attn_weights.amax(dim=2, keepdim=True)\n                elif aggregate is None:\n                    token_retrievers = q_len\n                    head_retrievers = self.num_heads\n                else:\n                    raise NotImplementedError()\n\n                mem_selected_idx = mem_attn_weights.topk(dim=-1,k=top_k)[1].sort(dim=-1)[0].view(bsz, head_retrievers, token_retrievers, top_k)\n                \n                selected_indices = torch.arange(0, top_k * (mem_freq + 1), device=query_states.device, dtype=position_ids.dtype)\n                selected_indices = torch.where(mem_selected_idx >= num_mems - top_k, mem_freq + 1, 0).unsqueeze(-1) + selected_indices.view(1, 1, 1, top_k, mem_freq + 1)\n                selected_indices = selected_indices.view(bsz, head_retrievers, token_retrievers, -1) + prefix_len\n\n                \n                \n                \n                mem_selected_idx = mem_selected_idx.to(past_key_mem.device)\n                \n                mem_selected_idx = mem_selected_idx.view(bsz, self.num_heads, token_retrievers, top_k, 1, 1).expand(bsz, self.num_heads, token_retrievers, top_k, mem_freq + 1, self.head_dim) \n                selected_keys = past_key_mem.unsqueeze(2).expand(bsz, self.num_heads, token_retrievers, -1, mem_freq + 1, self.head_dim)\n                selected_keys = selected_keys.take_along_dim(mem_selected_idx, dim=3).to(query_states.device)\n                selected_values = past_value_mem.unsqueeze(2).expand(bsz, self.num_heads, token_retrievers, -1, mem_freq + 1, self.head_dim).take_along_dim(mem_selected_idx, dim=3).to(query_states.device)\n\n                if aggregate == \"max_over_tokens\":\n                    selected_indices = selected_indices.squeeze(2)\n                    selected_keys = selected_keys.view(bsz, self.num_heads, -1, self.head_dim)\n                    selected_keys = apply_rotary_pos_emb(None, selected_keys, cos, sin, selected_indices)[1] \n                    key_states = torch.cat((selected_keys, key_states), dim=2)\n                    value_states = torch.cat((selected_values.view(bsz, self.num_heads, -1, self.head_dim), value_states), dim=2)\n                    expected_att_size = (bsz, self.num_heads, q_len, key_states.shape[2])\n                else:\n                    selected_indices = selected_indices.expand(bsz, self.num_heads, q_len, -1)\n                    selected_keys = selected_keys.view(bsz, self.num_heads, token_retrievers, -1, self.head_dim).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                    selected_keys = apply_rotary_pos_emb(None, selected_keys, cos, sin, selected_indices)[1]\n                    selected_values = selected_values.view(bsz, self.num_heads, token_retrievers, -1, self.head_dim).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                    attn_prefix = torch.matmul(query_states.unsqueeze(3), selected_keys.transpose(3, 4)).squeeze(3) / math.sqrt(self.head_dim)\n                    expected_att_size = (bsz, self.num_heads, q_len, q_len + incomplete_len)\n                \n                is_mem_prefix = torch.cat((is_mem.new_zeros((mem_freq, )), is_mem.new_ones((1, )))).unsqueeze(0).repeat((top_k, 1))\n                if use_flash:\n                    is_mem_prefix = is_mem_prefix.view(1, -1)\n                else:\n                    is_mem_prefix = is_mem_prefix.view(1, 1, 1, -1).expand(1, 1, q_len, -1)\n                    last_section_mask = torch.cat((last_section_mask.new_zeros((1, 1, q_len, top_k * (mem_freq + 1))), last_section_mask), dim=-1)\n                is_mem = torch.cat((is_mem_prefix, is_mem), dim=-1)\n\n\n                past_key_states = torch.cat([past_key_value[0], key_states_before_pos], dim=2)\n                past_value_states = torch.cat([past_key_value[1], orig_value_states], dim=2)\n\n                if offload_cache_to_cpu:\n                    past_key_value = (past_key_states, past_value_states, mem_key_nopos, past_key_mem.to(\"cpu\"), past_value_mem.to(\"cpu\"), *past_key_value[5:]) if use_cache else None\n                else:\n                    past_key_value = (past_key_states, past_value_states) if use_cache else None\n\n        else:\n            if mem_freq is None:\n                past_key_states = key_states\n            else:\n                past_key_states = key_states_before_pos\n            past_value_states = value_states\n            expected_att_size = (bsz, self.num_heads, q_len, kv_seq_len)\n            past_key_value = (past_key_states, past_value_states) if use_cache else None\n\n        if use_flash:\n            assert attn_prefix is None\n            assert not output_attentions\n            assert mem_freq is not None\n            attn_output = fused_landmark_attention(query_states, key_states, value_states, is_mem, block_size=mem_freq+1)\n            attn_weights = None\n        else:\n            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n            if attn_weights.size() != expected_att_size:\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask[...,-attn_weights.shape[-1]:]\n                attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n            if attn_prefix is not None:\n                attn_weights = torch.cat((attn_prefix, attn_weights), dim=-1)\n            # upcast attention to fp32\n            if is_mem is None:\n                raise ValueError(\"Don't use this without landmarks\")\n                attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n            else:\n                attn_weights = landmark_grouped_softmax(attn_weights, dim=-1, is_mem=is_mem.expand(-1, self.num_heads, -1, -1), last_section_mask=last_section_mask).to(query_states.dtype)\n            if attn_prefix is not None:\n                attn_prefix, attn_weights = torch.split(attn_weights, (attn_prefix.shape[-1], attn_weights.shape[-1] - attn_prefix.shape[-1]), dim=-1)\n            attn_output = torch.matmul(attn_weights, value_states)\n            if attn_prefix is not None:\n                attn_output += torch.matmul(attn_prefix.unsqueeze(3), selected_values).squeeze(3)\n\n            if not output_attentions:\n                attn_weights = None\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, attn_weights, past_key_value", "\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaLandmarkConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False,\n        use_flash: bool = False,\n        cache_top_k: Optional[int] = None,\n        mem_freq: Optional[int] = None\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            is_mem=is_mem,\n            last_section_mask=last_section_mask,\n            offload_cache_to_cpu=offload_cache_to_cpu,\n            use_flash=use_flash,\n            cache_top_k=cache_top_k,\n            mem_freq=mem_freq\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs", "\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.", "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaLandmarkConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n", "\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaLandmarkConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value", "\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.", "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n", "            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n", "            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,", "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n", "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This", "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.", "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n", "\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaLandmarkConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaLandmarkConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n        use_flash: Optional[bool] = None,\n        cache_top_k: Optional[int] = None,\n        mem_freq: Optional[int] = None\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        is_mem = None\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n            if self.config.mem_id is not None:\n                with torch.no_grad():\n                    is_mem = input_ids == self.config.mem_id\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n            if self.config.mem_id is not None:\n                raise NotImplementedError\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            if is_mem is not None:\n                pass\n                #raise NotImplementedError\n            past_key_values_length = past_key_values[0][0].shape[2]\n            if len(past_key_values[0]) > 2:\n                past_key_values_length += past_key_values[0][3].shape[2] * past_key_values[0][3].shape[3]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n        \n        last_section_mask = None\n        if is_mem is not None and not use_flash:\n            is_mem = is_mem.unsqueeze(1).unsqueeze(2)\n            current_len = input_ids.shape[1]\n            mem_ids = torch.where(attention_mask[..., -current_len:] < -1, 0, torch.cumsum(is_mem, -1) - is_mem.int())\n            last_section_mask = torch.amax(mem_ids, -1, keepdim=True) == mem_ids\n            attention_mask[..., -current_len:].masked_fill_(last_section_mask & is_mem, torch.tensor(torch.finfo(inputs_embeds.dtype).min, device=inputs_embeds.device))\n            last_section_mask.logical_and_(attention_mask[..., -current_len:] > -1)\n            is_mem = is_mem.logical_and(attention_mask[..., -current_len:] > -1)\n            \n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                    is_mem,\n                    last_section_mask,\n                    offload_cache_to_cpu,\n                    use_flash,\n                    cache_top_k,\n                    mem_freq\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    is_mem=is_mem,\n                    last_section_mask=last_section_mask,\n                    offload_cache_to_cpu=offload_cache_to_cpu,\n                    use_flash=use_flash,\n                    cache_top_k=cache_top_k,\n                    mem_freq=mem_freq,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )", "\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.auto_insert_landmarks = False\n        self.always_use_flash = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n        use_flash: Optional[bool] = None,\n        cache_top_k: Optional[int] = None,\n        max_chunk_length: Optional[int] = 0,\n        mem_freq: Optional[int] = None,\n        drop_last_logit_if_mem: Optional[bool] = False,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        use_flash = use_flash if use_flash is not None else self.always_use_flash\n\n        if self.auto_insert_landmarks:\n            mem_freq = self.config.mem_freq\n            assert self.config.mem_freq is not None\n            block_size = self.config.mem_freq + 1\n            input_ids = input_ids.view(input_ids.shape[0], -1, block_size - 1)\n            input_ids = torch.cat((input_ids, input_ids.new_full((input_ids.shape[0], input_ids.shape[1], 1), self.config.mem_id)), dim=-1)\n            input_ids = input_ids.view(input_ids.shape[0], -1)\n            if attention_mask is not None:\n                attention_mask = attention_mask.view(attention_mask.shape[0], -1, block_size - 1)\n                attention_mask = torch.cat((attention_mask, attention_mask.new_ones((attention_mask.shape[0], attention_mask.shape[1], 1))), dim=-1)\n                attention_mask = attention_mask.view(attention_mask.shape[0], -1)\n\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if max_chunk_length == 0:\n            if cache_top_k is not None:\n                max_chunk_length = self.config.train_context_length - self.config.train_context_length % (mem_freq + 1) - (cache_top_k + 1) * (mem_freq + 1)\n                if max_chunk_length <= 0:\n                    raise ValueError(\"K is too large for this model.\")\n            else:\n                max_chunk_length = None\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        window_len = max_chunk_length or input_ids.shape[1]\n        if use_flash:\n            assert window_len % (mem_freq + 1) == 0\n        last_logits = None\n        for step, idx in enumerate(range(0, input_ids.shape[1], window_len)):\n            if idx >= 1:\n                if output_attentions or output_hidden_states:\n                    raise NotImplementedError\n                if not use_cache:\n                    raise NotImplementedError\n            outputs = self.model(\n                input_ids=input_ids[:, idx:idx + window_len],\n                attention_mask=attention_mask[:, :idx + window_len + attention_mask.shape[1] - input_ids.shape[1]] if attention_mask is not None else None,\n                position_ids=position_ids[:, idx:idx + window_len] if position_ids is not None else None,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds[:, idx:idx + window_len] if inputs_embeds is not None else None,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                offload_cache_to_cpu=offload_cache_to_cpu,\n                use_flash=(use_flash or self.auto_insert_landmarks),\n                cache_top_k=cache_top_k,\n                mem_freq=mem_freq,\n            )\n            past_key_values = outputs[1]\n            if last_logits is not None:\n                last_logits = torch.cat((last_logits, outputs[0]), dim=-2)\n            last_logits = outputs[0]\n\n        hidden_states = last_logits\n        if self.auto_insert_landmarks:\n            block_size = self.config.mem_freq + 1\n            hidden_states = hidden_states.reshape(hidden_states.shape[0], hidden_states.shape[1] // block_size, block_size, hidden_states.shape[2])\n            hidden_states = hidden_states[:, :, :block_size - 1]\n            hidden_states = hidden_states.reshape(hidden_states.shape[0], -1, hidden_states.shape[3])\n        if drop_last_logit_if_mem:\n            is_any_mem = (input_ids[:, -1] == self.config.mem_id).any()\n            are_all_mem = (input_ids[:, -1] == self.config.mem_id).all()\n            assert is_any_mem == are_all_mem\n            if is_any_mem:\n                hidden_states = hidden_states[:, :-1]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_mem_id(self, mem_id):\n        if self.config.mem_id is not None:\n            assert mem_id == self.config.mem_id, \"Chanigng mem_id can break the model. If you really intend to do this, manually disable this check\"\n        self.config.mem_id = mem_id\n\n    def enable_landmark_insertion(self):\n        self.auto_insert_landmarks = True\n\n    def enable_flash(self):\n        self.always_use_flash = True\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        total_len = input_ids.shape[1]\n        if past_key_values:\n            prev_len = input_ids.shape[1] - 1\n            use_flash = False if kwargs.get(\"use_flash\") is not None else None\n        else:\n            prev_len = 0\n            use_flash = kwargs.get(\"use_flash\")\n\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        mem_freq = kwargs.get(\"mem_freq\") or self.config.mem_freq\n\n        if mem_freq is not None:\n            if position_ids is not None:\n                raise NotImplementedError\n            T = input_ids.shape[1]\n\n            prev_incomplete_len = prev_len % mem_freq\n            prev_complete_len = prev_len - prev_incomplete_len\n            incomplete_len = total_len % mem_freq\n            new_full_len = total_len - prev_complete_len - incomplete_len\n\n            prev_input, input_ids_with_mem, input_ids_without_mem = torch.split(input_ids, (prev_complete_len, new_full_len, incomplete_len), dim=-1)\n            \n            bsz, q_len = input_ids.size()\n            input_ids_with_mem = input_ids_with_mem.view(bsz, -1, mem_freq)            \n            input_ids_with_mem = torch.cat(\n                (\n                    input_ids_with_mem, \n                    input_ids_with_mem.new_full((bsz, input_ids_with_mem.shape[1], 1), self.config.mem_id)\n                ), \n                dim=-1\n            ).view(bsz, -1)\n            input_ids = torch.cat((prev_input, input_ids_with_mem, input_ids_without_mem), dim=-1)\n            if attention_mask is not None:\n                attention_mask_with_mem, attention_mask_without_mem = torch.split(attention_mask, (prev_complete_len + new_full_len, incomplete_len), dim=-1)\n                attention_mask_with_mem = attention_mask_with_mem.view(bsz, -1, mem_freq)\n                attention_mask_with_mem = torch.cat(\n                    (\n                        attention_mask_with_mem, \n                        attention_mask_with_mem.new_ones((bsz, attention_mask_with_mem.shape[1], 1))\n                    ), \n                    dim=-1\n                ).view(bsz, -1)\n                attention_mask = torch.cat((attention_mask_with_mem, attention_mask_without_mem), dim=-1)\n            \n            \n        input_ids = input_ids[:, prev_len:]\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            position_ids = position_ids[:, -input_ids.shape[1]:].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None and mem_freq is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"offload_cache_to_cpu\": kwargs.get(\"offload_cache_to_cpu\"),\n                \"use_flash\": use_flash,\n                \"cache_top_k\": kwargs.get(\"cache_top_k\"),\n                \"max_chunk_length\": kwargs.get(\"max_chunk_length\", 0),\n                \"mem_freq\": mem_freq,\n                \"drop_last_logit_if_mem\": not self.config.include_landmark_in_loss,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past", "\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a", "\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )", ")\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )", ""]}
{"filename": "llama/ltriton/test_flash_landmark_attention.py", "chunked_list": ["import torch\nimport math\n\nfrom flash_landmark_attention import fused_landmark_attention\n\n\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False)\n            \n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        #x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        x_exp = torch.exp((x - maxes).to(torch.float32))\n\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(dim, resp_mem_idx, x_exp, )\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        #probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        probs = x_exp / denom\n        \n        \n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs)\n\n        return probs\n\n    @staticmethod\n    def backward(ctx, grad_probs):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n        \n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx", "\ndef landmark_grouped_softmax(x, dim, is_mem, last_section_mask):\n    \n    last_and_rest_mask = last_section_mask # | mask\n\n    full_access_mask =  is_mem | last_and_rest_mask\n\n    max_mem_cnt = 16\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    mem_bucket_id = max_mem_cnt - 1\n    resp_mem_idx = torch.where(last_and_rest_mask, \n                                max_mem_cnt - 1,\n                                torch.where(is_mem, mem_bucket_id, mem_group_idx))\n    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape, ))\n    group_prob.scatter_(dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs)\n    probs = probs.mul(torch.where(full_access_mask, last_section_mask, torch.gather(group_prob, dim, resp_mem_idx)))\n\n\n    return probs", "\nbatch = 2\nnheads = 8\nseqlen_q = 1024\nseqlen_k = 1024 #512\nd = 128\nuse_I_for_v = False\nmem_freq = 63\nq = torch.rand((batch, seqlen_q, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nk = torch.rand((batch, seqlen_k, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nif not use_I_for_v:\n    v = torch.rand((batch, seqlen_k, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nelse:\n    v = torch.eye(seqlen_k, d).cuda().to(torch.bfloat16)\n    v = v.view(1, 1, seqlen_k, d).expand(batch, nheads, seqlen_k, d)", "q = torch.rand((batch, seqlen_q, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nk = torch.rand((batch, seqlen_k, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nif not use_I_for_v:\n    v = torch.rand((batch, seqlen_k, nheads, d)).cuda().to(torch.bfloat16).transpose(1, 2)\nelse:\n    v = torch.eye(seqlen_k, d).cuda().to(torch.bfloat16)\n    v = v.view(1, 1, seqlen_k, d).expand(batch, nheads, seqlen_k, d)\nq.requires_grad = True\nk.requires_grad = True\nv.requires_grad = True", "k.requires_grad = True\nv.requires_grad = True\nblock_size = mem_freq + 1\nis_mem = torch.arange(0, seqlen_k, device=q.device) % block_size == (block_size - 1)\nout = fused_landmark_attention(q, k, v, is_mem, block_size=block_size)\n\ndef f():\n    import math\n    att = q @ k.transpose(-1, -2) / math.sqrt(d)\n    att_mask = torch.tril(torch.ones((1, 1, seqlen_q, seqlen_k), device=q.device), diagonal=seqlen_k - seqlen_q)== 1.\n\n    last_section_mask = (torch.arange(0, seqlen_k, device=q.device) // (mem_freq + 1))[None, :] == (torch.arange(seqlen_k - seqlen_q, seqlen_k, device=q.device) // (mem_freq + 1))[:, None]\n\n    last_section_mask = last_section_mask.unsqueeze(0).unsqueeze(1)\n    is_mem_ = is_mem.view(1, 1, 1, seqlen_k)\n    mask = att_mask & ~(last_section_mask & is_mem_)\n    last_section_mask = last_section_mask & mask\n    is_mem_ = is_mem_ & mask\n    is_mem_ = is_mem_.expand(batch, nheads, seqlen_q, seqlen_k)\n    last_section_msak = last_section_mask.expand(batch, nheads, seqlen_q, seqlen_k)\n    att.masked_fill_(~mask, float(\"-inf\"))\n    \n\n    att = landmark_grouped_softmax(att, -1, is_mem_, last_section_mask).to(q.dtype)\n    att.masked_fill_(~mask, 0.0)\n    exact_out = att @ v\n    return exact_out    ", "exact_out = f()\n\ndef make_f_grad(func):\n    def f_():\n        exact_out = func()\n        return torch.autograd.grad((exact_out**2).sum(), [q, k, v])\n    return f_\n\n\ndef f_exact():\n    return fused_landmark_attention(q, k, v, is_mem, block_size=block_size)", "\ndef f_exact():\n    return fused_landmark_attention(q, k, v, is_mem, block_size=block_size)\n\ndef f_torch():\n    return torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n\nif use_I_for_v and d >= seqlen_k:\n    assert torch.allclose(out.sum(-1), torch.ones_like(out.sum(-1)), atol=1e-03, rtol=1e-03), out.sum(-1)\n    assert torch.allclose(exact_out.sum(-1), torch.ones_like(exact_out.sum(-1)))", "\n#print(\"Exact\", exact_out[:, :, mem_freq-1:mem_freq+2])\n#print(\"Fused\", out[:, :, mem_freq-1:mem_freq+2])\nassert torch.allclose(out, exact_out, rtol=1e-02, atol=1e-02), (out, exact_out)\n#print(\"Diff\", (out - exact_out).max(dim=-2))\n\n\n#print(last_section_mask[0, 0])\n#print(att[0, 0])\n#print(is_mem[0, 0])", "#print(att[0, 0])\n#print(is_mem[0, 0])\n#print(is_mem[0, 0, -1])\n\ngrads = torch.autograd.grad((out ** 2).sum(), [q, k, v])\nexact_grads = torch.autograd.grad((exact_out ** 2).sum(), [q, k, v])\n#print(len(exact_grads))\n\nfor grad, exact_grad, t in zip(grads, exact_grads, [\"q\", \"k\", \"v\"]):\n    torch.set_printoptions(sci_mode=False)\n    if not torch.allclose(grad, exact_grad, rtol=4e-02, atol=4e-02):\n        #print((grad, exact_grad, t))\n        print(\"Failed d\", t)", "for grad, exact_grad, t in zip(grads, exact_grads, [\"q\", \"k\", \"v\"]):\n    torch.set_printoptions(sci_mode=False)\n    if not torch.allclose(grad, exact_grad, rtol=4e-02, atol=4e-02):\n        #print((grad, exact_grad, t))\n        print(\"Failed d\", t)\n    #print(t, (grad - exact_grad).max())\n    #print(t, torch.argmax((grad - exact_grad).amax(dim=-1), dim=-1))\n\nprint(\"Done once\")    \n#print(v.grad)", "print(\"Done once\")    \n#print(v.grad)\n\nimport timeit\nprint(\"Exact: \", timeit.timeit(f, number=500))\nprint(\"Fused: \", timeit.timeit(f_exact, number=500))\nprint(\"Torch: \", timeit.timeit(f_torch, number=500))\n\nprint(\"Exact Grad: \", timeit.timeit(make_f_grad(f), number=500))\nprint(\"Fused Grad: \", timeit.timeit(make_f_grad(f_exact), number=500))", "print(\"Exact Grad: \", timeit.timeit(make_f_grad(f), number=500))\nprint(\"Fused Grad: \", timeit.timeit(make_f_grad(f_exact), number=500))\nprint(\"Torch Grad: \", timeit.timeit(make_f_grad(f_torch), number=500))\n"]}
{"filename": "llama/ltriton/flash_landmark_attention.py", "chunked_list": ["import math\nimport triton\nimport torch\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(#debug, sdz, sdh, sdm, sdn,\n    Q, K, V, sm_scale, \n    Out,\n    sqz, sqh, sqm, sqd, # shape = (Z,H,N_CTX_Q,D)\n    skz, skh, skn, skd, # shape = (Z,H,N_CTX_KV,D)\n    svz, svh, svn, svd, # shape = (Z,H,N_CTX_KV,D)\n    soz, soh, som, sod, # shape = (Z,H,N_CTX_Q,D)\n    L, M,\n    Z, H, N_CTX_Q, N_CTX_KV, \n    BLOCK: tl.constexpr, # will load BLOCK_M queries, and compute self attention by blocks of BLOCK_N keys\n    BLOCK_DMODEL: tl.constexpr, # dimensionality of heads: D\n    N_PREFIX_Q: tl.constexpr,\n):\n    \n    start_m = tl.program_id(0) # idx of sequence length chunk of size 128 (BLOCK_N)\n    off_hz = tl.program_id(1) # idx of head_batch (unique idx for each head in each batch)\n\n    BLOCK_M: tl.constexpr = BLOCK\n    BLOCK_N: tl.constexpr = BLOCK\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # indices of queries we want to process\n    offs_m_real = (start_m + N_PREFIX_Q) * BLOCK_M + tl.arange(0, BLOCK_M)  # indices of queries we want to process\n    offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)\n    offs_n = tl.arange(0, BLOCK_N) # indices of keys we want to process, we start from [0, BLOCK_N-1] and update in the loop\n    offs_d = tl.arange(0, BLOCK_DMODEL) # we want to process all the dimensions of a given head\n\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd # Q.view(Z*H,N_CTX_Q,D)[off_hz, start_m*BLOCK_M:(start_m+1)*BLOCK_M, :].squeeze() that's a BLOCK_M*D matrix\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd # K.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].transpose(1,2).squeeze() that's a D*BLOCK_N matrix\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd # V.view(Z*H,N_CTX_KV,D)[off_hz, 0:BLOCK_N, :].squeeze() that's a BLOCK_N*D matrix\n\n    # pointers to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    # Load values\n    q_vals = tl.load(Q + offs_q, mask=offs_m[:, None] < N_CTX_Q, other=0) \n    \n\n    for start_n in range(0, (N_PREFIX_Q + start_m)):\n        # Load values for K and K_idx\n        k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n\n        # compute qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n        qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n        qk *= sm_scale\n        # causal masking\n        qk = tl.where(offs_m_real[:,None] >= offs_n[None,:], qk, float(\"-inf\"))\n        landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, qk, float(\"-inf\")), 1)\n        normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, float(\"-inf\"), qk)\n        normal_m = tl.max(normal_qk, 1)\n        normal_p = tl.exp(normal_qk - normal_m[:, None])\n        normal_denom = tl.sum(normal_p, 1)\n\n        # compute attention weights\n        m_curr = tl.maximum(landmark_qk, m_prev) # compute new m\n        m_curr_ = m_curr # tl.where(m_curr != float('-inf'), m_curr, float(0.0))  # ADDITIONAL CHECK IF YOU GET NaNs\n        l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n        landmark_p = tl.exp(landmark_qk - m_curr_)\n        l_curr = landmark_p + l_prev \n        l_rcp = 1. / l_curr # rescale operands of matmuls\n        # l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)  # ADDITIONAL CHECK IF YOU GET NaNs\n        landmark_p *= l_rcp\n\n        acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n        # update acc\n        v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n        acc += tl.dot((landmark_p[:, None] * normal_p / normal_denom[:, None]).to(Q.dtype.element_ty), v_vals, allow_tf32=False) \n\n\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n\n        # update offsets\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n\n    k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n    # compute qk\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n    qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n    qk *= sm_scale\n    # causal masking\n    qk = tl.where(offs_m_real[:,None] >= offs_n[None,:], qk, float(\"-inf\"))\n\n    m_curr = tl.maximum(tl.max(qk, 1), m_prev) # compute new m\n    m_curr_ = m_curr#m_curr_ = tl.where(m_curr != float('-inf'), m_curr, float(0.0))\n\n    l_prev *= tl.exp(m_prev - m_curr_) # correct old l\n    p = tl.exp(qk - m_curr_[:, None])\n    l_curr = tl.sum(p, 1) + l_prev \n\n    l_rcp = 1. / l_curr # rescale operands of matmuls\n    # l_rcp = tl.where((l_rcp == float('inf')), 0, l_rcp)  # ADDITIONAL CHECK IF YOU GET NaNs\n    p *= l_rcp[:, None]\n    acc *= (l_prev * l_rcp)[:, None] # weight for each value vector\n    # update acc\n    p = p.to(Q.dtype.element_ty)\n    v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n    acc += tl.dot(p, v_vals, allow_tf32=False) \n\n    l_prev = l_curr\n    m_prev = m_curr\n \n    # store L and M\n    offs_L = off_hz * N_CTX_Q + offs_m # L is of shape (Z*H, N_CTX_Q), here we point to L[off_hz, start_m*Block_M:(start_m+1)*Block_M]\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    # store results to output\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)", "\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, soz, soh, som, sod,\n    DO, L, slzh, slm,\n    NewDO, Delta, N_CTX_Q,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    # load\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o).to(tl.float32)#, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    do = tl.load(DO + off_o).to(tl.float32)#, mask=off_m[:, None] < N_CTX_Q, other=0.0).to(tl.float32)\n    denom = tl.load(L + off_l).to(tl.float32)#, mask=off_m < N_CTX_Q, other=1.0).to(tl.float32)\n    # denom = tl.where(denom == 0, 1.0, denom)  # ADDITIONAL CHECK IF YOU GET NaNs\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_o, do)#, mask=off_m[:, None] < N_CTX_Q)\n    tl.store(Delta + off_l, delta)#, mask=off_m < N_CTX_Q)", "\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    L, M,\n    D,\n    sqz, sqh, sqm, sqd,\n    skz, skh, skn, skd,\n    svz, svh, svn, svd,\n    Z, H, N_CTX_Q, N_CTX_KV,\n    BLOCK: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    N_PREFIX_Q: tl.constexpr,\n):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n\n    BLOCK_M: tl.constexpr = BLOCK\n    BLOCK_N: tl.constexpr = BLOCK\n\n    # offset pointers for batch/head\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    \n    # pointer to row-wise quantities in value-like data\n    D_ptrs = D + off_hz * N_CTX_Q # pointer to D.view(Z*H,N_CTX_Q)[off_hz]\n    m_ptrs = M + off_hz * N_CTX_Q # pointer to m.view(Z*H,N_CTX_Q)[off_hz]\n\n    for start_n in range(0, N_CTX_KV, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n        # pointers for keys and values\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n\n        k = tl.load(k_ptrs)#, mask=offs_n[:, None] < N_CTX_KV)\n        v = tl.load(v_ptrs)#, mask=offs_n[:, None] < N_CTX_KV)\n\n        if start_n < N_PREFIX_Q * BLOCK_M:\n            start_q_index = 0\n        elif N_CTX_Q <= start_n - N_PREFIX_Q * BLOCK_M:\n            start_q_index = start_n - N_PREFIX_Q * BLOCK_M\n        else:\n            first_start_m = start_n - N_PREFIX_Q * BLOCK_M\n            first_start_m = tl.multiple_of(first_start_m, BLOCK_M)\n            offs_m = (first_start_m + tl.arange(0, BLOCK_M))\n            offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M # indices of queries we want to process\n            offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)    \n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            \n            q = tl.load(q_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk = tl.where(offs_m_real[:,None] >= (offs_n[None,:]), qk, float(\"-inf\"))\n\n            m = tl.load(m_ptrs + offs_m) #, mask=offs_m < N_CTX_Q)\n            m_ = m # tl.where(m != float('-inf'), m, 0.0)\n\n            last_p = tl.exp(qk * sm_scale - m_[:, None])\n\n            do = tl.load(do_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dv\n            dv += tl.dot(tl.trans(last_p.to(Q.dtype.element_ty)), do, allow_tf32=False)\n\n\n            Di = tl.load(D_ptrs + offs_m) #, mask=offs_m < N_CTX_Q)\n            # compute dp = dot(v, do)\n            last_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            last_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            # compute ds = p * (dp - delta[:, None])\n            ds = last_p * last_dp * sm_scale\n            \n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q, allow_tf32=False)\n\n            dq = tl.load(dq_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k, allow_tf32=False)\n            tl.store(dq_ptrs, dq) #, mask=offs_m[:, None] < N_CTX_Q)\n            start_q_index = first_start_m + BLOCK_M\n\n        for start_m in range(start_q_index, N_CTX_Q, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            offs_m = (start_m + tl.arange(0, BLOCK_M))\n            # offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M # indices of queries we want to process\n            # offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)    \n\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            \n            q = tl.load(q_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk *= sm_scale\n            # qk = tl.where(offs_m_real[:,None] >= (offs_n[None,:]), qk, float(\"-inf\"))  # This should not be necessary anymore since we separate the first step\n\n            landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, qk, float(\"-inf\")), 1)\n            normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, float(\"-inf\"), qk)\n\n            m = tl.load(m_ptrs + offs_m)#, mask=offs_m < N_CTX_Q)\n            m_ = m # tl.where(m != float('-inf'), m, 0.0)  # ADDITIONAL CHECK IF YOU GET NaNs\n\n            p = tl.exp(landmark_qk - m_) # BLOCK_M\n\n            do = tl.load(do_ptrs)#, mask=offs_m[:,None] < N_CTX_Q) # BLOCK_M x H\n\n            normal_m = tl.max(normal_qk, 1)\n            normal_p = tl.exp(normal_qk - normal_m[:, None])\n            normal_p_normalized = normal_p / tl.sum(normal_p, 1)[:, None] # BLOCK_M x (BLOCK_N - 1)\n            normal_kv = tl.dot(normal_p_normalized.to(Q.dtype.element_ty), v, allow_tf32=False) # BLOCK_M x H\n\n            normal_D = tl.sum(do * normal_kv, 1)\n\n            # compute dv\n            dv += tl.dot(tl.trans((p[:, None] * normal_p_normalized).to(Q.dtype.element_ty)), do, allow_tf32=False)\n\n            Di = tl.load(D_ptrs + offs_m)#, mask=offs_m < N_CTX_Q)\n            # compute dp and ds for landmark\n            dp = tl.zeros([BLOCK_M], dtype=tl.float32) - Di\n            dp += normal_D \n            landmark_ds = p * dp\n            # compute dp and ds for others\n            normal_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - normal_D[:, None]\n            normal_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            normal_ds = p[:, None] * normal_p_normalized * normal_dp \n            # merge\n            ds = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1, landmark_ds[:, None], normal_ds)\n            ds *= sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q, allow_tf32=False)\n\n            dq = tl.load(dq_ptrs) #, mask=offs_m[:,None] < N_CTX_Q)\n            # compute dq\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k, allow_tf32=False)\n            tl.store(dq_ptrs, dq) #, mask=offs_m[:, None] < N_CTX_Q)\n         \n        # write-back\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv) #, mask=offs_n[:, None] < N_CTX_KV)\n        tl.store(dk_ptrs, dk) #, mask=offs_n[:, None] < N_CTX_KV)", "\n\nclass FusedLandmarkAttention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, n_prefix_q, sm_scale, block_size):\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n\n        # shape constraints\n        batch, nheads, seqlen_q, d = q.shape\n        _, _, seqlen_k, _ = k.shape\n        assert k.shape == (batch, nheads, seqlen_k, d)\n        assert v.shape == (batch, nheads, seqlen_k, d)\n        assert d <= 128, 'FlashAttention only support head dimensions up to 128'\n        assert q.dtype == k.dtype == v.dtype, 'All tensors must have the same type'\n        #assert q.dtype in [torch.float16, torch.bfloat16], 'Only support fp16 and bf16'\n        assert q.is_cuda and k.is_cuda and v.is_cuda\n        \n        BLOCK = block_size\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n        num_warps = 4 if d <= 64 else 8\n\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            L, m,\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            BLOCK=BLOCK, BLOCK_DMODEL=d,\n            N_PREFIX_Q=n_prefix_q,\n            num_warps=num_warps, num_stages=2\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = d\n        ctx.N_PREFIX_Q = n_prefix_q\n        ctx.BLOCK = BLOCK\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = ctx.BLOCK\n        q, k, v, o, l, m = ctx.saved_tensors\n        assert q.shape[2] % BLOCK == 0, \"Backward supported only for full blocks\"\n        assert k.shape[2] % BLOCK == 0, \"Backward supported only for full blocks\"\n\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        _bwd_preprocess[(ctx.grid[0], ctx.grid[1])](\n            o, o.stride(0), o.stride(1), o.stride(2), o.stride(3), do, l, l.stride(0), l.stride(1),\n            do_scaled, delta, q.shape[2],\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], k.shape[2],\n            BLOCK=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, \n            N_PREFIX_Q=ctx.N_PREFIX_Q,\n            num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None, None, None", "\ndef fused_landmark_attention(q, k, v, is_mem, sm_scale=None, block_size=64):\n\n    expected_is_mem = torch.arange(0, is_mem.shape[-1], device=is_mem.device) % block_size == (block_size - 1)\n    assert (is_mem == expected_is_mem).all()\n\n    n_history_kv = k.shape[-2] - q.shape[-2]\n    assert n_history_kv % block_size == 0\n    n_history_blocks = n_history_kv // block_size\n\n    if sm_scale is None:\n        sm_scale = 1.0 / math.sqrt(q.size(-1))\n\n    return FusedLandmarkAttention.apply(q, k, v, n_history_blocks, sm_scale, block_size)", ""]}
