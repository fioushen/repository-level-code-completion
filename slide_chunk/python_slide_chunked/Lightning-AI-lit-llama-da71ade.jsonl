{"filename": "setup.py", "chunked_list": ["import os\n\nfrom setuptools import setup, find_packages\n\n\n_PATH_ROOT = os.path.dirname(__file__)\n\nwith open(os.path.join(_PATH_ROOT, \"README.md\"), encoding=\"utf-8\") as fo:\n    readme = fo.read()\n", "\nsetup(\n    name='lit-llama',\n    version='0.1.0',\n    description='Implementation of the LLaMA language model',\n    author='Lightning AI',\n    url='https://github.com/lightning-AI/lit-llama',\n    install_requires=[\n        \"torch>=2.0.0\",\n        \"lightning @ git+https://github.com/Lightning-AI/lightning@master\",", "        \"torch>=2.0.0\",\n        \"lightning @ git+https://github.com/Lightning-AI/lightning@master\",\n        \"sentencepiece\",\n        \"bitsandbytes\",\n    ],\n    packages=find_packages(),\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n)\n", ")\n"]}
{"filename": "generate.py", "chunked_list": ["import sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import lazy_load, llama_model_lookup, quantization\n\n\n@torch.no_grad()\ndef generate(\n    model: LLaMA,\n    idx: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    max_seq_length: Optional[int] = None,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n    eos_id: Optional[int] = None,\n) -> torch.Tensor:\n    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n\n    The implementation of this function is modified from A. Karpathy's nanoGPT.\n\n    Args:\n        model: The model to use.\n        idx: Tensor of shape (T) with indices of the prompt sequence.\n        max_new_tokens: The number of new tokens to generate.\n        max_seq_length: The maximum sequence length allowed.\n        temperature: Scales the predicted logits by 1 / temperature\n        top_k: If specified, only sample among the tokens with the k highest probabilities\n        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n    \"\"\"\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = idx.size(0)\n    T_new = T + max_new_tokens\n    if max_seq_length is None:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = idx.device, idx.dtype\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(T_new, dtype=dtype, device=device)\n    empty[:T] = idx\n    idx = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    if idx.device.type == \"xla\":\n        import torch_xla.core.xla_model as xm\n\n        xm.mark_step()\n\n    # generate max_new_tokens tokens\n    for _ in range(max_new_tokens):\n        x = idx.index_select(0, input_pos).view(1, -1)\n\n        # forward\n        logits = model(x, max_seq_length, input_pos)\n        logits = logits[0, -1] / temperature\n\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n\n        # advance\n        input_pos = input_pos[-1:] + 1\n\n        if idx.device.type == \"xla\":\n            xm.mark_step()\n\n        # concatenate the new generation\n        idx = idx.index_copy(0, input_pos, idx_next)\n\n        # if <eos> token is triggered, return the output (stop generation)\n        if idx_next == eos_id:\n            return idx[:input_pos]  # include the EOS token\n\n    return idx", "\n@torch.no_grad()\ndef generate(\n    model: LLaMA,\n    idx: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    max_seq_length: Optional[int] = None,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n    eos_id: Optional[int] = None,\n) -> torch.Tensor:\n    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n\n    The implementation of this function is modified from A. Karpathy's nanoGPT.\n\n    Args:\n        model: The model to use.\n        idx: Tensor of shape (T) with indices of the prompt sequence.\n        max_new_tokens: The number of new tokens to generate.\n        max_seq_length: The maximum sequence length allowed.\n        temperature: Scales the predicted logits by 1 / temperature\n        top_k: If specified, only sample among the tokens with the k highest probabilities\n        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n    \"\"\"\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = idx.size(0)\n    T_new = T + max_new_tokens\n    if max_seq_length is None:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = idx.device, idx.dtype\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(T_new, dtype=dtype, device=device)\n    empty[:T] = idx\n    idx = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    if idx.device.type == \"xla\":\n        import torch_xla.core.xla_model as xm\n\n        xm.mark_step()\n\n    # generate max_new_tokens tokens\n    for _ in range(max_new_tokens):\n        x = idx.index_select(0, input_pos).view(1, -1)\n\n        # forward\n        logits = model(x, max_seq_length, input_pos)\n        logits = logits[0, -1] / temperature\n\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n\n        # advance\n        input_pos = input_pos[-1:] + 1\n\n        if idx.device.type == \"xla\":\n            xm.mark_step()\n\n        # concatenate the new generation\n        idx = idx.index_copy(0, input_pos, idx_next)\n\n        # if <eos> token is triggered, return the output (stop generation)\n        if idx_next == eos_id:\n            return idx[:input_pos]  # include the EOS token\n\n    return idx", "\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    *,\n    num_samples: int = 1,\n    max_new_tokens: int = 50,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        prompt: The prompt string to use for generating the samples.\n        num_samples: The number of text samples to generate.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(checkpoint_path) as checkpoint:\n        name = llama_model_lookup(checkpoint)\n\n        with fabric.init_module(empty_init=True), quantization(mode=quantize):\n            model = LLaMA.from_name(name)\n\n        model.load_state_dict(checkpoint)\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=fabric.device)\n    prompt_length = encoded.size(0)\n\n    L.seed_everything(1234)\n    for i in range(num_samples):\n        t0 = time.perf_counter()\n        y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k)\n        t = time.perf_counter() - t0\n\n        model.reset_cache()\n        print(tokenizer.decode(y))\n        tokens_generated = y.size(0) - prompt_length\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    warnings.filterwarnings(\n        # Triggered in bitsandbytes/autograd/_functions.py:298\n        \"ignore\", \n        message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n    )\n    CLI(main)", ""]}
{"filename": "finetune/full.py", "chunked_list": ["\"\"\"\nInstruction-tuning on the Alpaca dataset using a regular finetuning procedure (updating all layers).\n\nNote: If you run into a CUDA error \"Expected is_sm80 to be true, but got false\", uncomment the line\n`torch.backends.cuda.enable_flash_sdp(False)` in the script below (see https://github.com/Lightning-AI/lit-llama/issues/101).\n\"\"\"\nimport sys\nfrom pathlib import Path\nimport os\nimport time", "import os\nimport time\nfrom functools import partial\n\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\nimport numpy as np\nimport torch\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n", "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama.model import Block, LLaMA, LLaMAConfig\nfrom lit_llama.tokenizer import Tokenizer\nfrom lit_llama.utils import save_model_checkpoint", "from lit_llama.tokenizer import Tokenizer\nfrom lit_llama.utils import save_model_checkpoint\nfrom scripts.prepare_alpaca import generate_prompt\n\n\ninstruction_tuning = True\neval_interval = 1000\nsave_interval = 1000\neval_iters = 100\nlog_interval = 100", "eval_iters = 100\nlog_interval = 100\ndevices = 4\n\n# Hyperparameters\nlearning_rate = 3e-5\nbatch_size = 128 / devices\nmicro_batch_size = 4\ngradient_accumulation_iters = batch_size // micro_batch_size\nassert gradient_accumulation_iters > 0", "gradient_accumulation_iters = batch_size // micro_batch_size\nassert gradient_accumulation_iters > 0\nepoch_size = 50000  # train dataset size\nnum_epochs = 5\nmax_iters = num_epochs * (epoch_size // micro_batch_size) // devices\nweight_decay = 0.0\nblock_size = 512\nwarmup_iters = 100\n\n\ndef main(\n    data_dir: str = \"data/alpaca\",\n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    out_dir: str = \"out/full/alpaca\",\n):\n\n    auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n    strategy = FSDPStrategy(auto_wrap_policy=auto_wrap_policy, activation_checkpointing=Block, limit_all_gathers=True)\n\n    fabric = L.Fabric(accelerator=\"cuda\", devices=devices, precision=\"bf16-mixed\", strategy=strategy)\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig.from_name(\"7B\")\n    config.block_size = block_size\n\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.device:\n        torch.set_default_tensor_type(torch.HalfTensor)\n        model = LLaMA(config).bfloat16()\n        torch.set_default_tensor_type(torch.FloatTensor)\n        model.load_state_dict(checkpoint, strict=False) \n\n    model = fabric.setup_module(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, foreach=False)\n    optimizer = fabric.setup_optimizers(optimizer)\n\n    train(fabric, model, optimizer, train_data, val_data, out_dir)\n\n    # Save the final checkpoint at the end of training\n    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-full-finetuned.pth\"))", "\n\ndef main(\n    data_dir: str = \"data/alpaca\",\n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    out_dir: str = \"out/full/alpaca\",\n):\n\n    auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n    strategy = FSDPStrategy(auto_wrap_policy=auto_wrap_policy, activation_checkpointing=Block, limit_all_gathers=True)\n\n    fabric = L.Fabric(accelerator=\"cuda\", devices=devices, precision=\"bf16-mixed\", strategy=strategy)\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig.from_name(\"7B\")\n    config.block_size = block_size\n\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.device:\n        torch.set_default_tensor_type(torch.HalfTensor)\n        model = LLaMA(config).bfloat16()\n        torch.set_default_tensor_type(torch.FloatTensor)\n        model.load_state_dict(checkpoint, strict=False) \n\n    model = fabric.setup_module(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, foreach=False)\n    optimizer = fabric.setup_optimizers(optimizer)\n\n    train(fabric, model, optimizer, train_data, val_data, out_dir)\n\n    # Save the final checkpoint at the end of training\n    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-full-finetuned.pth\"))", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_data: np.ndarray,\n    val_data: np.ndarray,\n    out_dir: str,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n    step_count = 0\n    model.train()\n\n    for iter_num in range(max_iters):\n\n        is_accumulating = (iter_num + 1) % gradient_accumulation_iters != 0\n\n        if step_count <= warmup_iters:\n            # linear warmup\n            lr = learning_rate * step_count / warmup_iters\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        t0 = time.time()\n        \n        input_ids, targets = get_batch(fabric, train_data)\n        with fabric.no_backward_sync(model, enabled=is_accumulating):\n            logits = model(input_ids)\n            loss = loss_fn(logits, targets)\n            fabric.backward(loss / gradient_accumulation_iters)\n\n        if not is_accumulating:\n            optimizer.step()\n            optimizer.zero_grad()\n            step_count += 1\n\n            if step_count % eval_interval == 0:\n                val_loss = validate(fabric, model, val_data)\n                fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n                fabric.barrier()\n\n            if step_count % save_interval == 0:\n                print(f\"Saving weights to {out_dir}\")\n                save_model_checkpoint(fabric, model, os.path.join(out_dir, f\"iter-{iter_num:06d}-ckpt.pth\"))\n\n        dt = time.time() - t0\n        if iter_num % log_interval == 0:\n            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")", "\n\ndef generate_response(model, instruction):\n    tokenizer = Tokenizer(\"checkpoints/lit-llama/tokenizer.model\")\n    sample = {\"instruction\": instruction, \"input\": \"\"}\n    prompt = instruction\n    if instruction_tuning:\n        prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    output = generate(\n        model,\n        idx=encoded,\n        max_seq_length=block_size,\n        max_new_tokens=100,\n    )\n    output = tokenizer.decode(output)\n    return output # output.split(\"### Response:\")[1].strip()", "\n\n@torch.no_grad()\ndef validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        input_ids, targets = get_batch(fabric, val_data)\n        logits = model(input_ids)\n        loss = loss_fn(logits, targets)\n        losses[k] = loss.item()\n    out = losses.mean()\n\n    # produce an example:\n    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n    \n    output = generate_response(model, instruction)\n    fabric.print(instruction)\n    fabric.print(output)\n\n    model.train()\n    return out.item()", "\n\ndef loss_fn(logits, targets):\n    # shift the targets such that output n predicts token n+1\n    logits = logits[..., :-1, :].contiguous()\n    targets = targets[..., 1:].contiguous()\n    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    return loss\n\n\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\n\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\n\ndef load_datasets(data_dir):\n    train_data = torch.load(os.path.join(data_dir, \"train.pt\"))\n    val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n    return train_data, val_data\n\n\nif __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n\n    from jsonargparse.cli import CLI\n\n    CLI(main)", "if __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n\n    from jsonargparse.cli import CLI\n\n    CLI(main)\n", ""]}
{"filename": "finetune/adapter_v2.py", "chunked_list": ["\"\"\"\nInstruction-tuning with LLaMA-Adapter v2 on the Alpaca dataset following the paper\n\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\nhttps://arxiv.org/abs/2304.15010\n\nThis script runs on a single GPU by default. You can adjust the `micro_batch_size` to fit your GPU memory.\nYou can finetune within 1 hour as done in the original paper using DeepSpeed Zero-2 on 8 A100 GPUs by setting the\ndevices variable to `devices = 8` and `micro_batch_size = 8` (or higher).\n", "devices variable to `devices = 8` and `micro_batch_size = 8` (or higher).\n\nNote: If you run into a CUDA error \"Expected is_sm80 to be true, but got false\", uncomment the line\n`torch.backends.cuda.enable_flash_sdp(False)` in the script below (see https://github.com/Lightning-AI/lit-llama/issues/101).\n\"\"\"\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nimport shutil", "from pathlib import Path\nimport shutil\n\nimport lightning as L\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()", "# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama.adapter import LLaMA, LLaMAConfig\nfrom lit_llama.adapter_v2 import (\n    mark_only_adapter_v2_as_trainable,\n    add_adapter_v2_parameters_to_linear_layers,\n    adapter_v2_state_from_state_dict", "    add_adapter_v2_parameters_to_linear_layers,\n    adapter_v2_state_from_state_dict\n    )\nfrom lit_llama.tokenizer import Tokenizer\nfrom scripts.prepare_alpaca import generate_prompt\nfrom lightning.fabric.strategies import DeepSpeedStrategy\n\n\neval_interval = 600\nsave_interval = 1000", "eval_interval = 600\nsave_interval = 1000\neval_iters = 100\nlog_interval = 1\ndevices = 1\n\n# Hyperparameters\nlearning_rate = 9e-3\nbatch_size = 64 / devices\nmicro_batch_size = 4", "batch_size = 64 / devices\nmicro_batch_size = 4\ngradient_accumulation_iters = batch_size // micro_batch_size\nassert gradient_accumulation_iters > 0\nepoch_size = 50000  # train dataset size\nnum_epochs = 5\nmax_iters = num_epochs * (epoch_size // micro_batch_size) // devices\nweight_decay = 0.02\nmax_seq_length = 256  # see scripts/prepare_alpaca.py\nwarmup_iters = 2 * (epoch_size // micro_batch_size) // devices  # 2 epoch", "max_seq_length = 256  # see scripts/prepare_alpaca.py\nwarmup_iters = 2 * (epoch_size // micro_batch_size) // devices  # 2 epoch\n\nds_config = {\n    \"train_micro_batch_size_per_gpu\": micro_batch_size,\n    \"gradient_accumulation_steps\": gradient_accumulation_iters,\n    \"zero_optimization\": {\"stage\": 2},\n}\n\n\ndef main(\n    data_dir: str = \"data/alpaca\", \n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    out_dir: str = \"out/adapter_v2/alpaca\",\n):\n\n    fabric = L.Fabric(\n        accelerator=\"cuda\",\n        devices=1,\n        strategy=(DeepSpeedStrategy(config=ds_config) if devices > 1 else \"auto\"),\n        precision=\"bf16-true\",\n    )\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig(block_size=max_seq_length)\n\n    if not os.path.isfile(pretrained_path):\n        raise FileNotFoundError(\n            f\"Can't find the pretrained weights at {pretrained_path}.\"\n            \" Please follow the instructions in the README to download them.\"\n        )\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.init_module():\n        model = LLaMA(config)\n        # strict=False because missing keys due to adapter weights not contained in state dict\n        model.load_state_dict(checkpoint, strict=False)\n\n    add_adapter_v2_parameters_to_linear_layers(model)\n    mark_only_adapter_v2_as_trainable(model)\n\n    num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f\"Number of trainable parameters: {num_params}\")\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    model, optimizer = fabric.setup(model, optimizer)\n    train(fabric, model, optimizer, train_data, val_data, out_dir)\n\n    # Save the final checkpoint at the end of training\n    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-adapter-finetuned.pth\"))", "\n\ndef main(\n    data_dir: str = \"data/alpaca\", \n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    out_dir: str = \"out/adapter_v2/alpaca\",\n):\n\n    fabric = L.Fabric(\n        accelerator=\"cuda\",\n        devices=1,\n        strategy=(DeepSpeedStrategy(config=ds_config) if devices > 1 else \"auto\"),\n        precision=\"bf16-true\",\n    )\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig(block_size=max_seq_length)\n\n    if not os.path.isfile(pretrained_path):\n        raise FileNotFoundError(\n            f\"Can't find the pretrained weights at {pretrained_path}.\"\n            \" Please follow the instructions in the README to download them.\"\n        )\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.init_module():\n        model = LLaMA(config)\n        # strict=False because missing keys due to adapter weights not contained in state dict\n        model.load_state_dict(checkpoint, strict=False)\n\n    add_adapter_v2_parameters_to_linear_layers(model)\n    mark_only_adapter_v2_as_trainable(model)\n\n    num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f\"Number of trainable parameters: {num_params}\")\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    model, optimizer = fabric.setup(model, optimizer)\n    train(fabric, model, optimizer, train_data, val_data, out_dir)\n\n    # Save the final checkpoint at the end of training\n    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-adapter-finetuned.pth\"))", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_data: np.ndarray,\n    val_data: np.ndarray,\n    out_dir: str,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n    step_count = 0\n\n    for iter_num in range(max_iters):\n\n        if step_count <= warmup_iters:\n            # linear warmup\n            lr = learning_rate * step_count / warmup_iters\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        t0 = time.time()\n\n        input_ids, targets = get_batch(fabric, train_data)\n        with fabric.no_backward_sync(model, enabled=((iter_num + 1) % gradient_accumulation_iters != 0)):\n            logits = model(input_ids)\n            loss = loss_fn(logits, targets)\n            fabric.backward(loss / gradient_accumulation_iters)\n\n        if (iter_num + 1) % gradient_accumulation_iters == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            step_count += 1\n                \n            if step_count % eval_interval == 0:\n                val_loss = validate(fabric, model, val_data)\n                fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n                fabric.barrier()\n\n            if step_count % save_interval == 0:\n                print(f\"Saving adapter weights to {out_dir}\")\n                # TODO: Provide a function/script to merge the adapter weights with pretrained weights\n                save_model_checkpoint(fabric, model, os.path.join(out_dir, f\"iter-{iter_num:06d}.pth\"))\n\n        dt = time.time() - t0\n        if iter_num % log_interval == 0:\n            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")", "\n\ndef generate_response(model, instruction, input=\"\"):\n    tokenizer = Tokenizer(\"checkpoints/lit-llama/tokenizer.model\")\n    sample = {\"instruction\": instruction, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    output = generate(\n        model,\n        idx=encoded,\n        max_seq_length=max_seq_length,\n        max_new_tokens=100,\n        temperature=0.8,\n    )\n    output = tokenizer.decode(output)\n    return output # output.split(\"### Response:\")[1].strip()", "\n\n@torch.no_grad()\ndef validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        input_ids, targets = get_batch(fabric, val_data)\n        logits = model(input_ids)\n        loss = loss_fn(logits, targets)\n        losses[k] = loss.item()\n    val_loss = losses.mean()\n\n    # produce an example:\n    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n    output = generate_response(model, instruction)\n    fabric.print(instruction)\n    fabric.print(output)\n\n    model.train()\n    return val_loss.item()", "\ndef loss_fn(logits, targets):\n    # shift the targets such that output n predicts token n+1\n    logits = logits[..., :-1, :].contiguous()\n    targets = targets[..., 1:].contiguous()\n    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    return loss\n    \n\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\n\ndef load_datasets(data_dir):\n    train_data = torch.load(os.path.join(data_dir, \"train.pt\"))\n    val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n    return train_data, val_data\n\n\ndef save_model_checkpoint(fabric, model, file_path):\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\n        tmp_path = file_path.with_suffix(\".tmp\")\n        fabric.save(tmp_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            # and only keep the adapter weights\n            state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_path)\n            state_dict = adapter_v2_state_from_state_dict(state_dict)\n            torch.save(state_dict, file_path)\n            shutil.rmtree(tmp_path)\n    else:\n        state_dict = adapter_v2_state_from_state_dict(model.state_dict())\n        if fabric.global_rank == 0:\n            torch.save(state_dict, file_path)\n        fabric.barrier()", "def save_model_checkpoint(fabric, model, file_path):\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\n        tmp_path = file_path.with_suffix(\".tmp\")\n        fabric.save(tmp_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            # and only keep the adapter weights\n            state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_path)\n            state_dict = adapter_v2_state_from_state_dict(state_dict)\n            torch.save(state_dict, file_path)\n            shutil.rmtree(tmp_path)\n    else:\n        state_dict = adapter_v2_state_from_state_dict(model.state_dict())\n        if fabric.global_rank == 0:\n            torch.save(state_dict, file_path)\n        fabric.barrier()", "\n\nif __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n\n    from jsonargparse.cli import CLI\n\n    CLI(main)", ""]}
{"filename": "finetune/adapter.py", "chunked_list": ["\"\"\"\nInstruction-tuning with LLaMA-Adapter on the Alpaca dataset following the paper\n\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\nhttps://arxiv.org/abs/2303.16199\n\nThis script runs on a single GPU by default. You can adjust the `micro_batch_size` to fit your GPU memory.\nYou can finetune within 1 hour as done in the original paper using DeepSpeed Zero-2 on 8 A100 GPUs by setting the\ndevices variable to `devices = 8` and `micro_batch_size = 8` (or higher).\n", "devices variable to `devices = 8` and `micro_batch_size = 8` (or higher).\n\nNote: If you run into a CUDA error \"Expected is_sm80 to be true, but got false\", uncomment the line\n`torch.backends.cuda.enable_flash_sdp(False)` in the script below (see https://github.com/Lightning-AI/lit-llama/issues/101).\n\"\"\"\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nimport shutil", "from pathlib import Path\nimport shutil\n\nimport lightning as L\nimport numpy as np\nimport torch\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))", "wd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama.adapter import LLaMA, LLaMAConfig, mark_only_adapter_as_trainable, adapter_state_from_state_dict\nfrom lit_llama.tokenizer import Tokenizer\nfrom scripts.prepare_alpaca import generate_prompt\nfrom lightning.fabric.strategies import DeepSpeedStrategy\n\n", "\n\ninstruction_tuning = True\neval_interval = 600\nsave_interval = 1000\neval_iters = 100\nlog_interval = 1\ndevices = 1\n\n# Hyperparameters", "\n# Hyperparameters\nlearning_rate = 9e-3\nbatch_size = 64 / devices\nmicro_batch_size = 4\ngradient_accumulation_iters = batch_size // micro_batch_size\nassert gradient_accumulation_iters > 0\nepoch_size = 50000  # train dataset size\nnum_epochs = 5\nmax_iters = num_epochs * (epoch_size // micro_batch_size) // devices", "num_epochs = 5\nmax_iters = num_epochs * (epoch_size // micro_batch_size) // devices\nweight_decay = 0.02\nmax_seq_length = 256  # see scripts/prepare_alpaca.py\nwarmup_iters = 2 * (epoch_size // micro_batch_size) // devices  # 2 epochs\n\nds_config = {\n    \"train_micro_batch_size_per_gpu\": micro_batch_size,\n    \"gradient_accumulation_steps\": gradient_accumulation_iters,\n    \"zero_optimization\": {\"stage\": 2},", "    \"gradient_accumulation_steps\": gradient_accumulation_iters,\n    \"zero_optimization\": {\"stage\": 2},\n}\n\n\ndef main(\n    data_dir: str = \"data/alpaca\", \n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    out_dir: str = \"out/adapter/alpaca\",\n):\n\n    fabric = L.Fabric(\n        accelerator=\"cuda\", \n        devices=devices, \n        strategy=(DeepSpeedStrategy(config=ds_config) if devices > 1 else \"auto\"), \n        precision=\"bf16-true\",\n    )\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig(block_size=max_seq_length)\n\n    if not os.path.isfile(pretrained_path):\n        raise FileNotFoundError(\n            f\"Can't find the pretrained weights at {pretrained_path}.\"\n            \" Please follow the instructions in the README to download them.\"\n        )\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.init_module():\n        model = LLaMA(config)\n        # strict=False because missing keys due to adapter weights not containted in state dict\n        model.load_state_dict(checkpoint, strict=False)\n\n    mark_only_adapter_as_trainable(model)\n\n    num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f\"Number of trainable parameters: {num_params}\")\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    model, optimizer = fabric.setup(model, optimizer)\n    train(fabric, model, optimizer, train_data, val_data, out_dir)\n\n    # Save the final checkpoint at the end of training\n    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-adapter-finetuned.pth\"))", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_data: np.ndarray,\n    val_data: np.ndarray,\n    out_dir: str,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n    step_count = 0\n\n    for iter_num in range(max_iters):\n\n        if step_count <= warmup_iters:\n            # linear warmup\n            lr = learning_rate * step_count / warmup_iters\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        t0 = time.time()\n\n        input_ids, targets = get_batch(fabric, train_data)\n        with fabric.no_backward_sync(model, enabled=((iter_num + 1) % gradient_accumulation_iters != 0)):\n            logits = model(input_ids)\n            loss = loss_fn(logits, targets)\n            fabric.backward(loss / gradient_accumulation_iters)\n\n        if (iter_num + 1) % gradient_accumulation_iters == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            step_count += 1\n                \n            if step_count % eval_interval == 0:\n                val_loss = validate(fabric, model, val_data)\n                fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n                fabric.barrier()\n\n            if step_count % save_interval == 0:\n                print(f\"Saving adapter weights to {out_dir}\")\n                # TODO: Provide a function/script to merge the adapter weights with pretrained weights\n                save_model_checkpoint(fabric, model, os.path.join(out_dir, f\"iter-{iter_num:06d}.pth\"))\n\n        dt = time.time() - t0\n        if iter_num % log_interval == 0:\n            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")", "\n\ndef generate_response(model, instruction, input=\"\"):\n    tokenizer = Tokenizer(\"checkpoints/lit-llama/tokenizer.model\")\n    sample = {\"instruction\": instruction, \"input\": input}\n    prompt = instruction\n    if instruction_tuning:\n        prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    output = generate(\n        model,\n        idx=encoded,\n        max_seq_length=max_seq_length,\n        max_new_tokens=100,\n        temperature=0.8,\n    )\n    output = tokenizer.decode(output)\n    return output # output.split(\"### Response:\")[1].strip()", "\n\n@torch.no_grad()\ndef validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        input_ids, targets = get_batch(fabric, val_data)\n        logits = model(input_ids)\n        loss = loss_fn(logits, targets)\n        losses[k] = loss.item()\n    val_loss = losses.mean()\n\n    # produce an example:\n    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n    output = generate_response(model, instruction)\n    fabric.print(instruction)\n    fabric.print(output)\n\n    model.train()\n    return val_loss.item()", "\ndef loss_fn(logits, targets):\n    # shift the targets such that output n predicts token n+1\n    logits = logits[..., :-1, :].contiguous()\n    targets = targets[..., 1:].contiguous()\n    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    return loss\n    \n\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\n\ndef load_datasets(data_dir):\n    train_data = torch.load(os.path.join(data_dir, \"train.pt\"))\n    val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n    return train_data, val_data\n\n\ndef save_model_checkpoint(fabric, model, file_path):\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\n        tmp_path = file_path.with_suffix(\".tmp\")\n        fabric.save(tmp_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            # and only keep the adapter weights\n            state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_path)\n            state_dict = adapter_state_from_state_dict(state_dict)\n            torch.save(state_dict, file_path)\n            shutil.rmtree(tmp_path)\n    else:\n        state_dict = adapter_state_from_state_dict(model.state_dict())\n        if fabric.global_rank == 0:\n            torch.save(state_dict, file_path)\n        fabric.barrier()", "def save_model_checkpoint(fabric, model, file_path):\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\n        tmp_path = file_path.with_suffix(\".tmp\")\n        fabric.save(tmp_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            # and only keep the adapter weights\n            state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_path)\n            state_dict = adapter_state_from_state_dict(state_dict)\n            torch.save(state_dict, file_path)\n            shutil.rmtree(tmp_path)\n    else:\n        state_dict = adapter_state_from_state_dict(model.state_dict())\n        if fabric.global_rank == 0:\n            torch.save(state_dict, file_path)\n        fabric.barrier()", "\n\nif __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n\n    from jsonargparse.cli import CLI\n\n    CLI(main)", ""]}
{"filename": "finetune/lora.py", "chunked_list": ["\"\"\"\nInstruction-tuning with LoRA on the Alpaca dataset.\n\nNote: If you run into a CUDA error \"Expected is_sm80 to be true, but got false\", uncomment the line\n`torch.backends.cuda.enable_flash_sdp(False)` in the script below (see https://github.com/Lightning-AI/lit-llama/issues/101).\n\"\"\"\nimport sys\nfrom pathlib import Path\nimport os\nimport time", "import os\nimport time\n\nimport lightning as L\nimport numpy as np\nimport torch\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))", "wd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama.lora import mark_only_lora_as_trainable, lora, lora_state_dict\nfrom lit_llama.model import LLaMA, LLaMAConfig\nfrom lit_llama.tokenizer import Tokenizer\nfrom scripts.prepare_alpaca import generate_prompt\n\n", "\n\ninstruction_tuning = True\neval_interval = 100\nsave_interval = 100\neval_iters = 100\nlog_interval = 1\n\n# Hyperparameters\nlearning_rate = 3e-4", "# Hyperparameters\nlearning_rate = 3e-4\nbatch_size = 128\nmicro_batch_size = 4\ngradient_accumulation_iters = batch_size // micro_batch_size\nassert gradient_accumulation_iters > 0\nmax_iters = 50000 * 3 // micro_batch_size\nweight_decay = 0.0\nmax_seq_length = 256  # see scripts/prepare_alpaca.py\nlora_r = 8", "max_seq_length = 256  # see scripts/prepare_alpaca.py\nlora_r = 8\nlora_alpha = 16\nlora_dropout = 0.05\nwarmup_iters = 100\n\n\ndef main(\n    data_dir: str = \"data/alpaca\", \n    pretrained_path: str = \"checkpoints/lit-llama/7B/lit-llama.pth\",\n    tokenizer_path: str = \"checkpoints/lit-llama/tokenizer.model\",\n    out_dir: str = \"out/lora/alpaca\",\n):\n\n    fabric = L.Fabric(accelerator=\"cuda\", devices=1, precision=\"bf16-true\")\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets(data_dir=data_dir)\n\n    config = LLaMAConfig.from_name(\"7B\")\n    config.block_size = max_seq_length\n\n    checkpoint = torch.load(pretrained_path)\n\n    with fabric.init_module(), lora(r=lora_r, alpha=lora_alpha, dropout=lora_dropout, enabled=True):\n        model = LLaMA(config)\n        # strict=False because missing keys due to LoRA weights not contained in checkpoint state\n        model.load_state_dict(checkpoint, strict=False)\n    \n    mark_only_lora_as_trainable(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    model, optimizer = fabric.setup(model, optimizer)\n    train(fabric, model, optimizer, train_data, val_data, tokenizer_path, out_dir)\n\n    # Save the final LoRA checkpoint at the end of training\n    checkpoint = lora_state_dict(model)\n    fabric.save(os.path.join(out_dir, \"lit-llama-lora-finetuned.pth\"), checkpoint)", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_data: np.ndarray,\n    val_data: np.ndarray,\n    tokenizer_path: str,\n    out_dir: str,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n    step_count = 0\n\n    for iter_num in range(max_iters):\n\n        if step_count <= warmup_iters:\n            # linear warmup\n            lr = learning_rate * step_count / warmup_iters\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        t0 = time.time()\n\n        input_ids, targets = get_batch(fabric, train_data)\n        with fabric.no_backward_sync(model, enabled=((iter_num + 1) % gradient_accumulation_iters != 0)):\n            logits = model(input_ids)\n            loss = loss_fn(logits, targets)\n            fabric.backward(loss / gradient_accumulation_iters)\n\n        if (iter_num + 1) % gradient_accumulation_iters == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            step_count += 1\n                \n            if step_count % eval_interval == 0:\n                val_loss = validate(fabric, model, val_data, tokenizer_path)\n                fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n                fabric.barrier()\n\n            if step_count % save_interval == 0:\n                print(f\"Saving LoRA weights to {out_dir}\")\n                # We are only saving the LoRA weights\n                # TODO: Provide a function/script to merge the LoRA weights with pretrained weights\n                checkpoint = lora_state_dict(model)\n                fabric.save(os.path.join(out_dir, f\"iter-{iter_num:06d}-ckpt.pth\"), checkpoint)\n\n        dt = time.time() - t0\n        if iter_num % log_interval == 0:\n            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")", "\n\ndef generate_response(model, instruction, tokenizer_path):\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": instruction, \"input\": \"\"}\n    prompt = instruction\n    if instruction_tuning:\n        prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    output = generate(\n        model,\n        idx=encoded,\n        max_seq_length=max_seq_length,\n        max_new_tokens=100,\n    )\n    output = tokenizer.decode(output)\n    return output # output.split(\"### Response:\")[1].strip()", "\n\n@torch.no_grad()\ndef validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray, tokenizer_path: str) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        input_ids, targets = get_batch(fabric, val_data)\n        logits = model(input_ids)\n        loss = loss_fn(logits, targets)\n        losses[k] = loss.item()\n    out = losses.mean()\n\n    # produce an example:\n    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n    \n    output = generate_response(model, instruction, tokenizer_path)\n    fabric.print(instruction)\n    fabric.print(output)\n\n    model.train()\n    return out.item()", "\ndef loss_fn(logits, targets):\n    # shift the targets such that output n predicts token n+1\n    logits = logits[..., :-1, :].contiguous()\n    targets = targets[..., 1:].contiguous()\n    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n    return loss\n    \n\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\ndef get_batch(fabric: L.Fabric, data: list):\n    ix = torch.randint(len(data), (micro_batch_size,))\n\n    input_ids = [data[i][\"input_ids\"].type(torch.int64) for i in ix]\n    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n\n    max_len = max(len(s) for s in input_ids)\n\n    def pad_right(x, pad_id):\n        # pad right based on the longest sequence\n        n = max_len - len(x)\n        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n\n    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y", "\n\ndef load_datasets(data_dir):\n    train_data = torch.load(os.path.join(data_dir, \"train.pt\"))\n    val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n    return train_data, val_data\n\n\nif __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n    \n    from jsonargparse.cli import CLI\n\n    CLI(main)", "if __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n    \n    from jsonargparse.cli import CLI\n\n    CLI(main)\n", ""]}
{"filename": "pretrain/redpajama.py", "chunked_list": ["import os\nimport sys\nimport math\nimport glob\nimport time\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Tuple, Optional\n\nimport lightning as L", "\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n\nimport numpy as np\n", "import numpy as np\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama.model import Block, LLaMA, LLaMAConfig\nfrom lit_llama.packed_dataset import PackedDataset, CombinedDataset\nfrom lit_llama.utils import save_model_checkpoint\n", "from lit_llama.utils import save_model_checkpoint\n\n\nout_dir = \"out/training\"\nsave_interval = 1000\neval_interval = 1000\neval_iters = 100\nlog_interval = 1\n\n# compile = False", "\n# compile = False\n\n# Hyperparameters\nlearning_rate = 6e-4\nbatch_size = 125\nmicro_batch_size = 5\nmax_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\nweight_decay = 1e-1\nbeta1 = 0.9", "weight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nwarmup_iters = 2000\nlr_decay_iters = max_iters\nmin_lr = 6e-5\n\n", "\n\n# Data proportions from https://arxiv.org/pdf/2302.13971.pdf Table 1\ndata_config = [\n    (\"arxiv\", 2.5),\n    (\"book\", 4.5),\n    (\"c4\", 15.0),\n    (\"cc\", 67.0),\n    (\"github\", 4.5),\n    (\"stackexchange\", 2.0),", "    (\"github\", 4.5),\n    (\"stackexchange\", 2.0),\n    (\"wikipedia\", 4.5),\n]\n\n\ndef main(\n    devices: int = 4,\n    train_data_dir: Path = \"data/lit-redpajama\",\n    val_data_dir: Optional[Path] = None,\n) -> None:\n    auto_wrap_policy = partial(\n        transformer_auto_wrap_policy, transformer_layer_cls={Block}\n    )\n    strategy = FSDPStrategy(\n        auto_wrap_policy=auto_wrap_policy, activation_checkpointing=Block, limit_all_gathers=True\n    )\n\n    fabric = L.Fabric(\n        accelerator=\"cuda\", devices=devices, precision=\"bf16-mixed\", strategy=strategy\n    )\n    fabric.launch()\n    fabric.seed_everything(1337)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    config = LLaMAConfig.from_name(\"7B\")\n\n    train_dataloader, val_dataloader = create_dataloaders(\n        batch_size=micro_batch_size,\n        block_size=config.block_size,\n        fabric=fabric,\n        train_data_dir=train_data_dir,\n        val_data_dir=val_data_dir,\n        seed=1338,\n    )\n    if val_dataloader is None:\n        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n    else:\n        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n\n    with fabric.device:\n        torch.set_default_dtype(torch.bfloat16)\n        model = LLaMA(config)\n        model.apply(model._init_weights)\n        torch.set_default_dtype(torch.float32)\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=weight_decay,\n        betas=(beta1, beta2),\n        foreach=False,\n    )\n\n    model, optimizer = fabric.setup(model, optimizer)\n\n    process_batch_size = batch_size // devices\n    gradient_accumulation_iters = process_batch_size // micro_batch_size\n\n    train(fabric, model, optimizer, train_dataloader, val_dataloader, gradient_accumulation_iters, devices)", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_dataloader: DataLoader,\n    val_dataloader: Optional[DataLoader],\n    grad_accum_steps: int,\n    devices: int,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n\n    step_count = 0\n\n    step_time = 0.0\n    tokens = 0\n    tokens_sec = 0.0\n    prev_t1 = time.time()\n\n    for iter_num, train_data in enumerate(train_dataloader):\n        t0 = time.time()\n\n        # determine and set the learning rate for this iteration\n        lr = get_lr(iter_num) if decay_lr else learning_rate\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n\n        input_ids = train_data[:, 0 : model.config.block_size].contiguous()\n        targets = train_data[:, 1 : model.config.block_size + 1].contiguous()\n        \n        is_accumulating = (iter_num + 1) % grad_accum_steps != 0\n\n        with fabric.no_backward_sync(model, enabled=is_accumulating):\n            logits = model(input_ids)\n            loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n            )\n            fabric.backward(loss / grad_accum_steps)\n\n        t1 = time.time()\n\n        if not is_accumulating:\n            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            step_count += 1\n\n            t1 = time.time()\n\n            if val_dataloader is not None and step_count % eval_interval == 0:\n                val_loss = validate(fabric, model, val_dataloader)\n                fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n                fabric.barrier()\n                fabric.log_dict(\n                    {\"iter\": iter_num, \"val_loss\": val_loss, \"step\": step_count, \"lr\": lr}\n                )\n\n            if step_count % save_interval == 0:\n                fabric.print(f\"Saving checkpoint to {out_dir}\")\n                save_model_checkpoint(\n                    fabric, model, os.path.join(out_dir, f\"iter-{iter_num:06d}-ckpt.pth\")\n                )\n\n        dt = t1 - t0\n\n        tokens += micro_batch_size * model.config.block_size\n        step_time += t1 - prev_t1\n        prev_t1 = t1\n\n        if iter_num % log_interval == 0:\n            tokens_sec_str = f\"{tokens / step_time:.0f}\" if not is_accumulating else \"-\"\n\n            fabric.log_dict(\n                {\"iter\": iter_num, \"train_loss\": loss, \"step\": step_count, \"lr\": lr}\n            )\n            fabric.print(\n                    f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms, speed: {tokens_sec_str} toks/s/device\"\n            )\n\n        if not is_accumulating:\n            tokens = 0\n            step_time = 0.0\n\n        if iter_num > max_iters:\n            break", "\n\n@torch.no_grad()\ndef validate(\n    fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader\n) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k, val_data in enumerate(val_dataloader):\n        input_ids = val_data[:, 0 : model.config.block_size].contiguous()\n        targets = val_data[:, 1 : model.config.block_size + 1].contiguous()\n        logits = model(input_ids)\n        loss = torch.nn.functional.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n        )\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out", "\n\ndef create_dataloader(\n    batch_size: int,\n    block_size: int,\n    data_dir: str,\n    fabric,\n    shuffle: bool = True,\n    seed: int = 12345,\n) -> DataLoader:\n    datasets = []\n    for prefix, _ in data_config:\n        filenames = glob.glob(os.path.join(data_dir, prefix + \"*\"))\n        dataset = PackedDataset(\n            filenames, n_chunks=4, block_size=block_size, shuffle=shuffle, seed=seed,\n            num_processes=fabric.world_size, process_rank=fabric.global_rank,\n        )\n        datasets.append(dataset)\n\n    if not datasets:\n        raise RuntimeError(\n            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n        )\n\n    weights = [weight for _, weight in data_config]\n    sum_weights = sum(weights)\n    weights = [el / sum_weights for el in weights]\n\n    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n\n    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)", "\n\ndef create_dataloaders(\n    batch_size: int,\n    block_size: int,\n    fabric,\n    train_data_dir: str = \"data/lit-redpajama\",\n    val_data_dir: Optional[str] = None,\n    seed: int = 12345,\n) -> Tuple[DataLoader, DataLoader]:\n    # Increase by one because we need the next word as well\n    effective_block_size = block_size + 1\n    train_dataloader = create_dataloader(\n        batch_size=batch_size,\n        block_size=effective_block_size,\n        fabric=fabric,\n        data_dir=train_data_dir,\n        shuffle=True,\n        seed=seed,\n    )\n    val_dataloader = (\n        create_dataloader(\n            batch_size=batch_size,\n            block_size=effective_block_size,\n            fabric=fabric,\n            data_dir=val_data_dir,\n            shuffle=False,\n            seed=seed,\n        )\n        if val_data_dir\n        else None\n    )\n    return train_dataloader, val_dataloader", "\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)", "\n\nif __name__ == \"__main__\":\n    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n    # torch.backends.cuda.enable_flash_sdp(False)\n    torch.set_float32_matmul_precision(\"high\")\n\n    from jsonargparse.cli import CLI\n\n    CLI(main)", ""]}
{"filename": "pretrain/shakespeare.py", "chunked_list": ["\"\"\"\nThis script is a placeholder for training LLaMA from scratch.\nCurrently, it just trains on the Shakespeare dataset.\n\"\"\"\nfrom pathlib import Path\nimport sys\nimport os\nimport time\nfrom functools import partial\nfrom typing import Tuple", "from functools import partial\nfrom typing import Tuple\n\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\n\nimport torch\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n\nimport numpy as np", "\nimport numpy as np\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama.model import Block, LLaMA, LLaMAConfig\nfrom lit_llama.utils import save_model_checkpoint\n", "from lit_llama.utils import save_model_checkpoint\n\n\nout_dir = \"out/training\"\neval_interval = 2000\neval_iters = 200\nlog_interval = 1\n# compilation fails as it does not support torch.complex64 for RoPE\n# compile = False\n", "# compile = False\n\n# Hyperparameters\nlearning_rate = 6e-4\nbatch_size = 2\nmax_iters = 600000\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0", "beta2 = 0.95\ngrad_clip = 1.0\n\n# For shakespeare, choose smaller block size than vanilla LLaMA\nblock_size = 1024\n\n\ndef main() -> None:\n    auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={Block})\n    strategy = FSDPStrategy(auto_wrap_policy=auto_wrap_policy, activation_checkpointing=Block, limit_all_gathers=True)\n\n    fabric = L.Fabric(accelerator=\"cuda\", devices=4, precision=\"bf16-mixed\", strategy=strategy)\n    fabric.launch()\n    fabric.seed_everything(1337 + fabric.global_rank)\n\n    if fabric.global_rank == 0:\n        os.makedirs(out_dir, exist_ok=True)\n\n    train_data, val_data = load_datasets()\n\n    config = LLaMAConfig.from_name(\"7B\")\n    config.block_size = block_size\n    config.vocab_size = 100  # from prepare_shakespeare.py\n\n    with fabric.device:\n        model = LLaMA(config)\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    model = fabric.setup_module(model)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False)\n    optimizer = fabric.setup_optimizers(optimizer)\n\n    train(fabric, model, optimizer, train_data, val_data)", "\n\ndef train(\n    fabric: L.Fabric,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    train_data: np.ndarray,\n    val_data: np.ndarray,\n) -> None:\n    \"\"\"The training loop.\n\n    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n    \"\"\"\n\n    iter_num = 0\n\n    while True:\n        # TODO: add learning rate scheduling\n\n        # evaluate the loss on train/val sets and write checkpoints\n        if iter_num > 0 and iter_num % eval_interval == 0:\n            val_loss = validate(fabric, model, val_data)\n            fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n            fabric.print(f\"Saving checkpoint to {out_dir}\")\n            save_model_checkpoint(fabric, model, os.path.join(out_dir, f\"iter-{iter_num:06d}-ckpt.pth\"))\n\n        t0 = time.time()\n\n        input_ids, targets = get_batch(\n            fabric,\n            train_data,\n            block_size=model.config.block_size,  # type: ignore[union-attr,arg-type]\n        )\n        logits = model(input_ids)\n        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\n        fabric.backward(loss)\n\n        # TODO: Gradient clipping\n        # if grad_clip != 0.0:\n        #     fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        dt = time.time() - t0\n        if iter_num % log_interval == 0:\n            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")\n        iter_num += 1\n\n        if iter_num > max_iters:\n            break", "\n\n@torch.no_grad()\ndef validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n    fabric.print(\"Validating ...\")\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        input_ids, targets = get_batch(\n            fabric,\n            val_data,\n            block_size=model.config.block_size,  # type: ignore[union-attr,arg-type]\n        )\n        logits = model(input_ids)\n        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out", "\n\ndef get_batch(fabric: L.Fabric, data: np.ndarray, block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64)) for i in ix])\n    x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n    return x, y\n\n\ndef load_datasets(data_dir: str = \"data/shakespeare\") -> Tuple[np.ndarray, np.ndarray]:\n    train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n    val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n    return train_data, val_data", "\n\ndef load_datasets(data_dir: str = \"data/shakespeare\") -> Tuple[np.ndarray, np.ndarray]:\n    train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n    val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n    return train_data, val_data\n\n\nif __name__ == \"__main__\":\n    torch.set_float32_matmul_precision(\"high\")\n    main()", "if __name__ == \"__main__\":\n    torch.set_float32_matmul_precision(\"high\")\n    main()\n"]}
{"filename": "scripts/prepare_redpajama.py", "chunked_list": ["import json\nimport glob\nimport os\nfrom pathlib import Path\nimport sys\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n", "sys.path.append(str(wd))\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lit_llama import Tokenizer\nimport lit_llama.packed_dataset as packed_dataset\n\n\nfilenames_sample = [", "\nfilenames_sample = [\n    \"arxiv_sample.jsonl\",\n    \"book_sample.jsonl\",\n    \"c4_sample.jsonl\",\n    \"cc_2019-30_sample.jsonl\",\n    \"cc_2020-05_sample.jsonl\",\n    \"cc_2021-04_sample.jsonl\",\n    \"cc_2022-05_sample.jsonl\",\n    \"cc_2023-06_sample.jsonl\",", "    \"cc_2022-05_sample.jsonl\",\n    \"cc_2023-06_sample.jsonl\",\n    \"github_sample.jsonl\",\n    \"stackexchange_sample.jsonl\",\n    \"wikipedia_sample.jsonl\",\n]\n\nfilename_sets = {\n    \"arxiv\": \"arxiv/arxiv*\",\n    \"book\": \"book/book*\",", "    \"arxiv\": \"arxiv/arxiv*\",\n    \"book\": \"book/book*\",\n    \"c4\": \"c4/c4-train*\",\n    \"common_crawl\": \"common_crawl/*\",\n    \"github\": \"github/filtered*\",\n    \"stackexchange\": \"stackexchange/stackexchange*\",\n    \"wikipedia\": \"wikipedia/wiki*\",\n}\n\n\ndef prepare_sample(\n    source_path: Path,\n    tokenizer_path: Path,\n    destination_path: Path,\n    chunk_size: int,\n    match = \"\"\n) -> None:\n    \"\"\"Prepare the \"Red Pajama\" dataset. We assume tokenizer has been trained (i.e. we reuse LLaMA's tokenizer model).\"\"\"\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for name in filenames_sample:\n        if match and match not in name:\n            continue\n\n        filepath = source_path / name\n\n        if not filepath.is_file():\n            raise RuntimeError(\n                f\"Input file not found at {filepath}. \\n\"\n                \"Make sure you download the data, e.g. wget -i https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt or through \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \\n\"\n            )\n\n        prefix, _ = os.path.splitext(name)\n\n        builder = packed_dataset.PackedDatasetBuilder(\n            outdir=destination_path,\n            prefix=prefix,\n            chunk_size=chunk_size,\n            sep_token=tokenizer.bos_id,\n            dtype=\"auto\",\n            vocab_size=tokenizer.vocab_size,\n        )\n\n        print(f\"Processing {name}\")\n\n        with open(filepath, encoding=\"utf-8\") as f:\n            for row in tqdm(f):\n                text = json.loads(row)[\"text\"]\n                text_ids = tokenizer.encode(text)\n                builder.add_array(np.array(text_ids, dtype=builder.dtype))\n\n        builder.write_reminder()", "\n\ndef prepare_sample(\n    source_path: Path,\n    tokenizer_path: Path,\n    destination_path: Path,\n    chunk_size: int,\n    match = \"\"\n) -> None:\n    \"\"\"Prepare the \"Red Pajama\" dataset. We assume tokenizer has been trained (i.e. we reuse LLaMA's tokenizer model).\"\"\"\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for name in filenames_sample:\n        if match and match not in name:\n            continue\n\n        filepath = source_path / name\n\n        if not filepath.is_file():\n            raise RuntimeError(\n                f\"Input file not found at {filepath}. \\n\"\n                \"Make sure you download the data, e.g. wget -i https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt or through \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \\n\"\n            )\n\n        prefix, _ = os.path.splitext(name)\n\n        builder = packed_dataset.PackedDatasetBuilder(\n            outdir=destination_path,\n            prefix=prefix,\n            chunk_size=chunk_size,\n            sep_token=tokenizer.bos_id,\n            dtype=\"auto\",\n            vocab_size=tokenizer.vocab_size,\n        )\n\n        print(f\"Processing {name}\")\n\n        with open(filepath, encoding=\"utf-8\") as f:\n            for row in tqdm(f):\n                text = json.loads(row)[\"text\"]\n                text_ids = tokenizer.encode(text)\n                builder.add_array(np.array(text_ids, dtype=builder.dtype))\n\n        builder.write_reminder()", "\n\ndef prepare_full(\n    source_path: Path,\n    tokenizer_path: Path,\n    destination_path: Path,\n    chunk_size: int,\n    match: str = \"\"\n) -> None:\n    \"\"\"Prepare the \"Red Pajama\" dataset. We assume tokenizer has been trained (i.e. we reuse LLaMA's tokenizer model).\"\"\"\n    import zstandard as zstd\n\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for set_name, pattern in filename_sets.items():\n        if match and match not in set_name:\n            continue\n\n        is_cc = set_name == \"common_crawl\"\n\n        filenames = glob.glob(os.path.join(source_path, pattern), recursive=True)\n\n        if not filenames:\n            raise RuntimeError(\n                f\"No files matching {pattern} found at {source_path}. \\n\"\n                \"Make sure you download the data, e.g. wget -i https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt or through \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T \\n\"\n                \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \\n\"\n            )\n\n        builder = packed_dataset.PackedDatasetBuilder(\n            outdir=destination_path,\n            prefix=set_name,\n            chunk_size=chunk_size,\n            sep_token=tokenizer.bos_id,\n            dtype=\"auto\",\n            vocab_size=tokenizer.vocab_size,\n        )\n\n        for name in filenames:\n            filepath = source_path / name\n\n            print(f\"Processing {name}\")\n\n            if is_cc:\n                with zstd.open(open(filepath, \"rb\"), \"rt\", encoding=\"utf-8\") as f:\n                    for row in tqdm(f):\n                        text = json.loads(row)[\"text\"]\n                        text_ids = tokenizer.encode(text)\n                        builder.add_array(np.array(text_ids, dtype=builder.dtype))\n            else:\n                with open(filepath, encoding=\"utf-8\") as f:\n                    for row in tqdm(f):\n                        text = json.loads(row)[\"text\"]\n                        text_ids = tokenizer.encode(text)\n                        builder.add_array(np.array(text_ids, dtype=builder.dtype))\n\n        builder.write_reminder()", "\n\ndef prepare(\n    source_path: Path = Path(\"data/RedPajama-Data-1T-Sample\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    destination_path: Path = Path(\"data/red_pajama_sample\"),\n    chunk_size: int = 2049 * 1024,  # 2048 block size + 1 for causal (from LLama), 1024 blocks\n    sample: bool = False,\n    match: str = \"\",\n) -> None:\n    \"\"\"Prepare the \"Red Pajama\" dataset. We assume tokenizer has been trained (i.e. we reuse LLaMA's tokenizer model).\"\"\"\n    if sample:\n        prepare_sample(\n            source_path=source_path,\n            tokenizer_path=tokenizer_path,\n            destination_path=destination_path,\n            chunk_size=chunk_size,\n            match=match,\n        )\n    else:\n        prepare_full(\n            source_path=source_path,\n            tokenizer_path=tokenizer_path,\n            destination_path=destination_path,\n            chunk_size=chunk_size,\n            match=match,\n        )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)\n"]}
{"filename": "scripts/convert_hf_checkpoint.py", "chunked_list": ["import collections\nimport contextlib\nimport gc\nimport json\nimport shutil\nimport sys\nfrom pathlib import Path\n\nimport torch\n", "import torch\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama.model import LLaMA, LLaMAConfig\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, incremental_save\n\n", "\n\n@torch.no_grad()\ndef convert_hf_checkpoint(\n    *,\n    output_dir: Path = Path(\"checkpoints/lit-llama/7B\"),\n    checkpoint_dir: Path = Path(\"checkpoints/hf-llama/7B\"),\n    model_size: str = \"7B\",\n    dtype: str = \"float32\",\n    verify: bool = False,\n) -> None:\n    \"\"\"\n    Perform the reverse operation of: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # the tokenizer is the same for all model sizes, so we store it in the parent dir\n    shutil.copy(checkpoint_dir / \"tokenizer.model\", output_dir.parent)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    print(\"Initializing lit-llama\")\n    config = LLaMAConfig.from_name(model_size)\n\n    with EmptyInitOnDevice(device=\"meta\", dtype=dtype):\n        model = LLaMA(config)\n\n    qkv_size = model.transformer.h[0].attn.c_attn.weight.shape[0] // 3\n\n    # initialize a new empty state dict to hold our new weights\n    sd_meta = model.state_dict()\n    sd = {}\n\n    # Load the json file containing weight mapping\n    pytorch_bin_map_json_path = checkpoint_dir / \"pytorch_model.bin.index.json\"\n    with open(pytorch_bin_map_json_path) as json_map:\n        bin_index = json.load(json_map)\n    bin_files = set(checkpoint_dir / bin for bin in bin_index[\"weight_map\"].values())\n    if not bin_files:\n        raise ValueError(f\"Expected {str(checkpoint_dir)!r} to contain .bin files\")\n\n    def permute(w):\n        dim = config.n_embd\n        w = w._load_tensor().to(dtype)\n        return (\n            w.view(config.n_head, 2, dim // config.n_head // 2, dim)\n            .transpose(1, 2)\n            .reshape(dim, dim)\n        )\n\n    weight_map = {\n        \"self_attn.o_proj.weight\": \"attn.c_proj.weight\",\n        \"self_attn.q_proj.weight\": \"attn.c_attn.weight\",\n        \"self_attn.k_proj.weight\": \"attn.c_attn.weight\",\n        \"self_attn.v_proj.weight\": \"attn.c_attn.weight\",\n        \"mlp.gate_proj.weight\": \"mlp.c_fc1.weight\",\n        \"mlp.up_proj.weight\": \"mlp.c_fc2.weight\",\n        \"mlp.down_proj.weight\": \"mlp.c_proj.weight\",\n        \"input_layernorm.weight\": \"rms_1.scale\",\n        \"post_attention_layernorm.weight\": \"rms_2.scale\",\n        \"model.embed_tokens.weight\": \"transformer.wte.weight\",\n        \"model.norm.weight\": \"transformer.ln_f.scale\",\n        \"lm_head.weight\": \"lm_head.weight\",\n    }\n\n    print(f\"Saving to disk at {output_dir}\")\n    unprocessed_weights = collections.defaultdict(dict)\n\n    with incremental_save(output_dir / \"lit-llama.pth\") as saver:\n        # for checkpoints that split the QKV across several files, we need to keep all the bin files\n        # open, so we use `ExitStack` to close them all together at the end\n        with contextlib.ExitStack() as stack:\n            for bin_file in bin_files:\n                print(\"Processing\", bin_file)\n                hf_weights = stack.enter_context(lazy_load(bin_file))\n                for name, param in hf_weights.items():\n                    skip = False\n                    if \"rotary_emb.inv_freq\" in name:\n                        continue\n                    if \"model.layers\" in name:\n                        block_id = int(name.split(\".\")[2])\n                        from_name = \".\".join(name.split(\".\")[3:])\n                        to_name = weight_map[from_name]\n                        sd_key = f\"transformer.h.{block_id}.{to_name}\"\n\n                        if \"q_proj\" in name:\n                            unprocessed_weights[sd_key][\"q_proj\"] = param\n                            skip = True\n                        elif \"k_proj\" in name:\n                            unprocessed_weights[sd_key][\"k_proj\"] = param\n                            skip = True\n                        elif \"v_proj\" in name:\n                            unprocessed_weights[sd_key][\"v_proj\"] = param\n                            skip = True\n                        if skip and len(unprocessed_weights[sd_key]) == 3:\n                            w = torch.empty(\n                                sd_meta[sd_key].shape, dtype=sd_meta[sd_key].dtype\n                            )\n                            w[:qkv_size] = permute(unprocessed_weights[sd_key][\"q_proj\"])\n                            w[qkv_size:-qkv_size] = permute(\n                                unprocessed_weights[sd_key][\"k_proj\"]\n                            )\n                            w[-qkv_size:] = (\n                                unprocessed_weights[sd_key][\"v_proj\"]\n                                ._load_tensor()\n                                .to(dtype)\n                            )\n                            sd[sd_key] = w\n                            del unprocessed_weights[sd_key]\n                            skip = False\n                        else:\n                            sd[sd_key] = param._load_tensor().to(dtype)\n                    else:\n                        sd_key = weight_map[name]\n                        sd[sd_key] = param._load_tensor().to(dtype)\n                    if not skip:\n                        sd[sd_key] = saver.store_early(sd[sd_key])\n                gc.collect()\n        saver.save(sd)\n\n    assert len(unprocessed_weights) == 0, f\"unexpected partial weights {list(unprocessed_weights)}\"\n    if verify:\n        try:\n            from transformers import LlamaForCausalLM\n        except ImportError:\n            raise ImportError(\"verify=True requires transformers to be installed, please `pip install transformers`\")\n        print(\"Verifying...\")\n\n        token_sample = torch.randint(0, config.vocab_size, size=(1, config.block_size), dtype=torch.int64)\n        out = model(token_sample)\n        del model\n        gc.collect()\n\n        print(\"Loading original model for comparison\")\n        model_hf = LlamaForCausalLM.from_pretrained(checkpoint_dir)\n        out_hf = model_hf(token_sample)[\"logits\"]\n\n        print(\"Comparing outputs\")\n        assert out.device.type == out_hf.device.type\n        assert out.dtype == out_hf.dtype\n        assert torch.testing.assert_close(out, out_hf)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(convert_hf_checkpoint)\n\n"]}
{"filename": "scripts/convert_checkpoint.py", "chunked_list": ["import gc\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict\n\nimport torch\nfrom tqdm import tqdm\n\n\"\"\"\nSample usage:", "\"\"\"\nSample usage:\n\n```bash\npython -m scripts.convert_checkpoint -h\n\npython -m scripts.convert_checkpoint converted\n```\n\"\"\"\n", "\"\"\"\n\n\ndef convert_state_dict(state_dict: Dict[str, torch.Tensor], dtype: torch.dtype = torch.float32) -> Dict[str, torch.Tensor]:\n    converted = {}\n    converted[\"transformer.wte.weight\"] = state_dict[\"tok_embeddings.weight\"].to(dtype)\n    converted[\"lm_head.weight\"] = state_dict[\"output.weight\"].to(dtype)\n    converted[\"transformer.ln_f.scale\"] = state_dict[\"norm.weight\"].to(dtype)\n\n    for layer_idx in sorted(set([k.split(\".\")[1] for k in state_dict if k.startswith(\"layers\")])):\n        # attention\n        # the wq, wk, wv from the FB model are stacked in our model as c_attn\n        converted[f\"transformer.h.{layer_idx}.attn.c_attn.weight\"] = torch.cat(\n            (\n                state_dict[f\"layers.{layer_idx}.attention.wq.weight\"].to(dtype),\n                state_dict[f\"layers.{layer_idx}.attention.wk.weight\"].to(dtype),\n                state_dict[f\"layers.{layer_idx}.attention.wv.weight\"].to(dtype),\n            )\n        )\n        converted[f\"transformer.h.{layer_idx}.attn.c_proj.weight\"] = state_dict[\n            f\"layers.{layer_idx}.attention.wo.weight\"\n        ].to(dtype)\n        # mlp\n        converted[f\"transformer.h.{layer_idx}.mlp.c_fc1.weight\"] = state_dict[\n            f\"layers.{layer_idx}.feed_forward.w1.weight\"\n        ].to(dtype)\n        converted[f\"transformer.h.{layer_idx}.mlp.c_proj.weight\"] = state_dict[\n            f\"layers.{layer_idx}.feed_forward.w2.weight\"\n        ].to(dtype)\n        converted[f\"transformer.h.{layer_idx}.mlp.c_fc2.weight\"] = state_dict[\n            f\"layers.{layer_idx}.feed_forward.w3.weight\"\n        ].to(dtype)\n        # rms norm\n        converted[f\"transformer.h.{layer_idx}.rms_1.scale\"] = state_dict[f\"layers.{layer_idx}.attention_norm.weight\"].to(dtype)\n        converted[f\"transformer.h.{layer_idx}.rms_2.scale\"] = state_dict[f\"layers.{layer_idx}.ffn_norm.weight\"].to(dtype)\n    return converted", "\n\nshard_dims = {\n    \"lm_head.weight\": 0,\n    \"wte.weight\": 1,\n    \"attn.c_attn.weight\": 0,\n    \"attn.c_proj.weight\": 1,\n    \"mlp.c_fc1.weight\": 0,\n    \"mlp.c_fc2.weight\": 0,\n    \"mlp.c_proj.weight\": 1", "    \"mlp.c_fc2.weight\": 0,\n    \"mlp.c_proj.weight\": 1\n}\n\n\ndef meta_weights_for_nano_model(\n    *,\n    output_dir: Path = Path(\"checkpoints/lit-llama\"),\n    checkpoint_dir: Path = Path(\"checkpoints/llama/\"),\n    model_size: str = \"7B\",\n    dtype: str = \"float32\",\n) -> None:\n    output_dir = output_dir / model_size\n    checkpoint_dir = checkpoint_dir / model_size\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # the tokenizer is the same for all model sizes, so we store it in the parent dir\n    shutil.copy(checkpoint_dir.parent / \"tokenizer.model\", output_dir.parent)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    checkpoint_files = sorted(checkpoint_dir.glob(\"*.pth\"))\n    checkpoint_files.sort()\n    n_checkpoints = len(checkpoint_files)\n\n    if n_checkpoints == 0:\n        raise RuntimeError(f\"No checkpoints were found at checkpoint_dir {checkpoint_dir}. `consolidated.0*.pth` files expected at that location.\")\n\n    # for the bigger models, there are multiple model-parallel checkpoints\n    # and we combine them into one single file\n    combined = None\n    for file in tqdm(checkpoint_files, total=n_checkpoints):\n        checkpoint = torch.load(file, map_location=\"cpu\")\n        converted = convert_state_dict(checkpoint, dtype=dtype)\n        if combined is None:\n            combined = converted\n            continue\n        for name, param in converted.items():\n            dim = None\n            for k, d in shard_dims.items():\n                if k in name:\n                    dim = d\n                    break\n            if dim is None:\n                # Extra check: assert that tensors are the same if not sharded\n                # assert torch.allclose(combined[name], param)\n                continue\n            combined[name] = torch.cat((combined[name], param), dim=dim)\n\n        del checkpoint\n        del converted\n        gc.collect()\n\n    for name, param in combined.items():\n        if \"c_attn\" not in name:\n            continue\n\n        # Turn [Q1, K1, V1, Q2, K2, V2, ...] into [Q1, Q2, ..., K1, K2, .., V1, V2, ...]\n\n        src_chunk_len = param.shape[0] // n_checkpoints\n        mat_len = src_chunk_len // 3\n        dst_chunk_len = mat_len * n_checkpoints\n        attn = torch.clone(param)\n        for i in range(n_checkpoints):\n            for j in range(3):\n                param[j * dst_chunk_len + i * mat_len: j * dst_chunk_len + (i+1) * mat_len] = \\\n                    attn[i * src_chunk_len + j * mat_len: i * src_chunk_len + (j+1) * mat_len]\n\n        del attn\n        gc.collect()\n\n    torch.save(combined, output_dir / \"lit-llama.pth\")", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(meta_weights_for_nano_model)\n"]}
{"filename": "scripts/prepare_shakespeare.py", "chunked_list": ["# MIT License\n\n# Copyright (c) 2022 Andrej Karpathy\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport sys\nfrom pathlib import Path\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()", "# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nimport numpy as np\nimport requests\n\n\ndef prepare(destination_path: Path = Path(\"data/shakespeare\")) -> None:\n    \"\"\"Prepare the \"Tiny Shakespeare\" dataset.\"\"\"\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    # download the tiny shakespeare dataset\n    input_file_path = destination_path / \"input.txt\"\n    if not input_file_path.exists():\n        data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n        with open(input_file_path, \"w\") as f:\n            f.write(requests.get(data_url).text)\n\n    with open(input_file_path) as f:\n        data = f.read()\n    n = len(data)\n    train_data = data[: int(n * 0.9)]\n    val_data = data[int(n * 0.9) :]\n\n    from lit_llama import Tokenizer\n\n    Tokenizer.train(input=input_file_path, destination=destination_path, vocab_size=100)\n    tokenizer = Tokenizer(destination_path / \"tokenizer.model\")\n    train_ids = tokenizer.encode(train_data)\n    val_ids = tokenizer.encode(val_data)\n    print(f\"train has {len(train_ids):,} tokens\")\n    print(f\"val has {len(val_ids):,} tokens\")\n\n    # export to bin files\n    train_ids = np.array(train_ids, dtype=np.uint16)\n    val_ids = np.array(val_ids, dtype=np.uint16)\n    train_ids.tofile(destination_path / \"train.bin\")\n    val_ids.tofile(destination_path / \"val.bin\")", "def prepare(destination_path: Path = Path(\"data/shakespeare\")) -> None:\n    \"\"\"Prepare the \"Tiny Shakespeare\" dataset.\"\"\"\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    # download the tiny shakespeare dataset\n    input_file_path = destination_path / \"input.txt\"\n    if not input_file_path.exists():\n        data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n        with open(input_file_path, \"w\") as f:\n            f.write(requests.get(data_url).text)\n\n    with open(input_file_path) as f:\n        data = f.read()\n    n = len(data)\n    train_data = data[: int(n * 0.9)]\n    val_data = data[int(n * 0.9) :]\n\n    from lit_llama import Tokenizer\n\n    Tokenizer.train(input=input_file_path, destination=destination_path, vocab_size=100)\n    tokenizer = Tokenizer(destination_path / \"tokenizer.model\")\n    train_ids = tokenizer.encode(train_data)\n    val_ids = tokenizer.encode(val_data)\n    print(f\"train has {len(train_ids):,} tokens\")\n    print(f\"val has {len(val_ids):,} tokens\")\n\n    # export to bin files\n    train_ids = np.array(train_ids, dtype=np.uint16)\n    val_ids = np.array(val_ids, dtype=np.uint16)\n    train_ids.tofile(destination_path / \"train.bin\")\n    val_ids.tofile(destination_path / \"val.bin\")", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)\n"]}
{"filename": "scripts/prepare_alpaca.py", "chunked_list": ["\"\"\"Implementation derived from https://github.com/tloen/alpaca-lora\"\"\"\nimport sys\nfrom pathlib import Path\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nimport torch\nimport requests", "import torch\nimport requests\nimport json\nfrom torch.utils.data import random_split\nfrom lit_llama.tokenizer import Tokenizer\nfrom tqdm import tqdm\n\n\nDATA_FILE = \"https://raw.githubusercontent.com/tloen/alpaca-lora/main/alpaca_data_cleaned_archive.json\"\nDATA_FILE_NAME = \"alpaca_data_cleaned_archive.json\"", "DATA_FILE = \"https://raw.githubusercontent.com/tloen/alpaca-lora/main/alpaca_data_cleaned_archive.json\"\nDATA_FILE_NAME = \"alpaca_data_cleaned_archive.json\"\nIGNORE_INDEX = -1\n\n\ndef prepare(\n    destination_path: Path = Path(\"data/alpaca\"), \n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    test_split_size: int = 2000,\n    max_seq_length: int = 256,\n    seed: int = 42,\n    mask_inputs: bool = False,  # as in alpaca-lora\n    data_file_name: str = DATA_FILE_NAME\n) -> None:\n    \"\"\"Prepare the Alpaca dataset for instruction tuning.\n    \n    The output is a training and validation dataset saved as `train.pt` and `val.pt`,\n    which stores the preprocessed and tokenized prompts and labels.\n    \"\"\"\n    \n    destination_path.mkdir(parents=True, exist_ok=True)\n    file_path = destination_path / data_file_name\n    download(file_path)\n\n    # TODO: If we don't have the Meta weights, where do we get the tokenizer from?\n    tokenizer = Tokenizer(tokenizer_path)\n    \n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n\n    # Partition the dataset into train and test\n    train_split_size = len(data) - test_split_size\n    train_set, test_set = random_split(\n        data, \n        lengths=(train_split_size, test_split_size),\n        generator=torch.Generator().manual_seed(seed),\n    )\n    train_set, test_set = list(train_set), list(test_set)\n\n    print(f\"train has {len(train_set):,} samples\")\n    print(f\"val has {len(test_set):,} samples\")\n\n    print(\"Processing train split ...\")\n    train_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in tqdm(train_set)]\n    torch.save(train_set, file_path.parent / \"train.pt\")\n\n    print(\"Processing test split ...\")\n    test_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in tqdm(test_set)]\n    torch.save(test_set, file_path.parent / \"test.pt\")", "\n\ndef download(file_path: Path):\n    \"\"\"Downloads the raw json data file and saves it in the given destination.\"\"\"\n    if file_path.exists():\n        return\n    with open(file_path, \"w\") as f:\n        f.write(requests.get(DATA_FILE).text)\n\n\ndef prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n    \"\"\"Processes a single sample.\n    \n    Each sample in the dataset consists of:\n    - instruction: A string describing the task\n    - input: A string holding a special input value for the instruction.\n        This only applies to some samples, and in others this is empty.\n    - output: The response string\n\n    This function processes this data to produce a prompt text and a label for\n    supervised training. The input text is formed as a single message including all\n    the instruction, the input (optional) and the response.\n    The label/target is the same message but can optionally have the instruction + input text\n    masked out (mask_inputs=True).\n\n    Finally, both the prompt and the label get tokenized. If desired, all tokens\n    in the label that correspond to the original input prompt get masked out (default).\n    \"\"\"\n    full_prompt = generate_prompt(example)\n    full_prompt_and_response = full_prompt + example[\"output\"]\n    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n\n    # The labels are the full prompt with response, but with the prompt masked out\n    labels = encoded_full_prompt_and_response.clone()\n    if mask_inputs:\n        labels[:len(encoded_full_prompt)] = IGNORE_INDEX\n\n    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}", "\n\ndef prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n    \"\"\"Processes a single sample.\n    \n    Each sample in the dataset consists of:\n    - instruction: A string describing the task\n    - input: A string holding a special input value for the instruction.\n        This only applies to some samples, and in others this is empty.\n    - output: The response string\n\n    This function processes this data to produce a prompt text and a label for\n    supervised training. The input text is formed as a single message including all\n    the instruction, the input (optional) and the response.\n    The label/target is the same message but can optionally have the instruction + input text\n    masked out (mask_inputs=True).\n\n    Finally, both the prompt and the label get tokenized. If desired, all tokens\n    in the label that correspond to the original input prompt get masked out (default).\n    \"\"\"\n    full_prompt = generate_prompt(example)\n    full_prompt_and_response = full_prompt + example[\"output\"]\n    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n\n    # The labels are the full prompt with response, but with the prompt masked out\n    labels = encoded_full_prompt_and_response.clone()\n    if mask_inputs:\n        labels[:len(encoded_full_prompt)] = IGNORE_INDEX\n\n    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}", "\n\ndef tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n\n\ndef generate_prompt(example):\n    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n    'response' field.\"\"\"\n\n    if example[\"input\"]:\n        return (\n            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n        )\n    return (\n        \"Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)\n"]}
{"filename": "scripts/download.py", "chunked_list": ["import os\nfrom typing import Optional\nfrom urllib.request import urlretrieve\n\nfiles = {\n    \"original_model.py\": \"https://gist.githubusercontent.com/lantiga/fd36849fb1c498da949a0af635318a7b/raw/7dd20f51c2a1ff2886387f0e25c1750a485a08e1/llama_model.py\",\n    \"original_adapter.py\": \"https://gist.githubusercontent.com/awaelchli/546f33fcdb84cc9f1b661ca1ca18418d/raw/e81d8f35fb1fec53af1099349b0c455fc8c9fb01/original_adapter.py\",\n}\n\n\ndef download_original(wd: str) -> None:\n    for file, url in files.items():\n        filepath = os.path.join(wd, file)\n        if not os.path.isfile(filepath):\n            print(f\"Downloading original implementation to {filepath!r}\")\n            urlretrieve(url=url, filename=file)\n            print(\"Done\")\n        else:\n            print(\"Original implementation found. Skipping download.\")", "\n\ndef download_original(wd: str) -> None:\n    for file, url in files.items():\n        filepath = os.path.join(wd, file)\n        if not os.path.isfile(filepath):\n            print(f\"Downloading original implementation to {filepath!r}\")\n            urlretrieve(url=url, filename=file)\n            print(\"Done\")\n        else:\n            print(\"Original implementation found. Skipping download.\")", "\n\ndef download_from_hub(repo_id: Optional[str] = None, local_dir: str = \"checkpoints/hf-llama/7B\") -> None:\n    if repo_id is None:\n        raise ValueError(\"Please pass `--repo_id=...`. You can try googling 'huggingface hub llama' for options.\")\n\n    from huggingface_hub import snapshot_download\n\n    snapshot_download(repo_id, local_dir=local_dir, local_dir_use_symlinks=False)\n", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(download_from_hub)\n"]}
{"filename": "scripts/prepare_any_text.py", "chunked_list": ["\"\"\"Implementation derived from https://github.com/tloen/alpaca-lora\"\"\"\nimport sys\nfrom pathlib import Path\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nimport torch\nimport requests", "import torch\nimport requests\nimport json\nfrom torch.utils.data import random_split\nfrom lit_llama.tokenizer import Tokenizer\nfrom tqdm import tqdm\n\n\nIGNORE_INDEX = -1\n", "IGNORE_INDEX = -1\n\nDATA_FILE_NAME = \"input.txt\"\n\n\ndef prepare(\n    destination_path: Path = Path(\"data/any\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    test_split_ratio: float = 0.9,  # default 90% train, 10% validation\n    max_seq_length: int = 256,\n    seed: int = 42,\n    data_file_name: str = DATA_FILE_NAME,\n) -> None:\n    \"\"\"Prepare any dataset for finetuning (akin to Shakespheare full tuning).\n\n    The output is a training and validation dataset saved as `train.pt` and `val.pt`,\n    which stores the preprocessed and tokenized prompts and labels.\n    \"\"\"\n\n    destination_path.mkdir(parents=True, exist_ok=True)\n    file_path = destination_path / data_file_name\n    if not file_path.exists():\n        raise AssertionError(f\"{data_file_name} is provided by the user\")\n\n    # TODO: If we don't have the Meta weights, where do we get the tokenizer from?\n    tokenizer = Tokenizer(tokenizer_path)\n\n    data = []\n\n    with open(file_path, \"r\") as input_file:\n        for line in input_file.readlines():\n            data.append(line)\n\n    # Partition the dataset into train and test\n    train_split_size = int(len(data) * test_split_ratio)\n    test_split_size = len(data) - train_split_size\n    train_set, test_set = random_split(\n        data,\n        lengths=(train_split_size, test_split_size),\n        generator=torch.Generator().manual_seed(seed),\n    )\n    train_set, test_set = list(train_set), list(test_set)\n\n    print(f\"train has {len(train_set):,} samples\")\n    print(f\"val has {len(test_set):,} samples\")\n\n    print(\"Processing train split ...\")\n    train_set = [\n        prepare_line(line, tokenizer, max_seq_length) for line in tqdm(train_set)\n    ]\n    torch.save(train_set, file_path.parent / \"train.pt\")\n\n    print(\"Processing test split ...\")\n    test_set = [\n        prepare_line(line, tokenizer, max_seq_length) for line in tqdm(test_set)\n    ]\n    torch.save(test_set, file_path.parent / \"test.pt\")", "\n\ndef prepare_line(line: str, tokenizer: Tokenizer, max_length: int):\n    \"\"\"Processes a single sample.\n\n    This function processes the line to produce the tokenized version of it.\n    \"\"\"\n    encoded_full_prompt = tokenize(tokenizer, line, max_length=max_length, eos=False)\n    return {\n        \"input_ids\": encoded_full_prompt,\n        \"labels\": encoded_full_prompt,\n    }", "\n\ndef tokenize(\n    tokenizer: Tokenizer, string: str, max_length: int, eos=True\n) -> torch.Tensor:\n    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)", "if __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)\n"]}
{"filename": "scripts/convert_lora_weights.py", "chunked_list": ["import sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\nimport torch.nn as nn\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\nfrom lit_llama.lora import lora\n\ndef del_lora_state_dict(model: nn.Module):", "\ndef del_lora_state_dict(model: nn.Module):\n    base_model_dict = model.state_dict()\n    key_to_delete = [k for k in base_model_dict if \"lora_\" in k]\n    for del_key in key_to_delete:\n        del base_model_dict[del_key]\n    return base_model_dict\n\n\ndef lora_model_lookup(checkpoint: dict) -> int:", "\ndef lora_model_lookup(checkpoint: dict) -> int:\n    \"\"\"Returns the LoRA rank from the adapter checkpoint.\n\n    \"\"\"\n    return checkpoint[\"transformer.h.0.attn.c_attn.lora_B\"].shape[1]\n     \n\ndef main(\n    accelerator: str = \"auto\",", "def main(\n    accelerator: str = \"auto\",\n    lora_path: Optional[Path] = None,\n    checkpoint_path: Optional[Path] = None,\n    dtype: str = \"bfloat16\",\n) -> None:\n    \"\"\"Merges lora weights to base model.\n\n    Args:\n        accelerator: The hardware to run on. Possible choices are:", "    Args:\n        accelerator: The hardware to run on. Possible choices are:\n            ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\n        lora_path: Path to the checkpoint with trained LoRA weights, which are the output of\n            `finetune_lora.py`.\n        checkpoint_path: The checkpoint path to load.\n        dtype: `torch.dtype` to work with\n    \"\"\"\n    if not lora_path:\n        lora_path = Path(\"out/lora/alpaca/lit-llama-lora-finetuned.pth\")", "    if not lora_path:\n        lora_path = Path(\"out/lora/alpaca/lit-llama-lora-finetuned.pth\")\n    if not checkpoint_path:\n        checkpoint_path = Path(f\"./checkpoints/lit-llama/7B/lit-llama.pth\")\n\n    assert lora_path.is_file()\n    assert checkpoint_path.is_file()\n\n    fabric = L.Fabric(accelerator=accelerator, devices=1)\n", "    fabric = L.Fabric(accelerator=accelerator, devices=1)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n", "    t0 = time.time()\n\n    with (lazy_load(checkpoint_path) as pretrained_checkpoint,\n          lazy_load(lora_path) as lora_checkpoint):\n        name = llama_model_lookup(pretrained_checkpoint)\n        rank = lora_model_lookup(lora_checkpoint)\n\n        with EmptyInitOnDevice(\n                device=fabric.device, dtype=dtype\n        ), lora(r=rank, alpha=16, dropout=0.05, enabled=True):", "                device=fabric.device, dtype=dtype\n        ), lora(r=rank, alpha=16, dropout=0.05, enabled=True):\n            model = LLaMA.from_name(name)\n\n            # 1. Load the pretrained weights\n            model.load_state_dict(pretrained_checkpoint, strict=False)\n            # 2. Load the fine-tuned lora weights\n            model.load_state_dict(lora_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)", "\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    base_model_dict = del_lora_state_dict(model)\n    save_path = lora_path.with_stem(f\"{lora_path.stem}-lora-merged-weights\")\n    print(\"Saving LoRA to base model weights ...\")\n    torch.save(base_model_dict, save_path)\n    print(f\"Model saved at {save_path}\")\n", "    print(f\"Model saved at {save_path}\")\n\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(main)"]}
{"filename": "scripts/prepare_dolly.py", "chunked_list": ["\"\"\"Implementation derived from https://github.com/tloen/alpaca-lora\"\"\"\nimport sys\nfrom pathlib import Path\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nimport torch\nimport requests", "import torch\nimport requests\nimport json\nfrom torch.utils.data import random_split\nfrom lit_llama.tokenizer import Tokenizer\nfrom tqdm import tqdm\n\n\nDATA_FILE = \"https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\"\nDATA_FILE_NAME = \"dolly_data_cleaned.json\"", "DATA_FILE = \"https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\"\nDATA_FILE_NAME = \"dolly_data_cleaned.json\"\nIGNORE_INDEX = -1\n\n\ndef prepare(\n    destination_path: Path = Path(\"data/dolly\"), \n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    test_split_size: int = 2000,\n    max_seq_length: int = 1024,\n    seed: int = 42,\n    mask_inputs: bool = False,  # as in alpaca-lora\n) -> None:\n    \"\"\"Prepare the Dolly dataset for instruction tuning.\n    \n    The output is a training and validation dataset saved as `train.pt` and `val.pt`,\n    which stores the preprocessed and tokenized prompts and labels.\n    \"\"\"\n    \n    destination_path.mkdir(parents=True, exist_ok=True)\n    file_path = destination_path / DATA_FILE_NAME\n    download(file_path)\n\n    # TODO: If we don't have the Meta weights, where do we get the tokenizer from?\n    tokenizer = Tokenizer(tokenizer_path)\n\n    with open(file_path, \"r\") as file:\n        data = file.readlines()\n        data = [json.loads(line) for line in data]\n    for item in data:\n        item[\"input\"] = item.pop(\"context\")\n        item[\"output\"] = item.pop(\"response\")\n\n    # Partition the dataset into train and test\n    train_split_size = len(data) - test_split_size\n    train_set, test_set = random_split(\n        data, \n        lengths=(train_split_size, test_split_size),\n        generator=torch.Generator().manual_seed(seed),\n    )\n    train_set, test_set = list(train_set), list(test_set)\n\n    print(f\"train has {len(train_set):,} samples\")\n    print(f\"val has {len(test_set):,} samples\")\n\n    print(\"Processing train split ...\")\n    train_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in tqdm(train_set)]\n    torch.save(train_set, file_path.parent / \"train.pt\")\n\n    print(\"Processing test split ...\")\n    test_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in tqdm(test_set)]\n    torch.save(test_set, file_path.parent / \"test.pt\")", "\n\ndef download(file_path: Path):\n    \"\"\"Downloads the raw json data file and saves it in the given destination.\"\"\"\n    if file_path.exists():\n        return\n    with open(file_path, \"w\") as f:\n        f.write(requests.get(DATA_FILE).text)\n\n\ndef prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n    \"\"\"Processes a single sample.\n    \n    Each sample in the dataset consists of:\n    - instruction: A string describing the task\n    - input: A string holding a special input value for the instruction.\n        This only applies to some samples, and in others this is empty.\n    - output: The response string\n\n    This function processes this data to produce a prompt text and a label for\n    supervised training. The prompt text is formed as a single message including both\n    the instruction and the input. The label/target is the same message but with the\n    response attached.\n\n    Finally, both the prompt and the label get tokenized. If desired, all tokens\n    in the label that correspond to the original input prompt get masked out (default).\n    \"\"\"\n    full_prompt = generate_prompt(example)\n    full_prompt_and_response = full_prompt + example[\"output\"]\n    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n\n    # The labels are the full prompt with response, but with the prompt masked out\n    labels = encoded_full_prompt_and_response.clone()\n    if mask_inputs:\n        labels[:len(encoded_full_prompt)] = IGNORE_INDEX\n\n    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}", "\n\ndef prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n    \"\"\"Processes a single sample.\n    \n    Each sample in the dataset consists of:\n    - instruction: A string describing the task\n    - input: A string holding a special input value for the instruction.\n        This only applies to some samples, and in others this is empty.\n    - output: The response string\n\n    This function processes this data to produce a prompt text and a label for\n    supervised training. The prompt text is formed as a single message including both\n    the instruction and the input. The label/target is the same message but with the\n    response attached.\n\n    Finally, both the prompt and the label get tokenized. If desired, all tokens\n    in the label that correspond to the original input prompt get masked out (default).\n    \"\"\"\n    full_prompt = generate_prompt(example)\n    full_prompt_and_response = full_prompt + example[\"output\"]\n    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n\n    # The labels are the full prompt with response, but with the prompt masked out\n    labels = encoded_full_prompt_and_response.clone()\n    if mask_inputs:\n        labels[:len(encoded_full_prompt)] = IGNORE_INDEX\n\n    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}", "\n\ndef tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n\n\ndef generate_prompt(example):\n    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n    'response' field.\"\"\"\n\n    if example[\"input\"]:\n        return (\n            f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n        )\n    return (\n        f\"Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    CLI(prepare)\n"]}
{"filename": "generate/full.py", "chunked_list": ["import sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).absolute().parent.parent\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import quantization\nfrom scripts.prepare_alpaca import generate_prompt\nfrom generate import generate\n", "from generate import generate\n\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    *,\n    num_samples: int = 1,\n    max_new_tokens: int = 50,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Optional[Path] = None,\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    model_size: str = \"7B\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        prompt: The prompt string to use for generating the samples.\n        num_samples: The number of text samples to generate.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        model_size: The model size to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    if not checkpoint_path:\n        checkpoint_path = Path(f\"checkpoints/lit-llama/{model_size}/lit-llama.pth\")\n    assert checkpoint_path.is_file(), checkpoint_path\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    \n    with fabric.init_module(empty_init=True), quantization(mode=quantize):\n        model = LLaMA.from_name(model_size)\n\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint)\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": prompt, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=fabric.device)\n    prompt_length = encoded.size(0)\n\n    L.seed_everything(1234)\n    for i in range(num_samples):\n        t0 = time.perf_counter()\n        y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k)\n        t = time.perf_counter() - t0\n\n        model.reset_cache()\n        print(tokenizer.decode(y))\n        tokens_generated = y.size(0) - prompt_length\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    warnings.filterwarnings(\n        # Triggered in bitsandbytes/autograd/_functions.py:298\n        \"ignore\", \n        message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n    )\n    CLI(main)", ""]}
{"filename": "generate/adapter_v2.py", "chunked_list": ["import sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA\nfrom lit_llama.utils import lazy_load, llama_model_lookup, quantization\nfrom lit_llama.adapter_v2 import add_adapter_v2_parameters_to_linear_layers", "from lit_llama.utils import lazy_load, llama_model_lookup, quantization\nfrom lit_llama.adapter_v2 import add_adapter_v2_parameters_to_linear_layers\nfrom scripts.prepare_alpaca import generate_prompt\n\n\ndef main(\n    prompt: str = \"What food do lamas eat?\",\n    input: str = \"\",\n    adapter_path: Path = Path(\"out/adapter_v2/alpaca/lit-llama-adapter-finetuned.pth\"),\n    pretrained_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = None,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n) -> None:\n    \"\"\"Generates a response based on a given instruction and an optional input.\n    This script will only work with checkpoints from the instruction-tuned LLaMA-Adapter model.\n    See `finetune_adapter_v2.py`.\n\n    Args:\n        prompt: The prompt/instruction (Alpaca style).\n        adapter_path: Path to the checkpoint with trained adapter weights, which are the output of\n            `finetune_adapter_v2.py`.\n        input: Optional input (Alpaca style).\n        pretrained_path: The path to the checkpoint with pretrained LLaMA weights.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n    \"\"\"\n    assert adapter_path.is_file()\n    assert pretrained_path.is_file()\n    assert tokenizer_path.is_file()\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with fabric.init_module(empty_init=True), quantization(mode=quantize):\n            model = LLaMA.from_name(name)\n            add_adapter_v2_parameters_to_linear_layers(model)\n\n        # 1. Load the pretrained weights\n        model.load_state_dict(pretrained_checkpoint, strict=False)\n        # 2. Load the fine-tuned adapter weights\n        model.load_state_dict(adapter_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": prompt, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n    prompt_length = encoded.size(0)\n\n    t0 = time.perf_counter()\n    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n    t = time.perf_counter() - t0\n\n    model.reset_cache()\n    output = tokenizer.decode(y)\n    output = output.split(\"### Response:\")[1].strip()\n    print(output)\n\n    tokens_generated = y.size(0) - prompt_length\n    print(f\"\\n\\nTime for inference: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    CLI(main)", ""]}
{"filename": "generate/adapter.py", "chunked_list": ["import sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA\nfrom lit_llama.utils import lazy_load, llama_model_lookup, quantization\nfrom scripts.prepare_alpaca import generate_prompt", "from lit_llama.utils import lazy_load, llama_model_lookup, quantization\nfrom scripts.prepare_alpaca import generate_prompt\n\n\ndef main(\n    prompt: str = \"What food do lamas eat?\",\n    input: str = \"\",\n    adapter_path: Path = Path(\"out/adapter/alpaca/lit-llama-adapter-finetuned.pth\"),\n    pretrained_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = None,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n) -> None:\n    \"\"\"Generates a response based on a given instruction and an optional input.\n    This script will only work with checkpoints from the instruction-tuned LLaMA-Adapter model.\n    See `finetune_adapter.py`.\n\n    Args:\n        prompt: The prompt/instruction (Alpaca style).\n        adapter_path: Path to the checkpoint with trained adapter weights, which are the output of\n            `finetune_adapter.py`.\n        input: Optional input (Alpaca style).\n        pretrained_path: The path to the checkpoint with pretrained LLaMA weights.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n    \"\"\"\n    assert adapter_path.is_file()\n    assert pretrained_path.is_file()\n    assert tokenizer_path.is_file()\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with fabric.init_module(empty_init=True), quantization(mode=quantize):\n            model = LLaMA.from_name(name)\n\n        # 1. Load the pretrained weights\n        model.load_state_dict(pretrained_checkpoint, strict=False)\n        # 2. Load the fine-tuned adapter weights\n        model.load_state_dict(adapter_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": prompt, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n    prompt_length = encoded.size(0)\n\n    t0 = time.perf_counter()\n    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n    t = time.perf_counter() - t0\n\n    model.reset_cache()\n    output = tokenizer.decode(y)\n    output = output.split(\"### Response:\")[1].strip()\n    print(output)\n\n    tokens_generated = y.size(0) - prompt_length\n    print(f\"\\n\\nTime for inference: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    CLI(main)", ""]}
{"filename": "generate/lora.py", "chunked_list": ["import sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package", "\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom generate import generate\nfrom lit_llama import Tokenizer, LLaMA\nfrom lit_llama.lora import lora\nfrom lit_llama.utils import lazy_load, llama_model_lookup\nfrom scripts.prepare_alpaca import generate_prompt", "from lit_llama.utils import lazy_load, llama_model_lookup\nfrom scripts.prepare_alpaca import generate_prompt\n\nlora_r = 8\nlora_alpha = 16\nlora_dropout = 0.05\n\n\ndef main(\n    prompt: str = \"What food do lamas eat?\",\n    input: str = \"\",\n    lora_path: Path = Path(\"out/lora/alpaca/lit-llama-lora-finetuned.pth\"),\n    pretrained_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = None,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n) -> None:\n    \"\"\"Generates a response based on a given instruction and an optional input.\n    This script will only work with checkpoints from the instruction-tuned LoRA model.\n    See `finetune_lora.py`.\n\n    Args:\n        prompt: The prompt/instruction (Alpaca style).\n        lora_path: Path to the checkpoint with trained LoRA weights, which are the output of\n            `finetune_lora.py`.\n        input: Optional input (Alpaca style).\n        pretrained_path: The path to the checkpoint with pretrained LLaMA weights.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n    \"\"\"\n    assert lora_path.is_file()\n    assert pretrained_path.is_file()\n    assert tokenizer_path.is_file()\n\n    if quantize is not None:\n        raise NotImplementedError(\"Quantization in LoRA is not supported yet\")\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n\n    with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(lora_path) as lora_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with fabric.init_module(empty_init=True), lora(r=lora_r, alpha=lora_alpha, dropout=lora_dropout, enabled=True):\n            model = LLaMA.from_name(name)\n\n            # 1. Load the pretrained weights\n            model.load_state_dict(pretrained_checkpoint, strict=False)\n            # 2. Load the fine-tuned lora weights\n            model.load_state_dict(lora_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": prompt, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    t0 = time.perf_counter()\n    output = generate(\n        model,\n        idx=encoded,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        eos_id=tokenizer.eos_id\n    )\n    t = time.perf_counter() - t0\n\n    output = tokenizer.decode(output)\n    output = output.split(\"### Response:\")[1].strip()\n    print(output)\n\n    print(f\"\\n\\nTime for inference: {t:.02f} sec total, {max_new_tokens / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "def main(\n    prompt: str = \"What food do lamas eat?\",\n    input: str = \"\",\n    lora_path: Path = Path(\"out/lora/alpaca/lit-llama-lora-finetuned.pth\"),\n    pretrained_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = None,\n    max_new_tokens: int = 100,\n    top_k: int = 200,\n    temperature: float = 0.8,\n) -> None:\n    \"\"\"Generates a response based on a given instruction and an optional input.\n    This script will only work with checkpoints from the instruction-tuned LoRA model.\n    See `finetune_lora.py`.\n\n    Args:\n        prompt: The prompt/instruction (Alpaca style).\n        lora_path: Path to the checkpoint with trained LoRA weights, which are the output of\n            `finetune_lora.py`.\n        input: Optional input (Alpaca style).\n        pretrained_path: The path to the checkpoint with pretrained LLaMA weights.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n    \"\"\"\n    assert lora_path.is_file()\n    assert pretrained_path.is_file()\n    assert tokenizer_path.is_file()\n\n    if quantize is not None:\n        raise NotImplementedError(\"Quantization in LoRA is not supported yet\")\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n\n    with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(lora_path) as lora_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with fabric.init_module(empty_init=True), lora(r=lora_r, alpha=lora_alpha, dropout=lora_dropout, enabled=True):\n            model = LLaMA.from_name(name)\n\n            # 1. Load the pretrained weights\n            model.load_state_dict(pretrained_checkpoint, strict=False)\n            # 2. Load the fine-tuned lora weights\n            model.load_state_dict(lora_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    sample = {\"instruction\": prompt, \"input\": input}\n    prompt = generate_prompt(sample)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n\n    t0 = time.perf_counter()\n    output = generate(\n        model,\n        idx=encoded,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        eos_id=tokenizer.eos_id\n    )\n    t = time.perf_counter() - t0\n\n    output = tokenizer.decode(output)\n    output = output.split(\"### Response:\")[1].strip()\n    print(output)\n\n    print(f\"\\n\\nTime for inference: {t:.02f} sec total, {max_new_tokens / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    CLI(main)", ""]}
{"filename": "tests/test_prepare_shakespeare.py", "chunked_list": ["import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nwd = (Path(__file__).parent.parent / \"scripts\").absolute()\n\n\ndef test_prepare(tmp_path):\n    sys.path.append(str(wd))\n\n    import prepare_shakespeare\n\n    prepare_shakespeare.prepare(tmp_path)\n\n    assert set(os.listdir(tmp_path)) == {\"train.bin\", \"tokenizer.model\", \"tokenizer.vocab\", \"input.txt\", \"val.bin\"}", "def test_prepare(tmp_path):\n    sys.path.append(str(wd))\n\n    import prepare_shakespeare\n\n    prepare_shakespeare.prepare(tmp_path)\n\n    assert set(os.listdir(tmp_path)) == {\"train.bin\", \"tokenizer.model\", \"tokenizer.vocab\", \"input.txt\", \"val.bin\"}\n\n\ndef test_cli():\n    cli_path = wd / \"prepare_shakespeare.py\"\n    output = subprocess.check_output([sys.executable, cli_path, \"-h\"])\n    output = str(output.decode())\n    assert 'Prepare the \"Tiny Shakespeare\"' in output", "\n\ndef test_cli():\n    cli_path = wd / \"prepare_shakespeare.py\"\n    output = subprocess.check_output([sys.executable, cli_path, \"-h\"])\n    output = str(output.decode())\n    assert 'Prepare the \"Tiny Shakespeare\"' in output\n"]}
{"filename": "tests/test_adapter.py", "chunked_list": ["from dataclasses import asdict\nimport pytest\nimport sys\nimport torch\n\n\n@pytest.mark.skipif(sys.platform == \"win32\", reason=\"EmptyInitOnDevice on CPU not working for Windows.\")\n@pytest.mark.parametrize(\"model_size\", [\"7B\", \"13B\", \"30B\", \"65B\"])\ndef test_config_identical(model_size, lit_llama):\n    import lit_llama.adapter as llama_adapter\n    import lit_llama.model as llama\n    from lit_llama.utils import EmptyInitOnDevice\n\n    llama_config = asdict(llama.LLaMAConfig.from_name(model_size))\n    adapter_config = asdict(llama_adapter.LLaMAConfig.from_name(model_size))\n\n    del adapter_config[\"adapter_prompt_length\"]\n    del adapter_config[\"adapter_start_layer\"]\n    assert adapter_config == llama_config\n\n    with EmptyInitOnDevice():\n        llama_model = llama.LLaMA.from_name(model_size)\n        adapter_model = llama_adapter.LLaMA.from_name(model_size)\n        assert llama_model.lm_head.weight.shape == adapter_model.lm_head.weight.shape", "def test_config_identical(model_size, lit_llama):\n    import lit_llama.adapter as llama_adapter\n    import lit_llama.model as llama\n    from lit_llama.utils import EmptyInitOnDevice\n\n    llama_config = asdict(llama.LLaMAConfig.from_name(model_size))\n    adapter_config = asdict(llama_adapter.LLaMAConfig.from_name(model_size))\n\n    del adapter_config[\"adapter_prompt_length\"]\n    del adapter_config[\"adapter_start_layer\"]\n    assert adapter_config == llama_config\n\n    with EmptyInitOnDevice():\n        llama_model = llama.LLaMA.from_name(model_size)\n        adapter_model = llama_adapter.LLaMA.from_name(model_size)\n        assert llama_model.lm_head.weight.shape == adapter_model.lm_head.weight.shape", "\n\ndef test_adapter_load_gating_factor(lit_llama):\n    \"\"\"Tests backward-compatible loading of checkpoints after the `gating_factor` was extended per-head\n    in PR #297.\n    \"\"\"\n    import lit_llama.adapter as llama_adapter\n    from lit_llama.utils import lazy_load\n\n    config = llama_adapter.LLaMAConfig(n_head=4, block_size=100, n_embd=16)\n    attn = llama_adapter.CausalSelfAttention(config=config, block_idx=3)\n\n    # Old checkpoint format\n    state_dict={\n        \"gating_factor\": torch.tensor(0.42),  # in old checkpoints, this was a scalar\n        \"c_attn.weight\": torch.zeros(3 * 16, 16),\n        \"c_proj.weight\": torch.zeros(16, 16),\n        \"adapter_wte.weight\": torch.zeros(10, 16),\n    }\n    attn.load_state_dict(state_dict=state_dict)\n    assert torch.equal(attn.gating_factor, torch.full((1, 4, 1, 1), 0.42))\n\n    # New checkpoint format\n    state_dict={\n        \"gating_factor\": torch.tensor([0.42, 0.42, 0.42, 0.42]).reshape(1, 4, 1, 1),\n        \"c_attn.weight\": torch.zeros(3 * 16, 16),\n        \"c_proj.weight\": torch.zeros(16, 16),\n        \"adapter_wte.weight\": torch.zeros(10, 16),\n    }\n    attn.load_state_dict(state_dict=state_dict)\n    assert torch.equal(attn.gating_factor, torch.full((1, 4, 1, 1), 0.42))", ""]}
{"filename": "tests/test_generate.py", "chunked_list": ["import functools\nimport subprocess\nimport sys\nfrom contextlib import contextmanager, redirect_stdout\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import mock\nfrom unittest.mock import Mock, call, ANY\n\nimport torch", "\nimport torch\n\nwd = Path(__file__).parent.parent.absolute()\n\n\n@functools.lru_cache(maxsize=1)\ndef load_generate_script():\n    sys.path.append(str(wd))\n\n    import generate as generate\n\n    return generate", "\n\ndef test_generate():\n    generate = load_generate_script()\n\n    from lit_llama.model import LLaMA, LLaMAConfig\n\n    T, C = 5, 3\n    logits = torch.randn(T, C)\n    input_idx = torch.randint(10, size=(T,))\n\n    config = LLaMAConfig(block_size=128, vocab_size=16, n_layer=1, n_head=4, n_embd=8)\n    model = LLaMA(config)\n    max_new_tokens = 20\n\n    multinomial_results = []\n    original_multinomial = torch.multinomial\n\n    def multinomial(*args, **kwargs):\n        out = original_multinomial(*args, **kwargs)\n        multinomial_results.append(out)\n        return out\n\n    with mock.patch(\"torch.multinomial\", multinomial):\n        out = generate.generate(model, input_idx, max_new_tokens, max_seq_length=10, top_k=4)\n\n    assert out.size(0) == T + max_new_tokens\n    multinomial_results = torch.hstack(multinomial_results)\n    expected = torch.cat((input_idx, multinomial_results))\n    assert out.shape == expected.shape\n    torch.testing.assert_close(out, expected)", "\n\n@mock.patch(\"torch.cuda.is_bf16_supported\", return_value=False)\ndef test_main(tmp_path, monkeypatch):\n    generate = load_generate_script()\n\n    checkpoint_path = tmp_path / \"ckpt\"\n    checkpoint_path.touch()\n    tokenizer_path = tmp_path / \"tokenizer\"\n    tokenizer_path.touch()\n\n    class FabricMock(Mock):\n        @property\n        def device(self):\n            return torch.device(\"cpu\")\n\n        @contextmanager\n        def init_module(self, empty_init):\n            yield\n\n    monkeypatch.setattr(generate.L, \"Fabric\", FabricMock)\n    model_mock = Mock()\n    monkeypatch.setattr(generate.LLaMA, \"from_name\", model_mock)\n    lookup_mock = Mock(return_value=\"1T\")\n    monkeypatch.setattr(generate, \"llama_model_lookup\", lookup_mock)\n    load_mock = Mock()\n    load_mock.return_value = load_mock\n    load_mock.__enter__ = Mock()\n    load_mock.__exit__ = Mock()\n    monkeypatch.setattr(generate.torch, \"load\", load_mock)\n    monkeypatch.setattr(generate, \"lazy_load\", load_mock)\n    tokenizer_mock = Mock()\n    tokenizer_mock.return_value.encode.return_value = torch.tensor([[1, 2, 3]])\n    tokenizer_mock.return_value.decode.return_value = \"foo bar baz\"\n    monkeypatch.setattr(generate, \"Tokenizer\", tokenizer_mock)\n    generate_mock = Mock()\n    generate_mock.return_value = torch.tensor([[3, 2, 1]])\n    monkeypatch.setattr(generate, \"generate\", generate_mock)\n\n    num_samples = 2\n    out = StringIO()\n    with redirect_stdout(out):\n        generate.main(\n            checkpoint_path=checkpoint_path,\n            tokenizer_path=tokenizer_path,\n            temperature=2.0,\n            top_k=2,\n            num_samples=num_samples,\n        )\n\n    model_mock.assert_called_once_with(\"1T\")\n    load_mock.assert_called_once_with(checkpoint_path)\n    tokenizer_mock.assert_called_once_with(tokenizer_path)\n    assert len(tokenizer_mock.return_value.decode.mock_calls) == num_samples\n    assert torch.allclose(tokenizer_mock.return_value.decode.call_args[0][0], generate_mock.return_value)\n    assert generate_mock.mock_calls == [call(ANY, ANY, 50, temperature=2.0, top_k=2)] * num_samples\n    # only the generated result is printed to stdout\n    assert out.getvalue() == \"foo bar baz\\n\" * num_samples", "\n\ndef test_cli():\n    cli_path = wd / \"generate.py\"\n    output = subprocess.check_output([sys.executable, cli_path, \"-h\"])\n    output = str(output.decode())\n    assert \"Generates text samples\" in output\n"]}
{"filename": "tests/test_rope.py", "chunked_list": ["import torch\n\n\n@torch.no_grad()\ndef test_rope(lit_llama, orig_llama) -> None:\n    torch.manual_seed(1)\n\n    bs, seq_len, n_head, n_embed = 1, 6, 2, 8\n    x = torch.randint(0, 10000, size=(bs, seq_len, n_head, n_embed // n_head)).float()\n\n    freqs_cis = orig_llama.precompute_freqs_cis(n_embed // n_head, seq_len)\n    llama_rope_cache = lit_llama.build_rope_cache(seq_len, n_embed // n_head, dtype=x.dtype, device=x.device)\n    torch.testing.assert_close(freqs_cis, torch.view_as_complex(llama_rope_cache))\n\n    llama_x_rope = lit_llama.apply_rope(x, llama_rope_cache)\n    orig_llama_x_rope, _ = orig_llama.apply_rotary_emb(x, x, freqs_cis)\n    torch.testing.assert_close(llama_x_rope, orig_llama_x_rope)", ""]}
{"filename": "tests/test_lora.py", "chunked_list": ["import torch\n\n\ndef test_lora_layer_replacement(lit_llama):\n    from lit_llama.lora import lora, CausalSelfAttention as LoRACausalSelfAttention\n    from lit_llama.model import LLaMA, LLaMAConfig\n    \n    config = LLaMAConfig()\n    config.n_layer = 2\n    config.n_head = 4\n    config.n_embd = 8\n    config.block_size = 8\n    config.vocab_size = 8\n\n    with lora(r=8, alpha=8, dropout=0.1):\n        model = LLaMA(config)\n\n    assert isinstance(model.transformer.h[0].attn, LoRACausalSelfAttention)\n    assert isinstance(model.transformer.h[1].attn, LoRACausalSelfAttention)", "\n\ndef test_lora_merge_unmerge(lit_llama):\n    from lit_llama.lora import lora, mark_only_lora_as_trainable\n    from lit_llama.model import LLaMA, LLaMAConfig\n    \n    config = LLaMAConfig(n_layer=1, n_head=2, n_embd=8, block_size=8, vocab_size=8)\n\n    with lora(r=8, alpha=8, dropout=0.1):\n        model = LLaMA(config)\n    \n    initial_weight = model.transformer.h[0].attn.c_attn.weight.clone()\n    model.train()\n    assert torch.equal(model.transformer.h[0].attn.c_attn.weight, initial_weight)\n\n    # perform an update to the LoRA weights\n    mark_only_lora_as_trainable(model)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n    model(torch.randint(0, 8, size=(2, 4), dtype=torch.int64)).sum().backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    # the weight remains unchanged (only lora A and B change)\n    assert torch.equal(model.transformer.h[0].attn.c_attn.weight, initial_weight)\n\n    # 'merge' and then 'unmerge' should neutralize themselves\n    weight_before = model.transformer.h[0].attn.c_attn.weight.clone()\n    model.eval()\n    assert not torch.equal(model.transformer.h[0].attn.c_attn.weight, weight_before)\n    model.train()\n    # note: numerically, `W + (A * B) - (A * B) == W` does not hold exactly\n    assert torch.allclose(model.transformer.h[0].attn.c_attn.weight, weight_before)\n\n    # calling eval/train multiple times in a row should not merge/unmerge multiple times\n    model.eval()\n    assert model.transformer.h[0].attn.c_attn.merged\n    weight_after = model.transformer.h[0].attn.c_attn.weight.clone()\n    model.eval()\n    model.eval()\n    assert torch.equal(model.transformer.h[0].attn.c_attn.weight, weight_after)\n    model.train()\n    assert not model.transformer.h[0].attn.c_attn.merged\n    weight_after = model.transformer.h[0].attn.c_attn.weight.clone()\n    model.train()\n    model.train()\n    assert torch.equal(model.transformer.h[0].attn.c_attn.weight, weight_after)", ""]}
{"filename": "tests/test_prepare_redpajama.py", "chunked_list": ["import json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom unittest import mock\nfrom unittest.mock import Mock, call, ANY\n\nwd = (Path(__file__).parent.parent / \"scripts\").absolute()\n", "wd = (Path(__file__).parent.parent / \"scripts\").absolute()\n\nimport requests\n\n\ndef train_tokenizer(destination_path):\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    # download the tiny shakespeare dataset\n    input_file_path = destination_path / \"input.txt\"\n    if not input_file_path.exists():\n        data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n        with open(input_file_path, \"w\") as f:\n            f.write(requests.get(data_url).text)\n\n    from lit_llama import Tokenizer\n    Tokenizer.train(input=input_file_path, destination=destination_path, vocab_size=100)\n\n    return destination_path / \"tokenizer.model\"", " \n\ndef test_prepare_sample(tmp_path):\n    sys.path.append(str(wd))\n\n    tokenizer_path = train_tokenizer(tmp_path)\n\n    sample_path = tmp_path / \"sample\"\n    source_path = sample_path / \"source\"\n    dest_path = sample_path / \"dest\"\n\n    source_path.mkdir(parents=True, exist_ok=True)\n\n    sample = {\n        \"meta\": {\"some\": \"info\"},\n        \"text\": \"some text\"\n    }\n\n    jsonl_sample = \"\\n\".join([json.dumps(el) for el in [sample] * 2])\n\n    import prepare_redpajama\n\n    for filename in prepare_redpajama.filenames_sample:\n        with open(source_path / filename, \"w\") as f:\n            f.write(jsonl_sample)\n\n    prepare_redpajama.prepare(source_path=source_path, tokenizer_path=tokenizer_path, destination_path=dest_path, sample=True)\n\n    bin_files = [el.replace(\".jsonl\", \"_0000000000.bin\") for el in prepare_redpajama.filenames_sample]\n\n    assert set(os.listdir(dest_path)) == set(bin_files)\n\n    from lit_llama import Tokenizer\n    from lit_llama.packed_dataset import PackedDataset\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    # artificially set block_size to fit the text\n    block_size = len(tokenizer.encode(\"some text\"))\n\n    for filename in bin_files:\n        filenames = [os.path.join(dest_path, filename)]\n        dataset = PackedDataset(filenames=filenames, n_chunks=1, block_size=block_size, shuffle=False)\n        dataset_iter = iter(dataset)\n        assert tokenizer.decode(next(dataset_iter)) == \"some text\"\n        assert tokenizer.decode(next(dataset_iter)) == \"some text\"", "\n\ndef test_prepare_full(tmp_path):\n    sys.path.append(str(wd))\n\n    tokenizer_path = train_tokenizer(tmp_path)\n\n    full_path = tmp_path / \"full\"\n    source_path = full_path / \"source\"\n    dest_path = full_path / \"dest\"\n\n    source_path.mkdir(parents=True, exist_ok=True)\n\n    sample = {\n        \"meta\": {\"some\": \"info\"},\n        \"text\": \"some text\"\n    }\n\n    jsonl_sample = \"\\n\".join([json.dumps(el) for el in [sample] * 2])\n\n    import prepare_redpajama\n\n    arxiv_file = source_path / \"arxiv\" / \"arxiv_0.jsonl\"\n    arxiv_file.parent.mkdir(parents=True, exist_ok=True)\n    with open(arxiv_file, \"w\") as f:\n        f.write(jsonl_sample)\n\n    import zstandard as zstd\n\n    cc_file = source_path / \"common_crawl\" / \"cc_0.jsonl\"\n    cc_file.parent.mkdir(parents=True, exist_ok=True)\n    with zstd.open(cc_file, \"wt\", encoding=\"utf-8\") as f:\n        f.write(jsonl_sample)\n\n    filename_sets = {\n        \"arxiv\": \"arxiv/arxiv*\",\n        \"common_crawl\": \"common_crawl/*\",\n    }\n\n    with mock.patch(\"prepare_redpajama.filename_sets\", filename_sets):\n        prepare_redpajama.prepare(source_path=source_path, tokenizer_path=tokenizer_path, destination_path=dest_path, sample=False)\n\n        all_names = prepare_redpajama.filename_sets.keys()\n        bin_files = [el + \"_0000000000.bin\" for el in all_names]\n\n    assert set(os.listdir(dest_path)) == set(bin_files)\n\n    from lit_llama import Tokenizer\n    from lit_llama.packed_dataset import PackedDataset\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    # artificially set block_size to fit the text\n    block_size = len(tokenizer.encode(\"some text\"))\n\n    filenames = [os.path.join(dest_path, el) for el in bin_files]\n\n    for filename in filenames:\n        dataset = PackedDataset(filenames=[filename], n_chunks=1, block_size=block_size, shuffle=False)\n        dataset_iter = iter(dataset)\n        assert tokenizer.decode(next(dataset_iter)) == \"some text\"\n        assert tokenizer.decode(next(dataset_iter)) == \"some text\"", "\n\ndef test_cli():\n    cli_path = wd / \"prepare_redpajama.py\"\n    output = subprocess.check_output([sys.executable, cli_path, \"-h\"])\n    output = str(output.decode())\n    assert 'Prepare the \"Red Pajama\"' in output\n"]}
{"filename": "tests/test_adapter_v2.py", "chunked_list": ["import pytest\nimport sys\n\n\n@pytest.mark.skipif(sys.platform == \"win32\", reason=\"EmptyInitOnDevice on CPU not working for Windows.\")\n@pytest.mark.parametrize(\"model_size\", [\"7B\", \"13B\", \"30B\", \"65B\"])\ndef test_config_identical(model_size, lit_llama):\n    import torch.nn as nn\n    import lit_llama.adapter as llama_adapter\n    from lit_llama.adapter_v2 import adapter_v2_linear_with_bias_and_scale\n    import lit_llama.model as llama\n    from lit_llama.utils import EmptyInitOnDevice\n\n    with EmptyInitOnDevice():\n        llama_model = llama.LLaMA.from_name(model_size)\n        adapter_model = llama_adapter.LLaMA.from_name(model_size)\n\n        for module in adapter_model.modules():\n            if isinstance(module, nn.Linear):\n                adapter_v2_linear_with_bias_and_scale(module)\n\n        print(adapter_model.transformer.h[2].attn.c_attn.adapter_bias)\n        assert not hasattr(llama_model.transformer.h[2].attn.c_attn, 'adapter_bias')\n        assert not hasattr(llama_model.transformer.h[2].attn.c_attn, 'adapter_scale')\n        assert hasattr(adapter_model.transformer.h[2].attn.c_attn, 'adapter_bias')\n        assert hasattr(adapter_model.transformer.h[2].attn.c_attn, 'adapter_scale')"]}
{"filename": "tests/test_model.py", "chunked_list": ["import torch\nimport pytest\nimport sys\n\n\ndef copy_mlp(llama_mlp, orig_llama_mlp) -> None:\n    orig_llama_mlp.w1.weight.copy_(llama_mlp.c_fc1.weight)\n    orig_llama_mlp.w3.weight.copy_(llama_mlp.c_fc2.weight)\n    orig_llama_mlp.w2.weight.copy_(llama_mlp.c_proj.weight)\n", "\n\ndef copy_attention(llama_attn, orig_llama_attn) -> None:\n    n_embd = llama_attn.c_attn.weight.shape[1]\n    orig_llama_attn.wq.weight.copy_(llama_attn.c_attn.weight[:n_embd])\n    orig_llama_attn.wk.weight.copy_(llama_attn.c_attn.weight[n_embd:-n_embd])\n    orig_llama_attn.wv.weight.copy_(llama_attn.c_attn.weight[-n_embd:])\n    orig_llama_attn.wo.weight.copy_(llama_attn.c_proj.weight)\n\n\ndef copy_block(llama_block, orig_llama_block) -> None:\n    orig_llama_block.attention_norm.weight.copy_(llama_block.rms_1.scale)\n    copy_attention(llama_block.attn, orig_llama_block.attention)\n    orig_llama_block.ffn_norm.weight.copy_(llama_block.rms_2.scale)\n    copy_mlp(llama_block.mlp, orig_llama_block.feed_forward)", "\n\ndef copy_block(llama_block, orig_llama_block) -> None:\n    orig_llama_block.attention_norm.weight.copy_(llama_block.rms_1.scale)\n    copy_attention(llama_block.attn, orig_llama_block.attention)\n    orig_llama_block.ffn_norm.weight.copy_(llama_block.rms_2.scale)\n    copy_mlp(llama_block.mlp, orig_llama_block.feed_forward)\n\n\ndef copy_weights(llama_model, orig_llama_model) -> None:\n    orig_llama_model.tok_embeddings.weight.copy_(llama_model.transformer.wte.weight)\n    for llama_block, orig_llama_block in zip(llama_model.transformer.h, orig_llama_model.layers):\n        copy_block(llama_block, orig_llama_block)\n    orig_llama_model.norm.weight.copy_(llama_model.transformer.ln_f.scale)\n    orig_llama_model.output.weight.copy_(llama_model.lm_head.weight)", "\ndef copy_weights(llama_model, orig_llama_model) -> None:\n    orig_llama_model.tok_embeddings.weight.copy_(llama_model.transformer.wte.weight)\n    for llama_block, orig_llama_block in zip(llama_model.transformer.h, orig_llama_model.layers):\n        copy_block(llama_block, orig_llama_block)\n    orig_llama_model.norm.weight.copy_(llama_model.transformer.ln_f.scale)\n    orig_llama_model.output.weight.copy_(llama_model.lm_head.weight)\n\n\n@torch.no_grad()", "\n@torch.no_grad()\n@pytest.mark.parametrize(\"kv_cache\", (False, True))\ndef test_to_orig_llama(lit_llama, orig_llama, kv_cache) -> None:\n    block_size = 64\n    vocab_size = 32000\n    n_layer = 16\n    n_head = 16\n    n_embd = 32\n    batch_size = 3\n\n    llama_config = lit_llama.LLaMAConfig(\n        block_size=block_size, vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd\n    )\n    orig_llama_config = orig_llama.ModelArgs(\n        dim=n_embd,\n        n_layers=n_layer,\n        n_heads=n_head,\n        vocab_size=vocab_size,\n        norm_eps=1e-5,\n        max_seq_len=block_size,\n        max_batch_size=batch_size,\n    )\n\n    seq_len = orig_llama_config.max_seq_len\n    token_sample = torch.randint(0, orig_llama_config.vocab_size, size=(batch_size, seq_len), dtype=torch.int64)\n\n    llama_model = lit_llama.LLaMA(llama_config)\n    llama_model.apply(llama_model._init_weights)\n    orig_llama_model = orig_llama.Transformer(orig_llama_config)\n\n    copy_weights(llama_model, orig_llama_model)\n\n    orig_llama_embed = orig_llama_model.tok_embeddings(token_sample)\n    llama_embed = llama_model.transformer.wte(token_sample)\n    assert torch.allclose(orig_llama_embed, llama_embed)\n\n    llama_rope = llama_model.build_rope_cache(token_sample)\n    llama_mask = llama_model.build_mask_cache(token_sample)\n    orig_llama_mask = torch.full((1, 1, seq_len, seq_len), float(\"-inf\"))\n    orig_llama_mask = torch.triu(orig_llama_mask, diagonal=1)\n    if kv_cache:\n        orig_llama_block_out = orig_llama_model.layers[0](\n            orig_llama_embed, 0, orig_llama_model.freqs_cis[:seq_len], orig_llama_mask\n        )\n        theirs_k_cache = orig_llama_model.layers[0].attention.cache_k\n        theirs_v_cache = orig_llama_model.layers[0].attention.cache_v\n        head_size = n_embd // n_head\n        kv_cache_shape = (batch_size, n_head, block_size, head_size)\n        ours_kv_cache = torch.zeros(kv_cache_shape), torch.zeros(kv_cache_shape)\n        (llama_block_out, ours_kv_cache) = llama_model.transformer.h[0](\n            llama_embed, llama_rope, llama_mask, seq_len, torch.arange(block_size), ours_kv_cache\n        )\n        ours_k_cache = ours_kv_cache[0].permute(0, 2, 1, 3)\n        ours_v_cache = ours_kv_cache[1].permute(0, 2, 1, 3)\n        torch.testing.assert_close(ours_k_cache, theirs_k_cache)\n        torch.testing.assert_close(ours_v_cache, theirs_v_cache)\n    else:\n        orig_llama_block_out = orig_llama_model.layers[0](\n            orig_llama_embed, 0, orig_llama_model.freqs_cis[:seq_len], orig_llama_mask\n        )\n        (llama_block_out, _) = llama_model.transformer.h[0](llama_embed, llama_rope, llama_mask, seq_len)\n    assert torch.allclose(orig_llama_block_out, llama_block_out)\n\n    expected = orig_llama_model(token_sample, 0)\n    out = llama_model(token_sample)\n    assert torch.allclose(out, expected)", "\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"Requires CUDA\")\n@torch.no_grad()\ndef test_bfloat16_llama_init(lit_llama, orig_llama) -> None:\n    from lit_llama.utils import EmptyInitOnDevice\n\n    block_size = 64\n    vocab_size = 32000\n    n_layer = 16\n    n_head = 16\n    n_embd = 32\n\n    llama_config = lit_llama.LLaMAConfig(\n        block_size=block_size, vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd\n    )\n    llama_model = lit_llama.LLaMA(llama_config)\n    llama_model.apply(llama_model._init_weights)\n\n    batch_size = 3\n\n    token_sample = torch.randint(0, vocab_size, size=(batch_size, block_size), dtype=torch.int64)\n\n    expected = llama_model(token_sample)\n\n    with EmptyInitOnDevice(device=\"cuda\", dtype=torch.bfloat16):\n        llama_model2 = lit_llama.LLaMA(llama_config)\n    llama_model2.load_state_dict(llama_model.state_dict(keep_vars=True))\n\n    out = llama_model2(token_sample.cuda()).float().cpu()\n    torch.testing.assert_close(out, expected, atol=5e-3, rtol=1e-3)", "\n\ndef copy_adapter_weights(llama_model, orig_llama_model) -> None:\n    # copy the gating parameter\n    for llama_block, orig_llama_block in zip(llama_model.transformer.h, orig_llama_model.layers):\n        if hasattr(llama_block.attn, \"gating_factor\"):\n            llama_block.attn.gating_factor.copy_(orig_llama_block.attention.gate)\n\n    # In the original model, there is one embedding layer for all blocks combined\n    orig_adapter_wte = orig_llama_model.adapter_query.weight.reshape(\n        orig_llama_model.params.adapter_layer, orig_llama_model.params.adapter_len, orig_llama_model.params.dim\n    )\n\n    # In ours, the embedding layer is split across the individual attention layers\n    index = 0\n    for llama_block in llama_model.transformer.h:\n        if hasattr(llama_block.attn, \"adapter_wte\"):\n            llama_block.attn.adapter_wte.weight.copy_(orig_adapter_wte[index])\n            index += 1", "\n\ndef enable_gate(model):\n    for name, param in model.named_parameters():\n        if \"gating_factor\" in name or \"gate\" in name:\n            param.fill_(1)\n\n\n@torch.no_grad()\ndef test_adapter_parity(orig_llama_adapter):\n    \"\"\"Test parity between our implementation of LLaMA-Adapter and the reference code.\"\"\"\n    import lit_llama.adapter as lit_llama\n\n    orig_llama = orig_llama_adapter\n\n    block_size = 32\n    vocab_size = 100\n    n_layer = 2\n    n_head = 4\n    n_embd = 16\n    adapter_prompt_length: int = 10\n    adapter_start_layer: int = 0\n\n    llama_config = lit_llama.LLaMAConfig(\n        block_size=block_size,\n        vocab_size=vocab_size,\n        n_layer=n_layer,\n        n_head=n_head,\n        n_embd=n_embd,\n        adapter_prompt_length=adapter_prompt_length,\n        adapter_start_layer=adapter_start_layer,\n    )\n    orig_llama_config = orig_llama.ModelArgs(\n        dim=n_embd,\n        n_layers=n_layer,\n        n_heads=n_head,\n        vocab_size=vocab_size,\n        norm_eps=1e-5,\n        max_seq_len=block_size,\n        adapter_len=adapter_prompt_length,\n        adapter_layer=(n_layer - adapter_start_layer),\n    )\n\n    batch_size = 3\n    token_sample = torch.randint(\n        0, orig_llama_config.vocab_size, size=(batch_size, orig_llama_config.max_seq_len), dtype=torch.int64\n    )\n\n    llama_model = lit_llama.LLaMA(llama_config)\n    llama_model.apply(llama_model._init_weights)\n    orig_llama_model = orig_llama.Transformer(orig_llama_config)\n\n    copy_weights(llama_model, orig_llama_model)\n    copy_adapter_weights(llama_model, orig_llama_model)\n\n    # make the gate non-zero, otherwise the adapter is disabled and the model\n    # identical to regular LLaMA\n    enable_gate(llama_model)\n    enable_gate(orig_llama_model)\n\n    expected = orig_llama_model(token_sample, 0)\n    out = llama_model(token_sample)\n    assert torch.allclose(out, expected)", "@torch.no_grad()\ndef test_adapter_parity(orig_llama_adapter):\n    \"\"\"Test parity between our implementation of LLaMA-Adapter and the reference code.\"\"\"\n    import lit_llama.adapter as lit_llama\n\n    orig_llama = orig_llama_adapter\n\n    block_size = 32\n    vocab_size = 100\n    n_layer = 2\n    n_head = 4\n    n_embd = 16\n    adapter_prompt_length: int = 10\n    adapter_start_layer: int = 0\n\n    llama_config = lit_llama.LLaMAConfig(\n        block_size=block_size,\n        vocab_size=vocab_size,\n        n_layer=n_layer,\n        n_head=n_head,\n        n_embd=n_embd,\n        adapter_prompt_length=adapter_prompt_length,\n        adapter_start_layer=adapter_start_layer,\n    )\n    orig_llama_config = orig_llama.ModelArgs(\n        dim=n_embd,\n        n_layers=n_layer,\n        n_heads=n_head,\n        vocab_size=vocab_size,\n        norm_eps=1e-5,\n        max_seq_len=block_size,\n        adapter_len=adapter_prompt_length,\n        adapter_layer=(n_layer - adapter_start_layer),\n    )\n\n    batch_size = 3\n    token_sample = torch.randint(\n        0, orig_llama_config.vocab_size, size=(batch_size, orig_llama_config.max_seq_len), dtype=torch.int64\n    )\n\n    llama_model = lit_llama.LLaMA(llama_config)\n    llama_model.apply(llama_model._init_weights)\n    orig_llama_model = orig_llama.Transformer(orig_llama_config)\n\n    copy_weights(llama_model, orig_llama_model)\n    copy_adapter_weights(llama_model, orig_llama_model)\n\n    # make the gate non-zero, otherwise the adapter is disabled and the model\n    # identical to regular LLaMA\n    enable_gate(llama_model)\n    enable_gate(orig_llama_model)\n\n    expected = orig_llama_model(token_sample, 0)\n    out = llama_model(token_sample)\n    assert torch.allclose(out, expected)", "\n\n@pytest.mark.skipif(sys.platform in (\"win32\", \"darwin\"), reason=\"torch.compile not supported on this platform\")\ndef test_model_compile(lit_llama):\n    llama_config = lit_llama.LLaMAConfig(block_size=8, vocab_size=8, n_layer=2, n_head=2, n_embd=4)\n    model = lit_llama.LLaMA(llama_config)\n    model.apply(model._init_weights)\n\n    model = torch.compile(model)\n\n    sample = torch.randint(model.config.vocab_size, size=(2, model.config.block_size), dtype=torch.int64)\n    for _ in range(3):\n        _ = model(sample)", ""]}
{"filename": "tests/test_packed_dataset.py", "chunked_list": ["import os\nfrom unittest.mock import MagicMock\nimport requests\n\nfrom torch.utils.data import IterableDataset\n\n\ndef train_tokenizer(destination_path):\n    destination_path.mkdir(parents=True, exist_ok=True)\n\n    # download the tiny shakespeare dataset\n    input_file_path = destination_path / \"input.txt\"\n    if not input_file_path.exists():\n        data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n        with open(input_file_path, \"w\") as f:\n            f.write(requests.get(data_url).text)\n\n    from lit_llama import Tokenizer\n    Tokenizer.train(\n        input=input_file_path,\n        destination=destination_path,\n        vocab_size=100,\n    )\n\n    return destination_path / \"tokenizer.model\"", "\n\ndef test_packed_dataset(tmp_path):\n    tokenizer_path = train_tokenizer(tmp_path)\n\n    from lit_llama import Tokenizer\n    tokenizer = Tokenizer(tokenizer_path)\n\n    texts = [\n      \"The moment of truth is upon us.\",\n      \"Time to open the fridge.\"\n    ]\n\n    from lit_llama.packed_dataset import PackedDatasetBuilder, PackedDataset, HDR_SIZE\n\n    block_size = 10\n    n_blocks = 2\n    chunk_size = block_size * n_blocks\n\n    builder = PackedDatasetBuilder(\n        outdir=tmp_path,\n        prefix=\"packed_dataset\",\n        chunk_size=chunk_size,\n        sep_token=tokenizer.bos_id,\n        dtype=\"auto\",\n        vocab_size=100,\n    )\n\n    text_ids = []\n\n    for text in texts:\n        text_ids = tokenizer.encode(text)\n        assert text_ids[0] == tokenizer.bos_id\n        builder.add_array(text_ids)\n\n    filenames = builder.filenames\n\n    assert len(filenames) == 2\n    assert os.path.basename(filenames[0]) == \"packed_dataset_0000000000.bin\"\n    assert os.path.basename(filenames[1]) == \"packed_dataset_0000000001.bin\"\n\n    import numpy as np\n\n    ex_tokenized = [\n        tokenizer.encode(text).numpy().astype(builder.dtype)\n        for text in texts\n    ]\n    ex_tokenized = np.concatenate(ex_tokenized)\n    ex_tokenized = ex_tokenized[:2 * chunk_size]\n\n    for filename, el in zip(filenames, np.array_split(ex_tokenized, 2)):\n        mmap = np.memmap(filename, mode=\"r\", order=\"C\", offset=HDR_SIZE)\n        count = len(mmap) // np.dtype(builder.dtype).itemsize\n        arr = np.frombuffer(\n            mmap, dtype=builder.dtype, count=count, offset=0\n        )\n        where_bos = np.where(arr == tokenizer.bos_id)\n        # we expect two BOS tokens, one per file\n        assert len(where_bos) == 1\n        assert np.array_equal(arr, el)\n\n    dataset = PackedDataset(filenames=filenames, n_chunks=2, block_size=block_size, shuffle=False)\n\n    ex_split = np.array_split(ex_tokenized, ex_tokenized.shape[0] // block_size)\n\n    for item, el in zip(dataset, ex_split):\n        assert np.array_equal(item, el)\n\n    dataset = PackedDataset(filenames=filenames, n_chunks=2, block_size=block_size, seed=12345)\n\n    for i, item in enumerate(dataset):\n        block_idxs = iter(dataset)._block_idxs\n        assert np.array_equal(item, ex_split[block_idxs[i]])\n\n    dataset = PackedDataset(filenames=filenames, n_chunks=2, block_size=block_size, seed=12345, wrap=True)\n\n    for i, item in enumerate(dataset):\n        if i > 24:\n            break\n\n    dataset = PackedDataset(filenames=filenames, n_chunks=1, block_size=block_size, seed=12345)\n\n    for i, item in enumerate(dataset):\n        block_idxs = iter(dataset)._block_idxs\n        chunk_idx = i // n_blocks * n_blocks\n        assert np.array_equal(item, ex_split[chunk_idx + block_idxs[i % n_blocks]])\n\n    block_size_ = block_size // 2\n    ex_split = np.array_split(ex_tokenized, ex_tokenized.shape[0] // block_size_)\n    dataset = PackedDataset(filenames=filenames, n_chunks=2, block_size=block_size_, seed=12345)\n\n    for i, item in enumerate(dataset):\n        block_idxs = iter(dataset)._block_idxs\n        assert np.array_equal(item, ex_split[block_idxs[i]])\n\n    block_size_ = block_size // 3\n    n_chunks = 2\n    ex_chunks = np.split(ex_tokenized, n_chunks)\n    n_splits = ex_tokenized.shape[0] // n_chunks // block_size_\n    ex_splits = [np.split(el[:n_splits * block_size_], n_splits) for el in ex_chunks]\n    ex_split = sum(ex_splits, [])\n\n    dataset = PackedDataset(filenames=filenames, n_chunks=n_chunks, block_size=block_size_, seed=12345)\n\n    for i, item in enumerate(dataset):\n        block_idxs = iter(dataset)._block_idxs\n        assert np.array_equal(item, ex_split[block_idxs[i]])", "\n\nclass SimpleDataset(IterableDataset):\n    def __init__(self, start, end):\n        super().__init__()\n        self._start = start\n        self._end = end\n\n    def __iter__(self):\n        return iter(range(self._start, self._end))", "        \n\ndef test_combined_dataset(tmp_path):\n    from lit_llama.packed_dataset import CombinedDataset\n\n    dataset1 = SimpleDataset(0, 10)\n    dataset2 = SimpleDataset(10, 20)\n    dataset = CombinedDataset(datasets=[dataset1, dataset2], weights=[1.0, 0.0], seed=12345)\n\n    res = [el for el in dataset]\n    assert res == list(range(0, 10))\n\n    dataset1 = SimpleDataset(0, 10)\n    dataset2 = SimpleDataset(10, 20)\n    dataset = CombinedDataset(datasets=[dataset1, dataset2], weights=[0.0, 1.0], seed=12345)\n\n    res = [el for el in dataset]\n    assert res == list(range(10, 20))\n\n    dataset1 = SimpleDataset(0, 10)\n    dataset2 = SimpleDataset(10, 20)\n    dataset = CombinedDataset(datasets=[dataset1, dataset2], weights=[0.5, 0.5], seed=12345)\n\n    res = [el for el in dataset]\n    assert 9 in res or 19 in res\n    if len(res) > 10:\n        assert 0 in res and 10 in res", "\n\ndef test_sharded_packed_dataset(monkeypatch):\n    import lit_llama.packed_dataset\n    from lit_llama.packed_dataset import PackedDataset\n\n    dataset_iterator_mock = MagicMock()\n    monkeypatch.setattr(lit_llama.packed_dataset, \"PackedDatasetIterator\", dataset_iterator_mock)\n    filenames = [str(i) for i in range(10)]\n\n    # world_size = 1, rank = 0\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == filenames\n    dataset_iterator_mock.reset_mock()\n    # world_size = 2, rank = 0\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2, num_processes=2, process_rank=0))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == [\"0\", \"2\", \"4\", \"6\", \"8\"]\n    dataset_iterator_mock.reset_mock()\n    # world_size = 2, rank = 1\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2, num_processes=2, process_rank=1))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == [\"1\", \"3\", \"5\", \"7\", \"9\"]\n    dataset_iterator_mock.reset_mock()\n    \n    # world_size = 3, rank = 0 (dataset size not cleanly divisible by world size)\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2, num_processes=3, process_rank=0))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == [\"0\", \"3\", \"6\"]\n    dataset_iterator_mock.reset_mock()\n    # world_size = 3, rank = 1 (dataset size not cleanly divisible by world size)\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2, num_processes=3, process_rank=1))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == [\"1\", \"4\", \"7\"]\n    dataset_iterator_mock.reset_mock()\n    # world_size = 3, rank = 2 (dataset size not cleanly divisible by world size)\n    iter(PackedDataset(filenames=filenames, n_chunks=2, block_size=2, num_processes=3, process_rank=2))\n    assert dataset_iterator_mock.call_args[1][\"filenames\"] == [\"2\", \"5\", \"8\"]", ""]}
{"filename": "tests/test_utils.py", "chunked_list": ["import tempfile\nimport pathlib\n\nimport torch\n\n\nclass ATensor(torch.Tensor):\n    pass\n\n\ndef test_lazy_load_basic(lit_llama):\n    import lit_llama.utils\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        m = torch.nn.Linear(5, 3)\n        path = pathlib.Path(tmpdirname)\n        fn = str(path / \"test.pt\")\n        torch.save(m.state_dict(), fn)\n        with lit_llama.utils.lazy_load(fn) as sd_lazy:\n            assert \"NotYetLoadedTensor\" in str(next(iter(sd_lazy.values())))\n            m2 = torch.nn.Linear(5, 3)\n            m2.load_state_dict(sd_lazy)\n\n        x = torch.randn(2, 5)\n        actual = m2(x)\n        expected = m(x)\n        torch.testing.assert_close(actual, expected)", "\n\ndef test_lazy_load_basic(lit_llama):\n    import lit_llama.utils\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        m = torch.nn.Linear(5, 3)\n        path = pathlib.Path(tmpdirname)\n        fn = str(path / \"test.pt\")\n        torch.save(m.state_dict(), fn)\n        with lit_llama.utils.lazy_load(fn) as sd_lazy:\n            assert \"NotYetLoadedTensor\" in str(next(iter(sd_lazy.values())))\n            m2 = torch.nn.Linear(5, 3)\n            m2.load_state_dict(sd_lazy)\n\n        x = torch.randn(2, 5)\n        actual = m2(x)\n        expected = m(x)\n        torch.testing.assert_close(actual, expected)", "\n\ndef test_lazy_load_subclass(lit_llama):\n    import lit_llama.utils\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        path = pathlib.Path(tmpdirname)\n        fn = str(path / \"test.pt\")\n        t = torch.randn(2, 3)[:, 1:]\n        sd = {\n            1: t,\n            2: torch.nn.Parameter(t),\n            3: torch.Tensor._make_subclass(ATensor, t),\n        }\n        torch.save(sd, fn)\n        with lit_llama.utils.lazy_load(fn) as sd_lazy:\n            for k in sd.keys():\n                actual = sd_lazy[k]\n                expected = sd[k]\n                torch.testing.assert_close(actual._load_tensor(), expected)", "\n\ndef test_incremental_write(tmp_path, lit_llama):\n    import lit_llama.utils\n\n    sd = {str(k): torch.randn(5, 10) for k in range(3)}\n    sd_expected = {k: v.clone() for k, v in sd.items()}\n    fn = str(tmp_path / \"test.pt\")\n    with lit_llama.utils.incremental_save(fn) as f:\n        sd[\"0\"] = f.store_early(sd[\"0\"])\n        sd[\"2\"] = f.store_early(sd[\"2\"])\n        f.save(sd)\n    sd_actual = torch.load(fn)\n    assert sd_actual.keys() == sd_expected.keys()\n    for k, v_expected in sd_expected.items():\n        v_actual = sd_actual[k]\n        torch.testing.assert_close(v_expected, v_actual)", "\n\ndef test_find_multiple(lit_llama):\n    from lit_llama.utils import find_multiple\n\n    assert find_multiple(17, 5) == 20\n    assert find_multiple(30, 7) == 35\n    assert find_multiple(10, 2) == 10\n    assert find_multiple(5, 10) == 10\n", ""]}
{"filename": "tests/test_rmsnorm.py", "chunked_list": ["import torch\n\n\n@torch.no_grad()\ndef test_rmsnorm(lit_llama, orig_llama) -> None:\n    block_size = 16\n    vocab_size = 16\n\n    sample = torch.rand(size=(2, block_size, vocab_size), dtype=torch.float32)\n\n    eps = 1e-6\n    orig_llama_rmsnorm = orig_llama.RMSNorm(vocab_size, eps=eps)(sample)\n    llama_rmsnorm = lit_llama.RMSNorm(vocab_size, eps=eps)(sample)\n\n    assert torch.allclose(orig_llama_rmsnorm, llama_rmsnorm)", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import sys\nfrom pathlib import Path\n\nimport pytest\n\nwd = Path(__file__).parent.parent.absolute()\n\n\n@pytest.fixture()\ndef orig_llama():\n    sys.path.append(str(wd))\n\n    from scripts.download import download_original\n\n    download_original(wd)\n\n    import original_model\n\n    return original_model", "@pytest.fixture()\ndef orig_llama():\n    sys.path.append(str(wd))\n\n    from scripts.download import download_original\n\n    download_original(wd)\n\n    import original_model\n\n    return original_model", "\n\n@pytest.fixture()\ndef orig_llama_adapter():\n    sys.path.append(str(wd))\n\n    from scripts.download import download_original\n\n    download_original(wd)\n\n    import original_adapter\n\n    return original_adapter", "\n\n@pytest.fixture()\ndef lit_llama():\n    # this adds support for running tests without the package installed\n    sys.path.append(str(wd))\n\n    import lit_llama\n\n    return lit_llama", ""]}
{"filename": "quantize/gptq.py", "chunked_list": ["# This adapts GPTQ's quantization process: https://github.com/IST-DASLab/gptq/\n# E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\n# portions copyright by the authors licensed under the Apache License 2.0\nimport gc\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch", "\nimport torch\nfrom datasets import load_dataset\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.quantization import GPTQQuantizer", "from lit_llama import LLaMA, Tokenizer\nfrom lit_llama.quantization import GPTQQuantizer\nfrom lit_llama.utils import EmptyInitOnDevice, llama_model_lookup\n\n\ndef get_sample_data():\n    traindata = load_dataset(\n        \"allenai/c4\",\n        \"allenai--c4\",\n        data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\n        split=\"train\",\n    )\n    # heuristic for the data size?\n    txt = \"\\n\".join(\n        traindata[i][\"text\"] for i in torch.randperm(len(traindata))[:1000].tolist()\n    )\n    return txt", "\n\n@torch.no_grad()\ndef llama_blockwise_quantization(\n    model, sample_inputs, working_device, *, bits=4, groupsize=-1\n):\n    \"\"\"\n    This is the classic post-training quantization of all linear layers.\n    We quantize in order, i.e. when observing the inputs, we use the outputs of the previously quantized layers rather\n    than doing them all at once.\n    \"\"\"\n    print(model)\n    print(model.config)\n\n    print(\"Getting inputs for first block\")\n    model.transformer.wte.to(working_device)\n    sample_inputs = sample_inputs.to(working_device)\n    inps = model.transformer.wte(sample_inputs)\n    model.transformer.wte.to(\"cpu\")\n    torch.cuda.empty_cache()\n\n    rope_cache = model.build_rope_cache(sample_inputs)\n    mask_cache = model.build_mask_cache(sample_inputs)\n\n    print(\"Starting to quantize blocks\")\n    outs = torch.zeros_like(inps)\n\n    # better than relying on enumeration? originally the code bundled\n    # the two mlp fc layers\n    # we could automate this with a lot of hooks and another iteration\n    submodules_to_process = [\n        \"attn.c_attn\",\n        \"attn.c_proj\",\n        \"mlp.c_fc1\",\n        \"mlp.c_fc2\",\n        \"mlp.c_proj\",\n    ]\n\n    for i, block in enumerate(model.transformer.h):\n        block.to(working_device)\n\n        for name in submodules_to_process:\n            print(i, name, end=\" \")\n            t0 = time.perf_counter()\n            print(\"collecting stats\", end=\" \")\n            sys.stdout.flush()\n            module = block.get_submodule(name)\n\n            gptq = GPTQQuantizer(\n                module,\n                bits=bits,\n                groupsize=groupsize,\n                actorder=(groupsize == -1),\n            )\n            handle = module.register_forward_hook(gptq.collect_input_stats)\n            for j in range(inps.size(0)):\n                outs[j : j + 1], _ = block(\n                    inps[j : j + 1],\n                    rope=rope_cache,\n                    mask=mask_cache,\n                    max_seq_length=model.config.block_size\n                )\n\n            handle.remove()\n\n            print(\"quantizing\", end=\" \")\n            sys.stdout.flush()\n            q_module, error = gptq.quantize()\n\n            # replace the linear module with the quantized module\n            pname, dname = name.rsplit(\".\", 1)\n            setattr(block.get_submodule(pname), dname, q_module)\n\n            # cleanup in an attempt to not run out of memory\n            del gptq\n            gc.collect()\n            torch.cuda.empty_cache()\n            t1 = time.perf_counter()\n            print(f\"time {int(t1 - t0 + 0.5)}s quantization error {error:.1f}\")\n\n        for j in range(inps.size(0)):\n            outs[j : j + 1], _ = block(\n                inps[j : j + 1],\n                rope=rope_cache,\n                mask=mask_cache,\n                max_seq_length=model.config.block_size\n            )\n\n        block.cpu()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # the outputs are the next block's inputs and we'll reuse the old inputs\n        inps, outs = outs, inps\n\n    model.transformer.ln_f.to(working_device)\n    for j in range(inps.size(0)):\n        outs[j : j + 1] = model.transformer.ln_f(inps[j : j + 1])\n    model.transformer.ln_f.to(\"cpu\")\n    inps, outs = outs, inps\n\n    model.lm_head.to(working_device)\n    gptq = GPTQQuantizer(\n        model.lm_head,\n        bits=bits,\n        groupsize=groupsize,\n        actorder=(groupsize == -1),\n    )\n    handle = model.lm_head.register_forward_hook(gptq.collect_input_stats)\n    for j in range(inps.size(0)):\n        model.lm_head(inps[j : j + 1])\n    handle.remove()\n    q_module, error = gptq.quantize()\n    model.lm_head = q_module\n    model.lm_head.to(\"cpu\")", "\n\ndef main(\n    *,\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    output_path: Optional[Path] = None,\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    n_samples: int = 128,\n    dtype: str = \"float32\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        checkpoint_path: The checkpoint path to load.\n        output_path: Path to write the quantized model's state dict to.\n        tokenizer_path: The tokenizer path to load.\n        n_samples: Number of example inputs to use for statistics (default: 128)\n        dtype: The dtype to use to load the model.\n        quantize: Mode to quantize the model to:\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n            Note that ``\"llm.int8\"```does not need a quantization step.\n    \"\"\"\n    assert checkpoint_path.is_file()\n    assert tokenizer_path.is_file()\n    if output_path is None:\n        output_path = checkpoint_path.parent / \"llama-gptq.4bit.pth\"\n    assert output_path.parent.is_dir() and (not output_path.exists() or output_path.is_file())\n\n    device = \"cuda\"\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    if quantize == \"gptq.int4\":\n        bits = 4\n    elif quantize == \"gptq.int8\":\n        bits = 8\n    else:\n        raise RuntimeError(f\"unknown/unsupported quantization mode {quantize}\")\n\n    # we avoid loading the entire model on the GPU and do this block by block\n    with EmptyInitOnDevice(\n        device=\"cpu\",\n        dtype=dtype,\n    ):\n        print(\"Loading model ...\", file=sys.stderr)\n        t0 = time.time()\n        checkpoint = torch.load(checkpoint_path)\n        name = llama_model_lookup(checkpoint)\n        model = LLaMA.from_name(name)\n        model.load_state_dict(checkpoint)\n        print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    test_string = get_sample_data()\n    encoded_text = tokenizer.encode(\n        test_string,\n        bos=True,\n        eos=False,\n    )\n    block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)\n    encoded_text = encoded_text[: n_samples * block_size].reshape(n_samples, block_size)\n\n    t0 = time.perf_counter()\n    llama_blockwise_quantization(model, encoded_text, device, bits=bits)\n    t = time.perf_counter() - t0\n\n    print(\n        f\"\\n\\nTime for quantization: {t:.02f} sec total\",\n        file=sys.stderr,\n    )\n    print(\n        f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\",\n        file=sys.stderr,\n    )\n\n    torch.save(model.state_dict(), output_path)", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    CLI(main)\n"]}
{"filename": "lit_llama/quantization.py", "chunked_list": ["import os\nfrom contextlib import contextmanager\nimport warnings\nimport math\n\nimport torch\n\n# configuration for bitsandbytes before import\nos.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\nwarnings.filterwarnings(", "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\",\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n)\nwarnings.filterwarnings(", ")\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\",\n)\n\ntry:\n    import bitsandbytes as bnb  # noqa: E402\nexcept:\n    bnb = None", "\ntry:\n    import triton  # noqa: E402\n    import triton.language as tl  # noqa: E402\nexcept:\n    triton = None\n\nif bnb is not None:\n\n    class Linear8bitLt(bnb.nn.Linear8bitLt):\n        \"\"\"Wraps `bnb.nn.Linear8bitLt` and enables instantiation directly on the device and\n        re-quantizaton when loading the state dict.\n\n\n        This should only be used for inference. For training, use `bnb.nn.Linear8bitLt` directly.\n        \"\"\"\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs, has_fp16_weights=False, threshold=6.0)\n            # We quantize the initial weight here so we don't end up filling the device\n            # memory with float32 weights which could lead to OOM.\n            self._quantize_weight(self.weight.data)\n\n        def _load_from_state_dict(self, local_state_dict, *args, **kwargs):\n            # There is only one key that ends with `*.weight`, the other one is the bias\n            weight_key = next(\n                (name for name in local_state_dict.keys() if name.endswith(\"weight\")),\n                None,\n            )\n            if weight_key is None:\n                return\n\n            # Load the weight from the state dict and re-quantize it\n            weight = local_state_dict.pop(weight_key)\n            self._quantize_weight(weight)\n\n            # If there is a bias, let nn.Module load it\n            if local_state_dict:\n                super()._load_from_state_dict(local_state_dict, *args, **kwargs)\n\n        def _quantize_weight(self, weight: torch.Tensor) -> None:\n            # This code is taken and adapted from `bnb.nn.Int8Params.cuda()`\n            B = weight.contiguous().half().cuda()\n            CB, CBt, SCB, SCBt, coo_tensorB = bnb.functional.double_quant(B)\n            del CBt\n            del SCBt\n            self.weight.data = CB\n            setattr(self.weight, \"CB\", CB)\n            setattr(self.weight, \"SCB\", SCB)", "\n\nif triton is not None:\n    # This is adapted from the OpenAI Triton matmul example.\n    @triton.autotune(\n        configs=[\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 128,\n                    \"BLOCK_SIZE_N\": 256,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=3,\n                num_warps=8,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 256,\n                    \"BLOCK_SIZE_N\": 128,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=3,\n                num_warps=8,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 256,\n                    \"BLOCK_SIZE_N\": 64,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 64,\n                    \"BLOCK_SIZE_N\": 256,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 128,\n                    \"BLOCK_SIZE_N\": 128,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 128,\n                    \"BLOCK_SIZE_N\": 64,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 64,\n                    \"BLOCK_SIZE_N\": 128,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 128,\n                    \"BLOCK_SIZE_N\": 32,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=4,\n                num_warps=4,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 64,\n                    \"BLOCK_SIZE_N\": 32,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=5,\n                num_warps=2,\n            ),\n            triton.Config(\n                {\n                    \"BLOCK_SIZE_M\": 32,\n                    \"BLOCK_SIZE_N\": 64,\n                    \"BLOCK_SIZE_K\": 32,\n                    \"GROUP_SIZE_M\": 8,\n                },\n                num_stages=5,\n                num_warps=2,\n            ),\n        ],\n        key=[\"M\", \"N\", \"K\"],\n    )\n    @triton.jit\n    def linear_kernel_4bit_weight(\n        # Pointers to matrices\n        a_ptr,\n        b_ptr,\n        c_ptr,\n        bscales_ptr,\n        bzeros_ptr,\n        # bdequant,\n        # Matrix dimensions\n        M,\n        N,\n        K,\n        # The stride variables represent how much to increase the ptr by when moving by 1\n        # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n        # by to get the element one row down (A has M rows)\n        stride_am,\n        stride_ak,\n        stride_bk,\n        stride_bn,\n        stride_cm,\n        stride_cn,\n        # Meta-parameters\n        BLOCK_SIZE_M: tl.constexpr,\n        BLOCK_SIZE_N: tl.constexpr,\n        BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_M: tl.constexpr,\n    ):\n        \"\"\"Kernel for computing the matmul C = A x B.T.\n        A has shape (M, K), B has shape (N, K) and C has shape (M, N)\n        \"\"\"\n        # -----------------------------------------------------------\n        # Map program ids `pid` to the block of C it should compute.\n        # This is done in a grouped ordering to promote L2 data reuse\n        # See above `L2 Cache Optimizations` section for details\n        pid = tl.program_id(axis=0)\n        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n        group_id = pid // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + (pid % group_size_m)\n        pid_n = (pid % num_pid_in_group) // group_size_m\n\n        # ----------------------------------------------------------\n        # Create pointers for the first blocks of A and B.\n        # We will advance this pointer as we move in the K direction\n        # and accumulate\n        # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n        # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n        # see above `Pointer Arithmetics` section for details\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        a_mask = offs_am[:, None] < M\n        b_mask = offs_bn[None, :] < N\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (\n            (offs_k[:, None] // 2) * stride_bk + offs_bn[None, :] * stride_bn\n        )\n\n        bscales_ptrs = bscales_ptr + offs_bn[None, :]\n        bzeros_ptrs = bzeros_ptr + offs_bn[None, :]\n\n        scale = tl.load(bscales_ptrs)\n        zero = tl.load(bzeros_ptrs)\n        # -----------------------------------------------------------\n        # Iterate to compute a block of the C matrix\n        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n        # of fp32 values for higher accuracy.\n        # `accumulator` will be converted back to fp16 after the loop\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, K, BLOCK_SIZE_K):\n            # wasteful as it is to load everything twice, my attempts at avoiding it lead to slower code\n            b12 = tl.load(b_ptrs, mask=b_mask)\n            # Note that for simplicity, we don't apply a mask in K here.\n            a = tl.load(a_ptrs, mask=a_mask).to(tl.float32)\n            b = (\n                ((b12.to(tl.uint8) >> ((offs_k[:, None] % 2) * 4)) & 0xF).to(tl.float32)\n                - zero\n            ) * scale\n            accumulator += tl.dot(a, b)\n\n            # Advance the ptrs to the next K block\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += (BLOCK_SIZE_K // 2) * stride_bk\n        c = accumulator\n\n        # -----------------------------------------------------------\n        # Write back the block of the output matrix C\n        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n        tl.store(c_ptrs, c, mask=c_mask)\n\n    def qlinear_4bit_weight(inp, weight, scales, zeros):\n        weight = weight.t().contiguous()\n        c_shape = inp.shape[:-1] + weight.shape[-1:]\n        inp = inp.reshape(-1, inp.shape[-1]).contiguous()\n        # we pad the input to amortize triton compilation cost better\n        PAD_TO = 256\n        if inp.shape[0] % PAD_TO != 0:\n            c_crop = inp.shape[0]\n            new_inp_shape0 = inp.shape[0] + PAD_TO - inp.shape[0] % PAD_TO\n            inp2 = inp.new_empty((new_inp_shape0, inp.shape[1]))\n            inp2[: inp.shape[0]] = inp\n            inp2[inp.shape[0] :].zero_()\n            inp = inp2\n        else:\n            c_crop = None\n\n        assert inp.shape[1] == weight.shape[0] * 2, \"incompatible dimensions\"\n\n        assert scales.shape == (weight.shape[1], 1)\n        assert zeros.shape == (weight.shape[1], 1)\n        scales = scales.contiguous()\n        zeros = zeros.contiguous()\n        K, N = weight.shape\n        M, K = inp.shape\n        assert (\n            K % 32 == 0\n        ), \"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\"\n        # allocates output\n        c = torch.empty((M, N), device=inp.device, dtype=inp.dtype)\n        # 1D launch kernel where each block gets its own program.\n        grid = lambda META: (\n            triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n        )\n        linear_kernel_4bit_weight[grid](\n            inp,\n            weight,\n            c,\n            scales,\n            zeros,\n            M,\n            N,\n            K,\n            inp.stride(0),\n            inp.stride(1),\n            weight.stride(0),\n            weight.stride(1),\n            c.stride(0),\n            c.stride(1),\n        )\n        return c[:c_crop].reshape(c_shape)\n\nelse:\n    qlinear_4bit_weight = None", "\n\n# for correctness but with terrible perf\nclass ColBlockQuantizedLinear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias: bool, *, bits, tile_cols):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.tile_cols = tile_cols if tile_cols != -1 else self.in_features\n        self.bits = bits\n        self.entries_per_byte = 8 // bits\n        assert self.entries_per_byte > 0 and self.entries_per_byte * self.bits == 8\n        assert in_features % self.entries_per_byte == 0\n        self.register_buffer(\n            \"quant_weight\",\n            torch.empty(\n                (self.out_features, self.in_features // self.entries_per_byte),\n                dtype=torch.uint8,\n            )\n            .t()\n            .contiguous()\n            .t(),\n        )\n        self.register_buffer(\n            \"scales\",\n            torch.empty(\n                (\n                    self.out_features,\n                    (self.in_features + self.tile_cols - 1) // self.tile_cols,\n                )\n            ),\n        )\n        self.register_buffer(\"zeros\", torch.empty_like(self.scales))\n        assert isinstance(bias, bool)\n        if bias:\n            self.register_buffer(\"bias\", torch.empty((self.out_features,)))\n        else:\n            self.register_buffer(\"bias\", None)\n\n    def pack_weight(self, weight):\n        weight = weight.to(device=self.quant_weight.device, copy=True)\n        for j in range(self.scales.size(1)):\n            weight[:, j * self.tile_cols : (j + 1) * self.tile_cols] /= self.scales[\n                :, j : j + 1\n            ]\n            weight[:, j * self.tile_cols : (j + 1) * self.tile_cols] += self.zeros[\n                :, j : j + 1\n            ]\n        weight = weight.clamp_(min=0, max=2**self.bits - 1).to(dtype=torch.uint8)\n        self.quant_weight.zero_()\n        for nr in range(self.entries_per_byte):\n            self.quant_weight += weight[:, nr :: self.entries_per_byte] << (\n                nr * self.bits\n            )\n\n    def get_weight(self, dtype=torch.float):\n        weight = torch.empty(\n            (self.out_features, self.in_features),\n            device=self.quant_weight.device,\n            dtype=dtype,\n        )\n        mask = (1 << self.bits) - 1\n        for nr in range(self.entries_per_byte):\n            weight[:, nr :: self.entries_per_byte] = (\n                (self.quant_weight >> (nr * self.bits)) & mask\n            ).float()\n        self.quant_weight.to(dtype)\n        for j in range(self.scales.size(1)):\n            weight[:, j * self.tile_cols : (j + 1) * self.tile_cols] -= self.zeros[\n                :, j : j + 1\n            ]\n            weight[:, j * self.tile_cols : (j + 1) * self.tile_cols] *= self.scales[\n                :, j : j + 1\n            ]\n        return weight\n\n    def forward(self, inp):\n        if (\n            triton is not None\n            and self.bits == 4\n            and self.quant_weight.device.type == \"cuda\"\n            and self.zeros.shape[1] == 1\n            and self.quant_weight.shape[1] % 32 == 0\n        ):\n            return qlinear_4bit_weight(inp, self.quant_weight, self.scales, self.zeros)\n        weight = self.get_weight(dtype=inp.dtype)\n        return torch.nn.functional.linear(inp, weight, self.bias)", "\n\nclass GPTQQuantizer:\n    # The algorithm and code has been taken from  https://github.com/IST-DASLab/gptq/\n    # E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\n    # portions copyright by the authors licensed under the Apache License 2.0\n    # All errors are our own.\n\n    def __init__(\n        self,\n        linear_module,\n        *,\n        bits,\n        perchannel=True,\n        sym=False,\n        blocksize=128,\n        percdamp=0.01,\n        groupsize=-1,\n        actorder=False\n    ):\n        assert isinstance(linear_module, torch.nn.Linear)\n\n        self.linear_module = linear_module\n        self.dev = self.linear_module.weight.device\n        self.rows = linear_module.weight.shape[0]\n        self.columns = linear_module.weight.shape[1]\n        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n        self.nsamples = 0\n        self.bits = bits\n        self.maxq = 2**bits - 1\n        self.perchannel = perchannel\n        self.sym = sym\n        self.blocksize = blocksize\n        self.percdamp = percdamp\n        self.groupsize = groupsize\n        self.actorder = actorder\n        self.tile_cols = self.columns if groupsize == -1 else groupsize\n        self.scales = torch.zeros(\n            (self.rows, (self.columns + self.tile_cols - 1) // self.tile_cols),\n            dtype=self.linear_module.weight.dtype,\n            device=self.dev,\n        )\n        self.zeros = torch.zeros_like(self.scales)\n        assert not (\n            self.actorder and self.groupsize != -1\n        ), \"The permutation trick does not work for grouped quantization\"\n\n    @staticmethod\n    def quantize_weight(x, scale, zero, maxq):\n        q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n        x_rec = scale * (q - zero)\n        return x_rec\n\n    def find_params_weight(self, x):\n        dev = x.device\n\n        shape = x.shape\n        if self.perchannel:\n            x = x.flatten(1)\n        else:\n            x = x.flatten().unsqueeze(0)\n\n        tmp = torch.zeros(x.shape[0], device=dev)\n        xmin = torch.minimum(x.min(1)[0], tmp)\n        xmax = torch.maximum(x.max(1)[0], tmp)\n\n        if self.sym:\n            xmax = torch.maximum(torch.abs(xmin), xmax)\n            tmp = xmin < 0\n            if torch.any(tmp):\n                xmin[tmp] = -xmax[tmp]\n        tmp = (xmin == 0) & (xmax == 0)\n        xmin[tmp] = -1\n        xmax[tmp] = +1\n\n        scale = (xmax - xmin) / self.maxq\n        if self.sym:\n            zero = torch.full_like(scale, (self.maxq + 1) / 2)\n        else:\n            zero = torch.round(-xmin / scale)\n\n        if not self.perchannel:\n            tmp = shape[0]\n            scale = scale.repeat(tmp)\n            zero = zero.repeat(tmp)\n\n        shape = [-1] + [1] * (len(shape) - 1)\n        scale = scale.reshape(shape)\n        zero = zero.reshape(shape)\n        return scale, zero\n\n    def collect_input_stats(self, _1, inp, _2):\n        inp = inp[0].detach()\n        self.last_inp = inp\n        if len(inp.shape) == 2:\n            inp = inp.unsqueeze(0)\n        tmp = inp.shape[0]\n        if len(inp.shape) == 3:\n            inp = inp.reshape((-1, inp.shape[-1]))\n        inp = inp.t()\n        self.H *= self.nsamples / (self.nsamples + tmp)\n        self.nsamples += tmp\n        # inp = inp.float()\n        inp = math.sqrt(2 / self.nsamples) * inp.float()\n        # self.H += 2 / self.nsamples * inp.matmul(inp.t())\n        self.H += inp.matmul(inp.t())\n\n    def quantize(self):\n        W = self.linear_module.weight.detach().to(dtype=torch.float, copy=True)\n\n        scale, zero = self.find_params_weight(W)\n        self.scales[:] = scale\n        self.zeros[:] = zero\n\n        H = self.H\n        del self.H\n        dead = torch.diag(H) == 0\n        H[dead, dead] = 1\n        W[:, dead] = 0\n        if self.actorder:\n            perm = torch.argsort(torch.diag(H), descending=True)\n            W = W[:, perm]\n            H = H[perm][:, perm]\n\n        Losses = torch.zeros_like(W)\n        Q = torch.zeros_like(W)\n\n        damp = self.percdamp * torch.mean(torch.diag(H))\n        diag = torch.arange(self.columns, device=self.dev)\n        H[diag, diag] += damp\n        H = torch.linalg.cholesky(H)\n        H = torch.cholesky_inverse(H)\n        H = torch.linalg.cholesky(H, upper=True)\n        Hinv = H\n\n        for i1 in range(0, self.columns, self.blocksize):\n            i2 = min(i1 + self.blocksize, self.columns)\n            count = i2 - i1\n\n            W1 = W[:, i1:i2].clone()\n            Q1 = torch.zeros_like(W1)\n            Err1 = torch.zeros_like(W1)\n            Losses1 = torch.zeros_like(W1)\n            Hinv1 = Hinv[i1:i2, i1:i2]\n\n            for i in range(count):\n                w = W1[:, i]\n                d = Hinv1[i, i]\n\n                if self.groupsize != -1:\n                    if (i1 + i) % self.groupsize == 0:\n                        scale, zero = self.find_params_weight(\n                            W[:, (i1 + i) : (i1 + i + self.groupsize)]\n                        )\n                        self.scales[:, (i1 + i) // self.groupsize] = scale\n                        self.zeros[:, (i1 + i) // self.groupsize] = zero\n\n                q = self.quantize_weight(w.unsqueeze(1), scale, zero, self.maxq)\n                q = q.squeeze(1)\n                assert q.dim() == 1\n                Q1[:, i] = q\n                Losses1[:, i] = (w - q) ** 2 / d**2\n\n                err1 = (w - q) / d\n                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n                Err1[:, i] = err1\n\n            Q[:, i1:i2] = Q1\n            Losses[:, i1:i2] = Losses1 / 2\n\n            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n\n        if self.actorder:\n            invperm = torch.argsort(perm)\n            Q = Q[:, invperm]\n\n        weight = Q.reshape(self.linear_module.weight.shape).to(\n            self.linear_module.weight.data.dtype\n        )\n        error = torch.sum(Losses).item()\n\n        q_module = ColBlockQuantizedLinear(\n            self.linear_module.in_features,\n            self.linear_module.out_features,\n            self.linear_module.bias is not None,\n            bits=self.bits,\n            tile_cols=self.groupsize,\n        ).to(self.dev)\n        q_module.scales = self.scales\n        q_module.zeros = self.zeros\n        q_module.pack_weight(weight)\n        q_module.bias = self.linear_module.bias\n        return q_module, error", ""]}
{"filename": "lit_llama/model.py", "chunked_list": ["\"\"\"Full definition of a LLaMA Language Model, all of it in this single file.\n\nBased on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n\"\"\"\n# mypy: ignore-errors\nimport math\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom typing_extensions import Self\n\nfrom lit_llama.utils import find_multiple\n\n\nMaskCache = torch.Tensor", "\nMaskCache = torch.Tensor\nRoPECache = torch.Tensor\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\n@dataclass\nclass LLaMAConfig:\n    block_size: int = 2048\n    vocab_size: int = 32000\n    padded_vocab_size: Optional[int] = None\n    n_layer: int = 32\n    n_head: int = 32\n    n_embd: int = 4096\n\n    def __post_init__(self):\n        if self.padded_vocab_size is None:\n            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n\n    @classmethod\n    def from_name(cls, name: str) -> Self:\n        return cls(**llama_configs[name])", "\n\nllama_configs = {\n    \"7B\": dict(n_layer=32, n_head=32, n_embd=4096),\n    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n}\n\n\nclass LLaMA(nn.Module):\n    def __init__(self, config: LLaMAConfig) -> None:\n        super().__init__()\n        assert config.padded_vocab_size is not None\n        self.config = config\n\n        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n                ln_f=RMSNorm(config.n_embd),\n            )\n        )\n\n        self.rope_cache: Optional[RoPECache] = None\n        self.mask_cache: Optional[MaskCache] = None\n        self.kv_caches: List[KVCache] = []\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n\n    def forward(\n        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n        B, T = idx.size()\n\n        block_size = self.config.block_size\n        if max_seq_length is None:\n            max_seq_length = block_size\n        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n\n        if self.rope_cache is None:\n            self.rope_cache = self.build_rope_cache(idx)\n        if self.mask_cache is None:\n            self.mask_cache = self.build_mask_cache(idx)\n\n        if input_pos is not None:\n            rope = self.rope_cache.index_select(0, input_pos)\n            mask = self.mask_cache.index_select(2, input_pos)\n            mask = mask[:, :, :, :max_seq_length]\n        else:\n            rope = self.rope_cache[:T]\n            mask = self.mask_cache[:, :, :T, :T]\n\n        # forward the model itself\n        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n\n        if input_pos is None:  # proxy for use_cache=False\n            for block in self.transformer.h:\n                x, _ = block(x, rope, mask, max_seq_length)\n        else:\n            if not self.kv_caches:\n                head_size = self.config.n_embd // self.config.n_head\n                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n                self.kv_caches = [\n                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n                    for _ in range(self.config.n_layer)\n                ]\n            for i, block in enumerate(self.transformer.h):\n                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)  # (b, t, vocab_size)\n\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str) -> Self:\n        return cls(LLaMAConfig.from_name(name))\n\n    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n        return build_rope_cache(\n            seq_len=self.config.block_size,\n            n_elem=self.config.n_embd // self.config.n_head,\n            dtype=idx.dtype,\n            device=idx.device,\n        )\n\n    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n\n    def reset_cache(self) -> None:\n        self.kv_caches.clear()\n        if self.mask_cache.device.type == \"xla\":\n            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n            self.rope_cache = None\n            self.mask_cache = None", "\n\nclass LLaMA(nn.Module):\n    def __init__(self, config: LLaMAConfig) -> None:\n        super().__init__()\n        assert config.padded_vocab_size is not None\n        self.config = config\n\n        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n                ln_f=RMSNorm(config.n_embd),\n            )\n        )\n\n        self.rope_cache: Optional[RoPECache] = None\n        self.mask_cache: Optional[MaskCache] = None\n        self.kv_caches: List[KVCache] = []\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n\n    def forward(\n        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n        B, T = idx.size()\n\n        block_size = self.config.block_size\n        if max_seq_length is None:\n            max_seq_length = block_size\n        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n\n        if self.rope_cache is None:\n            self.rope_cache = self.build_rope_cache(idx)\n        if self.mask_cache is None:\n            self.mask_cache = self.build_mask_cache(idx)\n\n        if input_pos is not None:\n            rope = self.rope_cache.index_select(0, input_pos)\n            mask = self.mask_cache.index_select(2, input_pos)\n            mask = mask[:, :, :, :max_seq_length]\n        else:\n            rope = self.rope_cache[:T]\n            mask = self.mask_cache[:, :, :T, :T]\n\n        # forward the model itself\n        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n\n        if input_pos is None:  # proxy for use_cache=False\n            for block in self.transformer.h:\n                x, _ = block(x, rope, mask, max_seq_length)\n        else:\n            if not self.kv_caches:\n                head_size = self.config.n_embd // self.config.n_head\n                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n                self.kv_caches = [\n                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n                    for _ in range(self.config.n_layer)\n                ]\n            for i, block in enumerate(self.transformer.h):\n                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)  # (b, t, vocab_size)\n\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str) -> Self:\n        return cls(LLaMAConfig.from_name(name))\n\n    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n        return build_rope_cache(\n            seq_len=self.config.block_size,\n            n_elem=self.config.n_embd // self.config.n_head,\n            dtype=idx.dtype,\n            device=idx.device,\n        )\n\n    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n\n    def reset_cache(self) -> None:\n        self.kv_caches.clear()\n        if self.mask_cache.device.type == \"xla\":\n            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n            self.rope_cache = None\n            self.mask_cache = None", "\n\nclass Block(nn.Module):\n    def __init__(self, config: LLaMAConfig) -> None:\n        super().__init__()\n        self.rms_1 = RMSNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.rms_2 = RMSNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        rope: RoPECache,\n        mask: MaskCache,\n        max_seq_length: int,\n        input_pos: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n        h, new_kv_cache = self.attn(self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n        x = x + h\n        x = x + self.mlp(self.rms_2(x))\n        return x, new_kv_cache", "\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config: LLaMAConfig) -> None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        rope: RoPECache,\n        mask: MaskCache,\n        max_seq_length: int,\n        input_pos: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\n        head_size = C // self.n_head\n        k = k.view(B, T, self.n_head, head_size)\n        q = q.view(B, T, self.n_head, head_size)\n        v = v.view(B, T, self.n_head, head_size)\n\n        q = apply_rope(q, rope)\n        k = apply_rope(k, rope)\n\n        k = k.transpose(1, 2)  # (B, nh, T, hs)\n        q = q.transpose(1, 2)  # (B, nh, T, hs)\n        v = v.transpose(1, 2)  # (B, nh, T, hs)\n\n        if kv_cache is not None:\n            cache_k, cache_v = kv_cache\n            # check if reached token limit\n            if input_pos[-1] >= max_seq_length:\n                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n                # shift 1 position to the left\n                cache_k = torch.roll(cache_k, -1, dims=2)\n                cache_v = torch.roll(cache_v, -1, dims=2)\n            k = cache_k.index_copy(2, input_pos, k)\n            v = cache_v.index_copy(2, input_pos, v)\n            kv_cache = k, v\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        #  att = att.masked_fill(mask[:,:,:T,:T] == 0, float('-inf'))\n        #  att = F.softmax(att, dim=-1)\n        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\n        # efficient attention using Flash Attention CUDA kernels\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n\n        return y, kv_cache", "\n\nclass MLP(nn.Module):\n    def __init__(self, config: LLaMAConfig) -> None:\n        super().__init__()\n        hidden_dim = 4 * config.n_embd\n        n_hidden = int(2 * hidden_dim / 3)\n        n_hidden = find_multiple(n_hidden, 256)\n\n        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n        x = self.c_proj(x)\n        return x", "\n\nclass RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization.\n\n    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n    \"\"\"\n\n    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(size))\n        self.eps = eps\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # NOTE: the original RMSNorm paper implementation is not equivalent\n        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n        # rms_x = norm_x * d_x ** (-1. / 2)\n        # x_normed = x / (rms_x + self.eps)\n        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n        x_normed = x * torch.rsqrt(norm_x + self.eps)\n        return self.scale * x_normed", "\n\ndef build_rope_cache(\n    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n) -> RoPECache:\n    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\n    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n    transformers/rope/__init__.py. MIT License:\n    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n    \"\"\"\n    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n\n    # Create position indexes `[0, 1, ..., seq_len - 1]`\n    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n\n    # Calculate the product of position index and $\\theta_i$\n    idx_theta = torch.outer(seq_idx, theta).float()\n\n    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n\n    # this is to mimic the behaviour of complex32, else we will get different results\n    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n        cache = cache.half()\n    return cache", "\n\ndef apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n    # truncate to support variable sizes\n    T = x.size(1)\n    rope_cache = rope_cache[:T]\n\n    # cast because the reference does\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n        ],\n        -1,\n    )\n\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)", ""]}
{"filename": "lit_llama/tokenizer.py", "chunked_list": ["import os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nfrom sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n\n\nclass Tokenizer:\n    \"\"\"Tokenizer for LLaMA.\"\"\"\n\n    def __init__(self, model_path: Path) -> None:\n        self.processor = SentencePieceProcessor(model_file=str(model_path))\n        self.bos_id = self.processor.bos_id()\n        self.eos_id = self.processor.eos_id()\n        self.pad_id = self.processor.pad_id()\n\n    @property\n    def vocab_size(self) -> int:\n        return self.processor.vocab_size()\n\n    def encode(\n        self,\n        string: str,\n        bos: bool = True,\n        eos: bool = False,\n        max_length: int = -1,\n        pad: bool = False,\n        device: Optional[torch.device] = None\n    ) -> torch.Tensor:\n        tokens = self.processor.encode(string)\n        if bos:\n            tokens = [self.bos_id] + tokens\n        if eos:\n            tokens = tokens + [self.eos_id]\n        if max_length > 0:\n            tokens = tokens[:max_length]\n        if pad and len(tokens) < max_length:\n            tokens += [self.pad_id] * (max_length - len(tokens))\n\n        return torch.tensor(tokens, dtype=torch.int, device=device)\n\n    def decode(self, tokens: torch.Tensor) -> str:\n        return self.processor.decode(tokens.tolist())\n\n    @staticmethod\n    def train(input: str, destination: str, vocab_size=32000) -> None:\n        model_prefix = os.path.join(destination, \"tokenizer\")\n        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)", "class Tokenizer:\n    \"\"\"Tokenizer for LLaMA.\"\"\"\n\n    def __init__(self, model_path: Path) -> None:\n        self.processor = SentencePieceProcessor(model_file=str(model_path))\n        self.bos_id = self.processor.bos_id()\n        self.eos_id = self.processor.eos_id()\n        self.pad_id = self.processor.pad_id()\n\n    @property\n    def vocab_size(self) -> int:\n        return self.processor.vocab_size()\n\n    def encode(\n        self,\n        string: str,\n        bos: bool = True,\n        eos: bool = False,\n        max_length: int = -1,\n        pad: bool = False,\n        device: Optional[torch.device] = None\n    ) -> torch.Tensor:\n        tokens = self.processor.encode(string)\n        if bos:\n            tokens = [self.bos_id] + tokens\n        if eos:\n            tokens = tokens + [self.eos_id]\n        if max_length > 0:\n            tokens = tokens[:max_length]\n        if pad and len(tokens) < max_length:\n            tokens += [self.pad_id] * (max_length - len(tokens))\n\n        return torch.tensor(tokens, dtype=torch.int, device=device)\n\n    def decode(self, tokens: torch.Tensor) -> str:\n        return self.processor.decode(tokens.tolist())\n\n    @staticmethod\n    def train(input: str, destination: str, vocab_size=32000) -> None:\n        model_prefix = os.path.join(destination, \"tokenizer\")\n        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)", ""]}
{"filename": "lit_llama/adapter_v2.py", "chunked_list": ["import torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom lit_llama.adapter import LLaMA\n\n\ndef get_adapter_substrings():\n    substrings = [\"adapter_wte\", \"gating_factor\"]  # regular adapter v1 parameters\n    substrings.extend([\"adapter_scale\", \"adapter_bias\"])  # adapter v2: new bias and scale used in Linear\n    substrings.extend([\"rms_1\", \"rms_2\", \"ln_f\"])  # adapter v2: RMSNorm parameters are now trainable\n    return substrings", "def get_adapter_substrings():\n    substrings = [\"adapter_wte\", \"gating_factor\"]  # regular adapter v1 parameters\n    substrings.extend([\"adapter_scale\", \"adapter_bias\"])  # adapter v2: new bias and scale used in Linear\n    substrings.extend([\"rms_1\", \"rms_2\", \"ln_f\"])  # adapter v2: RMSNorm parameters are now trainable\n    return substrings\n\n\ndef mark_only_adapter_v2_as_trainable(model: LLaMA) -> None:\n    \"\"\"Sets `requires_grad=False` for all non-adapter weights.\"\"\"\n    for name, param in model.named_parameters():\n        param.requires_grad = any(s in name for s in get_adapter_substrings())", "\n\ndef adapter_v2_state_from_state_dict(state_dict: dict) -> dict:\n    \"\"\"Returns the model state dict with only the adapter weights for saving.\"\"\"\n    return {name: param for name, param in state_dict.items()\n            if any(s in name for s in get_adapter_substrings())}\n\n\ndef adapter_v2_new_forward(self, input: Tensor) -> Tensor:\n    return self.adapter_scale * (\n        F.linear(input, self.weight, self.bias) + self.adapter_bias\n    )", "def adapter_v2_new_forward(self, input: Tensor) -> Tensor:\n    return self.adapter_scale * (\n        F.linear(input, self.weight, self.bias) + self.adapter_bias\n    )\n\n\ndef adapter_v2_linear_with_bias_and_scale(layer):\n    layer.adapter_bias = torch.nn.Parameter(torch.zeros(layer.weight.shape[0]), requires_grad=True)\n    layer.adapter_scale = torch.nn.Parameter(torch.ones(layer.weight.shape[0]), requires_grad=True)\n    bound_method = adapter_v2_new_forward.__get__(layer, layer.__class__)\n    setattr(layer, 'forward', bound_method)\n    return layer", "\n\ndef add_adapter_v2_parameters_to_linear_layers(model):\n    for module in model.modules():\n        if isinstance(module, nn.Linear):\n            adapter_v2_linear_with_bias_and_scale(module)\n"]}
{"filename": "lit_llama/__init__.py", "chunked_list": ["from lit_llama.model import LLaMAConfig, LLaMA, RMSNorm, build_rope_cache, apply_rope\nfrom lit_llama.tokenizer import Tokenizer\n"]}
{"filename": "lit_llama/adapter.py", "chunked_list": ["\"\"\"Implementation of the paper:\n\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\nhttps://arxiv.org/abs/2303.16199\n\n                                                                             |              Prefix cross-attention\n                                                                             |\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                        |               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2506        x        \u2506                                                        |               \u2506      prefix      \u2506\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        |               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "  \u2506        x        \u2506                                                        |               \u2506      prefix      \u2506\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                        |               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           |                                                                 |                        |\n           \u25bc                                                                 |                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                       |              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2506  self-attention  \u2506 --------------------------------------------------------------\u2510      \u2506  linear projection  \u2506\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                       |       \u2506      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           |                                                                 |       \u2506                |         \\\n           \u25bc                                                                 |       \u25bc                \u25bc          \u25bc\n         \u256d\u2500\u2500\u2500\u256e     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u256d\u2500\u2500\u2500\u256e \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     |  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510", "           \u25bc                                                                 |       \u25bc                \u25bc          \u25bc\n         \u256d\u2500\u2500\u2500\u256e     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u256d\u2500\u2500\u2500\u256e \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     |  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2506 + \u2506 \u25c0\u2500\u2500 \u2506  gating factor \u2506-\u2506 x \u2506-\u2506  prefix cross-attention  \u2506     |  \u2506  query  \u2506    \u2506  prefix key  \u2506  \u2506  prefix value  \u2506\n         \u2570\u2500\u2500\u2500\u256f     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2570\u2500\u2500\u2500\u256f \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     |  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           |                                                                 |          \\             |           /\n           \u25bc                                                                 |           \u25bc            \u25bc          \u25bc\n                                                                             |         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                             |         \u2506  scaled dot-product attention  \u2506\n                                                                             |         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n", "                                                                             |         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nIn order to inject learnable information from the prefix to pretrained weights we need to sum outputs from\nself-attention and prefix cross-attention (times gating factor). For prefix cross-attention we need `query` (from\nself-attention as a result of linear projection), `prefix key` and `prefix value` (from cross-attention as a result of\nlinear projection).\nThe output of prefix cross-attention is multiplied by gating factor, which is a learnable parameter that is needed to\navoid potential disruption of pretrained weights caused by incorporating randomly initialized tensors. This factor is\ninitialized with zeros to avoid noise from the adaption prompts at the early training stage.", "avoid potential disruption of pretrained weights caused by incorporating randomly initialized tensors. This factor is\ninitialized with zeros to avoid noise from the adaption prompts at the early training stage.\nMore about it: https://lightning.ai/pages/community/article/understanding-llama-adapters/\n\nNotes about implementation: as per paper adapter's prefix is concatenated with the input, while here outputs of\nself-attention and prefix cross-attention are summed. Both variants are mathematically equivalent:\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter/issues/47\n\"\"\"\n# mypy: ignore-errors\nfrom dataclasses import dataclass", "# mypy: ignore-errors\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, List, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nimport lit_llama.model as llama\nfrom lit_llama.model import build_rope_cache, apply_rope, RMSNorm, MLP, KVCache, RoPECache", "import lit_llama.model as llama\nfrom lit_llama.model import build_rope_cache, apply_rope, RMSNorm, MLP, KVCache, RoPECache\n\n\n@dataclass\nclass LLaMAConfig(llama.LLaMAConfig):\n    adapter_prompt_length: int = 10\n    adapter_start_layer: int = 2\n\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"A modification of `lit_llama.model.CausalSelfAttention` that adds the attention\n    over the adaption prompt.\"\"\"\n\n    def __init__(self, config: LLaMAConfig, block_idx: int) -> None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        if block_idx >= config.adapter_start_layer:\n            # adapter embedding layer\n            self.adapter_wte = nn.Embedding(config.adapter_prompt_length, config.n_embd)\n            # a learnable gating factor (to avoid potential disruption of pretrained weights) initialized with zeros (to\n            # avoid noise from adaption prompts at the early training stage)\n            self.gating_factor = torch.nn.Parameter(torch.zeros(1, config.n_head, 1, 1))\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        self.block_idx = block_idx\n        self.adapter_prompt_length = config.adapter_prompt_length\n        self.adapter_start_layer = config.adapter_start_layer\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        rope: RoPECache,\n        mask: torch.Tensor,\n        max_seq_length: int,\n        input_pos: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n        adapter_kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[KVCache], Optional[KVCache]]:\n        # notation:\n        # - B  | batch\n        # - T  | time-step (sequence length)\n        # - C  | embeddings size (n_embd) = head size * num heads\n        # - hs | head size\n        # - nh | number of heads\n\n        B, T, C = x.size()\n\n        # instead of calculating `query`, `key` and `value` by separately multiplying input `x` with corresponding\n        # weight matrices do it (for all heads) in a single multiplication with a matrix of 3x size (concatenated\n        # weights for q, k, v) and then split the result along `embedding size` dimension\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # (B, T, 3 * C) --> 3 * (B, T, C)\n\n        # in order to move head_size (hs) dimension right after batch (B) dimension, we need to first split\n        # embedding size (C) dimension into num_heads (nh) and head_size (hs)\n        head_size = C // self.n_head\n        k = k.view(B, T, self.n_head, head_size)\n        q = q.view(B, T, self.n_head, head_size)\n        v = v.view(B, T, self.n_head, head_size)\n\n        # \"Unlike standard positional embeddings rotary embeddings must be applied at every layer\"\n        q = apply_rope(q, rope) # (B, T, nh, hs)\n        k = apply_rope(k, rope) # (B, T, nh, hs)\n\n        # now `key`, 'query` and `value` tensors are correctly represented: for each element in a batch (B)\n        # there is a number of heads (nh) and for each head there is a sequence of elements (T), each of them is\n        # represented by a vector of size `hs`\n        k = k.transpose(1, 2)  # (B, nh, T, hs)\n        q = q.transpose(1, 2)  # (B, nh, T, hs)\n        v = v.transpose(1, 2)  # (B, nh, T, hs)\n\n        if kv_cache is not None:\n            cache_k, cache_v = kv_cache # 2 * (B, nh, max_seq_length, hs)\n            # check if reached token limit\n            if input_pos[-1] >= max_seq_length:\n                # if we reached token limit and thus there is no space to put newly calculated `key` and `value`\n                # right next to cached ones, we need to rotate cache tensor along `max_seq_length` dimension by one\n                # element to the left: this will free up space for new `key` and `value`\n                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n                # shift 1 position to the left\n                cache_k = torch.roll(cache_k, -1, dims=2)\n                cache_v = torch.roll(cache_v, -1, dims=2)\n            k = cache_k.index_copy(2, input_pos, k) # (B, nh, max_seq_length, hs)\n            v = cache_v.index_copy(2, input_pos, v) # (B, nh, max_seq_length, hs)\n            kv_cache = k, v\n\n        # efficient attention using Flash Attention CUDA kernels\n        # \u2193 (B, nh, T, hs) @ (B, nh, T, hs).mT --> (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0) # (B, nh, T, hs)\n\n        # \"Adapters are applied to the topmost layers to better tune the language\n        # representations with higher-level semantics\".\n        if self.block_idx >= self.adapter_start_layer:\n            if adapter_kv_cache is not None:\n                ak, av = adapter_kv_cache # 2 * (B, nh, aT, hs)\n            else:\n                prefix = self.adapter_wte.weight.reshape(1, self.adapter_prompt_length, self.n_embd)\n                aT = prefix.size(1)\n                _, ak, av = self.c_attn(prefix).split(self.n_embd, dim=2) # (1, aT, 3 * C) --> 3 * (1, aT, C)\n                ak = ak.view(1, aT, self.n_head, head_size).repeat(B, 1, 1, 1).transpose(1, 2) # (B, nh, aT, hs)\n                av = av.view(1, aT, self.n_head, head_size).repeat(B, 1, 1, 1).transpose(1, 2) # (B, nh, aT, hs)\n                adapter_kv_cache = (ak, av)\n\n            # Apply cross-attention with `query`, `adapter_key`, `adapter_value` and sum the output with the output\n            # obtained from self-attention step. This is mathematically equivalent to concatenation of prefix and input as per paper.\n            amask = torch.ones(q.shape[-2], ak.shape[-2], dtype=torch.bool, device=x.device) # (T, aT)\n            # \u2193 (B, nh, T, hs) @ (B, nh, aT, hs).mT --> (B, nh, T, aT) @ (B, nh, aT, hs) --> (B, nh, T, hs)\n            ay = F.scaled_dot_product_attention(q, ak, av, attn_mask=amask, dropout_p=0.0, is_causal=False) # (B, nh, T, hs)\n            y = y + self.gating_factor * ay\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y) # (B, T, C)\n\n        return y, kv_cache, adapter_kv_cache\n\n    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n        \"\"\"For backward compatibility with old checkpoints that have a single gating value for all heads.\"\"\"\n        name = prefix + \"gating_factor\"\n        if name in state_dict:\n            tensor = state_dict[name]\n            # in case we are loading with `utils.lazy_load()`\n            tensor = tensor._load_tensor() if hasattr(tensor, \"_load_tensor\") else tensor\n\n            if len(tensor.shape) < 4:\n                # For old checkpoints with unified gating value\n                state_dict[name] = tensor.reshape(1, 1, 1, 1).repeat(1, self.n_head, 1, 1)\n            else:\n                state_dict[name] = tensor\n\n        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)", "\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"A modification of `lit_llama.model.CausalSelfAttention` that adds the attention\n    over the adaption prompt.\"\"\"\n\n    def __init__(self, config: LLaMAConfig, block_idx: int) -> None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        if block_idx >= config.adapter_start_layer:\n            # adapter embedding layer\n            self.adapter_wte = nn.Embedding(config.adapter_prompt_length, config.n_embd)\n            # a learnable gating factor (to avoid potential disruption of pretrained weights) initialized with zeros (to\n            # avoid noise from adaption prompts at the early training stage)\n            self.gating_factor = torch.nn.Parameter(torch.zeros(1, config.n_head, 1, 1))\n\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        self.block_idx = block_idx\n        self.adapter_prompt_length = config.adapter_prompt_length\n        self.adapter_start_layer = config.adapter_start_layer\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        rope: RoPECache,\n        mask: torch.Tensor,\n        max_seq_length: int,\n        input_pos: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n        adapter_kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[KVCache], Optional[KVCache]]:\n        # notation:\n        # - B  | batch\n        # - T  | time-step (sequence length)\n        # - C  | embeddings size (n_embd) = head size * num heads\n        # - hs | head size\n        # - nh | number of heads\n\n        B, T, C = x.size()\n\n        # instead of calculating `query`, `key` and `value` by separately multiplying input `x` with corresponding\n        # weight matrices do it (for all heads) in a single multiplication with a matrix of 3x size (concatenated\n        # weights for q, k, v) and then split the result along `embedding size` dimension\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # (B, T, 3 * C) --> 3 * (B, T, C)\n\n        # in order to move head_size (hs) dimension right after batch (B) dimension, we need to first split\n        # embedding size (C) dimension into num_heads (nh) and head_size (hs)\n        head_size = C // self.n_head\n        k = k.view(B, T, self.n_head, head_size)\n        q = q.view(B, T, self.n_head, head_size)\n        v = v.view(B, T, self.n_head, head_size)\n\n        # \"Unlike standard positional embeddings rotary embeddings must be applied at every layer\"\n        q = apply_rope(q, rope) # (B, T, nh, hs)\n        k = apply_rope(k, rope) # (B, T, nh, hs)\n\n        # now `key`, 'query` and `value` tensors are correctly represented: for each element in a batch (B)\n        # there is a number of heads (nh) and for each head there is a sequence of elements (T), each of them is\n        # represented by a vector of size `hs`\n        k = k.transpose(1, 2)  # (B, nh, T, hs)\n        q = q.transpose(1, 2)  # (B, nh, T, hs)\n        v = v.transpose(1, 2)  # (B, nh, T, hs)\n\n        if kv_cache is not None:\n            cache_k, cache_v = kv_cache # 2 * (B, nh, max_seq_length, hs)\n            # check if reached token limit\n            if input_pos[-1] >= max_seq_length:\n                # if we reached token limit and thus there is no space to put newly calculated `key` and `value`\n                # right next to cached ones, we need to rotate cache tensor along `max_seq_length` dimension by one\n                # element to the left: this will free up space for new `key` and `value`\n                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n                # shift 1 position to the left\n                cache_k = torch.roll(cache_k, -1, dims=2)\n                cache_v = torch.roll(cache_v, -1, dims=2)\n            k = cache_k.index_copy(2, input_pos, k) # (B, nh, max_seq_length, hs)\n            v = cache_v.index_copy(2, input_pos, v) # (B, nh, max_seq_length, hs)\n            kv_cache = k, v\n\n        # efficient attention using Flash Attention CUDA kernels\n        # \u2193 (B, nh, T, hs) @ (B, nh, T, hs).mT --> (B, nh, T, T) @ (B, nh, T, hs) --> (B, nh, T, hs)\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0) # (B, nh, T, hs)\n\n        # \"Adapters are applied to the topmost layers to better tune the language\n        # representations with higher-level semantics\".\n        if self.block_idx >= self.adapter_start_layer:\n            if adapter_kv_cache is not None:\n                ak, av = adapter_kv_cache # 2 * (B, nh, aT, hs)\n            else:\n                prefix = self.adapter_wte.weight.reshape(1, self.adapter_prompt_length, self.n_embd)\n                aT = prefix.size(1)\n                _, ak, av = self.c_attn(prefix).split(self.n_embd, dim=2) # (1, aT, 3 * C) --> 3 * (1, aT, C)\n                ak = ak.view(1, aT, self.n_head, head_size).repeat(B, 1, 1, 1).transpose(1, 2) # (B, nh, aT, hs)\n                av = av.view(1, aT, self.n_head, head_size).repeat(B, 1, 1, 1).transpose(1, 2) # (B, nh, aT, hs)\n                adapter_kv_cache = (ak, av)\n\n            # Apply cross-attention with `query`, `adapter_key`, `adapter_value` and sum the output with the output\n            # obtained from self-attention step. This is mathematically equivalent to concatenation of prefix and input as per paper.\n            amask = torch.ones(q.shape[-2], ak.shape[-2], dtype=torch.bool, device=x.device) # (T, aT)\n            # \u2193 (B, nh, T, hs) @ (B, nh, aT, hs).mT --> (B, nh, T, aT) @ (B, nh, aT, hs) --> (B, nh, T, hs)\n            ay = F.scaled_dot_product_attention(q, ak, av, attn_mask=amask, dropout_p=0.0, is_causal=False) # (B, nh, T, hs)\n            y = y + self.gating_factor * ay\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y) # (B, T, C)\n\n        return y, kv_cache, adapter_kv_cache\n\n    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n        \"\"\"For backward compatibility with old checkpoints that have a single gating value for all heads.\"\"\"\n        name = prefix + \"gating_factor\"\n        if name in state_dict:\n            tensor = state_dict[name]\n            # in case we are loading with `utils.lazy_load()`\n            tensor = tensor._load_tensor() if hasattr(tensor, \"_load_tensor\") else tensor\n\n            if len(tensor.shape) < 4:\n                # For old checkpoints with unified gating value\n                state_dict[name] = tensor.reshape(1, 1, 1, 1).repeat(1, self.n_head, 1, 1)\n            else:\n                state_dict[name] = tensor\n\n        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)", "\n\nclass Block(nn.Module):\n    \"\"\"The implementation is identical to `lit_llama.model.Block` with the exception that\n    we replace the attention layer where adaption is implemented.\"\"\"\n\n    def __init__(self, config: LLaMAConfig, block_idx: int) -> None:\n        super().__init__()\n        self.rms_1 = RMSNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config, block_idx)\n        self.rms_2 = RMSNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        rope: RoPECache,\n        mask: torch.Tensor,\n        max_seq_length: int,\n        input_pos: Optional[torch.Tensor] = None,\n        kv_cache: Optional[KVCache] = None,\n        adapter_kv_cache: Optional[KVCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[KVCache], Optional[KVCache]]:\n        h, new_kv_cache, new_adapter_kv_cache = self.attn(\n            self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache, adapter_kv_cache\n        )\n        x = x + h\n        x = x + self.mlp(self.rms_2(x))\n        return x, new_kv_cache, new_adapter_kv_cache", "\n\nclass LLaMA(llama.LLaMA):\n    \"\"\"The implementation is identical to `lit_llama.model.LLaMA` with the exception that\n    the `Block` saves the layer index and passes it down to the attention layer.\"\"\"\n\n    def __init__(self, config: LLaMAConfig) -> None:\n        nn.Module.__init__(self)\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.vocab_size, config.n_embd),\n                h=nn.ModuleList(Block(config, i) for i in range(config.n_layer)),\n                ln_f=RMSNorm(config.n_embd),\n            )\n        )\n\n        self.rope_cache: Optional[RoPECache] = None\n        self.mask_cache: Optional[torch.Tensor] = None\n        self.kv_caches: List[KVCache] = []\n        self.adapter_kv_caches: List[KVCache] = []\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(LLaMAConfig.from_name(name))\n\n    def reset_cache(self) -> None:\n        super().reset_cache()\n        self.adapter_kv_caches.clear()\n\n    def forward(\n        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n        B, T = idx.size()\n\n        block_size = self.config.block_size\n        if max_seq_length is None:\n            max_seq_length = block_size\n        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n\n        if self.rope_cache is None:\n            self.rope_cache = self.build_rope_cache(idx) # (block_size, head_size / 2, 2)\n        if self.mask_cache is None:\n            self.mask_cache = self.build_mask_cache(idx) # (1, 1, block_size, block_size)\n\n        if input_pos is not None:\n            rope = self.rope_cache.index_select(0, input_pos)\n            mask = self.mask_cache.index_select(2, input_pos)\n            mask = mask[:, :, :, :max_seq_length]\n        else:\n            rope = self.rope_cache[:T]\n            mask = self.mask_cache[:, :, :T, :T]\n\n        # forward the model itself\n        x = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n\n        if input_pos is None:  # proxy for use_cache=False\n            for block in self.transformer.h:\n                x, *_ = block(x, rope, mask, max_seq_length)\n        else:\n            if not self.kv_caches:\n                head_size = self.config.n_embd // self.config.n_head\n                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n                self.kv_caches = [\n                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n                    for _ in range(self.config.n_layer)\n                ]\n            if not self.adapter_kv_caches:\n                self.adapter_kv_caches = [None for _ in range(self.config.n_layer)]\n            for i, block in enumerate(self.transformer.h):\n                x, self.kv_caches[i], self.adapter_kv_caches[i] = block(\n                    x, rope, mask, max_seq_length, input_pos, self.kv_caches[i], self.adapter_kv_caches[i]\n                )\n\n        x = self.transformer.ln_f(x) # (B, T, n_embd)\n\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n\n        return logits", "\n\ndef mark_only_adapter_as_trainable(model: LLaMA) -> None:\n    \"\"\"Sets `requires_grad=False` for all non-adapter weights.\"\"\"\n    for name, param in model.named_parameters():\n        param.requires_grad = \"adapter_wte\" in name or \"gating_factor\" in name\n\n\ndef adapter_state_from_state_dict(state_dict: dict) -> dict:\n    \"\"\"Returns the model state dict with only the adapter weights for saving.\"\"\"\n    return {name: param for name, param in state_dict.items() if \"adapter_wte\" in name or \"gating_factor\" in name}", "def adapter_state_from_state_dict(state_dict: dict) -> dict:\n    \"\"\"Returns the model state dict with only the adapter weights for saving.\"\"\"\n    return {name: param for name, param in state_dict.items() if \"adapter_wte\" in name or \"gating_factor\" in name}\n"]}
{"filename": "lit_llama/utils.py", "chunked_list": ["\"\"\"Utility functions for training and inference.\"\"\"\n\nimport functools\nimport pickle\nimport warnings\nfrom io import BytesIO\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport torch", "\nimport torch\nimport torch.utils._device\nfrom lightning.fabric.strategies import DeepSpeedStrategy, FSDPStrategy\nfrom torch.distributed.fsdp import FullStateDictConfig\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp import StateDictType\nfrom torch.serialization import normalize_storage_type\n\nllama_model_sizes = {", "\nllama_model_sizes = {\n    4096: \"7B\",  # 7B n_embd=4096\n    5120: \"13B\",  # 13B n_embd=5120\n    6656: \"30B\",  # 30B n_embd=6656\n    8192: \"65B\",  # 65B n_embd=8192\n}\n\n\ndef llama_model_lookup(checkpoint: dict) -> str:\n    \"\"\"Returns the LLaMA model name from the checkpoint.\n    \n    Checks the width of the lm_head.weight matrix, as these uniquely identify the model.\n    \"\"\"\n    embedding_size = checkpoint['transformer.wte.weight'].shape[1]\n    return llama_model_sizes[embedding_size]", "\ndef llama_model_lookup(checkpoint: dict) -> str:\n    \"\"\"Returns the LLaMA model name from the checkpoint.\n    \n    Checks the width of the lm_head.weight matrix, as these uniquely identify the model.\n    \"\"\"\n    embedding_size = checkpoint['transformer.wte.weight'].shape[1]\n    return llama_model_sizes[embedding_size]\n\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)", "\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n\ndef save_model_checkpoint(fabric, model, file_path):\n    \"\"\"Handles boilerplate logic for retrieving and saving the state_dict.\n    \n    This will be upstreamed to Fabric soon.\n    \"\"\"\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict\n\n        fabric.save(file_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            convert_zero_checkpoint_to_fp32_state_dict(file_path, file_path.with_suffix(\".pth\"))\n        return\n\n    if isinstance(fabric.strategy, FSDPStrategy):\n        save_policy = FullStateDictConfig(offload_to_cpu=(fabric.world_size > 1), rank0_only=True)\n        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, save_policy):\n            state_dict = model._forward_module.state_dict()\n    else:\n        state_dict = model.state_dict()\n\n    if fabric.global_rank == 0:\n        torch.save(state_dict, file_path)\n    fabric.barrier()", "def save_model_checkpoint(fabric, model, file_path):\n    \"\"\"Handles boilerplate logic for retrieving and saving the state_dict.\n    \n    This will be upstreamed to Fabric soon.\n    \"\"\"\n    file_path = Path(file_path)\n\n    if isinstance(fabric.strategy, DeepSpeedStrategy):\n        from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict\n\n        fabric.save(file_path, {\"model\": model})\n        fabric.barrier()\n        if fabric.global_rank == 0:\n            # Create a consolidated checkpoint with the same name next to the deepspeed checkpoint\n            convert_zero_checkpoint_to_fp32_state_dict(file_path, file_path.with_suffix(\".pth\"))\n        return\n\n    if isinstance(fabric.strategy, FSDPStrategy):\n        save_policy = FullStateDictConfig(offload_to_cpu=(fabric.world_size > 1), rank0_only=True)\n        with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, save_policy):\n            state_dict = model._forward_module.state_dict()\n    else:\n        state_dict = model.state_dict()\n\n    if fabric.global_rank == 0:\n        torch.save(state_dict, file_path)\n    fabric.barrier()", "\n\nclass EmptyInitOnDevice(torch.overrides.TorchFunctionMode):\n    def __init__(self, device=None, dtype=None, quantization_mode=None):\n        \"\"\"\n        Create tensors with given device and dtype and don't run initialization\n           (but instead use \"empty tensors\", i.e. uninitialized memory).\n\n            device: `torch.device` to work with\n            dtype: `torch.dtype` to work with\n            quantization_mode: optional string, quantization mode to work with, default `None`.\n                 Available modes: `llm.int8` bitsnbytes LLM.int8 quantization (only on GPU)\n                                  `gptq.int4`, `gptq.int8`: GPTQ pre-quantized models\n\n        Example::\n            with EmptyInitOnDevice(\"cuda\", dtype=torch.bfloat16):\n               model = LLaMA.from_name('7B')\n            model.load_state_dict(torch.load('llama-lit/7B/lit-llama.pth'))\"\"\"\n\n        self.quantization_mode = quantization_mode\n        self.quantized_linear_cls = None\n        if self.quantization_mode == 'llm.int8':\n            if device.type != \"cuda\":\n                raise ValueError(\"Quantization is only supported on the GPU.\")\n            from .quantization import Linear8bitLt\n            self.quantized_linear_cls = Linear8bitLt\n        elif self.quantization_mode == 'gptq.int4':\n            from .quantization import ColBlockQuantizedLinear\n            self.quantized_linear_cls = functools.partial(ColBlockQuantizedLinear, bits=4, tile_cols=-1)\n        elif self.quantization_mode == 'gptq.int8':\n            from .quantization import ColBlockQuantizedLinear\n            self.quantized_linear_cls = functools.partial(ColBlockQuantizedLinear, bits=8, tile_cols=-1)\n        elif self.quantization_mode is not None:\n            raise RuntimeError(f\"unknown quantization mode {self.quantization_mode}\")\n        self.device = device\n        self.dtype = dtype\n\n    def __enter__(self):\n        if self.quantized_linear_cls != None:\n            self.torch_linear_cls = torch.nn.Linear\n            torch.nn.Linear = self.quantized_linear_cls\n        return super().__enter__()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.quantized_linear_cls != None:\n            torch.nn.Linear = self.torch_linear_cls\n        return super().__exit__(exc_type, exc_val, exc_tb)\n\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        kwargs = kwargs or {}\n        if getattr(func, \"__module__\", None) == \"torch.nn.init\":\n            if \"tensor\" in kwargs:\n                return kwargs[\"tensor\"]\n            else:\n                return args[0]\n        if (\n            self.device is not None\n            and func in torch.utils._device._device_constructors()\n            and kwargs.get(\"device\") is None\n        ):\n            kwargs[\"device\"] = self.device\n        if (\n            self.dtype is not None\n            and func in torch.utils._device._device_constructors()\n            and kwargs.get(\"dtype\") is None\n        ):\n            kwargs[\"dtype\"] = self.dtype\n        return func(*args, **kwargs)", "\n\n@contextmanager\ndef quantization(mode: str = None):\n    quantized_linear_cls = None\n    if mode == 'llm.int8':\n        from .quantization import Linear8bitLt\n        quantized_linear_cls = Linear8bitLt\n    elif mode == 'gptq.int4':\n        from .quantization import ColBlockQuantizedLinear\n        quantized_linear_cls = functools.partial(ColBlockQuantizedLinear, bits=4, tile_cols=-1)\n    elif mode == 'gptq.int8':\n        from .quantization import ColBlockQuantizedLinear\n        quantized_linear_cls = functools.partial(ColBlockQuantizedLinear, bits=8, tile_cols=-1)\n    elif mode is not None:\n        raise ValueError(f\"Unknown quantization mode: {mode}\")\n\n    enabled = mode is not None\n    torch_linear_cls = torch.nn.Linear\n    if enabled:\n        torch.nn.Linear = quantized_linear_cls\n    yield\n    if enabled:\n        torch.nn.Linear = torch_linear_cls", "\n\n# this is taken from torchhacks https://github.com/lernapparat/torchhacks\n\n\nclass NotYetLoadedTensor:\n    def __init__(self, metatensor, archiveinfo, storageinfo, rebuild_args):\n        self.metatensor = metatensor\n        self.archiveinfo = archiveinfo\n        self.storageinfo = storageinfo\n        self.rebuild_args = rebuild_args\n\n    @classmethod\n    def rebuild_from_type_v2(cls, func, new_type, args, state, *, archiveinfo=None):\n        ret = func(*args)\n        if isinstance(ret, NotYetLoadedTensor):\n            old_lt = ret._load_tensor\n\n            def _load_tensor():\n                t = old_lt()\n                return torch._tensor._rebuild_from_type_v2(\n                    lambda: t, new_type, (), state\n                )\n\n            ret._load_tensor = _load_tensor\n            return ret\n        return torch._tensor._rebuild_from_type_v2(func, new_type, args, state)\n\n    @classmethod\n    def rebuild_parameter(\n        cls, data, requires_grad, backward_hooks, *, archiveinfo=None\n    ):\n        if isinstance(data, NotYetLoadedTensor):\n            old_lt = data._load_tensor\n\n            def _load_tensor():\n                t = old_lt()\n                return torch._utils._rebuild_parameter(t, requires_grad, backward_hooks)\n\n            data._load_tensor = _load_tensor\n            return data\n        return torch._utils._rebuild_parameter(data, requires_grad, backward_hooks)\n\n    @classmethod\n    def rebuild_tensor_v2(\n        cls,\n        storage,\n        storage_offset,\n        size,\n        stride,\n        requires_grad,\n        backward_hooks,\n        metadata=None,\n        *,\n        archiveinfo=None,\n    ):\n        rebuild_args = (\n            storage_offset,\n            size,\n            stride,\n            requires_grad,\n            backward_hooks,\n            metadata,\n        )\n        metatensor = torch._utils._rebuild_tensor_v2(\n            storage,\n            storage_offset,\n            size,\n            stride,\n            requires_grad,\n            backward_hooks,\n            metadata,\n        )\n        storageinfo = storage.archiveinfo\n        return NotYetLoadedTensor(metatensor, archiveinfo, storageinfo, rebuild_args)\n\n    def _load_tensor(self):\n        name, storage_cls, fn, device, size = self.storageinfo\n        dtype = self.metatensor.dtype\n\n        uts = (\n            self.archiveinfo.zipfile_context.zf.get_storage_from_record(\n                f\"data/{fn}\",\n                size * torch._utils._element_size(dtype),\n                torch.UntypedStorage,\n            )\n            ._typed_storage()\n            ._untyped_storage\n        )\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            storage = torch.storage.TypedStorage(\n                wrap_storage=uts, dtype=self.metatensor.dtype, _internal=True\n            )\n        tensor = torch._utils._rebuild_tensor_v2(storage, *self.rebuild_args)\n        return tensor\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        loaded_args = [\n            (a._load_tensor() if isinstance(a, NotYetLoadedTensor) else a) for a in args\n        ]\n        res = func(*loaded_args, **kwargs)\n        # gc.collect would be costly here, maybe do it optionally\n        return res\n\n    def __getattr__(self, name):\n        # properties\n        ## TODO: device, is_...??\n        ## TODO: mH, mT, H, T, data, imag, real\n        ## name ???\n        if name in {\n            \"dtype\",\n            \"grad\",\n            \"grad_fn\",\n            \"layout\",\n            \"names\",\n            \"ndim\",\n            \"output_nr\",\n            \"requires_grad\",\n            \"retains_grad\",\n            \"shape\",\n            \"volatile\",\n        }:\n            return getattr(self.metatensor, name)\n        if name in {\"size\"}:\n            return getattr(self.metatensor, name)\n        # materializing with contiguous is needed for quantization\n        if name in {\"contiguous\"}:\n            return getattr(self._load_tensor(), name)\n\n        raise AttributeError(f\"{type(self)} does not have {name}\")\n\n    def __repr__(self):\n        return f\"NotYetLoadedTensor({repr(self.metatensor)})\"", "\n\nclass LazyLoadingUnpickler(pickle.Unpickler):\n    def __init__(self, file, zipfile_context):\n        super().__init__(file)\n        self.zipfile_context = zipfile_context\n\n    def find_class(self, module, name):\n        res = super().find_class(module, name)\n        if module == \"torch._utils\" and name == \"_rebuild_tensor_v2\":\n            return functools.partial(\n                NotYetLoadedTensor.rebuild_tensor_v2, archiveinfo=self\n            )\n        elif module == \"torch._tensor\" and name == \"_rebuild_from_type_v2\":\n            return functools.partial(\n                NotYetLoadedTensor.rebuild_from_type_v2, archiveinfo=self\n            )\n        elif module == \"torch._utils\" and name == \"_rebuild_parameter\":\n            return functools.partial(\n                NotYetLoadedTensor.rebuild_parameter, archiveinfo=self\n            )\n        return res\n\n    def persistent_load(self, pid):\n        name, cls, fn, device, size = pid\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            s = torch.storage.TypedStorage(dtype=cls().dtype, device=\"meta\")\n        s.archiveinfo = pid\n        return s", "\n\nclass lazy_load:\n    def __init__(self, fn):\n        self.zf = torch._C.PyTorchFileReader(str(fn))\n        with BytesIO(self.zf.get_record(\"data.pkl\")) as pkl:\n            mup = LazyLoadingUnpickler(pkl, self)\n            self.sd = mup.load()\n\n    def __enter__(self):\n        return self.sd\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        del self.zf  # I don't think there is a way to force closing...\n        self.zf = None", "\n\nclass SavingProxyForStorage:\n    def __init__(self, obj, saver, protocol_version=5):\n        self.protocol_version = protocol_version\n        self.saver = saver\n        if not (isinstance(obj, torch.storage.TypedStorage) or torch.is_storage(obj)):\n            raise TypeError(f\"expected storage, not {type(obj)}\")\n\n        # this logic is taken from PyTorch 2.0+ torch/serialization.py\n        if isinstance(obj, torch.storage.TypedStorage):\n            # PT upstream wants to deprecate this eventually...\n            storage = obj._untyped_storage\n            storage_type_str = obj._pickle_storage_type()\n            storage_type = getattr(torch, storage_type_str)\n            storage_numel = obj._size()\n        else:\n            storage = obj\n            storage_type = normalize_storage_type(type(obj))\n            storage_numel = storage.nbytes()\n\n        storage_key = saver._write_storage_and_return_key(storage)\n        location = torch.serialization.location_tag(storage)\n\n        self.storage_info = (\n            \"storage\",\n            storage_type,\n            storage_key,\n            location,\n            storage_numel,\n        )\n\n    def __reduce_ex__(self, protocol_version):\n        assert False, \"this should be handled with out of band\"", "\n\nclass SavingProxyForTensor:\n    def __init__(self, tensor, saver, protocol_version=5):\n        self.protocol_version = protocol_version\n        self.reduce_ret_fn, (storage, *other_reduce_args) = tensor.__reduce_ex__(\n            protocol_version\n        )\n        assert isinstance(\n            storage, torch.storage.TypedStorage\n        ), \"Please check for updates\"\n        storage_proxy = SavingProxyForStorage(\n            storage, saver, protocol_version=protocol_version\n        )\n        self.reduce_args = (storage_proxy, *other_reduce_args)\n\n    def __reduce_ex__(self, protocol_version):\n        if protocol_version != self.protocol_version:\n            raise RuntimeError(\n                f\"Unexpected protocol version: expected {self.protocol_version}, got {protocol_version}\"\n            )\n        return self.reduce_ret_fn, self.reduce_args", "\n\nclass IncrementalPyTorchPickler(pickle.Pickler):\n    def __init__(self, saver, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.storage_dtypes = {}\n        self.saver = saver\n        self.id_map = {}\n\n    # this logic is taken from PyTorch 2.0+ torch/serialization.py\n    def persistent_id(self, obj):\n        # FIXME: the docs say that persistent_id should only return a string\n        # but torch store returns tuples. This works only in the binary protocol\n        # see\n        # https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects\n        # https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537\n        if isinstance(obj, SavingProxyForStorage):\n            return obj.storage_info\n\n        if isinstance(obj, torch.storage.TypedStorage) or torch.is_storage(obj):\n            if isinstance(obj, torch.storage.TypedStorage):\n                # TODO: Once we decide to break serialization FC, this case\n                # can be deleted\n                storage = obj._untyped_storage\n                storage_dtype = obj.dtype\n                storage_type_str = obj._pickle_storage_type()\n                storage_type = getattr(torch, storage_type_str)\n                storage_numel = obj._size()\n\n            else:\n                storage = obj\n                storage_dtype = torch.uint8\n                storage_type = normalize_storage_type(type(obj))\n                storage_numel = storage.nbytes()\n\n            # If storage is allocated, ensure that any other saved storages\n            # pointing to the same data all have the same dtype. If storage is\n            # not allocated, don't perform this check\n            if storage.data_ptr() != 0:\n                if storage.data_ptr() in self.storage_dtypes:\n                    if storage_dtype != self.storage_dtypes[storage.data_ptr()]:\n                        raise RuntimeError(\n                            \"Cannot save multiple tensors or storages that \"\n                            \"view the same data as different types\"\n                        )\n                else:\n                    self.storage_dtypes[storage.data_ptr()] = storage_dtype\n\n            storage_key = self.id_map.get(storage._cdata)\n            if storage_key is None:\n                storage_key = self.saver._write_storage_and_return_key(storage)\n                self.id_map[storage._cdata] = storage_key\n            location = torch.serialization.location_tag(storage)\n\n            return (\"storage\", storage_type, storage_key, location, storage_numel)\n\n        return None", "\n\nclass incremental_save:\n    def __init__(self, name):\n        self.name = name\n        self.zipfile = torch._C.PyTorchFileWriter(str(name))\n        self.has_saved = False\n        self.next_key = 0\n\n    def __enter__(self):\n        return self\n\n    def store_early(self, tensor):\n        if isinstance(tensor, torch.Tensor):\n            return SavingProxyForTensor(tensor, self)\n        raise TypeError(f\"can only store tensors early, not {type(tensor)}\")\n\n    def save(self, obj):\n        if self.has_saved:\n            raise RuntimeError(\"have already saved\")\n        # Write the pickle data for `obj`\n        data_buf = BytesIO()\n        pickler = IncrementalPyTorchPickler(self, data_buf, protocol=5)\n        pickler.dump(obj)\n        data_value = data_buf.getvalue()\n        self.zipfile.write_record(\"data.pkl\", data_value, len(data_value))\n        self.has_saved = True\n\n    def _write_storage_and_return_key(self, storage):\n        if self.has_saved:\n            raise RuntimeError(\"have already saved\")\n        key = self.next_key\n        self.next_key += 1\n        name = f\"data/{key}\"\n        if storage.device.type != \"cpu\":\n            storage = storage.cpu()\n        num_bytes = storage.nbytes()\n        self.zipfile.write_record(name, storage.data_ptr(), num_bytes)\n        return key\n\n    def __exit__(self, type, value, traceback):\n        self.zipfile.write_end_of_file()", ""]}
{"filename": "lit_llama/lora.py", "chunked_list": ["# Derived from https://github.com/microsoft/LoRA\n#  ------------------------------------------------------------------------------------------\n#  Copyright (c) Microsoft Corporation. All rights reserved.\n#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------\n\nr\"\"\"\n    Low Ranking Adaptation for LLMs scheme.\n\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510", "\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2506         h         \u2506\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u25b2\n                       |\n                       +\n                    /     \\\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e     Matrix initialization:\n    \u2506                 \u2506     \\      B      /      B = 0", "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e     Matrix initialization:\n    \u2506                 \u2506     \\      B      /      B = 0\n    \u2506   pretrained    \u2506      \\    r*d    /       A = N(0, sigma^2)\n    \u2506    weights      \u2506       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u2506                 \u2506       |    r    |        r - rank\n    \u2506   W e R^(d*d)   \u2506       | \u25c0\u2500\u2500\u2500\u2500\u2500\u25b6 |\n    \u2506                 \u2506       \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      /     A     \\\n              \u25b2             /     d*r     \\\n               \\           \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f", "              \u25b2             /     d*r     \\\n               \\           \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                \\                \u25b2\n                 \\              /\n                  \\            /\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2506         x         \u2506\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWith LoRA (Low Ranking Adaptation: https://arxiv.org/abs/2106.09685) instead of learning weights of size d*d,", "\nWith LoRA (Low Ranking Adaptation: https://arxiv.org/abs/2106.09685) instead of learning weights of size d*d,\nwe can freeze the pretrained weights and instead learn two matrices of size d*r and r*d (they will store weight updates\nfor the pretrained weights): the number of parameters in this case will be reduced drastically (depending on the rank of\ncourse) yet after multiplication of matrices d*r and r*d we will get a matrix d*d which we can sum with frozen\npretrained weights and thus fine-tune the model.\n\nThe goal of this approach is to move weight updates into a separate matrix which is decomposed with\ntwo matrices of a lower rank.\n\"\"\"", "two matrices of a lower rank.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport math\nfrom typing import Dict, List\n", "from typing import Dict, List\n\nimport lit_llama.model as llama\n\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\n\n\nclass LoRALayer():\n    def __init__(\n        self, \n        r: int, \n        lora_alpha: int, \n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        \"\"\"Store LoRA specific attributes in a class.\n\n        Args:\n            r: rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of\n                the weights of the model.  The rank can be as low as 1: https://arxiv.org/pdf/2106.09685.pdf (section 7.2)\n            lora_alpha: alpha is needed for scaling updates as alpha/r\n                \"This scaling helps to reduce the need to retune hyperparameters when we vary r\"\n                https://arxiv.org/pdf/2106.09685.pdf (section 4.1)\n            lora_dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\n            merge_weights: whether we want to merge pretrained weights and LoRA weight updates. This is useful if one wants to use\n                fine-tuned model as a standalone one (without storing LoRA weights separately) plus it helps to reduce\n                overhead during inference.\n        \"\"\"\n        self.r = r\n        self.lora_alpha = lora_alpha\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights", "class LoRALayer():\n    def __init__(\n        self, \n        r: int, \n        lora_alpha: int, \n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        \"\"\"Store LoRA specific attributes in a class.\n\n        Args:\n            r: rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of\n                the weights of the model.  The rank can be as low as 1: https://arxiv.org/pdf/2106.09685.pdf (section 7.2)\n            lora_alpha: alpha is needed for scaling updates as alpha/r\n                \"This scaling helps to reduce the need to retune hyperparameters when we vary r\"\n                https://arxiv.org/pdf/2106.09685.pdf (section 4.1)\n            lora_dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\n            merge_weights: whether we want to merge pretrained weights and LoRA weight updates. This is useful if one wants to use\n                fine-tuned model as a standalone one (without storing LoRA weights separately) plus it helps to reduce\n                overhead during inference.\n        \"\"\"\n        self.r = r\n        self.lora_alpha = lora_alpha\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights", "\n\nclass MergedLinear(nn.Linear, LoRALayer):\n    # LoRA implemented in a dense layer\n    def __init__(\n        self, \n        # \u2193 this part is for pretrained weights\n        in_features: int, \n        out_features: int, \n        # \u2193 the remaining part is for LoRA\n        r: int = 0, \n        lora_alpha: int = 1, \n        lora_dropout: float = 0.,\n        enable_lora: List[bool] = [False],\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        **kwargs\n    ):\n        \"\"\"LoRA wrapper around linear class that is used for calculation of q, k and v matrices.\n\n        This class has three weight matrices:\n            1. Pretrained weights are stored as `self.weight` (because of the nn.Linear inheritance)\n            2. LoRA A matrix as `self.lora_A`\n            3. LoRA B matrix as `self.lora_B`\n        Only LoRA's A and B matrices are updated, pretrained weights stay frozen.\n\n        Args:\n            in_features: number of input features of the pretrained weights\n            out_features: number of output features of the pretrained weights\n            r: rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of\n                the weights of the model.  The rank can be as low as 1: https://arxiv.org/pdf/2106.09685.pdf (section 7.2)\n            lora_alpha: alpha is needed for scaling updates as alpha/r\n                \"This scaling helps to reduce the need to retune hyperparameters when we vary r\"\n                https://arxiv.org/pdf/2106.09685.pdf (section 4.1)\n            lora_dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\n            enable_lora: MergeLinear class is for attention mechanism where qkv are calculated with a single weight matrix. If we\n                don't want to apply LoRA for all three (query, key and value) we can set it as False. For example if we want\n                to apply LoRA only to `query` and `value` but keep `key` without weight updates we should pass `[True,\n                False, True]`\n            fan_in_fan_out: set this to True if the layer to replace stores weight like (fan_in, fan_out).  For example, gpt-2 uses\n                `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`\n                https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#LL53C9-L53C112\n            merge_weights: whether we want to merge pretrained weights and LoRA weight updates. This is useful if one wants to use\n                fine-tuned model as a standalone one (without storing LoRA weight separately) plus it helps to reduce\n                overhead during inference.\n        \"\"\"\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n                           merge_weights=merge_weights)\n        assert out_features % len(enable_lora) == 0, \\\n            'The length of enable_lora must divide out_features'\n        self.enable_lora = enable_lora\n        self.fan_in_fan_out = fan_in_fan_out\n\n        # Actual trainable parameters\n        # To better understand initialization let's imagine that we have such parameters:\n        # \u26ac in_features: 128 (embeddings_size)\n        # \u26ac out_features: 384 (3 * embedding_size)\n        # \u26ac r: 2\n        # \u26ac enable_lora: [True, False, True]\n        if r > 0 and any(enable_lora):\n            self.lora_A = nn.Parameter(\n                self.weight.new_zeros((r * sum(enable_lora), in_features)))  # (4, 128)\n            self.lora_B = nn.Parameter(\n                self.weight.new_zeros((out_features // len(enable_lora) * sum(enable_lora), r))  # (256, 2)\n            ) # weights for Conv1D with groups=sum(enable_lora)\n            # Notes about shapes above\n            # - self.lora_A has shape (4, 128): 4 because rank is 2 and LoRA is applied only to two matrices;\n            # 128 is the input size of the x (embedding size). (4, 128) and not (128, 4) because later on in\n            # F.linear function weights are automatically transposed. In addition conv1d requires channels to\n            # be before seq length\n            # - self.lora_B has shape (256, 2): 256 because LoRA is applied only to two matrices, so the output is\n            # 128*2; 2 tells to have two channels per group for group convolution\n\n            # Scaling:\n            # This balances the pretrained model`s knowledge and the new task-specific adaptation\n            # https://lightning.ai/pages/community/tutorial/lora-llm/\n            # So, set alpha to 1.0 to fully add LoRA. If the LoRA seems to have too much effect (i.e., overfitted), set\n            # alpha to lower value. If the LoRA seems to have too little effect, set alpha to higher than 1.0. You can\n            # tune these values to your needs. This value can be even slightly greater than 1.0!\n            # https://github.com/cloneofsimo/lora\n            self.scaling = self.lora_alpha / self.r\n\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False # (384, 128)\n\n            # Compute the indices\n            # Indices are needed to properly pad weight updates with zeros. If we want to fine-tune queries and values,\n            # but not keys, then the weights update should be:\n            #\n            # [[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,],\n            #  [....................................],\n            #  [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,]]\n            #      \u2191              \u2191            \u2191\n            # ________________________________________\n            # | query         | key       | value    |\n            # ----------------------------------------\n            self.lora_ind = self.weight.new_zeros(\n                (out_features, ), dtype=torch.bool\n            ).view(len(enable_lora), -1)  # (3, 128)\n            self.lora_ind[enable_lora, :] = True  # (3, 128)\n            self.lora_ind = self.lora_ind.view(-1)  # (384,)\n        self.reset_parameters()\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n    def reset_parameters(self):\n        \"\"\"Reset all the weights, even including pretrained ones.\"\"\"\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, 'lora_A'):\n            # initialize A the same way as the default for nn.Linear and B to zero\n            # Wondering why 'a' is equal to math.sqrt(5)?: https://github.com/pytorch/pytorch/issues/15314\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B)\n\n    def zero_pad(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Properly pad weight updates with zeros.\n\n        If, based on `self.enable_lora`, we want to fine-tune queries and values, but not keys,\n        then the weights update should be:\n\n        [[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,],\n         [....................................],\n         [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,]]\n            \u2191              \u2191            \u2191\n        ________________________________________\n        | query         | key       | value    |\n        ----------------------------------------\n\n        Args:\n            x: tensor with weights update that will be padded with zeros if necessary\n\n        Returns:\n            A tensor with weight updates and zeros for deselected q, k or v\n        \"\"\"\n        # Let's image that:\n        # \u26ac input x has shape (64, 64, 256): (batch_size, sequence_length, embeddings_size)\n        # \u26ac embeddings_size: 128\n        # \u26ac self.out_features: 384 (3 * embeddings_size)\n        # \u26ac enable_lora: [True, False, True]\n        # Then x has embeddings_size of 256 (2 * 128 as enable_lora only for query and value, not keys) and expected\n        # embeddings_size is 384 (self.out_features), so that means that we need to pad from 256 to 384 with zeros, but\n        # only for key updates (this is where self.lora_ind comes in handy)\n        # Note: double transpose (in the beginning and in the end) is basically a guard for two-dimensional tensors\n        # for example when we want to merge/unmerge LoRA weights and pretrained weights\n        x = x.transpose(0, 1)\n        result = x.new_zeros((*x.shape[:-1], self.out_features))  # (64, 64, 384)\n        result = result.view(-1, self.out_features)  # (4096, 384)\n        result[:, self.lora_ind] = x.reshape(\n            -1, self.out_features // len(self.enable_lora) * sum(self.enable_lora)\n        )  # (4096, 256)\n        return result.view((*x.shape[:-1], self.out_features)).transpose(0, 1)  # (64, 64, 384)\n\n    def train(self, mode: bool = True):\n        \"\"\"Set the module into train or eval mode if `mode` is True of False respectively.\n\n        For train mode (train(True)) if weights are merged we need to subtract weights updates (LoRA_A @ LoRA_B) from\n        pretrained weights so we can continue training LoRA's matrices A and B and keep pretrained weights frozen.\n\n        For eval mode (train(False)) if weights are not merged we need to add weight updates to pretrained weights in\n        order to reduce computational overhead during inference.\n\n        Args:\n            mode: if True the module will be set into train mode (affects Dropout and BatchNorm), if False - eval mode.\n\n        \"\"\"\n        def T(w):\n            return w.T if self.fan_in_fan_out else w\n        # despite being called from nn.Linear this method will put all layers into train mode, including nn.Dropout\n        # of course except parameters (such as self.lora_A, self.lora_B)\n        nn.Linear.train(self, mode)\n\n        # if train(True) -> unmerge unless we already have them unmerged\n        # if train(False) -> merge unless we already have them merged\n        should = self.merged if mode else not self.merged\n\n        # Let's assume that:\n        # \u26ac self.weight.data: (384, 128) or (3 * embedding_size, embedding_size)\n        # \u26ac self.lora_A.data: (4, 128)\n        # \u26ac self.lora_B.data: (256, 2)\n        if self.merge_weights and should:\n            if self.r > 0 and any(self.enable_lora):\n                delta_w = F.conv1d(\n                    self.lora_A.data.unsqueeze(0),   # (4, 128) -> (1, 4, 128)\n                    self.lora_B.data.unsqueeze(-1),  # (256, 2) -> (256, 2, 1)\n                    groups=sum(self.enable_lora)\n                ).squeeze(0) # (1, 4, 128) @ (256, 2, 1) -> (1, 256, 128) -> (256, 128)\n                # -1: W = W - delta_W (unmerge), +1: W = W + delta_W (merge)\n                sign = -1 if mode else 1\n                self.weight.data += sign * self.zero_pad(T(delta_w * self.scaling)) # (256, 128) after zero_pad (384, 128)\n            self.merged = not mode\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Do the forward pass.\n\n        If LoRA's weights are merged with pretrained ones then it's a simple matrix multiplication.\n        If not, then multiply pretrained weights with input, apply LoRA on input and do summation.\n\n        Args:\n            x: input tensor of shape (batch_size, context_length, embedding_size)\n\n        Returns:\n            Output tensor of shape (batch_size, context_length, 3 * embedding_size)\n        \"\"\"\n        def T(w):\n            return w.T if self.fan_in_fan_out else w\n\n        # Let's assume that:\n        # \u26ac x: (64, 64, 128) or (batch_size, context_length, embedding_size)\n        # \u26ac self.weight: (384, 128) or (3 * embedding_size, embedding_size)\n        # \u26ac self.lora_A.data: (4, 128)\n        # \u26ac self.lora_B.data: (256, 2)\n\n        # the logic here is that the weights are merged only during inference\n        # so if they are merged we don't need to do anything with LoRA's A and B matrices\n        # but if the weights are not merged that means that the forward method is called during\n        # training and we need to forward pass input through pretrained weights, LoRA A and B matrices\n        # and do the summation (as per scheme at the top of the file)\n        if self.merged:\n            return F.linear(x, T(self.weight), bias=self.bias)\n        else:\n            # `F.linear` automatically transposes the second argument (T(self.weight) in our case)\n            result = F.linear(x, T(self.weight), bias=self.bias)  # (64, 64, 128) @ (384, 128) -> (64, 64, 384)\n            if self.r > 0:\n                after_A = F.linear(self.lora_dropout(x), self.lora_A)  # (64, 64, 128) @ (4, 128) -> (64, 64, 4)\n                # For F.conv1d:\n                # \u26ac input: input tensor of shape (mini-batch, in_channels, iW)\n                # \u26ac weight: filters of shape (out_channels, in_channels/groups, kW)\n                # \u26ac groups: split input into groups, in_channels should be divisible by the number of groups. Default: 1\n                # presumably iW - sequence width/length, kW - kernel width\n                after_B = F.conv1d(\n                    after_A.transpose(-2, -1),  # (64, 64, 4) -> (64, 4, 64)\n                    self.lora_B.unsqueeze(-1),  # (256, 2) -> (256, 2, 1)\n                    groups=sum(self.enable_lora)\n                ).transpose(-2, -1)  # (64, 4, 64) @ (256, 2, 1) -> (64, 256, 64) -> (64, 64, 256)\n                result += self.zero_pad(after_B) * self.scaling  # (64, 64, 256) after zero_pad (64, 64, 384)\n            return result", "\n\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n    \"\"\"Freeze all modules except LoRA's and depending on 'bias' value unfreezes bias weights.\n\n    Args:\n        model: model with LoRA layers\n        bias: \n            ``\"none\"``: all bias weights will be frozen,\n            ``\"lora_only\"``: only bias weight for LoRA layers will be unfrozen,\n            ``\"all\"``: all bias weights will be unfrozen.\n\n    Raises:\n        NotImplementedError: if `bias` not in [\"none\", \"lora_only\", \"all\"]\n    \"\"\"\n    # freeze all layers except LoRA's\n    for n, p in model.named_parameters():\n        if 'lora_' not in n:\n            p.requires_grad = False\n\n    # depending on the `bias` value unfreeze bias weights\n    if bias == 'none':\n        return\n    elif bias == 'all':\n        for n, p in model.named_parameters():\n            if 'bias' in n:\n                p.requires_grad = True\n    elif bias == 'lora_only':\n        for m in model.modules():\n            if isinstance(m, LoRALayer) and \\\n                hasattr(m, 'bias') and \\\n                m.bias is not None:\n                    m.bias.requires_grad = True\n    else:\n        raise NotImplementedError", "\n\ndef lora_state_dict(model: nn.Module, bias: str = 'none') -> Dict[str, torch.Tensor]:\n    \"\"\"Return state_dict with weights of LoRA's A and B matrices and with biases depending on the `bias` value.\n\n    Args:\n        model: model with LoRA layers\n        bias: \n            ``\"none\"``: state dict will not store bias weights,\n            ``\"lora_only\"``: state dict will store bias weights only from LoRA layers,\n            ``\"all\"``: state dict will store all bias weights.\n\n    Returns:\n        Weights and biases of LoRA layers\n\n    Raises:\n        NotImplementedError: if `bias` not in [\"none\", \"lora_only\", \"all\"]\n    \"\"\"\n    my_state_dict = model.state_dict()\n    if bias == 'none':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k}\n    elif bias == 'all':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k or 'bias' in k}\n    elif bias == 'lora_only':\n        to_return = {}\n        for k in my_state_dict:\n            if 'lora_' in k:\n                to_return[k] = my_state_dict[k]\n                bias_name = k.split('lora_')[0]+'bias'\n                if bias_name in my_state_dict:\n                    to_return[bias_name] = my_state_dict[bias_name]\n        return to_return\n    else:\n        raise NotImplementedError", "\n\n@dataclass\nclass LoRAConfig:\n    r: float = 0.0\n    alpha: float = 1.0\n    dropout: float = 0.0\n\n\nclass CausalSelfAttention(llama.CausalSelfAttention):\n    lora_config = None\n\n    def __init__(self, config: llama.LLaMAConfig) -> None:\n        \"\"\"Causal self-attention with calculating qkv matrices with a single matrix* and Low Ranking Adaptation for\n        parameter-efficient fine-tuning.\n\n        *Instead of creating multiple heads and concatenating the result (in addition to creating separate matrices for\n        query, key and value for each head) we can do this in a single pass with a single weight matrix.\n\n        Args:\n            config: \n                ``\"block_size\"``: size of the context of the model,\n                ``\"vocab_size\"``: number of unique tokens,\n                ``\"padded_vocab_size\"``: padded size of the vocabulary to the nearest multiple of 64 (leads to a greater performance),\n                ``\"n_layer\"``: number of transformer blocks (self-attention + MLP),\n                ``\"n_head\"``: number of heads in multi-head attention mechanism,\n                ``\"n_embd\"``: size of the embedding: vector representation of each token.\n        \"\"\"\n        # Skip the parent class __init__ altogether and replace it to avoid\n        # useless allocations\n        nn.Module.__init__(self)\n        assert config.n_embd % config.n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = MergedLinear(\n            in_features=config.n_embd,\n            out_features=3 * config.n_embd,\n            r=self.lora_config.r,\n            lora_alpha=self.lora_config.alpha,\n            lora_dropout=self.lora_config.dropout,\n            enable_lora=[True, False, True],\n            fan_in_fan_out = False,\n            merge_weights=True,\n            bias=False)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        self.rope_cache = None", "\nclass CausalSelfAttention(llama.CausalSelfAttention):\n    lora_config = None\n\n    def __init__(self, config: llama.LLaMAConfig) -> None:\n        \"\"\"Causal self-attention with calculating qkv matrices with a single matrix* and Low Ranking Adaptation for\n        parameter-efficient fine-tuning.\n\n        *Instead of creating multiple heads and concatenating the result (in addition to creating separate matrices for\n        query, key and value for each head) we can do this in a single pass with a single weight matrix.\n\n        Args:\n            config: \n                ``\"block_size\"``: size of the context of the model,\n                ``\"vocab_size\"``: number of unique tokens,\n                ``\"padded_vocab_size\"``: padded size of the vocabulary to the nearest multiple of 64 (leads to a greater performance),\n                ``\"n_layer\"``: number of transformer blocks (self-attention + MLP),\n                ``\"n_head\"``: number of heads in multi-head attention mechanism,\n                ``\"n_embd\"``: size of the embedding: vector representation of each token.\n        \"\"\"\n        # Skip the parent class __init__ altogether and replace it to avoid\n        # useless allocations\n        nn.Module.__init__(self)\n        assert config.n_embd % config.n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = MergedLinear(\n            in_features=config.n_embd,\n            out_features=3 * config.n_embd,\n            r=self.lora_config.r,\n            lora_alpha=self.lora_config.alpha,\n            lora_dropout=self.lora_config.dropout,\n            enable_lora=[True, False, True],\n            fan_in_fan_out = False,\n            merge_weights=True,\n            bias=False)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.block_size = config.block_size\n        self.rope_cache = None", "\n\n@contextmanager\ndef lora(r, alpha, dropout, enabled: bool = True):\n    \"\"\"Apply context manager under which you can instantiate the model with LoRA.\n\n    In a nutshell the code inside this function forces to use LoRA variant of causal self-attention\n    instead of the original one (without LoRA).\n\n    Args:\n        r: rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of\n            the weights of the model.  The rank can be as low as 1: https://arxiv.org/pdf/2106.09685.pdf (section 7.2)\n        alpha: alpha is needed for scaling updates as alpha/r\n            \"This scaling helps to reduce the need to retune hyperparameters when we vary r\"\n            https://arxiv.org/pdf/2106.09685.pdf (section 4.1)\n        dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\n        enabled: enables/disables LoRA\n    \"\"\"\n    if not enabled:\n        yield\n        return\n\n    CausalSelfAttention.lora_config = LoRAConfig(r=r, alpha=alpha, dropout=dropout)\n    # when entering context manager replace link to causal self-attention class from original\n    # to a variant with LoRA\n    causal_self_attention = llama.CausalSelfAttention\n    llama.CausalSelfAttention = CausalSelfAttention\n    yield\n    # when exiting context manager - restore link to original causal self-attention class\n    llama.CausalSelfAttention = causal_self_attention\n\n    CausalSelfAttention.lora_config = None", ""]}
{"filename": "lit_llama/packed_dataset.py", "chunked_list": ["# Very loosely inspired by indexed_dataset in Fairseq, Megatron\n# https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/data/indexed_dataset.py\n\n\nimport os\nimport struct\nimport random\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom torch.utils.data import IterableDataset, get_worker_info\n\n\ndtypes = {\n    1: np.uint8,\n    2: np.int8,\n    3: np.int16,\n    4: np.int32,", "    3: np.int16,\n    4: np.int32,\n    5: np.int64,\n    6: np.float32,\n    7: np.float64,\n    8: np.uint16,\n}\n\n\ndef code(dtype):\n    for k in dtypes.keys():\n        if dtypes[k] == dtype:\n            return k\n    raise ValueError(dtype)", "\ndef code(dtype):\n    for k in dtypes.keys():\n        if dtypes[k] == dtype:\n            return k\n    raise ValueError(dtype)\n\n\nHDR_MAGIC = b\"LITPKDS\"\nHDR_SIZE = 24  # bytes", "HDR_MAGIC = b\"LITPKDS\"\nHDR_SIZE = 24  # bytes\n\n\nclass PackedDataset(IterableDataset):\n    def __init__(self, filenames, n_chunks, block_size, seed=12345, shuffle=True, wrap=False, num_processes=1, process_rank=0):\n        self._filenames = filenames\n        self._n_chunks = n_chunks\n        self._block_size = block_size\n        self._seed = seed\n        self._shuffle = shuffle\n        self._wrap = wrap\n        self._num_processes = num_processes\n        self._process_rank = process_rank\n\n    def __iter__(self):\n        worker_info = get_worker_info()\n        num_workers = worker_info.num_workers if worker_info is not None else 1\n        worker_id = worker_info.id if worker_info is not None else 0\n        num_shards = num_workers * self._num_processes\n        shard_id = self._process_rank * num_workers + worker_id\n\n        max_num_files = len(self._filenames) // num_shards * num_shards\n        filenames = self._filenames[shard_id : max_num_files : num_shards]\n\n        return PackedDatasetIterator(\n            filenames=filenames,\n            n_chunks=self._n_chunks,\n            block_size=self._block_size,\n            seed=self._seed,\n            shuffle=self._shuffle,\n            wrap=self._wrap,\n        )", "\n\nclass PackedDatasetBuilder(object):\n    def __init__(\n        self,\n        outdir,\n        prefix,\n        chunk_size,\n        sep_token,\n        dtype=\"auto\",\n        vocab_size=None,\n    ):\n        if dtype == \"auto\":\n            if vocab_size is None:\n                raise ValueError(\"vocab_size cannot be None when dtype='auto'\")\n            if vocab_size is not None and vocab_size < 65500:\n                self._dtype = np.uint16\n            else:\n                self._dtype = np.int32\n        else:\n            self._dtype = dtype\n        self._counter = 0\n        self._chunk_size = chunk_size\n        self._outdir = outdir\n        self._prefix = prefix\n        self._sep_token = sep_token\n        self._arr = np.zeros(self._chunk_size, dtype=self._dtype)\n        self._arr.fill(self._sep_token)\n        self._idx = 0\n        self._version = 1\n        self._filenames = []\n\n    def _write_chunk(self):\n        filename = f\"{self._prefix}_{self._counter:010d}.bin\"\n        filename = os.path.join(self._outdir, filename)\n\n        with open(filename, \"wb\") as f:\n            f.write(HDR_MAGIC)\n            f.write(struct.pack(\"<Q\", self._version))\n            f.write(struct.pack(\"<B\", code(self._dtype)))\n            f.write(struct.pack(\"<Q\", self._chunk_size))\n            f.write(self._arr.tobytes(order=\"C\"))\n\n        self._filenames.append(filename)\n        self._counter += 1\n        self._arr.fill(self._sep_token)\n        self._idx = 0\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def filenames(self):\n        return self._filenames.copy()\n\n    def add_array(self, arr):\n        while self._idx + arr.shape[0] > self._chunk_size:\n            part_len = self._chunk_size - self._idx\n            self._arr[self._idx : self._idx + part_len] = arr[:part_len]\n            self._write_chunk()\n            arr = arr[part_len:]\n\n        arr_len = arr.shape[0]\n        self._arr[self._idx : self._idx + arr_len] = arr\n        self._idx += arr_len\n\n    def write_reminder(self):\n        self._write_chunk()", "\n\nclass PackedDatasetIterator:\n    def __init__(self, filenames, n_chunks, block_size, seed, shuffle, wrap):\n        self._seed = seed\n        self._shuffle = shuffle\n        self._rng = np.random.default_rng(seed) if shuffle else None\n        self._block_idxs = None\n\n        self._wrap = wrap\n\n        # TODO: instead of filenames, we could have a single text stream\n        #       (or text file) with the sequence of all files to be\n        #       fetched/loaded.\n        self._filenames = filenames\n        self._file_idx = 0\n\n        self._n_chunks = n_chunks\n\n        self._dtype = None\n        self._block_size = block_size\n        self._n_blocks = None\n\n        self._mmaps = []\n        self._buffers = []\n\n        self._block_idxs = []\n        self._curr_idx = 0\n\n        self._load_n_chunks()\n\n    def _read_header(self, path):\n        with open(path, \"rb\") as f:\n            magic = f.read(len(HDR_MAGIC))\n            assert magic == HDR_MAGIC, \"File doesn't match expected format.\"\n            version = struct.unpack(\"<Q\", f.read(8))\n            assert (1,) == version\n            (dtype_code,) = struct.unpack(\"<B\", f.read(1))\n            dtype = dtypes[dtype_code]\n            (chunk_size,) = struct.unpack(\"<Q\", f.read(8))\n        return dtype, chunk_size\n\n    def _close_mmaps(self):\n        for mmap in self._mmaps:\n            mmap._mmap.close()\n\n    def _load_n_chunks(self):\n        self._close_mmaps()\n        self._mmaps = []\n        self._buffers = []\n\n        if self._n_chunks > len(self._filenames[self._file_idx:]):\n            if not self._wrap:\n                raise StopIteration\n            else:\n                self._file_idx = 0\n\n        for i in range(self._n_chunks):\n            filename = self._filenames[self._file_idx + i]\n            if self._dtype is None:\n                self._dtype, self._chunk_size = self._read_header(\n                    filename\n                )\n                self._n_blocks = self._chunk_size // self._block_size\n            # TODO: check header matches with previous files\n            mmap = np.memmap(filename, mode=\"r\", order=\"C\", offset=HDR_SIZE)\n            self._mmaps.append(mmap)\n            self._buffers.append(memoryview(mmap))\n\n        self._file_idx += self._n_chunks\n        n_all_blocks = self._n_chunks * self._n_blocks\n\n        self._block_idxs = (\n            self._rng.permutation(n_all_blocks)\n            if self._shuffle\n            else range(n_all_blocks)\n        )\n\n        self._curr_idx = 0\n\n    def __del__(self):\n        self._close_mmaps()\n        del self._mmaps\n        del self._buffers\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._curr_idx >= len(self._block_idxs):\n            self._load_n_chunks()\n            # TODO: trigger fetching next next n_chunks if remote\n        block_idx = self._block_idxs[self._curr_idx]\n        chunk_id = block_idx // self._n_blocks\n        buffer = self._buffers[chunk_id]\n        elem_id = (block_idx % self._n_blocks) * self._block_size\n        offset = np.dtype(self._dtype).itemsize * elem_id\n        arr = np.frombuffer(\n            buffer, dtype=self._dtype, count=self._block_size, offset=offset\n        )\n        self._curr_idx += 1\n        return torch.from_numpy(arr.astype(np.int64))", "\n\nclass CombinedDataset(IterableDataset):\n    def __init__(self, datasets, seed, weights=None):\n        self._seed = seed\n        self._datasets = datasets\n        self._weights = weights\n        n_datasets = len(datasets)\n        if weights is None:\n            self._weights = [1 / n_datasets] * n_datasets\n\n    def __iter__(self):\n        return CombinedDatasetIterator(self._datasets, self._seed, self._weights)", "\n\nclass CombinedDatasetIterator:\n    def __init__(self, datasets, seed, weights):\n        self._datasets = [iter(el) for el in datasets]\n        self._weights = weights\n        self._rng = random.Random(seed)\n\n    def __next__(self):\n        dataset, = self._rng.choices(self._datasets, weights=self._weights, k=1)\n        return next(dataset)", ""]}
{"filename": "evaluate/full.py", "chunked_list": ["# This mimics GPTQ's evaluation metrics: https://github.com/IST-DASLab/gptq/\n# Thanks to E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\nimport math\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch", "import lightning as L\nimport torch\nimport tqdm\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import EmptyInitOnDevice", "from lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import EmptyInitOnDevice\n\nfrom datasets import load_dataset\n\n\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\n\ndef main(\n    datasets: str = \"wikitext,ptb,c4\",\n    *,\n    # compilation fails as it does not support torch.complex64 for RoPE\n    # compile: bool = False,\n    accelerator: str = \"auto\",\n    checkpoint_path: Optional[Path] = None,\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    model_size: str = \"7B\",\n    dtype: str = \"float32\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        datasets: The datasets to use as a comma separated string\n        # compile: Whether to compile the model.\n        accelerator: The hardware to run on. Possible choices are:\n            ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        dtype: The tensor dtype for choosing the floating-point precision \n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    if not checkpoint_path:\n        checkpoint_path = Path(f\"checkpoints/lit-llama/{model_size}/lit-llama.pth\")\n    assert checkpoint_path.is_file()\n    assert tokenizer_path.is_file()\n\n    fabric = L.Fabric(accelerator=accelerator, devices=1)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    with EmptyInitOnDevice(\n        device=fabric.device, dtype=dtype, quantization_mode=quantize\n    ):\n        print(\"Loading model ...\", file=sys.stderr)\n        t0 = time.time()\n        model = LLaMA.from_name(model_size)\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint)\n        print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    total_toks = 0\n    model = fabric.setup_module(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for dsname in datasets.split(\",\"):\n        test_string = load_eval_data(dsname)\n        encoded_text = tokenizer.encode(\n            test_string, bos=True, eos=False, device=fabric.device\n        )\n        encoded_text = encoded_text[\n            None, : 256 * model.config.block_size\n        ]  # add batch dimension, trim like gptq implementation\n        t0 = time.perf_counter()\n\n        nlls = 0\n        toks = 0\n        with torch.inference_mode():\n            block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)\n            for i in tqdm.tqdm(range(0, encoded_text.shape[1], block_size)):\n                inp = encoded_text[:, i : i + block_size]\n                logits = model(inp)[0]\n                nll = torch.nn.functional.cross_entropy(\n                    logits[:-1], inp[0, 1:].to(dtype=torch.long), reduction=\"sum\"\n                )\n                toks += inp.size(1) - 1\n                nlls += nll.item()\n\n        print(encoded_text.shape, logits.shape)\n        ppl = math.exp(nlls / toks)\n        print(f\"Perplexity on {dsname}: {ppl:.2f}\")\n        total_toks += toks\n\n    t = time.perf_counter() - t0\n    print(\n        f\"\\n\\nTime for inference: {t:.02f} sec total, {total_toks / t:.02f} tokens/sec\",\n        file=sys.stderr,\n    )\n    print(\n        f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\",\n        file=sys.stderr,\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    CLI(main)\n"]}
{"filename": "evaluate/adapter_v2.py", "chunked_list": ["# This mimics GPTQ's evaluation metrics: https://github.com/IST-DASLab/gptq/\n# Thanks to E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\nimport math\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch", "import lightning as L\nimport torch\nimport tqdm\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA", "from lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\nfrom lit_llama.adapter_v2 import add_adapter_v2_parameters_to_linear_layers\nfrom scripts.prepare_alpaca import generate_prompt\n\nfrom datasets import load_dataset\n\n\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\n\n@torch.inference_mode()\ndef main(\n    datasets: str = \"wikitext,ptb,c4\",\n    *,\n    accelerator: str = \"auto\",\n    adapter_path: Path = Path(\"out/adapter_v2/alpaca/lit-llama-adapter-finetuned.pth\"),\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    dtype: str = \"float32\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        datasets: The datasets to use as a comma separated string\n        accelerator: The hardware to run on. Possible choices are:\n            ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\n        adapter_path: Path to the checkpoint with trained adapter weights, which are the output of\n            `finetune_adapter_v2.py`.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        dtype: The tensor dtype for choosing the floating-point precision \n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    assert adapter_path.is_file()\n    assert checkpoint_path.is_file()\n    assert tokenizer_path.is_file()\n\n    fabric = L.Fabric(accelerator=accelerator, devices=1)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(checkpoint_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with EmptyInitOnDevice(\n            device=fabric.device, dtype=dtype, quantization_mode=quantize\n        ):\n            model = LLaMA.from_name(name)\n            add_adapter_v2_parameters_to_linear_layers(model)\n\n        # 1. Load the pretrained weights\n        model.load_state_dict(pretrained_checkpoint, strict=False)\n        # 2. Load the fine-tuned adapter weights\n        model.load_state_dict(adapter_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    total_toks = 0\n    model = fabric.setup_module(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for dsname in datasets.split(\",\"):\n        test_string = load_eval_data(dsname)\n\n        sample = {\"instruction\": test_string, \"input\": input}\n        test_string = generate_prompt(sample)\n\n        encoded_text = tokenizer.encode(\n            test_string, bos=True, eos=False, device=fabric.device\n        )\n        encoded_text = encoded_text[\n            None, : 256 * model.config.block_size\n        ]  # add batch dimension, trim like gptq implementation\n        t0 = time.perf_counter()\n\n        nlls = 0\n        toks = 0\n\n        block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)\n        for i in tqdm.tqdm(range(0, encoded_text.shape[1], block_size)):\n            inp = encoded_text[:, i : i + block_size]\n            logits = model(inp)[0]\n            nll = torch.nn.functional.cross_entropy(\n                logits[:-1], inp[0, 1:].to(dtype=torch.long), reduction=\"sum\"\n            )\n            toks += inp.size(1) - 1\n            nlls += nll.item()\n\n        print(encoded_text.shape, logits.shape)\n        ppl = math.exp(nlls / toks)\n        print(f\"Perplexity on {dsname}: {ppl:.2f}\")\n        total_toks += toks\n\n    t = time.perf_counter() - t0\n    print(\n        f\"\\n\\nTime for inference: {t:.02f} sec total, {total_toks / t:.02f} tokens/sec\",\n        file=sys.stderr,\n    )\n    print(\n        f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\",\n        file=sys.stderr,\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    CLI(main)\n"]}
{"filename": "evaluate/adapter.py", "chunked_list": ["# This mimics GPTQ's evaluation metrics: https://github.com/IST-DASLab/gptq/\n# Thanks to E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\nimport math\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch", "import lightning as L\nimport torch\nimport tqdm\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA", "from lit_llama import Tokenizer\nfrom lit_llama.adapter import LLaMA\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\nfrom scripts.prepare_alpaca import generate_prompt\n\nfrom datasets import load_dataset\n\ninstruction_tuning = True\n\n\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\n\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\n\n@torch.inference_mode()\ndef main(\n    datasets: str = \"wikitext,ptb,c4\",\n    *,\n    # compilation fails as it does not support torch.complex64 for RoPE\n    # compile: bool = False,\n    accelerator: str = \"auto\",\n    adapter_path: Path = Path(\"out/adapter/alpaca/lit-llama-adapter-finetuned.pth\"),\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    dtype: str = \"float32\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        datasets: The datasets to use as a comma separated string\n        # compile: Whether to compile the model.\n        accelerator: The hardware to run on. Possible choices are:\n            ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\n        adapter_path: Path to the checkpoint with trained adapter weights, which are the output of\n            `finetune_adapter.py`.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        dtype: The tensor dtype for choosing the floating-point precision \n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    assert adapter_path.is_file()\n    assert checkpoint_path.is_file()\n    assert tokenizer_path.is_file()\n\n    fabric = L.Fabric(accelerator=accelerator, devices=1)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(checkpoint_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with EmptyInitOnDevice(\n                device=fabric.device, dtype=dtype, quantization_mode=quantize\n        ):\n            model = LLaMA.from_name(name)\n\n        # 1. Load the pretrained weights\n        model.load_state_dict(pretrained_checkpoint, strict=False)\n        # 2. Load the fine-tuned adapter weights\n        model.load_state_dict(adapter_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    total_toks = 0\n    model = fabric.setup_module(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for dsname in datasets.split(\",\"):\n        test_string = load_eval_data(dsname)\n\n        if instruction_tuning:\n            sample = {\"instruction\": test_string, \"input\": input}\n            test_string = generate_prompt(sample)\n\n        encoded_text = tokenizer.encode(\n            test_string, bos=True, eos=False, device=fabric.device\n        )\n        encoded_text = encoded_text[\n            None, : 256 * model.config.block_size\n        ]  # add batch dimension, trim like gptq implementation\n        t0 = time.perf_counter()\n\n        nlls = 0\n        toks = 0\n        block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)\n        for i in tqdm.tqdm(range(0, encoded_text.shape[1], block_size)):\n            inp = encoded_text[:, i : i + block_size]\n            logits = model(inp)[0]\n            nll = torch.nn.functional.cross_entropy(\n                logits[:-1], inp[0, 1:].to(dtype=torch.long), reduction=\"sum\"\n            )\n            toks += inp.size(1) - 1\n            nlls += nll.item()\n\n        print(encoded_text.shape, logits.shape)\n        ppl = math.exp(nlls / toks)\n        print(f\"Perplexity on {dsname}: {ppl:.2f}\")\n        total_toks += toks\n\n    t = time.perf_counter() - t0\n    print(\n        f\"\\n\\nTime for inference: {t:.02f} sec total, {total_toks / t:.02f} tokens/sec\",\n        file=sys.stderr,\n    )\n    print(\n        f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\",\n        file=sys.stderr,\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    CLI(main)\n"]}
{"filename": "evaluate/lora.py", "chunked_list": ["# This mimics GPTQ's evaluation metrics: https://github.com/IST-DASLab/gptq/\n# Thanks to E. Frantar et al GPTQ: Accurate Post-training Compression for GPT, arXiv:2210.17323\nimport math\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch", "import lightning as L\nimport torch\nimport tqdm\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup", "from lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\nfrom lit_llama.lora import lora\nfrom scripts.prepare_alpaca import generate_prompt\n\nfrom datasets import load_dataset\n\ninstruction_tuning = True\nlora_r = 8\nlora_alpha = 16", "lora_r = 8\nlora_alpha = 16\nlora_dropout = 0.05\n\n\ndef load_eval_data(dataset_name: str) -> str:\n    # this mimics gptq datautils\n    if dataset_name == \"wikitext\":\n        # traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n        testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"text\"])\n    elif dataset_name == \"ptb\":\n        testdata = load_dataset(\"ptb_text_only\", \"penn_treebank\", split=\"test\")\n        testdata = \"\\n\\n\".join(testdata[\"sentence\"])\n    elif dataset_name == \"c4\":\n        testdata = load_dataset(\n            \"allenai/c4\",\n            \"allenai--c4\",\n            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n            split=\"validation\",\n        )\n        testdata = \" \".join(testdata[:1100][\"text\"])\n\n    else:\n        raise ValueError(\"invalid dataset name (wikitext, ptb, c4 are allowed)\")\n    return testdata", "\n\ndef main(\n    datasets: str = \"wikitext,ptb,c4\",\n    *,\n    # compilation fails as it does not support torch.complex64 for RoPE\n    # compile: bool = False,\n    accelerator: str = \"auto\",\n    lora_path: Path = Path(\"out/lora/alpaca/lit-llama-lora-finetuned.pth\"),\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    dtype: str = \"float32\",\n    quantize: Optional[str] = None,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer\n       finetuned with LoRA.\n\n    Args:\n        datasets: The datasets to use as a comma separated string\n        # compile: Whether to compile the model.\n        accelerator: The hardware to run on. Possible choices are:\n            ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\n        lora_path: Path to the checkpoint with trained LoRA weights, which are the output of\n            `finetune_lora.py`.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        dtype: The tensor dtype for choosing the floating-point precision \n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    assert lora_path.is_file()\n    assert checkpoint_path.is_file()\n    assert tokenizer_path.is_file()\n\n    if quantize is not None:\n        raise NotImplementedError(\"Quantization in LoRA is not supported yet\")\n\n    fabric = L.Fabric(accelerator=accelerator, devices=1)\n\n    dt = getattr(torch, dtype, None)\n    if not isinstance(dt, torch.dtype):\n        raise ValueError(f\"{dtype} is not a valid dtype.\")\n    dtype = dt\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n\n    with lazy_load(checkpoint_path) as pretrained_checkpoint, lazy_load(lora_path) as lora_checkpoint:\n        name = llama_model_lookup(pretrained_checkpoint)\n\n        with EmptyInitOnDevice(\n                device=fabric.device, dtype=dtype, quantization_mode=quantize\n        ), lora(r=lora_r, alpha=lora_alpha, dropout=lora_dropout, enabled=True):\n            model = LLaMA.from_name(name)\n\n            # 1. Load the pretrained weights\n            model.load_state_dict(pretrained_checkpoint, strict=False)\n            # 2. Load the fine-tuned lora weights\n            model.load_state_dict(lora_checkpoint, strict=False)\n\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n\n    # if compile:\n    #     model = torch.compile(model)\n\n    total_toks = 0\n    model = fabric.setup_module(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n\n    for dsname in datasets.split(\",\"):\n        test_string = load_eval_data(dsname)\n\n        if instruction_tuning:\n            sample = {\"instruction\": test_string, \"input\": input}\n            test_string = generate_prompt(sample)\n        \n        encoded_text = tokenizer.encode(\n            test_string, bos=True, eos=False, device=fabric.device\n        )\n        encoded_text = encoded_text[\n            None, : 256 * model.config.block_size\n        ]  # add batch dimension, trim like gptq implementation\n        t0 = time.perf_counter()\n\n        nlls = 0\n        toks = 0\n        with torch.inference_mode():\n            block_size = 2048  # this is for compat with gptq, and indeed we get much worse beyond this (https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L30)\n            for i in tqdm.tqdm(range(0, encoded_text.shape[1], block_size)):\n                inp = encoded_text[:, i : i + block_size]\n                logits = model(inp)[0]\n                nll = torch.nn.functional.cross_entropy(\n                    logits[:-1], inp[0, 1:].to(dtype=torch.long), reduction=\"sum\"\n                )\n                toks += inp.size(1) - 1\n                nlls += nll.item()\n\n        print(encoded_text.shape, logits.shape)\n        ppl = math.exp(nlls / toks)\n        print(f\"Perplexity on {dsname}: {ppl:.2f}\")\n        total_toks += toks\n\n    t = time.perf_counter() - t0\n    print(\n        f\"\\n\\nTime for inference: {t:.02f} sec total, {total_toks / t:.02f} tokens/sec\",\n        file=sys.stderr,\n    )\n    print(\n        f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\",\n        file=sys.stderr,\n    )", "\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    CLI(main)\n"]}
