{"filename": "setup.py", "chunked_list": ["from pathlib import Path\n\nfrom setuptools import find_packages, setup\n\nif __name__ == \"__main__\":\n    with Path(Path(__file__).parent, \"README.md\").open(encoding=\"utf-8\") as file:\n        long_description = file.read()\n\n    # TODO: This is a hack to get around the fact that we can't read the requirements.txt file, we should fix this.\n    # def _read_reqs(relpath):\n    #     fullpath = os.path.join(Path(__file__).parent, relpath)\n    #     with open(fullpath) as f:\n    #         return [\n    #             s.strip()\n    #             for s in f.readlines()\n    #             if (s.strip() and not s.startswith(\"#\"))\n    #         ]\n\n    REQUIREMENTS = [\n        \"einops\",\n        \"einops-exts\",\n        \"transformers\",\n        \"torch\",\n        \"torchvision\",\n        \"pillow\",\n        \"more-itertools\",\n        \"datasets\",\n        \"braceexpand\",\n        \"webdataset\",\n        \"wandb\",\n        \"nltk\",\n        \"scipy\",\n        \"inflection\",\n        \"sentencepiece\",\n        \"open_clip_torch\",\n    ]\n\n    setup(\n        name=\"mmgpt\",\n        packages=find_packages(),\n        include_package_data=True,\n        version=\"0.0.1\",\n        license=\"Apache 2.0\",\n        description=\"An open-source framework for multi-modality instruction fine-tuning\",\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        data_files=[(\".\", [\"README.md\"])],\n        keywords=[\"machine learning\"],\n        install_requires=REQUIREMENTS,\n    )", ""]}
{"filename": "app.py", "chunked_list": ["import os\n\nimport gradio as gr\nimport torch\nfrom PIL import Image\n\nfrom mmgpt.models.builder import create_model_and_transforms\n\nTEMPLATE = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nresponse_split = \"### Response:\"", "TEMPLATE = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nresponse_split = \"### Response:\"\n\n\nclass Inferencer:\n\n    def __init__(self, finetune_path, llama_path, open_flamingo_path):\n        ckpt = torch.load(finetune_path, map_location=\"cpu\")\n        if \"model_state_dict\" in ckpt:\n            state_dict = ckpt[\"model_state_dict\"]\n            # remove the \"module.\" prefix\n            state_dict = {\n                k[7:]: v\n                for k, v in state_dict.items() if k.startswith(\"module.\")\n            }\n        else:\n            state_dict = ckpt\n        tuning_config = ckpt.get(\"tuning_config\")\n        if tuning_config is None:\n            print(\"tuning_config not found in checkpoint\")\n        else:\n            print(\"tuning_config found in checkpoint: \", tuning_config)\n        model, image_processor, tokenizer = create_model_and_transforms(\n            model_name=\"open_flamingo\",\n            clip_vision_encoder_path=\"ViT-L-14\",\n            clip_vision_encoder_pretrained=\"openai\",\n            lang_encoder_path=llama_path,\n            tokenizer_path=llama_path,\n            pretrained_model_path=open_flamingo_path,\n            tuning_config=tuning_config,\n        )\n        model.load_state_dict(state_dict, strict=False)\n        model.half()\n        model = model.to(\"cuda\")\n        model.eval()\n        tokenizer.padding_side = \"left\"\n        tokenizer.add_eos_token = False\n        self.model = model\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n\n    def __call__(self, prompt, imgpaths, max_new_token, num_beams, temperature,\n                 top_k, top_p, do_sample):\n        if len(imgpaths) > 1:\n            raise gr.Error(\n                \"Current only support one image, please clear gallery and upload one image\"\n            )\n        lang_x = self.tokenizer([prompt], return_tensors=\"pt\")\n        if len(imgpaths) == 0 or imgpaths is None:\n            for layer in self.model.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(True)\n            output_ids = self.model.lang_encoder.generate(\n                input_ids=lang_x[\"input_ids\"].cuda(),\n                attention_mask=lang_x[\"attention_mask\"].cuda(),\n                max_new_tokens=max_new_token,\n                num_beams=num_beams,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                do_sample=do_sample,\n            )[0]\n            for layer in self.model.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(False)\n        else:\n            images = (Image.open(fp) for fp in imgpaths)\n            vision_x = [self.image_processor(im).unsqueeze(0) for im in images]\n            vision_x = torch.cat(vision_x, dim=0)\n            vision_x = vision_x.unsqueeze(1).unsqueeze(0).half()\n\n            output_ids = self.model.generate(\n                vision_x=vision_x.cuda(),\n                lang_x=lang_x[\"input_ids\"].cuda(),\n                attention_mask=lang_x[\"attention_mask\"].cuda(),\n                max_new_tokens=max_new_token,\n                num_beams=num_beams,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                do_sample=do_sample,\n            )[0]\n        generated_text = self.tokenizer.decode(\n            output_ids, skip_special_tokens=True)\n        # print(generated_text)\n        result = generated_text.split(response_split)[-1].strip()\n        return result", "\n\nclass PromptGenerator:\n\n    def __init__(\n        self,\n        prompt_template=TEMPLATE,\n        ai_prefix=\"Response\",\n        user_prefix=\"Instruction\",\n        sep: str = \"\\n\\n### \",\n        buffer_size=0,\n    ):\n        self.all_history = list()\n        self.ai_prefix = ai_prefix\n        self.user_prefix = user_prefix\n        self.buffer_size = buffer_size\n        self.prompt_template = prompt_template\n        self.sep = sep\n\n    def add_message(self, role, message):\n        self.all_history.append([role, message])\n\n    def get_images(self):\n        img_list = list()\n        if self.buffer_size > 0:\n            all_history = self.all_history[-2 * (self.buffer_size + 1):]\n        elif self.buffer_size == 0:\n            all_history = self.all_history[-2:]\n        else:\n            all_history = self.all_history[:]\n        for his in all_history:\n            if type(his[-1]) == tuple:\n                img_list.append(his[-1][-1])\n        return img_list\n\n    def get_prompt(self):\n        format_dict = dict()\n        if \"{user_prefix}\" in self.prompt_template:\n            format_dict[\"user_prefix\"] = self.user_prefix\n        if \"{ai_prefix}\" in self.prompt_template:\n            format_dict[\"ai_prefix\"] = self.ai_prefix\n        prompt_template = self.prompt_template.format(**format_dict)\n        ret = prompt_template\n        if self.buffer_size > 0:\n            all_history = self.all_history[-2 * (self.buffer_size + 1):]\n        elif self.buffer_size == 0:\n            all_history = self.all_history[-2:]\n        else:\n            all_history = self.all_history[:]\n        context = []\n        have_image = False\n        for role, message in all_history[::-1]:\n            if message:\n                if type(message) is tuple and message[\n                        1] is not None and not have_image:\n                    message, _ = message\n                    context.append(self.sep + \"Image:\\n<image>\" + self.sep +\n                                   role + \":\\n\" + message)\n                else:\n                    context.append(self.sep + role + \":\\n\" + message)\n            else:\n                context.append(self.sep + role + \":\\n\")\n\n        ret += \"\".join(context[::-1])\n        return ret", "\n\ndef to_gradio_chatbot(prompt_generator):\n    ret = []\n    for i, (role, msg) in enumerate(prompt_generator.all_history):\n        if i % 2 == 0:\n            if type(msg) is tuple:\n                import base64\n                from io import BytesIO\n\n                msg, image = msg\n                if type(image) is str:\n                    from PIL import Image\n\n                    image = Image.open(image)\n                max_hw, min_hw = max(image.size), min(image.size)\n                aspect_ratio = max_hw / min_hw\n                max_len, min_len = 800, 400\n                shortest_edge = int(\n                    min(max_len / aspect_ratio, min_len, min_hw))\n                longest_edge = int(shortest_edge * aspect_ratio)\n                H, W = image.size\n                if H > W:\n                    H, W = longest_edge, shortest_edge\n                else:\n                    H, W = shortest_edge, longest_edge\n                image = image.resize((H, W))\n                # image = image.resize((224, 224))\n                buffered = BytesIO()\n                image.save(buffered, format=\"JPEG\")\n                img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n                img_str = f'<img src=\"data:image/png;base64,{img_b64_str}\" alt=\"user upload image\" />'\n                msg = msg + img_str\n            ret.append([msg, None])\n        else:\n            ret[-1][-1] = msg\n    return ret", "\n\ndef bot(\n    text,\n    image,\n    state,\n    prompt,\n    ai_prefix,\n    user_prefix,\n    seperator,\n    history_buffer,\n    max_new_token,\n    num_beams,\n    temperature,\n    top_k,\n    top_p,\n    do_sample,\n):\n    state.prompt_template = prompt\n    state.ai_prefix = ai_prefix\n    state.user_prefix = user_prefix\n    state.sep = seperator\n    state.buffer_size = history_buffer\n    if image:\n        state.add_message(user_prefix, (text, image))\n    else:\n        state.add_message(user_prefix, text)\n    state.add_message(ai_prefix, None)\n    inputs = state.get_prompt()\n    image_paths = state.get_images()[-1:]\n\n    inference_results = inferencer(inputs, image_paths, max_new_token,\n                                   num_beams, temperature, top_k, top_p,\n                                   do_sample)\n    state.all_history[-1][-1] = inference_results\n    memory_allocated = str(round(torch.cuda.memory_allocated() / 1024**3,\n                                 2)) + 'GB'\n    return state, to_gradio_chatbot(state), \"\", None, inputs, memory_allocated", "\n\ndef clear(state):\n    state.all_history = []\n    return state, to_gradio_chatbot(state), \"\", None, \"\"\n\n\ntitle_markdown = (\"\"\"\n    # \ud83e\udd16 Multi-modal GPT\n    [[Project]](https://github.com/open-mmlab/Multimodal-GPT.git)\"\"\")", "    # \ud83e\udd16 Multi-modal GPT\n    [[Project]](https://github.com/open-mmlab/Multimodal-GPT.git)\"\"\")\n\n\ndef build_conversation_demo():\n    with gr.Blocks(title=\"Multi-modal GPT\") as demo:\n        gr.Markdown(title_markdown)\n\n        state = gr.State(PromptGenerator())\n        with gr.Row():\n            with gr.Column(scale=3):\n                memory_allocated = gr.Textbox(\n                    value=init_memory, label=\"Memory\")\n                imagebox = gr.Image(type=\"filepath\")\n                # TODO config parameters\n                with gr.Accordion(\n                        \"Parameters\",\n                        open=True,\n                ):\n                    max_new_token_bar = gr.Slider(\n                        0, 1024, 512, label=\"max_new_token\", step=1)\n                    num_beams_bar = gr.Slider(\n                        0.0, 10, 3, label=\"num_beams\", step=1)\n                    temperature_bar = gr.Slider(\n                        0.0, 1.0, 1.0, label=\"temperature\", step=0.01)\n                    topk_bar = gr.Slider(0, 100, 20, label=\"top_k\", step=1)\n                    topp_bar = gr.Slider(0, 1.0, 1.0, label=\"top_p\", step=0.01)\n                    do_sample = gr.Checkbox(True, label=\"do_sample\")\n                with gr.Accordion(\n                        \"Prompt\",\n                        open=False,\n                ):\n                    with gr.Row():\n                        ai_prefix = gr.Text(\"Response\", label=\"AI Prefix\")\n                        user_prefix = gr.Text(\n                            \"Instruction\", label=\"User Prefix\")\n                        seperator = gr.Text(\"\\n\\n### \", label=\"Seperator\")\n                    history_buffer = gr.Slider(\n                        -1, 10, -1, label=\"History buffer\", step=1)\n                    prompt = gr.Text(TEMPLATE, label=\"Prompt\")\n                    model_inputs = gr.Textbox(label=\"Actual inputs for Model\")\n\n            with gr.Column(scale=6):\n                with gr.Row():\n                    with gr.Column():\n                        chatbot = gr.Chatbot(elem_id=\"chatbot\").style(\n                            height=750)\n                with gr.Row():\n                    with gr.Column(scale=8):\n                        textbox = gr.Textbox(\n                            show_label=False,\n                            placeholder=\"Enter text and press ENTER\",\n                        ).style(container=False)\n                        submit_btn = gr.Button(value=\"Submit\")\n                        clear_btn = gr.Button(value=\"\ud83d\uddd1\ufe0f  Clear history\")\n        cur_dir = os.path.dirname(os.path.abspath(__file__))\n        gr.Examples(\n            examples=[\n                [\n                    f\"{cur_dir}/docs/images/demo_image.jpg\",\n                    \"What is in this image?\"\n                ],\n            ],\n            inputs=[imagebox, textbox],\n        )\n        textbox.submit(\n            bot,\n            [\n                textbox,\n                imagebox,\n                state,\n                prompt,\n                ai_prefix,\n                user_prefix,\n                seperator,\n                history_buffer,\n                max_new_token_bar,\n                num_beams_bar,\n                temperature_bar,\n                topk_bar,\n                topp_bar,\n                do_sample,\n            ],\n            [\n                state, chatbot, textbox, imagebox, model_inputs,\n                memory_allocated\n            ],\n        )\n        submit_btn.click(\n            bot,\n            [\n                textbox,\n                imagebox,\n                state,\n                prompt,\n                ai_prefix,\n                user_prefix,\n                seperator,\n                history_buffer,\n                max_new_token_bar,\n                num_beams_bar,\n                temperature_bar,\n                topk_bar,\n                topp_bar,\n                do_sample,\n            ],\n            [\n                state, chatbot, textbox, imagebox, model_inputs,\n                memory_allocated\n            ],\n        )\n        clear_btn.click(clear, [state],\n                        [state, chatbot, textbox, imagebox, model_inputs])\n    return demo", "\n\nif __name__ == \"__main__\":\n    llama_path = \"checkpoints/llama-7b_hf\"\n    open_flamingo_path = \"checkpoints/OpenFlamingo-9B/checkpoint.pt\"\n    finetune_path = \"checkpoints/mmgpt-lora-v0-release.pt\"\n\n    inferencer = Inferencer(\n        llama_path=llama_path,\n        open_flamingo_path=open_flamingo_path,\n        finetune_path=finetune_path)\n    init_memory = str(round(torch.cuda.memory_allocated() / 1024**3, 2)) + 'GB'\n    demo = build_conversation_demo()\n    demo.queue(concurrency_count=3)\n    IP = \"0.0.0.0\"\n    PORT = 8997\n    demo.launch(server_name=IP, server_port=PORT, share=True)", ""]}
{"filename": "mmgpt/__init__.py", "chunked_list": ["from .models.builder import create_model_and_transforms\nfrom .models.open_flamingo import Flamingo\n"]}
{"filename": "mmgpt/models/__init__.py", "chunked_list": [""]}
{"filename": "mmgpt/models/builder.py", "chunked_list": ["from .open_flamingo import create_model_and_transforms as create_open_flamingo_model_and_transforms\nimport torch.nn as nn\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\ndef create_model_and_transforms(\n    model_name: str,\n    clip_vision_encoder_path: str,\n    clip_vision_encoder_pretrained: str,\n    lang_encoder_path: str,\n    tokenizer_path: str,\n    tuning_config,\n    pretrained_model_path,\n    **kwargs,\n):\n    if model_name == \"open_flamingo\":\n        return create_open_flamingo_model_and_transforms(\n            clip_vision_encoder_path=clip_vision_encoder_path,\n            clip_vision_encoder_pretrained=clip_vision_encoder_pretrained,\n            lang_encoder_path=lang_encoder_path,\n            tokenizer_path=tokenizer_path,\n            tuning_config=tuning_config,\n            pretrained_model_path=pretrained_model_path,\n            **kwargs,\n        )\n    # TODO: support BLIP2\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")", "\n# only for debugging\ndef create_toy_model_and_transforms(\n    model_name: str,\n    clip_vision_encoder_path: str,\n    clip_vision_encoder_pretrained: str,\n    lang_encoder_path: str,\n    tokenizer_path: str,\n    tuning_config,\n    pretrained_model_path,\n    **kwargs,\n):\n    print(\"init toy vision encoder\")\n    import torchvision\n\n    image_processor = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.Resize((224, 224)),\n            torchvision.transforms.ToTensor(),\n        ]\n    )\n    print(\"init tokenizer\")\n    text_tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n    # add Flamingo special tokens to the tokenizer\n    text_tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]})\n    if text_tokenizer.pad_token is None:\n        # Issue: GPT models don't have a pad token, which we use to\n        # modify labels for the loss.\n        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n\n    class ToyModel(nn.Module):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.input_embeddings = nn.Embedding(38000, 512)\n            self.layer = nn.Linear(512, 512)\n            self.config = {\"hidden_size\": 512}\n\n        def forward(self, lang_x, **kwargs):\n            x = self.input_embeddings(lang_x)\n            x = self.layer(x)\n            loss = x.sum()\n\n            return (loss,)\n\n    model = ToyModel()\n\n    return model, image_processor, text_tokenizer", ""]}
{"filename": "mmgpt/models/blip2/__init__.py", "chunked_list": [""]}
{"filename": "mmgpt/models/open_flamingo/__init__.py", "chunked_list": ["from .builder import create_model_and_transforms\nfrom .flamingo import Flamingo\nfrom .flamingo_lm import FlamingoLMMixin\n"]}
{"filename": "mmgpt/models/open_flamingo/utils.py", "chunked_list": ["def extend_instance(obj, mixin):\n    \"\"\"Apply mixins to a class instance after creation\"\"\"\n    base_cls = obj.__class__\n    base_cls_name = obj.__class__.__name__\n    obj.__class__ = type(\n        base_cls_name, (mixin, base_cls), {}\n    )  # mixin needs to go first for our forward() logic to work\n\n\ndef getattr_recursive(obj, att):\n    \"\"\"\n    Return nested attribute of obj\n    Example: getattr_recursive(obj, 'a.b.c') is equivalent to obj.a.b.c\n    \"\"\"\n    if att == \"\":\n        return obj\n    i = att.find(\".\")\n    if i < 0:\n        return getattr(obj, att)\n    else:\n        return getattr_recursive(getattr(obj, att[:i]), att[i + 1 :])", "\ndef getattr_recursive(obj, att):\n    \"\"\"\n    Return nested attribute of obj\n    Example: getattr_recursive(obj, 'a.b.c') is equivalent to obj.a.b.c\n    \"\"\"\n    if att == \"\":\n        return obj\n    i = att.find(\".\")\n    if i < 0:\n        return getattr(obj, att)\n    else:\n        return getattr_recursive(getattr(obj, att[:i]), att[i + 1 :])", "\n\ndef setattr_recursive(obj, att, val):\n    \"\"\"\n    Set nested attribute of obj\n    Example: setattr_recursive(obj, 'a.b.c', val) is equivalent to obj.a.b.c = val\n    \"\"\"\n    if \".\" in att:\n        obj = getattr_recursive(obj, \".\".join(att.split(\".\")[:-1]))\n    setattr(obj, att.split(\".\")[-1], val)", ""]}
{"filename": "mmgpt/models/open_flamingo/flamingo.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\nimport torch\nfrom einops import rearrange\nfrom torch import nn\n\nfrom .helpers import PerceiverResampler\n\n\nclass Flamingo(nn.Module):\n    def __init__(\n        self,\n        vision_encoder: nn.Module,\n        lang_encoder: nn.Module,\n        eoc_token_id: int,\n        media_token_id: int,\n        vis_dim: int,\n        cross_attn_every_n_layers: int = 1,\n        use_media_placement_augmentation: bool = False,\n    ):\n        \"\"\"\n        Args:\n            vision_encoder (nn.Module): HF CLIPModel\n            lang_encoder (nn.Module): HF causal language model\n            eoc_token_id (int): Token id for <|endofchunk|>\n            media_token_id (int): Token id for <image>\n            vis_dim (int): Dimension of the visual features.\n                Visual features are projected to match this shape along the last dimension.\n            cross_attn_every_n_layers (int, optional): How often to apply cross attention after transformer layer. Defaults to 1.\n            use_media_placement_augmentation (bool, optional): Whether to randomly assign images to the preceding or following text in training. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.eoc_token_id = eoc_token_id\n        self.media_token_id = media_token_id\n        self.use_media_placement_augmentation = use_media_placement_augmentation\n        self.vis_dim = vis_dim\n        self.vision_encoder = vision_encoder\n        self.perceiver = PerceiverResampler(dim=self.vis_dim)\n        self.lang_encoder = lang_encoder\n        self.lang_encoder.init_flamingo(\n            media_token_id=media_token_id,\n            vis_hidden_size=self.vis_dim,\n            cross_attn_every_n_layers=cross_attn_every_n_layers,\n            use_media_placement_augmentation=self.use_media_placement_augmentation,\n        )\n\n    def forward(\n        self,\n        vision_x: torch.Tensor,\n        lang_x: torch.Tensor,\n        attention_mask: torch.Tensor = None,\n        labels: torch.Tensor = None,\n        use_cached_vision_x: bool = False,\n        clear_conditioned_layers: bool = True,\n        past_key_values=None,\n        use_cache: bool = False,\n    ):\n        \"\"\"\n        Forward pass of Flamingo.\n\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W) with F=1\n            lang_x (torch.Tensor): Language input ids\n                shape (B, T_txt)\n            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n            labels (torch.Tensor, optional): Labels. Defaults to None.\n            clear_conditioned_layers: if True, clear the conditioned layers\n                once the foward pass is completed. Set this to false if the\n                same set of images will be reused in another subsequent\n                forward pass.\n            past_key_values: pre-computed values to pass to language model.\n                See past_key_values documentation in Hugging Face\n                CausalLM models.\n            use_cache: whether to use cached key values. See use_cache\n                documentation in Hugging Face CausalLM models.\n        \"\"\"\n        if vision_x is None and use_cached_vision_x is False:\n            for layer in self.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(True)\n            output = self.lang_encoder(\n                input_ids=lang_x,\n                attention_mask=attention_mask,\n                labels=labels,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n            )\n            for layer in self.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(False)\n            return output\n        assert (\n            vision_x is not None\n        ) or use_cached_vision_x, \"Must provide either vision_x or use_cached_vision_x to True.\"\n\n        if use_cached_vision_x:\n            # Case: use cached; vision_x should be cached and other\n            # vision-related inputs should not be provided.\n            assert vision_x is None, \"Expect vision_x to be None when use_cached_vision_x is True.\"\n            assert self.lang_encoder.is_conditioned()\n\n        else:\n            # Case: do not use caching (i.e. this is a standard forward pass);\n            self._encode_vision_x(vision_x=vision_x)\n\n        output = self.lang_encoder(\n            input_ids=lang_x,\n            attention_mask=attention_mask,\n            labels=labels,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n        )\n\n        if clear_conditioned_layers:\n            self.lang_encoder.clear_conditioned_layers()\n\n        return output\n\n    def generate(\n        self,\n        vision_x: torch.Tensor,\n        lang_x: torch.Tensor,\n        attention_mask: torch.Tensor = None,\n        num_beams=1,\n        max_new_tokens=None,\n        temperature=1.0,\n        top_k=0,\n        top_p=1.0,\n        no_repeat_ngram_size=0,\n        prefix_allowed_tokens_fn=None,\n        length_penalty=1.0,\n        num_return_sequences=1,\n        do_sample=False,\n        early_stopping=False,\n    ):\n        \"\"\"\n        Generate text conditioned on vision and language inputs.\n\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W)\n                images in the same chunk are collated along T_img, and frames are collated along F\n                currently only F=1 is supported (single-frame videos)\n            lang_x (torch.Tensor): Language input\n                shape (B, T_txt)\n            max_length (int, optional): Maximum length of the output. Defaults to None.\n            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n            num_beams (int, optional): Number of beams. Defaults to 1.\n            max_new_tokens (int, optional): Maximum new tokens. Defaults to None.\n            temperature (float, optional): Temperature. Defaults to 1.0.\n            top_k (int, optional): Top k. Defaults to 0.\n            top_p (float, optional): Top p. Defaults to 1.0.\n            no_repeat_ngram_size (int, optional): No repeat ngram size. Defaults to 0.\n            length_penalty (float, optional): Length penalty. Defaults to 1.0.\n            num_return_sequences (int, optional): Number of return sequences. Defaults to 1.\n            do_sample (bool, optional): Do sample. Defaults to False.\n            early_stopping (bool, optional): Early stopping. Defaults to False.\n        Returns:\n            torch.Tensor: lang_x with generated tokens appended to it\n        \"\"\"\n        if num_beams > 1:\n            vision_x = vision_x.repeat_interleave(num_beams, dim=0)\n\n        self._encode_vision_x(vision_x=vision_x)\n\n        output = self.lang_encoder.generate(\n            lang_x,\n            attention_mask=attention_mask,\n            # eos_token_id=self.eoc_token_id,\n            num_beams=num_beams,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            no_repeat_ngram_size=no_repeat_ngram_size,\n            length_penalty=length_penalty,\n            num_return_sequences=num_return_sequences,\n            do_sample=do_sample,\n            early_stopping=early_stopping,\n        )\n\n        self.lang_encoder.clear_conditioned_layers()\n        return output\n\n    def _encode_vision_x(self, vision_x: torch.Tensor):\n        \"\"\"\n        Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W)\n                Images in the same chunk are collated along T_img, and frames are collated along F\n                Currently only F=1 is supported (single-frame videos)\n\n        rearrange code based on https://github.com/dhansmair/flamingo-mini\n        \"\"\"\n\n        assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n        b, T, F = vision_x.shape[:3]\n        assert F == 1, \"Only single frame supported\"\n\n        vision_x = rearrange(vision_x, \"b T F c h w -> (b T F) c h w\")\n        with torch.no_grad():\n            vision_x = self.vision_encoder.visual(vision_x)[1]\n        vision_x = rearrange(vision_x, \"(b T F) v d -> b T F v d\", b=b, T=T, F=F)\n\n        vision_x = self.perceiver(vision_x)  # reshapes to (b, T, n, d)\n\n        for layer in self.lang_encoder._get_decoder_layers():\n            layer.condition_vis_x(vision_x)", "class Flamingo(nn.Module):\n    def __init__(\n        self,\n        vision_encoder: nn.Module,\n        lang_encoder: nn.Module,\n        eoc_token_id: int,\n        media_token_id: int,\n        vis_dim: int,\n        cross_attn_every_n_layers: int = 1,\n        use_media_placement_augmentation: bool = False,\n    ):\n        \"\"\"\n        Args:\n            vision_encoder (nn.Module): HF CLIPModel\n            lang_encoder (nn.Module): HF causal language model\n            eoc_token_id (int): Token id for <|endofchunk|>\n            media_token_id (int): Token id for <image>\n            vis_dim (int): Dimension of the visual features.\n                Visual features are projected to match this shape along the last dimension.\n            cross_attn_every_n_layers (int, optional): How often to apply cross attention after transformer layer. Defaults to 1.\n            use_media_placement_augmentation (bool, optional): Whether to randomly assign images to the preceding or following text in training. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.eoc_token_id = eoc_token_id\n        self.media_token_id = media_token_id\n        self.use_media_placement_augmentation = use_media_placement_augmentation\n        self.vis_dim = vis_dim\n        self.vision_encoder = vision_encoder\n        self.perceiver = PerceiverResampler(dim=self.vis_dim)\n        self.lang_encoder = lang_encoder\n        self.lang_encoder.init_flamingo(\n            media_token_id=media_token_id,\n            vis_hidden_size=self.vis_dim,\n            cross_attn_every_n_layers=cross_attn_every_n_layers,\n            use_media_placement_augmentation=self.use_media_placement_augmentation,\n        )\n\n    def forward(\n        self,\n        vision_x: torch.Tensor,\n        lang_x: torch.Tensor,\n        attention_mask: torch.Tensor = None,\n        labels: torch.Tensor = None,\n        use_cached_vision_x: bool = False,\n        clear_conditioned_layers: bool = True,\n        past_key_values=None,\n        use_cache: bool = False,\n    ):\n        \"\"\"\n        Forward pass of Flamingo.\n\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W) with F=1\n            lang_x (torch.Tensor): Language input ids\n                shape (B, T_txt)\n            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n            labels (torch.Tensor, optional): Labels. Defaults to None.\n            clear_conditioned_layers: if True, clear the conditioned layers\n                once the foward pass is completed. Set this to false if the\n                same set of images will be reused in another subsequent\n                forward pass.\n            past_key_values: pre-computed values to pass to language model.\n                See past_key_values documentation in Hugging Face\n                CausalLM models.\n            use_cache: whether to use cached key values. See use_cache\n                documentation in Hugging Face CausalLM models.\n        \"\"\"\n        if vision_x is None and use_cached_vision_x is False:\n            for layer in self.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(True)\n            output = self.lang_encoder(\n                input_ids=lang_x,\n                attention_mask=attention_mask,\n                labels=labels,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n            )\n            for layer in self.lang_encoder._get_decoder_layers():\n                layer.condition_only_lang_x(False)\n            return output\n        assert (\n            vision_x is not None\n        ) or use_cached_vision_x, \"Must provide either vision_x or use_cached_vision_x to True.\"\n\n        if use_cached_vision_x:\n            # Case: use cached; vision_x should be cached and other\n            # vision-related inputs should not be provided.\n            assert vision_x is None, \"Expect vision_x to be None when use_cached_vision_x is True.\"\n            assert self.lang_encoder.is_conditioned()\n\n        else:\n            # Case: do not use caching (i.e. this is a standard forward pass);\n            self._encode_vision_x(vision_x=vision_x)\n\n        output = self.lang_encoder(\n            input_ids=lang_x,\n            attention_mask=attention_mask,\n            labels=labels,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n        )\n\n        if clear_conditioned_layers:\n            self.lang_encoder.clear_conditioned_layers()\n\n        return output\n\n    def generate(\n        self,\n        vision_x: torch.Tensor,\n        lang_x: torch.Tensor,\n        attention_mask: torch.Tensor = None,\n        num_beams=1,\n        max_new_tokens=None,\n        temperature=1.0,\n        top_k=0,\n        top_p=1.0,\n        no_repeat_ngram_size=0,\n        prefix_allowed_tokens_fn=None,\n        length_penalty=1.0,\n        num_return_sequences=1,\n        do_sample=False,\n        early_stopping=False,\n    ):\n        \"\"\"\n        Generate text conditioned on vision and language inputs.\n\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W)\n                images in the same chunk are collated along T_img, and frames are collated along F\n                currently only F=1 is supported (single-frame videos)\n            lang_x (torch.Tensor): Language input\n                shape (B, T_txt)\n            max_length (int, optional): Maximum length of the output. Defaults to None.\n            attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n            num_beams (int, optional): Number of beams. Defaults to 1.\n            max_new_tokens (int, optional): Maximum new tokens. Defaults to None.\n            temperature (float, optional): Temperature. Defaults to 1.0.\n            top_k (int, optional): Top k. Defaults to 0.\n            top_p (float, optional): Top p. Defaults to 1.0.\n            no_repeat_ngram_size (int, optional): No repeat ngram size. Defaults to 0.\n            length_penalty (float, optional): Length penalty. Defaults to 1.0.\n            num_return_sequences (int, optional): Number of return sequences. Defaults to 1.\n            do_sample (bool, optional): Do sample. Defaults to False.\n            early_stopping (bool, optional): Early stopping. Defaults to False.\n        Returns:\n            torch.Tensor: lang_x with generated tokens appended to it\n        \"\"\"\n        if num_beams > 1:\n            vision_x = vision_x.repeat_interleave(num_beams, dim=0)\n\n        self._encode_vision_x(vision_x=vision_x)\n\n        output = self.lang_encoder.generate(\n            lang_x,\n            attention_mask=attention_mask,\n            # eos_token_id=self.eoc_token_id,\n            num_beams=num_beams,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            no_repeat_ngram_size=no_repeat_ngram_size,\n            length_penalty=length_penalty,\n            num_return_sequences=num_return_sequences,\n            do_sample=do_sample,\n            early_stopping=early_stopping,\n        )\n\n        self.lang_encoder.clear_conditioned_layers()\n        return output\n\n    def _encode_vision_x(self, vision_x: torch.Tensor):\n        \"\"\"\n        Compute media tokens from vision input by passing it through vision encoder and conditioning language model.\n        Args:\n            vision_x (torch.Tensor): Vision input\n                shape (B, T_img, F, C, H, W)\n                Images in the same chunk are collated along T_img, and frames are collated along F\n                Currently only F=1 is supported (single-frame videos)\n\n        rearrange code based on https://github.com/dhansmair/flamingo-mini\n        \"\"\"\n\n        assert vision_x.ndim == 6, \"vision_x should be of shape (b, T_img, F, C, H, W)\"\n        b, T, F = vision_x.shape[:3]\n        assert F == 1, \"Only single frame supported\"\n\n        vision_x = rearrange(vision_x, \"b T F c h w -> (b T F) c h w\")\n        with torch.no_grad():\n            vision_x = self.vision_encoder.visual(vision_x)[1]\n        vision_x = rearrange(vision_x, \"(b T F) v d -> b T F v d\", b=b, T=T, F=F)\n\n        vision_x = self.perceiver(vision_x)  # reshapes to (b, T, n, d)\n\n        for layer in self.lang_encoder._get_decoder_layers():\n            layer.condition_vis_x(vision_x)", ""]}
{"filename": "mmgpt/models/open_flamingo/flamingo_lm.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\nimport random\n\nimport torch.nn as nn\n\nfrom .helpers import GatedCrossAttentionBlock\nfrom .utils import getattr_recursive, setattr_recursive\n\n\nclass FlamingoLayer(nn.Module):\n    def __init__(self, gated_cross_attn_layer, decoder_layer):\n        super().__init__()\n        self.gated_cross_attn_layer = gated_cross_attn_layer\n        self.decoder_layer = decoder_layer\n        self.vis_x = None\n        self.media_locations = None\n        self.only_lang_x = False\n\n    def is_conditioned(self) -> bool:\n        \"\"\"Check whether the layer is conditioned.\"\"\"\n        return self.vis_x is not None\n\n    # Used this great idea from this implementation of Flamingo (https://github.com/dhansmair/flamingo-mini/)\n    def condition_vis_x(self, vis_x):\n        self.vis_x = vis_x\n\n    def condition_only_lang_x(self, only_lang_x=False):\n        self.only_lang_x = only_lang_x\n\n    def condition_media_locations(self, media_locations):\n        self.media_locations = media_locations\n\n    def condition_attend_previous(self, attend_previous):\n        self.attend_previous = attend_previous\n\n    def forward(\n        self,\n        lang_x,\n        attention_mask=None,\n        **decoder_layer_kwargs,\n    ):\n        if self.gated_cross_attn_layer is None or self.only_lang_x:\n            return self.decoder_layer(lang_x, attention_mask=attention_mask, **decoder_layer_kwargs)\n\n        if self.vis_x is None:\n            raise ValueError(\"vis_x must be conditioned before forward pass\")\n\n        if self.media_locations is None:\n            raise ValueError(\"media_locations must be conditioned before forward pass\")\n\n        lang_x = self.gated_cross_attn_layer(\n            lang_x,\n            self.vis_x,\n            media_locations=self.media_locations,\n            attend_previous=self.attend_previous,\n        )\n        lang_x = self.decoder_layer(lang_x, attention_mask=attention_mask, **decoder_layer_kwargs)\n        return lang_x", "\nclass FlamingoLayer(nn.Module):\n    def __init__(self, gated_cross_attn_layer, decoder_layer):\n        super().__init__()\n        self.gated_cross_attn_layer = gated_cross_attn_layer\n        self.decoder_layer = decoder_layer\n        self.vis_x = None\n        self.media_locations = None\n        self.only_lang_x = False\n\n    def is_conditioned(self) -> bool:\n        \"\"\"Check whether the layer is conditioned.\"\"\"\n        return self.vis_x is not None\n\n    # Used this great idea from this implementation of Flamingo (https://github.com/dhansmair/flamingo-mini/)\n    def condition_vis_x(self, vis_x):\n        self.vis_x = vis_x\n\n    def condition_only_lang_x(self, only_lang_x=False):\n        self.only_lang_x = only_lang_x\n\n    def condition_media_locations(self, media_locations):\n        self.media_locations = media_locations\n\n    def condition_attend_previous(self, attend_previous):\n        self.attend_previous = attend_previous\n\n    def forward(\n        self,\n        lang_x,\n        attention_mask=None,\n        **decoder_layer_kwargs,\n    ):\n        if self.gated_cross_attn_layer is None or self.only_lang_x:\n            return self.decoder_layer(lang_x, attention_mask=attention_mask, **decoder_layer_kwargs)\n\n        if self.vis_x is None:\n            raise ValueError(\"vis_x must be conditioned before forward pass\")\n\n        if self.media_locations is None:\n            raise ValueError(\"media_locations must be conditioned before forward pass\")\n\n        lang_x = self.gated_cross_attn_layer(\n            lang_x,\n            self.vis_x,\n            media_locations=self.media_locations,\n            attend_previous=self.attend_previous,\n        )\n        lang_x = self.decoder_layer(lang_x, attention_mask=attention_mask, **decoder_layer_kwargs)\n        return lang_x", "\n\nclass FlamingoLMMixin(nn.Module):\n    \"\"\"\n    Mixin to add cross-attention layers to a language model.\n    \"\"\"\n\n    def set_decoder_layers_attr_name(self, decoder_layers_attr_name):\n        self.decoder_layers_attr_name = decoder_layers_attr_name\n\n    def _get_decoder_layers(self):\n        return getattr_recursive(self, self.decoder_layers_attr_name)\n\n    def _set_decoder_layers(self, value):\n        setattr_recursive(self, self.decoder_layers_attr_name, value)\n\n    def init_flamingo(\n        self,\n        media_token_id,\n        vis_hidden_size,\n        cross_attn_every_n_layers,\n        use_media_placement_augmentation,\n    ):\n        \"\"\"\n        Initialize Flamingo by adding a new gated cross attn to the decoder. Store the media token id for computing the media locations.\n        \"\"\"\n\n        self.gated_cross_attn_layers = nn.ModuleList(\n            [\n                GatedCrossAttentionBlock(dim=self.config.hidden_size, dim_visual=vis_hidden_size)\n                if (layer_idx + 1) % cross_attn_every_n_layers == 0\n                else None\n                for layer_idx, _ in enumerate(self._get_decoder_layers())\n            ]\n        )\n        self._set_decoder_layers(\n            nn.ModuleList(\n                [\n                    FlamingoLayer(gated_cross_attn_layer, decoder_layer)\n                    for gated_cross_attn_layer, decoder_layer in zip(\n                        self.gated_cross_attn_layers, self._get_decoder_layers()\n                    )\n                ]\n            )\n        )\n        self.media_token_id = media_token_id\n        self.use_media_placement_augmentation = use_media_placement_augmentation\n        self.initialized_flamingo = True\n\n    def forward(self, *input, **kwargs):\n        \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n        if not self.initialized_flamingo:\n            raise ValueError(\"Flamingo layers are not initialized. Please call `init_flamingo` first.\")\n\n        input_ids = kwargs[\"input_ids\"] if \"input_ids\" in kwargs else input[0]\n        media_locations = input_ids == self.media_token_id\n        attend_previous = (random.random() < 0.5) if self.use_media_placement_augmentation else False\n\n        for layer in self.get_decoder().layers:\n            layer.condition_media_locations(media_locations)\n            layer.condition_attend_previous(attend_previous)\n\n        return super().forward(*input, **kwargs)  # Call the other parent's forward method\n\n    def is_conditioned(self) -> bool:\n        \"\"\"Check whether all decoder layers are already conditioned.\"\"\"\n        return all(l.is_conditioned() for l in self._get_decoder_layers())\n\n    def clear_conditioned_layers(self):\n        for layer in self._get_decoder_layers():\n            layer.condition_vis_x(None)\n            layer.condition_media_locations(None)\n            layer.condition_attend_previous(None)", ""]}
{"filename": "mmgpt/models/open_flamingo/builder.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\nimport open_clip\nimport torch\nimport torch.nn as nn\nfrom bigmodelvis import Visualization\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\nfrom .flamingo import Flamingo\nfrom .flamingo_lm import FlamingoLMMixin", "from .flamingo import Flamingo\nfrom .flamingo_lm import FlamingoLMMixin\nfrom .utils import extend_instance\n\n\ndef create_model_and_transforms(\n    clip_vision_encoder_path: str,\n    clip_vision_encoder_pretrained: str,\n    lang_encoder_path: str,\n    tokenizer_path: str,\n    decoder_layers_attr_name: str = None,\n    pretrained_model_path: str = None,\n    tuning_config=None,\n    **flamingo_kwargs,\n):\n    \"\"\"\n    Initialize a Flamingo model from a pretrained vision encoder and language encoder.\n    Appends special tokens to the tokenizer and freezes backbones.\n\n    Args:\n        clip_vision_encoder_path (str): path to pretrained clip model (e.g. \"ViT-B-32\")\n        clip_vision_encoder_pretrained (str): name of pretraining dataset for clip model (e.g. \"laion2b_s32b_b79k\")\n        lang_encoder_path (str): path to pretrained language encoder\n        tokenizer_path (str): path to pretrained tokenizer\n        decoder_layers_attr_name (str, optional): name of the decoder layers attribute. Defaults to None.\n    Returns:\n        Flamingo: Flamingo model from pretrained vision and language encoders\n        Image processor: Pipeline to preprocess input images\n        Tokenizer: A tokenizer for the language model\n    \"\"\"\n    print(\"init clip vision encoder\")\n    vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n        clip_vision_encoder_path, pretrained=clip_vision_encoder_pretrained\n    )\n    # set the vision encoder to output the visual features\n    vision_encoder.visual.output_tokens = True\n    print(\"init tokenizer\")\n    text_tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n    # add Flamingo special tokens to the tokenizer\n    text_tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]})\n    if text_tokenizer.pad_token is None:\n        # Issue: GPT models don't have a pad token, which we use to\n        # modify labels for the loss.\n        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n    text_tokenizer.bos_token_id = 1\n    text_tokenizer.eos_token_id = 2\n\n    print(\"init llama\")\n    lang_encoder = LlamaForCausalLM.from_pretrained(lang_encoder_path)\n    extend_instance(lang_encoder, FlamingoLMMixin)\n\n    if decoder_layers_attr_name is None:\n        decoder_layers_attr_name = _infer_decoder_layers_attr_name(lang_encoder)\n    lang_encoder.set_decoder_layers_attr_name(decoder_layers_attr_name)\n    lang_encoder.resize_token_embeddings(len(text_tokenizer))\n\n    model = Flamingo(\n        vision_encoder,\n        lang_encoder,\n        text_tokenizer.encode(\"<|endofchunk|>\")[-1],\n        text_tokenizer.encode(\"<image>\")[-1],\n        vis_dim=open_clip.get_model_config(clip_vision_encoder_path)[\"vision_cfg\"][\"width\"],\n        cross_attn_every_n_layers=4,\n        **flamingo_kwargs,\n    )\n\n    if pretrained_model_path is not None:\n        print(f\"loading pretrained model from {pretrained_model_path}\")\n        model.load_state_dict(torch.load(pretrained_model_path), strict=False)\n\n    # Freeze all parameters\n    model.requires_grad_(False)\n    assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n\n    if tuning_config is not None:\n        model = prepare_model_for_tuning(model, tuning_config)\n    else:\n        raise ValueError(\"tuning_config must be provided\")\n\n    print(\n        f\"Flamingo model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n    )\n\n    return model, image_processor, text_tokenizer", "\n\ndef _infer_decoder_layers_attr_name(model):\n    for k in __KNOWN_DECODER_LAYERS_ATTR_NAMES:\n        if k.lower() in model.__class__.__name__.lower():\n            return __KNOWN_DECODER_LAYERS_ATTR_NAMES[k]\n\n    raise ValueError(\n        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n    )", "\n\n__KNOWN_DECODER_LAYERS_ATTR_NAMES = {\n    \"opt\": \"model.decoder.layers\",\n    \"gptneo\": \"transformer.h\",\n    \"gptj\": \"transformer.h\",\n    \"gpt-j\": \"transformer.h\",\n    \"pythia\": \"gpt_neox.layers\",\n    \"llama\": \"model.layers\",\n}", "    \"llama\": \"model.layers\",\n}\n\n\ndef prepare_model_for_tuning(model: nn.Module, config):\n    if config.lora:\n        lora_config = LoraConfig(\n            r=config.lora_r,\n            lora_alpha=config.lora_alpha,\n            target_modules=config.lora_target_modules,\n            lora_dropout=config.lora_dropout,\n            bias=\"none\",  # won't use bias currently\n            modules_to_save=[],  # TODO: might be helpful if save partial model\n            task_type=\"VL\",\n        )\n        model.lang_encoder = get_peft_model(model.lang_encoder, peft_config=lora_config)\n\n    # manually unfreeze modules, we use a `substring` fashion mathcing\n    for name, param in model.named_parameters():\n        if any(substr in name for substr in config.unfrozen):\n            param.requires_grad = True\n\n    if config.vis and is_rank0():\n        Visualization(model).structure_graph()\n    return model", "\n\n# temporary workaround, should use a common utils in the future\ndef is_rank0():\n    if not torch.distributed.is_initialized():\n        return True\n    return torch.distributed.get_rank() == 0\n"]}
{"filename": "mmgpt/models/open_flamingo/helpers.py", "chunked_list": ["\"\"\"\nTaken from https://github.com/lucidrains/flamingo-pytorch\n\"\"\"\n\nimport torch\nfrom einops import rearrange, repeat\nfrom einops_exts import rearrange_many\nfrom torch import einsum, nn\n\n\ndef exists(val):\n    return val is not None", "\n\ndef exists(val):\n    return val is not None\n\n\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )", "\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm_media = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, T, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, T, n2, D)\n        \"\"\"\n        x = self.norm_media(x)\n        latents = self.norm_latents(latents)\n\n        h = self.heads\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n        q = q * self.scale\n\n        # attention\n        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n        return self.to_out(out)", "\n\nclass PerceiverResampler(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth=6,\n        dim_head=64,\n        heads=8,\n        num_latents=64,\n        max_num_media=None,\n        max_num_frames=None,\n        ff_mult=4,\n    ):\n        super().__init__()\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n        self.frame_embs = nn.Parameter(torch.randn(max_num_frames, dim)) if exists(max_num_frames) else None\n        self.media_time_embs = nn.Parameter(torch.randn(max_num_media, 1, dim)) if exists(max_num_media) else None\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, T, F, v, D)\n        Returns:\n            shape (b, T, n, D) where n is self.num_latents\n        \"\"\"\n        b, T, F, v = x.shape[:4]\n\n        # frame and media time embeddings\n        if exists(self.frame_embs):\n            frame_embs = repeat(self.frame_embs[:F], \"F d -> b T F v d\", b=b, T=T, v=v)\n            x = x + frame_embs\n        x = rearrange(x, \"b T F v d -> b T (F v) d\")  # flatten the frame and spatial dimensions\n        if exists(self.media_time_embs):\n            x = x + self.media_time_embs[:T]\n\n        # blocks\n        latents = repeat(self.latents, \"n d -> b T n d\", b=b, T=T)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        return self.norm(latents)", "\n\n# gated cross attention\n\n\nclass MaskedCrossAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_visual,\n        dim_head=64,\n        heads=8,\n        only_attend_immediate_media=True,\n    ):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim_visual, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n        # whether for text to only attend to immediate preceding image, or all previous images\n        self.only_attend_immediate_media = only_attend_immediate_media\n\n    def forward(self, x, media, media_locations=None, attend_previous=True):\n        \"\"\"\n        Args:\n            x (torch.Tensor): text features\n                shape (B, T_txt, D_txt)\n            media (torch.Tensor): image features\n                shape (B, T_img, n, D_img) where n is the dim of the latents\n            media_locations: boolean mask identifying the media tokens in x\n                shape (B, T_txt)\n            attend_previous: bool\n                If false, ignores immediately preceding image and starts attending when following image\n        \"\"\"\n        _, T_img, n = media.shape[:3]\n        h = self.heads\n\n        x = self.norm(x)\n\n        q = self.to_q(x)\n        media = rearrange(media, \"b t n d -> b (t n) d\")\n\n        k, v = self.to_kv(media).chunk(2, dim=-1)\n        q, k, v = rearrange_many((q, k, v), \"b n (h d) -> b h n d\", h=h)\n\n        q = q * self.scale\n\n        sim = einsum(\"... i d, ... j d -> ... i j\", q, k)\n\n        if exists(media_locations):\n            # at each boolean of True, increment the time counter (relative to media time)\n            text_time = media_locations.cumsum(dim=-1)\n            media_time = torch.arange(T_img, device=x.device) + 1\n\n            if not attend_previous:\n                text_time[~media_locations] += 1\n                # make sure max is still the number of images in the sequence\n                text_time[\n                    text_time\n                    > repeat(\n                        torch.count_nonzero(media_locations, dim=1),\n                        \"b -> b i\",\n                        i=text_time.shape[1],\n                    )\n                ] = 0\n\n            # text time must equal media time if only attending to most immediate image\n            # otherwise, as long as text time is greater than media time (if attending to all previous images / media)\n            mask_op = torch.eq if self.only_attend_immediate_media else torch.ge\n\n            text_to_media_mask = mask_op(\n                rearrange(text_time, \"b i -> b 1 i 1\"),\n                repeat(media_time, \"j -> 1 1 1 (j n)\", n=n),\n            )\n            sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        if exists(media_locations) and self.only_attend_immediate_media:\n            # any text without a preceding media needs to have attention zeroed out\n            text_without_media_mask = text_time == 0\n            text_without_media_mask = rearrange(text_without_media_mask, \"b i -> b 1 i 1\")\n            attn = attn.masked_fill(text_without_media_mask, 0.0)\n\n        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        return self.to_out(out)", "\n\nclass GatedCrossAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        dim_visual,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n        only_attend_immediate_media=True,\n    ):\n        super().__init__()\n        self.attn = MaskedCrossAttention(\n            dim=dim,\n            dim_visual=dim_visual,\n            dim_head=dim_head,\n            heads=heads,\n            only_attend_immediate_media=only_attend_immediate_media,\n        )\n        self.attn_gate = nn.Parameter(torch.tensor([0.0]))\n\n        self.ff = FeedForward(dim, mult=ff_mult)\n        self.ff_gate = nn.Parameter(torch.tensor([0.0]))\n\n    def forward(\n        self,\n        x,\n        media,\n        media_locations=None,\n        attend_previous=True,\n    ):\n        x = (\n            self.attn(\n                x,\n                media,\n                media_locations=media_locations,\n                attend_previous=attend_previous,\n            )\n            * self.attn_gate.tanh()\n            + x\n        )\n        x = self.ff(x) * self.ff_gate.tanh() + x\n\n        return x", ""]}
{"filename": "mmgpt/train/distributed.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\nimport os\n\nimport torch\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n", "\n\ndef is_global_master(args):\n    return args.rank == 0\n\n\ndef is_local_master(args):\n    return args.local_rank == 0\n\n\ndef is_master(args, local=False):\n    return is_local_master(args) if local else is_global_master(args)", "\n\ndef is_master(args, local=False):\n    return is_local_master(args) if local else is_global_master(args)\n\n\ndef is_using_horovod():\n    # NOTE w/ horovod run, OMPI vars should be set, but w/ SLURM PMI vars will be set\n    # Differentiating between horovod and DDP use via SLURM may not be possible, so horovod arg still required...\n    ompi_vars = [\"OMPI_COMM_WORLD_RANK\", \"OMPI_COMM_WORLD_SIZE\"]\n    pmi_vars = [\"PMI_RANK\", \"PMI_SIZE\"]\n    if all([var in os.environ for var in ompi_vars]) or all([var in os.environ for var in pmi_vars]):\n        return True\n    else:\n        return False", "\n\ndef is_using_distributed():\n    if \"WORLD_SIZE\" in os.environ:\n        return int(os.environ[\"WORLD_SIZE\"]) > 1\n    if \"SLURM_NTASKS\" in os.environ:\n        return int(os.environ[\"SLURM_NTASKS\"]) > 1\n    return False\n\n\ndef world_info_from_env():\n    local_rank = 0\n    for v in (\n        \"LOCAL_RANK\",\n        \"MPI_LOCALRANKID\",\n        \"SLURM_LOCALID\",\n        \"OMPI_COMM_WORLD_LOCAL_RANK\",\n    ):\n        if v in os.environ:\n            local_rank = int(os.environ[v])\n            break\n    global_rank = 0\n    for v in (\"RANK\", \"PMI_RANK\", \"SLURM_PROCID\", \"OMPI_COMM_WORLD_RANK\"):\n        if v in os.environ:\n            global_rank = int(os.environ[v])\n            break\n    world_size = 1\n    for v in (\"WORLD_SIZE\", \"PMI_SIZE\", \"SLURM_NTASKS\", \"OMPI_COMM_WORLD_SIZE\"):\n        if v in os.environ:\n            world_size = int(os.environ[v])\n            break\n\n    return local_rank, global_rank, world_size", "\n\ndef world_info_from_env():\n    local_rank = 0\n    for v in (\n        \"LOCAL_RANK\",\n        \"MPI_LOCALRANKID\",\n        \"SLURM_LOCALID\",\n        \"OMPI_COMM_WORLD_LOCAL_RANK\",\n    ):\n        if v in os.environ:\n            local_rank = int(os.environ[v])\n            break\n    global_rank = 0\n    for v in (\"RANK\", \"PMI_RANK\", \"SLURM_PROCID\", \"OMPI_COMM_WORLD_RANK\"):\n        if v in os.environ:\n            global_rank = int(os.environ[v])\n            break\n    world_size = 1\n    for v in (\"WORLD_SIZE\", \"PMI_SIZE\", \"SLURM_NTASKS\", \"OMPI_COMM_WORLD_SIZE\"):\n        if v in os.environ:\n            world_size = int(os.environ[v])\n            break\n\n    return local_rank, global_rank, world_size", "\n\ndef init_distributed_device(args):\n    # Distributed training = training on more than one GPU.\n    # Works in both single and multi-node scenarios.\n    args.distributed = False\n    args.world_size = 1\n    args.rank = 0  # global rank\n    args.local_rank = 0\n    if args.horovod:\n        assert hvd is not None, \"Horovod is not installed\"\n        hvd.init()\n        args.local_rank = int(hvd.local_rank())\n        args.rank = hvd.rank()\n        args.world_size = hvd.size()\n        args.distributed = True\n        os.environ[\"LOCAL_RANK\"] = str(args.local_rank)\n        os.environ[\"RANK\"] = str(args.rank)\n        os.environ[\"WORLD_SIZE\"] = str(args.world_size)\n    elif is_using_distributed():\n        if \"SLURM_PROCID\" in os.environ:\n            # DDP via SLURM\n            args.local_rank, args.rank, args.world_size = world_info_from_env()\n            # SLURM var -> torch.distributed vars in case needed\n            os.environ[\"LOCAL_RANK\"] = str(args.local_rank)\n            os.environ[\"RANK\"] = str(args.rank)\n            os.environ[\"WORLD_SIZE\"] = str(args.world_size)\n            torch.distributed.init_process_group(\n                backend=args.dist_backend,\n                init_method=args.dist_url,\n                world_size=args.world_size,\n                rank=args.rank,\n            )\n        else:\n            # DDP via torchrun, torch.distributed.launch\n            args.local_rank, _, _ = world_info_from_env()\n            torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url)\n            args.world_size = torch.distributed.get_world_size()\n            args.rank = torch.distributed.get_rank()\n        args.distributed = True\n    else:\n        # needed to run on single gpu\n        torch.distributed.init_process_group(\n            backend=args.dist_backend,\n            init_method=args.dist_url,\n            world_size=1,\n            rank=0,\n        )\n\n    if torch.cuda.is_available():\n        if args.distributed and not args.no_set_device_rank:\n            device = \"cuda:%d\" % args.local_rank\n        else:\n            device = \"cuda:0\"\n        torch.cuda.set_device(device)\n    else:\n        device = \"cpu\"\n    args.device = device\n    device = torch.device(device)\n    return device", "\n\ndef is_rank0():\n    if not torch.distributed.is_initialized():\n        return True\n    return torch.distributed.get_rank() == 0\n"]}
{"filename": "mmgpt/train/instruction_finetune.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\n\nimport argparse\nimport copy\nimport glob\nimport os\nimport random\nimport time\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nimport wandb\nfrom mmengine import Config\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom tqdm import tqdm\nfrom transformers import (\n    get_constant_schedule_with_warmup,", "from transformers import (\n    get_constant_schedule_with_warmup,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\n\nfrom mmgpt import create_model_and_transforms\nfrom mmgpt.models.builder import create_toy_model_and_transforms\nfrom mmgpt.datasets import InfiniteSampler, build_dataset\nfrom mmgpt.train.distributed import init_distributed_device, world_info_from_env", "from mmgpt.datasets import InfiniteSampler, build_dataset\nfrom mmgpt.train.distributed import init_distributed_device, world_info_from_env\nfrom mmgpt.train.train_utils import AverageMeter, get_autocast, get_cast_dtype, get_checkpoint\n\n\ndef random_seed(seed=42, rank=0):\n    torch.manual_seed(seed + rank)\n    np.random.seed(seed + rank)\n    random.seed(seed + rank)\n", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--vision_encoder_path\", default=\"ViT-L-14\", type=str)\n    parser.add_argument(\"--vision_encoder_pretrained\", default=\"openai\", type=str)\n    parser.add_argument(\"--lm_path\", default=\"checkpoints/llama-7b_hf\", type=str)\n    parser.add_argument(\n        \"--tokenizer_path\",\n        default=\"checkpoints/llama-7b_hf\",\n        type=str,\n        help=\"path to tokenizer\",\n    )\n    parser.add_argument(\n        \"--pretrained_path\",\n        default=\"checkpoints/OpenFlamingo-9B/checkpoint.pt\",\n        type=str,\n        help=\"path to pretrained model\",\n    )\n    parser.add_argument(\n        \"--run_name\",\n        type=str,\n        default=\"train-my-gpt4\",\n        help=\"used to name saving directory and wandb run\",\n    )\n    parser.add_argument(\"--use_media_placement_augmentation\", action=\"store_true\")\n    parser.add_argument(\"--offline\", action=\"store_true\")\n    parser.add_argument(\"--num_epochs\", type=int, default=1)\n    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"log loss every n steps\")\n    # Sum of gradient optimization batch size\n    parser.add_argument(\n        \"--resume_from_checkpoint\",\n        type=str,\n        help=\"path to checkpoint to resume from, this should contain model, optimizer, and lr_scheduler states\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--delete_previous_checkpoint\",\n        action=\"store_true\",\n        help=\"delete previous checkpoint when saving new checkpoint\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--learning_rate\", default=1e-5, type=float)\n    parser.add_argument(\n        \"--lr_scheduler\",\n        default=\"constant\",\n        type=str,\n        help=\"constant, linear, or cosine\",\n    )\n    parser.add_argument(\"--warmup_steps\", default=100, type=int)\n    parser.add_argument(\"--weight_decay\", default=0.1, type=float)\n    parser.add_argument(\n        \"--precision\",\n        choices=[\"amp\", \"amp_bf16\", \"amp_bfloat16\", \"bf16\", \"fp16\", \"fp32\"],\n        default=\"amp\",\n        help=\"Floating point precision.\",\n    )\n    # data args\n    parser.add_argument(\"--workers\", type=int, default=0)\n    parser.add_argument(\"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--dataset_config\", type=str, default=None, help=\"path to dataset config file\")\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=16)\n    # Finetune config\n    parser.add_argument(\"--tuning_config\", type=str, default=None, help=\"path to tuning config file\")\n    # distributed training args\n    parser.add_argument(\n        \"--dist-url\",\n        default=\"env://\",\n        type=str,\n        help=\"url used to set up distributed training\",\n    )\n    parser.add_argument(\"--dist-backend\", default=\"nccl\", type=str, help=\"distributed backend\")\n    parser.add_argument(\n        \"--horovod\",\n        default=False,\n        action=\"store_true\",\n        help=\"Use horovod for distributed training.\",\n    )\n    parser.add_argument(\n        \"--no-set-device-rank\",\n        default=False,\n        action=\"store_true\",\n        help=\"Don't set device index from local rank (when CUDA_VISIBLE_DEVICES restricted to one per proc).\",\n    )\n    # wandb args\n    parser.add_argument(\"--report_to_wandb\", default=False, action=\"store_true\")\n    parser.add_argument(\n        \"--wandb_project\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--wandb_entity\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--save_checkpoints_to_wandb\",\n        default=False,\n        action=\"store_true\",\n        help=\"save checkpoints to wandb\",\n    )\n\n    args = parser.parse_args()\n\n    if args.save_checkpoints_to_wandb and not args.report_to_wandb:\n        raise ValueError(\"save_checkpoints_to_wandb requires report_to_wandb\")\n\n    if args.offline:\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n        os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n\n    args.local_rank, args.rank, args.world_size = world_info_from_env()\n\n    if args.rank == 0:\n        if not os.path.exists(args.run_name):\n            os.makedirs(args.run_name)\n\n    device_id = init_distributed_device(args)\n\n    random_seed(args.seed)\n\n    if args.tuning_config is not None:\n        tuning_config = Config.fromfile(args.tuning_config)\n    else:\n        raise ValueError(\"tuning_config must be specified\")\n\n    model, image_processor, tokenizer = create_model_and_transforms(\n        model_name=\"open_flamingo\",\n        clip_vision_encoder_path=args.vision_encoder_path,\n        clip_vision_encoder_pretrained=args.vision_encoder_pretrained,\n        lang_encoder_path=args.lm_path,\n        tokenizer_path=args.tokenizer_path if args.tokenizer_path else args.lm_path,\n        use_media_placement_augmentation=args.use_media_placement_augmentation,\n        pretrained_model_path=args.pretrained_path,\n        tuning_config=tuning_config.tuning_config,\n    )\n\n    if args.dataset_config is not None:\n        dataset_config = Config.fromfile(args.dataset_config)\n    else:\n        raise ValueError(\"dataset_config must be specified\")\n\n    dataset = build_dataset(\n        dataset_config=dataset_config.visual_datasets,\n        vis_processor=image_processor,\n        tokenizer=tokenizer,\n    )\n    train_dataloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        num_workers=args.workers,\n        sampler=DistributedSampler(dataset, shuffle=True, drop_last=True),\n        collate_fn=dataset.collater,\n    )\n\n    # build language dataset and dataloader for multi-modality training\n    if dataset_config.get('language_datasets') is not None and len(dataset_config.language_datasets) > 0:\n        lang_dataset = build_dataset(\n            dataset_config=dataset_config.language_datasets,\n            tokenizer=tokenizer,\n        )\n        lang_dataloader = DataLoader(\n            lang_dataset,\n            batch_size=args.batch_size,\n            num_workers=args.workers,\n            sampler=InfiniteSampler(lang_dataset, shuffle=True),\n            collate_fn=lang_dataset.collater,\n        )\n        lang_dataloader = iter(lang_dataloader)\n    else:\n        lang_dataloader = None\n\n    random_seed(args.seed, args.rank)\n\n    print(f\"Start running training on rank {args.rank}.\")\n\n    if args.rank == 0 and args.report_to_wandb:\n        wandb.init(\n            project=args.wandb_project,\n            entity=args.wandb_entity,\n            name=args.run_name,\n            config=vars(args),\n        )\n\n    device_id = args.rank % torch.cuda.device_count()\n    model = model.to(device_id)\n\n    ddp_model = DDP(model, device_ids=[device_id], find_unused_parameters=True)\n\n    def get_grouped_params(model):\n        params_with_wd, params_without_wd = [], []\n\n        def apply_decay(x):\n            return (\n                \"gated_cross_attn_layer\" in x\n                and \"ff_gate\" not in x\n                and \"attn_gate\" not in x\n                and \"norm\" not in x\n                and \"bias\" not in x\n            )\n\n        for n, p in model.named_parameters():\n            # if p.requires_grad:\n            if apply_decay(n):\n                params_with_wd.append(p)\n            else:\n                params_without_wd.append(p)\n\n        return [\n            {\"params\": params_with_wd, \"weight_decay\": args.weight_decay},\n            {\"params\": params_without_wd, \"weight_decay\": 0.0},\n        ]\n\n    optimizer = torch.optim.AdamW(get_grouped_params(ddp_model), lr=args.learning_rate)\n\n    total_training_steps = len(train_dataloader) * args.num_epochs\n\n    if args.rank == 0:\n        print(f\"Total training steps: {total_training_steps}\")\n\n    if args.lr_scheduler == \"linear\":\n        lr_scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=args.warmup_steps,\n            num_training_steps=total_training_steps // args.gradient_accumulation_steps,\n        )\n    elif args.lr_scheduler == \"cosine\":\n        lr_scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=args.warmup_steps,\n            num_training_steps=total_training_steps // args.gradient_accumulation_steps,\n        )\n    else:\n        lr_scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps)\n\n    # check if a checkpoint exists for this run\n    if os.path.exists(f\"{args.run_name}\") and args.resume_from_checkpoint is None:\n        checkpoint_list = glob.glob(f\"{args.run_name}/checkpoint_*.pt\")\n        if len(checkpoint_list) == 0:\n            print(f\"Found no checkpoints for run {args.run_name}.\")\n        else:\n            args.resume_from_checkpoint = sorted(checkpoint_list, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))[-1]\n            print(f\"Found checkpoint {args.resume_from_checkpoint} for run {args.run_name}.\")\n\n    resume_from_epoch = 0\n    if args.resume_from_checkpoint is not None:\n        if args.rank == 0:\n            print(f\"Loading checkpoint from {args.resume_from_checkpoint}\")\n        checkpoint = torch.load(args.resume_from_checkpoint, map_location=\"cpu\")\n        ddp_model.load_state_dict(checkpoint[\"model_state_dict\"], False)\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n        resume_from_epoch = checkpoint[\"epoch\"] + 1\n\n    ddp_model.train()\n\n    for epoch in range(resume_from_epoch, args.num_epochs):\n        train_dataloader.sampler.set_epoch(epoch)\n\n        train_one_epoch(\n            args=args,\n            model=ddp_model,\n            epoch=epoch,\n            tokenizer=tokenizer,\n            optimizer=optimizer,\n            lr_scheduler=lr_scheduler,\n            train_dataloader=train_dataloader,\n            language_dataloader=lang_dataloader,\n            device_id=device_id,\n            wandb=wandb,\n        )\n\n        if args.rank == 0:\n            if not os.path.exists(args.run_name):\n                os.makedirs(args.run_name)\n\n            checkpoint_dict = {\n                \"epoch\": epoch,\n                \"model_state_dict\": get_checkpoint(ddp_model),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n                \"tuning_config\": tuning_config,\n            }\n\n            print(f\"Saving checkpoint to {args.run_name}/checkpoint_{epoch}.pt\")\n            torch.save(checkpoint_dict, f\"{args.run_name}/checkpoint_{epoch}.pt\")\n            if args.report_to_wandb and args.save_checkpoints_to_wandb:\n                wandb.save(f\"{args.run_name}/checkpoint_{epoch}.pt\")\n\n            if args.delete_previous_checkpoint:\n                if epoch > 0:\n                    os.remove(f\"{args.run_name}/checkpoint_{epoch-1}.pt\")\n    if args.rank == 0:\n        torch.save(\n            {\"model_state_dict\": get_checkpoint(ddp_model.module), \"tuning_config\": tuning_config},\n            f\"{args.run_name}/final_weights.pt\",\n        )\n        if args.report_to_wandb and args.save_checkpoints_to_wandb:\n            wandb.save(f\"{args.run_name}/final_weights.pt\")", "\n\ndef train_one_epoch(\n    args,\n    model,\n    epoch,\n    train_dataloader,\n    language_dataloader,\n    tokenizer,\n    optimizer,\n    lr_scheduler,\n    device_id,\n    wandb,\n):\n    num_batches_per_epoch = len(train_dataloader)\n\n    total_training_steps = num_batches_per_epoch * args.num_epochs\n\n    autocast = get_autocast(args.precision)\n    cast_dtype = get_cast_dtype(args.precision)\n\n    model.train()\n\n    # setup logging\n    step_time_m = AverageMeter()  # time for one optimizer step (> 1 batch if using gradient accum)\n    data_time_m = (\n        AverageMeter()\n    )  # avg time to load one batch of both C4 AND laion (= 1 batch regardless of gradient accum)\n    end = time.time()\n\n    # loop through dataloader\n    for num_steps, batch in tqdm(\n        enumerate(train_dataloader),\n        disable=args.rank != 0,\n        total=total_training_steps,\n        initial=(epoch * num_batches_per_epoch),\n    ):\n        data_time_m.update(time.time() - end)\n\n        global_step = num_steps + epoch * num_batches_per_epoch\n\n        #### VISION FORWARD PASS ####\n        images = batch[\"image\"].to(device_id, dtype=cast_dtype, non_blocking=True).unsqueeze(1).unsqueeze(1)\n        input_ids = batch[\"input_ids\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n        attention_mask = batch[\"attention_mask\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n        labels = batch[\"labels\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n\n        with autocast():\n            loss_batch = model(\n                vision_x=images,\n                lang_x=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n            )[0]\n        loss = loss_batch / args.gradient_accumulation_steps\n        loss_vision = loss  # for logging\n\n        #### BACKWARD PASS ####\n        loss.backward()\n\n        #### LANGUAGE FORWARD PASS ####\n        if language_dataloader is not None:\n            batch_lang = next(language_dataloader)\n            lang_input_ids = batch_lang[\"input_ids\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n            lang_attention_mask = batch_lang[\"attention_mask\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n            lang_labels = batch_lang[\"labels\"].to(device_id, dtype=cast_dtype, non_blocking=True)\n\n            with autocast():\n                lang_loss_batch = model(\n                    vision_x=None,\n                    lang_x=lang_input_ids,\n                    attention_mask=lang_attention_mask,\n                    labels=lang_labels,\n                )[0]\n            lang_loss = lang_loss_batch / args.gradient_accumulation_steps\n            #### BACKWARD PASS ####\n            lang_loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # step optimizer and log\n        if (((num_steps + 1) % args.gradient_accumulation_steps) == 0) or (num_steps == num_batches_per_epoch - 1):\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            # step time and reset end outside of rank 0\n            step_time_m.update(time.time() - end)\n            end = time.time()\n\n            if args.rank == 0 and args.report_to_wandb:\n                # compute within rank 0\n                samples_per_second = (\n                    args.gradient_accumulation_steps * args.batch_size * args.world_size / step_time_m.val\n                )\n                samples_per_second_per_gpu = args.gradient_accumulation_steps * args.batch_size / step_time_m.val\n\n                wandb.log(\n                    {\n                        \"data_time\": data_time_m.avg,\n                        \"step_time\": step_time_m.avg,\n                        \"samples_per_second\": samples_per_second,\n                        \"samples_per_second_per_gpu\": samples_per_second_per_gpu,\n                        \"lr\": optimizer.param_groups[0][\"lr\"],\n                    },\n                    commit=False,\n                )\n                step_time_m.reset()\n                data_time_m.reset()\n\n                loss_log = {\n                    \"loss\": loss.item(),\n                    \"loss_vision\": loss_vision.item(),\n                    \"global_step\": global_step,\n                }\n                if language_dataloader is not None:\n                    loss_log[\"loss_lang\"] = lang_loss.item()\n\n                wandb.log(loss_log, commit=True)\n\n        # Log loss to console\n        if ((num_steps + 1) % args.logging_steps == 0) and args.rank == 0:\n            print(\n                f\"Step {num_steps+1}/{num_batches_per_epoch} of epoch {epoch+1}/{args.num_epochs} complete. Loss: {loss.item():.3f}\"\n            )", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "mmgpt/train/__init__.py", "chunked_list": ["\n"]}
{"filename": "mmgpt/train/train_utils.py", "chunked_list": ["\"\"\"Modified from https://github.com/mlfoundations/open_flamingo\"\"\"\nimport time\nfrom contextlib import suppress\n\nimport torch\nfrom tqdm import tqdm\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype", "def get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef get_autocast(precision):\n    if precision == \"amp\":\n        return torch.cuda.amp.autocast\n    elif precision == \"amp_bfloat16\" or precision == \"amp_bf16\":\n        # amp_bfloat16 is more stable than amp float16 for clip training\n        return lambda: torch.cuda.amp.autocast(dtype=torch.bfloat16)\n    else:\n        return suppress", "\ndef get_autocast(precision):\n    if precision == \"amp\":\n        return torch.cuda.amp.autocast\n    elif precision == \"amp_bfloat16\" or precision == \"amp_bf16\":\n        # amp_bfloat16 is more stable than amp float16 for clip training\n        return lambda: torch.cuda.amp.autocast(dtype=torch.bfloat16)\n    else:\n        return suppress\n", "\n\ndef train_one_epoch(\n    args,\n    model,\n    epoch,\n    laion_loader,\n    mmc4_loader,\n    tokenizer,\n    optimizer,\n    lr_scheduler,\n    device_id,\n    wandb,\n):\n    num_batches_per_epoch_laion = laion_loader.num_batches\n    num_batches_per_epoch_mmc4 = mmc4_loader.num_batches\n\n    assert (\n        num_batches_per_epoch_laion == num_batches_per_epoch_mmc4\n    ), \"Number of batches in laion and mmc4 datasets must be the same\"\n    num_batches_per_epoch = num_batches_per_epoch_mmc4\n    total_training_steps = num_batches_per_epoch * args.num_epochs\n\n    autocast = get_autocast(args.precision)\n    cast_dtype = get_cast_dtype(args.precision)\n\n    media_token_id = tokenizer(\"<image>\", add_special_tokens=False)[\"input_ids\"][-1]\n    endofchunk_token_id = tokenizer(\"<|endofchunk|>\", add_special_tokens=False)[\"input_ids\"][-1]\n\n    model.train()\n\n    # setup logging\n    step_time_m = AverageMeter()  # time for one optimizer step (> 1 batch if using gradient accum)\n    data_time_m = (\n        AverageMeter()\n    )  # avg time to load one batch of both C4 AND laion (= 1 batch regardless of gradient accum)\n    end = time.time()\n\n    # loop through dataloader\n    for num_steps, (batch_laion, batch_mmc4) in tqdm(\n        enumerate(zip(laion_loader, mmc4_loader)),\n        disable=args.rank != 0,\n        total=total_training_steps,\n        initial=(epoch * num_batches_per_epoch),\n    ):\n        data_time_m.update(time.time() - end)\n\n        global_step = num_steps + epoch * num_batches_per_epoch\n\n        #### LAION FORWARD PASS ####\n        images = batch_laion[0].to(device_id, dtype=cast_dtype, non_blocking=True).unsqueeze(1).unsqueeze(1)\n\n        input_ids = batch_laion[1][0].to(device_id, dtype=cast_dtype, non_blocking=True)\n        attention_mask = batch_laion[1][1].to(device_id, dtype=cast_dtype, non_blocking=True)\n\n        labels = input_ids.clone()\n        labels[labels == tokenizer.pad_token_id] = -100\n        labels[:, 0] = -100\n        labels[labels == media_token_id] = -100\n        labels.to(device_id)\n\n        with autocast():\n            loss_laion = model(\n                vision_x=images,\n                lang_x=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n            )[0]\n        divided_loss_laion = loss_laion / args.gradient_accumulation_steps\n\n        #### C4 FORWARD PASS ####\n        images = batch_mmc4[0].to(device_id, dtype=cast_dtype, non_blocking=True).unsqueeze(2)\n        input_ids = torch.stack([x[0] for x in batch_mmc4[1]]).squeeze(1)\n        attention_mask = torch.stack([x[1] for x in batch_mmc4[1]]).squeeze(1)\n\n        # NOTE: irena: expected shape of clip_text_input_ids / attention_mask is (N, I, max_seq_len)\n        labels = input_ids.clone()\n        labels[labels == tokenizer.pad_token_id] = -100\n        labels[:, 0] = -100\n\n        for i in range(labels.shape[0]):\n            # remove loss for any token before the first <image> token\n            label_idx = 0\n            while label_idx < labels.shape[1] and labels[i][label_idx] != media_token_id:\n                labels[i][label_idx] = -100\n                label_idx += 1\n\n            # get index of all endofchunk tokens in the sequence\n            endofchunk_idxs = torch.where(labels[i] == endofchunk_token_id)[0]\n            for endofchunk_idx in endofchunk_idxs:\n                token_idx = endofchunk_idx + 1\n                while token_idx < labels.shape[1] and labels[i][token_idx] != media_token_id:\n                    labels[i][token_idx] = -100\n                    token_idx += 1\n\n        labels[labels == media_token_id] = -100\n        labels.to(device_id)\n\n        with autocast():\n            loss_mmc4 = model(\n                vision_x=images,\n                lang_x=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n            )[0]\n\n            # if loss is nan, skip this batch\n            if torch.isnan(loss_mmc4):\n                print(\"loss is nan, skipping this batch\")\n                print(\"input_ids: \", tokenizer.batch_decode(input_ids))\n                print(\"labels: \", labels)\n                print(\"images: \", images)\n                optimizer.zero_grad()\n                continue\n\n        divided_loss_mmc4 = loss_mmc4 / args.gradient_accumulation_steps\n\n        #### BACKWARD PASS ####\n        loss = divided_loss_laion * args.loss_multiplier_laion + divided_loss_mmc4 * args.loss_multiplier_mmc4\n        loss.backward()\n\n        #### MASK GRADIENTS FOR EMBEDDINGS ####\n        # Note (anas): Do not apply weight decay to embeddings as it will break this function.\n        def mask_embedding(m):\n            if isinstance(m, torch.nn.Embedding) and m.weight.requires_grad:\n                zero_mask = torch.zeros_like(m.weight.grad)\n                zero_mask[media_token_id] = torch.ones_like(zero_mask[media_token_id])\n                zero_mask[endofchunk_token_id] = torch.ones_like(zero_mask[endofchunk_token_id])\n                m.weight.grad = m.weight.grad * zero_mask\n\n        model.apply(mask_embedding)\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # step optimizer and log\n        if (((num_steps + 1) % args.gradient_accumulation_steps) == 0) or (num_steps == num_batches_per_epoch - 1):\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            # step time and reset end outside of rank 0\n            step_time_m.update(time.time() - end)\n            end = time.time()\n\n            if args.rank == 0 and args.report_to_wandb:\n                # compute within rank 0\n                laion_samples_per_second = (\n                    args.gradient_accumulation_steps * args.batch_size_laion * args.world_size / step_time_m.val\n                )\n                laion_samples_per_second_per_gpu = (\n                    args.gradient_accumulation_steps * args.batch_size_laion / step_time_m.val\n                )\n\n                c4_samples_per_second = (\n                    args.gradient_accumulation_steps * args.batch_size_mmc4 * args.world_size / step_time_m.val\n                )\n                c4_samples_per_second_per_gpu = (\n                    args.gradient_accumulation_steps * args.batch_size_mmc4 / step_time_m.val\n                )\n\n                wandb.log(\n                    {\n                        \"data_time\": data_time_m.avg,\n                        \"step_time\": step_time_m.avg,\n                        \"laion_samples_per_second\": laion_samples_per_second,\n                        \"laion_samples_per_second_per_gpu\": laion_samples_per_second_per_gpu,\n                        \"c4_samples_per_second\": c4_samples_per_second,\n                        \"c4_samples_per_second_per_gpu\": c4_samples_per_second_per_gpu,\n                        \"lr\": optimizer.param_groups[0][\"lr\"],\n                    },\n                    commit=False,\n                )\n                step_time_m.reset()\n                data_time_m.reset()\n\n                wandb.log(\n                    {\n                        \"loss_laion\": divided_loss_laion.item(),\n                        \"global_step\": global_step,\n                    },\n                    commit=False,\n                )\n                wandb.log(\n                    {\"loss_mmc4\": divided_loss_mmc4.item(), \"global_step\": global_step},\n                    commit=True,\n                )\n\n        # Log loss to console\n        if ((num_steps + 1) % args.logging_steps == 0) and args.rank == 0:\n            print(\n                f\"Step {num_steps+1}/{num_batches_per_epoch} of epoch {epoch+1}/{args.num_epochs} complete. Loss LAION: {loss_laion.item():.3f} // Loss MMC4: {loss_mmc4.item():.3f}\"\n            )", "\n\ndef get_checkpoint(model: torch.nn.Module):\n    state_dict = model.state_dict()\n    parameters = {k: v for k, v in model.named_parameters()}\n    # remove duplicate parameters\n    duplicate_keys = set(state_dict.keys()) - set(parameters.keys())\n    for k in duplicate_keys:\n        del state_dict[k]\n    # remove non-grad parameters\n    for name, p in parameters.items():\n        if not p.requires_grad:\n            del state_dict[name]\n\n    return state_dict", "\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", ""]}
{"filename": "mmgpt/datasets/alpaca_gpt4_dataset.py", "chunked_list": ["import json\n\nfrom mmgpt.datasets.dolly_dataset import DollyDataset\n\n\nclass AlpacaGPT4Dataset(DollyDataset):\n    \"\"\"\n    ```json\n    [\n        {\n            \"instruction\": \"Identify the odd one out.\",\n            \"input\": \"Twitter, Instagram, Telegram\",\n            \"output\": \"The odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.\"\n        },\n    ]\n    \"\"\"\n\n    def load_annotation(self, ann_path):\n        self.annotation = json.load(open(ann_path, \"r\"))\n\n    def process_text(self, ann):\n        instruction = ann[\"instruction\"]\n        input = ann[\"input\"]\n        output = ann[\"output\"]\n        instruction = self.prompter(instruction=instruction, input=input)\n        return dict(instruction=instruction, answer=output)", ""]}
{"filename": "mmgpt/datasets/aokvqa_dataset.py", "chunked_list": ["import random\n\nfrom .vqa_dataset import VQADataset\n\nREASON_QUESTIONS = [\n    \"Why?\",\n    \"Why is this?\",\n    \"And why?\",\n    \"What is the reason?\",\n    \"And can you tell me why?\",", "    \"What is the reason?\",\n    \"And can you tell me why?\",\n    \"Can you tell me why?\",\n    \"Can you tell me the reason?\",\n]\n\n\nclass AOKVQADataset(VQADataset):\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths, **kwargs)\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n        question = question + \" \" + random.choice(REASON_QUESTIONS)\n\n        choices = ann[\"choices\"]\n        true_answer = choices[ann[\"correct_choice_idx\"]]\n        answer = \"The answer is \" + true_answer + \". Because \" + \" \".join(ann[\"rationales\"])\n\n        is_option = random.random() < self.option_prob and len(choices) > 1\n        if is_option:\n            instruction = self.prompter(question, choices)\n        else:\n            instruction = self.prompter(question)\n\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)", "\n\ndef build_aokvqa_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/coco/images\",\n    ann_paths=[\"data/aokvqa/annotations/aokvqa_v1p0_train.json\"],\n    sample_image=False,\n):\n    return AOKVQADataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n        sample_image=sample_image,\n    )", ""]}
{"filename": "mmgpt/datasets/dial_dataset.py", "chunked_list": ["from .vqa_dataset import VQADataset\n\nTEMPLATE = {\n    \"description\": \"Template used by Alpaca-LoRA.\",\n    # \"prompt_choice\": \"Below is a multiple choice question about an image, along with answer options. Please choose the correct answer from these options.\\n\\n### Image:\\n{image}\\n\\n### Question:\\n{question}\\n\\n### Options:\\n{options}\\n\\n### Answer:\\n\",\n    # \"prompt_qa\": \"Below is a question about an image. Write a response to answer the question.\\n\\n### Image:\\n{image}\\n\\n### Question:\\n{question}\\n\\n### Answer:\\n\",\n    \"prompt_choice\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{options}\\n\\n### Response:\\n\",\n    \"prompt_qa\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"prompt_dial\": \"\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"response_split\": \"### Response:\",", "    \"prompt_dial\": \"\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"response_split\": \"### Response:\",\n}\n\n\nclass DialPrompter:\n    def __call__(self, question, options=None):\n        if options:\n            options = \", \".join(options)\n            res = TEMPLATE[\"prompt_choice\"].format(image=\"<image>\", question=question, options=options)\n        else:\n            res = TEMPLATE[\"prompt_dial\"].format(question=question)\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(TEMPLATE[\"response_split\"])[-1].strip()", "\n\nclass DialDataset(VQADataset):\n    def __init__(self, *args, **kwargs):\n        super(DialDataset, self).__init__(*args, **kwargs)\n        self.prompter = DialPrompter()\n\n    def _add_instance_ids(self, key=\"id\"):\n        for idx, ann in enumerate(self.annotation):\n            ann[key] = str(idx)\n\n    def process_text(self, anns):\n        # TODO remove this\n        begin_string = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\".format(\n            image=\"<image>\"\n        )\n        num_convs = len(anns[\"conversations\"]) // 2\n        conv_list = []\n        for conv_id in range(num_convs):\n            question = anns[\"conversations\"][conv_id][\"value\"]\n            # remove '<image>' tag and '\\n'\n            question = question.replace(\"<image>\", \"\").replace(\"\\n\", \"\")\n            answer = anns[\"conversations\"][conv_id + 1][\"value\"]\n            instruction = self.prompter(question)\n            if conv_id == 0:\n                single_conv = dict(instruction=begin_string + instruction, answer=answer)\n            else:\n                single_conv = dict(instruction=instruction, answer=answer)\n            conv_list.append(single_conv)\n        return conv_list\n\n    def __getitem__(self, index):\n        ann = self.annotation[index]\n        image = self.process_image(ann)\n        text_list = self.process_text(ann)\n        res_list = []\n        for text in text_list:\n            single_res = self.tokenize(text)\n            single_res[\"instruction\"] = text[\"instruction\"]\n            single_res[\"answer\"] = text[\"answer\"]\n            res_list.append(single_res)\n\n        input_ids = []\n        attention_mask = []\n        labels = []\n        instruction = []\n        answer = []\n        for res in res_list:\n            input_ids.extend(res[\"input_ids\"])\n            attention_mask.extend(res[\"attention_mask\"])\n            labels.extend(res[\"labels\"])\n            instruction.extend(res[\"instruction\"])\n            answer.extend(res[\"answer\"])\n\n        res = dict(\n            input_ids=input_ids, attention_mask=attention_mask, labels=labels, instruction=instruction, answer=answer\n        )\n        res.update(image=image)\n        return res", ""]}
{"filename": "mmgpt/datasets/clevr_dataset.py", "chunked_list": ["import json\nimport os\nimport random\nfrom collections import defaultdict\n\nfrom PIL import Image\n\nfrom .vqa_dataset import VQADataset\n\n\nclass CLEVRDataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset. It also contains Dialog.\n\n    Note: The image is a little bit simple. with several objects and simple background.\n    \"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            ann = json.load(open(ann_path, \"r\"))\n            annotation.extend(ann[\"questions\"])\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"image_filename\"]].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n        answer = ann[\"answer\"]\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)\n\n    def process_image(self, ann):\n        split = ann[\"split\"]\n        image_path = os.path.join(self.vis_root, split, ann[\"image_filename\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\nclass CLEVRDataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset. It also contains Dialog.\n\n    Note: The image is a little bit simple. with several objects and simple background.\n    \"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            ann = json.load(open(ann_path, \"r\"))\n            annotation.extend(ann[\"questions\"])\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"image_filename\"]].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n        answer = ann[\"answer\"]\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)\n\n    def process_image(self, ann):\n        split = ann[\"split\"]\n        image_path = os.path.join(self.vis_root, split, ann[\"image_filename\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\ndef build_clevr_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/clevr/CLEVR_v1.0/images\",\n    ann_paths=[\n        \"data/clevr/CLEVR_v1.0/questions/CLEVR_train_questions.json\",\n        \"data/clevr/CLEVR_v1.0/questions/CLEVR_val_questions.json\",\n    ],\n    sample_image=False,\n):\n    return CLEVRDataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n        sample_image=sample_image,\n    )", ""]}
{"filename": "mmgpt/datasets/coco_caption_dataset.py", "chunked_list": ["\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport json\nimport os\nimport random", "import os\nimport random\n\nimport numpy as np\nfrom PIL import Image\nfrom transformers import LlamaTokenizer\n\nfrom .vqa_dataset import VQADataset\n\nQUESTIONS = [", "\nQUESTIONS = [\n    \"please describe the image\",\n    \"can you describe the image\",\n    \"Could you provide a description of the image?\",\n    \"What do you see in this image?\",\n    \"Share your thoughts on the content of the image.\",\n    \"Please narrate what's happening in the picture.\",\n    \"Can you give a brief explanation of the image?\",\n    \"Describe the main elements and details present in the image.\",", "    \"Can you give a brief explanation of the image?\",\n    \"Describe the main elements and details present in the image.\",\n    \"In your own words, what is depicted in the image?\",\n    \"Can you outline the key aspects of the image?\",\n    \"What are the most striking features in this image?\",\n    \"Please provide a summary of the image's content.\",\n    \"Describe the overall theme or concept captured in the image.\",\n    \"How would you explain the image's composition and focus?\",\n    \"What is the focal point or main subject of the image?\",\n    \"How do the different components of the image interact with each other?\",", "    \"What is the focal point or main subject of the image?\",\n    \"How do the different components of the image interact with each other?\",\n    \"What would be a fitting caption for this image?\",\n    \"Can you create a concise description that captures the essence of the image?\",\n    \"How would you briefly summarize the content of this image in a phrase or sentence?\",\n    \"Please provide a catchy and relevant caption for this picture.\",\n    \"If you were to give this image a title, what would it be?\",\n    \"Describe the image in one creative sentence.\",\n    \"Please suggest a memorable phrase that encapsulates the image's content.\",\n    \"What engaging phrase would best represent this image?\",", "    \"Please suggest a memorable phrase that encapsulates the image's content.\",\n    \"What engaging phrase would best represent this image?\",\n    \"Can you create an expressive caption that highlights the main theme of the image?\",\n    \"How would you sum up the image's story for a caption?\",\n    \"Provide an eye-catching caption that conveys the image's core message.\",\n    \"If you were to give this image a headline, what would it say?\",\n    \"Can you craft a captivating caption that communicates the essence of the image?\",\n    \"How would you describe the image's content in a powerful caption?\",\n    \"Please provide an inventive title to summarize the scene depicted in the image.\",\n    \"Compose a concise and striking phrase that reflects the image's key elements.\",", "    \"Please provide an inventive title to summarize the scene depicted in the image.\",\n    \"Compose a concise and striking phrase that reflects the image's key elements.\",\n    \"If you were to create a caption for this image, what would it be?\",\n    \"Offer a compelling caption that highlights the central focus of the image.\",\n    \"Can you produce a unique caption that encapsulates the image's overall mood?\",\n    \"Please generate an attention-grabbing caption that would best illustrate the events captured in this image\",\n    \"How would you express the image's main idea in an impactful sentence?\",\n    \"Please create a vivid and concise title that conveys the essence of the picture.\",\n    \"Compose an imaginative caption that reflects the image's most striking features.\",\n    \"What memorable statement would best represent the scene illustrated in this image?\",", "    \"Compose an imaginative caption that reflects the image's most striking features.\",\n    \"What memorable statement would best represent the scene illustrated in this image?\",\n    \"Draft an evocative caption that brings the image to life for the reader.\",\n    \"Can you suggest an insightful caption that highlights the underlying message of the image?\",\n    \"What engaging phrase would effectively convey the action or subject matter depicted in this picture?\",\n    \"How would you encapsulate the image's core theme in a concise and expressive manner?\",\n    \"Please provide a creative and impactful title that captures the spirit of the image.\",\n    \"Craft a captivating caption that showcases the image's most prominent attributes.\",\n    \"What intriguing statement would best sum up the scene presented in this image?\",\n    \"Develop a descriptive caption that paints a vivid picture for the viewer.\",", "    \"What intriguing statement would best sum up the scene presented in this image?\",\n    \"Develop a descriptive caption that paints a vivid picture for the viewer.\",\n    \"Can you give a detailed account of the image's contents?\",\n    \"What are the key elements and features visible in this image?\",\n    \"How would you narrate the events or actions depicted in the picture?\",\n    \"Please share your observations about the various components present in the image.\",\n    \"What is the overall theme or concept captured in this image? Can you describe it?\",\n]\n\n\nclass COCOCaptionDataset(VQADataset):\n    def __init__(\n        self, tokenizer, vis_processor=None, vis_root=None, ann_paths=[], add_eos=True, ignore_instruction=True\n    ):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        self.tokenizer: LlamaTokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\")))\n\n        self.vis_processor = vis_processor\n\n        instructions = []\n        for question in QUESTIONS:\n            # instruction = f\"Below is a question about an image. Write a response to answer the question.\\n\\n### Image:\\n<image>\\n\\n### Question:\\n{question}\\n\\n### Answer:\\n\".format(\n            #    question\n            # )\n            instruction = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\".format(\n                image=\"<image>\", question=question\n            )\n            instructions.append(instruction)\n        self.instructions = instructions\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        all_captions = ann[\"caption\"]\n        if not isinstance(all_captions, list):\n            all_captions = [all_captions]\n        caption = random.choice(all_captions)\n        instruction = random.choice(self.instructions)\n\n        return dict(instruction=instruction, answer=caption)", "\n\nclass COCOCaptionDataset(VQADataset):\n    def __init__(\n        self, tokenizer, vis_processor=None, vis_root=None, ann_paths=[], add_eos=True, ignore_instruction=True\n    ):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        self.tokenizer: LlamaTokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\")))\n\n        self.vis_processor = vis_processor\n\n        instructions = []\n        for question in QUESTIONS:\n            # instruction = f\"Below is a question about an image. Write a response to answer the question.\\n\\n### Image:\\n<image>\\n\\n### Question:\\n{question}\\n\\n### Answer:\\n\".format(\n            #    question\n            # )\n            instruction = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\".format(\n                image=\"<image>\", question=question\n            )\n            instructions.append(instruction)\n        self.instructions = instructions\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        all_captions = ann[\"caption\"]\n        if not isinstance(all_captions, list):\n            all_captions = [all_captions]\n        caption = random.choice(all_captions)\n        instruction = random.choice(self.instructions)\n\n        return dict(instruction=instruction, answer=caption)", ""]}
{"filename": "mmgpt/datasets/text_ocr_dataset.py", "chunked_list": ["import json\nimport os\nimport random\n\nimport numpy as np\nfrom PIL import Image\nfrom transformers import LlamaTokenizer\n\nfrom .vqa_dataset import VQADataset, VQAPrompter\n", "from .vqa_dataset import VQADataset, VQAPrompter\n\n\nclass TextOCRDataset(VQADataset):\n    def __init__(\n        self, tokenizer, vis_processor=None, vis_root=None, ann_paths=[], add_eos=True, ignore_instruction=True\n    ):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        assert tokenizer.add_eos_token is False, \"tokenizer should not add eos token by default\"\n        self.tokenizer: LlamaTokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\"))[\"data\"])\n\n        self.vis_processor = vis_processor\n\n        self._add_instance_ids()\n        self.option_prob = 0.5\n        self.prompter = VQAPrompter()\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image_id\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n\n        answer_weight = {}\n        for answer in ann[\"answers\"]:\n            if answer in answer_weight.keys():\n                answer_weight[answer] += 1 / len(ann[\"answers\"])\n            else:\n                answer_weight[answer] = 1 / len(ann[\"answers\"])\n\n        answers = list(answer_weight.keys())\n        weights = list(answer_weight.values())\n\n        # create instruction\n        true_answer = answers[np.argmax(weights)]\n        is_option = random.random() < self.option_prob and len(answers) > 1\n        if is_option:\n            instruction = self.prompter(question, answers)\n        else:\n            instruction = self.prompter(question)\n\n        return dict(instruction=instruction, answer=true_answer)", ""]}
{"filename": "mmgpt/datasets/dolly_dataset.py", "chunked_list": ["import copy\nimport json\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom transformers import LlamaTokenizer\n\nTEMPLATE = {\n    \"description\": \"Template used by LLM.\",\n    \"prompt_no_input_format\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",", "    \"description\": \"Template used by LLM.\",\n    \"prompt_no_input_format\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n    \"prompt_with_input_format\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n    \"response_split\": \"### Response:\",\n}\n\n\nclass LMPrompter:\n    def __call__(self, instruction, input=None):\n        if input is None or len(input) == 0:\n            return TEMPLATE[\"prompt_no_input_format\"].format(instruction=instruction)\n        else:\n            return TEMPLATE[\"prompt_with_input_format\"].format(instruction=instruction, input=input)\n\n    def get_response(self, output: str) -> str:\n        return output.split(TEMPLATE[\"response_split\"])[-1].strip()", "\n\nclass DollyDataset(Dataset):\n    \"\"\"Each line of the annotation file is a json object with the following fields:\n\n    {\n        \"instruction\": \"What is a dispersive prism?\",\n        \"context\": \"In optics, a dispersive prism is an optical prism that is used to disperse light, that is, to separate light into its spectral components (the colors of the rainbow). Different wavelengths (colors) of light will be deflected by the prism at different angles.[1] This is a result of the prism material's index of refraction varying with wavelength (dispersion). Generally, longer wavelengths (red) undergo a smaller deviation than shorter wavelengths (blue). The dispersion of white light into colors by a prism led Sir Isaac Newton to conclude that white light consisted of a mixture of different colors.\",\n        \"response\": \"A dispersive prism is an optical prism that disperses the light's different wavelengths at different angles. When white light is shined through a dispersive prism it will separate into the different colors of the rainbow.\",\n        \"category\": \"summarization\"\n    }\n\n    \"\"\"\n\n    def __init__(self, tokenizer, ann_path: str, add_eos=True, ignore_instruction=True, **kwargs):\n        \"\"\"\n        ann_path (string): directory to store the annotation file\n        \"\"\"\n        assert tokenizer.add_eos_token is False, \"tokenizer should not add eos token by default\"\n        self.tokenizer: LlamaTokenizer = tokenizer\n\n        self.annotation = []\n        self.prompter = LMPrompter()\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n        self.load_annotation(ann_path)\n\n    def load_annotation(self, ann_path):\n        self.annotation = []\n        for line in open(ann_path, \"r\").readlines():\n            self.annotation.append(json.loads(line))\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def process_text(self, ann):\n        instruction = ann[\"instruction\"]\n        context = ann[\"context\"]\n        response = ann[\"response\"]\n        instruction = self.prompter(instruction=instruction, input=context)\n        return dict(instruction=instruction, answer=response)\n\n    def tokenize(self, text):\n        res = self.tokenizer(\n            text[\"instruction\"] + text[\"answer\"],\n            return_tensors=None,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=512,\n        )\n\n        # manually add eos token\n        if res[\"input_ids\"][-1] != self.tokenizer.eos_token_id and len(res[\"input_ids\"]) < 512 and self.add_eos:\n            res[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            res[\"attention_mask\"].append(1)\n        labels = copy.deepcopy(res[\"input_ids\"])\n        # ignore instruction_token\n        if self.ignore_instruction:\n            instruction_token = self.tokenizer(\n                text[\"instruction\"], return_tensors=None, padding=\"do_not_pad\", truncation=True, max_length=512\n            )\n            labels = [-100] * len(instruction_token[\"input_ids\"]) + labels[len(instruction_token[\"input_ids\"]) :]\n\n        res.update(labels=labels)\n        return res\n\n    def __getitem__(self, index):\n        ann = self.annotation[index]\n        text = self.process_text(ann)\n        res = self.tokenize(text)\n        res.update(text)\n        return res\n\n    def collater(self, samples):\n        question_list, answer_list, input_id_list, attention_mask_list, labels_list = [], [], [], [], []\n\n        for sample in samples:\n            question_list.append(sample[\"instruction\"])\n            answer_list.append(sample[\"answer\"])\n            input_id_list.append(sample[\"input_ids\"])\n            attention_mask_list.append(sample[\"attention_mask\"])\n            labels_list.append(sample[\"labels\"])\n\n        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n        # same length to return tensors.\n        max_label_length = max(len(l) for l in labels_list)\n        padding_side = self.tokenizer.padding_side\n        padded_labels = []\n        for l in labels_list:\n            remainder = [-100] * (max_label_length - len(l))\n            if isinstance(l, list):\n                l = l + remainder if padding_side == \"right\" else remainder + l\n            elif padding_side == \"right\":\n                l = np.concatenate([l, remainder]).astype(np.int64)\n            else:\n                l = np.concatenate([remainder, l]).astype(np.int64)\n            padded_labels.append(l)\n\n        padded_samples = self.tokenizer.pad(\n            {\"input_ids\": input_id_list, \"attention_mask\": attention_mask_list, \"labels\": padded_labels},\n            return_tensors=\"pt\",\n            padding=\"longest\",\n        )\n\n        labels = padded_samples[\"labels\"]\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        labels[:, 0] = -100\n        return {\n            \"input_ids\": padded_samples[\"input_ids\"],\n            \"attention_mask\": padded_samples[\"attention_mask\"],\n            \"labels\": labels,\n            \"instruction\": question_list,\n            \"answer\": answer_list,\n        }", "\n\ndef build_dolly_dataset(\n    tokenizer,\n    ann_path=\"data/dolly/databricks-dolly-15k.jsonl\",\n    **kwargs,\n):\n    return DollyDataset(\n        tokenizer=tokenizer,\n        ann_path=ann_path,\n        **kwargs,\n    )", ""]}
{"filename": "mmgpt/datasets/cc_sbu_align_dataset.py", "chunked_list": ["import json\nimport os\nimport random\n\nfrom PIL import Image\n\nfrom .vqa_dataset import VQADataset, VQAPrompter\n\nQUESTIONS = [\n    \"please describe the image\",", "QUESTIONS = [\n    \"please describe the image\",\n    \"can you describe the image\",\n    \"Could you provide a description of the image?\",\n    \"What do you see in this image?\",\n    \"Share your thoughts on the content of the image.\",\n    \"Please narrate what's happening in the picture.\",\n    \"Can you give a brief explanation of the image?\",\n    \"Describe the main elements and details present in the image.\",\n    \"In your own words, what is depicted in the image?\",", "    \"Describe the main elements and details present in the image.\",\n    \"In your own words, what is depicted in the image?\",\n    \"Can you outline the key aspects of the image?\",\n    \"What are the most striking features in this image?\",\n    \"Please provide a summary of the image's content.\",\n    \"Describe the overall theme or concept captured in the image.\",\n    \"How would you explain the image's composition and focus?\",\n    \"What is the focal point or main subject of the image?\",\n    \"How do the different components of the image interact with each other?\",\n    \"What would be a fitting caption for this image?\",", "    \"How do the different components of the image interact with each other?\",\n    \"What would be a fitting caption for this image?\",\n    \"Can you create a concise description that captures the essence of the image?\",\n    \"How would you briefly summarize the content of this image in a phrase or sentence?\",\n    \"Please provide a catchy and relevant caption for this picture.\",\n    \"If you were to give this image a title, what would it be?\",\n    \"Describe the image in one creative sentence.\",\n    \"Please suggest a memorable phrase that encapsulates the image's content.\",\n    \"What engaging phrase would best represent this image?\",\n    \"Can you create an expressive caption that highlights the main theme of the image?\",", "    \"What engaging phrase would best represent this image?\",\n    \"Can you create an expressive caption that highlights the main theme of the image?\",\n    \"How would you sum up the image's story for a caption?\",\n    \"Provide an eye-catching caption that conveys the image's core message.\",\n    \"If you were to give this image a headline, what would it say?\",\n    \"Can you craft a captivating caption that communicates the essence of the image?\",\n    \"How would you describe the image's content in a powerful caption?\",\n    \"Please provide an inventive title to summarize the scene depicted in the image.\",\n    \"Compose a concise and striking phrase that reflects the image's key elements.\",\n    \"If you were to create a caption for this image, what would it be?\",", "    \"Compose a concise and striking phrase that reflects the image's key elements.\",\n    \"If you were to create a caption for this image, what would it be?\",\n    \"Offer a compelling caption that highlights the central focus of the image.\",\n    \"Can you produce a unique caption that encapsulates the image's overall mood?\",\n    \"Please generate an attention-grabbing caption that would best illustrate the events captured in this image\",\n    \"How would you express the image's main idea in an impactful sentence?\",\n    \"Please create a vivid and concise title that conveys the essence of the picture.\",\n    \"Compose an imaginative caption that reflects the image's most striking features.\",\n    \"What memorable statement would best represent the scene illustrated in this image?\",\n    \"Draft an evocative caption that brings the image to life for the reader.\",", "    \"What memorable statement would best represent the scene illustrated in this image?\",\n    \"Draft an evocative caption that brings the image to life for the reader.\",\n    \"Can you suggest an insightful caption that highlights the underlying message of the image?\",\n    \"What engaging phrase would effectively convey the action or subject matter depicted in this picture?\",\n    \"How would you encapsulate the image's core theme in a concise and expressive manner?\",\n    \"Please provide a creative and impactful title that captures the spirit of the image.\",\n    \"Craft a captivating caption that showcases the image's most prominent attributes.\",\n    \"What intriguing statement would best sum up the scene presented in this image?\",\n    \"Develop a descriptive caption that paints a vivid picture for the viewer.\",\n    \"Can you give a detailed account of the image's contents?\",", "    \"Develop a descriptive caption that paints a vivid picture for the viewer.\",\n    \"Can you give a detailed account of the image's contents?\",\n    \"What are the key elements and features visible in this image?\",\n    \"How would you narrate the events or actions depicted in the picture?\",\n    \"Please share your observations about the various components present in the image.\",\n    \"What is the overall theme or concept captured in this image? Can you describe it?\",\n]\n\n\nclass CcSbuAlignDataset(VQADataset):\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, add_eos=True, ignore_instruction=True):\n        self.tokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\"))[\"annotations\"])\n\n        self.vis_processor = vis_processor\n        self.prompter = VQAPrompter()\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def process_text(self, ann):\n        # random select a question\n        question = random.choice(QUESTIONS)\n        answer = ann[\"caption\"]\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image_id\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\nclass CcSbuAlignDataset(VQADataset):\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, add_eos=True, ignore_instruction=True):\n        self.tokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\"))[\"annotations\"])\n\n        self.vis_processor = vis_processor\n        self.prompter = VQAPrompter()\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def process_text(self, ann):\n        # random select a question\n        question = random.choice(QUESTIONS)\n        answer = ann[\"caption\"]\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image_id\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\ndef build_ccsbualign_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/cc_sbu_align/image/\",\n    ann_paths=[\"data/cc_sbu_align/filter_cap.json\"],\n    **kwargs,\n):\n    return CcSbuAlignDataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n    )", ""]}
{"filename": "mmgpt/datasets/__init__.py", "chunked_list": ["from .builder import build_dataset  # noqa: F401\nfrom .dial_dataset import DialDataset  # noqa: F401\nfrom .samplers import InfiniteSampler  # noqa: F401\nfrom .vqa_dataset import VQADataset  # noqa: F401\n"]}
{"filename": "mmgpt/datasets/vqa_dataset.py", "chunked_list": ["\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport copy\nimport json\nimport os", "import json\nimport os\nimport random\nfrom collections import defaultdict\nfrom typing import Iterable\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import ConcatDataset, Dataset", "from PIL import Image\nfrom torch.utils.data import ConcatDataset, Dataset\nfrom torch.utils.data.dataloader import default_collate\nfrom transformers import LlamaTokenizer\n\nTEMPLATE = {\n    \"description\": \"Template used by Alpaca-LoRA.\",\n    # \"prompt_choice\": \"Below is a multiple choice question about an image, along with answer options. Please choose the correct answer from these options.\\n\\n### Image:\\n{image}\\n\\n### Question:\\n{question}\\n\\n### Input:\\n{options}\\n\\n### Answer:\\n\",\n    # \"prompt_qa\": \"Below is a question about an image. Write a response to answer the question.\\n\\n### Image:\\n{image}\\n\\n### Question:\\n{question}\\n\\n### Answer:\\n\",\n    \"prompt_choice\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{options}\\n\\n### Response:\\n\",", "    # \"prompt_qa\": \"Below is a question about an image. Write a response to answer the question.\\n\\n### Image:\\n{image}\\n\\n### Question:\\n{question}\\n\\n### Answer:\\n\",\n    \"prompt_choice\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{options}\\n\\n### Response:\\n\",\n    \"prompt_qa\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Image:\\n{image}\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"response_split\": \"### Response:\",\n}\n\n\nclass VQAPrompter:\n    def __call__(self, question, options=None):\n        if options:\n            options = \", \".join(options)\n            res = TEMPLATE[\"prompt_choice\"].format(image=\"<image>\", question=question, options=options)\n        else:\n            res = TEMPLATE[\"prompt_qa\"].format(image=\"<image>\", question=question)\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(TEMPLATE[\"response_split\"])[-1].strip()", "\n\nclass VQADataset(Dataset):\n    def __init__(\n        self,\n        tokenizer,\n        vis_processor=None,\n        vis_root=None,\n        ann_paths=[],\n        add_eos=True,\n        ignore_instruction=True,\n        sample_image=False,\n    ):\n        \"\"\"\n        vis_root (string): Root directory of images (e.g. coco/images/)\n        ann_root (string): directory to store the annotation file\n        \"\"\"\n        assert tokenizer.add_eos_token is False, \"tokenizer should not add eos token by default\"\n        self.tokenizer: LlamaTokenizer = tokenizer\n        self.vis_root = vis_root\n\n        self.annotation = []\n        for ann_path in ann_paths:\n            self.annotation.extend(json.load(open(ann_path, \"r\")))\n\n        self.sample_image = sample_image\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n\n        self.vis_processor = vis_processor\n\n        self._add_instance_ids()\n        self.option_prob = 0.5\n        self.prompter = VQAPrompter()\n        self.add_eos = add_eos\n        self.ignore_instruction = ignore_instruction\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"image\"]].append(ann)\n        # image_name_list = list(image_list.keys())\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def _add_instance_ids(self, key=\"instance_id\"):\n        for idx, ann in enumerate(self.annotation):\n            ann[key] = str(idx)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"image\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n\n        answer_weight = {}\n        for answer in ann[\"answer\"]:\n            if answer in answer_weight.keys():\n                answer_weight[answer] += 1 / len(ann[\"answer\"])\n            else:\n                answer_weight[answer] = 1 / len(ann[\"answer\"])\n\n        answers = list(answer_weight.keys())\n        weights = list(answer_weight.values())\n\n        # create instruction\n        true_answer = answers[np.argmax(weights)]\n        is_option = random.random() < self.option_prob and len(answers) > 1\n        if is_option:\n            instruction = self.prompter(question, answers)\n        else:\n            instruction = self.prompter(question)\n\n        return dict(instruction=instruction, answer=true_answer)\n\n    def tokenize(self, text):\n        res = self.tokenizer(\n            text[\"instruction\"] + text[\"answer\"],\n            return_tensors=None,\n            padding=\"do_not_pad\",\n            truncation=True,\n            max_length=512,\n        )\n\n        # manually add eos token\n        if res[\"input_ids\"][-1] != self.tokenizer.eos_token_id and len(res[\"input_ids\"]) < 512 and self.add_eos:\n            res[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            res[\"attention_mask\"].append(1)\n        labels = copy.deepcopy(res[\"input_ids\"])\n        # ignore instruction_token\n        if self.ignore_instruction:\n            instruction_token = self.tokenizer(\n                text[\"instruction\"], return_tensors=None, padding=\"do_not_pad\", truncation=True, max_length=512\n            )\n            labels = [-100] * len(instruction_token[\"input_ids\"]) + labels[len(instruction_token[\"input_ids\"]) :]\n\n        res.update(labels=labels)\n        return res\n\n    def __getitem__(self, index):\n        ann = self.annotation[index]\n        image = self.process_image(ann)\n        text = self.process_text(ann)\n        res = self.tokenize(text)\n        res.update(image=image)\n        res.update(text)\n        return res\n\n    def collater(self, samples):\n        image_list, question_list, answer_list, input_id_list, attention_mask_list, labels_list = [], [], [], [], [], []\n\n        for sample in samples:\n            image_list.append(sample[\"image\"])\n            question_list.append(sample[\"instruction\"])\n            answer_list.append(sample[\"answer\"])\n            input_id_list.append(sample[\"input_ids\"])\n            attention_mask_list.append(sample[\"attention_mask\"])\n            labels_list.append(sample[\"labels\"])\n\n        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n        # same length to return tensors.\n        max_label_length = max(len(l) for l in labels_list)\n        padding_side = self.tokenizer.padding_side\n        padded_labels = []\n        for l in labels_list:\n            remainder = [-100] * (max_label_length - len(l))\n            if isinstance(l, list):\n                l = l + remainder if padding_side == \"right\" else remainder + l\n            elif padding_side == \"right\":\n                l = np.concatenate([l, remainder]).astype(np.int64)\n            else:\n                l = np.concatenate([remainder, l]).astype(np.int64)\n            padded_labels.append(l)\n\n        padded_samples = self.tokenizer.pad(\n            {\"input_ids\": input_id_list, \"attention_mask\": attention_mask_list, \"labels\": padded_labels},\n            return_tensors=\"pt\",\n            padding=\"longest\",\n        )\n\n        labels = padded_samples[\"labels\"]\n        media_token_id = self.tokenizer(\"<image>\", add_special_tokens=False)[\"input_ids\"][-1]\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        labels[:, 0] = -100\n        labels[labels == media_token_id] = -100\n        return {\n            \"image\": torch.stack(image_list, dim=0),\n            \"input_ids\": padded_samples[\"input_ids\"],\n            \"attention_mask\": padded_samples[\"attention_mask\"],\n            \"labels\": labels,\n            \"instruction\": question_list,\n            \"answer\": answer_list,\n        }", "\n\nclass ConcatDataset(ConcatDataset):\n    def __init__(self, datasets: Iterable[Dataset]) -> None:\n        super().__init__(datasets)\n\n    def collater(self, samples):\n        # TODO For now only supports datasets with same underlying collater implementations\n\n        all_keys = set()\n        for s in samples:\n            all_keys.update(s)\n\n        shared_keys = all_keys\n        for s in samples:\n            shared_keys = shared_keys & set(s.keys())\n\n        samples_shared_keys = []\n        for s in samples:\n            samples_shared_keys.append({k: s[k] for k in s.keys() if k in shared_keys})\n\n        return self.datasets[0].collater(samples_shared_keys)", ""]}
{"filename": "mmgpt/datasets/nlvr_dataset.py", "chunked_list": ["import copy\nimport json\nimport os\nimport random\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\n", "from PIL import Image\n\nfrom .vqa_dataset import VQADataset\n\nQUESTIONS = [\n    \"Is this true?\",\n    \"Is this right?\",\n    \"Can you confirm this information?\" \"Do you agree with this statement?\",\n    \"Does this align with your understanding?\",\n    \"How do you interpret this information?\",", "    \"Does this align with your understanding?\",\n    \"How do you interpret this information?\",\n    \"Does this align with your understanding?\",\n    \"Can you confirm this?\",\n    \"Is this statement correct?\",\n    \"Could you verify this information?\",\n    \"Do you agree with this?\",\n    \"Is this accurate?\",\n    \"Can you validate this claim?\",\n    \"Are these details valid?\",", "    \"Can you validate this claim?\",\n    \"Are these details valid?\",\n    \"Is this factually correct?\",\n    \"Is the following information correct?\",\n    \"Could you please verify this fact?\",\n    \"Do you agree with this assertion?\",\n    \"Are these details accurate?\",\n    \"Does this claim hold true?\",\n]\n", "]\n\n\nclass NLVRv1Dataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset.\"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            if \"train.json\" in ann_path:\n                split = \"train\"\n            elif \"dev.json\" in ann_path:\n                split = \"dev\"\n            elif \"test.json\" in ann_path:\n                split = \"test\"\n            else:\n                raise ValueError(f\"Unknown split for {ann_path}\")\n\n            with open(ann_path, \"r\") as f:\n                for line in f.readlines():\n                    line = line.strip()\n                    if len(line) != 0:\n                        ann = json.loads(line)\n                        ann[\"split\"] = split\n                        annotation.append(ann)\n\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            img_key = f\"{ann['split']}-{ann['identifier']}\"\n            image_list[img_key].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"sentence\"] + \" \" + random.choice(QUESTIONS)\n        true_answer = ann[\"label\"]\n\n        if random.random() < self.option_prob:\n            instruction = self.prompter(question, [\"true\", \"false\"])\n        else:\n            instruction = self.prompter(question)\n\n        return dict(instruction=instruction, answer=true_answer)\n\n    def process_image(self, ann):\n        # each question have 6 images, we can random select one of them.\n        # TODO: check whether using all 6 images?\n        random_id = random.randint(0, 5)\n        image_name = f\"{ann['split']}-{ann['identifier']}-{random_id}.png\"\n        image_path = os.path.join(self.vis_root, ann[\"split\"], \"images\", ann[\"directory\"], image_name)\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\nclass NLVRv2Dataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset.\"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths, **kwargs)\n        self.flip_prob = 0.5\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"images\"][0]].append(ann)\n        # image_name_list = list(image_list.keys())\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"sentence\"] + \" \" + random.choice(QUESTIONS)\n        true_answer = ann[\"label\"]\n\n        if random.random() < self.option_prob:\n            instruction = self.prompter(question, [\"true\", \"false\"])\n        else:\n            instruction = self.prompter(question)\n\n        return dict(instruction=instruction, answer=true_answer)\n\n    def process_image(self, ann):\n        image_0_path = os.path.join(self.vis_root, ann[\"images\"][0])\n        image_1_path = os.path.join(self.vis_root, ann[\"images\"][1])\n\n        image_0 = Image.open(image_0_path).convert(\"RGB\")\n        image_1 = Image.open(image_1_path).convert(\"RGB\")\n        image_0 = self.vis_processor(image_0)\n        image_1 = self.vis_processor(image_1)\n        return image_0, image_1\n\n    @staticmethod\n    def _flip(samples):\n        sentence = samples[\"sentence\"]\n        image0, image1 = samples[\"image0\"], samples[\"image1\"]\n\n        if \"left\" not in sentence and \"right\" not in sentence:\n            if random.random() < 0.5:\n                image0, image1 = image1, image0\n        else:\n            if random.random() < 0.5:\n                sentence = sentence.replace(\"left\", \"[TEMP_TOKEN]\")\n                sentence = sentence.replace(\"right\", \"left\")\n                sentence = sentence.replace(\"[TEMP_TOKEN]\", \"right\")\n\n                image0, image1 = image1, image0\n\n        samples[\"sentence\"] = sentence\n        samples[\"image0\"] = image0\n        samples[\"image1\"] = image1\n\n        return samples\n\n    def __getitem__(self, index):\n        ann = copy.deepcopy(self.annotation[index])\n        image_0, image_1 = self.process_image(ann)\n        if random.random() < self.flip_prob:\n            samples = self._flip({\"sentence\": ann[\"sentence\"], \"image0\": image_0, \"image1\": image_1})\n            image_0, image_1 = samples[\"image0\"], samples[\"image1\"]\n            ann[\"sentence\"] = samples[\"sentence\"]\n        # concat\n        # TODO: https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip_models/blip_nlvr.py\n        # model logic need update if using nlvr2\n        image = torch.cat([image_0, image_1], dim=2)\n        image = F.interpolate(image[None, ...], size=(image_0.shape[1], image_0.shape[2]))[0]\n        text = self.process_text(ann)\n        res = self.tokenize(text)\n        res.update(image=image)\n        res.update(text)\n        return res", "\n\ndef build_nlvrv1_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/nlvr\",\n    ann_paths=[\"data/nlvr//train/train.json\"],\n    sample_image=False,\n):\n    return NLVRv1Dataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n        sample_image=sample_image,\n    )", "\n\ndef build_nlvrv2_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/nlvr2\",\n    ann_paths=[\"data/nlvr2/annotations/nlvr_train.json\"],\n    sample_image=False,\n):\n    return NLVRv2Dataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n        sample_image=sample_image,\n    )", ""]}
{"filename": "mmgpt/datasets/ocr_vqa_dataset.py", "chunked_list": ["import os\nimport random\n\nfrom PIL import Image\n\nfrom .vqa_dataset import VQADataset\n\n\nclass OCRVQADataset(VQADataset):\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"filename\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        index = random.choice(list(range(len(ann[\"questions\"]))))\n        question = ann[\"questions\"][index]\n        answer = ann[\"answers\"][index]\n\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)", "class OCRVQADataset(VQADataset):\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"filename\"])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image\n\n    def process_text(self, ann):\n        index = random.choice(list(range(len(ann[\"questions\"]))))\n        question = ann[\"questions\"][index]\n        answer = ann[\"answers\"][index]\n\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)", ""]}
{"filename": "mmgpt/datasets/gqa_dataset.py", "chunked_list": ["import json\nimport os\nimport random\nfrom collections import defaultdict\n\nfrom PIL import Image\n\nfrom .vqa_dataset import VQADataset\n\n\nclass GQADataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset.\"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n        self.answer_prob = 1.0\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            ann = json.load(open(ann_path, \"r\"))\n            for k, v in ann.items():\n                v[\"question_id\"] = k\n                annotation.append(v)\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"imageId\"]].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n\n        answer = ann[\"answer\"]\n        full_answer = ann[\"fullAnswer\"]\n\n        # TODO: check which one is better\n        # Random select answer or full_answer\n        if random.random() < self.answer_prob:\n            select_answer = full_answer\n        else:\n            select_answer = answer\n\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=select_answer)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"imageId\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\nclass GQADataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset.\"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n        self.answer_prob = 1.0\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            ann = json.load(open(ann_path, \"r\"))\n            for k, v in ann.items():\n                v[\"question_id\"] = k\n                annotation.append(v)\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"imageId\"]].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"question\"]\n\n        answer = ann[\"answer\"]\n        full_answer = ann[\"fullAnswer\"]\n\n        # TODO: check which one is better\n        # Random select answer or full_answer\n        if random.random() < self.answer_prob:\n            select_answer = full_answer\n        else:\n            select_answer = answer\n\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=select_answer)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"imageId\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n\n        image = self.vis_processor(image)\n        return image", "\n\ndef build_gqa_dataset(\n    tokenizer,\n    vis_processor,\n    vis_root=\"data/gqa/images\",\n    ann_paths=[\n        \"data/gqa/questions/train_all_questions/train_all_questions_0.json\",\n        \"data/gqa/questions/val_all_questions.json\",\n    ],\n    sample_image=False,\n):\n    return GQADataset(\n        tokenizer=tokenizer,\n        vis_processor=vis_processor,\n        vis_root=vis_root,\n        ann_paths=ann_paths,\n        sample_image=sample_image,\n    )", ""]}
{"filename": "mmgpt/datasets/builder.py", "chunked_list": ["import numpy as np\nimport torch\n\nfrom .alpaca_gpt4_dataset import AlpacaGPT4Dataset  # noqa: F401\nfrom .aokvqa_dataset import AOKVQADataset  # noqa: F401\nfrom .cc_sbu_align_dataset import CcSbuAlignDataset  # noqa: F401\nfrom .clevr_dataset import CLEVRDataset  # noqa: F401\nfrom .coco_caption_dataset import COCOCaptionDataset  # noqa: F401\nfrom .dial_dataset import DialDataset  # noqa: F401\nfrom .dolly_dataset import DollyDataset  # noqa: F401", "from .dial_dataset import DialDataset  # noqa: F401\nfrom .dolly_dataset import DollyDataset  # noqa: F401\nfrom .gqa_dataset import GQADataset  # noqa: F401\nfrom .llava_dataset import LlavaDataset  # noqa: F401\nfrom .nlvr_dataset import NLVRv1Dataset, NLVRv2Dataset  # noqa: F401\nfrom .ocr_vqa_dataset import OCRVQADataset  # noqa: F401\nfrom .snli_ve_datasets import SNLIVEDataset  # noqa: F401\nfrom .text_ocr_dataset import TextOCRDataset  # noqa: F401\nfrom .vqa_dataset import ConcatDataset, VQADataset  # noqa: F401\nfrom .baize_dataset import BaiZeDataset  # noqa: F401", "from .vqa_dataset import ConcatDataset, VQADataset  # noqa: F401\nfrom .baize_dataset import BaiZeDataset  # noqa: F401\n\n\ndef build_dataset(dataset_config, **kwargs):\n    if isinstance(dataset_config, list):\n        datasets = [build_dataset(cfg, **kwargs) for cfg in dataset_config]\n        return ConcatDataset(datasets)\n    dataset_type = dataset_config.pop(\"type\")\n    sample = dataset_config.pop(\"sample\", -1)\n    if dataset_type == \"llava\":\n        dataset = LlavaDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"vqa\":\n        dataset = VQADataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"minigpt4\":\n        dataset = CcSbuAlignDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"llava_dial\":\n        dataset = DialDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"coco_dial\":\n        dataset = DialDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"aokvqa\":\n        dataset = AOKVQADataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"okvqa\":\n        dataset = VQADataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"text_ocr\":\n        dataset = TextOCRDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"ocr_vqa\":\n        dataset = OCRVQADataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"coco_caption\":\n        dataset = COCOCaptionDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"gqa\":\n        dataset = GQADataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"clevr\":\n        dataset = CLEVRDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"nlvrv1\":\n        dataset = NLVRv1Dataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"nlvrv2\":\n        dataset = NLVRv2Dataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"snlive\":\n        dataset = SNLIVEDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"dolly\":\n        dataset = DollyDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"alpaca_gpt4\":\n        dataset = AlpacaGPT4Dataset(\n            **dataset_config,\n            **kwargs,\n        )\n    elif dataset_type == \"baize\":\n        dataset = BaiZeDataset(\n            **dataset_config,\n            **kwargs,\n        )\n    else:\n        raise NotImplementedError\n\n    if sample > 0:\n        random_indices = np.random.choice(len(dataset), min(sample, len(dataset)), replace=False)\n        subsample_dataset = torch.utils.data.Subset(dataset, random_indices)\n        subsample_dataset.collater = dataset.collater\n        return subsample_dataset\n    else:\n        return dataset", ""]}
{"filename": "mmgpt/datasets/baize_dataset.py", "chunked_list": ["import json\n\nfrom mmgpt.datasets.dolly_dataset import DollyDataset\n\n\nTEMPLATE = {\n    \"description\": \"Template used by Alpaca-LoRA.\",\n    \"prompt_choice\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{options}\\n\\n### Response:\\n\",\n    \"prompt_qa\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"prompt_dial\": \"\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",", "    \"prompt_qa\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"prompt_dial\": \"\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\",\n    \"response_split\": \"### Response:\",\n}\n\nclass LangDialPrompter:\n    def __call__(self, question, options=None):\n        if options:\n            options = \", \".join(options)\n            res = TEMPLATE[\"prompt_choice\"].format(image=\"<image>\", question=question, options=options)\n        else:\n            res = TEMPLATE[\"prompt_dial\"].format(question=question)\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(TEMPLATE[\"response_split\"])[-1].strip()", "\nclass BaiZeDataset(DollyDataset):\n    \"\"\"\n    ```json\n    [\n        {\n            \"instruction\": \"Identify the odd one out.\",\n            \"input\": \"Twitter, Instagram, Telegram\",\n            \"output\": \"The odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.\"\n        },\n    ]\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(BaiZeDataset, self).__init__(*args, **kwargs)\n        self.prompter = LangDialPrompter()\n\n    def load_annotation(self, ann_path):\n        self.annotation = json.load(open(ann_path, \"r\"))\n\n    def process_text(self, anns):\n        # TODO remove this\n        begin_string = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n        convs = anns['input'].split(\"[|Human|] \")\n        conv_list = []\n        for conv_id, one_conv in enumerate(convs[1:-1]):\n            question, answer = one_conv.split(\"[|AI|] \")\n            question = question.replace(\"\\n\", \"\")\n            answer = answer.replace(\"\\n\", \"\")\n            instruction = self.prompter(question)\n            if conv_id == 0:\n                single_conv = dict(instruction=begin_string + instruction, answer=answer)\n            else:\n                single_conv = dict(instruction=instruction, answer=answer)\n            conv_list.append(single_conv)\n        return conv_list\n    \n    def __getitem__(self, index):\n        ann = self.annotation[index]\n        text_list = self.process_text(ann)\n        res_list = []\n        for text in text_list:\n            single_res = self.tokenize(text)\n            single_res[\"instruction\"] = text[\"instruction\"]\n            single_res[\"answer\"] = text[\"answer\"]\n            res_list.append(single_res)\n\n        input_ids = []\n        attention_mask = []\n        labels = []\n        instruction = []\n        answer = []\n        for res in res_list:\n            input_ids.extend(res[\"input_ids\"])\n            attention_mask.extend(res[\"attention_mask\"])\n            labels.extend(res[\"labels\"])\n            instruction.append(res[\"instruction\"])\n            answer.append(res[\"answer\"])\n\n        res = dict(\n            input_ids=input_ids, attention_mask=attention_mask, labels=labels, instruction=instruction, answer=answer\n        )\n        return res", ""]}
{"filename": "mmgpt/datasets/snli_ve_datasets.py", "chunked_list": ["import json\nimport os\nimport random\nfrom collections import defaultdict\n\nfrom PIL import Image\n\nfrom .vqa_dataset import VQADataset\n\nQUESTIONS = [", "\nQUESTIONS = [\n    \"What do you think of the above sentence?\",\n    \"Can you confirm this statement?\",\n    \"How do you interpret the given information?\",\n    \"What is your opinion on this matter?\",\n    \"Could you provide your perspective on this statement?\",\n    \"How would you respond to the provided claim?\",\n    \"What are your thoughts regarding the mentioned subject?\",\n    \"Can you elaborate on this idea in English?\",", "    \"What are your thoughts regarding the mentioned subject?\",\n    \"Can you elaborate on this idea in English?\",\n    \"Do you have any insights or feedback on this topic?\",\n    \"What's your take on the given statement?\",\n    \"What is your perspective on the given statement?\",\n    \"How would you interpret this remark?\",\n    \"Could you please provide your opinion on this?\",\n    \"Can you share your understanding of the above point?\",\n    \"Would you mind elaborating on this topic?\",\n    \"What are your views about the given statement?\",", "    \"Would you mind elaborating on this topic?\",\n    \"What are your views about the given statement?\",\n    \"How do you feel about the presented information?\",\n    \"Could you provide your perspective on this?\",\n    \"What is your opinion regarding this statement?\",\n    \"Can you share your thoughts about the mentioned claim?\",\n    \"How would you interpret the above comment?\",\n    \"Would you mind sharing your insights on this issue?\",\n]\n", "]\n\n\nclass SNLIVEDataset(VQADataset):\n    \"\"\"Visual Reasoning Dataset.\"\"\"\n\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths=[], **kwargs)\n\n        self.annotation = self.load_annotations(ann_paths)\n        if self.sample_image:\n            print(\"randomly sample one annotation for each image\")\n            self.annotation = self.parse_annotation(self.annotation)\n        self._add_instance_ids()\n\n    @staticmethod\n    def load_annotations(ann_paths):\n        annotation = []\n        for ann_path in ann_paths:\n            with open(ann_path, \"r\") as f:\n                for line in f.readlines():\n                    line = line.strip()\n                    if len(line) != 0:\n                        ann = json.loads(line)\n                        annotation.append(ann)\n        return annotation\n\n    def parse_annotation(self, annotation):\n        image_list = defaultdict(list)\n        for ann in annotation:\n            image_list[ann[\"Flickr30K_ID\"]].append(ann)\n        annotation = []\n        for ann_list in image_list.values():\n            annotation.append(random.choice(ann_list))\n        return annotation\n\n    def process_text(self, ann):\n        question = ann[\"sentence2\"] + \" \" + random.choice(QUESTIONS)\n        answer = ann[\"gold_label\"]\n        if random.random() < self.option_prob:\n            instruction = self.prompter(question, [\"entailment\", \"neutral\", \"contradiction\"])\n        else:\n            instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)\n\n    def process_image(self, ann):\n        image_path = os.path.join(self.vis_root, ann[\"Flickr30K_ID\"] + \".jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.vis_processor(image)\n        return image", ""]}
{"filename": "mmgpt/datasets/llava_dataset.py", "chunked_list": ["from .vqa_dataset import VQADataset\n\n\nclass LlavaDataset(VQADataset):\n    def __init__(self, tokenizer, vis_processor, vis_root, ann_paths, **kwargs):\n        super().__init__(tokenizer, vis_processor, vis_root, ann_paths, **kwargs)\n\n    def _add_instance_ids(self, key=\"id\"):\n        for idx, ann in enumerate(self.annotation):\n            ann[key] = str(idx)\n\n    def process_text(self, ann):\n        question = ann[\"conversations\"][0][\"value\"]\n        # remove '<image>' tag and '\\n'\n        question = question.replace(\"<image>\", \"\").replace(\"\\n\", \"\")\n        answer = ann[\"conversations\"][1][\"value\"]\n        instruction = self.prompter(question)\n        return dict(instruction=instruction, answer=answer)", ""]}
{"filename": "mmgpt/datasets/samplers/__init__.py", "chunked_list": ["from .infinite_sampler import InfiniteSampler\n"]}
{"filename": "mmgpt/datasets/samplers/infinite_sampler.py", "chunked_list": ["import itertools\n\nimport torch\nfrom torch.utils.data.sampler import Sampler\n\nfrom mmgpt.train.distributed import world_info_from_env\n\n\nclass InfiniteSampler(Sampler):\n    def __init__(self, dataset: int, shuffle: bool = True, seed: int = 0):\n        self._size = len(dataset)\n        self._shuffle = shuffle\n        self._seed = int(seed)\n        _, rank, world_size = world_info_from_env()\n\n        self._rank = rank\n        self._world_size = world_size\n\n    def __iter__(self):\n        start = self._rank\n        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)\n\n    def _infinite_indices(self):\n        g = torch.Generator()\n        g.manual_seed(self._seed)\n        while True:\n            if self._shuffle:\n                yield from torch.randperm(self._size, generator=g).tolist()\n            else:\n                yield from torch.arange(self._size).tolist()", "class InfiniteSampler(Sampler):\n    def __init__(self, dataset: int, shuffle: bool = True, seed: int = 0):\n        self._size = len(dataset)\n        self._shuffle = shuffle\n        self._seed = int(seed)\n        _, rank, world_size = world_info_from_env()\n\n        self._rank = rank\n        self._world_size = world_size\n\n    def __iter__(self):\n        start = self._rank\n        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)\n\n    def _infinite_indices(self):\n        g = torch.Generator()\n        g.manual_seed(self._seed)\n        while True:\n            if self._shuffle:\n                yield from torch.randperm(self._size, generator=g).tolist()\n            else:\n                yield from torch.arange(self._size).tolist()", ""]}
{"filename": "configs/dataset_config.py", "chunked_list": ["visual_datasets = [\n    dict(\n        type=\"llava\",\n        vis_root=\"data/coco/train2017\",\n        ann_paths=[\n            \"data/llava/detail_23k.json\",\n            \"data/llava/complex_reasoning_77k.json\",\n        ],\n    ),\n    dict(", "    ),\n    dict(\n        type=\"llava_dial\",\n        vis_root=\"data/coco/train2017\",\n        ann_paths=[\n            \"data/llava/conversation_58k.json\",\n        ],\n    ),\n    dict(\n        type=\"aokvqa\",", "    dict(\n        type=\"aokvqa\",\n        vis_root=\"data/coco/images\",\n        ann_paths=[\n            \"data/aokvqa/annotations/aokvqa_v1p0_train.json\",\n        ],\n        sample=5000,\n    ),\n    dict(\n        type=\"minigpt4\",", "    dict(\n        type=\"minigpt4\",\n        vis_root=\"data/cc_sbu_align/image\",\n        ann_paths=[\n            \"data/cc_sbu_align/filter_cap.json\",\n        ],\n    ),\n    dict(\n        type=\"coco_caption\",\n        vis_root=\"data/coco\",", "        type=\"coco_caption\",\n        vis_root=\"data/coco\",\n        ann_paths=[\n            \"data/coco/annotations/coco_karpathy_train_converted.json\",\n            \"data/coco/annotations/coco_karpathy_val.json\",\n        ],\n        sample=512,\n    ),\n    dict(\n        type=\"ocr_vqa\",", "    dict(\n        type=\"ocr_vqa\",\n        vis_root=\"data/OCR_VQA/image\",\n        ann_paths=[\n            \"data/OCR_VQA/downloaded_dataset.json\",\n        ],\n        sample=512,\n    ),\n]\n", "]\n\nlanguage_datasets = [\n    dict(\n        type=\"dolly\",\n        ann_path=\"data/dolly/databricks-dolly-15k.jsonl\",\n    ),\n    dict(\n        type=\"alpaca_gpt4\",\n        ann_path=\"data/alpaca_gpt4/alpaca_gpt4_data.json\",", "        type=\"alpaca_gpt4\",\n        ann_path=\"data/alpaca_gpt4/alpaca_gpt4_data.json\",\n    ),\n    dict(\n        type=\"baize\",\n        ann_path=\"data/baize/quora_chat_data.json\",\n    ),\n]\n", ""]}
{"filename": "configs/lora_config.py", "chunked_list": ["tuning_config = dict(\n    lora=True,\n    lora_target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_kv\", \"to_out\", \"ff.1\", \"ff.3\"],\n    lora_r=16,\n    lora_alpha=16,\n    lora_dropout=0.0,\n    vis=True,\n    unfrozen=[],\n)\n", ")\n"]}
