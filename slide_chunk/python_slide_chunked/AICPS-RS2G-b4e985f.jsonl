{"filename": "__init__.py", "chunked_list": [""]}
{"filename": "scene_graph/scene_graph.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\n\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport networkx as nx\nimport sys, os\nfrom pathlib import Path", "import sys, os\nfrom pathlib import Path\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom scene_graph.nodes import Node\nfrom networkx.drawing import nx_pydot\nimport pandas as pd\nimport torch\nimport math\nfrom collections import defaultdict\n", "from collections import defaultdict\n\n'''Create scenegraph using raw Carla json frame data or raw image data'''\nclass SceneGraph:\n    \n    #graph can be initialized with a framedict containing raw Carla data to load all objects at once\n    def __init__(self, relation_extractor, framedict= None, framenum=None, bounding_boxes = None, bev = None, coco_class_names=None, platform='carla'):\n        #configure relation extraction settings\n        self.relation_extractor = relation_extractor\n        \n        self.platform = platform\n        \n        if self.platform == \"carla\":\n            self.g = nx.MultiDiGraph() #initialize scenegraph as networkx graph\n            self.road_node = Node(\"Root Road\", {\"name\":\"Root Road\"}, \"road\", self.relation_extractor.actors.index(\"road\"))\n            self.add_node(self.road_node)   #adding the road as the root node\n            self.parse_json(framedict) # processing json framedict\n        elif self.platform == \"image\":\n\n            self.g = nx.MultiDiGraph()  # initialize scenegraph as networkx graph\n            # road and lane settings.\n            self.road_node = Node(\"Root Road\", {\"name\":\"Root Road\"}, \"road\", self.relation_extractor.actors.index(\"road\"))\n            self.add_node(self.road_node)   # adding the road as the root node\n    \n            # set ego location to middle-bottom of image.\n            # set ego location to middle-bottom of image.\n            self.ego_location = bev.get_projected_point(\n                                    bev.params['width']/2, \n                                    bev.params['height'])\n            \n            self.ego_location = bev.apply_depth_estimation(\n                                    self.ego_location[0], \n                                    self.ego_location[1])\n            \n            #import pdb; pdb.set_trace()\n            self.egoNode = Node('ego car', {\n\n                                       'location_x': self.ego_location[0], \n                                       'location_y': self.ego_location[1]}, \n                                       'ego_car', self.relation_extractor.actors.index(\"ego_car\"))\n    \n            # add ego-vehicle to graph\n            self.add_node(self.egoNode)\n            \n            # add middle, right, and left lanes to graph\n            self.relation_extractor.extract_relative_lanes(self) \n    \n            # convert bounding boxes to nodes and build relations.\n            boxes, labels, image_size = bounding_boxes\n            self.get_nodes_from_bboxes(bev, boxes, labels, coco_class_names)\n            self.relation_extractor.extract_semantic_relations(self)\n\n\n    def get_nodes_from_bboxes(self, bev, boxes, labels, coco_class_names):\n        for idx, (box, label) in enumerate(zip(boxes, labels)):\n            box = box.cpu().numpy().tolist()\n            class_name = coco_class_names[label]\n            #import pdb; pdb.set_trace()\n            attr = {'left': box[0], 'top': box[1], 'right': box[2], 'bottom': box[3]}\n            \n            # exclude vehicle dashboard\n            if attr['top'] >= bev.params['height'] - 100: continue;\n            \n\n            # filter traffic participants\n            actor_type = \"\"\n            for actor_ in range(len(self.relation_extractor.actors)):\n                if class_name == self.relation_extractor.actors[actor_]:\n                    actor_type = self.relation_extractor.actors[actor_]\n                    actor_value = actor_\n                elif f\"{self.relation_extractor.actors[actor_].upper()}_NAMES\" in self.relation_extractor.conf.relation_extraction_settings:\n                    if class_name in self.relation_extractor.conf.relation_extraction_settings[f\"{self.relation_extractor.actors[actor_].upper()}_NAMES\"]: #ie specific car name\n                        actor_type = self.relation_extractor.actors[actor_]\n                        actor_value = actor_\n            if actor_type == \"\": #if actor's type not included in ACTOR_NAMES\n                continue\n\n            # map center-bottom of bounding box to warped image\n            x_mid = (attr['right'] + attr['left']) / 2\n            y_bottom = attr['bottom']\n            x_bev, y_bev = bev.get_projected_point(x_mid, y_bottom)\n\n            # approximate locations / distances in feet\n            attr['location_x'], attr['location_y'] = bev.apply_depth_estimation(x_bev, y_bev)\n            \n\n            # due to bev warp, vehicles far from horizon get warped behind car, thus we will default them as far from vehcile\n            if attr['location_y'] > self.egoNode.attr['location_y']:\n                # should store this in a list dictating the filename of the scene\n                print('BEV warped to behind vehicle')\n                attr['location_y'] = self.egoNode.attr['location_y'] - self.relation_extractor.proximity_rels[-1][1] #assuming the last proximity threshold will be the most vague\n\n            attr['rel_location_x'] = attr['location_x'] - self.egoNode.attr['location_x']           # x position relative to ego (neg left, pos right)\n            attr['rel_location_y'] = attr['location_y'] - self.egoNode.attr['location_y']           # y position relative to ego (neg vehicle ahead of ego)\n            attr['distance_abs'] = math.sqrt(attr['rel_location_x']**2 + attr['rel_location_y']**2) # absolute distance from ego\n            #import pdb; pdb.set_trace()\n            node = Node('%s_%d' % (actor_type, idx), attr, actor_type, actor_value)\n            \n            # add vehicle to graph\n            self.add_node(node) #change\n\n            # add lane vehicle relations to graph\n            self.relation_extractor.add_mapping_to_relative_lanes(self, node)\n\n\n    def add_node(self, node):\n        '''Add a single node to graph. node can be any hashable datatype including objects'''\n        color = 'white'\n        if 'ego' in node.name.lower():\n            color = 'red'\n        elif 'car' in node.name.lower():\n            color = 'green'\n        elif 'lane' in node.name.lower():\n            color = 'yellow'\n        self.g.add_node(node, attr=node.attr, label=node.name, style='filled', fillcolor=color)\n\n\n# add all pair-wise relations between two nodes\n    def add_relations(self, relations_list):\n        #import pdb; pdb.set_trace()\n        for relation in relations_list:\n            self.add_relation(relation)\n    \n\n    # add a single pair-wise relation between two nodes\n    def add_relation(self, relation):\n        if relation != []:\n            node1, edge, node2 = relation\n            if node1 in self.g.nodes and node2 in self.g.nodes:\n                self.g.add_edge(node1, node2, value=self.relation_extractor.rels.index(edge), label=edge, color=self.relation_extractor.relational_colors[edge]) #relations might need to be turned into objects not just remain strings\n                \n            else:\n                raise NameError(\"One or both nodes in relation do not exist in graph. Relation: \" + str(relation))\n            \n\n    #parses actor dict and adds nodes to graph. this can be used for all actor types.\n    def add_actor_dict(self, key, actordict): #TODO test with signs and different actor types besides cars\n        for actor_id, attr in actordict.items():\n            # filter actors behind ego \n            n = Node(None, None, attr['name'], None)   #using the actor key as the node name and the dict as its attributes.\n            n.label, n.value = self.relation_extractor.get_actor_type(n)   \n            n.attr = attr\n            x1, y1 = math.cos(math.radians(self.egoNode.attr['rotation'][0])), math.sin(math.radians(self.egoNode.attr['rotation'][0]))\n            x2, y2 = attr['location'][0] - self.egoNode.attr['location'][0], attr['location'][1] - self.egoNode.attr['location'][1]\n            inner_product = x1*x2 + y1*y2\n            length_product = math.sqrt(x1**2+y1**2) + math.sqrt(x2**2+y2**2)\n            degree = math.degrees(math.acos(inner_product / length_product))\n            \n            if key == \"sign\":\n              import pdb; pdb.set_trace()\n            \n            if (degree <=190 or degree >= 350):#TEST FOR CARLA #if degree <= 80 or (degree >=280 and degree <= 360):\n                # if abs(self.egoNode.attr['lane_idx'] - attr['lane_idx']) <= 1 \\\n                # or (\"invading_lane\" in self.egoNode.attr and (2*self.egoNode.attr['invading_lane'] - self.egoNode.attr['orig_lane_idx']) == attr['lane_idx']):\n                n.name = n.label.lower() + \"_\" + actor_id\n                \n                self.add_node(n)\n                self.relation_extractor.add_mapping_to_relative_lanes(self, n)\n            \n\n    #add the contents of a whole framedict to the graph\n    def parse_json(self, framedict):\n        \n#        self.egoNode = Node(\"ego:\"+framedict['ego']['name'], framedict['ego'], 'CAR')    \n        self.egoNode = Node('ego car', framedict['ego'], 'ego_car', self.relation_extractor.actors.index(\"ego_car\"))\n        self.add_node(self.egoNode) #change\n\n        #rotating axes to align with ego. yaw axis is the primary rotation axis in vehicles\n        self.ego_yaw = math.radians(self.egoNode.attr['rotation'][0])\n        self.ego_cos_term = math.cos(self.ego_yaw)\n        self.ego_sin_term = math.sin(self.ego_yaw)\n        self.relation_extractor.extract_relative_lanes(self)\n\n#         self.relation_extractor = RelationExtractor(self.egoNode) #see line 99\n        for key, attrs in framedict.items():   \n            if key == 'actors' or key == 'sign':\n              self.add_actor_dict(key, attrs)\n        self.relation_extractor.extract_semantic_relations(self)\n        \n\n    def visualize(self, filename=None):\n        #import pdb;pdb.set_trace()\n        A = nx_pydot.to_pydot(self.g)\n        A.write_png(filename)\n\n    \n#==========================================================================================\n# this is for creation of trainer input using carla data\n#==========================================================================================\n    \n    def get_carla_node_embeddings(self, feature_list):\n        rows = []\n        labels=[]\n        ego_attrs = None\n        \n        #extract ego attrs for creating relative features\n        for node, data in self.g.nodes.items():\n            if \"ego\" in str(node):\n                ego_attrs = data['attr']\n        if ego_attrs == None:\n            raise NameError(\"Ego not found in scenegraph\")\n    \n        #rotating axes to align with ego. yaw axis is the primary rotation axis in vehicles\n        ego_yaw = math.radians(ego_attrs['rotation'][0])\n        cos_term = math.cos(ego_yaw)\n        sin_term = math.sin(ego_yaw)\n    \n        def rotate_coords(x, y): \n            new_x = (x*cos_term) + (y*sin_term)\n            new_y = ((-x)*sin_term) + (y*cos_term)\n            return new_x, new_y\n            \n        def get_carla_embedding(node, row):\n            row['type_'+str(node.value)] = 1 #assign 1hot class label\n            return row\n        \n        for idx, node in enumerate(self.g.nodes):\n            d = defaultdict()\n            row = get_carla_embedding(node, d)\n            labels.append(node.value)\n            rows.append(row)\n            \n        embedding = pd.DataFrame(data=rows, columns=feature_list)\n        embedding = embedding.fillna(value=0) #fill in NaN with zeros\n        embedding = torch.FloatTensor(embedding.values)\n        \n        return embedding\n    \n    \n    def get_carla_edge_embeddings(self, node_name2idx):\n        edge_index = []\n        edge_attr = []\n        for src, dst, edge in self.g.edges(data=True):\n            edge_index.append((node_name2idx[src], node_name2idx[dst]))\n            edge_attr.append(edge['value'])\n    \n        edge_index = torch.transpose(torch.LongTensor(edge_index), 0, 1)\n        edge_attr  = torch.LongTensor(edge_attr)\n        \n        return edge_index, edge_attr\n    \n    #===================================================================\n    \n    # this is for creation of trainer input using image data \n    #===================================================================\n    \n    def get_real_image_node_embeddings(self, feature_list):\n        rows = []\n        labels = []\n        ego_attrs = None\n\n        # extract ego attrs for creating relative features\n        for node, data in self.g.nodes.items():\n            if \"ego\" in str(node).lower():\n                ego_attrs = data['attr']\n\n        if ego_attrs == None:\n            raise NameError(\"Ego not found in scenegraph\")\n\n        def get_real_embedding(node, row):\n            # for key in self.feature_list:\n            #     if key in node.attr:\n            #         row[key] = node.attr[key]\n            row['type_'+str(node.value)] = 1  # assign 1hot class label\n            return row\n\n        for idx, node in enumerate(self.g.nodes):\n            d = defaultdict()\n            row = get_real_embedding(node, d)\n            \n            labels.append(node.value)\n            rows.append(row)\n\n        embedding = pd.DataFrame(data=rows, columns=feature_list)\n        embedding = embedding.fillna(value=0)  # fill in NaN with zeros\n        embedding = torch.FloatTensor(embedding.values)\n        #import pdb; pdb.set_trace()\n        return embedding\n\n    def get_real_image_edge_embeddings(self, node_name2idx):\n      edge_index = []\n      edge_attr = []\n      for src, dst, edge in self.g.edges(data=True):\n          #import pdb; pdb.set_trace()\n          edge_index.append((node_name2idx[src], node_name2idx[dst]))\n          edge_attr.append(edge['value'])\n  \n      edge_index = torch.transpose(torch.LongTensor(edge_index), 0, 1)\n      edge_attr = torch.LongTensor(edge_attr)\n  \n      return edge_index, edge_attr", "    \n    #==================================================================\n    \n    \n"]}
{"filename": "scene_graph/__init__.py", "chunked_list": [""]}
{"filename": "scene_graph/nodes.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nclass Node:\n    def __init__(self, name, attr, label=None, value = None):\n        self.name = name  # Car-1, Car-2.\n        self.attr = attr  # bounding box info\n        self.label = label  # ActorType (ie \"car\")\n        self.value = value # ActorType index in the config's ACTOR_NAMES list \n \n    def __repr__(self):\n        return \"%s\" % self.name", "    \n"]}
{"filename": "scene_graph/relation_extractor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport itertools\nimport math\nimport sys\nimport os\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom scene_graph.nodes import Node\n", "from scene_graph.nodes import Node\n\n\n#This class extracts relations for every pair of entities in a scene\nclass RelationExtractor:\n    def __init__(self, config):\n        self.conf = config\n        self.actors = config.relation_extraction_settings[\"ACTOR_NAMES\"]\n        self.rels = config.relation_extraction_settings[\"RELATION_NAMES\"]\n        self.wanted_directional_relation_dict = {(i[0],i[1]):i[2] for i in config.relation_extraction_settings[\"directional_relation_list\"]}\n        self.wanted_proximity_relation_dict = {(i[0],i[1]):i[2] for i in config.relation_extraction_settings[\"proximity_relation_list\"]}\n        self.proximity_rels = self.conf.relation_extraction_settings[\"PROXIMITY_THRESHOLDS\"]\n        self.directional_rels = config.relation_extraction_settings[\"DIRECTIONAL_THRESHOLDS\"]\n        self.relational_colors = {i[0]:i[1] for i in config.relation_extraction_settings[\"RELATION_COLORS\"]}\n        self.LANE_THRESHOLD = self.conf.relation_extraction_settings['LANE_THRESHOLD'] # feet. if object's center is more than this distance away from ego's center, build left or right lane relation\n#         feet. if object's center is within this distance of ego's center, build middle lane relation\n        #self.CENTER_LANE_THRESHOLD = self.conf.relation_extraction_settings['CENTER_LANE_THRESHOLD']\n\n    def get_actor_type(self, actor):\n        for actor_ in range(len(self.actors)):\n            if actor.label == self.actors[actor_]:\n                return self.actors[actor_], actor_ #return the actor type along with its index in the ACTOR_NAMES list\n            elif actor.label.lower() == self.actors[actor_]:\n                return self.actors[actor_], actor_\n            elif f\"{self.actors[actor_].upper()}_NAMES\" in self.conf.relation_extraction_settings:\n              for actor_names in self.conf.relation_extraction_settings[f\"{self.actors[actor_].upper()}_NAMES\"]: #go through different names of actor type (ie Tesla for type car)\n                  if actor_names in actor.label:\n                      return self.actors[actor_], actor_\n                  elif actor_names in actor.label.lower():\n                      return self.actors[actor_], actor_\n        raise NameError(\"Actor name not found for actor with name: \" + actor.attr[\"name\"])\n\n    def get_config(self):\n        return self.conf\n            \n    #takes in two entities and extracts all relations between those two entities. extracted relations are bidirectional    \n    def extract_relations(self, actor1, actor2):\n        type1 ,_ = self.get_actor_type(actor1)\n        type2 ,_= self.get_actor_type(actor2)\n        relations_list = []\n        if (type1,type2) in self.wanted_proximity_relation_dict.keys():\n            relations_list += self.extract_distance_relations_actor1_actor2(actor1, actor2, type1, type2) #always pass in order that they are defined in the list\n        if (type1,type2) in self.wanted_directional_relation_dict.keys():\n            relations_list += self.extract_directional_relation_actor1_actor2(actor1, actor2, type1, type2) #always pass in order that they are defined in the list\n        return relations_list\n        \n\n    def extract_relative_lanes(self, scene_graph): #keep as you will always need to add lanes\n        if self.conf.dataset_type == \"carla\":\n            scene_graph.left_lane = Node(\"lane_left\", {\"curr\":\"lane_left\"}, \"lane\",  self.actors.index(\"lane\")) #change actor.lane to just lane \n            scene_graph.right_lane = Node(\"lane_right\", {\"curr\":\"lane_right\"}, \"lane\",  self.actors.index(\"lane\"))\n            scene_graph.middle_lane = Node(\"lane_middle\", {\"curr\":\"lane_middle\"}, \"lane\",  self.actors.index(\"lane\"))\n        elif self.conf.dataset_type == \"image\":\n            scene_graph.left_lane = Node('Left Lane', {}, \"lane\",  self.actors.index(\"lane\"))\n            scene_graph.right_lane = Node('Right Lane', {}, \"lane\",  self.actors.index(\"lane\"))\n            scene_graph.middle_lane = Node('Middle Lane', {}, \"lane\",  self.actors.index(\"lane\"))\n        scene_graph.add_node(scene_graph.left_lane)\n        scene_graph.add_node(scene_graph.right_lane)\n        scene_graph.add_node(scene_graph.middle_lane)\n        #if \"isIn\" in self.directional_rels:\n        scene_graph.add_relation([scene_graph.left_lane, \"isIn\", scene_graph.road_node]) #if we assume lanes and roads must be in graph, then just check to see if isIn in the wanted relations?\n        scene_graph.add_relation([scene_graph.right_lane, \"isIn\", scene_graph.road_node])\n        scene_graph.add_relation([scene_graph.middle_lane, \"isIn\", scene_graph.road_node])\n        scene_graph.add_relation([scene_graph.egoNode, \"isIn\", scene_graph.middle_lane])    \n#         else:\n#             raise ValueError(\"isIn relation absent from config\")\n\n    def add_mapping_to_relative_lanes(self, scene_graph, object_node): #leave this in if we can assume that there will always be lanes\n        if self.conf.dataset_type == \"carla\":\n            _, ego_y = self.rotate_coords(scene_graph, scene_graph.egoNode.attr['location'][0], scene_graph.egoNode.attr['location'][1]) #NOTE: X corresponds to forward/back displacement and Y corresponds to left/right displacement\n            _, new_y = self.rotate_coords(scene_graph, object_node.attr['location'][0], object_node.attr['location'][1])\n            y_diff = new_y - ego_y\n            if y_diff < -self.LANE_THRESHOLD:\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.left_lane])\n            elif y_diff >  self.LANE_THRESHOLD:\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.right_lane])\n            elif y_diff <= self.LANE_THRESHOLD and y_diff >= -self.LANE_THRESHOLD: #check\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.middle_lane])\n#            elif abs(y_diff) <= self.CENTER_LANE_THRESHOLD:\n#                scene_graph.add_relation([object_node, \"isIn\", scene_graph.middle_lane])\n        elif self.conf.dataset_type == \"image\": \n            if object_node.attr['rel_location_x'] < -self.LANE_THRESHOLD:\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.left_lane]) \n            elif object_node.attr['rel_location_x'] > self.LANE_THRESHOLD:\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.right_lane])\n#            elif abs(object_node.attr['rel_location_x']) <= self.CENTER_LANE_THRESHOLD:\n#                scene_graph.add_relation([object_node, \"isIn\", scene_graph.middle_lane])\n            elif object_node.attr['rel_location_x'] <= self.LANE_THRESHOLD and object_node.attr['rel_location_x'] >= -self.LANE_THRESHOLD:\n                scene_graph.add_relation([object_node, \"isIn\", scene_graph.middle_lane])\n\n    def extract_semantic_relations(self, scene_graph):\n        for node1, node2 in itertools.combinations(scene_graph.g.nodes, 2):\n            if node1.name != node2.name and (node1.name != \"Root Road\" and node2.name != \"Root Road\"): #dont build self-relations\n                scene_graph.add_relations(self.extract_relations(node1, node2))\n                \n\n    #copied from get_node_embeddings(). rotates coordinates to be relative to ego vector.\n    def rotate_coords(self, scene_graph, x, y): \n        new_x = (x*scene_graph.ego_cos_term) + (y*scene_graph.ego_sin_term)\n        new_y = ((-x)*scene_graph.ego_sin_term) + (y*scene_graph.ego_cos_term)\n        return new_x, new_y\n\n#~~~~~~~~~specific relations for each pair of actors possible~~~~~~~~~~~~\n#actor 1 corresponds to the first actor in the function name and actor2 the second\n\n    def extract_distance_relations_actor1_actor2(self, actor1, actor2, type1, type2):\n        relation_list = []\n        if self.euclidean_distance(actor1, actor2) <= self.wanted_proximity_relation_dict[(type1,type2)]:\n            relation_list += self.create_proximity_relations(actor1, actor2)\n            relation_list += self.create_proximity_relations(actor2, actor1)\n            return relation_list\n        return relation_list\n\n\n    def extract_directional_relation_actor1_actor2(self, actor1, actor2, type1, type2):\n        relation_list = []\n        if self.euclidean_distance(actor1, actor2) <= self.wanted_directional_relation_dict[(type1,type2)]:\n            # One of these relations get overwritten in the visualizer for some reason...\n            relation_list += self.extract_directional_relation(actor1, actor2)\n            relation_list += self.extract_directional_relation(actor2, actor1)\n            return relation_list\n        return relation_list\n    \n#~~~~~~~~~~~~~~~~~~UTILITY FUNCTIONS~~~~~~~~~~~~~~~~~~~~~~\n    #return euclidean distance between actors\n    def euclidean_distance(self, actor1, actor2):\n        if self.conf.dataset_type == \"carla\":\n            l1 = actor1.attr['location']\n            l2 = actor2.attr['location']\n            distance = math.sqrt((l1[0] - l2[0])**2 + (l1[1]- l2[1])**2 + (l1[2] - l2[2])**2)\n        elif self.conf.dataset_type == \"image\":\n            l1 = (actor1.attr['location_x'], actor1.attr['location_y'])\n            l2 = (actor2.attr['location_x'], actor2.attr['location_y'])\n            distance = math.sqrt((l1[0] - l2[0])**2 + (l1[1] - l2[1])**2)\n            # print(actor1, actor2, distance)\n        return distance\n        \n    #check if an actor is in a certain lane\n    def in_lane(self, actor1, actor2):\n        if 'lane_idx' in actor1.attr.keys():\n            # calculate the distance bewteen actor1 and actor2\n            # if it is below 3.5 then they have is in relation.\n                # if actor1 is ego: if actor2 is not equal to the ego_lane's index then it's invading relation.\n            if actor1.attr['lane_idx'] == actor2.attr['lane_idx']:\n                return True\n            if \"invading_lane\" in actor1.attr:\n                if actor1.attr['invading_lane'] == actor2.attr['lane_idx']:\n                    return True\n                if \"orig_lane_idx\" in actor1.attr:\n                    if actor1.attr['orig_lane_idx'] == actor2.attr['lane_idx']:\n                        return True\n        else:\n            return False\n    \n    def create_proximity_relations(self, actor1, actor2): #how\n        for relation in self.proximity_rels:\n            if self.euclidean_distance(actor1, actor2) <= relation[1]:\n                return [[actor1,relation[0], actor2]]\n        return []\n\n    def extract_directional_relation(self, actor1, actor2):\n        relation_list = []\n        if self.conf.dataset_type == \"carla\":\n            # gives directional relations between actors based on their 2D absolute positions.      \n            x1, y1 = math.cos(math.radians(actor1.attr['rotation'][0])), math.sin(math.radians(actor1.attr['rotation'][0]))\n            x2, y2 = actor2.attr['location'][0] - actor1.attr['location'][0], actor2.attr['location'][1] - actor1.attr['location'][1]\n            x2, y2 = x2 / math.sqrt(x2**2+y2**2), y2 / math.sqrt(x2**2+y2**2)\n            \n            degree =  math.degrees(math.atan2(y2, x2)) - \\\n                 math.degrees(math.atan2(y1, x1))\n        \n        elif self.conf.dataset_type == \"image\":\n            x1 = math.cos(math.radians(0)) \n            y1 = math.sin(math.radians(0))\n            x2 = actor2.attr['location_x'] - actor1.attr['location_x']\n            y2 = actor2.attr['location_y'] - actor1.attr['location_y']\n            x2 /= math.sqrt(x2**2 + y2**2)\n            y2 /= math.sqrt(x2**2 + y2**2)\n      \n            degree = math.degrees(math.atan2(y1, x1)) - \\\n                 math.degrees(math.atan2(y2, x2))\n        \n        \n        if degree < 0: \n            degree += 360\n        \n        degree %= 360\n         \n             \n        for direction_rel in self.directional_rels:\n            list_of_ranges = direction_rel[1]\n            for ranges in list_of_ranges:\n                if degree >= ranges[0] and degree <= ranges[1]:\n                    relation_list.append([actor2, direction_rel[0], actor1])           \n    \n        \n        if self.conf.dataset_type == \"carla\":\n            if actor2.attr['lane_idx'] < actor1.attr['lane_idx']: # actor2 to the left of actor1 \n                relation_list.append([actor2, \"toLeftOf\", actor1])\n            elif actor2.attr['lane_idx'] > actor1.attr['lane_idx']: # actor2 to the right of actor1 \n                relation_list.append([actor2, \"toRightOf\", actor1])\n               \n        elif self.conf.dataset_type == \"image\":  \n#            if abs(actor2.attr['location_x'] - actor1.attr['location_x']) <= self.CENTER_LANE_THRESHOLD:\n#                pass\n            if (actor2.attr['location_x'] - actor1.attr['location_x']) <= self.LANE_THRESHOLD and (actor2.attr['location_x'] - actor1.attr['location_x']) >= -self.LANE_THRESHOLD: #if in the same lane, don't want left or right relations to be built\n                pass\n            # actor2 to the left of actor1\n            elif actor2.attr['location_x'] < actor1.attr['location_x']:\n                relation_list.append([actor2, \"toLeftOf\", actor1])\n            # actor2 to the right of actor1\n            elif actor2.attr['location_x'] > actor1.attr['location_x']:\n                relation_list.append([actor2, \"toRightOf\", actor1])\n            # disable rear relations help the inference.\n             \n        return relation_list", "\n\n"]}
{"filename": "scene_graph/extraction/image_extractor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport pdb\nimport sys\nfrom pathlib import Path\n\nimport cv2\n", "import cv2\n\nsys.path.append(str(Path(\"../../\")))\nfrom os.path import isfile, join\nimport roadscene2vec.data.dataset as ds\nfrom roadscene2vec.scene_graph.extraction.extractor import Extractor as ex\nfrom roadscene2vec.scene_graph.scene_graph import SceneGraph\n\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.data import MetadataCatalog", "from detectron2.engine import DefaultPredictor\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.utils import visualizer \nfrom detectron2.config import get_cfg\nfrom detectron2 import model_zoo\nfrom roadscene2vec.scene_graph.extraction.bev import bev\nfrom tqdm import tqdm\n\n'''RealExtractor initializes relational settings and creates an ImageSceneGraphSequenceGenerator object to extract scene graphs using raw image data.'''\nclass RealExtractor(ex):\n    def __init__(self, config):\n        super(RealExtractor, self).__init__(config) \n\n        self.input_path = self.conf.location_data['input_path']\n        self.dataset = ds.SceneGraphDataset(self.conf)\n\n        if not os.path.exists(self.input_path):\n            raise FileNotFoundError(self.input_path)\n\n        # detectron setup\n        model_path = 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'\n        self.cfg = get_cfg()\n        self.cfg.merge_from_file(model_zoo.get_config_file(model_path))\n        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_path)\n        self.coco_class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get('thing_classes')\n        self.predictor = DefaultPredictor(self.cfg)\n\n        # bev setup\n        if config.custom_bev == True:\n            self.custom_bev = True\n        else:\n            self.custom_bev = False\n            self.bev = bev.BEV(config.image_settings['BEV_PATH'], mode='deploy')\n\n\n    '''Load scenegraphs using raw image frame tensors'''\n    def load(self): #seq_tensors[seq][frame/jpgname] = frame tensor\n        try:\n            all_sequence_dirs = [x for x in Path(self.input_path).iterdir() if x.is_dir()]\n            dir_order = None\n            try:\n                all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: int(x.stem.split('_')[0]))  \n            except:\n                print('failed to sort by sequence number, sorting by folder name instead.')\n                all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: x.stem)\n                dir_order = list(range(len(all_sequence_dirs)))\n            self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n            for path in tqdm(all_sequence_dirs):\n                if dir_order is None:\n                    seq = int(path.stem.split('_')[0])\n                else:\n                    seq = dir_order.pop(0)\n                label_path = (path/\"label.txt\").resolve()\n                ignore_path = (path/\"ignore.txt\").resolve()\n                if ignore_path.exists(): #record ignored sequences, and only load the sequences that were not ignored\n                    with open(str(path/\"ignore.txt\"), 'r') as label_f:\n                        ignore_label = int(label_f.read())\n                        if ignore_label:\n                            self.dataset.ignore.append(seq)\n                            continue #skip to next seq if ignore path exists\n                seq_images = self.load_images(path)\n                bev_path = (path/'bev.json').resolve()\n                if self.custom_bev: #assign custom bev per seq. if enabled\n                    assert bev_path.exists(), \"Custom BEV path does not exist. Please check your config file.\"\n                    self.bev = bev.BEV(bev_path, mode='deploy')\n                self.dataset.scene_graphs[seq] = {}\n                for frame, img in seq_images.items():\n                    out_img_path = None\n                    bounding_boxes = self.get_bounding_boxes(img_tensor=img, out_img_path=out_img_path)\n                    \n                    scenegraph = SceneGraph(self.relation_extractor,    \n                                                bounding_boxes = bounding_boxes, \n                                                bev = self.bev,\n                                                coco_class_names=self.coco_class_names, \n                                                platform=self.dataset_type)\n\n                    self.dataset.scene_graphs[seq][frame] = scenegraph\n                self.dataset.action_types[seq] = \"lanechange\" #path.stem.split('_')[2] #TODO: this is broken for carla. also, we cannot assume that our users will have their data in this format.\n                if label_path.exists():\n                    with open(str(path/'label.txt'), 'r') as label_file:\n                        lines = label_file.readlines()\n                        l0 = 1.0 if float(lines[0].strip().split(\",\")[0]) >= 0 else 0.0 \n                        self.dataset.labels[seq] = l0\n\n        except Exception as e:\n            import traceback\n            print('We have problem creating the real image scenegraphs')\n            print(e)\n            traceback.print_exc()\n            pdb.set_trace()\n            \n    #returns a numpy array representation of a sequence of images in format (H,W,C)\n    def load_images(self, path):\n        raw_images_loc = (path/'raw_images').resolve()\n        images = sorted([Path(f) for f in os.listdir(raw_images_loc) if isfile(join(raw_images_loc, f)) and \".DS_Store\" not in f and \"Thumbs\" not in f], key = lambda x: int(x.stem.split(\".\")[0]))\n        images = [join(raw_images_loc,i) for i in images] \n        sequence_tensor = {}\n        modulo = 0\n        acc_number = 0\n        if(self.framenum != None):\n            modulo = int(len(images) / self.framenum)  #subsample to frame limit\n        if(self.framenum == None or modulo == 0):\n            modulo = 1\n        for i in range(0, len(images)):\n            if (i % modulo == 0 and self.framenum == None) or (i % modulo == 0 and acc_number < self.framenum):\n                image_path = images[i]\n                frame_num = int(Path(image_path).stem)\n                im = cv2.imread(str(image_path), cv2.IMREAD_COLOR) \n                sequence_tensor[frame_num] = im \n                acc_number += 1\n        return sequence_tensor\n        \n    def get_bounding_box_annotated_image(self, im):\n        v = visualizer.Visualizer(im[:, :, ::-1], \n            MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), \n            scale=1.2)\n        out = v.draw_instance_predictions(self.predictor(im)['instances'].to('cpu'))\n        return out.get_image()[:, :, ::-1]\n            \n    def get_bounding_boxes(self, img_tensor, out_img_path=None):\n        im = img_tensor\n        outputs = self.predictor(im)\n        if out_img_path:\n            # We can use `Visualizer` to draw the predictions on the image.\n            out = self.get_bounding_box_annotated_image(im)\n            cv2.imwrite(out_img_path, out)\n\n        # todo: after done scp to server\n        # crop im to remove ego car's hood\n        # find threshold then remove from pred_boxes, pred_classes, check image_size\n        bounding_boxes = outputs['instances'].pred_boxes, outputs['instances'].pred_classes, outputs['instances'].image_size\n        return bounding_boxes\n\n    \n    '''Returns SceneGraphDataset object containing scengraphs, labels, and action types'''\n    def getDataSet(self):\n        try:\n            return self.dataset\n        except Exception as e:\n            import traceback\n            print('We have problem creating scenegraph dataset object from the extracted real image scenegraphs')\n            print(e)\n            traceback.print_exc()", "'''RealExtractor initializes relational settings and creates an ImageSceneGraphSequenceGenerator object to extract scene graphs using raw image data.'''\nclass RealExtractor(ex):\n    def __init__(self, config):\n        super(RealExtractor, self).__init__(config) \n\n        self.input_path = self.conf.location_data['input_path']\n        self.dataset = ds.SceneGraphDataset(self.conf)\n\n        if not os.path.exists(self.input_path):\n            raise FileNotFoundError(self.input_path)\n\n        # detectron setup\n        model_path = 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'\n        self.cfg = get_cfg()\n        self.cfg.merge_from_file(model_zoo.get_config_file(model_path))\n        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_path)\n        self.coco_class_names = MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]).get('thing_classes')\n        self.predictor = DefaultPredictor(self.cfg)\n\n        # bev setup\n        if config.custom_bev == True:\n            self.custom_bev = True\n        else:\n            self.custom_bev = False\n            self.bev = bev.BEV(config.image_settings['BEV_PATH'], mode='deploy')\n\n\n    '''Load scenegraphs using raw image frame tensors'''\n    def load(self): #seq_tensors[seq][frame/jpgname] = frame tensor\n        try:\n            all_sequence_dirs = [x for x in Path(self.input_path).iterdir() if x.is_dir()]\n            dir_order = None\n            try:\n                all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: int(x.stem.split('_')[0]))  \n            except:\n                print('failed to sort by sequence number, sorting by folder name instead.')\n                all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: x.stem)\n                dir_order = list(range(len(all_sequence_dirs)))\n            self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n            for path in tqdm(all_sequence_dirs):\n                if dir_order is None:\n                    seq = int(path.stem.split('_')[0])\n                else:\n                    seq = dir_order.pop(0)\n                label_path = (path/\"label.txt\").resolve()\n                ignore_path = (path/\"ignore.txt\").resolve()\n                if ignore_path.exists(): #record ignored sequences, and only load the sequences that were not ignored\n                    with open(str(path/\"ignore.txt\"), 'r') as label_f:\n                        ignore_label = int(label_f.read())\n                        if ignore_label:\n                            self.dataset.ignore.append(seq)\n                            continue #skip to next seq if ignore path exists\n                seq_images = self.load_images(path)\n                bev_path = (path/'bev.json').resolve()\n                if self.custom_bev: #assign custom bev per seq. if enabled\n                    assert bev_path.exists(), \"Custom BEV path does not exist. Please check your config file.\"\n                    self.bev = bev.BEV(bev_path, mode='deploy')\n                self.dataset.scene_graphs[seq] = {}\n                for frame, img in seq_images.items():\n                    out_img_path = None\n                    bounding_boxes = self.get_bounding_boxes(img_tensor=img, out_img_path=out_img_path)\n                    \n                    scenegraph = SceneGraph(self.relation_extractor,    \n                                                bounding_boxes = bounding_boxes, \n                                                bev = self.bev,\n                                                coco_class_names=self.coco_class_names, \n                                                platform=self.dataset_type)\n\n                    self.dataset.scene_graphs[seq][frame] = scenegraph\n                self.dataset.action_types[seq] = \"lanechange\" #path.stem.split('_')[2] #TODO: this is broken for carla. also, we cannot assume that our users will have their data in this format.\n                if label_path.exists():\n                    with open(str(path/'label.txt'), 'r') as label_file:\n                        lines = label_file.readlines()\n                        l0 = 1.0 if float(lines[0].strip().split(\",\")[0]) >= 0 else 0.0 \n                        self.dataset.labels[seq] = l0\n\n        except Exception as e:\n            import traceback\n            print('We have problem creating the real image scenegraphs')\n            print(e)\n            traceback.print_exc()\n            pdb.set_trace()\n            \n    #returns a numpy array representation of a sequence of images in format (H,W,C)\n    def load_images(self, path):\n        raw_images_loc = (path/'raw_images').resolve()\n        images = sorted([Path(f) for f in os.listdir(raw_images_loc) if isfile(join(raw_images_loc, f)) and \".DS_Store\" not in f and \"Thumbs\" not in f], key = lambda x: int(x.stem.split(\".\")[0]))\n        images = [join(raw_images_loc,i) for i in images] \n        sequence_tensor = {}\n        modulo = 0\n        acc_number = 0\n        if(self.framenum != None):\n            modulo = int(len(images) / self.framenum)  #subsample to frame limit\n        if(self.framenum == None or modulo == 0):\n            modulo = 1\n        for i in range(0, len(images)):\n            if (i % modulo == 0 and self.framenum == None) or (i % modulo == 0 and acc_number < self.framenum):\n                image_path = images[i]\n                frame_num = int(Path(image_path).stem)\n                im = cv2.imread(str(image_path), cv2.IMREAD_COLOR) \n                sequence_tensor[frame_num] = im \n                acc_number += 1\n        return sequence_tensor\n        \n    def get_bounding_box_annotated_image(self, im):\n        v = visualizer.Visualizer(im[:, :, ::-1], \n            MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), \n            scale=1.2)\n        out = v.draw_instance_predictions(self.predictor(im)['instances'].to('cpu'))\n        return out.get_image()[:, :, ::-1]\n            \n    def get_bounding_boxes(self, img_tensor, out_img_path=None):\n        im = img_tensor\n        outputs = self.predictor(im)\n        if out_img_path:\n            # We can use `Visualizer` to draw the predictions on the image.\n            out = self.get_bounding_box_annotated_image(im)\n            cv2.imwrite(out_img_path, out)\n\n        # todo: after done scp to server\n        # crop im to remove ego car's hood\n        # find threshold then remove from pred_boxes, pred_classes, check image_size\n        bounding_boxes = outputs['instances'].pred_boxes, outputs['instances'].pred_classes, outputs['instances'].image_size\n        return bounding_boxes\n\n    \n    '''Returns SceneGraphDataset object containing scengraphs, labels, and action types'''\n    def getDataSet(self):\n        try:\n            return self.dataset\n        except Exception as e:\n            import traceback\n            print('We have problem creating scenegraph dataset object from the extracted real image scenegraphs')\n            print(e)\n            traceback.print_exc()", "    \n   \n    \n            \n"]}
{"filename": "scene_graph/extraction/carla_image_extractor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport pdb\nimport cv2\nfrom pathlib import Path\nfrom os.path import isfile, join\nimport sys\nsys.path.append(os.path.dirname(sys.path[0]))", "import sys\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom scene_graph.extraction.image_extractor import RealExtractor\nfrom scene_graph.scene_graph import SceneGraph\nfrom tqdm import tqdm\n\nclass CarlaRealExtractor(RealExtractor):\n    def __init__(self, config):\n        super(CarlaRealExtractor, self).__init__(config)\n        \n    def parse_img_info(self, path, seq):\n        raw_images_loc = (path/'sensor/vehicle').resolve()\n        self.dataset.scene_graphs[seq] = dict()\n        for vehicle_idx in sorted(os.listdir(raw_images_loc))[1:]:\n            direction_paths = dict()\n            for direction in self.conf.sensor_directions:\n                direction_paths[direction] = []\n                direction_path = (raw_images_loc/vehicle_idx/'perception/camera/Camera RGB'/direction).resolve()\n                image_paths = \\\n                sorted([(direction_path/f).resolve() for f in os.listdir(direction_path) if isfile(join(direction_path, f)) and \".DS_Store\" not in f and \"Thumbs\" not in f], \n                       key = lambda x: int(x.stem.split(\".\")[0]))\n                modulo = 0\n                acc_number = 0\n                if(self.framenum != None):\n                    modulo = int(len(image_paths) / self.framenum)  #subsample to frame limit\n                if(self.framenum == None or modulo == 0):\n                    modulo = 1\n                for i in range(0, len(image_paths)):\n                    if (i % modulo == 0 and self.framenum == None) or (i % modulo == 0 and acc_number < self.framenum):\n                        image_path = image_paths[i]\n                        im = cv2.imread(str(image_path), cv2.IMREAD_COLOR) \n                        out_img_path = None\n                        bounding_boxes = self.get_bounding_boxes(img_tensor=im, out_img_path=out_img_path)\n                        scenegraph = SceneGraph(self.relation_extractor,    \n                                                bounding_boxes = bounding_boxes, \n                                                bev = self.bev,      \n                                                coco_class_names=self.coco_class_names, \n                                                platform=self.dataset_type)\n                        direction_paths[direction].append(scenegraph)\n                        acc_number += 1\n                        \n            self.dataset.scene_graphs[seq][vehicle_idx] = direction_paths\n\n        \n    def parse_label(self, path, seq):\n        import random\n        self.dataset.labels[seq] = random.choice([1.0, 0.0])\n    \n    def load(self): #seq_tensors[seq][frame/jpgname] = frame tensor\n        try:\n            all_sequence_dirs = []\n            seq = 0\n            for scenario_path in Path(self.input_path).iterdir():\n                if scenario_path.is_dir():\n                    for seq_path in scenario_path.iterdir():\n                        all_sequence_dirs.append(seq_path)\n                        seq += 1\n                        self.dataset.action_types[seq] = scenario_path.stem.split('_')[0] \n                        scenario_path.stem.split('_')[0]\n                        print(self.dataset.action_types[seq])\n            self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n            for seq, path in enumerate(tqdm(all_sequence_dirs)):\n                self.parse_img_info(path, seq)\n                self.parse_label(path, seq)\n\n        except Exception as e:\n            import traceback\n            traceback.print_exc()\n            pdb.set_trace()\n            print('We have problem creating the real image scenegraphs')\n            print(e)", "                        "]}
{"filename": "scene_graph/extraction/__init__.py", "chunked_list": [""]}
{"filename": "scene_graph/extraction/extractor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys\nfrom abc import ABC\nfrom pathlib import Path\n\nsys.path.append(str(Path(\"../../\")))\nimport roadscene2vec.scene_graph.relation_extractor as r_e\n", "import roadscene2vec.scene_graph.relation_extractor as r_e\n\n\n'''\nThis class defines the abstract base class of scene-graph extractors. scene-graph extractors can extract data from many different formats to generate SceneGraphDatasets.\n'''\nclass Extractor(ABC):\n    def __init__(self, config):\n        self.conf = config\n        self.dataset_type = self.conf.dataset_type\n        self.scene_graphs = {}\n        self.relation_extractor = r_e.RelationExtractor(config)\n        self.framenum = self.conf.relation_extraction_settings[\"frames_limit\"]", "        "]}
{"filename": "scene_graph/extraction/carla_extractor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys, os\nfrom pathlib import Path\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom scene_graph.extraction.extractor import Extractor as ex\nfrom scene_graph.scene_graph import SceneGraph\nimport data.dataset as ds\nfrom tqdm import tqdm", "import data.dataset as ds\nfrom tqdm import tqdm\nimport ast\nfrom glob import glob\nimport json\n\n\"\"\"CarlaExtractor initializes relational settings and creates a CarlaSceneGraphSequenceGenerator object to extract scene graphs using raw scene data.\"\"\"\nclass CarlaExtractor(ex):\n    def __init__(self,config):\n        super(CarlaExtractor, self).__init__(config)\n        \n        self.input_path = self.conf.location_data['input_path']\n        self.dataset = ds.SceneGraphDataset(self.conf)\n\n        \n        \n    '''Load scenegraphs and store scenegraphs in the form {sequence{frame{scenegraph}} using the raw data given in the form {sequence{frame{raw_data}}'''\n    def load(self):\n        all_sequence_dirs = [x for x in Path(self.input_path).iterdir() if x.is_dir()]\n        all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: int(x.stem.split('_')[0]))  \n        self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n        sg_extracted = {}\n        for path in tqdm(all_sequence_dirs):\n              seq = int(path.stem.split('_')[0])\n              self.dataset.action_types[seq] = path.stem.split('_')[1]\n              label_path = (path/\"label.txt\").resolve()\n              metadata_path = (path/\"metadata.txt\").resolve()\n  \n              if label_path.exists():\n                  with open(str(path/'label.txt'), 'r') as label_file:\n                      lines = label_file.readlines()\n                      l0 = 1.0 if float(lines[0].strip().split(\",\")[0]) >= 0 else 0.0 \n                      self.dataset.labels[seq] = l0 \n  \n                \n              if not metadata_path.exists():\n                  raise FileNotFoundError((path/'metadata.txt').resolve())\n              else:\n                  with open(str(path/'metadata.txt'), 'r') as md_file:\n                      md = md_file.read()\n                      self.dataset.meta[seq] = ast.literal_eval(md)\n                    \n              txt_path = sorted(list(glob(\"%s/**/*.json\" % str(path/\"scene_raw\"), recursive=True)))[0]\n              with open(txt_path, 'r') as scene_dict_f:\n                  try:\n                      sg_extracted[seq] = {}\n                      framedict = json.loads(scene_dict_f.read()) \n                      image_frames = list(framedict.keys()) #this is the list of frame names\n                      image_frames = sorted(image_frames)\n                      #### filling the gap between lane change where some of ego node might miss the invading lane information. ####\n                      start_frame_number = 0; end_frame_number = 0; invading_lane_idx = None\n                      \n                      for idx, frame_number in enumerate(image_frames):\n                          if \"invading_lane\" in framedict[str(frame_number)]['ego']:\n                              start_frame_number = idx\n                              invading_lane_idx = framedict[str(frame_number)]['ego']['invading_lane']\n                              break\n  \n                      for frame_number in image_frames[::-1]:\n                          if \"invading_lane\" in framedict[str(frame_number)]['ego']:\n                              end_frame_number = image_frames.index(frame_number)\n                              break\n                  \n                      for idx in range(start_frame_number, end_frame_number):\n                          framedict[str(image_frames[idx])]['ego']['invading_lane'] = invading_lane_idx\n                      \n                      for frame, frame_dict in framedict.items():\n                          if str(frame) in image_frames: \n                              scenegraph = SceneGraph(self.relation_extractor, framedict = frame_dict, framenum = frame, platform = \"carla\")\n                              sg_extracted[seq][int(frame)] = scenegraph\n                      if self.framenum != None:\n                        sg_extracted[seq] = self.subsample(sg_extracted[seq])\n                  except Exception as e:\n                      import traceback\n                      print(\"We have problem creating the Carla scenegraphs\")\n                      print(e)\n                      traceback.print_exc()\n                  \n        self.dataset.scene_graphs = sg_extracted\n\n     \n    '''Returns SceneGraphDataset object containing scengraphs, labels, action types, and meta data'''\n    def getDataSet(self):\n        try:\n            return self.dataset\n        #can just pass in self.dataset.conf but opting not to do so for clarity\n        except Exception as e:\n            import traceback\n            print(\"We have problem creating scenegraph dataset object from the extracted Carla scenegraphs\")\n            print(e)\n            traceback.print_exc()\n    \n    \n    #remove this if breaks functionality or not needed\n    def subsample(self, scenegraphs): \n        '''\n            This function will subsample the original scenegraph sequence dataset (self.scenegraphs_sequence). \n            Before running this function, it includes a variant length of graph sequences. \n            We expect the length of graph sequences will be homogenenous after running this function.\n\n            The default value of number_of_frames will be 20; Could be a tunnable hyperparameters.\n        '''\n        number_of_frames=self.framenum\n        \n        sequence = {}\n        acc_number = 0\n        modulo = int(len(scenegraphs) / number_of_frames)\n        if modulo == 0:\n            modulo = 1\n\n        for idx, (timeframe, scenegraph) in enumerate(scenegraphs.items()):\n            if idx % modulo == 0 and acc_number < number_of_frames:\n                sequence[timeframe] = scenegraph\n                acc_number+=1\n        return sequence", ""]}
{"filename": "scene_graph/extraction/bev/__init__.py", "chunked_list": [""]}
{"filename": "scene_graph/extraction/bev/bev.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport json\nimport math\nfrom pathlib import Path\nimport pdb\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np", "import matplotlib.pyplot as plt\nimport numpy as np\n\n\nclass BEV():\n    def __init__(self, fname, mode='calibrate'):\n        if mode == 'deploy':\n            is_json = Path(fname).is_file() and '.json' in fname\n            assert is_json, 'ERROR! file {} does not exist or is not a json file...'.format(fname)\n            self.params = self.load_params(fname) # bev params\n            self.compute_homography_matrix()\n\n        if mode == 'calibrate':\n            is_img = Path(fname).is_file() and ('.jpg' in fname.name or '.png' in fname.name)\n            assert is_img, 'ERROR! file {} does not exist or is not an image...'.format(fname)\n            self.params = {'proj_ratio': 3} # bev params\n            self.fname = fname              # file\n            self.lane_length = 10\n            self.lane_width  = 12\n            \n            # clickable point(s)\n            self.point = None               \n            self.lane_points = [{}, {}, {}]\n\n    def read_img(self):\n        self.img = cv2.imread(self.fname, cv2.IMREAD_UNCHANGED)\n        height, width = self.img.shape[:-1]\n        self.params['height'] = height\n        self.params['width'] = width\n\n    def get_point(self):\n        return self.point\n\n    def set_point(self, event):\n        self.point = int(event.ydata)\n\n    def index_of_point(self, points, point):\n        try:\n            return points.index(point)\n        except ValueError:    \n            return None\n        \n    def is_none(self, item):\n        return item == None\n\n    def abs_diff(self, key, points):\n        return abs(points[0][key] - points[1][key])\n\n    def set_lane_points(self, event):\n        i = self.index_of_point(self.lane_points, {})\n        if not self.is_none(i):\n            self.lane_points[i] = {'xdata': event.xdata, 'ydata': event.ydata}\n            if self.is_none(self.index_of_point(self.lane_points, {})):\n                self.params['yscale'] = float('%.3f' % (self.lane_length / self.abs_diff('ydata', self.lane_points[:-1])))\n                self.params['xscale'] = float('%.3f' % (self.lane_width  / self.abs_diff('xdata', self.lane_points[1:])))\n\n                # save params after clicking on lane lines\n                self.save_params()\n\n    def onclick(self, event):\n        if not self.is_none(event.xdata) and not self.is_none(event.ydata):\n            if self.is_none(self.point):\n                self.set_point(event)\n            else:\n                self.set_lane_points(event)\n            self.display_bev()\n\n    def keydown(self, event):\n        if event.key == 'r':\n            self.reset_display()\n        if not self.is_none(self.point):\n            if event.key == 'e':\n                self.sharper_proj()\n            elif event.key == 'w':\n                self.softer_proj()\n            self.display_bev()\n\n    def sharper_proj(self):\n        self.params['proj_ratio'] += 2\n        \n    def softer_proj(self):\n        if self.params['proj_ratio'] > 3:\n            self.params['proj_ratio'] -= 2\n        else:\n            print('Cannot widen any further!')\n\n    def reset_display(self):\n        self.point = None\n        self.lane_points = [{} for _ in self.lane_points]\n        if 'xscale' in self.params:\n            del self.params['xscale'], self.params['yscale']\n        \n        plt.clf()\n        plt.imshow(cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB), animated=True)\n        self.show_instructions()\n        plt.draw()\n\n    def save_params(self, fname='bev.json'):\n        metadata = {}\n        for k in self.params.keys():\n            metadata[k] = self.params[k]\n\n        with open(fname, 'w') as f:\n            json.dump(metadata, f, indent=4)\n            print('- saved params to {}'.format(fname))\n\n    def load_params(self, fname):\n        with open(fname, 'r') as f:\n            return json.load(f)\n\n    def show_params(self):\n        parameters = ''\n        for k in self.params.keys():\n            parameters += '{}={}, '.format(k, self.params[k])\n        plt.figtext(0.5, 0.1, parameters[: -2], ha='center', weight='bold', bbox=dict(boxstyle='square', facecolor='gainsboro', alpha=0.5), wrap=True)\n\n    def apply_depth_estimation(self, x, y):\n        return (x * self.params['xscale'], y * self.params['yscale'])\n    \n    def get_projected_point(self, x, y):\n        # Replace line 140 image_scenegraph.py with call to this method\n        point = np.array([[[x, y + self.params['cropped_height']]]], dtype='float32')\n        return cv2.perspectiveTransform(point, self.M).squeeze()\n        \n    def compute_homography_matrix(self):\n        # Set ROI coords\n        padded_height      = self.params['height'] + self.params['cropped_height']\n        self.bottom_left   = [0, padded_height]\n        self.bottom_right  = [self.params['width'], padded_height]\n        self.top_left      = [0, self.params['height']]\n        self.top_right     = [self.params['width'], self.params['height']]\n        src = np.float32([self.bottom_left, self.bottom_right, self.top_left, self.top_right])\n        \n        # Projection domain\n        left_ratio  = math.floor(self.params['proj_ratio']/2) / self.params['proj_ratio']\n        right_ratio = math.ceil(self.params['proj_ratio']/2)  / self.params['proj_ratio']\n        bottom_left_ratio = int(self.params['width']*left_ratio)\n        bottom_right_ratio = int(self.params['width']*right_ratio)\n        \n        # Create v-shape projection\n        dst = np.float32([[bottom_left_ratio, padded_height], [bottom_right_ratio, padded_height], self.top_left, self.top_right]) \n        \n        # transformation matrices\n        self.M = cv2.getPerspectiveTransform(src, dst) \n        self.Minv = cv2.getPerspectiveTransform(dst, src) \n\n    def demo_points(self):\n        # Show sample points offset from horizon line\n        points = lambda x: x if type(x) == list else x.squeeze()\n        plot = lambda x, c, m : plt.plot(*points(x), color=c, marker=m, ms=7)\n        shift = lambda x: [x[0] + self.params['width'] - 1, x[1]]\n\n        bottom  = np.array([[[self.params['width']/2, self.params['horizon_height'] + 20]]], dtype='float32')\n        top     = np.array([[[self.params['width']/2, self.params['horizon_height'] - 20]]], dtype='float32')\n        bottom_ = shift(cv2.perspectiveTransform(bottom, self.M).squeeze())\n        top_    = shift(cv2.perspectiveTransform(top, self.M).squeeze())\n\n        plot(bottom, c='orange', m='*')\n        plot(top, c='green', m='*')\n        plot(bottom_, c='orange', m='*')\n        plot(top_, c='green', m='*')\n\n    def warpPerspective(self, img):\n        return cv2.warpPerspective(img, self.M, (self.params['width'], self.params['height'] + self.params['cropped_height']))\n    \n    def offset_image_height(self, img):\n        return cv2.copyMakeBorder(img, self.params['cropped_height'], 0, 0, 0, cv2.BORDER_CONSTANT)\n\n    def display_bev(self):\n        cropped_top = self.get_point()\n        # remove delete_me stuff\n        self.params['cropped_height'] = self.params['height'] - cropped_top\n        self.params['horizon_height'] = self.params['cropped_height'] + cropped_top\n        self.compute_homography_matrix()\n\n        # Apply np slicing for ROI crop (show on image)\n        img = self.offset_image_height(self.img.copy())\n        warped_img = self.warpPerspective(img) # Image warping\n        merge_imgs = np.hstack((img, warped_img))\n        plt.clf()\n        plt.imshow(cv2.cvtColor(merge_imgs, cv2.COLOR_BGR2RGB), animated=True) # Show results\n\n        # Show chosen horizon line\n        x = [0, 2*(self.params['width']) - 1]\n        y = [self.params['horizon_height'], self.params['horizon_height']]\n        plt.plot(x, y, color='red')\n\n        # Show user clicked points\n        for point in self.lane_points:\n            if 'xdata' in point:\n                # Projected image points\n                plt.plot(point['xdata'], point['ydata'], color='red', marker='o', ms=5)\n\n                # Original image points\n                x_inv, y_inv = cv2.perspectiveTransform(np.array([[[point['xdata'] - self.params['width'], point['ydata']]]]), self.Minv).squeeze()\n                plt.plot(x_inv, y_inv, color='red', marker='o', ms=5)\n            \n        # self.demo_points()\n    \n        self.show_instructions(bev=True)\n        self.show_params()\n        plt.draw()\n\n    def show_instructions(self, bev=False):\n        if not bev:\n            instructions = '''Find and click on the image's horizon line'''\n            plt.annotate(instructions, (self.params['width']/2, -self.params['height']/30), annotation_clip=False, ha='center', wrap=True)\n        else: \n            instructions = '''Original image'''\n            plt.annotate(instructions, (self.params['width']/2, -self.params['height']/30), annotation_clip=False, ha='center', wrap=True)\n\n            instructions = '''Projected image'''\n            plt.annotate(instructions, (self.params['width']*3/2, -self.params['height']/30), annotation_clip=False, ha='center', wrap=True)\n\n            instructions = '''Keypress [e] to elongate perspective, [w] to widen persepctive, [r] to reset image'''\n            plt.figtext(0.5, 0.01, instructions, ha='center', weight='bold', bbox=dict(boxstyle='square', facecolor='gainsboro', alpha=0.5), wrap=True)\n\n    def calibrate(self):\n        self.read_img()\n        fig = plt.figure()\n        plt.imshow(cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB), animated=True)\n        set_point = fig.canvas.mpl_connect('button_press_event', lambda event: self.onclick(event))\n        update_display = fig.canvas.mpl_connect('key_press_event', lambda event: self.keydown(event))\n        self.show_instructions()\n        plt.show()", "\nif __name__ == '__main__':\n    path = '/home/louisccc/NAS/louisccc/av/honda_data/filtered_clips/lanechange/6057_201702271632_lanechange/raw_images/33322.jpg'\n    #TODO: replace path with command line arg instead to enable user to input their own path.\n    bev = BEV(path, mode='calibrate')\n    bev.calibrate()"]}
{"filename": "scene_graph/extraction/bev/visualizer.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nfrom pathlib import Path\n\nimport cv2\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport numpy as np\n", "import numpy as np\n\nfrom bev import BEV\n\n'''\n    BEV Visualizer: run this script to view the bird's eye projected version of your image dataset\n    To be used after calibrating BEV projection in bev.py | BEV(path, mode='calibrate')\n'''\n\ndef read_int(fname):\n    with open(str(fname), 'r') as f:\n        return int(f.read())", "\ndef read_int(fname):\n    with open(str(fname), 'r') as f:\n        return int(f.read())\n\ndef skip_clip(clip):\n    ignore_path = (clip/\"ignore.txt\").resolve()\n    if ignore_path.exists():\n        if read_int(ignore_path): return True\n    return False", "\ndef bev_demo(bev, clip_path):\n    M = bev.compute_homography_matrix()\n    clips = [c for c in Path(clip_path).iterdir()]\n\n    for clip in clips:\n        if skip_clip(clip): continue\n        vid = []\n        counter = 1\n        fig = plt.figure()\n        frames = [f for f in (clip / \"raw_images\").iterdir()]\n        print(\"Now showing: {}\".format(str(clip)))\n        \n        for frame in frames:\n            print(frame)\n            img = cv2.imread(str(frame), cv2.IMREAD_UNCHANGED)\n            assert (bev.params['height'], bev.params['width']) == img.shape[:-1]\n            \n            # images\n            img = cv2.copyMakeBorder(img, bev.params['cropped_height'], 0, 0, 0, cv2.BORDER_CONSTANT)\n            warped_img = cv2.warpPerspective(img, M, (bev.params['width'], bev.params['height']+bev.params['cropped_height'])) # Image warping\n            merge_imgs = np.hstack((img, warped_img))\n            \n            # horizon line\n            x = [0, 2*(bev.params['width']) - 1]\n            y = [bev.params['horizon_height'], bev.params['horizon_height']]\n\n            vid.append([plt.imshow(cv2.cvtColor(merge_imgs, cv2.COLOR_BGR2RGB)), plt.plot(x, y, color='red')[0]])\n\n            if counter % 100 == 0:\n                ani = animation.ArtistAnimation(fig, vid, interval=200, blit=True, repeat_delay=1000)\n                plt.show()\n                fig = plt.figure()\n                vid = []\n\n            counter +=1", "\nif __name__ == \"__main__\":\n    clip_path = \"./honda\"\n    bev = BEV('bev.json', mode='deploy')\n    bev_demo(bev, clip_path)"]}
{"filename": "scripts/3_train_model.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport sys, pdb\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom learning.util.image_trainer import Image_Trainer\nfrom learning.util.scenegraph_trainer import Scenegraph_Trainer\nfrom util.config_parser import configuration\nimport wandb", "from util.config_parser import configuration\nimport wandb\n\n#Usage:\n#python 3_train_model.py --yaml_path ../config/rule_graph_risk_config.yaml\n\ndef train_Trainer(learning_config):\n    ''' Training the dynamic kg algorithm with different attention layer choice.'''\n\n    #wandb setup \n    wandb_arg = wandb.init(config=learning_config, \n                        project=learning_config.wandb_config['project'], \n                        entity=learning_config.wandb_config['entity'])\n    if learning_config.model_config['model_save_path'] == None:\n        learning_config.model_config['model_save_path'] = \"./saved_graph_models/\" + wandb_arg.name + \".pt\" # save models with wandb nametag instead of manual naming.\n\n    if learning_config.training_config[\"dataset_type\"] == \"real\":\n        trainer = Image_Trainer(learning_config, wandb_arg)\n        trainer.split_dataset()\n        trainer.build_model()\n        trainer.learn()\n        \n    elif learning_config.training_config[\"dataset_type\"] == \"scenegraph\":\n        trainer = Scenegraph_Trainer(learning_config, wandb_arg)\n        trainer.split_dataset()\n        trainer.build_model()\n        trainer.learn()\n    else:\n        raise ValueError(\"Task unrecognized\")\n\n    trainer.save_model()", "\nif __name__ == \"__main__\":\n    # the entry of dynkg pipeline training\n    learning_config = configuration(sys.argv[1:])\n    train_Trainer(learning_config)"]}
{"filename": "scripts/7_transfer_model.py", "chunked_list": ["import os\nimport sys\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom learning.util.image_trainer import Image_Trainer\nfrom learning.util.scenegraph_trainer import Scenegraph_Trainer\nfrom learning.util.rs2g_trainer import RS2G_Trainer\nfrom util.config_parser import configuration\nimport wandb\n\n#Usage:", "\n#Usage:\n#python 7_transfer_model.py --yaml_path ../config/transfer_rule_graph_risk_config.yaml\n#python 7_transfer_model.py --yaml_path ../config/transfer_ss_graph_risk_config.yaml\n\ndef train_Trainer(learning_config):\n    ''' Training the dynamic kg algorithm with different attention layer choice.'''\n\n    #wandb setup \n    wandb_arg = wandb.init(config=learning_config, \n                        project=learning_config.wandb_config['project'], \n                        entity=learning_config.wandb_config['entity'])\n    if learning_config.model_config['model_save_path'] == None:\n        learning_config.model_config['model_save_path'] = \"./saved_graph_models/\" + wandb_arg.name + \".pt\" # save models with wandb nametag instead of manual naming.\n\n    if learning_config.training_config[\"dataset_type\"] == \"real\":\n        trainer = Image_Trainer(learning_config, wandb_arg)\n        trainer.split_dataset()\n        trainer.load_model()\n        trainer.eval_model(current_epoch=0)\n        \n    elif learning_config.training_config[\"dataset_type\"] == \"scenegraph\":\n        if learning_config.model_config['model'] in ['rs2g']:\n            trainer = RS2G_Trainer(learning_config, wandb_arg)\n        else:\n            trainer = Scenegraph_Trainer(learning_config, wandb_arg)\n        trainer.build_transfer_learning_dataset()\n        trainer.load_model()\n        trainer.evaluate_transfer_learning()\n    else:\n        raise ValueError(\"Task unrecognized\")\n\n    trainer.save_model()", "\nif __name__ == \"__main__\":\n    # the entry of dynkg pipeline training\n    learning_config = configuration(sys.argv[1:])\n    train_Trainer(learning_config)"]}
{"filename": "scripts/6_train_rs2g_model.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom learning.util.rs2g_trainer import RS2G_Trainer\nfrom util.config_parser import configuration\nimport wandb\n\n#Usage:\n#python 3_train_model.py --yaml_path ../config/rs2g_graph_risk_config.yaml  ", "#Usage:\n#python 3_train_model.py --yaml_path ../config/rs2g_graph_risk_config.yaml  \n\ndef train_Trainer(learning_config):\n    ''' Training the dynamic kg algorithm with different attention layer choice.'''\n\n    #wandb setup \n    wandb_arg = wandb.init(config=learning_config, \n                        project=learning_config.wandb_config['project'], \n                        entity=learning_config.wandb_config['entity'])\n    if learning_config.model_config['model_save_path'] == None:\n        learning_config.model_config['model_save_path'] = \"./saved_graph_models/\" + wandb_arg.name + \".pt\" # save models with wandb nametag instead of manual naming.\n\n    assert learning_config.model_config['model'] in ['rs2g'], 'This script only supports the RS2G model.'\n\n    if learning_config.training_config[\"dataset_type\"] == \"scenegraph\":\n        trainer = RS2G_Trainer(learning_config, wandb_arg)\n        trainer.split_dataset()\n        trainer.build_model()\n        trainer.learn()\n        trainer.evaluate()\n    else:\n        raise ValueError(\"Task unrecognized\")\n\n    trainer.save_model()", "\nif __name__ == \"__main__\":\n    learning_config = configuration(sys.argv[1:])\n    train_Trainer(learning_config)"]}
{"filename": "data/dataset.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys, os\n# import pathlib\n# temp = pathlib.PosixPath\n# pathlib.PosixPath = pathlib.WindowsPath #hack fix to get it work on windows\nfrom pathlib import Path\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom abc import ABC", "sys.path.append(os.path.dirname(sys.path[0]))\nfrom abc import ABC\nimport pickle as pkl\n\n'''\nAbstract class defining dataset properties and functions\n\nDatasets must be structured as follows:\n# dataset_path / <sequence_id> / raw_images / <image files> (sorted in ascending filename order.)\n# dataset_path / <sequence_id> / gt_data / <ground truth data files> (sorted in ascending filename order.)", "# dataset_path / <sequence_id> / raw_images / <image files> (sorted in ascending filename order.)\n# dataset_path / <sequence_id> / gt_data / <ground truth data files> (sorted in ascending filename order.)\n# dataset_path / <sequence_id> / label.txt (sorted ascending filename order or simply one for entire sequence.)\n# dataset_path / <sequence_id> / metadata.txt (sorted in ascending filename order or one for the entire sequence.)\n\nAll directories under dataset_path will be considered to be sequences containing data and labels.\n\nThe resulting RawImageDataset will be stored in the following location:\n# dataset_path / <image_dataset_path>.pkl\n", "# dataset_path / <image_dataset_path>.pkl\n\nThe resulting SceneGraphDataset will be stored in the following location:\n# dataset_path / <sg_dataset_path>.pkl\n'''\nclass BaseDataset(ABC):\n    def __init__(self, config):\n        self.dataset_path = config.location_data[\"input_path\"]\n        self.config = config\n        self.data = None\n        self.labels = None\n        self.dataset_save_path = config.location_data[\"data_save_path\"]\n        self.dataset_type = config.dataset_type\n        self.action_types = None\n        self.ignore = []\n        self.folder_names = None\n\n\n    #load/save data from dataset_path into data, labels, meta\n    def save(self):\n        with open(self.dataset_save_path, 'wb') as f:\n            pkl.dump(self, f)\n\n    def load(self):\n        with open(self.dataset_save_path, 'rb') as f:\n            return pkl.load(f)", "\n'''\nDataset containing image data and associated information only.\n'''\nclass RawImageDataset(BaseDataset):\n    # REQ: the dataset that only contains raw images\n    # REQ: this dataset can be used for scene graph extractor\n    # REQ: this dataset can be used by CNN-based approach directly.\n    def __init__(self, config = None):\n        if config != None:\n            super(RawImageDataset, self).__init__(config)\n            self.im_height = None\n            self.im_width =  None\n            self.color_channels =  None\n            self.frame_limit = config.frame_data[\"frames_limit\"]\n            self.dataset_type = 'image'\n            self.data = {}   #{sequence{frame{frame_data}}} \n            self.labels = {} #{sequence{label}}\n            self.action_types = {} #{sequence{action}}\n            self.ignore = [] #sequences to ignore", "\n\n'''\nDataset containing scene-graph representations of the road scenes.\nThis dataset is generated by the scene graph extractors and saved as a pkl file.\n'''\nclass SceneGraphDataset(BaseDataset):\n    # REQ: the dataset that only contains scene-graphs\n    # meta data dict\n    #action types dict\n    # labels' dict\n    # should be able to be converted into graph dataset or sequenced graph dataset.\n    def __init__(self, config = None, scene_graphs= {}, action_types= {}, label_data= {},meta_data = {}):\n        if config != None:\n            super(SceneGraphDataset, self).__init__(config)\n            self.dataset_type = 'scenegraph'\n            self.scene_graphs = scene_graphs\n            self.meta = meta_data\n            self.labels = label_data\n            self.action_types = action_types\n\n\n    def process_carla_graph_sequences(self, scenegraphs, feature_list, frame_numbers = None, folder_name=None): \n        '''\n            this is for creation of trainer input using carla data\n            returns a dictionary containing sg metadata for each frame in a sequence\n            default frame_numbers to len of sg dict that contains scenegraphs for each frame of the given sequence\n            The self.scenegraphs_sequence should be having same length after the subsampling. \n            This function will get the graph-related features (node embeddings, edge types, adjacency matrix) from scenegraphs.\n            in tensor formats.\n        '''\n        if frame_numbers == None:\n            frame_numbers = sorted(list(scenegraphs.keys()))\n        scenegraphs = [scenegraphs[frames] for frames in sorted(scenegraphs.keys())]\n        sequence = []\n        for idx, (scenegraph, frame_number) in enumerate(zip(scenegraphs, frame_numbers)):\n            sg_dict = {}\n            \n            node_name2idx = {node:idx for idx, node in enumerate(scenegraph.g.nodes)}\n    \n            sg_dict['node_features']                    = scenegraph.get_carla_node_embeddings(feature_list)\n            sg_dict['edge_index'], sg_dict['edge_attr'] = scenegraph.get_carla_edge_embeddings(node_name2idx)\n            sg_dict['folder_name'] = folder_name\n            sg_dict['frame_number'] = frame_number\n            sg_dict['node_order'] = node_name2idx\n            sequence.append(sg_dict)\n    \n        return sequence\n  \n    #===================================================================\n    \n    # this is for creation of trainer input using image data \n    #===================================================================\n    \n    def process_real_image_graph_sequences(self, scenegraphs, feature_list, frame_numbers=None, folder_name=None):\n        '''\n            The self.scenegraphs_sequence should be having same length after the subsampling. \n            This function will get the graph-related features (node embeddings, edge types, adjacency matrix) from scenegraphs.\n            in tensor formats.\n        '''\n        if frame_numbers == None:\n            frame_numbers = sorted(list(scenegraphs.keys()))\n        scenegraphs = [scenegraphs[frames] for frames in sorted(scenegraphs.keys())]\n        sequence = []\n    \n        for idx, (scenegraph, frame_number) in enumerate(zip(scenegraphs, frame_numbers)):\n            sg_dict = {}\n    \n            node_name2idx = {node: idx for idx,\n                             node in enumerate(scenegraph.g.nodes)}\n    \n            sg_dict['node_features'] = scenegraph.get_real_image_node_embeddings(feature_list)\n            sg_dict['edge_index'], sg_dict['edge_attr'] = scenegraph.get_real_image_edge_embeddings(node_name2idx)\n            sg_dict['folder_name'] = folder_name\n            sg_dict['frame_number'] = frame_number\n            sg_dict['node_order'] = node_name2idx\n            sequence.append(sg_dict)\n    \n        return sequence", "    \n    \n    \n    #==================================================================\n\n"]}
{"filename": "data/__init__.py", "chunked_list": [""]}
{"filename": "data/proc/preprocessor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys\nfrom pathlib import Path\nfrom abc import ABC\n\n'''Abstract base class used to create CarlaPreprocessor and RealPreprocessor'''\nclass Preprocessor(ABC):\n    def __init__(self, config):\n        self.conf = config\n        self.dataset = None", "class Preprocessor(ABC):\n    def __init__(self, config):\n        self.conf = config\n        self.dataset = None\n\n"]}
{"filename": "data/proc/__init__.py", "chunked_list": [""]}
{"filename": "data/proc/real_preprocessor.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport sys, pdb\nfrom pathlib import Path\nsys.path.append(os.path.dirname(sys.path[0]))\nfrom data.proc.preprocessor import Preprocessor as prepproc\nfrom data import dataset as ds\nfrom pathlib import Path", "from data import dataset as ds\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport cv2\nfrom os import listdir\nfrom os.path import isfile, join\nimport numpy as np\n\n\n\"\"\"RealPreprocessor takes in config and returns RawImageDataset object.\"\"\"\nclass RealPreprocessor(prepproc):\n    def __init__(self,config):\n        super(RealPreprocessor, self).__init__(config) \n        self.dataset = ds.RawImageDataset(self.conf)\n        \n    '''Extract scene data using raw images of each frame.'''\n    def load(self):\n        if not os.path.exists(self.dataset.dataset_path):\n            raise FileNotFoundError(self.dataset.dataset_path)\n        all_sequence_dirs = [x for x in Path(self.dataset.dataset_path).iterdir() if x.is_dir()]\n        dir_order = None\n        try:\n            all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: int(x.stem.split('_')[0]))  \n        except:\n            print('failed to sort by sequence number, sorting by folder name instead.')\n            all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: x.stem)\n            dir_order = list(range(len(all_sequence_dirs)))\n        self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n        for path in tqdm(all_sequence_dirs):\n            if dir_order is None:\n                seq = int(path.stem.split('_')[0])\n            else:\n                seq = dir_order.pop(0)\n            label_path = (path/\"label.txt\").resolve()\n            ignore_path = (path/\"ignore.txt\").resolve()\n            \n            if ignore_path.exists(): #record ignored sequences, and only load the sequences that were not ignored\n                with open(str(path/\"ignore.txt\"), 'r') as label_f:\n                    ignore_label = int(label_f.read())\n                    if ignore_label:\n                        self.dataset.ignore.append(seq)\n                        continue #skip to next seq if ignore path exists\n\n            self.dataset.data[seq] = self._load_images(path)\n            self.dataset.action_types[seq] = \"lanechange\" #path.stem.split('_')[2] #TODO: this is broken for carla. also, we cannot assume that our users will have their data in this format.\n            if label_path.exists():\n                with open(str(path/'label.txt'), 'r') as label_file:\n                    lines = label_file.readlines()\n                    l0 = 1.0 if float(lines[0].strip().split(\",\")[0]) >= 0 else 0.0 \n                    self.dataset.labels[seq] = l0\n\n    '''Represent each frame in sequence in terms of a tensor'''               \n    def _load_images(self, path):\n        raw_images_loc = (path/'raw_images').resolve()\n        images = sorted([Path(f) for f in listdir(raw_images_loc) if isfile(join(raw_images_loc, f)) and \".DS_Store\" not in f and \"Thumbs\" not in f], key = lambda x: int(x.stem.split(\".\")[0]))\n        images = [join(raw_images_loc,i) for i in images] \n\n        sequence_tensor = {}\n        shape = None\n        modulo = 0\n        acc_number = 0\n        if(self.dataset.frame_limit != None):\n            modulo = int(len(images) / self.dataset.frame_limit)  #subsample to frame_limit \n        if(self.dataset.frame_limit == None or modulo == 0):\n            modulo = 1\n\n        self.dataset.im_height, self.dataset.im_width = self.conf.output_format[\"height\"], self.conf.output_format[\"width\"]\n        if self.conf.output_format[\"color\"] == \"RGB\":\n            self.dataset.color_channels = 3\n        elif self.conf.output_format[\"color\"] == \"Grayscale\":\n            self.dataset.color_channels = 1\n\n        for i in range(0, len(images)):\n            if (i % modulo == 0 and self.dataset.frame_limit == None) or (i % modulo == 0 and acc_number < self.dataset.frame_limit):\n                image_path = images[i]\n                frame_num = int(Path(image_path).stem)\n                if self.conf.output_format[\"color\"] == \"RGB\":\n                    im = cv2.imread(str(image_path), cv2.IMREAD_COLOR) \n                elif self.conf.output_format[\"color\"] == \"Greyscale\":\n                    im = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE) \n                im = cv2.resize(im, (self.dataset.im_width, self.dataset.im_height)).transpose(2, 0, 1) #convert to (channels, height, width) format\n                if shape != None:\n                    if im.shape != shape:\n                        raise ValueError(\"All images in a sequence must have the same shape\")\n                else:\n                    shape = im.shape\n                sequence_tensor[frame_num] = im \n                acc_number += 1\n        if len(sequence_tensor) != self.dataset.frame_limit: #zero pad if necessary\n            for i in range(len(sequence_tensor), self.dataset.frame_limit):\n                sequence_tensor[i] = np.zeros(shape, dtype=np.uint8)\n        assert len(sequence_tensor) == self.dataset.frame_limit if self.dataset.frame_limit != 0 else len(sequence_tensor) > 0\n        return sequence_tensor\n      \n    '''Returns RawImageDataset object containing scengraphs, labels, and action types'''\n    def getDataSet(self):\n        return self.dataset", "\n\"\"\"RealPreprocessor takes in config and returns RawImageDataset object.\"\"\"\nclass RealPreprocessor(prepproc):\n    def __init__(self,config):\n        super(RealPreprocessor, self).__init__(config) \n        self.dataset = ds.RawImageDataset(self.conf)\n        \n    '''Extract scene data using raw images of each frame.'''\n    def load(self):\n        if not os.path.exists(self.dataset.dataset_path):\n            raise FileNotFoundError(self.dataset.dataset_path)\n        all_sequence_dirs = [x for x in Path(self.dataset.dataset_path).iterdir() if x.is_dir()]\n        dir_order = None\n        try:\n            all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: int(x.stem.split('_')[0]))  \n        except:\n            print('failed to sort by sequence number, sorting by folder name instead.')\n            all_sequence_dirs = sorted(all_sequence_dirs, key=lambda x: x.stem)\n            dir_order = list(range(len(all_sequence_dirs)))\n        self.dataset.folder_names = [path.stem for path in all_sequence_dirs]\n        for path in tqdm(all_sequence_dirs):\n            if dir_order is None:\n                seq = int(path.stem.split('_')[0])\n            else:\n                seq = dir_order.pop(0)\n            label_path = (path/\"label.txt\").resolve()\n            ignore_path = (path/\"ignore.txt\").resolve()\n            \n            if ignore_path.exists(): #record ignored sequences, and only load the sequences that were not ignored\n                with open(str(path/\"ignore.txt\"), 'r') as label_f:\n                    ignore_label = int(label_f.read())\n                    if ignore_label:\n                        self.dataset.ignore.append(seq)\n                        continue #skip to next seq if ignore path exists\n\n            self.dataset.data[seq] = self._load_images(path)\n            self.dataset.action_types[seq] = \"lanechange\" #path.stem.split('_')[2] #TODO: this is broken for carla. also, we cannot assume that our users will have their data in this format.\n            if label_path.exists():\n                with open(str(path/'label.txt'), 'r') as label_file:\n                    lines = label_file.readlines()\n                    l0 = 1.0 if float(lines[0].strip().split(\",\")[0]) >= 0 else 0.0 \n                    self.dataset.labels[seq] = l0\n\n    '''Represent each frame in sequence in terms of a tensor'''               \n    def _load_images(self, path):\n        raw_images_loc = (path/'raw_images').resolve()\n        images = sorted([Path(f) for f in listdir(raw_images_loc) if isfile(join(raw_images_loc, f)) and \".DS_Store\" not in f and \"Thumbs\" not in f], key = lambda x: int(x.stem.split(\".\")[0]))\n        images = [join(raw_images_loc,i) for i in images] \n\n        sequence_tensor = {}\n        shape = None\n        modulo = 0\n        acc_number = 0\n        if(self.dataset.frame_limit != None):\n            modulo = int(len(images) / self.dataset.frame_limit)  #subsample to frame_limit \n        if(self.dataset.frame_limit == None or modulo == 0):\n            modulo = 1\n\n        self.dataset.im_height, self.dataset.im_width = self.conf.output_format[\"height\"], self.conf.output_format[\"width\"]\n        if self.conf.output_format[\"color\"] == \"RGB\":\n            self.dataset.color_channels = 3\n        elif self.conf.output_format[\"color\"] == \"Grayscale\":\n            self.dataset.color_channels = 1\n\n        for i in range(0, len(images)):\n            if (i % modulo == 0 and self.dataset.frame_limit == None) or (i % modulo == 0 and acc_number < self.dataset.frame_limit):\n                image_path = images[i]\n                frame_num = int(Path(image_path).stem)\n                if self.conf.output_format[\"color\"] == \"RGB\":\n                    im = cv2.imread(str(image_path), cv2.IMREAD_COLOR) \n                elif self.conf.output_format[\"color\"] == \"Greyscale\":\n                    im = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE) \n                im = cv2.resize(im, (self.dataset.im_width, self.dataset.im_height)).transpose(2, 0, 1) #convert to (channels, height, width) format\n                if shape != None:\n                    if im.shape != shape:\n                        raise ValueError(\"All images in a sequence must have the same shape\")\n                else:\n                    shape = im.shape\n                sequence_tensor[frame_num] = im \n                acc_number += 1\n        if len(sequence_tensor) != self.dataset.frame_limit: #zero pad if necessary\n            for i in range(len(sequence_tensor), self.dataset.frame_limit):\n                sequence_tensor[i] = np.zeros(shape, dtype=np.uint8)\n        assert len(sequence_tensor) == self.dataset.frame_limit if self.dataset.frame_limit != 0 else len(sequence_tensor) > 0\n        return sequence_tensor\n      \n    '''Returns RawImageDataset object containing scengraphs, labels, and action types'''\n    def getDataSet(self):\n        return self.dataset", "    \n            \n"]}
{"filename": "util/__init__.py", "chunked_list": [""]}
{"filename": "util/config_parser.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(os.path.dirname(sys.argv[0])), \"config\"))\nfrom argparse import ArgumentParser\nimport yaml\nfrom pathlib import Path", "import yaml\nfrom pathlib import Path\n\nclass configuration:\n    def __init__(self, args, from_function = False):\n        if type(args) != list:\n            from_function = True\n        if not(from_function):\n            ap = ArgumentParser(description='The parameters for use-case 2.')\n            ap.add_argument('--yaml_path', type=str, default=\"./IP-NetList.yaml\", help=\"The path of yaml config file.\")\n            ap.add_argument('--model', type=str, default = None, help=\"model override.\")\n            ap.add_argument('--input_path', type=str, default = None, help=\"input_path override.\")\n            ap.add_argument('--task_type', type=str, default = None, help=\"task type override.\")\n            args_parsed = ap.parse_args(args)\n            for arg_name in vars(args_parsed):\n                self.__dict__[arg_name] = getattr(args_parsed, arg_name)\n                self.yaml_path = Path(self.yaml_path).resolve()\n            #handle command line overrides.\n            if self.model != None:\n                self.model_config['model'] = self.model\n            if self.input_path != None:\n                self.location_data['input_path'] = self.input_path\n            if self.task_type != None:\n                self.training_config['task_type'] = self.task_type\n            \n            # if self.model_config['graph_extraction'] == 'rule_based':\n            #     self.training_config['load_lane_info'] = False\n                    \n        \n        if from_function:\n            self.yaml_path = Path(args).resolve()\n        with open(self.yaml_path, 'r') as f:\n            args = yaml.safe_load(f)\n            for arg_name, arg_value in args.items():\n                self.__dict__[arg_name] = arg_value\n\n\n        \n    @staticmethod\n    def parse_args(yaml_path):\n        return configuration(yaml_path,True)", "\n\n\n\nif __name__ == \"__main__\":\n    configuration(sys.argv[1:])"]}
{"filename": "util/script_exceptions.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\n#add more exceptions here for scripts if needed\n\nclass Invalid_Dataset_Type(Exception):\n    pass"]}
{"filename": "learning/__init__.py", "chunked_list": [""]}
{"filename": "learning/model/mrgcn.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(sys.path[0]))\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F", "import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchnlp.nn import Attention\nfrom torch.nn import Linear, LSTM\nfrom torch_geometric.nn import RGCNConv, TopKPooling, FastRGCNConv\nfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\nfrom .rgcn_sag_pooling import RGCNSAGPooling\n\nclass MRGCN(nn.Module):\n    \n    def __init__(self, config):\n        super(MRGCN, self).__init__()\n        self.num_features = config.model_config['num_of_classes']\n        self.num_relations = config.model_config['num_relations']\n        self.num_classes  = config.model_config['nclass']\n        self.num_layers = config.model_config['num_layers'] #defines number of RGCN conv layers.\n        self.hidden_dim = config.model_config['hidden_dim']\n        self.layer_spec = None if config.model_config['layer_spec'] == None else list(map(int, config.model_config['layer_spec'].split(',')))\n        self.lstm_dim1 = config.model_config['lstm_input_dim']\n        self.lstm_dim2 = config.model_config['lstm_output_dim']\n        self.rgcn_func = FastRGCNConv if config.model_config['conv_type'] == \"FastRGCNConv\" else RGCNConv\n        self.activation = F.relu if config.model_config['activation'] == 'relu' else F.leaky_relu\n        self.pooling_type = config.model_config['pooling_type']\n        self.readout_type = config.model_config['readout_type']\n        self.temporal_type = config.model_config['temporal_type']\n\n        self.dropout = config.model_config['dropout']\n        self.conv = []\n        total_dim = 0\n\n        if self.layer_spec == None:\n            if self.num_layers > 0:\n                self.conv.append(self.rgcn_func(self.num_features, self.hidden_dim, self.num_relations).to(config.model_config['device']))\n                total_dim += self.hidden_dim\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.hidden_dim, self.hidden_dim, self.num_relations).to(config.model_config['device']))\n                    total_dim += self.hidden_dim\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n        else:\n            if self.num_layers > 0:\n                print(\"using layer specification and ignoring hidden_dim parameter.\")\n                print(\"layer_spec: \" + str(self.layer_spec))\n                self.conv.append(self.rgcn_func(self.num_features, self.layer_spec[0], self.num_relations).to(config.model_config['device']))\n                total_dim += self.layer_spec[0]\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.layer_spec[i-1], self.layer_spec[i], self.num_relations).to(config.model_config['device']))\n                    total_dim += self.layer_spec[i]\n\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n\n        if self.pooling_type == \"sagpool\":\n            self.pool1 = RGCNSAGPooling(total_dim, self.num_relations, ratio=config.model_config['pooling_ratio'], rgcn_func=config.model_config['conv_type'])\n        elif self.pooling_type == \"topk\":\n            self.pool1 = TopKPooling(total_dim, ratio=config.model_config['pooling_ratio'])\n\n        self.fc1 = Linear(total_dim, self.lstm_dim1)\n        \n        if \"lstm\" in self.temporal_type:\n            self.lstm = LSTM(self.lstm_dim1, self.lstm_dim2, batch_first=True)\n            self.attn = Attention(self.lstm_dim2)\n            self.lstm_decoder = LSTM(self.lstm_dim2, self.lstm_dim2, batch_first=True)\n        else:\n            self.fc1_5 = Linear(self.lstm_dim1, self.lstm_dim2)\n\n        self.fc2 = Linear(self.lstm_dim2, self.num_classes)\n\n\n    def forward(self, x, edge_index, edge_attr, batch=None):\n        attn_weights = dict()\n        outputs = []\n        if self.num_layers > 0:\n            for i in range(self.num_layers):\n                x = self.activation(self.conv[i](x, edge_index, edge_attr))\n                x = F.dropout(x, self.dropout, training=self.training)\n                outputs.append(x)\n            x = torch.cat(outputs, dim=-1)\n        else:\n            x = self.activation(self.fc0_5(x))\n\n        if self.pooling_type == \"sagpool\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        elif self.pooling_type == \"topk\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        else: \n            attn_weights['batch'] = batch\n\n        if self.readout_type == \"add\":\n            x = global_add_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"mean\":\n            x = global_mean_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"max\":\n            x = global_max_pool(x, attn_weights['batch'])\n        else:\n            pass\n\n        x = self.activation(self.fc1(x))\n    \n        if self.temporal_type == \"mean\":\n            x = self.activation(self.fc1_5(x.mean(axis=0)))\n        elif self.temporal_type == \"lstm_last\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = h.flatten()\n        elif self.temporal_type == \"lstm_sum\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = x_predicted.sum(dim=1).flatten()\n        elif self.temporal_type == \"lstm_attn\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x, attn_weights['lstm_attn_weights'] = self.attn(h.view(1,1,-1), x_predicted)\n            x, (h_decoder, c_decoder) = self.lstm_decoder(x, (h, c))\n            x = x.flatten()\n        elif self.temporal_type == \"lstm_seq\": #used for step-by-step sequence prediction. \n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0)) #x_predicted is sequence of predictions for each frame, h is hidden state of last item, c is last cell state\n            x = x_predicted.squeeze(0) #we return x_predicted as we want to know the output of the LSTM for each value in the sequence\n        else:\n            pass\n                \n        return F.log_softmax(self.fc2(x), dim=-1), attn_weights", "class MRGCN(nn.Module):\n    \n    def __init__(self, config):\n        super(MRGCN, self).__init__()\n        self.num_features = config.model_config['num_of_classes']\n        self.num_relations = config.model_config['num_relations']\n        self.num_classes  = config.model_config['nclass']\n        self.num_layers = config.model_config['num_layers'] #defines number of RGCN conv layers.\n        self.hidden_dim = config.model_config['hidden_dim']\n        self.layer_spec = None if config.model_config['layer_spec'] == None else list(map(int, config.model_config['layer_spec'].split(',')))\n        self.lstm_dim1 = config.model_config['lstm_input_dim']\n        self.lstm_dim2 = config.model_config['lstm_output_dim']\n        self.rgcn_func = FastRGCNConv if config.model_config['conv_type'] == \"FastRGCNConv\" else RGCNConv\n        self.activation = F.relu if config.model_config['activation'] == 'relu' else F.leaky_relu\n        self.pooling_type = config.model_config['pooling_type']\n        self.readout_type = config.model_config['readout_type']\n        self.temporal_type = config.model_config['temporal_type']\n\n        self.dropout = config.model_config['dropout']\n        self.conv = []\n        total_dim = 0\n\n        if self.layer_spec == None:\n            if self.num_layers > 0:\n                self.conv.append(self.rgcn_func(self.num_features, self.hidden_dim, self.num_relations).to(config.model_config['device']))\n                total_dim += self.hidden_dim\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.hidden_dim, self.hidden_dim, self.num_relations).to(config.model_config['device']))\n                    total_dim += self.hidden_dim\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n        else:\n            if self.num_layers > 0:\n                print(\"using layer specification and ignoring hidden_dim parameter.\")\n                print(\"layer_spec: \" + str(self.layer_spec))\n                self.conv.append(self.rgcn_func(self.num_features, self.layer_spec[0], self.num_relations).to(config.model_config['device']))\n                total_dim += self.layer_spec[0]\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.layer_spec[i-1], self.layer_spec[i], self.num_relations).to(config.model_config['device']))\n                    total_dim += self.layer_spec[i]\n\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n\n        if self.pooling_type == \"sagpool\":\n            self.pool1 = RGCNSAGPooling(total_dim, self.num_relations, ratio=config.model_config['pooling_ratio'], rgcn_func=config.model_config['conv_type'])\n        elif self.pooling_type == \"topk\":\n            self.pool1 = TopKPooling(total_dim, ratio=config.model_config['pooling_ratio'])\n\n        self.fc1 = Linear(total_dim, self.lstm_dim1)\n        \n        if \"lstm\" in self.temporal_type:\n            self.lstm = LSTM(self.lstm_dim1, self.lstm_dim2, batch_first=True)\n            self.attn = Attention(self.lstm_dim2)\n            self.lstm_decoder = LSTM(self.lstm_dim2, self.lstm_dim2, batch_first=True)\n        else:\n            self.fc1_5 = Linear(self.lstm_dim1, self.lstm_dim2)\n\n        self.fc2 = Linear(self.lstm_dim2, self.num_classes)\n\n\n    def forward(self, x, edge_index, edge_attr, batch=None):\n        attn_weights = dict()\n        outputs = []\n        if self.num_layers > 0:\n            for i in range(self.num_layers):\n                x = self.activation(self.conv[i](x, edge_index, edge_attr))\n                x = F.dropout(x, self.dropout, training=self.training)\n                outputs.append(x)\n            x = torch.cat(outputs, dim=-1)\n        else:\n            x = self.activation(self.fc0_5(x))\n\n        if self.pooling_type == \"sagpool\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        elif self.pooling_type == \"topk\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        else: \n            attn_weights['batch'] = batch\n\n        if self.readout_type == \"add\":\n            x = global_add_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"mean\":\n            x = global_mean_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"max\":\n            x = global_max_pool(x, attn_weights['batch'])\n        else:\n            pass\n\n        x = self.activation(self.fc1(x))\n    \n        if self.temporal_type == \"mean\":\n            x = self.activation(self.fc1_5(x.mean(axis=0)))\n        elif self.temporal_type == \"lstm_last\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = h.flatten()\n        elif self.temporal_type == \"lstm_sum\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = x_predicted.sum(dim=1).flatten()\n        elif self.temporal_type == \"lstm_attn\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x, attn_weights['lstm_attn_weights'] = self.attn(h.view(1,1,-1), x_predicted)\n            x, (h_decoder, c_decoder) = self.lstm_decoder(x, (h, c))\n            x = x.flatten()\n        elif self.temporal_type == \"lstm_seq\": #used for step-by-step sequence prediction. \n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0)) #x_predicted is sequence of predictions for each frame, h is hidden state of last item, c is last cell state\n            x = x_predicted.squeeze(0) #we return x_predicted as we want to know the output of the LSTM for each value in the sequence\n        else:\n            pass\n                \n        return F.log_softmax(self.fc2(x), dim=-1), attn_weights"]}
{"filename": "learning/model/rs2g.py", "chunked_list": ["import os\nimport sys\nsys.path.append(os.path.dirname(sys.path[0]))\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchnlp.nn import Attention\nfrom torch.nn import Linear, LSTM\nfrom torch_geometric.nn import RGCNConv, TopKPooling, FastRGCNConv\nfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool", "from torch_geometric.nn import RGCNConv, TopKPooling, FastRGCNConv\nfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\nfrom .rgcn_sag_pooling import RGCNSAGPooling\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\n\n\n'''data-driven implementation of MRGCN (RS2G).'''\n\nclass RS2G(nn.Module):\n    \n    def __init__(self, config):\n        super(RS2G, self).__init__()\n        self.num_features = config.model_config['num_of_classes']\n        self.num_relations = config.model_config['num_relations']\n        self.num_classes  = config.model_config['nclass']\n        self.num_layers = config.model_config['num_layers'] #defines number of RGCN conv layers.\n        self.hidden_dim = config.model_config['hidden_dim']\n        self.layer_spec = None if config.model_config['layer_spec'] == None else list(map(int, config.model_config['layer_spec'].split(',')))\n        self.lstm_dim1 = config.model_config['lstm_input_dim']\n        self.lstm_dim2 = config.model_config['lstm_output_dim']\n        self.rgcn_func = FastRGCNConv if config.model_config['conv_type'] == \"FastRGCNConv\" else RGCNConv\n        self.activation = F.relu if config.model_config['activation'] == 'relu' else F.leaky_relu\n        self.pooling_type = config.model_config['pooling_type']\n        self.readout_type = config.model_config['readout_type']\n        self.temporal_type = config.model_config['temporal_type']\n        self.device = config.model_config['device']\n        self.dropout = config.model_config['dropout']\n        self.edge_ext_thresh = config.model_config['edge_ext_thresh']\n        self.conv = []\n        total_dim = 0\n\n        if self.layer_spec == None:\n            if self.num_layers > 0:\n                self.conv.append(self.rgcn_func(self.num_features, self.hidden_dim, self.num_relations).to(self.device))\n                total_dim += self.hidden_dim\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.hidden_dim, self.hidden_dim, self.num_relations).to(self.device))\n                    total_dim += self.hidden_dim\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n        else:\n            if self.num_layers > 0:\n                print(\"using layer specification and ignoring hidden_dim parameter.\")\n                print(\"layer_spec: \" + str(self.layer_spec))\n                self.conv.append(self.rgcn_func(self.num_features, self.layer_spec[0], self.num_relations).to(self.device))\n                total_dim += self.layer_spec[0]\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.layer_spec[i-1], self.layer_spec[i], self.num_relations).to(self.device))\n                    total_dim += self.layer_spec[i]\n\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n\n        if self.pooling_type == \"sagpool\":\n            self.pool1 = RGCNSAGPooling(total_dim, self.num_relations, ratio=config.model_config['pooling_ratio'], rgcn_func=config.model_config['conv_type'])\n        elif self.pooling_type == \"topk\":\n            self.pool1 = TopKPooling(total_dim, ratio=config.model_config['pooling_ratio'])\n\n        self.fc1 = Linear(total_dim, self.lstm_dim1)\n        \n        if \"lstm\" in self.temporal_type:\n            self.lstm = LSTM(self.lstm_dim1, self.lstm_dim2, batch_first=True)\n            self.attn = Attention(self.lstm_dim2)\n            self.lstm_decoder = LSTM(self.lstm_dim2, self.lstm_dim2, batch_first=True)\n        else:\n            self.fc1_5 = Linear(self.lstm_dim1, self.lstm_dim2)\n\n        self.fc2 = Linear(self.lstm_dim2, self.num_classes)\n\n        #~~~~~~~~~~~~Data-Driven Graph Encoders~~~~~~~~~~~~~~\n        #node encoder\n        if config.model_config['node_encoder_dim'] == 1:\n            self.node_encoder = Linear(15, self.num_features) \n        elif config.model_config['node_encoder_dim'] == 2:\n            self.node_encoder = nn.Sequential(\n                        nn.Linear(15, 30),\n                        nn.ReLU(),\n                        nn.Linear(30, self.num_features)\n                    )\n\n        #edge encoder. takes in two node embeddings and returns multilabel edge selection.\n        if config.model_config['edge_encoder_dim'] == 1:\n            self.edge_encoder = Linear(2 * 15, self.num_relations)\n        elif config.model_config['edge_encoder_dim'] == 2:\n            self.edge_encoder = nn.Sequential(\n                        nn.Linear(2 * 15, 30),\n                        nn.ReLU(),\n                        nn.Linear(30, self.num_relations)\n                    )\n\n\n    def forward(self, sequence):\n        #graph extraction component\n        graph_list = []\n        for i in range(len(sequence)):\n            graph = {}\n            node_feature_list = sequence[i]\n            graph['node_embeddings'] = self.activation(self.node_encoder(node_feature_list))\n            graph['edge_attr'] = []\n            graph['edge_index'] = []\n\n            new_arr = torch.ones([len(node_feature_list), len(node_feature_list)]).triu(diagonal=1)\n            new_arr_idx = torch.where(new_arr==1.0)\n            combo_list = torch.stack(new_arr_idx).t()\n\n            new_arr_2 = new_arr.flatten().int()\n            new_arr_idx2 = torch.where(new_arr_2==1.0)\n            node_combo_a = node_feature_list.unsqueeze(0).repeat((node_feature_list.size(0), 1,1))\n            node_combo_b = node_feature_list.unsqueeze(1).repeat((1, node_feature_list.size(0),1))\n            node_combo = torch.cat([node_combo_b, node_combo_a], dim=-1).flatten(start_dim=0, end_dim=1)\n            node_combinations = node_combo[new_arr_idx2]\n            edge_vectors = self.edge_encoder(node_combinations)\n            edge_vectors = torch.sigmoid(edge_vectors) #sigmoid to generate multilabel conf. scores, then binarize. \n            top_edges = torch.argmax(edge_vectors, dim=1) #get highest scoring edge.\n            graph['edge_index'] = torch.cat([combo_list, combo_list.flip(1)], dim=0) #make edges bidirectional\n            graph['edge_attr'] = torch.cat([top_edges, top_edges], dim=0) #make edges bidirectional\n            pos_idxs = edge_vectors > self.edge_ext_thresh #add all edge types that score > threshold\n            pos_idxs = pos_idxs.nonzero()\n            pos_edge_idx, pos_edge_attrs = combo_list[pos_idxs[:,0]], pos_idxs[:, 1]\n            graph['edge_index'] = torch.cat([graph['edge_index'], pos_edge_idx, pos_edge_idx.flip(1)], dim=0)\n            graph['edge_attr'] = torch.cat([graph['edge_attr'], pos_edge_attrs, pos_edge_attrs], dim=0)\n            graph['edge_index'] = torch.transpose(graph['edge_index'], 0, 1) \n            graph['edge_attr'] = graph['edge_attr']\n            graph_list.append(graph)\n        graph_data_list = [Data(x=g['node_embeddings'], edge_index=g['edge_index'], edge_attr=g['edge_attr']) for g in graph_list]\n        train_loader = DataLoader(graph_data_list, batch_size=len(graph_data_list))\n        sequence = next(iter(train_loader)).to(self.device)\n        x, edge_index, edge_attr, batch = sequence.x, sequence.edge_index, sequence.edge_attr, sequence.batch\n\n        #MRGCN component. downstream task\n        attn_weights = dict()\n        outputs = []\n        if self.num_layers > 0:\n            for i in range(self.num_layers):\n                x = self.activation(self.conv[i](x, edge_index, edge_attr))\n                x = F.dropout(x, self.dropout, training=self.training)\n                outputs.append(x)\n            x = torch.cat(outputs, dim=-1)\n        else:\n            x = self.activation(self.fc0_5(x))\n\n        if self.pooling_type == \"sagpool\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        elif self.pooling_type == \"topk\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        else: \n            attn_weights['batch'] = batch\n\n        if self.readout_type == \"add\":\n            x = global_add_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"mean\":\n            x = global_mean_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"max\":\n            x = global_max_pool(x, attn_weights['batch'])\n        else:\n            pass\n\n        x = self.activation(self.fc1(x))\n    \n        #temporal modeling\n        if self.temporal_type == \"mean\":\n            x = self.activation(self.fc1_5(x.mean(axis=0)))\n        elif self.temporal_type == \"lstm_last\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = h.flatten()\n        elif self.temporal_type == \"lstm_sum\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = x_predicted.sum(dim=1).flatten()\n        elif self.temporal_type == \"lstm_attn\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x, attn_weights['lstm_attn_weights'] = self.attn(h.view(1,1,-1), x_predicted)\n            x, (h_decoder, c_decoder) = self.lstm_decoder(x, (h, c))\n            x = x.flatten()\n        elif self.temporal_type == \"lstm_seq\": #used for step-by-step sequence prediction. \n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0)) #x_predicted is sequence of predictions for each frame, h is hidden state of last item, c is last cell state\n            x = x_predicted.squeeze(0) #we return x_predicted as we want to know the output of the LSTM for each value in the sequence\n        else:\n            pass\n\n        return {'output': F.log_softmax(self.fc2(x), dim=-1), \n                'graph_list': graph_list}", "\nclass RS2G(nn.Module):\n    \n    def __init__(self, config):\n        super(RS2G, self).__init__()\n        self.num_features = config.model_config['num_of_classes']\n        self.num_relations = config.model_config['num_relations']\n        self.num_classes  = config.model_config['nclass']\n        self.num_layers = config.model_config['num_layers'] #defines number of RGCN conv layers.\n        self.hidden_dim = config.model_config['hidden_dim']\n        self.layer_spec = None if config.model_config['layer_spec'] == None else list(map(int, config.model_config['layer_spec'].split(',')))\n        self.lstm_dim1 = config.model_config['lstm_input_dim']\n        self.lstm_dim2 = config.model_config['lstm_output_dim']\n        self.rgcn_func = FastRGCNConv if config.model_config['conv_type'] == \"FastRGCNConv\" else RGCNConv\n        self.activation = F.relu if config.model_config['activation'] == 'relu' else F.leaky_relu\n        self.pooling_type = config.model_config['pooling_type']\n        self.readout_type = config.model_config['readout_type']\n        self.temporal_type = config.model_config['temporal_type']\n        self.device = config.model_config['device']\n        self.dropout = config.model_config['dropout']\n        self.edge_ext_thresh = config.model_config['edge_ext_thresh']\n        self.conv = []\n        total_dim = 0\n\n        if self.layer_spec == None:\n            if self.num_layers > 0:\n                self.conv.append(self.rgcn_func(self.num_features, self.hidden_dim, self.num_relations).to(self.device))\n                total_dim += self.hidden_dim\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.hidden_dim, self.hidden_dim, self.num_relations).to(self.device))\n                    total_dim += self.hidden_dim\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n        else:\n            if self.num_layers > 0:\n                print(\"using layer specification and ignoring hidden_dim parameter.\")\n                print(\"layer_spec: \" + str(self.layer_spec))\n                self.conv.append(self.rgcn_func(self.num_features, self.layer_spec[0], self.num_relations).to(self.device))\n                total_dim += self.layer_spec[0]\n                for i in range(1, self.num_layers):\n                    self.conv.append(self.rgcn_func(self.layer_spec[i-1], self.layer_spec[i], self.num_relations).to(self.device))\n                    total_dim += self.layer_spec[i]\n\n            else:\n                self.fc0_5 = Linear(self.num_features, self.hidden_dim)\n                total_dim += self.hidden_dim\n\n        if self.pooling_type == \"sagpool\":\n            self.pool1 = RGCNSAGPooling(total_dim, self.num_relations, ratio=config.model_config['pooling_ratio'], rgcn_func=config.model_config['conv_type'])\n        elif self.pooling_type == \"topk\":\n            self.pool1 = TopKPooling(total_dim, ratio=config.model_config['pooling_ratio'])\n\n        self.fc1 = Linear(total_dim, self.lstm_dim1)\n        \n        if \"lstm\" in self.temporal_type:\n            self.lstm = LSTM(self.lstm_dim1, self.lstm_dim2, batch_first=True)\n            self.attn = Attention(self.lstm_dim2)\n            self.lstm_decoder = LSTM(self.lstm_dim2, self.lstm_dim2, batch_first=True)\n        else:\n            self.fc1_5 = Linear(self.lstm_dim1, self.lstm_dim2)\n\n        self.fc2 = Linear(self.lstm_dim2, self.num_classes)\n\n        #~~~~~~~~~~~~Data-Driven Graph Encoders~~~~~~~~~~~~~~\n        #node encoder\n        if config.model_config['node_encoder_dim'] == 1:\n            self.node_encoder = Linear(15, self.num_features) \n        elif config.model_config['node_encoder_dim'] == 2:\n            self.node_encoder = nn.Sequential(\n                        nn.Linear(15, 30),\n                        nn.ReLU(),\n                        nn.Linear(30, self.num_features)\n                    )\n\n        #edge encoder. takes in two node embeddings and returns multilabel edge selection.\n        if config.model_config['edge_encoder_dim'] == 1:\n            self.edge_encoder = Linear(2 * 15, self.num_relations)\n        elif config.model_config['edge_encoder_dim'] == 2:\n            self.edge_encoder = nn.Sequential(\n                        nn.Linear(2 * 15, 30),\n                        nn.ReLU(),\n                        nn.Linear(30, self.num_relations)\n                    )\n\n\n    def forward(self, sequence):\n        #graph extraction component\n        graph_list = []\n        for i in range(len(sequence)):\n            graph = {}\n            node_feature_list = sequence[i]\n            graph['node_embeddings'] = self.activation(self.node_encoder(node_feature_list))\n            graph['edge_attr'] = []\n            graph['edge_index'] = []\n\n            new_arr = torch.ones([len(node_feature_list), len(node_feature_list)]).triu(diagonal=1)\n            new_arr_idx = torch.where(new_arr==1.0)\n            combo_list = torch.stack(new_arr_idx).t()\n\n            new_arr_2 = new_arr.flatten().int()\n            new_arr_idx2 = torch.where(new_arr_2==1.0)\n            node_combo_a = node_feature_list.unsqueeze(0).repeat((node_feature_list.size(0), 1,1))\n            node_combo_b = node_feature_list.unsqueeze(1).repeat((1, node_feature_list.size(0),1))\n            node_combo = torch.cat([node_combo_b, node_combo_a], dim=-1).flatten(start_dim=0, end_dim=1)\n            node_combinations = node_combo[new_arr_idx2]\n            edge_vectors = self.edge_encoder(node_combinations)\n            edge_vectors = torch.sigmoid(edge_vectors) #sigmoid to generate multilabel conf. scores, then binarize. \n            top_edges = torch.argmax(edge_vectors, dim=1) #get highest scoring edge.\n            graph['edge_index'] = torch.cat([combo_list, combo_list.flip(1)], dim=0) #make edges bidirectional\n            graph['edge_attr'] = torch.cat([top_edges, top_edges], dim=0) #make edges bidirectional\n            pos_idxs = edge_vectors > self.edge_ext_thresh #add all edge types that score > threshold\n            pos_idxs = pos_idxs.nonzero()\n            pos_edge_idx, pos_edge_attrs = combo_list[pos_idxs[:,0]], pos_idxs[:, 1]\n            graph['edge_index'] = torch.cat([graph['edge_index'], pos_edge_idx, pos_edge_idx.flip(1)], dim=0)\n            graph['edge_attr'] = torch.cat([graph['edge_attr'], pos_edge_attrs, pos_edge_attrs], dim=0)\n            graph['edge_index'] = torch.transpose(graph['edge_index'], 0, 1) \n            graph['edge_attr'] = graph['edge_attr']\n            graph_list.append(graph)\n        graph_data_list = [Data(x=g['node_embeddings'], edge_index=g['edge_index'], edge_attr=g['edge_attr']) for g in graph_list]\n        train_loader = DataLoader(graph_data_list, batch_size=len(graph_data_list))\n        sequence = next(iter(train_loader)).to(self.device)\n        x, edge_index, edge_attr, batch = sequence.x, sequence.edge_index, sequence.edge_attr, sequence.batch\n\n        #MRGCN component. downstream task\n        attn_weights = dict()\n        outputs = []\n        if self.num_layers > 0:\n            for i in range(self.num_layers):\n                x = self.activation(self.conv[i](x, edge_index, edge_attr))\n                x = F.dropout(x, self.dropout, training=self.training)\n                outputs.append(x)\n            x = torch.cat(outputs, dim=-1)\n        else:\n            x = self.activation(self.fc0_5(x))\n\n        if self.pooling_type == \"sagpool\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        elif self.pooling_type == \"topk\":\n            x, edge_index, _, attn_weights['batch'], attn_weights['pool_perm'], attn_weights['pool_score'] = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        else: \n            attn_weights['batch'] = batch\n\n        if self.readout_type == \"add\":\n            x = global_add_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"mean\":\n            x = global_mean_pool(x, attn_weights['batch'])\n        elif self.readout_type == \"max\":\n            x = global_max_pool(x, attn_weights['batch'])\n        else:\n            pass\n\n        x = self.activation(self.fc1(x))\n    \n        #temporal modeling\n        if self.temporal_type == \"mean\":\n            x = self.activation(self.fc1_5(x.mean(axis=0)))\n        elif self.temporal_type == \"lstm_last\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = h.flatten()\n        elif self.temporal_type == \"lstm_sum\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x = x_predicted.sum(dim=1).flatten()\n        elif self.temporal_type == \"lstm_attn\":\n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0))\n            x, attn_weights['lstm_attn_weights'] = self.attn(h.view(1,1,-1), x_predicted)\n            x, (h_decoder, c_decoder) = self.lstm_decoder(x, (h, c))\n            x = x.flatten()\n        elif self.temporal_type == \"lstm_seq\": #used for step-by-step sequence prediction. \n            x_predicted, (h, c) = self.lstm(x.unsqueeze(0)) #x_predicted is sequence of predictions for each frame, h is hidden state of last item, c is last cell state\n            x = x_predicted.squeeze(0) #we return x_predicted as we want to know the output of the LSTM for each value in the sequence\n        else:\n            pass\n\n        return {'output': F.log_softmax(self.fc2(x), dim=-1), \n                'graph_list': graph_list}"]}
{"filename": "learning/model/cnn_lstm.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN_LSTM_Classifier(nn.Module):\n    '''\n    CNN+LSTM binary classifier\n    To call module provide the input_shape and cfg params\n    input_shape should be a tensor -> (batch_size, frames, channels, height, width) \n    '''\n    def __init__(self, input_shape, cfg):\n        super(CNN_LSTM_Classifier, self).__init__()\n        self.cfg = cfg\n        self.batch_size, self.frames, self.channels, self.height, self.width = input_shape\n        self.dropout = self.cfg.model_config['dropout']\n        self.kernel_size = (3, 3)\n        self.lstm_layers = 1\n        self.conv_size = lambda i, k, p, s: int((i-k+2*p)/s + 1)\n        self.pool_size = lambda i, k, p, s, pool : self.conv_size(i, k, p, s) // pool + 1\n        self.flat_size = lambda f, h, w : f*h*w\n        self.TimeDistributed = lambda curr_layer, prev_layer : torch.stack([curr_layer(prev_layer[:,i]) for i in range(self.frames)], dim=1)\n        self.enable_bnorm = self.cfg.model_config['bnorm']\n        # Note: conv_size and pool_size only work for square 2D matrices, if not a square matrix, run once for height dim and another time for width dim\n        '''\n        conv_size = lambda i, k, p, s: int((i-k+2*p)/s + 1)\n        pool_size = lambda i, k, p, s, pool : conv_size(i, k, p, s) // pool + 1\n        flat_size = lambda f, h, w : f*h*w\n        '''\n        self.bn1 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn2 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn3 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn4 = nn.BatchNorm1d(num_features=self.frames)\n        self.bn5 = nn.BatchNorm1d(num_features=self.frames)    \n\n        self.c1 = nn.Conv2d(in_channels=self.channels, out_channels=16, kernel_size=self.kernel_size)\n        self.c2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=self.kernel_size)\n        self.mp1 = nn.MaxPool2d(kernel_size=2)\n        self.flat = nn.Flatten(start_dim=1)\n        self.flat_dim = self.get_flat_dim()\n        self.l1 = nn.Linear(in_features=self.flat_dim, out_features=200)\n        self.l2 = nn.Linear(in_features=200, out_features=50)\n        self.lstm1 = nn.LSTM(input_size=50, hidden_size=20, num_layers=self.lstm_layers, batch_first=True) \n        self.l3 = nn.Linear(in_features=20, out_features=2)\n        self.layer_names = self.ordered_layers = [(\"c1\", self.c1),(\"c2\", self.c2),(\"mp1\", self.mp1),(\"flat\", self.flat), (\"l1\", self.l1),(\"l2\", self.l2),(\"lstm1\", self.lstm1),(\"l3\", self.l3)]\n\n    def get_flat_dim(self):\n        c1_h = self.conv_size(self.height, self.kernel_size[-1], 0, 1)\n        c1_w = self.conv_size(self.width, self.kernel_size[-1], 0, 1)\n        c2_h = self.conv_size(c1_h, self.kernel_size[-1], 0, 1)\n        c2_w = self.conv_size(c1_w, self.kernel_size[-1], 0, 1)\n        mp1_h = c2_h // 2\n        mp1_w = c2_w // 2\n        return self.flat_size(16, mp1_h, mp1_w)\n\n    def forward(self, x):\n        # Distribute learnable layers across all frames with shared weights\n        if self.enable_bnorm: # can use a larger learning rate w/ bnorm #not in config currently\n            x = F.relu(self.bn1(self.TimeDistributed(self.c1, x)))\n            x = F.relu(self.bn2(self.TimeDistributed(self.c2, x)))\n            x = F.dropout(self.bn3(self.TimeDistributed(self.mp1, x)), p=self.dropout, training=self.training)\n            x = F.dropout(self.TimeDistributed(self.flat, x), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.bn4(self.TimeDistributed(self.l1, x))), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.bn5(self.TimeDistributed(self.l2, x))), p=self.dropout, training=self.training)\n        else:\n            x = F.relu(self.TimeDistributed(self.c1, x))\n            x = F.relu(self.TimeDistributed(self.c2, x))\n            x = F.dropout(self.TimeDistributed(self.mp1, x), p=self.dropout, training=self.training)\n            x = F.dropout(self.TimeDistributed(self.flat, x), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.TimeDistributed(self.l1, x)), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.TimeDistributed(self.l2, x)), p=self.dropout, training=self.training)\n        if self.cfg.training_config[\"task_type\"] == \"collision_prediction\":\n            x = x.view(x.shape[0]*x.shape[1],50)\n            x = x.unsqueeze(1)\n        _,(x,_) = self.lstm1(x)\n        x = torch.squeeze(self.l3(x))\n        return F.log_softmax(x, dim=-1)", "class CNN_LSTM_Classifier(nn.Module):\n    '''\n    CNN+LSTM binary classifier\n    To call module provide the input_shape and cfg params\n    input_shape should be a tensor -> (batch_size, frames, channels, height, width) \n    '''\n    def __init__(self, input_shape, cfg):\n        super(CNN_LSTM_Classifier, self).__init__()\n        self.cfg = cfg\n        self.batch_size, self.frames, self.channels, self.height, self.width = input_shape\n        self.dropout = self.cfg.model_config['dropout']\n        self.kernel_size = (3, 3)\n        self.lstm_layers = 1\n        self.conv_size = lambda i, k, p, s: int((i-k+2*p)/s + 1)\n        self.pool_size = lambda i, k, p, s, pool : self.conv_size(i, k, p, s) // pool + 1\n        self.flat_size = lambda f, h, w : f*h*w\n        self.TimeDistributed = lambda curr_layer, prev_layer : torch.stack([curr_layer(prev_layer[:,i]) for i in range(self.frames)], dim=1)\n        self.enable_bnorm = self.cfg.model_config['bnorm']\n        # Note: conv_size and pool_size only work for square 2D matrices, if not a square matrix, run once for height dim and another time for width dim\n        '''\n        conv_size = lambda i, k, p, s: int((i-k+2*p)/s + 1)\n        pool_size = lambda i, k, p, s, pool : conv_size(i, k, p, s) // pool + 1\n        flat_size = lambda f, h, w : f*h*w\n        '''\n        self.bn1 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn2 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn3 = nn.BatchNorm3d(num_features=self.frames)\n        self.bn4 = nn.BatchNorm1d(num_features=self.frames)\n        self.bn5 = nn.BatchNorm1d(num_features=self.frames)    \n\n        self.c1 = nn.Conv2d(in_channels=self.channels, out_channels=16, kernel_size=self.kernel_size)\n        self.c2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=self.kernel_size)\n        self.mp1 = nn.MaxPool2d(kernel_size=2)\n        self.flat = nn.Flatten(start_dim=1)\n        self.flat_dim = self.get_flat_dim()\n        self.l1 = nn.Linear(in_features=self.flat_dim, out_features=200)\n        self.l2 = nn.Linear(in_features=200, out_features=50)\n        self.lstm1 = nn.LSTM(input_size=50, hidden_size=20, num_layers=self.lstm_layers, batch_first=True) \n        self.l3 = nn.Linear(in_features=20, out_features=2)\n        self.layer_names = self.ordered_layers = [(\"c1\", self.c1),(\"c2\", self.c2),(\"mp1\", self.mp1),(\"flat\", self.flat), (\"l1\", self.l1),(\"l2\", self.l2),(\"lstm1\", self.lstm1),(\"l3\", self.l3)]\n\n    def get_flat_dim(self):\n        c1_h = self.conv_size(self.height, self.kernel_size[-1], 0, 1)\n        c1_w = self.conv_size(self.width, self.kernel_size[-1], 0, 1)\n        c2_h = self.conv_size(c1_h, self.kernel_size[-1], 0, 1)\n        c2_w = self.conv_size(c1_w, self.kernel_size[-1], 0, 1)\n        mp1_h = c2_h // 2\n        mp1_w = c2_w // 2\n        return self.flat_size(16, mp1_h, mp1_w)\n\n    def forward(self, x):\n        # Distribute learnable layers across all frames with shared weights\n        if self.enable_bnorm: # can use a larger learning rate w/ bnorm #not in config currently\n            x = F.relu(self.bn1(self.TimeDistributed(self.c1, x)))\n            x = F.relu(self.bn2(self.TimeDistributed(self.c2, x)))\n            x = F.dropout(self.bn3(self.TimeDistributed(self.mp1, x)), p=self.dropout, training=self.training)\n            x = F.dropout(self.TimeDistributed(self.flat, x), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.bn4(self.TimeDistributed(self.l1, x))), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.bn5(self.TimeDistributed(self.l2, x))), p=self.dropout, training=self.training)\n        else:\n            x = F.relu(self.TimeDistributed(self.c1, x))\n            x = F.relu(self.TimeDistributed(self.c2, x))\n            x = F.dropout(self.TimeDistributed(self.mp1, x), p=self.dropout, training=self.training)\n            x = F.dropout(self.TimeDistributed(self.flat, x), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.TimeDistributed(self.l1, x)), p=self.dropout, training=self.training)\n            x = F.dropout(F.relu(self.TimeDistributed(self.l2, x)), p=self.dropout, training=self.training)\n        if self.cfg.training_config[\"task_type\"] == \"collision_prediction\":\n            x = x.view(x.shape[0]*x.shape[1],50)\n            x = x.unsqueeze(1)\n        _,(x,_) = self.lstm1(x)\n        x = torch.squeeze(self.l3(x))\n        return F.log_softmax(x, dim=-1)"]}
{"filename": "learning/model/__init__.py", "chunked_list": [""]}
{"filename": "learning/model/rgcn_sag_pooling.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport torch\nfrom torch_geometric.nn import RGCNConv, FastRGCNConv\nfrom torch_geometric.nn.pool.topk_pool import topk, filter_adj\nfrom torch_geometric.utils import softmax\nimport pdb\n\nclass RGCNSAGPooling(torch.nn.Module):\n    def __init__(self, in_channels, num_relations, ratio=0.5, min_score=None,\n                 multiplier=1, nonlinearity=torch.tanh, rgcn_func=\"FastRGCNConv\", **kwargs):\n        super(RGCNSAGPooling, self).__init__()\n\n        self.in_channels = in_channels\n        self.ratio = float(ratio)\n        self.gnn = FastRGCNConv(in_channels, 1, num_relations, **kwargs) if rgcn_func==\"FastRGCNConv\" else RGCNConv(in_channels, 1, num_relations, **kwargs)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.gnn.reset_parameters()\n\n\n    def forward(self, x, edge_index, edge_attr=None, batch=None, attn=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n\n        attn = x if attn is None else attn\n        attn = attn.unsqueeze(-1) if attn.dim() == 1 else attn\n        score = self.gnn(attn, edge_index, edge_attr).view(-1)\n\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(edge_index, edge_attr, perm,\n                                           num_nodes=score.size(0))\n\n        return x, edge_index, edge_attr, batch, perm, score[perm]\n\n\n    def __repr__(self):\n        return '{}({}, {}, {}={}, multiplier={})'.format(\n            self.__class__.__name__, self.gnn.__class__.__name__,\n            self.in_channels,\n            'ratio' if self.min_score is None else 'min_score',\n            self.ratio if self.min_score is None else self.min_score,\n            self.multiplier)", "\nclass RGCNSAGPooling(torch.nn.Module):\n    def __init__(self, in_channels, num_relations, ratio=0.5, min_score=None,\n                 multiplier=1, nonlinearity=torch.tanh, rgcn_func=\"FastRGCNConv\", **kwargs):\n        super(RGCNSAGPooling, self).__init__()\n\n        self.in_channels = in_channels\n        self.ratio = float(ratio)\n        self.gnn = FastRGCNConv(in_channels, 1, num_relations, **kwargs) if rgcn_func==\"FastRGCNConv\" else RGCNConv(in_channels, 1, num_relations, **kwargs)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.gnn.reset_parameters()\n\n\n    def forward(self, x, edge_index, edge_attr=None, batch=None, attn=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n\n        attn = x if attn is None else attn\n        attn = attn.unsqueeze(-1) if attn.dim() == 1 else attn\n        score = self.gnn(attn, edge_index, edge_attr).view(-1)\n\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(edge_index, edge_attr, perm,\n                                           num_nodes=score.size(0))\n\n        return x, edge_index, edge_attr, batch, perm, score[perm]\n\n\n    def __repr__(self):\n        return '{}({}, {}, {}={}, multiplier={})'.format(\n            self.__class__.__name__, self.gnn.__class__.__name__,\n            self.in_channels,\n            'ratio' if self.min_score is None else 'min_score',\n            self.ratio if self.min_score is None else self.min_score,\n            self.multiplier)"]}
{"filename": "learning/util/image_trainer.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys, os\nsys.path.append(os.path.dirname(sys.path[0]))\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom learning.util.trainer import Trainer\nfrom data.dataset import RawImageDataset", "from learning.util.trainer import Trainer\nfrom data.dataset import RawImageDataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom learning.util.metrics import get_metrics, log_im_wandb, log_wandb_categories\n", "from learning.util.metrics import get_metrics, log_im_wandb, log_wandb_categories\n\n\n'''Class implementing image based model training including support for splitting input dataset, \ncross-validation functionality, model inference metrics, and model evaluation.'''\n\nclass Image_Trainer(Trainer):\n    def __init__(self, config, wandb_a = None):\n        '''Class object initialization requires Config Parser object.'''\n        super(Image_Trainer, self).__init__(config, wandb_a)\n\n    def split_dataset(self): #this is init_dataset from multimodal\n        if (self.config.training_config['task_type'] in ['sequence_classification','collision_prediction']):\n            self.training_data, self.testing_data = self.build_real_image_dataset()\n            self.training_labels = np.array([i[1] for i in self.training_data])\n            self.testing_labels = np.array([i[1] for i in self.testing_data])\n            self.total_train_labels = np.concatenate([np.full(len(i[0]), i[1]) for i in self.training_data]) # used to compute frame-level class weighting\n            self.total_test_labels  = np.concatenate([np.full(len(i[0]), i[1]) for i in self.testing_data])\n            self.training_clip_name = np.array([i[2] for i in self.training_data])\n            self.testing_clip_name = np.array([i[2] for i in self.testing_data])\n            self.training_data = np.stack([i[0] for i in self.training_data], axis=0) #resulting shape is (sequence, image, channel, height, width)\n            self.testing_data = np.stack([i[0] for i in self.testing_data], axis=0)\n            if self.config.training_config['task_type'] == \"sequence_classification\":\n              self.class_weights = torch.from_numpy(compute_class_weight('balanced', \n                                                                         classes=np.unique(self.training_labels), \n                                                                         y=self.training_labels))\n              if self.config.training_config[\"n_fold\"] <= 1:\n                  print(\"Number of Training Sequences Included: \", len(self.training_data))\n                  print(\"Number of Testing Sequences Included: \", len(self.testing_data))\n                  print(\"Num of Training Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                  print(\"Num of Testing Labels in Each Class: \" + str(np.unique(self.testing_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights)) \n            elif self.config.training_config['task_type'] == \"collision_prediction\":\n                self.class_weights = torch.from_numpy(compute_class_weight('balanced', \n                                                                           classes=np.unique(self.total_train_labels), \n                                                                           y=self.total_train_labels))\n                if self.config.training_config[\"n_fold\"] <= 1:\n                    print(\"Number of Training Sequences Included: \", len(self.training_data))\n                    print(\"Number of Testing Sequences Included: \", len(self.testing_data))\n                    print(\"Number of Training Labels in Each Class: \" + str(np.unique(self.total_train_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                    print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.total_test_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n        else:\n            raise ValueError('split_dataset(): task type error') \n        \n\n    def prep_dataset(self, image_dataset):\n        class_0 = []\n        class_1 = []\n        \n        print(\"Loading Image Dataset\")\n        for seq in tqdm(image_dataset.labels): # for each seq (num total seq,frame,chan,h,w)\n            category = image_dataset.action_types[seq]\n            if category in self.unique_clips:\n                self.unique_clips[category] += 1\n            else:\n                self.unique_clips[category] = 1\n            seq_data = np.stack([value for value in image_dataset.data[seq].values()], axis=0)\n            if image_dataset.labels[seq] == 0:\n                class_0.append((seq_data,0,category))                                                  \n            elif image_dataset.labels[seq] == 1:\n                class_1.append((seq_data,1,category))\n        y_0 = [0]*len(class_0)  \n        y_1 = [1]*len(class_1)\n        min_number = min(len(class_0), len(class_1))\n        return class_0, class_1, y_0, y_1, min_number\n\n    def build_real_image_dataset(self):\n        '''\n        Returns lists of tuples train and test each containing (data, label, category). \n        This code assumes that all sequences are the same length.\n        '''\n        image_dataset = RawImageDataset()\n        image_dataset.dataset_save_path = self.config.location_data[\"input_path\"]\n        image_dataset = image_dataset.load()\n        self.frame_limit = image_dataset.frame_limit\n        self.color_channels = image_dataset.color_channels\n        self.im_width = image_dataset.im_width\n        self.im_height = image_dataset.im_height\n        class_0, class_1, y_0, y_1, min_number = self.prep_dataset(image_dataset)\n        \n        if self.config.training_config['downsample']: #TODO: fix this code. this only works if class 0 is always the majority class. \n            class_0, y_0 = resample(class_0, y_0, n_samples=min_number)\n        \n        if self.config.location_data[\"transfer_path\"] != None:\n            test_dataset = RawImageDataset()\n            test_dataset.dataset_save_path = self.config.location_data[\"transfer_path\"]\n            test_dataset = test_dataset.load()\n            test_class_0, test_class_1, _, _, _ = self.prep_dataset(test_dataset)\n            train_dataset = shuffle(class_0 + class_1) #training set will consist of the full training dataset\n            test_dataset = shuffle(test_class_0 + test_class_1) #testing set will consist of the full transfer dataset\n            return train_dataset, test_dataset\n        else:\n            train, test, _, _ = train_test_split(class_0+class_1, \n                                                            y_0+y_1, \n                                                            test_size=self.config.training_config['split_ratio'], \n                                                            shuffle=True, \n                                                            stratify=y_0+y_1, \n                                                            random_state=self.config.seed)\n            return train, test\n\n    def train(self):\n        if (self.config.training_config['task_type'] in ['sequence_classification','collision_prediction']):\n            tqdm_bar = tqdm(range(self.config.training_config['epochs']))\n            for epoch_idx in tqdm_bar: # iterate through epoch   \n                acc_loss_train = 0\n                permutation = np.random.permutation(len(self.training_data)) # shuffle dataset before each epoch\n                self.model.train()\n                for i in range(0, len(self.training_data), self.config.training_config['batch_size']): # iterate through batches of the dataset\n                    batch_index = i + self.config.training_config['batch_size'] if i + self.config.training_config['batch_size'] <= len(self.training_data) else len(self.training_data)\n                    indices = permutation[i:batch_index]\n                    batch_x = self.training_data[indices]\n                    batch_x = self.toGPU(batch_x, torch.float32)\n                    if self.config.training_config['task_type']  == 'sequence_classification': \n                      batch_y = self.training_labels[indices] #batch_x = (batch, frames, channel, h, w)\n                    elif self.config.training_config['task_type']  == 'collision_prediction':\n                      batch_y = np.concatenate([np.full(len(self.training_data[i]),self.training_labels[i]) for i in indices]) #batch_x consists of individual frames not sequences/groups of frames, batch_y extends labels of each sequence to all frames in the sequence\n                    batch_y = self.toGPU(batch_y, torch.long)\n                    \n                    output = self.model.forward(batch_x).view(-1, 2)\n                    loss_train = self.loss_func(output, batch_y)\n                    loss_train.backward()\n                    acc_loss_train += loss_train.detach().cpu().item() * len(indices)\n                    self.optimizer.step()\n                    del loss_train\n    \n                acc_loss_train /= len(self.training_data)\n                tqdm_bar.set_description('Epoch: {:04d}, loss_train: {:.4f}'.format(epoch_idx, acc_loss_train))\n                \n                # no cross validation \n                if epoch_idx % self.config.training_config['test_step'] == 0:\n                    self.eval_model(epoch_idx)                   \n        else:\n            raise ValueError('train(): task type error')\n              \n    def model_inference(self, X, y, clip_name):\n        labels = torch.LongTensor().to(self.config.model_config['device'])\n        outputs = torch.FloatTensor().to(self.config.model_config['device'])\n        # Dictionary storing (output, label) pair for all driving categories\n        categories = {'outputs': outputs, 'labels': labels}\n        batch_size = self.config.training_config['batch_size'] # NOTE: set to 1 when profiling or calculating inference time.\n        acc_loss = 0\n        inference_time = 0\n        prof_result = \"\"\n    \n        with torch.autograd.profiler.profile(enabled=False, use_cuda=True) as prof:\n            with torch.no_grad():\n                self.model.eval()\n    \n                for i in range(0, len(X), batch_size): # iterate through subsequences\n                    batch_index = i + batch_size if i + batch_size <= len(X) else len(X)\n                    \n                    batch_x = X[i:batch_index]\n                    batch_x = self.toGPU(batch_x, torch.float32)\n                    if self.config.training_config['task_type']  == 'sequence_classification': \n                      batch_y = y[i:batch_index]  #batch_x = (batch, frames, channel, h, w)\n                    elif self.config.training_config['task_type']  == 'collision_prediction':\n                      batch_y = np.concatenate([np.full(len(X[k]),y[k]) for k in range(i,batch_index)]) #batch_x consists of individual frames not sequences/groups of frames, batch_y extends labels of each sequence to all frames in the sequence\n                    batch_y = self.toGPU(batch_y, torch.long)\n                    batch_clip_name = clip_name[i:batch_index]\n\n                    output = self.model.forward(batch_x).view(-1, 2)\n                    loss_test = self.loss_func(output, batch_y)\n                    acc_loss += loss_test.detach().cpu().item() * len(batch_y)\n                    # store output, label statistics\n                    self.update_categorical_outputs(categories, output, batch_y, batch_clip_name)\n        # calculate one risk score per sequence (this is not implemented for each category)\n        sum_seq_len = 0\n        num_risky_sequences = 0\n        num_safe_sequences = 0\n        correct_risky_seq = 0\n        correct_safe_seq = 0\n        incorrect_risky_seq = 0\n        incorrect_safe_seq = 0\n        sequences = len(categories['labels'])\n        for indices in range(sequences):\n            seq_output = categories['outputs'][indices]\n            label = categories['labels'][indices]\n            pred = torch.argmax(seq_output)\n              \n            # risky clip\n            if label == 1:\n                num_risky_sequences += 1\n                sum_seq_len += seq_output.shape[0]\n                correct_risky_seq += self.correctness(label, pred)\n                incorrect_risky_seq += self.correctness(label, pred)\n            # non-risky clip\n            elif label == 0:\n                num_safe_sequences += 1\n                incorrect_safe_seq += self.correctness(label, pred)\n                correct_safe_seq += self.correctness(label, pred)\n          \n        avg_risky_seq_len = sum_seq_len / num_risky_sequences # sequence length for comparison with the prediction frame metric. \n        seq_tpr = correct_risky_seq / num_risky_sequences\n        seq_fpr = incorrect_safe_seq / num_safe_sequences\n        seq_tnr = correct_safe_seq / num_safe_sequences\n        seq_fnr = incorrect_risky_seq / num_risky_sequences\n        if prof != None:\n            prof_result = prof.key_averages().table(sort_by=\"cuda_time_total\")\n    \n        return  categories, \\\n                  acc_loss/len(X), \\\n                  avg_risky_seq_len, \\\n                  inference_time, \\\n                  prof_result, \\\n                  seq_tpr, \\\n                  seq_fpr, \\\n                  seq_tnr, \\\n                  seq_fnr\n\n    def correctness(self, output, pred):\n        return 1 if output == pred else 0\n\n    def update_categorical_outputs(self, categories, outputs, labels, clip_name):\n        '''\n            Aggregates output, label pairs for every driving category\n        '''\n        n = len(clip_name)\n        for i in range(n):\n            if self.config.training_config['task_type']  == 'sequence_classification': \n              categories['outputs'] = torch.cat([categories['outputs'], torch.unsqueeze(outputs[i], dim=0)], dim=0)\n              categories['labels'] = torch.cat([categories['labels'], torch.unsqueeze(labels[i], dim=0)], dim=0)\n            elif self.config.training_config['task_type']  == 'collision_prediction':\n              temps = [torch.unsqueeze(pred, dim=0) for pred in outputs[i*self.frame_limit: (i+1)*self.frame_limit]] #list of predictions for each frame\n              for temp in temps:\n                categories['outputs'] = torch.cat([categories['outputs'], temp], dim=0) #cat each prediction individually\n              temps = [torch.unsqueeze(pred, dim=0) for pred in labels[i*self.frame_limit: (i+1)*self.frame_limit]] #list of labels for each frame\n              for temp in temps:\n                categories['labels'] = torch.cat([categories['labels'], temp], dim=0) #cat each label individually\n              del temps   \n        # reshape outputs\n        categories['outputs'] = categories['outputs'].reshape(-1, 2)\n    \n    def eval_model(self, current_epoch=None):\n        metrics = {}\n        categories_train, \\\n        acc_loss_train, \\\n        train_avg_seq_len, \\\n        train_inference_time, \\\n        train_profiler_result, \\\n        seq_tpr, seq_fpr, seq_tnr, seq_fnr = self.model_inference(self.training_data, self.training_labels, self.training_clip_name) \n    \n        # Collect metrics from all driving categories\n        metrics['train'] = get_metrics(categories_train['outputs'], categories_train['labels'])\n        metrics['train']['loss'] = acc_loss_train\n        metrics['train']['avg_seq_len'] = train_avg_seq_len\n        metrics['train']['seq_tpr'] = seq_tpr\n        metrics['train']['seq_tnr'] = seq_tnr\n        metrics['train']['seq_fpr'] = seq_fpr\n        metrics['train']['seq_fnr'] = seq_fnr\n\n\n        categories_test, \\\n        acc_loss_test, \\\n        val_avg_seq_len, \\\n        test_inference_time, \\\n        test_profiler_result, \\\n        seq_tpr, seq_fpr, seq_tnr, seq_fnr = self.model_inference(self.testing_data, self.testing_labels, self.testing_clip_name) \n    \n        # Collect metrics from all driving categories\n        metrics['test'] = get_metrics(categories_test['outputs'], categories_test['labels'])\n        metrics['test']['loss'] = acc_loss_test\n        metrics['test']['avg_seq_len'] = val_avg_seq_len\n        metrics['test']['seq_tpr'] = seq_tpr\n        metrics['test']['seq_tnr'] = seq_tnr\n        metrics['test']['seq_fpr'] = seq_fpr\n        metrics['test']['seq_fnr'] = seq_fnr\n        metrics['avg_inf_time'] = (train_inference_time + test_inference_time) / ((len(self.training_labels) + len(self.testing_labels))*5)\n\n    \n           \n        print(\"\\ntrain loss: \" + str(acc_loss_train) + \", acc:\", metrics['train']['acc'], metrics['train']['confusion'], \"mcc:\", metrics['train']['mcc'], \\\n                 \"\\ntest loss: \" +  str(acc_loss_test) + \", acc:\",  metrics['test']['acc'],  metrics['test']['confusion'], \"mcc:\", metrics['test']['mcc'])\n    \n        self.update_im_best_metrics(metrics, current_epoch)\n        metrics['best_epoch'] = self.best_epoch\n        metrics['best_val_loss'] = self.best_val_loss\n        metrics['best_val_acc'] = self.best_val_acc\n        metrics['best_val_auc'] = self.best_val_auc\n        metrics['best_val_conf'] = self.best_val_confusion\n        metrics['best_val_f1'] = self.best_val_f1\n        metrics['best_val_mcc'] = self.best_val_mcc\n        metrics['best_val_acc_balanced'] = self.best_val_acc_balanced\n           \n        if self.config.training_config['n_fold'] <= 1 and self.log:  \n            self.log2wandb(metrics)\n    \n        return categories_train, categories_test, metrics\n   \n    def update_im_best_metrics(self, metrics, current_epoch):\n        if metrics['test']['loss'] < self.best_val_loss:\n            self.best_val_loss = metrics['test']['loss']\n            self.best_epoch = current_epoch if current_epoch != None else self.config.training_config['epochs']\n            self.best_val_acc = metrics['test']['acc']\n            self.best_val_auc = metrics['test']['auc']\n            self.best_val_confusion = metrics['test']['confusion']\n            self.best_val_f1 = metrics['test']['f1']\n            self.best_val_mcc = metrics['test']['mcc']\n            self.best_val_acc_balanced = metrics['test']['balanced_acc']\n\n                    \n    def update_im_cross_valid_metrics(self, categories_train, categories_test, metrics):\n        '''Stores cross-validation metrics for all driving categories'''\n        datasets = ['train', 'test']\n        if self.fold == 1:\n            for dataset in datasets:\n                categories = categories_train if dataset == 'train' else categories_test\n                self.results['outputs'+'_'+dataset] = categories['outputs']\n                self.results['labels'+'_'+dataset] = categories['labels']\n                self.results[dataset] = metrics[dataset]\n                self.results[dataset]['loss'] = metrics[dataset]['loss']\n                self.results[dataset]['avg_seq_len']  = metrics[dataset]['avg_seq_len'] \n                \n                # Best results\n                self.results['avg_inf_time']  = metrics['avg_inf_time']\n                self.results['best_epoch']    = metrics['best_epoch']\n                self.results['best_val_loss'] = metrics['best_val_loss']\n                self.results['best_val_acc']  = metrics['best_val_acc']\n                self.results['best_val_auc']  = metrics['best_val_auc']\n                self.results['best_val_conf'] = metrics['best_val_conf']\n                self.results['best_val_f1']   = metrics['best_val_f1']\n                self.results['best_val_mcc']  = metrics['best_val_mcc']\n                self.results['best_val_acc_balanced'] = metrics['best_val_acc_balanced']\n\n    \n        else:\n            for dataset in datasets:\n                categories = categories_train if dataset == 'train' else categories_test\n                self.results['outputs'+'_'+dataset] = torch.cat((self.results['outputs'+'_'+dataset], categories['outputs']), dim=0)\n                self.results['labels'+'_'+dataset]  = torch.cat((self.results['labels'+'_'+dataset], categories['labels']), dim=0)\n                self.results[dataset]['loss'] = np.append(self.results[dataset]['loss'], metrics[dataset]['loss'])\n                self.results[dataset]['avg_seq_len']  = np.append(self.results[dataset]['avg_seq_len'], metrics[dataset]['avg_seq_len'])\n                \n                # Best results\n                self.results['avg_inf_time']  = np.append(self.results['avg_inf_time'], metrics['avg_inf_time'])\n                self.results['best_epoch']    = np.append(self.results['best_epoch'], metrics['best_epoch'])\n                self.results['best_val_loss'] = np.append(self.results['best_val_loss'], metrics['best_val_loss'])\n                self.results['best_val_acc']  = np.append(self.results['best_val_acc'], metrics['best_val_acc'])\n                self.results['best_val_auc']  = np.append(self.results['best_val_auc'], metrics['best_val_auc'])\n                self.results['best_val_conf'] = np.append(self.results['best_val_conf'], metrics['best_val_conf'])\n                self.results['best_val_f1']   = np.append(self.results['best_val_f1'], metrics['best_val_f1'])\n                self.results['best_val_mcc']  = np.append(self.results['best_val_mcc'], metrics['best_val_mcc'])\n                self.results['best_val_acc_balanced'] = np.append(self.results['best_val_acc_balanced'], metrics['best_val_acc_balanced'])\n\n            \n        # Log final averaged results\n        if self.fold == self.config.training_config['n_fold']:\n            final_metrics = {}\n            for dataset in datasets:\n                final_metrics[dataset] = get_metrics(self.results['outputs'+'_'+dataset], self.results['labels'+'_'+dataset])\n                final_metrics[dataset]['loss'] = np.average(self.results[dataset]['loss'])\n                final_metrics[dataset]['avg_seq_len'] = np.average(self.results[dataset]['avg_seq_len'])\n\n                # Best results\n                final_metrics['avg_inf_time']  = np.average(self.results['avg_inf_time'])\n                final_metrics['best_epoch']    = np.average(self.results['best_epoch'])\n                final_metrics['best_val_loss'] = np.average(self.results['best_val_loss'])\n                final_metrics['best_val_acc']  = np.average(self.results['best_val_acc'])\n                final_metrics['best_val_auc']  = np.average(self.results['best_val_auc'])\n                final_metrics['best_val_conf'] = self.results['best_val_conf']\n                final_metrics['best_val_f1']   = np.average(self.results['best_val_f1'])\n                final_metrics['best_val_mcc']  = np.average(self.results['best_val_mcc'])\n                final_metrics['best_val_acc_balanced'] = np.average(self.results['best_val_acc_balanced'])\n    \n            print('\\nFinal Averaged Results')\n            print(\"\\naverage train loss: \" + str(final_metrics['train']['loss']) + \", average acc:\", final_metrics['train']['acc'], final_metrics['train']['confusion'], final_metrics['train']['auc'], \\\n                \"\\naverage test loss: \" +  str(final_metrics['test']['loss']) + \", average acc:\", final_metrics['test']['acc'],  final_metrics['test']['confusion'], final_metrics['test']['auc'])\n            if self.log:\n                self.log2wandb(final_metrics)\n            \n            # final combined results and metrics\n            return self.results['outputs_train'], self.results['labels_train'], self.results['outputs_test'], self.results['labels_test'], final_metrics\n\n    def cross_valid(self):\n        '''KFold cross validation with similar class distribution in each fold'''\n        skf = StratifiedKFold(n_splits=self.config.training_config[\"n_fold\"])\n        X = np.append(self.training_data, self.testing_data, axis=0)\n        clip_name = np.append(self.training_clip_name, self.testing_clip_name, axis=0)\n        y = np.append(self.training_labels, self.testing_labels, axis=0)\n\n        # self.results stores average metrics for the the n_folds\n        self.results = {}\n        self.fold = 1\n\n        # Split training and testing data based on n_splits (Folds)\n        for train_index, test_index in skf.split(X, y):\n            self.training_data, self.testing_data, self.training_clip_name, self.testing_clip_name, self.training_labels, self.testing_labels = None, None, None, None, None, None #clear vars to save memory\n            X_train, X_test = X[train_index], X[test_index]\n            clip_train, clip_test = clip_name[train_index], clip_name[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            self.class_weights = torch.from_numpy(compute_class_weight('balanced', np.unique(y_train), y_train))\n\n            # Update dataset\n            self.training_data = X_train\n            self.testing_data  = X_test\n            self.training_clip_name = clip_train\n            self.testing_clip_name = clip_test\n            self.training_labels = y_train\n            self.testing_labels  = y_test\n\n            print('\\nFold {}'.format(self.fold))\n            print(\"Number of Training Sequences Included: \", len(X_train))\n            print(\"Number of Testing Sequences Included: \",  len(X_test))\n            print(\"Num of Training Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n            print(\"Num of Testing Labels in Each Class: \"  + str(np.unique(self.testing_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n           \n            self.best_val_loss = 99999\n            self.train()\n            #self.log = True\n            categories_train, categories_test, metrics = self.eval_model(self.fold)\n            self.update_im_cross_valid_metrics(categories_train, categories_test, metrics)\n            #self.log = False\n\n            if self.fold != self.config.training_config[\"n_fold\"]:\n                self.reset_weights(self.model)\n                del self.optimizer\n                self.build_model()\n                \n            self.fold += 1            \n        del self.results\n        \n    def reset_weights(self, model):\n        for layer in model.children():\n            if hasattr(layer, 'reset_parameters'):\n                layer.reset_parameters()\n\n    def update_cross_valid_metrics(self, categories_train, categories_test, metrics):\n            '''\n                Stores cross-validation metrics for all driving categories\n            '''\n            datasets = ['train', 'test']\n            if self.fold == 1:\n                for dataset in datasets:\n                    categories = categories_train if dataset == 'train' else categories_test\n                    for category in self.unique_clips.keys():\n                        if category == 'all':\n                            self.results['outputs'+'_'+dataset] = categories['all']['outputs']\n                            self.results['labels'+'_'+dataset] = categories['all']['labels']\n                            self.results[dataset] = metrics[dataset]\n                            self.results[dataset]['loss'] = metrics[dataset]['loss']\n                            self.results[dataset]['avg_seq_len']  = metrics[dataset]['avg_seq_len'] \n                            \n                            # Best results\n                            self.results['avg_inf_time']  = metrics['avg_inf_time']\n                            self.results['best_epoch']    = metrics['best_epoch']\n                            self.results['best_val_loss'] = metrics['best_val_loss']\n                            self.results['best_val_acc']  = metrics['best_val_acc']\n                            self.results['best_val_auc']  = metrics['best_val_auc']\n                            self.results['best_val_conf'] = metrics['best_val_conf']\n                            self.results['best_val_f1']   = metrics['best_val_f1']\n                            self.results['best_val_mcc']  = metrics['best_val_mcc']\n                            self.results['best_val_acc_balanced'] = metrics['best_val_acc_balanced']\n                        elif category == \"lanechange\":\n                            self.results['outputs'+'_'+dataset] = categories['all']['outputs']\n                            self.results['labels'+'_'+dataset] = categories['all']['labels']\n                            self.results[dataset] = metrics[dataset]\n                            self.results[dataset]['loss'] = metrics[dataset]['loss']\n                            self.results[dataset]['avg_seq_len']  = metrics[dataset]['avg_seq_len'] \n                            \n                            # Best results\n                            self.results['avg_inf_time']  = metrics['avg_inf_time']\n                            self.results['best_epoch']    = metrics['best_epoch']\n                            self.results['best_val_loss'] = metrics['best_val_loss']\n                            self.results['best_val_acc']  = metrics['best_val_acc']\n                            self.results['best_val_auc']  = metrics['best_val_auc']\n                            self.results['best_val_conf'] = metrics['best_val_conf']\n                            self.results['best_val_f1']   = metrics['best_val_f1']\n                            self.results['best_val_mcc']  = metrics['best_val_mcc']\n                            self.results['best_val_acc_balanced'] = metrics['best_val_acc_balanced']                            \n                        else:\n                            self.results[dataset][category]['outputs'] = categories[category]['outputs']\n                            self.results[dataset][category]['labels'] = categories[category]['labels']\n    \n            else:\n                for dataset in datasets:\n                    categories = categories_train if dataset == 'train' else categories_test\n                    for category in self.unique_clips.keys():\n                        if category == 'all':\n                            self.results['outputs'+'_'+dataset] = torch.cat((self.results['outputs'+'_'+dataset], categories['all']['outputs']), dim=0)\n                            self.results['labels'+'_'+dataset]  = torch.cat((self.results['labels'+'_'+dataset], categories['all']['labels']), dim=0)\n                            self.results[dataset]['loss'] = np.append(self.results[dataset]['loss'], metrics[dataset]['loss'])\n                            self.results[dataset]['avg_seq_len']  = np.append(self.results[dataset]['avg_seq_len'], metrics[dataset]['avg_seq_len'])\n                            \n                            # Best results\n                            self.results['avg_inf_time']  = np.append(self.results['avg_inf_time'], metrics['avg_inf_time'])\n                            self.results['best_epoch']    = np.append(self.results['best_epoch'], metrics['best_epoch'])\n                            self.results['best_val_loss'] = np.append(self.results['best_val_loss'], metrics['best_val_loss'])\n                            self.results['best_val_acc']  = np.append(self.results['best_val_acc'], metrics['best_val_acc'])\n                            self.results['best_val_auc']  = np.append(self.results['best_val_auc'], metrics['best_val_auc'])\n                            self.results['best_val_conf'] = np.append(self.results['best_val_conf'], metrics['best_val_conf'])\n                            self.results['best_val_f1']   = np.append(self.results['best_val_f1'], metrics['best_val_f1'])\n                            self.results['best_val_mcc']  = np.append(self.results['best_val_mcc'], metrics['best_val_mcc'])\n                            self.results['best_val_acc_balanced'] = np.append(self.results['best_val_acc_balanced'], metrics['best_val_acc_balanced'])\n                        elif category == \"lanechange\":\n                            self.results['outputs'+'_'+dataset] = torch.cat((self.results['outputs'+'_'+dataset], categories['all']['outputs']), dim=0)\n                            self.results['labels'+'_'+dataset]  = torch.cat((self.results['labels'+'_'+dataset], categories['all']['labels']), dim=0)\n                            self.results[dataset]['loss'] = np.append(self.results[dataset]['loss'], metrics[dataset]['loss'])\n                            self.results[dataset]['avg_seq_len']  = np.append(self.results[dataset]['avg_seq_len'], metrics[dataset]['avg_seq_len'])\n                            \n                            # Best results\n                            self.results['avg_inf_time']  = np.append(self.results['avg_inf_time'], metrics['avg_inf_time'])\n                            self.results['best_epoch']    = np.append(self.results['best_epoch'], metrics['best_epoch'])\n                            self.results['best_val_loss'] = np.append(self.results['best_val_loss'], metrics['best_val_loss'])\n                            self.results['best_val_acc']  = np.append(self.results['best_val_acc'], metrics['best_val_acc'])\n                            self.results['best_val_auc']  = np.append(self.results['best_val_auc'], metrics['best_val_auc'])\n                            self.results['best_val_conf'] = np.append(self.results['best_val_conf'], metrics['best_val_conf'])\n                            self.results['best_val_f1']   = np.append(self.results['best_val_f1'], metrics['best_val_f1'])\n                            self.results['best_val_mcc']  = np.append(self.results['best_val_mcc'], metrics['best_val_mcc'])\n                            self.results['best_val_acc_balanced'] = np.append(self.results['best_val_acc_balanced'], metrics['best_val_acc_balanced'])\n                        else:\n                            self.results[dataset][category]['outputs'] = torch.cat((self.results[dataset][category]['outputs'], categories[category]['outputs']), dim=0)\n                            self.results[dataset][category]['labels']  = torch.cat((self.results[dataset][category]['labels'], categories[category]['labels']), dim=0)\n                \n            # Log final averaged results\n            if self.fold ==  self.config.training_config[\"n_fold\"]:\n                final_metrics = {}\n                for dataset in datasets:\n                    for category in self.unique_clips.keys():\n                        if category == 'all':\n                            final_metrics[dataset] = get_metrics(self.results['outputs'+'_'+dataset], self.results['labels'+'_'+dataset])\n                            final_metrics[dataset]['loss'] = np.average(self.results[dataset]['loss'])\n                            final_metrics[dataset]['avg_seq_len'] = np.average(self.results[dataset]['avg_seq_len'])\n    \n                            # Best results\n                            final_metrics['avg_inf_time']  = np.average(self.results['avg_inf_time'])\n                            final_metrics['best_epoch']    = np.average(self.results['best_epoch'])\n                            final_metrics['best_val_loss'] = np.average(self.results['best_val_loss'])\n                            final_metrics['best_val_acc']  = np.average(self.results['best_val_acc'])\n                            final_metrics['best_val_auc']  = np.average(self.results['best_val_auc'])\n                            final_metrics['best_val_conf'] = self.results['best_val_conf']\n                            final_metrics['best_val_f1']   = np.average(self.results['best_val_f1'])\n                            final_metrics['best_val_mcc']  = np.average(self.results['best_val_mcc'])\n                            final_metrics['best_val_acc_balanced'] = np.average(self.results['best_val_acc_balanced'])\n                        elif category == 'lanechange':\n                            final_metrics[dataset] = get_metrics(self.results['outputs'+'_'+dataset], self.results['labels'+'_'+dataset])\n                            final_metrics[dataset]['loss'] = np.average(self.results[dataset]['loss'])\n                            final_metrics[dataset]['avg_seq_len'] = np.average(self.results[dataset]['avg_seq_len'])\n    \n                            # Best results\n                            final_metrics['avg_inf_time']  = np.average(self.results['avg_inf_time'])\n                            final_metrics['best_epoch']    = np.average(self.results['best_epoch'])\n                            final_metrics['best_val_loss'] = np.average(self.results['best_val_loss'])\n                            final_metrics['best_val_acc']  = np.average(self.results['best_val_acc'])\n                            final_metrics['best_val_auc']  = np.average(self.results['best_val_auc'])\n                            final_metrics['best_val_conf'] = self.results['best_val_conf']\n                            final_metrics['best_val_f1']   = np.average(self.results['best_val_f1'])\n                            final_metrics['best_val_mcc']  = np.average(self.results['best_val_mcc'])\n                            final_metrics['best_val_acc_balanced'] = np.average(self.results['best_val_acc_balanced'])\n                        else: \n                            final_metrics[dataset][category] = get_metrics(self.results[dataset][category]['outputs'], self.results[dataset][category]['labels'])\n    \n                print('\\nFinal Averaged Results')\n                print(\"\\naverage train loss: \" + str(final_metrics['train']['loss']) + \", average acc:\", final_metrics['train']['acc'], final_metrics['train']['confusion'], final_metrics['train']['auc'], \\\n                    \"\\naverage test loss: \" +  str(final_metrics['test']['loss']) + \", average acc:\", final_metrics['test']['acc'],  final_metrics['test']['confusion'], final_metrics['test']['auc'])\n                if self.log:\n                    self.log2wandb(final_metrics)\n                \n                # final combined results and metrics\n                return self.results['outputs_train'], self.results['labels_train'], self.results['outputs_test'], self.results['labels_test'], final_metrics\n\n\n        \n    def log2wandb(self, metrics):\n        '''\n            Log metrics from all driving categories\n        '''\n        for category in self.unique_clips.keys():\n            if category == 'all':\n                log_im_wandb(metrics)\n            elif category == 'lanechange':\n                log_im_wandb(metrics)\n            else:\n                log_wandb_categories(metrics, id=category)"]}
{"filename": "learning/util/metrics.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score, \\\n    roc_curve, balanced_accuracy_score, matthews_corrcoef", "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score, \\\n    roc_curve, balanced_accuracy_score, matthews_corrcoef\nimport torch_geometric\nimport pdb\nimport torch\n\n#this file contains functions for scoring the prediction models.\n\n'''\n#Expected Inputs:", "'''\n#Expected Inputs:\noutputs: (n, 2) FloatTensor\nlabels: (n,) LongTensor\n'''\ndef get_metrics(outputs, labels):\n    labels_tensor = labels.cpu()\n    outputs_tensor = outputs.cpu()\n    preds = outputs_tensor.max(1)[1].type_as(labels_tensor).cpu() #binarized version of outputs_tensor.\n\n    metrics = {}\n    metrics['acc'] = accuracy_score(labels_tensor, preds)\n    metrics['f1'] = f1_score(labels_tensor, preds, average=\"binary\")\n    conf = confusion_matrix(labels_tensor, preds)\n    if len(conf) > 1:\n        metrics['fpr'] = conf[0][1] / (conf[0][1] + conf[0][0]) #FPR = FP/(FP+TN)\n        metrics['tnr'] = conf[0][0] / (conf[0][1] + conf[0][0]) #TNR = TN/(FP+TN)\n        metrics['fnr'] = conf[1][0] / (conf[1][0] + conf[1][1]) #FNR = FN/(FN+TP)\n    metrics['confusion'] = str(conf).replace('\\n', ',')\n    metrics['precision'] = precision_score(labels_tensor, preds, average=\"binary\")\n    metrics['recall'] = recall_score(labels_tensor, preds, average=\"binary\") #recall and TPR are the same. TPR = TP/(TP+FN)\n    metrics['auc'] = get_auc(outputs_tensor, labels_tensor)\n    metrics['label_distribution'] = str(np.unique(labels_tensor, return_counts=True)[1])\n    metrics['balanced_acc'] = balanced_accuracy_score(labels_tensor, preds)\n    metrics['mcc'] = matthews_corrcoef(labels_tensor, preds)\n    \n    return metrics ", "\n#returns onehot version of labels. can specify n_classes to force onehot size.\ndef encode_onehot(labels, n_classes=None):\n    if(n_classes):\n        classes = set(range(n_classes))\n    else:\n        classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n                    enumerate(classes)}\n    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n                             dtype=np.int32)\n    return labels_onehot", "\n#log data to to Weights & Biases\ndef log_wandb(metrics):\n    wandb.log({\n        \"train_acc\": metrics['train']['acc'],\n        \"val_acc\": metrics['test']['acc'],\n        \"train_acc_balanced\": metrics['train']['balanced_acc'],\n        \"val_acc_balanced\": metrics['test']['balanced_acc'],\n        \"train_loss\": metrics['train']['loss'],\n        \"val_loss\": metrics['test']['loss'],\n        'train_auc': metrics['train']['auc'],\n        'train_f1': metrics['train']['f1'],\n        'val_auc': metrics['test']['auc'],\n        'val_f1': metrics['test']['f1'],\n        'train_precision': metrics['train']['precision'],\n        'train_recall': metrics['train']['recall'],\n        'val_precision': metrics['test']['precision'],\n        'val_recall': metrics['test']['recall'],\n        'train_conf': metrics['train']['confusion'],\n        'val_conf': metrics['test']['confusion'],\n        'train_fpr': metrics['train']['fpr'],\n        'train_tnr': metrics['train']['tnr'],\n        'train_fnr': metrics['train']['fnr'],\n        'val_fpr': metrics['test']['fpr'],\n        'val_tnr': metrics['test']['tnr'],\n        'val_fnr': metrics['test']['fnr'],\n        'train_avg_seq_len': metrics['train']['avg_seq_len'],\n        'train_avg_pred_frame': metrics['train']['avg_prediction_frame'],\n        'val_avg_seq_len': metrics['test']['avg_seq_len'],\n        'val_avg_pred_frame': metrics['test']['avg_prediction_frame'],\n        'train_avg_pred_risky_indices': metrics['train']['avg_predicted_risky_indices'],\n        'train_avg_pred_safe_indices': metrics['train']['avg_predicted_safe_indices'],\n        'val_avg_pred_risky_indices': metrics['test']['avg_predicted_risky_indices'],\n        'val_avg_pred_safe_indices': metrics['test']['avg_predicted_safe_indices'],\n        'best_epoch': metrics['best_epoch'],\n        'best_val_loss': metrics['best_val_loss'],\n        'best_val_acc': metrics['best_val_acc'],\n        'best_val_auc': metrics['best_val_auc'],\n        'best_val_conf': metrics['best_val_conf'],\n        'best_val_mcc': metrics['best_val_mcc'],\n        'best_val_acc_balanced': metrics['best_val_acc_balanced'],\n        'train_mcc': metrics['train']['mcc'],\n        'val_mcc': metrics['test']['mcc'],\n        'avg_inf_time': metrics['avg_inf_time'],\n        'best_avg_pred_frame': metrics['best_avg_pred_frame'],\n    })", "    \n\ndef log_wandb_transfer_learning(metrics):\n    wandb.log({\n        \"val_acc\": metrics['test']['acc'],\n        \"val_acc_balanced\": metrics['test']['balanced_acc'],\n        \"val_loss\": metrics['test']['loss'],\n        'val_auc': metrics['test']['auc'],\n        'val_f1': metrics['test']['f1'],\n        'val_precision': metrics['test']['precision'],\n        'val_recall': metrics['test']['recall'],\n        'val_conf': metrics['test']['confusion'],\n        'val_fpr': metrics['test']['fpr'],\n        'val_tnr': metrics['test']['tnr'],\n        'val_fnr': metrics['test']['fnr'],\n        'val_avg_seq_len': metrics['test']['avg_seq_len'],\n        'val_avg_pred_frame': metrics['test']['avg_prediction_frame'],\n        'val_avg_pred_risky_indices': metrics['test']['avg_predicted_risky_indices'],\n        'val_avg_pred_safe_indices': metrics['test']['avg_predicted_safe_indices'],\n        'best_epoch': metrics['best_epoch'],\n        'best_val_loss': metrics['best_val_loss'],\n        'best_val_acc': metrics['best_val_acc'],\n        'best_val_auc': metrics['best_val_auc'],\n        'best_val_conf': metrics['best_val_conf'],\n        'best_val_mcc': metrics['best_val_mcc'],\n        'best_val_acc_balanced': metrics['best_val_acc_balanced'],\n        'val_mcc': metrics['test']['mcc'],\n        'avg_inf_time': metrics['avg_inf_time'],\n        'best_avg_pred_frame': metrics['best_avg_pred_frame'],\n        'test_seq_tpr': metrics['test']['seq_tpr'],\n        'test_seq_tnr': metrics['test']['seq_tnr'],\n        'test_seq_fpr': metrics['test']['seq_fpr'],\n        'test_seq_fnr': metrics['test']['seq_fnr']\n    })", "\ndef log_im_wandb(metrics):\n    wandb.log({\n        \"train_acc\": metrics['train']['acc'],\n        \"val_acc\": metrics['test']['acc'],\n        \"train_acc_balanced\": metrics['train']['balanced_acc'],\n        \"val_acc_balanced\": metrics['test']['balanced_acc'],\n        \"train_loss\": metrics['train']['loss'],\n        \"val_loss\": metrics['test']['loss'],\n        'train_auc': metrics['train']['auc'],\n        'train_f1': metrics['train']['f1'],\n        'val_auc': metrics['test']['auc'],\n        'val_f1': metrics['test']['f1'],\n        'train_precision': metrics['train']['precision'],\n        'train_recall': metrics['train']['recall'],\n        'val_precision': metrics['test']['precision'],\n        'val_recall': metrics['test']['recall'],\n        'train_conf': metrics['train']['confusion'],\n        'val_conf': metrics['test']['confusion'],\n        'train_fpr': metrics['train']['fpr'],\n        'train_tnr': metrics['train']['tnr'],\n        'train_fnr': metrics['train']['fnr'],\n        'val_fpr': metrics['test']['fpr'],\n        'val_tnr': metrics['test']['tnr'],\n        'val_fnr': metrics['test']['fnr'],\n        'train_avg_seq_len': metrics['train']['avg_seq_len'],\n        'val_avg_seq_len': metrics['test']['avg_seq_len'],\n        'best_epoch': metrics['best_epoch'],\n        'best_val_loss': metrics['best_val_loss'],\n        'best_val_acc': metrics['best_val_acc'],\n        'best_val_auc': metrics['best_val_auc'],\n        'best_val_conf': metrics['best_val_conf'],\n        'best_val_mcc': metrics['best_val_mcc'],\n        'best_val_acc_balanced': metrics['best_val_acc_balanced'],\n        'train_mcc': metrics['train']['mcc'],\n        'val_mcc': metrics['test']['mcc'],\n        'avg_inf_time': metrics['avg_inf_time'],\n    })", "\ndef log_wandb_categories(metrics, id):\n    wandb.log({\n        \"train_acc\"+\"_\"+id: metrics['train'][id]['acc'],\n        \"val_acc\"+\"_\"+id: metrics['test'][id]['acc'],\n        \"train_acc_balanced\"+\"_\"+id: metrics['train'][id]['balanced_acc'],\n        \"val_acc_balanced\"+\"_\"+id: metrics['test'][id]['balanced_acc'],\n        'train_auc'+\"_\"+id: metrics['train'][id]['auc'],\n        'train_f1'+\"_\"+id: metrics['train'][id]['f1'],\n        'val_auc'+\"_\"+id: metrics['test'][id]['auc'],\n        'val_f1'+\"_\"+id: metrics['test'][id]['f1'],\n        'train_precision'+\"_\"+id: metrics['train'][id]['precision'],\n        'train_recall'+\"_\"+id: metrics['train'][id]['recall'],\n        'val_precision'+\"_\"+id: metrics['test'][id]['precision'],\n        'val_recall'+\"_\"+id: metrics['test'][id]['recall'],\n        'train_conf'+\"_\"+id: metrics['train'][id]['confusion'],\n        'val_conf'+\"_\"+id: metrics['test'][id]['confusion'],\n        'train_fpr'+\"_\"+id: metrics['train'][id]['fpr'],\n        'train_tnr'+\"_\"+id: metrics['train'][id]['tnr'],\n        'train_fnr'+\"_\"+id: metrics['train'][id]['fnr'],\n        'val_fpr'+\"_\"+id: metrics['test'][id]['fpr'],\n        'val_tnr'+\"_\"+id: metrics['test'][id]['tnr'],\n        'val_fnr'+\"_\"+id: metrics['test'][id]['fnr'],\n        'train_mcc'+\"_\"+id: metrics['train'][id]['mcc'],\n        'val_mcc'+\"_\"+id: metrics['test'][id]['mcc'],\n})", "#~~~~~~~~~~Scoring Metrics~~~~~~~~~~\n#note: these scoring metrics only work properly for binary classification use cases (graph classification, dyngraph classification) \ndef get_auc(outputs, labels):\n    try:    \n        labels = encode_onehot(labels.numpy().tolist(), 2) #binary labels\n        auc = roc_auc_score(labels, outputs.numpy(), average=\"micro\")\n    except ValueError as err: \n        print(\"error calculating AUC: \", err)\n        auc = 0.0\n    return auc", "\n#NOTE: ROC curve is only generated for positive class (risky label) confidence values \n#render parameter determines if the figure is actually generated. If false, it saves the values to a csv file.\ndef get_roc_curve(outputs, labels, render=False):\n    risk_scores = []\n    outputs = preprocessing.normalize(outputs.numpy(), axis=0)\n    for i in outputs:\n        risk_scores.append(i[1])\n    fpr, tpr, thresholds = roc_curve(labels.numpy(), risk_scores)\n    roc = pd.DataFrame()\n    roc['fpr'] = fpr\n    roc['tpr'] = tpr\n    roc['thresholds'] = thresholds\n    roc.to_csv(\"ROC_data.csv\")\n\n    if(render):\n        plt.figure(figsize=(8,8))\n        plt.xlim((0,1))\n        plt.ylim((0,1))\n        plt.ylabel(\"TPR\")\n        plt.xlabel(\"FPR\")\n        plt.title(\"Receiver Operating Characteristic\")\n        plt.plot([0,1],[0,1], linestyle='dashed')\n        plt.plot(fpr,tpr, linewidth=2)\n        plt.savefig(\"ROC_curve.svg\")", "\n\n#~~~~~Graph Metrics~~~~~\n\ndef get_graph_metrics(graphs):\n    '''get average degree, average number of edges, and eigenvector centrality across all graphs'''\n    metrics = {'avg_degree' : 0,\n                'avg_num_edges' : 0,\n                'std_num_edges' : 0,\n                'avg_eigenvector_centrality' : 0} #TODO\n    edge_count_list = []\n    for graph in graphs:\n        metrics['avg_degree'] += torch.sum(torch_geometric.utils.degree(graph['edge_index'][0])).item()/len(graph['node_embeddings'])\n        edge_count_list.append(len(graph['edge_index'][0]))\n    metrics['avg_degree'] /= len(graphs)\n    metrics['avg_num_edges'] = sum(edge_count_list) / len(graphs)\n    metrics['std_num_edges'] = np.std(edge_count_list)\n    return metrics", ""]}
{"filename": "learning/util/__init__.py", "chunked_list": [""]}
{"filename": "learning/util/rs2g_trainer.py", "chunked_list": ["import sys, os\nsys.path.append(os.path.dirname(sys.path[0]))\n\nimport torch\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, StratifiedKFold", "from sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tqdm import tqdm\nfrom learning.util.trainer import Trainer\nfrom data.dataset import SceneGraphDataset\nfrom torch_geometric.loader import DataListLoader\nfrom learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning, get_graph_metrics\n\n\n'''trainer for RS2G data-driven MRGCN model.'''", "\n'''trainer for RS2G data-driven MRGCN model.'''\n\nclass RS2G_Trainer(Trainer):\n    def __init__(self, config, wandb_a = None):\n        super(RS2G_Trainer, self).__init__(config, wandb_a)\n        self.scene_graph_dataset = SceneGraphDataset()\n        self.feature_list = list()\n        for i in range(self.config.model_config['num_of_classes']):\n            self.feature_list.append(\"type_\"+str(i))\n        self.device = self.config.model_config['device']\n\n\n    def split_dataset(self):\n        if (self.config.training_config['task_type'] in ['sequence_classification','graph_classification','collision_prediction']):\n            self.training_data, self.testing_data = self.build_scenegraph_dataset()\n            self.total_train_labels = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.training_data]) # used to compute frame-level class weighting\n            self.total_test_labels  = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.testing_data])\n            self.training_labels = [data['label'] for data in self.training_data]\n            self.testing_labels  = [data['label'] for data in self.testing_data]\n            if self.config.training_config['task_type'] == 'sequence_classification':\n                self.class_weights = torch.from_numpy(compute_class_weight('balanced', classes=np.unique(self.training_labels), y=self.training_labels))\n                if self.config.training_config[\"n_fold\"] <= 1:\n                    print(\"Number of Sequences Included: \", len(self.training_data))\n                    print(\"Number of Testing Sequences Included: \", len(self.testing_data))\n                    print(\"Num Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                    print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.testing_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n            elif self.config.training_config['task_type'] == 'collision_prediction':\n                self.class_weights = torch.from_numpy(compute_class_weight('balanced', classes=np.unique(self.total_train_labels), y=self.total_train_labels))\n                if self.config.training_config[\"n_fold\"] <= 1:\n                    print(\"Number of Training Sequences Included: \", len(self.training_data))\n                    print(\"Number of Testing Sequences Included: \", len(self.testing_data))\n                    print(\"Number of Training Labels in Each Class: \" + str(np.unique(self.total_train_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                    print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.total_test_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n        else:\n            raise ValueError('split_dataset(): task type error') \n     \n\n    def build_transfer_learning_dataset(self): #this creates test dataset for transfer learning\n        scene_graph_dataset  = SceneGraphDataset()\n        scene_graph_dataset.dataset_save_path = self.config.location_data[\"transfer_path\"]\n        self.scene_graph_dataset = scene_graph_dataset.load()\n\n        self.transfer_data= []\n        sorted_seq = sorted(self.scene_graph_dataset.labels)\n        if self.config.training_config[\"scenegraph_dataset_type\"] == \"carla\":\n            for ind, seq in enumerate(sorted_seq): #for each seq in labels\n                data_to_append = {\"sequence\":self.scene_graph_dataset.process_carla_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                self.transfer_data.append(data_to_append)\n                    \n        elif self.config.training_config[\"scenegraph_dataset_type\"] == \"real\":\n            for ind, seq in enumerate(sorted_seq): \n                data_to_append = {\"sequence\": self.scene_graph_dataset.scene_graphs[seq], \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                self.transfer_data.append(data_to_append)\n        else:\n            raise ValueError('dataset_type unrecognized')\n                \n        self.total_transfer_data_labels = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.transfer_data])\n        self.transfer_data_labels = [data['label'] for data in self.transfer_data]\n        print('Running transfer learning on dataset: ', self.config.location_data[\"transfer_path\"])\n        print('total labels: ', len(self.transfer_data_labels))\n        self.class_weights = torch.tensor([1.0, 1.0])\n\n\n    def evaluate_transfer_learning(self, current_epoch=None):\n        metrics = {}\n        outputs_test, \\\n        labels_test, \\\n        acc_loss_test, \\\n        val_avg_prediction_frame, \\\n        val_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        test_inference_time, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.transfer_data, self.transfer_data_labels)\n\n        metrics['test'] = get_metrics(outputs_test, labels_test)\n        metrics['test']['loss'] = acc_loss_test\n        metrics['test']['avg_prediction_frame'] = val_avg_prediction_frame\n        metrics['test']['avg_seq_len'] = val_avg_seq_len\n        metrics['test']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['test']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['test']['seq_tpr'] = seq_tpr\n        metrics['test']['seq_tnr'] = seq_tnr\n        metrics['test']['seq_fpr'] = seq_fpr\n        metrics['test']['seq_fnr'] = seq_fnr\n        metrics['avg_inf_time'] = (test_inference_time) / (len(labels_test))\n\n\n        self.update_sg_best_metrics(metrics, current_epoch)\n        metrics['best_epoch'] = self.best_epoch\n        metrics['best_val_loss'] = self.best_val_loss\n        metrics['best_val_acc'] = self.best_val_acc\n        metrics['best_val_auc'] = self.best_val_auc\n        metrics['best_val_conf'] = self.best_val_confusion\n        metrics['best_val_f1'] = self.best_val_f1\n        metrics['best_val_mcc'] = self.best_val_mcc\n        metrics['best_val_acc_balanced'] = self.best_val_acc_balanced\n        metrics['best_avg_pred_frame'] = self.best_avg_pred_frame\n        \n        if self.config.training_config[\"n_fold\"] <= 1 and self.log:\n            log_wandb_transfer_learning(metrics)\n        \n        return outputs_test, labels_test, metrics\n\n\n    def build_scenegraph_dataset(self): #this creates test and training datasets\n        '''\n        Dataset format \n            scenegraphs_sequence: dict_keys(['sequence', 'label', 'folder_name'])\n                'sequence': scenegraph metadata\n                'label': classification output [0 -> non_risky (negative), 1 -> risky (positive)]\n                'folder_name': foldername storing sequence data\n    \n        Dataset modes\n            no downsample\n                all sequences used for the train and test set regardless of class distribution\n            downsample  \n                equal amount of positive and negative sequences used for the train and test set\n            transfer \n                replaces original test set with another dataset \n        '''\n        #Load sg dataset obj\n        scene_graph_dataset  = SceneGraphDataset()\n        scene_graph_dataset.dataset_save_path = self.config.location_data[\"input_path\"]\n        self.scene_graph_dataset = scene_graph_dataset.load()     \n    \n        class_0 = []\n        class_1 = []\n        sorted_seq = sorted(self.scene_graph_dataset.labels)\n        \n        \n        if self.config.training_config[\"scenegraph_dataset_type\"] == \"carla\":\n            for ind, seq in enumerate(sorted_seq): #for each seq in labels\n                data_to_append = {\"sequence\": self.scene_graph_dataset.scene_graphs[seq], \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                if self.scene_graph_dataset.labels[seq] == 0:\n                    class_0.append(data_to_append) \n                elif self.scene_graph_dataset.labels[seq] == 1:\n                    class_1.append(data_to_append)\n                    \n        elif self.config.training_config[\"scenegraph_dataset_type\"] == \"real\":\n            if self.config.training_config[\"scenegraph_data_format\"] == \"carla\":\n                for ind, seq in enumerate(sorted_seq): #for each seq in labels\n                    ego_idx = sorted(self.scene_graph_dataset.scene_graphs[seq].keys())[0]\n                    data_to_append = {\"sequence\": self.scene_graph_dataset.scene_graphs[seq], \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                    if self.scene_graph_dataset.labels[seq] == 0:\n                        class_0.append(data_to_append) \n                    elif self.scene_graph_dataset.labels[seq] == 1:\n                        class_1.append(data_to_append)\n            \n            elif self.config.training_config[\"scenegraph_data_format\"] == \"honda\":\n                for ind, seq in enumerate(sorted_seq): \n                    data_to_append = {\"sequence\": self.scene_graph_dataset.scene_graphs[seq], \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                    if self.scene_graph_dataset.labels[seq] == 0:\n                        class_0.append(data_to_append)  \n                    elif self.scene_graph_dataset.labels[seq] == 1:\n                        class_1.append(data_to_append)\n            else: \n                raise ValueError('scenegraph_data_format not recognized') \n\n        else:\n            raise ValueError('scenegraph_dataset_type not recognized') \n        \n        y_0 = [0]*len(class_0)  \n        y_1 = [1]*len(class_1)\n        min_number = min(len(class_0), len(class_1))\n        downsample = self.config.training_config[\"downsample\"]\n        \n        if downsample:\n            modified_class_0, modified_y_0 = resample(class_0, y_0, n_samples=min_number)\n        else:\n            modified_class_0, modified_y_0 = class_0, y_0\n        train, test, train_y, test_y = train_test_split(modified_class_0+class_1, modified_y_0+y_1, test_size=self.config.training_config[\"split_ratio\"], shuffle=True, stratify=modified_y_0+y_1, random_state=self.config.seed)\n        if self.config.location_data[\"transfer_path\"] != None:\n            self.build_transfer_learning_dataset()\n        return train, test\n\n\n    '''preprocess nodes for the self supervision task'''\n    def preprocess_nodes(self, nodes):\n        actor_names = self.config.relation_extraction_settings[\"ACTOR_NAMES\"]\n        node_feature_list = []\n        for node_id in nodes.keys():\n            node_features = []\n            if nodes[node_id]['label'] in ['Root Road']:\n                continue\n            if nodes[node_id]['label'] == 'ego car':\n                nodes[node_id]['attr'].update({'left': 640-100,\n                                                'top': 620,\n                                                'right': 640+100,\n                                                'bottom': 720})\n                nodes[node_id]['label'] = \"ego car_\" \n            if nodes[node_id]['label'] == 'Left Lane':\n                nodes[node_id]['attr'].update({'left': 0,\n                                                'top': 0,\n                                                'right': 0,\n                                                'bottom': 0})\n                nodes[node_id]['label'] = \"lane_l\"\n            if nodes[node_id]['label'] == 'Right Lane':\n                nodes[node_id]['attr'].update({'left': 1280,\n                                                'top': 0,\n                                                'right': 1280,\n                                                'bottom': 0})\n                nodes[node_id]['label'] = \"lane_r\" \n            if nodes[node_id]['label'] == 'Middle Lane':\n                nodes[node_id]['attr'].update({'left': 640,\n                                                'top': 0,\n                                                'right': 640,\n                                                'bottom': 0})\n                nodes[node_id]['label'] = \"lane_m\"\n            node_features = [0.0] * 15 #new format = [Left, Top, Right, Bottom, W, H, T] \n            node_features[0] = nodes[node_id]['attr']['left'] / 1280\n            node_features[1] = nodes[node_id]['attr']['top'] / 720\n            node_features[2] = nodes[node_id]['attr']['right'] / 1280\n            node_features[3] = nodes[node_id]['attr']['bottom'] / 720\n            node_features[4] = node_features[2] - node_features[0] / 1280\n            node_features[5] = node_features[3] - node_features[1] / 720\n            node_features[6 + actor_names.index(nodes[node_id]['label'].split('_')[0])] = 1\n            node_features = torch.FloatTensor(node_features)\n            node_feature_list.append(node_features)\n        return node_feature_list\n\n\n    def train(self):\n        if (self.config.training_config['task_type'] in ['sequence_classification','graph_classification','collision_prediction']):\n            tqdm_bar = tqdm(range(self.config.training_config['epochs']))\n    \n            for epoch_idx in tqdm_bar: # iterate through epoch   \n                acc_loss_train = 0\n                self.sequence_loader = DataListLoader(self.training_data, batch_size=self.config.training_config[\"batch_size\"])\n                for data_list in self.sequence_loader: # iterate through batches of the dataset\n                    self.model.train()\n                    self.optimizer.zero_grad()\n                    labels = torch.empty(0).long().to(self.device)\n                    outputs = torch.empty(0,2).to(self.device)\n                    for sequence in data_list: # iterate through scene-graph sequences in the batch\n                        data, label = sequence['sequence'], sequence['label'] #data= list of SceneGraph\n                        new_sequence = []\n                        \n                        data = [data[frame] for frame in sorted(data.keys())] if type(data) == dict else data\n                        for d in data: #this loop extracts a list of nodes and their attributes for each frame.\n                            g = d.g\n                            nodes = dict(g.nodes(data=True))\n                            node_feature_list = self.preprocess_nodes(nodes)\n                            new_sequence.append(torch.stack(node_feature_list).to(self.device))\n                        # new_sequence : B x N x 15\n                        result = self.model.forward(new_sequence)\n                        output = result['output']\n\n                        if self.config.training_config['task_type'] == 'sequence_classification': # seq vs graph based learning\n                            labels  = torch.cat([labels, torch.tensor(label).long().unsqueeze(0).to(self.device)], dim=0)\n                            new_sequence = torch.cat(new_sequence, dim=0)\n                        elif self.config.training_config['task_type'] in ['collision_prediction']:\n                            label = torch.tensor(np.full(output.shape[0], label)).long().to(self.device) #fill label to length of the sequence. shape (len_input_sequence, 1)\n                            labels  = torch.cat([labels, label], dim=0)\n                        else:\n                            raise ValueError('task_type is unimplemented')\n                        outputs = torch.cat([outputs, output.view(-1, 2)], dim=0) #in this case the output is of shape (len_input_sequence, 2)\n                    loss_train = self.loss_func(outputs, labels)\n                    loss_train.backward()\n                    acc_loss_train += loss_train.detach().cpu().item() * len(data_list)\n                    self.optimizer.step()\n                    del loss_train\n    \n                acc_loss_train /= len(self.training_data)\n                tqdm_bar.set_description('Epoch: {:04d}, loss_train: {:.4f}'.format(epoch_idx, acc_loss_train))\n    \n                if epoch_idx % self.config.training_config[\"test_step\"] == 0:\n                    self.evaluate(epoch_idx)\n                    \n        else:\n            raise ValueError('train(): task type error') \n    \n                \n    def cross_valid(self):\n    \n           # KFold cross validation with similar class distribution in each fold\n           skf = StratifiedKFold(n_splits=self.config.training_config[\"n_fold\"])\n           X = np.array(self.training_data + self.testing_data)\n           y = np.array(self.training_labels + self.testing_labels)\n    \n           # self.results stores average metrics for the the n_folds\n           self.results = {}\n           self.fold = 1\n    \n           # Split training and testing data based on n_splits (Folds)\n           for train_index, test_index in skf.split(X, y):\n               X_train, X_test = X[train_index], X[test_index]\n               y_train, y_test = y[train_index], y[test_index]\n    \n               self.training_data = X_train\n               self.testing_data  = X_test\n               self.training_labels = y_train\n               self.testing_labels  = y_test\n    \n               if self.config.training_config['task_type'] == 'sequence_classification':\n                   print('\\nFold {}'.format(self.fold))\n                   print(\"Number of Sequences Included: \", len(self.training_data))\n                   print(\"Num Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n      \n               elif self.config.training_config['task_type'] == 'collision_prediction':\n                   print('\\nFold {}'.format(self.fold))\n                   print(\"Number of Training Sequences Included: \", len(self.training_data))\n                   print(\"Number of Testing Sequences Included: \",  len(self.testing_data))\n                   print(\"Number of Training Labels in Each Class: \" + str(np.unique(self.total_train_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                   print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.total_test_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n               \n               self.best_val_loss = 99999\n               self.train()\n               outputs_train, labels_train, outputs_test, labels_test, metrics = self.evaluate(self.fold)\n               self.update_sg_cross_valid_metrics(outputs_train, labels_train, outputs_test, labels_test, metrics)\n    \n               if self.fold != self.config.training_config[\"n_fold\"]:            \n                   del self.model\n                   del self.optimizer\n                   self.build_model()\n                   \n               self.fold += 1            \n           del self.results\n                   \n                   \n    def inference(self, testing_data, testing_labels):\n        labels = torch.LongTensor().to(self.device)\n        outputs = torch.FloatTensor().to(self.device)\n        acc_loss_test = 0\n        attns_weights = []\n        node_attns = []\n        folder_names = []\n        sum_prediction_frame = 0\n        sum_seq_len = 0\n        num_risky_sequences = 0\n        num_safe_sequences = 0\n        sum_predicted_risky_indices = 0 #sum is calculated as (value * (index+1))/sum(range(seq_len)) for each value and index in the sequence.\n        sum_predicted_safe_indices = 0  #sum is calculated as ((1-value) * (index+1))/sum(range(seq_len)) for each value and index in the sequence.\n        inference_time = 0 #TODO: remove this metric\n        prof_result = \"\"\n        correct_risky_seq = 0\n        correct_safe_seq = 0\n        incorrect_risky_seq = 0\n        incorrect_safe_seq = 0\n        all_graph_lists = []\n        if 'log_graphs' in self.config.model_config.keys() and self.config.model_config['log_graphs']:\n            os.makedirs(self.config.model_config['ss_graph_path'] + 'correct', exist_ok=True)\n            os.makedirs(self.config.model_config['ss_graph_path'] + 'incorrect', exist_ok=True)\n            os.makedirs(self.config.model_config['orig_graph_path'] + 'correct', exist_ok=True)\n            os.makedirs(self.config.model_config['orig_graph_path'] + 'incorrect', exist_ok=True)\n\n        with torch.autograd.profiler.profile(enabled=False, use_cuda=True) as prof:\n            self.model.eval()\n            with torch.no_grad():\n                for i in range(len(testing_data)): # iterate through scenegraphs\n                    data, label = testing_data[i]['sequence'], testing_labels[i]\n                    folder_name = testing_data[i]['folder_name']\n                    folder_names.append(folder_name)\n                    new_sequence = []\n\n                    #this loop extracts a list of nodes and their attributes for each frame.\n                    data = [data[frame] for frame in sorted(data.keys())] if type(data) == dict else data\n                    for d in data:\n                        g = d.g\n                        nodes = dict(g.nodes(data=True))\n                        node_feature_list = self.preprocess_nodes(nodes)\n                        new_sequence.append(torch.stack(node_feature_list).to(self.device))\n                    out = self.model.forward(new_sequence)\n                    output, graph_list = out['output'], out['graph_list']\n                    inference_time += 0\n                    output = output.view(-1,2)\n                    label = torch.tensor(np.full(output.shape[0], label)).long().to(self.device) #fill label to length of the sequence.\n                    if 'log_graphs' in self.config.model_config.keys() and self.config.model_config['log_graphs']:\n                        if label == output.max(1)[1].type_as(label):\n                            torch.save(graph_list, self.config.model_config['ss_graph_path'] + 'correct/'+folder_name+\".pt\")\n                            torch.save(data, self.config.model_config['orig_graph_path'] + 'correct/'+folder_name+\".pt\")\n                        else:\n                            torch.save(graph_list, self.config.model_config['ss_graph_path'] + 'incorrect/'+folder_name+\".pt\")\n                            torch.save(data, self.config.model_config['orig_graph_path'] + 'incorrect/'+folder_name+\".pt\")\n                        all_graph_lists.append(graph_list)\n                    outputs = torch.cat([outputs, output], dim=0)\n                    labels = torch.cat([labels, label], dim=0)\n\n                    #log metrics for risky and non-risky clips separately.\n                    if(1 in label):\n                        preds = output.max(1)[1].type_as(label)\n                        num_risky_sequences += 1\n                        sum_seq_len += output.shape[0]\n                        if (1 in preds):\n                            correct_risky_seq += 1 #sequence level metrics\n                            sum_prediction_frame += torch.where(preds == 1)[0][0].item() #returns the first index of a \"risky\" prediction in this sequence.\n                            sum_predicted_risky_indices += torch.sum(torch.where(preds==1)[0]+1).item()/np.sum(range(output.shape[0]+1)) #(1*index)/seq_len added to sum.\n                        else:\n                            incorrect_risky_seq += 1\n                            sum_prediction_frame += output.shape[0] #if no risky predictions are made, then add the full sequence length to running avg.\n                    elif(0 in label):\n                        preds = output.max(1)[1].type_as(label)\n                        num_safe_sequences += 1\n                        if (1 in preds): #sequence level metrics\n                            incorrect_safe_seq += 1 \n                        else:\n                            correct_safe_seq += 1 \n\n                        if (0 in preds):\n                            sum_predicted_safe_indices += torch.sum(torch.where(preds==0)[0]+1).item()/np.sum(range(output.shape[0]+1)) #(1*index)/seq_len added to sum.\n\n                    loss_test = self.loss_func(output, label)\n                    acc_loss_test += loss_test.detach().cpu().item()\n\n        graph_metrics = get_graph_metrics([graph for sublist in all_graph_lists for graph in sublist])\n        print(graph_metrics)\n        avg_risky_prediction_frame = sum_prediction_frame / num_risky_sequences if num_risky_sequences != 0 else 0 #avg of first indices in a sequence that a risky frame is first correctly predicted.\n        avg_risky_seq_len = sum_seq_len / num_risky_sequences if num_risky_sequences != 0 else 0 #sequence length for comparison with the prediction frame metric. \n        avg_predicted_risky_indices = sum_predicted_risky_indices / num_risky_sequences if num_risky_sequences != 0 else 0\n        avg_predicted_safe_indices = sum_predicted_safe_indices / num_safe_sequences if num_safe_sequences != 0 else 0\n        seq_tpr = correct_risky_seq / num_risky_sequences if num_risky_sequences != 0 else 0\n        seq_fpr = incorrect_safe_seq / num_safe_sequences if num_safe_sequences != 0 else 0\n        seq_tnr = correct_safe_seq / num_safe_sequences if num_safe_sequences != 0 else 0\n        seq_fnr = incorrect_risky_seq / num_risky_sequences if num_risky_sequences != 0 else 0\n        if prof != None:\n            prof_result = prof.key_averages().table(sort_by=\"cuda_time_total\")\n\n        return outputs, \\\n                labels, \\\n                acc_loss_test/len(testing_data), \\\n                attns_weights, \\\n                node_attns, \\\n                avg_risky_prediction_frame, \\\n                avg_risky_seq_len, \\\n                avg_predicted_risky_indices, \\\n                avg_predicted_safe_indices, \\\n                inference_time, \\\n                prof_result, \\\n                seq_tpr, \\\n                seq_fpr, \\\n                seq_tnr, \\\n                seq_fnr, folder_names\n\n   \n\n\n    def evaluate(self, current_epoch=None):\n        metrics = {}\n        outputs_train, \\\n        labels_train, \\\n        acc_loss_train, \\\n        attns_train, \\\n        node_attns_train, \\\n        train_avg_prediction_frame, \\\n        train_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        train_inference_time, \\\n        train_profiler_result, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.training_data, self.training_labels)\n        torch.cuda.empty_cache() #clear training data from GPU memory\n\n        metrics['train'] = get_metrics(outputs_train, labels_train)\n        metrics['train']['loss'] = acc_loss_train\n        metrics['train']['avg_prediction_frame'] = train_avg_prediction_frame\n        metrics['train']['avg_seq_len'] = train_avg_seq_len\n        metrics['train']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['train']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['train']['seq_tpr'] = seq_tpr\n        metrics['train']['seq_tnr'] = seq_tnr\n        metrics['train']['seq_fpr'] = seq_fpr\n        metrics['train']['seq_fnr'] = seq_fnr\n        with open(\"graph_profile_metrics.txt\", mode='w') as f:\n            f.write(train_profiler_result)\n\n        outputs_test, \\\n        labels_test, \\\n        acc_loss_test, \\\n        attns_test, \\\n        node_attns_test, \\\n        val_avg_prediction_frame, \\\n        val_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        test_inference_time, \\\n        test_profiler_result, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.testing_data, self.testing_labels)\n        \n        metrics['test'] = get_metrics(outputs_test, labels_test)\n        metrics['test']['loss'] = acc_loss_test\n        metrics['test']['avg_prediction_frame'] = val_avg_prediction_frame\n        metrics['test']['avg_seq_len'] = val_avg_seq_len\n        metrics['test']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['test']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['test']['seq_tpr'] = seq_tpr\n        metrics['test']['seq_tnr'] = seq_tnr\n        metrics['test']['seq_fpr'] = seq_fpr\n        metrics['test']['seq_fnr'] = seq_fnr\n        metrics['avg_inf_time'] = (train_inference_time + test_inference_time) / (len(labels_train) + len(labels_test))\n\n        print(\"\\ntrain loss: \" + str(acc_loss_train) + \", acc:\", metrics['train']['acc'], metrics['train']['confusion'], \"mcc:\", metrics['train']['mcc'], \\\n              \"\\ntest loss: \" +  str(acc_loss_test) + \", acc:\",  metrics['test']['acc'],  metrics['test']['confusion'], \"mcc:\", metrics['test']['mcc'])\n        \n        self.update_sg_best_metrics(metrics, current_epoch)\n        metrics['best_epoch'] = self.best_epoch\n        metrics['best_val_loss'] = self.best_val_loss\n        metrics['best_val_acc'] = self.best_val_acc\n        metrics['best_val_auc'] = self.best_val_auc\n        metrics['best_val_conf'] = self.best_val_confusion\n        metrics['best_val_f1'] = self.best_val_f1\n        metrics['best_val_mcc'] = self.best_val_mcc\n        metrics['best_val_acc_balanced'] = self.best_val_acc_balanced\n        metrics['best_avg_pred_frame'] = self.best_avg_pred_frame\n        \n        if self.config.training_config[\"n_fold\"] <= 1 and self.log:\n            log_wandb(metrics)\n\n        torch.cuda.empty_cache() #clear test data from GPU memory\n\n        return outputs_train, labels_train, outputs_test, labels_test, metrics\n\n\n        #automatically save the model and metrics with the lowest validation loss\n    def update_sg_best_metrics(self, metrics, current_epoch):\n        if metrics['test']['loss'] < self.best_val_loss:\n            self.best_val_loss = metrics['test']['loss']\n            self.best_epoch = current_epoch if current_epoch != None else self.config.training_config[\"epochs\"]\n            self.best_val_acc = metrics['test']['acc']\n            self.best_val_auc = metrics['test']['auc']\n            self.best_val_confusion = metrics['test']['confusion']\n            self.best_val_f1 = metrics['test']['f1']\n            self.best_val_mcc = metrics['test']['mcc']\n            self.best_val_acc_balanced = metrics['test']['balanced_acc']\n            self.best_avg_pred_frame = metrics['test']['avg_prediction_frame']\n            self.save_model()\n\n    \n    # Averages metrics after the end of each cross validation fold\n    def update_sg_cross_valid_metrics(self, outputs_train, labels_train, outputs_test, labels_test, metrics):\n        if self.fold == 1:\n            self.results['outputs_train'] = outputs_train\n            self.results['labels_train'] = labels_train\n            self.results['train'] = metrics['train']\n            self.results['train']['loss'] = metrics['train']['loss']\n            self.results['train']['avg_prediction_frame'] = metrics['train']['avg_prediction_frame'] \n            self.results['train']['avg_seq_len']  = metrics['train']['avg_seq_len'] \n            self.results['train']['avg_predicted_risky_indices'] = metrics['train']['avg_predicted_risky_indices'] \n            self.results['train']['avg_predicted_safe_indices'] = metrics['train']['avg_predicted_safe_indices']\n\n            self.results['outputs_test'] = outputs_test\n            self.results['labels_test'] = labels_test\n            self.results['test'] = metrics['test']\n            self.results['test']['loss'] = metrics['test']['loss'] \n            self.results['test']['avg_prediction_frame'] = metrics['test']['avg_prediction_frame'] \n            self.results['test']['avg_seq_len'] = metrics['test']['avg_seq_len'] \n            self.results['test']['avg_predicted_risky_indices'] = metrics['test']['avg_predicted_risky_indices'] \n            self.results['test']['avg_predicted_safe_indices'] = metrics['test']['avg_predicted_safe_indices']\n            self.results['avg_inf_time'] = metrics['avg_inf_time']\n\n            self.results['best_epoch']    = metrics['best_epoch']\n            self.results['best_val_loss'] = metrics['best_val_loss']\n            self.results['best_val_acc']  = metrics['best_val_acc']\n            self.results['best_val_auc']  = metrics['best_val_auc']\n            self.results['best_val_conf'] = metrics['best_val_conf']\n            self.results['best_val_f1']   = metrics['best_val_f1']\n            self.results['best_val_mcc']  = metrics['best_val_mcc']\n            self.results['best_val_acc_balanced'] = metrics['best_val_acc_balanced']\n            self.results['best_avg_pred_frame'] = metrics['best_avg_pred_frame']\n        else:\n            self.results['outputs_train'] = torch.cat((self.results['outputs_train'], outputs_train), dim=0)\n            self.results['labels_train']  = torch.cat((self.results['labels_train'], labels_train), dim=0)\n            self.results['train']['loss'] = np.append(self.results['train']['loss'], metrics['train']['loss'])\n            self.results['train']['avg_prediction_frame'] = np.append(self.results['train']['avg_prediction_frame'], \n                                                                            metrics['train']['avg_prediction_frame'])\n            self.results['train']['avg_seq_len']  = np.append(self.results['train']['avg_seq_len'], metrics['train']['avg_seq_len'])\n            self.results['train']['avg_predicted_risky_indices'] = np.append(self.results['train']['avg_predicted_risky_indices'], \n                                                                                    metrics['train']['avg_predicted_risky_indices'])\n            self.results['train']['avg_predicted_safe_indices'] = np.append(self.results['train']['avg_predicted_safe_indices'], \n                                                                                    metrics['train']['avg_predicted_safe_indices'])\n            \n            self.results['outputs_test'] = torch.cat((self.results['outputs_test'], outputs_test), dim=0)\n            self.results['labels_test']  = torch.cat((self.results['labels_test'], labels_test), dim=0)\n            self.results['test']['loss'] = np.append(self.results['test']['loss'], metrics['test']['loss'])\n            self.results['test']['avg_prediction_frame'] = np.append(self.results['test']['avg_prediction_frame'], \n                                                                        metrics['test']['avg_prediction_frame'])\n            self.results['test']['avg_seq_len'] = np.append(self.results['test']['avg_seq_len'], metrics['test']['avg_seq_len'])\n            self.results['test']['avg_predicted_risky_indices'] = np.append(self.results['test']['avg_predicted_risky_indices'], \n                                                                                    metrics['test']['avg_predicted_risky_indices'])\n            self.results['test']['avg_predicted_safe_indices'] = np.append(self.results['test']['avg_predicted_safe_indices'], \n                                                                                    metrics['test']['avg_predicted_safe_indices'])\n            self.results['avg_inf_time'] = np.append(self.results['avg_inf_time'], metrics['avg_inf_time'])\n\n            self.results['best_epoch']    = np.append(self.results['best_epoch'], metrics['best_epoch'])\n            self.results['best_val_loss'] = np.append(self.results['best_val_loss'], metrics['best_val_loss'])\n            self.results['best_val_acc']  = np.append(self.results['best_val_acc'], metrics['best_val_acc'])\n            self.results['best_val_auc']  = np.append(self.results['best_val_auc'], metrics['best_val_auc'])\n            self.results['best_val_conf'] = np.append(self.results['best_val_conf'], metrics['best_val_conf'])\n            self.results['best_val_f1']   = np.append(self.results['best_val_f1'], metrics['best_val_f1'])\n            self.results['best_val_mcc']  = np.append(self.results['best_val_mcc'], metrics['best_val_mcc'])\n            self.results['best_val_acc_balanced'] = np.append(self.results['best_val_acc_balanced'], metrics['best_val_acc_balanced'])\n            self.results['best_avg_pred_frame'] = np.append(self.results['best_avg_pred_frame'], metrics['best_avg_pred_frame'])\n            \n        # Log final averaged results\n        if self.fold == self.config.training_config[\"n_fold\"]:\n            final_results = {}\n            final_results['train'] = get_metrics(self.results['outputs_train'], self.results['labels_train'])\n            final_results['train']['loss'] = np.average(self.results['train']['loss'])\n            final_results['train']['avg_prediction_frame'] = np.average(self.results['train']['avg_prediction_frame'])\n            final_results['train']['avg_seq_len'] = np.average(self.results['train']['avg_seq_len'])\n            final_results['train']['avg_predicted_risky_indices'] = np.average(self.results['train']['avg_predicted_risky_indices'])\n            final_results['train']['avg_predicted_safe_indices'] = np.average(self.results['train']['avg_predicted_safe_indices'])\n            \n            final_results['test'] = get_metrics(self.results['outputs_test'], self.results['labels_test'])\n            final_results['test']['loss'] = np.average(self.results['test']['loss'])\n            final_results['test']['avg_prediction_frame'] = np.average(self.results['test']['avg_prediction_frame'])\n            final_results['test']['avg_seq_len'] = np.average(self.results['test']['avg_seq_len'])\n            final_results['test']['avg_predicted_risky_indices'] = np.average(self.results['test']['avg_predicted_risky_indices'])\n            final_results['test']['avg_predicted_safe_indices'] = np.average(self.results['test']['avg_predicted_safe_indices'])\n            final_results['avg_inf_time'] = np.average(self.results['avg_inf_time'])\n\n            # Best results\n            final_results['best_epoch']    = np.average(self.results['best_epoch'])\n            final_results['best_val_loss'] = np.average(self.results['best_val_loss'])\n            final_results['best_val_acc']  = np.average(self.results['best_val_acc'])\n            final_results['best_val_auc']  = np.average(self.results['best_val_auc'])\n            final_results['best_val_conf'] = self.results['best_val_conf']\n            final_results['best_val_f1']   = np.average(self.results['best_val_f1'])\n            final_results['best_val_mcc']  = np.average(self.results['best_val_mcc'])\n            final_results['best_val_acc_balanced'] = np.average(self.results['best_val_acc_balanced'])\n            final_results['best_avg_pred_frame'] = np.average(self.results['best_avg_pred_frame'])\n\n            print('\\nFinal Averaged Results')\n            print(\"\\naverage train loss: \" + str(final_results['train']['loss']) + \", average acc:\", final_results['train']['acc'], final_results['train']['confusion'], final_results['train']['auc'], \\\n                \"\\naverage test loss: \" +  str(final_results['test']['loss']) + \", average acc:\", final_results['test']['acc'],  final_results['test']['confusion'], final_results['test']['auc'])\n            if self.log:\n                log_wandb(final_results)\n            \n            return self.results['outputs_train'], self.results['labels_train'], self.results['outputs_test'], self.results['labels_test'], final_results", "\n\n"]}
{"filename": "learning/util/scenegraph_trainer.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport sys, os\nsys.path.append(os.path.dirname(sys.path[0]))\nimport torch\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)", "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tqdm import tqdm\nfrom learning.util.trainer import Trainer\nfrom data.dataset import SceneGraphDataset\nfrom torch_geometric.loader import DataLoader, DataListLoader\nfrom learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning \nfrom torch_geometric.data import Data", "from learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning \nfrom torch_geometric.data import Data\n\n\n'''trainer for rule-based MRGCN baseline model'''\n\nclass Scenegraph_Trainer(Trainer):\n    def __init__(self, config, wandb_a = None):\n        super(Scenegraph_Trainer, self).__init__(config, wandb_a)\n        self.scene_graph_dataset = SceneGraphDataset()\n        self.feature_list = list()\n        for i in range(self.config.model_config['num_of_classes']):\n            self.feature_list.append(\"type_\"+str(i))\n\n    def split_dataset(self): #this is init_dataset from multimodal\n        if (self.config.training_config['task_type'] in ['sequence_classification','graph_classification','collision_prediction']):\n            self.training_data, self.testing_data = self.build_scenegraph_dataset()\n            self.total_train_labels = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.training_data]) # used to compute frame-level class weighting\n            self.total_test_labels  = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.testing_data])\n            self.training_labels = [data['label'] for data in self.training_data]\n            self.testing_labels  = [data['label'] for data in self.testing_data]\n            if self.config.training_config['task_type'] == 'sequence_classification':\n                self.class_weights = torch.from_numpy(compute_class_weight('balanced', classes=np.unique(self.training_labels), y=self.training_labels))\n                if self.config.training_config[\"n_fold\"] <= 1:\n                    print(\"Number of Sequences Included: \", len(self.training_data))\n                    print(\"Num Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n            elif self.config.training_config['task_type'] == 'collision_prediction':\n                self.class_weights = torch.from_numpy(compute_class_weight('balanced', classes=np.unique(self.total_train_labels), y=self.total_train_labels))\n                if self.config.training_config[\"n_fold\"] <= 1:\n                    print(\"Number of Training Sequences Included: \", len(self.training_data))\n                    print(\"Number of Testing Sequences Included: \", len(self.testing_data))\n                    print(\"Number of Training Labels in Each Class: \" + str(np.unique(self.total_train_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                    print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.total_test_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n        else:\n            raise ValueError('split_dataset(): task type error') \n     \n    def build_transfer_learning_dataset(self): #this creates test dataset for transfer learning\n        scene_graph_dataset  = SceneGraphDataset()\n        scene_graph_dataset.dataset_save_path = self.config.location_data[\"transfer_path\"]\n        self.scene_graph_dataset = scene_graph_dataset.load()\n\n        self.transfer_data= []\n        sorted_seq = sorted(self.scene_graph_dataset.labels)\n        if self.config.training_config[\"scenegraph_dataset_type\"] == \"carla\":\n            for ind, seq in enumerate(sorted_seq): #for each seq in labels\n                data_to_append = {\"sequence\":self.scene_graph_dataset.process_carla_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                self.transfer_data.append(data_to_append)\n                    \n        elif self.config.training_config[\"scenegraph_dataset_type\"] == \"real\":\n            for ind, seq in enumerate(sorted_seq): \n                data_to_append = {\"sequence\":self.scene_graph_dataset.process_real_image_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                self.transfer_data.append(data_to_append)\n        else:\n            raise ValueError('dataset_type unrecognized')\n                \n        self.total_transfer_data_labels = np.concatenate([np.full(len(data['sequence']), data['label']) for data in self.transfer_data])\n        self.transfer_data_labels = [data['label'] for data in self.transfer_data]\n        print('Running transfer learning on dataset: ', self.config.location_data[\"transfer_path\"])\n        print('total labels: ', len(self.transfer_data_labels))\n        self.class_weights = torch.tensor([1.0, 1.0])\n\n\n    def evaluate_transfer_learning(self, current_epoch=None):\n        metrics = {}\n        outputs_test, \\\n        labels_test, \\\n        acc_loss_test, \\\n        val_avg_prediction_frame, \\\n        val_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        test_inference_time, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.transfer_data, self.transfer_data_labels)\n\n        metrics['test'] = get_metrics(outputs_test, labels_test)\n        metrics['test']['loss'] = acc_loss_test\n        metrics['test']['avg_prediction_frame'] = val_avg_prediction_frame\n        metrics['test']['avg_seq_len'] = val_avg_seq_len\n        metrics['test']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['test']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['test']['seq_tpr'] = seq_tpr\n        metrics['test']['seq_tnr'] = seq_tnr\n        metrics['test']['seq_fpr'] = seq_fpr\n        metrics['test']['seq_fnr'] = seq_fnr\n        metrics['avg_inf_time'] = (test_inference_time) / (len(labels_test))\n\n\n        self.update_sg_best_metrics(metrics, current_epoch)\n        metrics['best_epoch'] = self.best_epoch\n        metrics['best_val_loss'] = self.best_val_loss\n        metrics['best_val_acc'] = self.best_val_acc\n        metrics['best_val_auc'] = self.best_val_auc\n        metrics['best_val_conf'] = self.best_val_confusion\n        metrics['best_val_f1'] = self.best_val_f1\n        metrics['best_val_mcc'] = self.best_val_mcc\n        metrics['best_val_acc_balanced'] = self.best_val_acc_balanced\n        metrics['best_avg_pred_frame'] = self.best_avg_pred_frame\n        \n        if self.config.training_config[\"n_fold\"] <= 1 and self.log:\n            log_wandb_transfer_learning(metrics)\n        \n        return outputs_test, labels_test, metrics\n\n\n        \n    def build_scenegraph_dataset(self): #this creates test and training datasets\n        '''\n        Dataset format \n            scenegraphs_sequence: dict_keys(['sequence', 'label', 'folder_name'])\n                'sequence': scenegraph metadata\n                'label': classification output [0 -> non_risky (negative), 1 -> risky (positive)]\n                'folder_name': foldername storing sequence data\n    \n        Dataset modes\n            no downsample\n                all sequences used for the train and test set regardless of class distribution\n            downsample  \n                equal amount of positive and negative sequences used for the train and test set\n            transfer \n                replaces original test set with another dataset \n        '''\n        #Load sg dataset obj\n        scene_graph_dataset  = SceneGraphDataset()\n        scene_graph_dataset.dataset_save_path = self.config.location_data[\"input_path\"]\n        self.scene_graph_dataset = scene_graph_dataset.load()     \n    \n        class_0 = []\n        class_1 = []\n        sorted_seq = sorted(self.scene_graph_dataset.labels)\n        if self.config.training_config[\"scenegraph_dataset_type\"] == \"carla\":\n            for ind, seq in enumerate(sorted_seq): #for each seq in labels\n                data_to_append = {\"sequence\":self.scene_graph_dataset.process_carla_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                if self.scene_graph_dataset.labels[seq] == 0:\n                    class_0.append(data_to_append)\n                elif self.scene_graph_dataset.labels[seq] == 1:\n                    class_1.append(data_to_append)\n                    \n        elif self.config.training_config[\"scenegraph_dataset_type\"] == \"real\":\n            for ind, seq in enumerate(sorted_seq): \n                data_to_append = {\"sequence\":self.scene_graph_dataset.process_real_image_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n                if self.scene_graph_dataset.labels[seq] == 0:\n                    class_0.append(data_to_append)\n                elif self.scene_graph_dataset.labels[seq] == 1:\n                    class_1.append(data_to_append)\n        elif self.config.training_config[\"scenegraph_dataset_type\"] != None:\n            raise ValueError('scenegraph_dataset_type not recognized') \n        \n            \n        y_0 = [0]*len(class_0)  \n        y_1 = [1]*len(class_1)\n\n        min_number = min(len(class_0), len(class_1))\n        \n        downsample = self.config.training_config[\"downsample\"]\n        \n        if downsample:\n            modified_class_0, modified_y_0 = resample(class_0, y_0, n_samples=min_number)\n        else:\n            modified_class_0, modified_y_0 = class_0, y_0\n        train, test, train_y, test_y = train_test_split(modified_class_0+class_1, modified_y_0+y_1, test_size=self.config.training_config[\"split_ratio\"], shuffle=True, stratify=modified_y_0+y_1, random_state=self.config.seed)\n        if self.config.location_data[\"transfer_path\"] != None:\n            self.build_transfer_learning_dataset()\n        return train, test\n\n\n\n    def train(self): #edit\n        if (self.config.training_config['task_type'] in ['sequence_classification','graph_classification','collision_prediction']):\n            tqdm_bar = tqdm(range(self.config.training_config['epochs']))\n    \n            for epoch_idx in tqdm_bar: # iterate through epoch   \n                acc_loss_train = 0\n                self.sequence_loader = DataListLoader(self.training_data, batch_size=self.config.training_config[\"batch_size\"])\n    \n                for data_list in self.sequence_loader: # iterate through batches of the dataset\n                    self.model.train()\n                    self.optimizer.zero_grad()\n                    labels = torch.empty(0).long().to(self.config.model_config[\"device\"])\n                    outputs = torch.empty(0,2).to(self.config.model_config[\"device\"])\n    \n                    for sequence in data_list: # iterate through scene-graph sequences in the batch\n                        data, label = sequence['sequence'], sequence['label'] \n                        graph_list = [Data(x=g['node_features'], edge_index=g['edge_index'], edge_attr=g['edge_attr']) for g in data]  \n                        self.train_loader = DataLoader(graph_list, batch_size=len(graph_list))\n                        sequence = next(iter(self.train_loader)).to(self.config.model_config[\"device\"])\n                        output, _ = self.model.forward(sequence.x, sequence.edge_index, sequence.edge_attr, sequence.batch)\n                        if self.config.training_config['task_type'] == 'sequence_classification': # seq vs graph based learning\n                            labels  = torch.cat([labels, torch.tensor([label]).long().to(self.config.model_config[\"device\"])], dim=0)\n                        elif self.config.training_config['task_type'] in ['collision_prediction']:\n                            label = torch.tensor(np.full(output.shape[0], label)).long().to(self.config.model_config[\"device\"]) #fill label to length of the sequence. shape (len_input_sequence, 1)\n                            labels  = torch.cat([labels, label], dim=0)\n                        else:\n                            raise ValueError('task_type is unimplemented')\n                        outputs = torch.cat([outputs, output.view(-1, 2)], dim=0) #in this case the output is of shape (len_input_sequence, 2)\n                    loss_train = self.loss_func(outputs, labels)\n                    loss_train.backward()\n                    acc_loss_train += loss_train.detach().cpu().item() * len(data_list)\n                    self.optimizer.step()\n                    del loss_train\n    \n                acc_loss_train /= len(self.training_data)\n                tqdm_bar.set_description('Epoch: {:04d}, loss_train: {:.4f}'.format(epoch_idx, acc_loss_train))\n    \n                if epoch_idx % self.config.training_config[\"test_step\"] == 0:\n                    self.evaluate(epoch_idx)\n                    \n        else:\n            raise ValueError('train(): task type error') \n    \n                \n    def cross_valid(self):\n    \n           # KFold cross validation with similar class distribution in each fold\n           skf = StratifiedKFold(n_splits=self.config.training_config[\"n_fold\"])\n           X = np.array(self.training_data + self.testing_data)\n           y = np.array(self.training_labels + self.testing_labels)\n    \n           # self.results stores average metrics for the the n_folds\n           self.results = {}\n           self.fold = 1\n    \n           # Split training and testing data based on n_splits (Folds)\n           for train_index, test_index in skf.split(X, y):\n               X_train, X_test = X[train_index], X[test_index]\n               y_train, y_test = y[train_index], y[test_index]\n    \n               self.training_data = X_train\n               self.testing_data  = X_test\n               self.training_labels = y_train\n               self.testing_labels  = y_test\n    \n               if self.config.training_config['task_type'] == 'sequence_classification':\n                   print('\\nFold {}'.format(self.fold))\n                   print(\"Number of Sequences Included: \", len(self.training_data))\n                   print(\"Num Labels in Each Class: \" + str(np.unique(self.training_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n      \n               elif self.config.training_config['task_type'] == 'collision_prediction':\n                   print('\\nFold {}'.format(self.fold))\n                   print(\"Number of Training Sequences Included: \", len(self.training_data))\n                   print(\"Number of Testing Sequences Included: \",  len(self.testing_data))\n                   print(\"Number of Training Labels in Each Class: \" + str(np.unique(self.total_train_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n                   print(\"Number of Testing Labels in Each Class: \" + str(np.unique(self.total_test_labels, return_counts=True)[1]) + \", Class Weights: \" + str(self.class_weights))\n               \n               self.best_val_loss = 99999\n               self.train()\n               outputs_train, labels_train, outputs_test, labels_test, metrics = self.evaluate(self.fold)\n               self.update_sg_cross_valid_metrics(outputs_train, labels_train, outputs_test, labels_test, metrics)\n    \n               if self.fold != self.config.training_config[\"n_fold\"]:            \n                   del self.model\n                   del self.optimizer\n                   self.build_model()\n                   \n               self.fold += 1            \n           del self.results\n                   \n                   \n    def inference(self, testing_data, testing_labels):\n            labels = torch.LongTensor().to(self.config.model_config[\"device\"])\n            outputs = torch.FloatTensor().to(self.config.model_config[\"device\"])\n            acc_loss_test = 0\n            attns_weights = []\n            node_attns = []\n            folder_names = []\n            sum_prediction_frame = 0\n            sum_seq_len = 0\n            num_risky_sequences = 0\n            num_safe_sequences = 0\n            sum_predicted_risky_indices = 0 #sum is calculated as (value * (index+1))/sum(range(seq_len)) for each value and index in the sequence.\n            sum_predicted_safe_indices = 0  #sum is calculated as ((1-value) * (index+1))/sum(range(seq_len)) for each value and index in the sequence.\n            inference_time = 0 #TODO: remove this metric\n            prof_result = \"\"\n            correct_risky_seq = 0\n            correct_safe_seq = 0\n            incorrect_risky_seq = 0\n            incorrect_safe_seq = 0\n    \n            with torch.autograd.profiler.profile(enabled=False, use_cuda=True) as prof:\n                with torch.no_grad():\n                    for i in range(len(testing_data)): # iterate through scenegraphs\n                        data, label = testing_data[i]['sequence'], testing_labels[i]\n                        folder_names.append(testing_data[i]['folder_name'])\n                        data_list = [Data(x=g['node_features'], edge_index=g['edge_index'], edge_attr=g['edge_attr']) for g in data]\n                        self.test_loader = DataLoader(data_list, batch_size=len(data_list))\n                        sequence = next(iter(self.test_loader)).to(self.config.model_config[\"device\"])\n                        self.model.eval()\n\n                        output, attns = self.model.forward(sequence.x, sequence.edge_index, sequence.edge_attr, sequence.batch)\n\n                        inference_time += 0\n                        output = output.view(-1,2)\n                        label = torch.tensor(np.full(output.shape[0], label)).long().to(self.config.model_config[\"device\"]) #fill label to length of the sequence.\n    \n                        #log metrics for risky and non-risky clips separately.\n                        if(1 in label):\n                            preds = output.max(1)[1].type_as(label)\n                            num_risky_sequences += 1\n                            sum_seq_len += output.shape[0]\n                            if (1 in preds):\n                                correct_risky_seq += 1 #sequence level metrics\n                                sum_prediction_frame += torch.where(preds == 1)[0][0].item() #returns the first index of a \"risky\" prediction in this sequence.\n                                sum_predicted_risky_indices += torch.sum(torch.where(preds==1)[0]+1).item()/np.sum(range(output.shape[0]+1)) #(1*index)/seq_len added to sum.\n                            else:\n                                incorrect_risky_seq += 1\n                                sum_prediction_frame += output.shape[0] #if no risky predictions are made, then add the full sequence length to running avg.\n                        elif(0 in label):\n                            preds = output.max(1)[1].type_as(label)\n                            num_safe_sequences += 1\n                            if (1 in preds): #sequence level metrics\n                                incorrect_safe_seq += 1 \n                            else:\n                                correct_safe_seq += 1 \n    \n                            if (0 in preds):\n                                sum_predicted_safe_indices += torch.sum(torch.where(preds==0)[0]+1).item()/np.sum(range(output.shape[0]+1)) #(1*index)/seq_len added to sum.\n    \n                        loss_test = self.loss_func(output, label)\n                        acc_loss_test += loss_test.detach().cpu().item()\n    \n                        outputs = torch.cat([outputs, output], dim=0)\n                        labels = torch.cat([labels, label], dim=0)\n    \n                        if 'lstm_attn_weights' in attns:\n                            attns_weights.append(attns['lstm_attn_weights'].squeeze().detach().cpu().numpy().tolist())\n                        if 'pool_score' in attns:\n                            node_attn = {}\n                            node_attn[\"original_batch\"] = sequence.batch.detach().cpu().numpy().tolist()\n                            node_attn[\"pool_perm\"] = attns['pool_perm'].detach().cpu().numpy().tolist()\n                            node_attn[\"pool_batch\"] = attns['batch'].detach().cpu().numpy().tolist()\n                            node_attn[\"pool_score\"] = attns['pool_score'].detach().cpu().numpy().tolist()\n                            node_attns.append(node_attn)\n    \n            avg_risky_prediction_frame = sum_prediction_frame / num_risky_sequences #avg of first indices in a sequence that a risky frame is first correctly predicted.\n            avg_risky_seq_len = sum_seq_len / num_risky_sequences #sequence length for comparison with the prediction frame metric. \n            avg_predicted_risky_indices = sum_predicted_risky_indices / num_risky_sequences\n            avg_predicted_safe_indices = sum_predicted_safe_indices / num_safe_sequences\n            seq_tpr = correct_risky_seq / num_risky_sequences\n            seq_fpr = incorrect_safe_seq / num_safe_sequences\n            seq_tnr = correct_safe_seq / num_safe_sequences\n            seq_fnr = incorrect_risky_seq / num_risky_sequences\n            if prof != None:\n                prof_result = prof.key_averages().table(sort_by=\"cuda_time_total\")\n    \n            return outputs, \\\n                    labels, \\\n                    acc_loss_test/len(testing_data), \\\n                    attns_weights, \\\n                    node_attns, \\\n                    avg_risky_prediction_frame, \\\n                    avg_risky_seq_len, \\\n                    avg_predicted_risky_indices, \\\n                    avg_predicted_safe_indices, \\\n                    inference_time, \\\n                    prof_result, \\\n                    seq_tpr, \\\n                    seq_fpr, \\\n                    seq_tnr, \\\n                    seq_fnr, folder_names\n\n   \n\n\n    def evaluate(self, current_epoch=None):\n        metrics = {}\n        outputs_train, \\\n        labels_train, \\\n        acc_loss_train, \\\n        attns_train, \\\n        node_attns_train, \\\n        train_avg_prediction_frame, \\\n        train_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        train_inference_time, \\\n        train_profiler_result, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.training_data, self.training_labels)\n\n        metrics['train'] = get_metrics(outputs_train, labels_train)\n        metrics['train']['loss'] = acc_loss_train\n        metrics['train']['avg_prediction_frame'] = train_avg_prediction_frame\n        metrics['train']['avg_seq_len'] = train_avg_seq_len\n        metrics['train']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['train']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['train']['seq_tpr'] = seq_tpr\n        metrics['train']['seq_tnr'] = seq_tnr\n        metrics['train']['seq_fpr'] = seq_fpr\n        metrics['train']['seq_fnr'] = seq_fnr\n        with open(\"graph_profile_metrics.txt\", mode='w') as f:\n            f.write(train_profiler_result)\n\n        outputs_test, \\\n        labels_test, \\\n        acc_loss_test, \\\n        val_avg_prediction_frame, \\\n        val_avg_seq_len, \\\n        avg_predicted_risky_indices, \\\n        avg_predicted_safe_indices, \\\n        test_inference_time, \\\n        seq_tpr, \\\n        seq_fpr, \\\n        seq_tnr, \\\n        seq_fnr, _ = self.inference(self.testing_data, self.testing_labels)\n\n        metrics['test'] = get_metrics(outputs_test, labels_test)\n        metrics['test']['loss'] = acc_loss_test\n        metrics['test']['avg_prediction_frame'] = val_avg_prediction_frame\n        metrics['test']['avg_seq_len'] = val_avg_seq_len\n        metrics['test']['avg_predicted_risky_indices'] = avg_predicted_risky_indices\n        metrics['test']['avg_predicted_safe_indices'] = avg_predicted_safe_indices\n        metrics['test']['seq_tpr'] = seq_tpr\n        metrics['test']['seq_tnr'] = seq_tnr\n        metrics['test']['seq_fpr'] = seq_fpr\n        metrics['test']['seq_fnr'] = seq_fnr\n        metrics['avg_inf_time'] = (train_inference_time + test_inference_time) / (len(labels_train) + len(labels_test))\n\n        print(\"\\ntrain loss: \" + str(acc_loss_train) + \", acc:\", metrics['train']['acc'], metrics['train']['confusion'], \"mcc:\", metrics['train']['mcc'], \\\n              \"\\ntest loss: \" +  str(acc_loss_test) + \", acc:\",  metrics['test']['acc'],  metrics['test']['confusion'], \"mcc:\", metrics['test']['mcc'])\n        \n        self.update_sg_best_metrics(metrics, current_epoch)\n        metrics['best_epoch'] = self.best_epoch\n        metrics['best_val_loss'] = self.best_val_loss\n        metrics['best_val_acc'] = self.best_val_acc\n        metrics['best_val_auc'] = self.best_val_auc\n        metrics['best_val_conf'] = self.best_val_confusion\n        metrics['best_val_f1'] = self.best_val_f1\n        metrics['best_val_mcc'] = self.best_val_mcc\n        metrics['best_val_acc_balanced'] = self.best_val_acc_balanced\n        metrics['best_avg_pred_frame'] = self.best_avg_pred_frame\n        \n        if self.config.training_config[\"n_fold\"] <= 1 and self.log:\n            log_wandb(metrics)\n        \n        return outputs_train, labels_train, outputs_test, labels_test, metrics\n\n\n        #automatically save the model and metrics with the lowest validation loss\n    def update_sg_best_metrics(self, metrics, current_epoch):\n        if metrics['test']['loss'] < self.best_val_loss:\n            self.best_val_loss = metrics['test']['loss']\n            self.best_epoch = current_epoch if current_epoch != None else self.config.training_config[\"epochs\"]\n            self.best_val_acc = metrics['test']['acc']\n            self.best_val_auc = metrics['test']['auc']\n            self.best_val_confusion = metrics['test']['confusion']\n            self.best_val_f1 = metrics['test']['f1']\n            self.best_val_mcc = metrics['test']['mcc']\n            self.best_val_acc_balanced = metrics['test']['balanced_acc']\n            self.best_avg_pred_frame = metrics['test']['avg_prediction_frame']\n            self.save_model()\n\n    \n    # Averages metrics after the end of each cross validation fold\n    def update_sg_cross_valid_metrics(self, outputs_train, labels_train, outputs_test, labels_test, metrics):\n        if self.fold == 1:\n            self.results['outputs_train'] = outputs_train\n            self.results['labels_train'] = labels_train\n            self.results['train'] = metrics['train']\n            self.results['train']['loss'] = metrics['train']['loss']\n            self.results['train']['avg_prediction_frame'] = metrics['train']['avg_prediction_frame'] \n            self.results['train']['avg_seq_len']  = metrics['train']['avg_seq_len'] \n            self.results['train']['avg_predicted_risky_indices'] = metrics['train']['avg_predicted_risky_indices'] \n            self.results['train']['avg_predicted_safe_indices'] = metrics['train']['avg_predicted_safe_indices']\n\n            self.results['outputs_test'] = outputs_test\n            self.results['labels_test'] = labels_test\n            self.results['test'] = metrics['test']\n            self.results['test']['loss'] = metrics['test']['loss'] \n            self.results['test']['avg_prediction_frame'] = metrics['test']['avg_prediction_frame'] \n            self.results['test']['avg_seq_len'] = metrics['test']['avg_seq_len'] \n            self.results['test']['avg_predicted_risky_indices'] = metrics['test']['avg_predicted_risky_indices'] \n            self.results['test']['avg_predicted_safe_indices'] = metrics['test']['avg_predicted_safe_indices']\n            self.results['avg_inf_time'] = metrics['avg_inf_time']\n\n            self.results['best_epoch']    = metrics['best_epoch']\n            self.results['best_val_loss'] = metrics['best_val_loss']\n            self.results['best_val_acc']  = metrics['best_val_acc']\n            self.results['best_val_auc']  = metrics['best_val_auc']\n            self.results['best_val_conf'] = metrics['best_val_conf']\n            self.results['best_val_f1']   = metrics['best_val_f1']\n            self.results['best_val_mcc']  = metrics['best_val_mcc']\n            self.results['best_val_acc_balanced'] = metrics['best_val_acc_balanced']\n            self.results['best_avg_pred_frame'] = metrics['best_avg_pred_frame']\n        else:\n            self.results['outputs_train'] = torch.cat((self.results['outputs_train'], outputs_train), dim=0)\n            self.results['labels_train']  = torch.cat((self.results['labels_train'], labels_train), dim=0)\n            self.results['train']['loss'] = np.append(self.results['train']['loss'], metrics['train']['loss'])\n            self.results['train']['avg_prediction_frame'] = np.append(self.results['train']['avg_prediction_frame'], \n                                                                            metrics['train']['avg_prediction_frame'])\n            self.results['train']['avg_seq_len']  = np.append(self.results['train']['avg_seq_len'], metrics['train']['avg_seq_len'])\n            self.results['train']['avg_predicted_risky_indices'] = np.append(self.results['train']['avg_predicted_risky_indices'], \n                                                                                    metrics['train']['avg_predicted_risky_indices'])\n            self.results['train']['avg_predicted_safe_indices'] = np.append(self.results['train']['avg_predicted_safe_indices'], \n                                                                                    metrics['train']['avg_predicted_safe_indices'])\n            \n            self.results['outputs_test'] = torch.cat((self.results['outputs_test'], outputs_test), dim=0)\n            self.results['labels_test']  = torch.cat((self.results['labels_test'], labels_test), dim=0)\n            self.results['test']['loss'] = np.append(self.results['test']['loss'], metrics['test']['loss'])\n            self.results['test']['avg_prediction_frame'] = np.append(self.results['test']['avg_prediction_frame'], \n                                                                        metrics['test']['avg_prediction_frame'])\n            self.results['test']['avg_seq_len'] = np.append(self.results['test']['avg_seq_len'], metrics['test']['avg_seq_len'])\n            self.results['test']['avg_predicted_risky_indices'] = np.append(self.results['test']['avg_predicted_risky_indices'], \n                                                                                    metrics['test']['avg_predicted_risky_indices'])\n            self.results['test']['avg_predicted_safe_indices'] = np.append(self.results['test']['avg_predicted_safe_indices'], \n                                                                                    metrics['test']['avg_predicted_safe_indices'])\n            self.results['avg_inf_time'] = np.append(self.results['avg_inf_time'], metrics['avg_inf_time'])\n\n            self.results['best_epoch']    = np.append(self.results['best_epoch'], metrics['best_epoch'])\n            self.results['best_val_loss'] = np.append(self.results['best_val_loss'], metrics['best_val_loss'])\n            self.results['best_val_acc']  = np.append(self.results['best_val_acc'], metrics['best_val_acc'])\n            self.results['best_val_auc']  = np.append(self.results['best_val_auc'], metrics['best_val_auc'])\n            self.results['best_val_conf'] = np.append(self.results['best_val_conf'], metrics['best_val_conf'])\n            self.results['best_val_f1']   = np.append(self.results['best_val_f1'], metrics['best_val_f1'])\n            self.results['best_val_mcc']  = np.append(self.results['best_val_mcc'], metrics['best_val_mcc'])\n            self.results['best_val_acc_balanced'] = np.append(self.results['best_val_acc_balanced'], metrics['best_val_acc_balanced'])\n            self.results['best_avg_pred_frame'] = np.append(self.results['best_avg_pred_frame'], metrics['best_avg_pred_frame'])\n            \n        # Log final averaged results\n        if self.fold == self.config.training_config[\"n_fold\"]:\n            final_results = {}\n            final_results['train'] = get_metrics(self.results['outputs_train'], self.results['labels_train'])\n            final_results['train']['loss'] = np.average(self.results['train']['loss'])\n            final_results['train']['avg_prediction_frame'] = np.average(self.results['train']['avg_prediction_frame'])\n            final_results['train']['avg_seq_len'] = np.average(self.results['train']['avg_seq_len'])\n            final_results['train']['avg_predicted_risky_indices'] = np.average(self.results['train']['avg_predicted_risky_indices'])\n            final_results['train']['avg_predicted_safe_indices'] = np.average(self.results['train']['avg_predicted_safe_indices'])\n            \n            final_results['test'] = get_metrics(self.results['outputs_test'], self.results['labels_test'])\n            final_results['test']['loss'] = np.average(self.results['test']['loss'])\n            final_results['test']['avg_prediction_frame'] = np.average(self.results['test']['avg_prediction_frame'])\n            final_results['test']['avg_seq_len'] = np.average(self.results['test']['avg_seq_len'])\n            final_results['test']['avg_predicted_risky_indices'] = np.average(self.results['test']['avg_predicted_risky_indices'])\n            final_results['test']['avg_predicted_safe_indices'] = np.average(self.results['test']['avg_predicted_safe_indices'])\n            final_results['avg_inf_time'] = np.average(self.results['avg_inf_time'])\n\n            # Best results\n            final_results['best_epoch']    = np.average(self.results['best_epoch'])\n            final_results['best_val_loss'] = np.average(self.results['best_val_loss'])\n            final_results['best_val_acc']  = np.average(self.results['best_val_acc'])\n            final_results['best_val_auc']  = np.average(self.results['best_val_auc'])\n            final_results['best_val_conf'] = self.results['best_val_conf']\n            final_results['best_val_f1']   = np.average(self.results['best_val_f1'])\n            final_results['best_val_mcc']  = np.average(self.results['best_val_mcc'])\n            final_results['best_val_acc_balanced'] = np.average(self.results['best_val_acc_balanced'])\n            final_results['best_avg_pred_frame'] = np.average(self.results['best_avg_pred_frame'])\n\n            print('\\nFinal Averaged Results')\n            print(\"\\naverage train loss: \" + str(final_results['train']['loss']) + \", average acc:\", final_results['train']['acc'], final_results['train']['confusion'], final_results['train']['auc'], \\\n                \"\\naverage test loss: \" +  str(final_results['test']['loss']) + \", average acc:\", final_results['test']['acc'],  final_results['test']['confusion'], final_results['test']['auc'])\n            if self.log:\n                log_wandb(final_results)\n            \n            return self.results['outputs_train'], self.results['labels_train'], self.results['outputs_test'], self.results['labels_test'], final_results", "\n\n"]}
{"filename": "learning/util/trainer.py", "chunked_list": ["#Copied from https://github.com/AICPS/roadscene2vec\n#Copyright (c) 2021 UC Irvine Advanced Integrated Cyber-Physical Systems Lab (AICPS)\n\nimport os\nimport sys\nfrom pathlib import Path\n\nsys.path.append(os.path.dirname(sys.path[0]))\nimport torch\nimport torch.optim as optim", "import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport numpy as np\nimport random\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom learning.model.cnn_lstm import CNN_LSTM_Classifier\nfrom learning.model.mrgcn import MRGCN", "from learning.model.cnn_lstm import CNN_LSTM_Classifier\nfrom learning.model.mrgcn import MRGCN\nfrom learning.model.rs2g import RS2G\n\n\n'''Class implementing basic trainer functionality such as model building, saving, and loading.'''\nclass Trainer:\n\n    def __init__(self, config, wandb_a = None):\n        self.config = config\n        if wandb_a != None:\n            self.log = True\n            self.wandb = wandb_a\n        else:\n            self.log = False             \n        if self.config.training_config[\"seed\"] != None: \n            self.config.seed = self.config.training_config[\"seed\"]\n            np.random.seed(self.config.seed)\n            torch.manual_seed(self.config.seed)\n        else:\n            self.config.seed = random.randint(0,2**32) #default value\n            np.random.seed(self.config.seed)\n            torch.manual_seed(self.config.seed)\n            \n        self.toGPU = lambda x, dtype: torch.as_tensor(x, dtype=dtype, device=self.config.model_config['device'])\n        self.initialize_best_metrics()\n\n\n    #defines initial values for \"best\" metrics so that they can be updated during training by the model.\n    def initialize_best_metrics(self):\n        self.best_val_loss = 99999\n        self.best_epoch = 0\n        self.best_val_acc = 0\n        self.best_val_auc = 0\n        self.best_val_confusion = []\n        self.best_val_f1 = 0\n        self.best_val_mcc = -1.0\n        self.best_val_acc_balanced = 0\n        self.unique_clips = {}\n\n    #abstract function implemented by subclasses\n    def split_dataset(self):\n        raise NotImplementedError\n\n\n    def build_model(self): #this involves changing mrcgn and mrgin files to be compatible with new config tht we pass in\n        # BD mode\n        #self.config.num_features = len(self.feature_list)\n        #self.config.num_relations = max([r.value for r in Relations])+1\n        if self.config.model_config[\"model\"] == \"mrgcn\":\n            self.model = MRGCN(self.config).to(self.config.model_config[\"device\"])\n        elif self.config.model_config[\"model\"] == \"rs2g\":\n            self.model = RS2G(self.config).to(self.config.model_config[\"device\"])\n        elif self.config.model_config[\"model\"]  == \"cnn_lstm\":\n            self.model = CNN_LSTM_Classifier((self.config.training_config['batch_size'], self.frame_limit,self.color_channels, self.im_height, self.im_width), self.config).to(self.config.model_config[\"device\"])\n        else:\n            raise Exception(\"model selection is invalid: \" + self.config.model_config[\"model\"])\n        \n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.training_config[\"learning_rate\"], weight_decay=self.config.training_config[\"weight_decay\"])\n        \n        if self.config.model_config[\"load_model\"]  == False:\n            if self.class_weights.shape[0] < 2:\n                self.loss_func = nn.CrossEntropyLoss()\n            else:\n                self.loss_func = nn.CrossEntropyLoss(weight=self.class_weights.float().to(self.config.model_config[\"device\"]))\n     \n            #wandb.watch(self.model, log=\"all\")\n            if self.log:\n                self.wandb.watch(self.model, log=\"all\")\n        else:\n            if self.class_weights.shape[0] < 2:\n                self.loss_func = nn.CrossEntropyLoss()\n            else:\n                self.loss_func = nn.CrossEntropyLoss(weight=self.class_weights.float().to(self.config.model_config[\"device\"]))\n\n\n    # Pick between Standard Training and KFold Cross Validation Training\n    def learn(self):  \n        if self.config.training_config[\"n_fold\"] <= 1 or self.config.location_data[\"transfer_path\"] != None:\n            print('\\nRunning Standard Training Loop\\n')\n            self.train()\n        else:\n            print('\\nRunning {}-Fold Cross Validation Training Loop\\n'.format(self.config.training_config[\"n_fold\"]))\n            self.cross_valid()\n\n    #abstract method implemented by subclasses\n    def train(self):\n        raise NotImplementedError\n\n    def inference(self):\n        raise NotImplementedError\n    \n    def cross_valid(self):\n        raise NotImplementedError\n\n    \n    def save_model(self):\n        \"\"\"Function to save the model.\"\"\"\n        saved_path = Path(self.config.model_config[\"model_save_path\"]).resolve()\n        os.makedirs(os.path.dirname(saved_path), exist_ok=True)\n        torch.save(self.model.state_dict(), str(saved_path))\n        with open(os.path.dirname(saved_path) + \"/model_parameters.txt\", \"w+\") as f:\n            f.write(str(self.config))\n            f.write('\\n')\n            f.write(str(' '.join(sys.argv)))\n\n    def load_model(self):\n        \"\"\"Function to load the model.\"\"\"\n        saved_path = Path(self.config.model_config[\"model_load_path\"]).resolve()\n        if saved_path.exists():\n            self.build_model()\n            self.model.load_state_dict(torch.load(str(saved_path), map_location=self.config.model_config[\"device\"]))\n            self.model.eval()\n        else:\n            raise FileNotFoundError(\"Failed to load model. Model load path does not exist: \" + str(saved_path))", ""]}
