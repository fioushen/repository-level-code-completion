{"filename": "setup.py", "chunked_list": ["\"\"\"Grasshopper Framework Package.\n\nOwned by Team Shield.\n\nPackage for framework that combines locust, pytest and some supporting utilities to\nprovide a fully featured, python based load testing options for API clients.\n\nNote that the only things we want handled in this setup method is setting the version,\nthe only thing that we are currently doing dynamically (in this case version).\nAll other configuration for the package is located in the setup.cfg, per the most recent", "the only thing that we are currently doing dynamically (in this case version).\nAll other configuration for the package is located in the setup.cfg, per the most recent\nguidance on python packaging best practices:\nhttps://packaging.python.org/en/latest/tutorials/packaging-projects/\n\n\"\"\"\nfrom setuptools import setup\n\nsetup(\n    entry_points={\"pytest11\": [\"locust_grasshopper = grasshopper.lib.fixtures\"]},", "setup(\n    entry_points={\"pytest11\": [\"locust_grasshopper = grasshopper.lib.fixtures\"]},\n)\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"The tests for the grasshopper package.\"\"\"\n"]}
{"filename": "tests/integration/test__configuration_integration_with_pytest.py", "chunked_list": ["from unittest.mock import MagicMock\n\nimport pytest\n\n\n@pytest.fixture\ndef environment():\n    return MagicMock()\n\n\ndef test_all_the_args(\n    environment,\n    global_defaults,\n    grasshopper_config_file_args,\n    cmdln_args,\n    env_var_args,\n    complete_configuration,\n    pre_processed_args,\n    scenario_file_args,\n):\n    print(f\"GLOBAL DEFAULTS: {global_defaults}\")\n    print(f\"GLOBAL CONFIG FILE: {grasshopper_config_file_args}\")\n    print(f\"CMDLINE ARGS: {cmdln_args}\")\n    print(f\"ENV VAR ARGS: {env_var_args}\")\n    print(f\"PRE-PROCESSED ARGS: {pre_processed_args}\")\n    print(f\"SCENARIO_FILE_ARGS: {scenario_file_args}\")\n    print(f\"COMPLETE NEW: {complete_configuration}\")", "\n\ndef test_all_the_args(\n    environment,\n    global_defaults,\n    grasshopper_config_file_args,\n    cmdln_args,\n    env_var_args,\n    complete_configuration,\n    pre_processed_args,\n    scenario_file_args,\n):\n    print(f\"GLOBAL DEFAULTS: {global_defaults}\")\n    print(f\"GLOBAL CONFIG FILE: {grasshopper_config_file_args}\")\n    print(f\"CMDLINE ARGS: {cmdln_args}\")\n    print(f\"ENV VAR ARGS: {env_var_args}\")\n    print(f\"PRE-PROCESSED ARGS: {pre_processed_args}\")\n    print(f\"SCENARIO_FILE_ARGS: {scenario_file_args}\")\n    print(f\"COMPLETE NEW: {complete_configuration}\")", ""]}
{"filename": "tests/integration/test__journey1.py", "chunked_list": ["import logging\n\nfrom locust import between, task\n\nfrom grasshopper.lib.grasshopper import BaseJourney, Grasshopper\nfrom grasshopper.lib.util.utils import custom_trend\n\nlogger = logging.getLogger(__name__)\n\n\nclass Journey1(BaseJourney):\n    wait_time = between(min_wait=30, max_wait=40)\n    defaults = {}\n\n    @task\n    @custom_trend(\"PX_TREND_google_home\")\n    def journey1_task(self):\n        logger.info(f\"VU {self.vu_number}: Starting journey1_task\")\n        response = self.client.get(\"https://google.com\", name=\"google_home\")\n        logger.info(f\"VU {self.vu_number}: Google result: {response.status_code}\")", "\n\nclass Journey1(BaseJourney):\n    wait_time = between(min_wait=30, max_wait=40)\n    defaults = {}\n\n    @task\n    @custom_trend(\"PX_TREND_google_home\")\n    def journey1_task(self):\n        logger.info(f\"VU {self.vu_number}: Starting journey1_task\")\n        response = self.client.get(\"https://google.com\", name=\"google_home\")\n        logger.info(f\"VU {self.vu_number}: Google result: {response.status_code}\")", "\n\ndef test_journey1(complete_configuration):\n    Journey1.update_incoming_scenario_args(complete_configuration)\n    locust_env = Grasshopper.launch_test(Journey1, **complete_configuration)\n    return locust_env\n"]}
{"filename": "tests/integration/conftest.py", "chunked_list": ["\"\"\"Module: Contest.py.\n\nConftest for use with integration tests for Grasshopper.\n\n\"\"\"\nimport os\n\n# import pytest\n\nGRASSHOPPER_CONFIG_FILE_PATH = os.path.join(", "\nGRASSHOPPER_CONFIG_FILE_PATH = os.path.join(\n    os.path.abspath(os.path.dirname(__file__)), \"grasshopper.config\"\n)\n\n# Leaving this here because sometimes we want to turn it on for testing, but we don't\n# want to use a config file unless the grasshopper consumer supplies one\n# @pytest.fixture(scope=\"session\")\n# def grasshopper_config_file_path():\n#     return GRASSHOPPER_CONFIG_FILE_PATH", "# def grasshopper_config_file_path():\n#     return GRASSHOPPER_CONFIG_FILE_PATH\n"]}
{"filename": "tests/unit/test__fixture__extra_env_var_keys.py", "chunked_list": ["import logging\n\nfrom assertpy import assert_that\n\nfrom tests.unit.conftest import (  # noqa: I202\n    CONFTEST_TEMPLATE,\n    message_was_not_logged,\n    perform_fixture_test_with_optional_log_capture,\n)\n", ")\n\nFIXTURE_UNDER_TEST = \"extra_env_var_keys\"\n\n\ndef test__extra_env_var_keys(extra_env_var_keys):\n    \"\"\"Validate that fixture returns default for extra keys.\"\"\"\n    assert_that(extra_env_var_keys).is_equal_to([])\n\n\ndef test__extra_env_var_keys__couple_defined(pytester):\n    \"\"\"Fixture detects extra env var keys, if supplied.\"\"\"\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"ENV1\", \"ENV2\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to(['ENV1', 'ENV2'])\n    \"\"\"\n    )\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__extra_env_var_keys__couple_defined(pytester):\n    \"\"\"Fixture detects extra env var keys, if supplied.\"\"\"\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"ENV1\", \"ENV2\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to(['ENV1', 'ENV2'])\n    \"\"\"\n    )\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__extra_env_var_keys__none(pytester, caplog):\n    \"\"\"Fixture should return the default and log a warning for invalid keys list.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return None\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to([])\n    \"\"\"\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Fixture configuration_extra_env_var_keys may only return \"\n        \"a list of strings\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", "\n\ndef test__extra_env_var_keys__empty_list(pytester, caplog):\n    \"\"\"Fixture should not error or log message if supplied an empty list.\"\"\"\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return []\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to([])\n    \"\"\"\n    )\n    perform_fixture_test_with_optional_log_capture(pytester)\n    target_message_re = (\n        \"Fixture configuration_extra_env_var_keys may only return a list of strings\"\n    )\n    not_logged, matches = message_was_not_logged(\n        caplog, target_message_re=target_message_re\n    )\n    assert_that(not_logged, f\"Matching messages {matches}\").is_true()", "\n\ndef test__extra_env_var_keys__invalid_type_in_list(pytester, caplog):\n    \"\"\"Fixture should return the default and log a warning for invalid keys list.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"a string\", {}]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to([])\n    \"\"\"\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Fixture configuration_extra_env_var_keys may only return \"\n        \"a list of strings\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", "\n\ndef test__extra_env_var_keys__not_a_list(pytester, caplog):\n    \"\"\"Fixture should return the default and log a warning for invalid keys list.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return {\"key\":\"value\"}\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to([])\n    \"\"\"\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Fixture configuration_extra_env_var_keys may only return \"\n        \"a list of strings\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", ""]}
{"filename": "tests/unit/test__fixture__env_var_extra_keys.py", "chunked_list": ["import os\nfrom unittest.mock import patch\n\n# Alteryx Packages\nfrom tests.unit.conftest import (\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,", "    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817\n    ConfigurationConstants as CC,\n)\n\nFIXTURE_UNDER_TEST = \"env_var_args\"\n", "FIXTURE_UNDER_TEST = \"env_var_args\"\n\n# Some comments on testing fixture env_var_args.\n# There are fairly complete unit tests for the extra_env_var_args fixture that assert\n# that _this fixture's value_ will _only_ be a list of strings or [].\n# Therefore, we only need to test this fixture passing in [] (default, what you get if\n# you don't patch it) and a list of strings.\n# Similarly, we only need to test env_var_args receiving a non-zero length string as the\n# custom prefix.\n# If any of the tests here are failing, it would be worthwhile to check the results of", "# custom prefix.\n# If any of the tests here are failing, it would be worthwhile to check the results of\n# `test__fixture__extra_env_var_args` and `test__fixture__env_var_prefix_key` before\n# debugging here.\n# Additionally, note that the expected behavior for env_var_args is lower case all keys\n# _after_ matching.\n\n# Matching some values from COMPLETE_ATTRS\nFAKE_ENVIRON_SOME_MATCHING_VALUES = {\"INFLUX_HOST\": \"1.1.1.1\", \"RUNTIME\": \"10\"}\n", "FAKE_ENVIRON_SOME_MATCHING_VALUES = {\"INFLUX_HOST\": \"1.1.1.1\", \"RUNTIME\": \"10\"}\n\n# Keys that don't match anything from COMPLETE_ATTRS\nFAKE_ENVIRON_NON_MATCHING_VALUES = {\n    \"COMPLETELY_UNRELATED_KEY\": \"completely unrelated value\",\n    \"ANOTHER_UNRELATED_KEY\": \"some other value\",\n}\n\n# Mix of keys that match and don't match COMPLETE_ATTRS\nFAKE_ENVIRON_MIX_MATCH_AND_NOT = {", "# Mix of keys that match and don't match COMPLETE_ATTRS\nFAKE_ENVIRON_MIX_MATCH_AND_NOT = {\n    **FAKE_ENVIRON_SOME_MATCHING_VALUES,\n    **FAKE_ENVIRON_NON_MATCHING_VALUES,\n}\n\n# Keys that match external sources (env_var_keys & custom prefix)\nFAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES = {\n    \"MY_UNIT_TEST_VAR\": \"testing var from extra keys set is loaded\",\n    \"UNIT_TEST_EXTRA\": \"testing that extra env var args are loaded\",", "    \"MY_UNIT_TEST_VAR\": \"testing var from extra keys set is loaded\",\n    \"UNIT_TEST_EXTRA\": \"testing that extra env var args are loaded\",\n}\n\n# Keys that won't resolve correctly paired with certain prefix keys\nFAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE = {\n    \"OOOPS_\": \"testing load will skip over any env var names it can't resolve\",\n}\n\n# All different types of keys, in order to test that the fixture can load all the", "\n# All different types of keys, in order to test that the fixture can load all the\n# different types of sources _at the same time_ and not include irrelevant data\nFAKE_ENVIRON_ALL_SOURCES_AND_NON_MATCHING_VALUES = {\n    **FAKE_ENVIRON_MIX_MATCH_AND_NOT,\n    **FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES,\n    **FAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE,\n    \"MY_\": \"value that should not load\",\n}\n", "}\n\n\n@patch.dict(os.environ, FAKE_ENVIRON_SOME_MATCHING_VALUES, clear=True)\ndef test__env_var_args__happy(pytester):\n    \"\"\"Fixture should load any env vars that match keys in COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    expected_config_args = {\"influx_host\": \"1.1.1.1\", \"runtime\": \"10\"}\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_NON_MATCHING_VALUES, clear=True)\ndef test__env_var_args__no_matches(pytester):\n    \"\"\"Fixture should return an empty config if there are no values that match keys\n    in COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, clear=True)\ndef test__env_var_args__environ_empty(pytester):\n    \"\"\"Fixture should return an empty config if environ is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES, clear=True)\ndef test__env_var_args__matching_extra_keys(pytester):\n    \"\"\"Fixture should load keys that match values in extra_env_var_keys.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def extra_env_var_keys():\n            return [\"UNIT_TEST_EXTRA\", \"KEY_THAT_IS_NOT_IN_ENVIRON\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_extra\": \"testing that extra env var args are loaded\"\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES, clear=True)\ndef test__env_var_args__matching_custom_prefix(pytester):\n    \"\"\"Fixture should load any keys in environ that begin with the custom prefix,\n    discarding the prefix portion.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'MY_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_var\": \"testing var from extra keys set is loaded\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE, clear=True)\ndef test__env_var_args__skipping_keys_that_dont_resolve_correctly(pytester):\n    \"\"\"Fixture returns an empty config if matching key results in a invalid key.\n    This can happen if, once the prefix is stripped, the key is an empty string.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'OOOPS_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_ALL_SOURCES_AND_NON_MATCHING_VALUES, clear=True)\ndef test__env_var_args__loading_from_all_sources(pytester):\n    \"\"\"Fixture should load values from all different sources, and ignore anything in\n    environ that doesn't match.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'MY_'\n\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"UNIT_TEST_EXTRA\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_var\": \"testing var from extra keys set is loaded\",\n        \"unit_test_extra\": \"testing that extra env var args are loaded\",\n        \"influx_host\": \"1.1.1.1\",\n        \"runtime\": \"10\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_SOME_MATCHING_VALUES, clear=True)\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__env_var_args__empty_attrs(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, clear=True)\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__env_var_args__empty_attrs_and_empty_env_vars(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", ""]}
{"filename": "tests/unit/test__fixture__pre_processed_args.py", "chunked_list": ["from tests.unit.conftest import (\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration\n", "from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\nFIXTURE_UNDER_TEST = \"pre_processed_args\"\n\n\ndef test__pre_processed_args__happy(pytester):\n    \"\"\"Fixture should pass on the entries found for special keys, scenario_file and\n    scenario_name.\"\"\"\n\n    expected_config_args = GHConfiguration(\n        {\n            \"scenario_file\": \"fake_scenario_file.yaml\",\n            \"scenario_name\": \"fake_scenario_name\",\n        }\n    )\n\n    patches = f\"\"\"\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration({expected_config_args})\n            return config\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__pre_processed_args__empty_sources(pytester):\n    \"\"\"Fixture should return an empty config if all sources are empty.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration()\n            return config\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__pre_processed_args__with_precedence_applied(pytester):\n    \"\"\"Test fixture loads with the correct precedence applied.\n\n    This test also covers some other test cases:\n    + should be able to handle a source that is empty, in this case global defaults\n    + should be able to handle a source that has extra, unrelated keys in addition to\n    the keys it is looking for (config file args)\n    + should be able to handle one key coming from one source and the other key coming\n    from another source\n\n    \"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_config_file_not_the_one.yaml\"})\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_env_var_not_the_one.yaml\",\n                \"scenario_name\": \"from_env_var_this_is_the_one\"})\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_cmdln_this_is_the_one.yaml\"})\n            return config\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    expected_config_args = {\n        \"scenario_file\": \"from_cmdln_this_is_the_one.yaml\",\n        \"scenario_name\": \"from_env_var_this_is_the_one\",\n    }\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", ""]}
{"filename": "tests/unit/test_metrics.py", "chunked_list": ["from unittest.mock import MagicMock, create_autospec\nfrom uuid import uuid4\n\nimport pytest\nfrom locust.env import Environment\nfrom locust.stats import RequestStats\n\nfrom grasshopper.lib.journeys.base_journey import BaseJourney\nfrom grasshopper.lib.util.metrics import count_iterations, task\n", "from grasshopper.lib.util.metrics import count_iterations, task\n\n\n@pytest.fixture(scope=\"session\")\ndef sample_func():\n    def sample(journey):\n        # just return the same object, so we can check that it's working correctly\n        return journey\n\n    return sample", "\n\n@pytest.fixture(scope=\"session\")\ndef sample_func_different():\n    def sample(journey):\n        # just return the same object, so we can check that it's working correctly\n        return journey\n\n    return sample\n", "\n\n@pytest.fixture\ndef mock_journey():\n    mock_journey = create_autospec(BaseJourney)\n    mock_journey.vu_iteration = 0\n    mock_journey.client_id = uuid4()\n    mock_journey.environment = create_autospec(Environment)\n    mock_journey.environment.stats = create_autospec(RequestStats)\n    mock_journey.environment.stats.num_iterations = 0\n    return mock_journey", "\n\ndef check_iteration_count(journey, count):\n    assert (\n        journey.environment.stats.num_iterations == count\n    ), \"Decorator did not actually increment the environment iterations count\"\n    assert (\n        journey.vu_iteration == count\n    ), \"Decorator did not actually increment the vu_iteration count on the journey\"\n", "\n\ndef test__count_iterations(sample_func, mock_journey):\n    # simulate applying our decorator, remember that\n    # @decorator\n    # def method(...): ...\n    # is syntactic sugar for replace method with decorator(method)\n    # therefore to call method, you would do (decorator(method))(...)\n    wrapped_func = count_iterations(sample_func)\n    assert callable(wrapped_func), \"Return value of decorator is not a callable.\"\n    result_of_calling_wrapped_func = wrapped_func(mock_journey)\n    check_iteration_count(result_of_calling_wrapped_func, 1)", "\n\ndef test__count_iterations_multiple_calls(sample_func, mock_journey):\n    wrapped_func = count_iterations(sample_func)\n    wrapped_func(mock_journey)\n    wrapped_func(mock_journey)\n    result_of_calling_wrapped_func = wrapped_func(mock_journey)\n    check_iteration_count(result_of_calling_wrapped_func, 3)\n\n\ndef test__count_iterations_multiple_functions(\n    sample_func, sample_func_different, mock_journey\n):\n    # check that multiple wraps (represents different tasks on same journey) can both\n    # increment the vu_iteration count correctly\n    wrapped_func = count_iterations(sample_func)\n    wrapped_func(mock_journey)\n    wrapped_diff = count_iterations(sample_func_different)\n    wrapped_diff(mock_journey)\n    result_of_calling_wrapped_func = wrapped_func(mock_journey)\n    check_iteration_count(result_of_calling_wrapped_func, 3)", "\n\ndef test__count_iterations_multiple_functions(\n    sample_func, sample_func_different, mock_journey\n):\n    # check that multiple wraps (represents different tasks on same journey) can both\n    # increment the vu_iteration count correctly\n    wrapped_func = count_iterations(sample_func)\n    wrapped_func(mock_journey)\n    wrapped_diff = count_iterations(sample_func_different)\n    wrapped_diff(mock_journey)\n    result_of_calling_wrapped_func = wrapped_func(mock_journey)\n    check_iteration_count(result_of_calling_wrapped_func, 3)", "\n\ndef test__task_default_weight(sample_func, mock_journey):\n    wrapped_func = task(sample_func)\n    assert callable(wrapped_func), \"Return value of decorator is not a callable.\"\n    result_of_calling_wrapped_func = wrapped_func(mock_journey)\n    # our task decorator wraps locust's task decorator with vu_iteration tracking\n    # check that the locust_task_weight attr was transferred to the new wrapped function\n    # this attr is how locust tells which tasks are to be collected when running a test\n    assert wrapped_func.locust_task_weight == 1\n    check_iteration_count(result_of_calling_wrapped_func, 1)", "\n\ndef test__task_specifying_weight(sample_func):\n    # use this class to check specifying the decorator with a weight\n    # there should be a way to check using the function equivalent, but I couldn't\n    # quite get the mocking correct in that case, since both our code and the locust\n    # code are assuming that the task is a bound method on the journey class\n    # the syntactic equivalent for the sugar @task(4) is\n    # the attribute fake_task = (task(4))(fake_task)\n    class FakeJourney(BaseJourney):\n        client_id = uuid4()\n        vu_iteration = 0\n\n        @task(4)\n        def fake_task(self):\n            return self\n\n    env = create_autospec(Environment)\n    env.stats = create_autospec(RequestStats)\n    env.stats.num_iterations = 0\n    env.events = MagicMock()\n    journey = FakeJourney(env)\n    journey_after = journey.fake_task()\n    assert journey_after.fake_task.locust_task_weight == 4", ""]}
{"filename": "tests/unit/test__fixture__helpers.py", "chunked_list": ["import os\nfrom unittest.mock import MagicMock, patch\n\nfrom grasshopper.lib.fixtures import _get_tagged_scenarios\n\n\ndef test_get_tagged_scenarios_happy():\n    config_mock = MagicMock()\n    config_mock.getoption = lambda a: \"asdf\"\n    raw_yaml_dict = {\n        \"scenario1\": {\"tags\": [\"asdf\"]},\n        \"scenario2\": {\"tags\": [\"foo\"]},\n    }\n    tagged_scenarios = _get_tagged_scenarios(raw_yaml_dict, config_mock, fspath=\"asdf\")\n    assert tagged_scenarios == {\"scenario1\": {\"tags\": [\"asdf\", \"scenario1\"]}}", "\n\n@patch.dict(os.environ, {\"TAGS\": \"foo\"}, clear=True)\ndef test_get_tagged_scenarios_happy_env_var():\n    config_mock = MagicMock()\n    config_mock.getoption = lambda a: None\n    raw_yaml_dict = {\n        \"scenario1\": {\"tags\": [\"asdf\"]},\n        \"scenario2\": {\"tags\": [\"foo\"]},\n    }\n    tagged_scenarios = _get_tagged_scenarios(raw_yaml_dict, config_mock, fspath=\"asdf\")\n    assert tagged_scenarios == {\"scenario2\": {\"tags\": [\"foo\", \"scenario2\"]}}", "\n\ndef test_get_tagged_scenarios_no_tags_supplied(caplog):\n    config_mock = MagicMock()\n    config_mock.getoption = lambda a: None\n    raw_yaml_dict = {\n        \"scenario1\": {\"tags\": [\"asdf\"]},\n        \"scenario2\": {\"tags\": [\"foo\"]},\n    }\n    tagged_scenarios = _get_tagged_scenarios(raw_yaml_dict, config_mock, fspath=\"asdf\")\n    expected = {\"scenario1\": {\"tags\": [\"asdf\"]}, \"scenario2\": {\"tags\": [\"foo\"]}}\n    assert tagged_scenarios == expected\n    assert \"ALL scenarios in asdf will be run!\" in caplog.text", ""]}
{"filename": "tests/unit/test__er_basic_console_reporter.py", "chunked_list": ["import logging\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom assertpy import assert_that\n\nfrom grasshopper.lib.reporting.er_basic_console_reporter import ERBasicConsoleReporter\nfrom grasshopper.lib.util.utils import epoch_time\n\n", "\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef capture_logging():\n    # this allows for all logging to show up in the fixture caplog\n    # caplog.text is just all the text, in one big str\n    # caplog.records lists each message logged separately, along with it's level\n    logging.root.setLevel(logging.DEBUG)\n    logging.root.propagate = True\n", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_suite_args():\n    args = {\n        \"grasshopper_global_args\": {\n            \"slack_webhook\": \"https://fake_url\",\n            \"rp_token\": \"fake-token\",\n        },\n        \"grasshopper_suite_args\": {\"runtime\": 10, \"users\": 2},\n    }\n    return args", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_test_args(fake_suite_args):\n    args = {\n        \"suite_args\": fake_suite_args,\n        \"journey_args\": {\"flow_name\": \"fake flow\", \"recipe_id\": 1234},\n    }\n    return args\n", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_passing_threshold():\n    threshold = {\n        \"less_than_in_ms\": 1000,\n        \"actual_value_in_ms\": 800,\n        \"percentile\": 0.8,\n        \"succeeded\": True,\n        \"http_method\": \"GET\",\n    }\n    return threshold", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_thresholds_single(fake_passing_threshold):\n    thresholds = [fake_passing_threshold]\n    return thresholds\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_trends_single_threshold(fake_thresholds_single):\n    trends = {\"fake trend\": {\"thresholds\": fake_thresholds_single}}\n    return trends", "@pytest.fixture(scope=\"session\")\ndef fake_trends_single_threshold(fake_thresholds_single):\n    trends = {\"fake trend\": {\"thresholds\": fake_thresholds_single}}\n    return trends\n\n\n@pytest.fixture(scope=\"session\")\ndef mock_locust_environment(fake_trends_single_threshold):\n    mock = MagicMock()\n    mock.stats.trends = fake_trends_single_threshold\n    return mock", "\n\ndef test__er_basic_console_reporter_name():\n    name = ERBasicConsoleReporter.get_name()\n    # according to the interface, this method just needs to return a non-zero length\n    # string\n    assert_that(name).is_type_of(str)\n    assert_that(name).is_not_empty()\n    new_name = \"brand new name\"\n    # only requirement here is that the er doesn't raise any errors if a notification\n    # method should call set_name; note that any given er has the freedom to not do\n    # anything when it's called ;) tbh, the set_name method mostly is used for unit\n    # testing\n    ERBasicConsoleReporter.set_name(new_name)", "\n\ndef test__er_basic_console_reporter_pre_suite(fake_suite_args, caplog):\n    start_epoch = epoch_time()\n    expected_line_1 = (\n        f\"Starting Suite fake suite 1 at {start_epoch} with the following arguments:\"\n    )\n    expected_line_2 = (\n        \"grasshopper_global_args=={'slack_webhook': 'https://fake_url', \"\n        \"'rp_token': 'fake-token'}\"\n    )\n    expected_line_3 = \"grasshopper_suite_args=={'runtime': 10, 'users': 2}\"\n    ERBasicConsoleReporter.event_pre_suite(\"fake suite 1\", start_epoch, fake_suite_args)\n\n    assert_that(caplog.text).contains(expected_line_1, expected_line_2, expected_line_3)", "\n\ndef test__er_basic_console_reporter_post_suite(fake_suite_args, caplog):\n    start_epoch = epoch_time()\n    end_epoch = start_epoch + 1000\n    expected_line_1 = \"Suite fake suite 1 complete\"\n    ERBasicConsoleReporter.event_post_suite(\n        \"fake suite 1\", start_epoch, end_epoch, fake_suite_args\n    )\n\n    assert_that(caplog.text).contains(expected_line_1)", "\n\ndef test__er_basic_console_reporter_pre_test(fake_test_args, caplog):\n    start_epoch = epoch_time()\n    expected_line_1 = (\n        f\"Starting Test fake test 1 at {start_epoch} with the following arguments:\"\n    )\n    expected_line_2 = (\n        \"suite_args=={'grasshopper_global_args': {'slack_webhook': \"\n        \"'https://fake_url', 'rp_token': 'fake-token'}, 'grasshopper_suite_args': \"\n        \"{'runtime': 10, 'users': 2}}\"\n    )\n    expected_line_3 = \"journey_args=={'flow_name': 'fake flow', 'recipe_id': 1234}\"\n    ERBasicConsoleReporter.event_pre_test(\"fake test 1\", start_epoch, fake_test_args)\n\n    assert_that(caplog.text).contains(expected_line_1, expected_line_2, expected_line_3)", "\n\ndef test__er_basic_console_reporter_post_test(\n    fake_test_args, mock_locust_environment, caplog\n):\n    start_epoch = epoch_time()\n    end_epoch = start_epoch + 1000\n    expected_line_1 = (\n        f\"Test fake test 1 complete at {end_epoch} with the following results:\"\n    )\n    expected_line_2 = f\"Epoch time stamp: {start_epoch} - {end_epoch}\"\n    expected_line_3 = \"Trend_Name                     Percentile Limit      Actual    \"\n    expected_line_4 = \"fake trend                     80         1000ms     800ms     \"\n\n    ERBasicConsoleReporter.event_post_test(\n        \"fake test 1\", start_epoch, end_epoch, mock_locust_environment, fake_test_args\n    )\n    # for some reason assert_that is not working correctly for just this one set of\n    # messages, so use the standard assert instead\n    assert expected_line_1 in caplog.text\n    assert expected_line_2 in caplog.text\n    assert expected_line_3 in caplog.text\n    assert expected_line_4 in caplog.text", ""]}
{"filename": "tests/unit/test__fixture__env_var_args.py", "chunked_list": ["import os\nfrom unittest.mock import patch\n\n# Alteryx Packages\nfrom tests.unit.conftest import (\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,", "    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817\n    ConfigurationConstants as CC,\n)\n\nFIXTURE_UNDER_TEST = \"env_var_args\"\n", "FIXTURE_UNDER_TEST = \"env_var_args\"\n\n# Some comments on testing fixture env_var_args.\n# There are fairly complete unit tests for the extra_env_var_args fixture that assert\n# that _this fixture's value_ will _only_ be a list of strings or [].\n# Therefore, we only need to test this fixture passing in [] (default, what you get if\n# you don't patch it) and a list of strings.\n# Similarly, we only need to test env_var_args receiving a non-zero length string as the\n# custom prefix.\n# If any of the tests here are failing, it would be worthwhile to check the results of", "# custom prefix.\n# If any of the tests here are failing, it would be worthwhile to check the results of\n# `test__fixture__extra_env_var_args` and `test__fixture__env_var_prefix_key` before\n# debugging here.\n# Additionally, note that the expected behavior for env_var_args is lower case all keys\n# _after_ matching.\n\n# Matching some values from COMPLETE_ATTRS\nFAKE_ENVIRON_SOME_MATCHING_VALUES = {\"INFLUX_HOST\": \"1.1.1.1\", \"RUNTIME\": \"10\"}\n", "FAKE_ENVIRON_SOME_MATCHING_VALUES = {\"INFLUX_HOST\": \"1.1.1.1\", \"RUNTIME\": \"10\"}\n\n# Keys that don't match anything from COMPLETE_ATTRS\nFAKE_ENVIRON_NON_MATCHING_VALUES = {\n    \"COMPLETELY_UNRELATED_KEY\": \"completely unrelated value\",\n    \"ANOTHER_UNRELATED_KEY\": \"some other value\",\n}\n\n# Mix of keys that match and don't match COMPLETE_ATTRS\nFAKE_ENVIRON_MIX_MATCH_AND_NOT = {", "# Mix of keys that match and don't match COMPLETE_ATTRS\nFAKE_ENVIRON_MIX_MATCH_AND_NOT = {\n    **FAKE_ENVIRON_SOME_MATCHING_VALUES,\n    **FAKE_ENVIRON_NON_MATCHING_VALUES,\n}\n\n# Keys that match external sources (env_var_keys & custom prefix)\nFAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES = {\n    \"MY_UNIT_TEST_VAR\": \"testing var from extra keys set is loaded\",\n    \"UNIT_TEST_EXTRA\": \"testing that extra env var args are loaded\",", "    \"MY_UNIT_TEST_VAR\": \"testing var from extra keys set is loaded\",\n    \"UNIT_TEST_EXTRA\": \"testing that extra env var args are loaded\",\n}\n\n# Keys that won't resolve correctly paired with certain prefix keys\nFAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE = {\n    \"OOOPS_\": \"testing load will skip over any env var names it can't resolve\",\n}\n\n# All different types of keys, in order to test that the fixture can load all the", "\n# All different types of keys, in order to test that the fixture can load all the\n# different types of sources _at the same time_ and not include irrelevant data\nFAKE_ENVIRON_ALL_SOURCES_AND_NON_MATCHING_VALUES = {\n    **FAKE_ENVIRON_MIX_MATCH_AND_NOT,\n    **FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES,\n    **FAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE,\n    \"MY_\": \"value that should not load\",\n}\n", "}\n\n\n@patch.dict(os.environ, FAKE_ENVIRON_SOME_MATCHING_VALUES, clear=True)\ndef test__env_var_args__happy(pytester):\n    \"\"\"Fixture should load any env vars that match keys in COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    expected_config_args = {\"influx_host\": \"1.1.1.1\", \"runtime\": \"10\"}\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_NON_MATCHING_VALUES, clear=True)\ndef test__env_var_args__no_matches(pytester):\n    \"\"\"Fixture should return an empty config if there are no values that match keys\n    in COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, clear=True)\ndef test__env_var_args__environ_empty(pytester):\n    \"\"\"Fixture should return an empty config if environ is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES, clear=True)\ndef test__env_var_args__matching_extra_keys(pytester):\n    \"\"\"Fixture should load keys that match values in extra_env_var_keys.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def extra_env_var_keys():\n            return [\"UNIT_TEST_EXTRA\", \"KEY_THAT_IS_NOT_IN_ENVIRON\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_extra\": \"testing that extra env var args are loaded\"\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_MATCHING_OTHER_SOURCES, clear=True)\ndef test__env_var_args__matching_custom_prefix(pytester):\n    \"\"\"Fixture should load any keys in environ that begin with the custom prefix,\n    discarding the prefix portion.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'MY_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_var\": \"testing var from extra keys set is loaded\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_KEYS_THAT_WONT_RESOLVE, clear=True)\ndef test__env_var_args__skipping_keys_that_dont_resolve_correctly(pytester):\n    \"\"\"Fixture returns an empty config if matching key results in a invalid key.\n    This can happen if, once the prefix is stripped, the key is an empty string.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'OOOPS_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_ALL_SOURCES_AND_NON_MATCHING_VALUES, clear=True)\ndef test__env_var_args__loading_from_all_sources(pytester):\n    \"\"\"Fixture should load values from all different sources, and ignore anything in\n    environ that doesn't match.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'MY_'\n\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"UNIT_TEST_EXTRA\"]\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"unit_test_var\": \"testing var from extra keys set is loaded\",\n        \"unit_test_extra\": \"testing that extra env var args are loaded\",\n        \"influx_host\": \"1.1.1.1\",\n        \"runtime\": \"10\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, FAKE_ENVIRON_SOME_MATCHING_VALUES, clear=True)\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__env_var_args__empty_attrs(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(os.environ, clear=True)\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__env_var_args__empty_attrs_and_empty_env_vars(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", ""]}
{"filename": "tests/unit/test_shapes.py", "chunked_list": ["from grasshopper.lib.util.shapes import Default\n\n\ndef test_default_shape():\n    shape = Default(runtime=100, spawn_rate=100, users=100)\n    assert shape.configured_runtime == 100\n\n    shape = Default()\n    assert shape.configured_runtime == 120\n\n    shape = Default(spawn_rate=100, users=100)\n    assert shape.configured_runtime == 120\n\n    Default.DEFAULT_RUNTIME = 1\n    shape = Default()\n    assert shape.configured_runtime == 1\n\n    Default.DEFAULT_RUNTIME = 1\n    shape = Default(runtime=100)\n    assert shape.configured_runtime == 100", ""]}
{"filename": "tests/unit/test_BaseJourney.py", "chunked_list": ["import logging\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom grasshopper.lib.journeys.base_journey import BaseJourney\nfrom grasshopper.lib.fixtures.grasshopper_constants import GrasshopperConstants\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef base_journey():\n    BaseJourney.reset_class_attributes()", "\n@pytest.fixture(scope=\"function\", autouse=True)\ndef base_journey():\n    BaseJourney.reset_class_attributes()\n\n\ndef test_set_tags_via_defaults():\n    journey = BaseJourney(MagicMock())\n    journey.defaults = {\"tags\": {\"foo\": \"bar\"}}\n    journey.update_incoming_scenario_args({\"not tags\": \"something else\"})\n    journey._merge_incoming_defaults_and_params()\n    assert journey._incoming_test_parameters[\"tags\"] == {\"foo\": \"bar\"}", "\n\ndef test_set_tags_none():\n    journey = BaseJourney(MagicMock())\n    journey._merge_incoming_defaults_and_params()\n    assert journey._incoming_test_parameters[\"tags\"] == {}\n\n\ndef test_set_test_parameters():\n    journey = BaseJourney(MagicMock())\n    journey.defaults = {\"foo1\": \"bar\", \"foo2\": \"bar2\"}\n    journey.replace_incoming_scenario_args({\"foo2\": \"bar3\", \"foo3\": \"bar4\"})\n    journey._merge_incoming_defaults_and_params()\n    assert journey._incoming_test_parameters == {\n        \"foo1\": \"bar\",\n        \"foo2\": \"bar3\",\n        \"foo3\": \"bar4\",\n    }", "def test_set_test_parameters():\n    journey = BaseJourney(MagicMock())\n    journey.defaults = {\"foo1\": \"bar\", \"foo2\": \"bar2\"}\n    journey.replace_incoming_scenario_args({\"foo2\": \"bar3\", \"foo3\": \"bar4\"})\n    journey._merge_incoming_defaults_and_params()\n    assert journey._incoming_test_parameters == {\n        \"foo1\": \"bar\",\n        \"foo2\": \"bar3\",\n        \"foo3\": \"bar4\",\n    }", "\n\ndef test_set_test_parameters_with_thresholds_and_tags():\n    BaseJourney.replace_incoming_scenario_args(\n        {\n            \"thresholds\": {\n                \"asdf1\": {\"type\": \"get\", \"limit\": 1},\n                \"asdf2\": {\"type\": \"post\", \"limit\": 2, \"percentile\": 0.8},\n            },\n            \"tags\": {\"foo\": \"bar\"},\n        }\n    )\n    journey = BaseJourney(MagicMock())\n    journey.on_start()\n    journey._set_thresholds()\n    assert journey.environment.stats.trends[\"asdf1\"] == {\n        \"thresholds\": [\n            {\n                \"less_than_in_ms\": 1,\n                \"actual_value_in_ms\": None,\n                \"percentile\": GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                \"succeeded\": None,\n                \"http_method\": \"GET\",\n            },\n        ],\n        \"tags\": {\"foo\": \"bar\"},\n    }\n    assert journey.environment.stats.trends[\"asdf2\"] == {\n        \"thresholds\": [\n            {\n                \"less_than_in_ms\": 2,\n                \"actual_value_in_ms\": None,\n                \"percentile\": 0.8,\n                \"succeeded\": None,\n                \"http_method\": \"POST\",\n            },\n        ],\n        \"tags\": {\"foo\": \"bar\"},\n    }", "\n\ndef test_verify_thresholds_collection_shape_successful():\n    journey = BaseJourney(MagicMock())\n    valid_thresholds_collection = {\n        \"asdf1\": {\"type\": \"get\", \"limit\": 1},\n        \"asdf2\": {\"type\": \"post\", \"limit\": 2, \"percentile\": 0.8},\n    }\n    assert journey._verify_thresholds_collection_shape(valid_thresholds_collection)\n", "\n\ndef test_verify_thresholds_collection_shape_invalid_limit(caplog):\n    journey = BaseJourney(MagicMock())\n    invalid_thresholds_collection_limit = {\n        \"asdf1\": {\"type\": \"get\", \"limit\": \"invalid_limit\"},\n        \"asdf2\": {\"type\": \"post\", \"limit\": 2, \"percentile\": 0.8},\n    }\n    with caplog.at_level(logging.WARNING):\n        is_valid = journey._verify_thresholds_collection_shape(\n            invalid_thresholds_collection_limit\n        )\n    assert not is_valid\n    assert \"limit\" in caplog.text", "\n\ndef test_verify_thresholds_collection_shape_invalid_type(caplog):\n    journey = BaseJourney(MagicMock())\n    invalid_thresholds_collection_type = {\n        \"asdf1\": {\"type\": \"invalid_type\", \"limit\": 1},\n        \"asdf2\": {\"type\": \"post\", \"limit\": 2, \"percentile\": 0.8},\n    }\n    with caplog.at_level(logging.WARNING):\n        is_valid = journey._verify_thresholds_collection_shape(\n            invalid_thresholds_collection_type\n        )\n    assert not is_valid\n    assert \"type `INVALID_TYPE` is invalid\" in caplog.text", "\n\ndef test_verify_thresholds_collection_shape_invalid_shape(caplog):\n    journey = BaseJourney(MagicMock())\n    invalid_thresholds_collection = []\n    with caplog.at_level(logging.WARNING):\n        is_valid = journey._verify_thresholds_collection_shape(\n            invalid_thresholds_collection\n        )\n    assert not is_valid\n    assert \"mapping\" in caplog.text", ""]}
{"filename": "tests/unit/test__fixture__legacy_fixtures.py", "chunked_list": ["from assertpy import assert_that\n\n# Alteryx Packages\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\ndef test__grasshopper_scenario_args(grasshopper_scenario_args):\n    # this fixture should always return {} now, but not removed in order to support\n    # backwards compatibility\n    assert_that(grasshopper_scenario_args).is_instance_of(GHConfiguration)\n    assert_that(grasshopper_scenario_args).is_equal_to(GHConfiguration())", "\n\ndef test__grasshopper_args(complete_configuration, grasshopper_args):\n    # grasshopper_args fixture should just match the complete_configuration fixture\n    # now, similarly kept around for backwards compatibility\n    assert_that(grasshopper_args).is_instance_of(GHConfiguration)\n    assert_that(grasshopper_args).is_equal_to(complete_configuration)\n"]}
{"filename": "tests/unit/test__reporter_extensions.py", "chunked_list": ["from unittest.mock import MagicMock, call\n\nimport pytest\nfrom assertpy import assert_that\nfrom locust import env as locust_environment\n\nfrom grasshopper.lib.reporting.reporter_extensions import (\n    IExtendedReporter,\n    ReporterExtensions,\n)", "    ReporterExtensions,\n)\nfrom grasshopper.lib.util.utils import epoch_time\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef clear_class_registrations():\n    ReporterExtensions.clear_registrations()\n\n", "\n\n@pytest.fixture\ndef mock_er_1():\n    mock_er = MagicMock(spec=IExtendedReporter)\n    mock_er.get_name.return_value = \"magicmock er #1\"\n    return mock_er\n\n\n@pytest.fixture\ndef mock_er_2():\n    mock_er = MagicMock(spec=IExtendedReporter)\n    mock_er.get_name.return_value = \"magicmock er #2\"\n    return mock_er", "\n@pytest.fixture\ndef mock_er_2():\n    mock_er = MagicMock(spec=IExtendedReporter)\n    mock_er.get_name.return_value = \"magicmock er #2\"\n    return mock_er\n\n\n@pytest.fixture\ndef mock_er_3():\n    mock_er = MagicMock(spec=IExtendedReporter)\n    mock_er.get_name.return_value = \"magicmock er #3\"\n    return mock_er", "@pytest.fixture\ndef mock_er_3():\n    mock_er = MagicMock(spec=IExtendedReporter)\n    mock_er.get_name.return_value = \"magicmock er #3\"\n    return mock_er\n\n\n@pytest.fixture\ndef mock_er_invalid():\n    mock_er = MagicMock()\n    mock_er.get_name.side_effect = AttributeError\n    return mock_er", "def mock_er_invalid():\n    mock_er = MagicMock()\n    mock_er.get_name.side_effect = AttributeError\n    return mock_er\n\n\ndef test__registration_happy(mock_er_1):\n    ReporterExtensions.register_er(mock_er_1)\n    assert_that(ReporterExtensions.ers).is_length(1)\n    assert_that(ReporterExtensions.registrations).is_length(1)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())", "\n\ndef test__registration_invalid(mock_er_1, mock_er_invalid):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_invalid)\n    assert_that(ReporterExtensions.ers).is_length(1)\n    assert_that(ReporterExtensions.registrations).is_length(1)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())\n\n\ndef test__registration_multiple(mock_er_1, mock_er_2, mock_er_3):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    ReporterExtensions.register_er(mock_er_3)\n    assert_that(ReporterExtensions.ers).is_length(3)\n    assert_that(ReporterExtensions.registrations).is_length(3)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_2.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_3.get_name())", "\n\ndef test__registration_multiple(mock_er_1, mock_er_2, mock_er_3):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    ReporterExtensions.register_er(mock_er_3)\n    assert_that(ReporterExtensions.ers).is_length(3)\n    assert_that(ReporterExtensions.registrations).is_length(3)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_2.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_3.get_name())", "\n\ndef test__unregister_happy(mock_er_1):\n    ReporterExtensions.register_er(mock_er_1)\n    assert_that(ReporterExtensions.registrations).is_length(1)\n    ReporterExtensions.unregister_er(mock_er_1.get_name())\n    assert_that(ReporterExtensions.registrations).is_length(0)\n\n\ndef test__unregister_multiple(mock_er_1, mock_er_2, mock_er_3):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    ReporterExtensions.register_er(mock_er_3)\n    assert_that(ReporterExtensions.registrations).is_length(3)\n    ReporterExtensions.unregister_er(mock_er_2.get_name())\n    assert_that(ReporterExtensions.registrations).is_length(2)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_3.get_name())", "\ndef test__unregister_multiple(mock_er_1, mock_er_2, mock_er_3):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    ReporterExtensions.register_er(mock_er_3)\n    assert_that(ReporterExtensions.registrations).is_length(3)\n    ReporterExtensions.unregister_er(mock_er_2.get_name())\n    assert_that(ReporterExtensions.registrations).is_length(2)\n    assert_that(ReporterExtensions.registrations).contains(mock_er_1.get_name())\n    assert_that(ReporterExtensions.registrations).contains(mock_er_3.get_name())", "\n\ndef test__unregister_empty():\n    ReporterExtensions.unregister_er(\"non existent er\")\n\n\ndef test__unregister_not_registered(mock_er_1):\n    ReporterExtensions.register_er(mock_er_1)\n    assert_that(ReporterExtensions.registrations).is_length(1)\n    # checking that no errors are raised on the unregister and also that the contents\n    # of the registrations were not affected\n    ReporterExtensions.unregister_er(\"non existent er\")\n    assert_that(ReporterExtensions.registrations).is_length(1)", "\n\ndef test__notify_pre_test_happy(mock_er_1, mock_er_2):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    test_name = \"fake test\"\n    start_epoch = epoch_time()\n    test_args = {\"key\": \"value\"}\n    ReporterExtensions.notify_pre_test(test_name, start_epoch, test_args=test_args)\n    mock_er_1.event_pre_test.assert_called_once_with(\n        test_name, start_epoch, test_args=test_args\n    )\n    mock_er_2.event_pre_test.assert_called_once_with(\n        test_name, start_epoch, test_args=test_args\n    )", "\n\ndef test__notify_post_test_happy(mock_er_1, mock_er_2):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    test_name = \"fake test\"\n    start_epoch = epoch_time()\n    end_epoch = start_epoch + 1000\n    test_args = {\"key\": \"value\"}\n    locust_mock = MagicMock(spec=locust_environment)\n    ReporterExtensions.notify_post_test(\n        test_name, start_epoch, end_epoch, locust_mock, test_args=test_args\n    )\n    mock_er_1.event_post_test.assert_called_once_with(\n        test_name, start_epoch, end_epoch, locust_mock, test_args=test_args\n    )\n    mock_er_2.event_post_test.assert_called_once_with(\n        test_name, start_epoch, end_epoch, locust_mock, test_args=test_args\n    )", "\n\ndef test__notify_post_test_multiple_events(mock_er_1, mock_er_2):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    test_name = \"fake test\"\n    start_epoch = epoch_time()\n    end_epoch = start_epoch + 1000\n    test_args = {\"key\": \"value\"}\n    locust_mock = MagicMock(spec=locust_environment)\n    ReporterExtensions.notify_post_test(\n        test_name, start_epoch, end_epoch, locust_mock, test_args=test_args\n    )\n    test_name2 = \"fake test 2\"\n    ReporterExtensions.notify_post_test(\n        test_name2, start_epoch, end_epoch, locust_mock, test_args=test_args\n    )\n    calls = [\n        call(test_name, start_epoch, end_epoch, locust_mock, test_args=test_args),\n        call(test_name2, start_epoch, end_epoch, locust_mock, test_args=test_args),\n    ]\n    mock_er_1.event_post_test.assert_has_calls(calls, any_order=True)\n    mock_er_2.event_post_test.assert_has_calls(calls, any_order=True)", "\n\ndef test__notify_pre_suite_happy(mock_er_1, mock_er_2):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    suite_name = \"fake suite\"\n    start_epoch = epoch_time()\n    suite_args = {\"key\": \"value\"}\n    ReporterExtensions.notify_pre_suite(suite_name, start_epoch, suite_args=suite_args)\n    mock_er_1.event_pre_suite.assert_called_once_with(\n        suite_name, start_epoch, suite_args=suite_args\n    )\n    mock_er_2.event_pre_suite.assert_called_once_with(\n        suite_name, start_epoch, suite_args=suite_args\n    )", "\n\ndef test__notify_post_suite_happy(mock_er_1, mock_er_2):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions.register_er(mock_er_2)\n    suite_name = \"fake suite\"\n    start_epoch = epoch_time()\n    end_epoch = start_epoch + 1000\n    suite_args = {\"key\": \"value\"}\n    ReporterExtensions.notify_post_suite(\n        suite_name, start_epoch, end_epoch, suite_args=suite_args\n    )\n    mock_er_1.event_post_suite.assert_called_once_with(\n        suite_name, start_epoch, end_epoch, suite_args=suite_args\n    )\n    mock_er_2.event_post_suite.assert_called_once_with(\n        suite_name, start_epoch, end_epoch, suite_args=suite_args\n    )", "\n\ndef test__notify_empty():\n    test_name = \"fake test\"\n    start_epoch = epoch_time()\n    test_args = {\"key\": \"value\"}\n    # checking to make sure that no errors are raised if a notify event happens and\n    # the registrations are empty\n    ReporterExtensions.notify_pre_test(test_name, start_epoch, test_args=test_args)\n", "\n\ndef test__notify_extra_kwargs(mock_er_1):\n    ReporterExtensions.register_er(mock_er_1)\n    test_name = \"fake test\"\n    start_epoch = epoch_time()\n    test_args = {\"key1\": \"value1\"}\n    ReporterExtensions.notify_pre_test(\n        test_name, start_epoch, test_args=test_args, key2=\"value2\"\n    )\n    # this is checking that any extra kwargs are being passed on to the event method\n    mock_er_1.event_pre_test.assert_called_once_with(\n        test_name, start_epoch, test_args=test_args, key2=\"value2\"\n    )", "\n\ndef test__notify_event_invalid_event_name(mock_er_1):\n    ReporterExtensions.register_er(mock_er_1)\n    ReporterExtensions._notify_event(\"event_that_does_not_exist\")\n"]}
{"filename": "tests/unit/test__fixture__global_defaults.py", "chunked_list": ["from unittest.mock import patch\n\nfrom assertpy import assert_that\n\n# Alteryx Packages\n# alias to make patches easier to read\nfrom tests.unit.conftest import (  # noqa: I202\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,", "    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817\n    ConfigurationConstants as CC,\n)\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration", ")\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\nFIXTURE_UNDER_TEST = \"global_defaults\"\n\nATTRS_WITH_NO_DEFAULTS = {\n    \"attr1\": {\n        \"opts\": [\"--attr1\"],\n        \"attrs\": {\n            \"action\": \"store\",", "        \"attrs\": {\n            \"action\": \"store\",\n            \"help\": \"Attr1\",\n        },\n    },\n    \"attr2\": {\n        \"opts\": [\"--attr2\"],\n        \"attrs\": {\n            \"action\": \"store\",\n            \"help\": \"Attr2\",", "            \"action\": \"store\",\n            \"help\": \"Attr2\",\n        },\n    },\n}\n\nATTRS_FEW_VALUES_AND_NOT_ALL_DEFAULTS = {\n    \"attr1\": {\n        \"opts\": [\"--attr1\"],\n        \"attrs\": {", "        \"opts\": [\"--attr1\"],\n        \"attrs\": {\n            \"action\": \"store\",\n            \"help\": \"Attr1\",\n        },\n        \"default\": \"Default 1\",\n    },\n    \"attr2\": {\n        \"opts\": [\"--attr2\"],\n        \"attrs\": {", "        \"opts\": [\"--attr2\"],\n        \"attrs\": {\n            \"action\": \"store\",\n            \"help\": \"Attr2\",\n        },\n        \"default\": 2,\n    },\n    \"attr3\": {\n        \"opts\": [\"--attr3\"],\n        \"attrs\": {", "        \"opts\": [\"--attr3\"],\n        \"attrs\": {\n            \"action\": \"store\",\n            \"help\": \"Attr3\",\n        },\n    },\n}\n\n\ndef test__global_defaults(current_global_defaults, global_defaults):\n    \"\"\"Test fixture matches the currently defined defaults in ATTRS.\"\"\"\n    assert_that(global_defaults).is_instance_of(GHConfiguration)\n    assert_that(global_defaults).is_equal_to(current_global_defaults)", "\ndef test__global_defaults(current_global_defaults, global_defaults):\n    \"\"\"Test fixture matches the currently defined defaults in ATTRS.\"\"\"\n    assert_that(global_defaults).is_instance_of(GHConfiguration)\n    assert_that(global_defaults).is_equal_to(current_global_defaults)\n\n\n@patch.dict(CC.COMPLETE_ATTRS, ATTRS_FEW_VALUES_AND_NOT_ALL_DEFAULTS, clear=True)\ndef test__global_defaults__finds_defaults_in_arbitrary_attrs(pytester):\n    \"\"\"Fixture loads from an arbitrary COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    expected_config_args = {\n        \"attr1\": \"Default 1\",\n        \"attr2\": 2,\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        ),\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "def test__global_defaults__finds_defaults_in_arbitrary_attrs(pytester):\n    \"\"\"Fixture loads from an arbitrary COMPLETE_ATTRS.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    expected_config_args = {\n        \"attr1\": \"Default 1\",\n        \"attr2\": 2,\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        ),\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(CC.COMPLETE_ATTRS, ATTRS_WITH_NO_DEFAULTS, clear=True)\ndef test__global_defaults__no_defaults_in_attrs(pytester):\n    \"\"\"Test fixture doesn't raise an error if COMPLETE_ATTRS does not have any attrs\n    with defaults defined.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__global_defaults__empty_attrs(pytester):\n    \"\"\"Fixture should not raise errors if configuration file is empty.\"\"\"\n\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=\"\")\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", ""]}
{"filename": "tests/unit/__init__.py", "chunked_list": ["\"\"\"The unit tests for the grasshopper package.\"\"\"\n"]}
{"filename": "tests/unit/test__shared_reporting.py", "chunked_list": ["import pytest\nfrom assertpy import assert_that\n\nfrom grasshopper.lib.reporting.shared_reporting import SharedReporting\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_passing_threshold():\n    threshold = {\n        \"less_than_in_ms\": 1000,\n        \"actual_value_in_ms\": 800,\n        \"percentile\": 0.8,\n        \"succeeded\": True,\n        \"http_method\": \"GET\",\n    }\n    return threshold", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_failing_threshold():\n    threshold = {\n        \"less_than_in_ms\": 1000,\n        \"actual_value_in_ms\": 1200,\n        \"percentile\": 0.8,\n        \"succeeded\": False,\n        \"http_method\": \"POST\",\n    }\n    return threshold", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_thresholds_single(fake_passing_threshold):\n    thresholds = [fake_passing_threshold]\n    return thresholds\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_thresholds_multiple(fake_passing_threshold, fake_failing_threshold):\n    thresholds = [fake_passing_threshold, fake_failing_threshold]\n    return thresholds", "@pytest.fixture(scope=\"session\")\ndef fake_thresholds_multiple(fake_passing_threshold, fake_failing_threshold):\n    thresholds = [fake_passing_threshold, fake_failing_threshold]\n    return thresholds\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_trends_single_threshold(fake_thresholds_single):\n    trends = {\"fake trend\": {\"thresholds\": fake_thresholds_single}}\n    return trends", "\n\n@pytest.fixture(scope=\"session\")\ndef fake_trends_multiple_thresholds(fake_thresholds_multiple):\n    trends = {\"fake trend\": {\"thresholds\": fake_thresholds_multiple}}\n    return trends\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_trends_multiple_trends(fake_thresholds_multiple):\n    trends = {\n        \"fake trend 1\": {\"thresholds\": fake_thresholds_multiple},\n        \"fake trend 2\": {\"thresholds\": fake_thresholds_multiple},\n    }\n    return trends", "@pytest.fixture(scope=\"session\")\ndef fake_trends_multiple_trends(fake_thresholds_multiple):\n    trends = {\n        \"fake trend 1\": {\"thresholds\": fake_thresholds_multiple},\n        \"fake trend 2\": {\"thresholds\": fake_thresholds_multiple},\n    }\n    return trends\n\n\n@pytest.fixture(scope=\"session\")\ndef fake_trends_no_thresholds(fake_thresholds_multiple):\n    trends = {\n        \"fake trend 1\": {},\n        \"fake trend 2\": {},\n    }\n    return trends", "\n@pytest.fixture(scope=\"session\")\ndef fake_trends_no_thresholds(fake_thresholds_multiple):\n    trends = {\n        \"fake trend 1\": {},\n        \"fake trend 2\": {},\n    }\n    return trends\n\n\ndef test__plain_threshold_string_happy(fake_passing_threshold):\n    expected_string = \"fake passing trend             80         1000ms     800ms     \"\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        \"fake passing trend\", fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__plain_threshold_string_happy(fake_passing_threshold):\n    expected_string = \"fake passing trend             80         1000ms     800ms     \"\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        \"fake passing trend\", fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)\n\n\ndef test__plain_threshold_string_custom_format_str(fake_passing_threshold):\n    expected_string = \"fake passing trend | 80 | 1000ms | 800ms\"\n    format_string = \"{} | {} | {} | {}\"\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        \"fake passing trend\", fake_passing_threshold, format_string=format_string\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__plain_threshold_string_custom_format_str(fake_passing_threshold):\n    expected_string = \"fake passing trend | 80 | 1000ms | 800ms\"\n    format_string = \"{} | {} | {} | {}\"\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        \"fake passing trend\", fake_passing_threshold, format_string=format_string\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)\n", "\n\ndef test__plain_threshold_string_invalid_threshold():\n    expected_string = (\n        \"UNABLE TO PROCESS THRESHOLD FOR TREND <fake trend>: KeyError | 'percentile'\"\n    )\n    threshold_string, _ = SharedReporting.plain_threshold_string(\"fake trend\", {})\n    assert_that(threshold_string).is_equal_to(expected_string)\n\n\ndef test__plain_threshold_string_trend_name_is_none(fake_passing_threshold):\n    expected_string = (\n        \"UNABLE TO PROCESS THRESHOLD FOR TREND <None>: TypeError | \"\n        \"unsupported format string passed to NoneType.__format__\"\n    )\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        None, fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__plain_threshold_string_trend_name_is_none(fake_passing_threshold):\n    expected_string = (\n        \"UNABLE TO PROCESS THRESHOLD FOR TREND <None>: TypeError | \"\n        \"unsupported format string passed to NoneType.__format__\"\n    )\n    threshold_string, _ = SharedReporting.plain_threshold_string(\n        None, fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__colorized_threshold_string_happy(fake_passing_threshold):\n    expected_string = (\n        \"\\x1b[1m\\x1b[32mfake trend                     80         \"\n        \"1000ms     800ms     \\x1b[0m\"\n    )\n    threshold_string, _ = SharedReporting.colorized_threshold_string(\n        \"fake trend\", fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__colorized_threshold_string_failed(fake_failing_threshold):\n    expected_string = (\n        \"\\x1b[1m\\x1b[31mfake trend                     80         \"\n        \"1000ms     1200ms    \\x1b[0m\"\n    )\n    threshold_string, _ = SharedReporting.colorized_threshold_string(\n        \"fake trend\", fake_failing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__colorized_threshold_string_invalid_threshold():\n    expected_string = (\n        \"\\x1b[1m\\x1b[31mUNABLE TO PROCESS THRESHOLD FOR TREND <fake trend>: \"\n        \"TypeError | 'NoneType' object is not subscriptable\\x1b[0m\"\n    )\n    threshold_string, _ = SharedReporting.colorized_threshold_string(\"fake trend\", None)\n    assert_that(threshold_string).is_equal_to(expected_string)\n", "\n\ndef test__colorized_threshold_string_empty_threshold():\n    expected_string = (\n        \"\\x1b[1m\\x1b[31mUNABLE TO PROCESS THRESHOLD FOR TREND <fake trend>: \"\n        \"KeyError | 'percentile'\\x1b[0m\"\n    )\n    threshold_string, _ = SharedReporting.colorized_threshold_string(\"fake trend\", {})\n    assert_that(threshold_string).is_equal_to(expected_string)\n", "\n\ndef test__colorized_threshold_trend_name_is_none(fake_passing_threshold):\n    expected_string = (\n        \"\\x1b[1m\\x1b[31mUNABLE TO PROCESS THRESHOLD FOR TREND <None>: \"\n        \"TypeError | unsupported format string passed to NoneType.__format__\\x1b[0m\"\n    )\n    threshold_string, _ = SharedReporting.colorized_threshold_string(\n        None, fake_passing_threshold\n    )\n    assert_that(threshold_string).is_equal_to(expected_string)", "\n\ndef test__calculate_threshold_lines_happy(fake_trends_single_threshold):\n    expected_line = \"fake trend                     80         1000ms     800ms     \"\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_single_threshold)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line)\n\n\ndef test__calculate_threshold_lines_multiple_thresholds(\n    fake_trends_multiple_thresholds,\n):\n    expected_line_1 = \"fake trend                     80         1000ms     800ms     \"\n    expected_line_2 = \"fake trend                     80         1000ms     1200ms    \"\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_multiple_thresholds)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line_1)\n    assert_that(lines).contains(expected_line_2)", "\ndef test__calculate_threshold_lines_multiple_thresholds(\n    fake_trends_multiple_thresholds,\n):\n    expected_line_1 = \"fake trend                     80         1000ms     800ms     \"\n    expected_line_2 = \"fake trend                     80         1000ms     1200ms    \"\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_multiple_thresholds)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line_1)\n    assert_that(lines).contains(expected_line_2)", "\n\ndef test__calculate_threshold_lines_multiple_trends(fake_trends_multiple_trends):\n    expected_line_1 = \"fake trend 1                   80         1000ms     800ms     \"\n    expected_line_2 = \"fake trend 1                   80         1000ms     1200ms    \"\n    expected_line_3 = \"fake trend 2                   80         1000ms     800ms     \"\n    expected_line_4 = \"fake trend 2                   80         1000ms     1200ms    \"\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_multiple_trends)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line_1)\n    assert_that(lines).contains(expected_line_2)\n    assert_that(lines).contains(expected_line_3)\n    assert_that(lines).contains(expected_line_4)", "\n\ndef test__calculate_threshold_lines_custom_format_str(fake_trends_multiple_trends):\n    expected_line_1 = \"fake trend 1 | 80 | 1000ms | 800ms\"\n    expected_line_2 = \"fake trend 1 | 80 | 1000ms | 1200ms\"\n    expected_line_3 = \"fake trend 2 | 80 | 1000ms | 800ms\"\n    expected_line_4 = \"fake trend 2 | 80 | 1000ms | 1200ms\"\n    format_string = \"{} | {} | {} | {}\"\n    lines = SharedReporting.calculate_threshold_lines(\n        fake_trends_multiple_trends, format_string=format_string\n    )\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line_1)\n    assert_that(lines).contains(expected_line_2)\n    assert_that(lines).contains(expected_line_3)\n    assert_that(lines).contains(expected_line_4)", "\n\ndef test__calculate_threshold_lines_custom_formatter(fake_trends_multiple_trends):\n    expected_line_1 = (\n        \"\\x1b[1m\\x1b[32mfake trend 1                   80         1000ms     \"\n        \"800ms     \\x1b[0m\"\n    )\n    expected_line_2 = (\n        \"\\x1b[1m\\x1b[31mfake trend 1                   80         1000ms     \"\n        \"1200ms    \\x1b[0m\"\n    )\n    expected_line_3 = (\n        \"\\x1b[1m\\x1b[32mfake trend 2                   80         1000ms     \"\n        \"800ms     \\x1b[0m\"\n    )\n    expected_line_4 = (\n        \"\\x1b[1m\\x1b[31mfake trend 2                   80         1000ms     \"\n        \"1200ms    \\x1b[0m\"\n    )\n    lines = SharedReporting.calculate_threshold_lines(\n        fake_trends_multiple_trends,\n        formatter=SharedReporting.colorized_threshold_string,\n    )\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).contains(expected_line_1)\n    assert_that(lines).contains(expected_line_2)\n    assert_that(lines).contains(expected_line_3)\n    assert_that(lines).contains(expected_line_4)", "\n\ndef test__calculate_threshold_lines_empty_trends():\n    lines = SharedReporting.calculate_threshold_lines({})\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).is_empty()\n\n\ndef test__calculate_threshold_lines_no_thresholds(fake_trends_no_thresholds):\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_no_thresholds)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).is_empty()", "def test__calculate_threshold_lines_no_thresholds(fake_trends_no_thresholds):\n    lines = SharedReporting.calculate_threshold_lines(fake_trends_no_thresholds)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).is_empty()\n\n\ndef test__calculate_threshold_lines_none():\n    lines = SharedReporting.calculate_threshold_lines(None)\n    assert_that(lines).is_type_of(list)\n    assert_that(lines).is_empty()", ""]}
{"filename": "tests/unit/test_grasshopper_configuration_fixtures.py", "chunked_list": ["import os\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\nfrom assertpy import assert_that\n\n# Alteryx Packages\n# alias to make patches easier to read\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817", "# alias to make patches easier to read\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817\n    ConfigurationConstants as CC,\n)\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n@pytest.fixture\ndef expected_global_defaults():\n    defaults = {\n        \"slack_report_failures_only\": False,\n        \"shape\": \"Default\",\n        \"users\": 1.0,\n        \"runtime\": 120.0,\n        \"spawn_rate\": 1.0,\n        \"scenario_delay\": 0.0,\n        \"cleanup_s3\": True,\n        \"slack\": False,\n        \"influx\": False,\n        \"report_portal\": False,\n        \"rp_launch_name\": \"Grasshopper Performance Test Run | Launch name unknown\",\n    }\n    return defaults", "def expected_global_defaults():\n    defaults = {\n        \"slack_report_failures_only\": False,\n        \"shape\": \"Default\",\n        \"users\": 1.0,\n        \"runtime\": 120.0,\n        \"spawn_rate\": 1.0,\n        \"scenario_delay\": 0.0,\n        \"cleanup_s3\": True,\n        \"slack\": False,\n        \"influx\": False,\n        \"report_portal\": False,\n        \"rp_launch_name\": \"Grasshopper Performance Test Run | Launch name unknown\",\n    }\n    return defaults", "\n\ndef test__grasshopper_scenario_args(grasshopper_scenario_args):\n    # this fixture should always return {} now, but not removed in order to support\n    # backwards compatibility\n    assert_that(grasshopper_scenario_args).is_instance_of(GHConfiguration)\n    assert_that(grasshopper_scenario_args).is_equal_to({})\n\n\ndef test__grasshopper_args(complete_configuration, grasshopper_args):\n    # grasshopper_args fixture should just match the complete_configuration fixture\n    # now, again kept around for backwards compatibility\n    assert_that(grasshopper_args).is_instance_of(GHConfiguration)\n    assert_that(grasshopper_args).is_equal_to(complete_configuration)", "\ndef test__grasshopper_args(complete_configuration, grasshopper_args):\n    # grasshopper_args fixture should just match the complete_configuration fixture\n    # now, again kept around for backwards compatibility\n    assert_that(grasshopper_args).is_instance_of(GHConfiguration)\n    assert_that(grasshopper_args).is_equal_to(complete_configuration)\n\n\ndef test__global_defaults(expected_global_defaults, global_defaults):\n    assert_that(global_defaults).is_instance_of(GHConfiguration)\n    assert_that(global_defaults).is_equal_to(expected_global_defaults)", "def test__global_defaults(expected_global_defaults, global_defaults):\n    assert_that(global_defaults).is_instance_of(GHConfiguration)\n    assert_that(global_defaults).is_equal_to(expected_global_defaults)\n\n\n@patch.dict(\n    CC.COMPLETE_ATTRS,\n    {\n        \"attr1\": {\n            \"opts\": [\"--attr1\"],", "        \"attr1\": {\n            \"opts\": [\"--attr1\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Attr1\",\n            },\n        }\n    },\n    clear=True,\n)\ndef test__global_defaults_no_defaults_in_attrs(pytester):\n    \"\"\"Test fixture doesn't raise an error if COMPLETE_ATTRS does not have any attrs\n    with defaults defined.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import global_defaults\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(global_defaults):\n            assert_that(global_defaults).is_instance_of(GHConfiguration)\n            assert_that(global_defaults).is_equal_to(GHConfiguration())\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "    clear=True,\n)\ndef test__global_defaults_no_defaults_in_attrs(pytester):\n    \"\"\"Test fixture doesn't raise an error if COMPLETE_ATTRS does not have any attrs\n    with defaults defined.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import global_defaults\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(global_defaults):\n            assert_that(global_defaults).is_instance_of(GHConfiguration)\n            assert_that(global_defaults).is_equal_to(GHConfiguration())\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__global_defaults_empty_attrs(pytester):\n    \"\"\"Test fixture doesn't raise an error if COMPLETE_ATTRS is empty.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import global_defaults\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(global_defaults):\n            assert_that(global_defaults).is_instance_of(GHConfiguration)\n            assert_that(global_defaults).is_equal_to({})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_args(pytester):\n    \"\"\"Test fixture loads a configuration file correctly.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = (\n        Path(__file__).parent / Path(\"../fixture_testing_data\") / \"mock_basic.config\"\n    )\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {'influx': True,\n                    'users': 1.0,\n                    'spawn_rate': 1.0,\n                    'runtime': 600,\n                    'flow_name': 'POC flow',\n                    'recipe_name': 'Untitled recipe',\n                    'engine': 'photon'}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_args_missing_section(pytester):\n    \"\"\"Test fixture does not error on a missing section.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = (\n        Path(__file__).parent\n        / Path(\"../fixture_testing_data\")\n        / \"mock_missing_section.config\"\n    )\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {'influx': True,\n                    'flow_name': 'POC flow',\n                    'recipe_name': 'Untitled recipe',\n                    'engine': 'photon'}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_not_found(pytester):\n    \"\"\"Test fixture return empty config if the file is not found.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = \"bogus_path\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_invalid_yaml(pytester):\n    \"\"\"Test fixture returns an empty config if the file is not valid yaml.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = (\n        Path(__file__).parent\n        / Path(\"../fixture_testing_data\")\n        / \"mock_invalid_yaml.config\"\n    )\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_empty_yaml(pytester):\n    \"\"\"Test fixture does not error on empty yaml file.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = (\n        Path(__file__).parent\n        / Path(\"../fixture_testing_data\")\n        / \"mock_empty_yaml.config\"\n    )\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__grasshopper_config_file_extra_section(pytester):\n    \"\"\"Test fixture ignores any extra sections in the yaml file.\"\"\"\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = (\n        Path(__file__).parent\n        / Path(\"../fixture_testing_data\")\n        / \"mock_extra_section.config\"\n    )\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        f\"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n        + \"\"\"\n        @pytest.fixture\n        def expected_config_args():\n            args = {'influx': True,\n                    'flow_name': 'POC flow',\n                    'recipe_name': 'Untitled recipe',\n                    'engine': 'photon'}\n            return args\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import grasshopper_config_file_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(grasshopper_config_file_args,\n                expected_config_args):\n            assert_that(grasshopper_config_file_args).is_instance_of(GHConfiguration)\n            assert_that(grasshopper_config_file_args).is_equal_to(expected_config_args)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__env_var_prefix_key(env_var_prefix_key):\n    \"\"\"Validate default value is supplied for prefix.\"\"\"\n    assert_that(env_var_prefix_key).is_equal_to(\"GH_\")\n\n\ndef test__env_var_prefix_key_with_diff_prefix(pytester):\n    \"\"\"Test fixture uses alternate definition for prefix when supplied.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def configuration_prefix_key():\n            return 'MY_'\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import env_var_prefix_key\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_prefix_key):\n            assert_that(env_var_prefix_key).is_equal_to('MY_')\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__extra_env_var_keys(extra_env_var_keys):\n    \"\"\"Validate that fixture returns default for extra keys.\"\"\"\n    assert_that(extra_env_var_keys).is_equal_to([])\n\n\ndef test__extra_env_var_keys_defined(pytester):\n    \"\"\"Test fixture uses alternate definition for env var keys if supplied.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        # patch the fixture that optionally supplies extra keys\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"ENV1\", \"ENV2\"]\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import extra_env_var_keys\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(extra_env_var_keys):\n            assert_that(extra_env_var_keys).is_equal_to(['ENV1', 'ENV2'])\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\n@patch.dict(os.environ, {\"INFLUX_HOST\": \"1.1.1.1\", \"RUNTIME\": \"10\"})\ndef test__env_var_args_happy(pytester):\n    \"\"\"Test fixture loads values correctly from the env vars.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import os\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import env_var_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_args):\n            assert_that(env_var_args).is_instance_of(GHConfiguration)\n            assert_that(env_var_args).is_equal_to({\"influx_host\": \"1.1.1.1\",\n                \"runtime\": \"10\"})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\n@patch.dict(os.environ, clear=True)\ndef test__env_var_args_no_matches(pytester):\n    \"\"\"Test fixture return empty config if there are no matches in env vars.\"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import os\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import env_var_args\n\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_args):\n            assert_that(env_var_args).is_instance_of(GHConfiguration)\n            assert_that(env_var_args).is_equal_to({})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\n@patch.dict(\n    os.environ,\n    {\n        \"MY_UNIT_TEST_VAR\": \"some value\",\n        \"UNIT_TEST_EXTRA\": \"some other value\",\n        \"USERS\": \"100\",\n        \"INFLUX_HOST\": \"1.1.1.1\",\n        \"MY_\": \"key is testing load will skip over any env var names it can't resolve\",", "        \"INFLUX_HOST\": \"1.1.1.1\",\n        \"MY_\": \"key is testing load will skip over any env var names it can't resolve\",\n    },\n    clear=True,\n)\ndef test__env_var_args_extra_sources(pytester):\n    \"\"\"Test fixture loads correctly from extra sources, namely keys that\n    start with the specified prefix and pulling extra keys from the env vars.\n\n    Covers:\n    + case where an env var with the prefix produces a key that is empty, should skip\n    that value but proceed to load all other variables\n    + a mix of sources for env vars - vars using the prefix, vars that have one of the\n    extra keys, vars that match the list of configuration attrs\n\n    \"\"\"\n\n    # create a temporary conftest.py file\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import os\n\n        # patch the fixture that optionally supplies extra keys\n        @pytest.fixture(scope=\"session\")\n        def configuration_extra_env_var_keys():\n            return [\"UNIT_TEST_EXTRA\"]\n\n        # patch the fixture used to supply the path to the config file\n        @pytest.fixture(scope=\"session\")\n        def env_var_prefix_key():\n            return 'MY_'\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import env_var_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_args):\n            assert_that(env_var_args).is_instance_of(GHConfiguration)\n            assert_that(env_var_args).contains_entry({\"unit_test_var\": \"some value\"})\n            assert_that(env_var_args).contains_entry(\n                {\"unit_test_extra\": \"some other value\"})\n            assert_that(env_var_args).contains_entry({\"users\": \"100\"})\n            assert_that(env_var_args).contains_entry({\"influx_host\": \"1.1.1.1\"})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__cmdln_args_happy(pytester):\n    \"\"\"Test fixture loads correctly.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n        from unittest.mock import MagicMock\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--spawn_rate\":.3})\n            return mock\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import cmdln_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(cmdln_args):\n            assert_that(cmdln_args).is_equal_to({\"users\": 42, \"spawn_rate\":.3})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__cmdln_args_empty_cmdln(pytester):\n    \"\"\"Test fixture loads correctly if there are not command line args supplied.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n        from unittest.mock import MagicMock\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig()\n            return mock\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import cmdln_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(cmdln_args):\n            assert_that(cmdln_args).is_equal_to({})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__cmdln_args_extra_args_coming_from_cmdln(pytester):\n    \"\"\"Test fixture loads correctly if there are other args in the collection, such\n    as pytest args or custom args that a consumer might add in.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n        from unittest.mock import MagicMock\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--log_cli_level\": \"debug\"})\n            return mock\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import cmdln_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(cmdln_args):\n            assert_that(cmdln_args).is_equal_to({\"users\": 42})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__pre_processed_args_happy(pytester):\n    \"\"\"Test fixture loads correctly.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration({\"scenario_file\":\"fake_scenario_file.yaml\",\n                                      \"scenario_name\": \"fake_scenario_name\"})\n            return config\n\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import pre_processed_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(pre_processed_args):\n            expected = GHConfiguration({\"scenario_file\":\"fake_scenario_file.yaml\",\n                        \"scenario_name\": \"fake_scenario_name\"})\n            assert_that(pre_processed_args).is_instance_of(GHConfiguration)\n            assert_that(pre_processed_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__pre_processed_args_empty(pytester):\n    \"\"\"Test fixture returns an empty config if the pre-process args are not\n    specified.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration({})\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import pre_processed_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(pre_processed_args):\n            assert_that(pre_processed_args).is_instance_of(GHConfiguration)\n            assert_that(pre_processed_args).is_equal_to({})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__pre_processed_args_with_precedence(pytester):\n    \"\"\"Test fixture loads with the correct precedence applied.\n\n    This test also covers some other test cases:\n    + should be able to handle a source that is empty, in this case global defaults\n    + should be able to handle a source that has extra, unrelated keys in addition to\n    the keys it is looking for (config file args)\n    + should be able to handle one key coming from one source and the other key coming\n    from another source\n\n    \"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_config_file_not_the_one.yaml\"})\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_env_var_not_the_one.yaml\",\n                \"scenario_name\": \"this_is_the_one\"})\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration({\"scenario_file\":\n                \"from_cmdln_this_is_the_one.yaml\"})\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import pre_processed_args\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(pre_processed_args):\n            expected = GHConfiguration({\"scenario_file\":\n                \"from_cmdln_this_is_the_one.yaml\",\n                \"scenario_name\": \"this_is_the_one\"\n                })\n            assert_that(pre_processed_args).is_instance_of(GHConfiguration)\n            assert_that(pre_processed_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__scenario_file_args_happy(pytester):\n    \"\"\"Test fixture loads with the correct precedence applied.\"\"\"\n\n    mock_scenario_file_path = (\n        Path(__file__).parent / Path(\"../fixture_testing_data\") / \"mock_scenarios.yaml\"\n    )\n\n    pytester.makeconftest(\n        \"\"\"\n                import pytest\n                pre_config = {}\n        \"\"\"\n        + f\"\"\"\n                pre_config['scenario_file'] = \"{mock_scenario_file_path}\"\n                pre_config['scenario_name'] = \"scenario1\"\n        \"\"\"\n        + \"\"\"\n                # patch the fixture that supplies a valid file and scenario name\n                @pytest.fixture(scope=\"session\")\n                def pre_processed_args():\n                    return pre_config\n\n                # import the fixture we want to test\n                from grasshopper.lib.fixtures import scenario_file_args\n\n            \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(scenario_file_args):\n            expected = GHConfiguration({\"users\": 1,\n                                        \"spawn_rate\": 1,\n                                        \"runtime\": 600,\n                                        \"flow_name\" : 'POC flow',\n                                        \"recipe_name\": 'Untitled recipe',\n                                        \"engine\": 'photon',\n                                        'scenario_test_file_name': 'test__journey1.py',\n                                        'scenario_tags': ['smoke', 'trend'],\n                                        \"thresholds\":\n                                        '{\"{CUSTOM}PX_TREND_google_home\": 1000}'\n                                        })\n            assert_that(scenario_file_args).is_instance_of(GHConfiguration)\n            assert_that(scenario_file_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__scenario_file_args_scenario_not_in_file(pytester):\n    \"\"\"Test fixture returns an empty config if the scenario is not found in the file.\"\"\"\n\n    mock_scenario_file_path = (\n        Path(__file__).parent / Path(\"../fixture_testing_data\") / \"mock_scenarios.yaml\"\n    )\n\n    pytester.makeconftest(\n        \"\"\"\n                import pytest\n                pre_config = {}\n        \"\"\"\n        + f\"\"\"\n                pre_config['scenario_file'] = \"{mock_scenario_file_path}\"\n                pre_config['scenario_name'] = \"bogus\"\n        \"\"\"\n        + \"\"\"\n                # patch the fixture that supplies a valid file and scenario name\n                @pytest.fixture(scope=\"session\")\n                def pre_processed_args():\n                    return pre_config\n\n                # import the fixture we want to test\n                from grasshopper.lib.fixtures import scenario_file_args\n\n            \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(scenario_file_args):\n            expected = GHConfiguration()\n            assert_that(scenario_file_args).is_instance_of(GHConfiguration)\n            assert_that(scenario_file_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__scenario_file_args_missing_scenario(pytester):\n    \"\"\"Test fixture return empty config if the scenario_name arg is missing.\"\"\"\n\n    mock_scenario_file_path = (\n        Path(__file__).parent / Path(\"../fixture_testing_data\") / \"mock_scenarios.yaml\"\n    )\n\n    pytester.makeconftest(\n        \"\"\"\n                import pytest\n                pre_config = {}\n        \"\"\"\n        + f\"\"\"\n                pre_config['scenario_file'] = \"{mock_scenario_file_path}\"\n        \"\"\"\n        + \"\"\"\n                # patch the fixture that supplies a valid file and scenario name\n                @pytest.fixture(scope=\"session\")\n                def pre_processed_args():\n                    return pre_config\n\n                # import the fixture we want to test\n                from grasshopper.lib.fixtures import scenario_file_args\n\n            \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(scenario_file_args):\n            expected = GHConfiguration()\n            assert_that(scenario_file_args).is_instance_of(GHConfiguration)\n            assert_that(scenario_file_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__scenario_file_args_file_not_found(pytester):\n    \"\"\"Test fixture returns an empty config if the scenario_file path is not found.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n                import pytest\n                pre_config = {}\n                pre_config['scenario_file'] = \"bogus_file.yaml\"\n\n                # patch the fixture that supplies a valid file and scenario name\n                @pytest.fixture(scope=\"session\")\n                def pre_processed_args():\n                    return pre_config\n\n                # import the fixture we want to test\n                from grasshopper.lib.fixtures import scenario_file_args\n\n            \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(scenario_file_args):\n            expected = GHConfiguration()\n            assert_that(scenario_file_args).is_instance_of(GHConfiguration)\n            assert_that(scenario_file_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__scenario_file_args_missing_scenario_keys(pytester):\n    \"\"\"Test fixture loads everything present, even if scenario is 'missing' keys.\"\"\"\n\n    mock_scenario_file_path = (\n        Path(__file__).parent / Path(\"../fixture_testing_data\") / \"mock_scenarios.yaml\"\n    )\n\n    pytester.makeconftest(\n        \"\"\"\n                import pytest\n                pre_config = {}\n        \"\"\"\n        + f\"\"\"\n                pre_config['scenario_file'] = \"{mock_scenario_file_path}\"\n                pre_config['scenario_name'] = \"scenario2\"\n        \"\"\"\n        + \"\"\"\n                # patch the fixture that supplies a file and scenario name\n                @pytest.fixture(scope=\"session\")\n                def pre_processed_args():\n                    return pre_config\n\n                # import the fixture we want to test\n                from grasshopper.lib.fixtures import scenario_file_args\n\n            \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(scenario_file_args):\n            expected = GHConfiguration({'flow_name': 'POC flow',\n                                        'recipe_name': 'Untitled recipe',\n                                        'engine': 'photon',\n                                        'thresholds':\n                                            '{\"{CUSTOM}PX_TREND_google_home\": 1000}',\n                                        'scenario_tags': ['smoke', 'trend']})\n            assert_that(scenario_file_args).is_instance_of(GHConfiguration)\n            assert_that(scenario_file_args).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__merge_sources_happy(pytester):\n    \"\"\"Test fixture loads correctly.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        cmd = {\"key1\": \"CMD_the_one\", \"CMD\": \"the_one\"}\n        env = {\"key1\": \"ENV_not_me\",\n                \"key2\": \"ENV_the_one\",\n                \"ENV\": \"the_one\",\n                \"sparse\": \"ENV_the_one\"}\n        sce = {\"key1\": \"SCE_not_me\",\n                \"key2\": \"SCE_not_me\",\n                \"key3\": \"SCE_the_one\",\n                \"SCE\": \"the_one\",\n                \"extra_key4\": \"SCE_the_one\",\n                \"extra_key5\": \"SCE_the_one\"}\n        cfg = {\"key1\": \"CFG_not_me\",\n                \"key2\": \"CFG_not_me\",\n                \"key3\": \"CFG_not_me\",\n                \"key4\": \"CFG_the_one\",\n                \"CFG\": \"the_one\",\n                \"sparse\": \"CFG_not_me\"}\n        dft = {\"key1\": \"DFT_not_me\",\n                \"key2\": \"DFT_not_me\",\n                \"key3\": \"DFT_not_me\",\n                \"key4\": \"DFT_the_one\",\n                \"key5\": \"DFT_the_one\",\n                \"DFT\": \"the_one\"}\n\n        @pytest.fixture(scope=\"session\")\n        def expected_merge():\n            expected = {'key1': 'CMD_the_one',\n                        'key2': 'ENV_the_one',\n                        'key3': 'SCE_the_one',\n                        'key4': 'CFG_the_one',\n                        'key5': 'DFT_the_one',\n                        \"DFT\": \"the_one\",\n                        \"CFG\": \"the_one\",\n                        'SCE': 'the_one',\n                        'extra_key4': 'SCE_the_one',\n                        'extra_key5': 'SCE_the_one',\n                        'ENV': 'the_one',\n                        \"sparse\": \"ENV_the_one\",\n                        'CMD': 'the_one'}\n\n            merged = GHConfiguration(expected)\n            return merged\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration(dft)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration(cfg)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def scenario_file_args():\n            config = GHConfiguration(sce)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration(env)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration(cmd)\n            return config\n\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import merge_sources\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(merge_sources, expected_merge):\n            assert_that(merge_sources).is_instance_of(GHConfiguration)\n            assert_that(merge_sources).is_equal_to(expected_merge)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__merge_sources_empty_source(pytester):\n    \"\"\"Test fixture loads correctly if one of the sources is empty.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        cmd = {\"key1\": \"CMD_the_one\", \"CMD\": \"the_one\"}\n        env = {\"key1\": \"ENV_not_me\",\n                \"key2\": \"ENV_the_one\",\n                \"ENV\": \"the_one\",\n                \"sparse\": \"ENV_the_one\"}\n        sce = {}\n        cfg = {\"key1\": \"CFG_not_me\",\n                \"key2\": \"CFG_not_me\",\n                \"key4\": \"CFG_the_one\",\n                \"CFG\": \"the_one\",\n                \"sparse\": \"CFG_not_me\"}\n        dft = {\"key1\": \"DFT_not_me\",\n                \"key2\": \"DFT_not_me\",\n                \"key4\": \"DFT_the_one\",\n                \"key5\": \"DFT_the_one\",\n                \"DFT\": \"the_one\"}\n\n        @pytest.fixture(scope=\"session\")\n        def expected_merge():\n            expected = {'key1': 'CMD_the_one',\n                        'key2': 'ENV_the_one',\n                        'key4': 'CFG_the_one',\n                        'key5': 'DFT_the_one',\n                        \"DFT\": \"the_one\",\n                        \"CFG\": \"the_one\",\n                        'ENV': 'the_one',\n                        \"sparse\": \"ENV_the_one\",\n                        'CMD': 'the_one'}\n\n            merged = GHConfiguration(expected)\n            return merged\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration(dft)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration(cfg)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def scenario_file_args():\n            config = GHConfiguration(sce)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration(env)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration(cmd)\n            return config\n\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import merge_sources\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(merge_sources, expected_merge):\n            assert_that(merge_sources).is_instance_of(GHConfiguration)\n            assert_that(merge_sources).is_equal_to(expected_merge)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__merge_sources_all_sources_empty(pytester):\n    \"\"\"Test fixture returns an empty config if all the sources are empty.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def scenario_file_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration()\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration()\n            return config\n\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import merge_sources\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(merge_sources):\n            assert_that(merge_sources).is_instance_of(GHConfiguration)\n            assert_that(merge_sources).is_equal_to(GHConfiguration())\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__merge_sources_invalid_source(pytester):\n    \"\"\"Test fixture does not raise an error if one source is invalid.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        cmd = {\"key1\": \"CMD_the_one\", \"CMD\": \"the_one\"}\n        env = {\"key1\": \"ENV_not_me\",\n                \"key2\": \"ENV_the_one\",\n                \"ENV\": \"the_one\",\n                \"sparse\": \"ENV_the_one\"}\n        sce = {\"key1\": \"SCE_not_me\",\n                \"key2\": \"SCE_not_me\",\n                \"key3\": \"SCE_the_one\",\n                \"SCE\": \"the_one\",\n                \"extra_key4\": \"SCE_the_one\",\n                \"extra_key5\": \"SCE_the_one\"}\n        dft = {\"key1\": \"DFT_not_me\",\n                \"key2\": \"DFT_not_me\",\n                \"key3\": \"DFT_not_me\",\n                \"key5\": \"DFT_the_one\",\n                \"DFT\": \"the_one\"}\n\n        @pytest.fixture(scope=\"session\")\n        def expected_merge():\n            expected = {'key1': 'CMD_the_one',\n                        'key2': 'ENV_the_one',\n                        'key3': 'SCE_the_one',\n                        'key5': 'DFT_the_one',\n                        \"DFT\": \"the_one\",\n                        'SCE': 'the_one',\n                        'extra_key4': 'SCE_the_one',\n                        'extra_key5': 'SCE_the_one',\n                        'ENV': 'the_one',\n                        \"sparse\": \"ENV_the_one\",\n                        'CMD': 'the_one'}\n\n            merged = GHConfiguration(expected)\n            return GHConfiguration(dft)\n\n        @pytest.fixture(scope=\"session\")\n        def global_defaults():\n            config = GHConfiguration(dft)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_args():\n            return None\n\n        @pytest.fixture(scope=\"session\")\n        def scenario_file_args():\n            config = GHConfiguration(sce)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def env_var_args():\n            config = GHConfiguration(env)\n            return config\n\n        @pytest.fixture(scope=\"session\")\n        def cmdln_args():\n            config = GHConfiguration(cmd)\n            return config\n\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import merge_sources\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(merge_sources, expected_merge):\n            assert_that(merge_sources).is_instance_of(GHConfiguration)\n            assert_that(merge_sources).is_equal_to(expected_merge)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__typecast_happy(pytester):\n    \"\"\"Test fixture applies the 'typecast' lambdas correctly.\n\n    Covers the following test cases:\n    + applying all typecast lambdas at least once\n    + checks that all the values that should be typecast (per the\n    ConfigurationConstants class) were actually typecast\n    + that if a value is already the correct format, no errors are raised\n    + extra values that don't have a typecast are passed through to the return\n    configuration\n    \"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n        @pytest.fixture(scope=\"session\")\n        def merge_sources():\n            cfg_values = {\"slack_report_failures_only\": \"false\",\n                            \"users\": \"1\",\n                            \"runtime\": 180,\n                            \"spawn_rate\": \".01\",\n                            \"scenario_delay\": \"20\",\n                            \"cleanup_s3\": True,\n                            \"slack\": \"false\",\n                            \"influx\": \"false\",\n                            \"report_portal\": \"false\",\n                            \"thresholds\": '{\"{CUSTOM}PX_TREND_google_home\": 1000}',\n                            \"no_typecast_str\": \"value not typecast\",\n                            \"no_typecast_int\": 10\n                            }\n            config = GHConfiguration(cfg_values)\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import typecast\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(typecast):\n            expected_dict = {'slack_report_failures_only': False,\n                            'users': 1,\n                            'runtime': 180.0,\n                            'spawn_rate': 0.01,\n                            'scenario_delay': 20.0,\n                            'cleanup_s3': True,\n                            'slack': False,\n                            'influx': False,\n                            'report_portal': False,\n                            'thresholds': {'{CUSTOM}PX_TREND_google_home': 1000},\n                            'no_typecast_str': 'value not typecast',\n                            'no_typecast_int': 10}\n\n            expected = GHConfiguration(expected_dict)\n            assert_that(typecast).is_instance_of(GHConfiguration)\n            assert_that(typecast).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__typecast_cast_fails(pytester):\n    \"\"\"Test fixture errors if a cast fails.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n        @pytest.fixture(scope=\"session\")\n        def merge_sources():\n            cfg_values = {\"runtime\": \"this is not a float\"}\n            config = GHConfiguration(cfg_values)\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import typecast\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(typecast):\n            # should raise a ValueError\n            pass\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(errors=1)", "\n\ndef test__typecast_no_casting(pytester):\n    \"\"\"Test fixture returns incoming values if there is no typecasting that needs to\n    happen.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n        @pytest.fixture(scope=\"session\")\n        def merge_sources():\n            cfg_values = {\"no_typecast_str\": \"value not typecast\"}\n            config = GHConfiguration(cfg_values)\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import typecast\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(typecast):\n            expected = GHConfiguration({\"no_typecast_str\": \"value not typecast\"})\n            assert_that(typecast).is_instance_of(GHConfiguration)\n            assert_that(typecast).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__typecast_empty_source(pytester):\n    \"\"\"Test fixture return empty config if incoming args are empty.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n        @pytest.fixture(scope=\"session\")\n        def merge_sources():\n            config = GHConfiguration()\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import typecast\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(typecast):\n            assert_that(typecast).is_instance_of(GHConfiguration)\n            assert_that(typecast).is_equal_to(GHConfiguration())\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__process_shape_happy(pytester):\n    \"\"\"Test fixture correctly loads the shape into the config.\n\n    Also, covers the case where a shape is overriding some values, with one key\n    previously existing in the config and one key not existing.\n\n    \"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n\n        @pytest.fixture(scope=\"session\")\n        def typecast():\n            config = GHConfiguration(shape=\"Trend\", spawn_rate=20)\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import process_shape\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Trend\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(process_shape):\n            assert_that(process_shape).is_instance_of(GHConfiguration)\n            assert_that(process_shape).contains_entry({\"shape\": \"Trend\"},\n                                                        {\"users\": 10},\n                                                        {\"spawn_rate\": .1})\n            assert_that(process_shape[\"shape_instance\"]).is_instance_of(Trend)\n    \"\"\"\n    )\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__process_shape_existing_instance(pytester):\n    \"\"\"Test fixture loads correctly if shape_instance already has a value.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Spike\n\n        @pytest.fixture(scope=\"session\")\n        def typecast():\n            config = GHConfiguration({\"shape\": \"Spike\", \"shape_instance\": Spike()})\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import process_shape\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Spike\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(process_shape):\n            assert_that(process_shape).is_instance_of(GHConfiguration)\n            assert_that(process_shape).is_length(2)\n            shape_instance = process_shape[\"shape_instance\"]\n            assert_that(shape_instance).is_instance_of(Spike)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__process_shape_not_found(pytester):\n    \"\"\"Test fixture errors if it is unable to load the specified shape.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        @pytest.fixture(scope=\"session\")\n        def typecast():\n            config = GHConfiguration({\"shape\": \"bogus\"})\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import process_shape\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Spike\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(process_shape):\n            pass\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(errors=1)", "\n\ndef test__process_shape_overrides(pytester):\n    \"\"\"Test fixture correctly is merging in 'approved' overrides from the shape\n    instance.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Testingfixturesonly\n\n        @pytest.fixture(scope=\"session\")\n        def typecast():\n            config = GHConfiguration(shape=\"Testingfixturesonly\",\n                                    key_that_should_not_override=\"the one\",\n                                    some_other_key=\"some other value\",\n                                    users=100)\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import process_shape\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from grasshopper.lib.util.shapes import Testingfixturesonly\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(process_shape):\n            assert_that(process_shape).is_instance_of(GHConfiguration)\n            assert_that(process_shape).is_length(7)\n            shape_instance = process_shape[\"shape_instance\"]\n            assert_that(shape_instance).is_instance_of(Testingfixturesonly)\n\n            # Smoke shape returns some overrides, make sure _only_ those that are\n            # \"approved\" are put into the dictionary\n            assert_that(process_shape).contains_entry({\"runtime\": 10})\n            assert_that(process_shape).contains_entry({\"users\": 20})\n            assert_that(process_shape).contains_entry({\"spawn_rate\": 0.004})\n            assert_that(process_shape).contains_entry({\"key_that_should_not_override\":\n                                                        \"the one\"})\n            assert_that(process_shape).contains_entry({\"some_other_key\":\n                                                        \"some other value\"})\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", "\n\ndef test__complete_configuration(pytester):\n    \"\"\"Test fixture returns incoming args, with only legacy args being translated to\n    the new key name.\"\"\"\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        import sys\n\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        expected_global_defaults = {\"slack_report_failures_only\": False,\n                                    \"shape\": \"Default\",\n                                    \"users\": 1.0,\n                                    \"runtime\": 120.0,\n                                    \"spawn_rate\": 1.0,\n                                    \"scenario_delay\": 0.0,\n                                    \"cleanup_s3\": True,\n                                    \"slack\": False,\n                                    \"influx\": False,\n                                    \"report_portal\": False\n                                }\n        @pytest.fixture(scope=\"session\")\n        def process_shape():\n            config = GHConfiguration(**expected_global_defaults, influxdb=\"2.2.2.2\")\n            return config\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import complete_configuration\n    \"\"\"\n    )\n\n    # create a temporary pytest test file\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(complete_configuration):\n            args = {\"slack_report_failures_only\": False,\n                        \"shape\": \"Default\",\n                        \"users\": 1.0,\n                        \"runtime\": 120.0,\n                        \"spawn_rate\": 1.0,\n                        \"scenario_delay\": 0.0,\n                        \"cleanup_s3\": True,\n                        \"slack\": False,\n                        \"influx\": False,\n                        \"report_portal\": False,\n                        \"influxdb\": \"2.2.2.2\",\n                        \"influx_host\": \"2.2.2.2\"\n                    }\n            expected = GHConfiguration(args)\n            assert_that(complete_configuration).is_instance_of(GHConfiguration)\n            assert_that(complete_configuration).is_equal_to(expected)\n    \"\"\"\n    )\n\n    # run all tests with pytest\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)", ""]}
{"filename": "tests/unit/test__fixture__env_var_prefix_key.py", "chunked_list": ["import logging\n\nfrom assertpy import assert_that\n\nfrom tests.unit.conftest import (  # noqa: I202\n    CONFTEST_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n\nFIXTURE_UNDER_TEST = \"env_var_prefix_key\"", "\nFIXTURE_UNDER_TEST = \"env_var_prefix_key\"\n\n\ndef test__env_var_prefix_key_default(env_var_prefix_key):\n    \"\"\"Validate default value is supplied for prefix.\"\"\"\n    assert_that(env_var_prefix_key).is_equal_to(\"GH_\")\n\n\ndef test__env_var_prefix_key__with_diff_prefix(pytester):\n    \"\"\"Fixture should return an alternate prefix when supplied.\"\"\"\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_prefix_key():\n            return 'MY_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_prefix_key):\n            assert_that(env_var_prefix_key).is_equal_to('MY_')\n    \"\"\"\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\ndef test__env_var_prefix_key__with_diff_prefix(pytester):\n    \"\"\"Fixture should return an alternate prefix when supplied.\"\"\"\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_prefix_key():\n            return 'MY_'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_prefix_key):\n            assert_that(env_var_prefix_key).is_equal_to('MY_')\n    \"\"\"\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__env_var_prefix_key__with_empty_prefix_str(pytester, caplog):\n    \"\"\"Fixture should fall bqck to the default if invalid prefix is supplied.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_prefix_key():\n            return ''\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_prefix_key):\n            assert_that(env_var_prefix_key).is_equal_to('GH_')\n    \"\"\"\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Fixture configuration_prefix_key may only be a non zero \"\n        \"length str\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", "\n\ndef test__env_var_prefix_key__prefix_is_none(pytester, caplog):\n    \"\"\"Fixture should fall bqck to the default if invalid prefix is supplied.\"\"\"\n\n    patches = \"\"\"\n        @pytest.fixture(scope=\"session\")\n        def configuration_prefix_key():\n            return None\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture(env_var_prefix_key):\n            assert_that(env_var_prefix_key).is_equal_to('GH_')\n    \"\"\"\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Fixture configuration_prefix_key may only be a non zero \"\n        \"length str\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", ""]}
{"filename": "tests/unit/test__fixture__grasshopper_config_file_args.py", "chunked_list": ["import logging\n\nfrom tests.unit.conftest import (\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    calculate_path,\n    perform_fixture_test_with_optional_log_capture,\n)", "    perform_fixture_test_with_optional_log_capture,\n)\n\nFIXTURE_UNDER_TEST = \"grasshopper_config_file_args\"\n\n\ndef test__grasshopper_config_file_args__happy(pytester):\n    # calculate the path before going into the other pytest context\n    mock_config_file_path = calculate_path(\"mock_basic.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"influx\": True,\n        \"users\": 1.0,\n        \"spawn_rate\": 1.0,\n        \"runtime\": 600,\n        \"flow_name\": \"POC flow\",\n        \"recipe_name\": \"Untitled recipe\",\n        \"engine\": \"photon\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__grasshopper_config_file_args__file_not_found(pytester, caplog):\n    \"\"\"Fixture should log a warning and return an empty config if path not found.\"\"\"\n    mock_config_file_path = calculate_path(\"bogus.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Skipping loading from grasshopper configuration file \"\n        \"because (.+) not found\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", "\n\ndef test__grasshopper_config_file_args__missing_section(pytester):\n    \"\"\"Fixture should silently ignore a missing section.\"\"\"\n    mock_config_file_path = calculate_path(\"mock_missing_section.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"influx\": True,\n        \"flow_name\": \"POC flow\",\n        \"recipe_name\": \"Untitled recipe\",\n        \"engine\": \"photon\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__grasshopper_config_file_args__extra_section(pytester):\n    \"\"\"Fixture should silently ignore any extra section(s).\"\"\"\n    mock_config_file_path = calculate_path(\"mock_extra_section.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\n        \"influx\": True,\n        \"flow_name\": \"POC flow\",\n        \"recipe_name\": \"Untitled recipe\",\n        \"engine\": \"photon\",\n    }\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__grasshopper_config_file_args__invalid_yaml(pytester, caplog):\n    \"\"\"Fixture should log a warning and return an empty config if not valid yaml.\"\"\"\n    mock_config_file_path = calculate_path(\"mock_invalid_yaml.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Unable to parse yaml file\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", "\n\ndef test__grasshopper_config_file_args__empty_yaml(pytester, caplog):\n    \"\"\"Fixture should log a warning and return an empty config if file is empty.\"\"\"\n    mock_config_file_path = calculate_path(\"mock_empty_yaml.config\")\n\n    patches = f\"\"\"@pytest.fixture(scope=\"session\")\n        def grasshopper_config_file_path():\n            return '{mock_config_file_path}'\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n    msg = {\n        \"target_level\": logging.WARNING,\n        \"target_message_re\": \"Unable to parse yaml file\",\n    }\n    perform_fixture_test_with_optional_log_capture(\n        pytester, caplog=caplog, target_messages=msg\n    )", ""]}
{"filename": "tests/unit/test_utils.py", "chunked_list": ["import time\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom termcolor import colored\n\nfrom grasshopper.lib.fixtures.grasshopper_constants import GrasshopperConstants\nfrom grasshopper.lib.util.check_constants import CheckConstants\nfrom grasshopper.lib.util.listeners import GrasshopperListeners\nfrom grasshopper.lib.util.utils import (", "from grasshopper.lib.util.listeners import GrasshopperListeners\nfrom grasshopper.lib.util.utils import (\n    check,\n    current_method_name,\n    custom_trend,\n    highlight_print,\n    logger,\n    report_checks_to_console,\n    report_thresholds_to_console,\n)", "    report_thresholds_to_console,\n)\n\n\n@pytest.fixture\ndef example_trends_dict():\n    trends = {\n        \"Trend One\": {\n            \"thresholds\": [\n                {\n                    \"percentile\": GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                    \"less_than_in_ms\": 1000,\n                    \"actual_value_in_ms\": 600,\n                    \"http_method\": \"CUSTOM\",\n                },\n                {\n                    \"percentile\": GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                    \"less_than_in_ms\": 1000,\n                    \"actual_value_in_ms\": 1200,\n                    \"http_method\": \"CUSTOM\",\n                },\n            ]\n        },\n        \"Trend Two\": {\n            \"thresholds\": [\n                {\n                    \"percentile\": GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                    \"less_than_in_ms\": 2000,\n                    \"actual_value_in_ms\": 1400,\n                    \"http_method\": \"CUSTOM\",\n                },\n                {\n                    \"percentile\": GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                    \"less_than_in_ms\": 2000,\n                    \"actual_value_in_ms\": 2800,\n                    \"http_method\": \"CUSTOM\",\n                },\n            ]\n        },\n    }\n    return trends", "\n\n@pytest.fixture\ndef example_checks_dict():\n    checks = {\n        \"Passing\": {\n            \"passed\": 3,\n            \"failed\": 0,\n            \"total\": 3,\n            \"verdict\": CheckConstants.VERDICT_ALL_PASSED,\n            \"percentage_passed_display\": \"100.0%\",\n            \"warning_threshold\": 1,\n        },\n        \"Failing\": {\n            \"passed\": 0,\n            \"failed\": 3,\n            \"total\": 3,\n            \"verdict\": CheckConstants.VERDICT_FAILED,\n            \"percentage_passed_display\": \"0.0%\",\n            \"warning_threshold\": 1,\n        },\n        \"Warning\": {\n            \"passed\": 2,\n            \"failed\": 1,\n            \"total\": 3,\n            \"verdict\": CheckConstants.VERDICT_PASSED_RATE_OVER_THRESHOLD,\n            \"percentage_passed_display\": \"66.67%\",\n            \"warning_threshold\": 0.5,\n        },\n    }\n    return checks", "\n\n@pytest.fixture\ndef mock_logging(monkeypatch):\n    logging_mock = MagicMock()\n    monkeypatch.setattr(logger, \"info\", logging_mock)\n    monkeypatch.setattr(logger, \"error\", logging_mock)\n    monkeypatch.setattr(logger, \"warning\", logging_mock)\n    monkeypatch.setattr(logger, \"fatal\", logging_mock)\n    monkeypatch.setattr(logger, \"debug\", logging_mock)\n    return logging_mock", "\n\ndef run_trend(environment, trend_name, func):\n    @custom_trend(trend_name)\n    def runner(env):\n        func(env)\n\n    return runner(environment)\n\n\ndef test_basic_checks(mock_logging: MagicMock):\n    \"\"\"\n    A check will add the correct data to the environment.stats.checks dictionary.\n    Each check should have a dictionary of its own with the check name as the key.\n    If the \"checks\" dictionary does not exist, one will be created.\n    \"\"\"\n    env = MagicMock()\n    del env.stats.checks\n\n    check(\"Sample 1\", True, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95}\n    }\n    assert env.stats.checks == expected\n    mock_logging.assert_not_called()\n\n    check(\"Sample 2\", False, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95},\n        \"Sample 2\": {\"passed\": 0, \"failed\": 1, \"total\": 1, \"warning_threshold\": 0.95},\n    }\n    assert env.stats.checks == expected\n    mock_logging.assert_called_once_with(\"Check failed: Sample 2\")", "\n\ndef test_basic_checks(mock_logging: MagicMock):\n    \"\"\"\n    A check will add the correct data to the environment.stats.checks dictionary.\n    Each check should have a dictionary of its own with the check name as the key.\n    If the \"checks\" dictionary does not exist, one will be created.\n    \"\"\"\n    env = MagicMock()\n    del env.stats.checks\n\n    check(\"Sample 1\", True, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95}\n    }\n    assert env.stats.checks == expected\n    mock_logging.assert_not_called()\n\n    check(\"Sample 2\", False, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95},\n        \"Sample 2\": {\"passed\": 0, \"failed\": 1, \"total\": 1, \"warning_threshold\": 0.95},\n    }\n    assert env.stats.checks == expected\n    mock_logging.assert_called_once_with(\"Check failed: Sample 2\")", "\n\ndef test_stacking_checks():\n    \"\"\"\n    A check will look for a pre-existing check of the same name and add data there.\n    A check will not overwrite or increment the data of another check.\n    \"\"\"\n    env = MagicMock()\n\n    check(\"Sample 1\", True, env=env)\n    check(\"Sample 1\", False, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 1, \"total\": 2, \"warning_threshold\": 0.95}\n    }\n    assert env.stats.checks == expected\n\n    check(\"Sample 2\", True, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 1, \"failed\": 1, \"total\": 2, \"warning_threshold\": 0.95},\n        \"Sample 2\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95},\n    }\n    assert env.stats.checks == expected\n\n    check(\"Sample 1\", True, env=env)\n    expected = {\n        \"Sample 1\": {\"passed\": 2, \"failed\": 1, \"total\": 3, \"warning_threshold\": 0.95},\n        \"Sample 2\": {\"passed\": 1, \"failed\": 0, \"total\": 1, \"warning_threshold\": 0.95},\n    }\n    assert env.stats.checks == expected", "\n\ndef test_check_warning():\n    \"\"\"\n    Check warnings will be properly set.\n    \"\"\"\n    env = MagicMock()\n    check(\"Warning value set\", True, env=env, flexible_warning=0.5)\n    assert env.stats.checks[\"Warning value set\"][\"warning_threshold\"] == 0.5\n", "\n\ndef test_check_failure_message(mock_logging):\n    \"\"\"\n    Check that (optional) failure messages are logged.\n    \"\"\"\n    env = MagicMock()\n    check(\"Test logging message\", False, env=env, msg_on_failure=\"Test logging message\")\n    mock_logging.assert_called_with(\"Failure message: Test logging message\")\n", "\n\ndef test_check_halt(mock_logging):\n    \"\"\"\n    A failing check will stop further execution if halt_on_failure is True.\n    \"\"\"\n    env = MagicMock()\n    check(\"Testing halt_on_failure\", False, env=env, halt_on_failure=True)\n    mock_logging.assert_called_with(\n        \"Check failed with halt_on_failure enabled. Stopping runner\"\n    )\n    env.runner.quit.assert_called_once()", "\n\ndef test_new_trend_payload():\n    \"\"\"\n    A trend will fire an event with the proper attributes, as well as a\n    correct response time.\n    \"\"\"\n\n    mock = MagicMock()\n    fire = mock.environment.events.request.fire\n    expected = {\n        # response_time not included, since we can't predict the value\n        \"request_type\": \"CUSTOM\",\n        \"name\": \"test_sleep_100_ms\",\n        \"response_time\": 0,\n        \"response_length\": 0,\n        \"response\": None,\n        \"context\": {},\n        \"exception\": None,\n    }\n\n    run_trend(mock, \"test_sleep_100_ms\", lambda a: time.sleep(0.1))\n\n    fire.assert_called_once()\n    assert expected.keys() <= fire.call_args.kwargs.keys()", "\n\ndef test_initialize():\n    \"\"\"\n    Should set the test start and stop.\n    \"\"\"\n    mock = MagicMock()\n    env = mock.parent.environment\n    GrasshopperListeners(environment=env)\n\n    env.events.test_start.add_listener.assert_called_once()\n    env.events.test_stop.add_listener.assert_called_once()", "\n\ndef test_current_method_name():\n    assert current_method_name() == \"test_current_method_name\"\n\n\ndef test_highlight_print(mock_logging):\n    highlight_print(\"Testing highlighted print\")\n    expected = colored(\"Testing highlighted print\", \"green\", attrs=[\"bold\"])\n    mock_logging.assert_called_with(expected)", "\n\ndef test_threshold_console_output(mock_logging, example_trends_dict):\n    # This would be set in _append_trend_data\n    example_trends_dict[\"Trend One\"][\"thresholds\"][0][\"succeeded\"] = True\n    example_trends_dict[\"Trend One\"][\"thresholds\"][1][\"succeeded\"] = False\n    example_trends_dict[\"Trend Two\"][\"thresholds\"][0][\"succeeded\"] = True\n    example_trends_dict[\"Trend Two\"][\"thresholds\"][1][\"succeeded\"] = False\n\n    report_thresholds_to_console(example_trends_dict)\n\n    call_strings = [\n        \"-------------------------------- THRESHOLD REPORT \"\n        \"--------------------------------\",\n        \"{:<45} {:<10} {:<10} {:<10}\".format(\n            \"Trend_Name\", \"Percentile\", \"Limit\", \"Actual\"\n        ),\n        colored(\n            \"{:<45} {:<10} {:<10} {:<10}\".format(\"Trend One\", 90, \"1000ms\", \"600ms\"),\n            \"green\",\n            attrs=[\"bold\"],\n        ),\n        colored(\n            \"{:<45} {:<10} {:<10} {:<10}\".format(\"Trend One\", 90, \"1000ms\", \"1200ms\"),\n            \"red\",\n            attrs=[\"bold\"],\n        ),\n        colored(\n            \"{:<45} {:<10} {:<10} {:<10}\".format(\"Trend Two\", 90, \"2000ms\", \"1400ms\"),\n            \"green\",\n            attrs=[\"bold\"],\n        ),\n        colored(\n            \"{:<45} {:<10} {:<10} {:<10}\".format(\"Trend Two\", 90, \"2000ms\", \"2800ms\"),\n            \"red\",\n            attrs=[\"bold\"],\n        ),\n    ]\n    for i in range(len(call_strings)):\n        assert mock_logging.call_args_list[i][0][0] == call_strings[i]", "\n\ndef test_checks_console_output(mock_logging, example_checks_dict):\n    report_checks_to_console(example_checks_dict)\n    call_strings = [\n        \"------------------------------------------------------- CHECKS REPORT \"\n        \"-------------------------------------------------------\",\n        \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\n            \"Check_Name\", \"Passed\", \"Failed\", \"Total\", \"Percentage\"\n        ),\n        colored(\n            \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\"Passing\", 3, 0, 3, \"100.0%\"),\n            \"green\",\n            attrs=[\"bold\"],\n        ),\n        colored(\n            \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\"Failing\", 0, 3, 3, \"0.0%\"),\n            \"red\",\n            attrs=[\"bold\"],\n        ),\n        colored(\n            \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\"Warning\", 2, 1, 3, \"66.67%\"),\n            \"yellow\",\n            attrs=[\"bold\"],\n        ),\n    ]\n    for i in range(len(call_strings)):\n        assert mock_logging.call_args_list[i][0][0] == call_strings[i]", "\n\ndef test_append_trend_data(mock_logging, example_trends_dict):\n    env = MagicMock()\n    listeners = GrasshopperListeners(environment=env)\n    listeners._append_trend_data(env)\n    mock_logging.assert_called_with(\"No threshold data to report.\")\n    mock_logging.reset_mock()\n\n    env.stats.trends = example_trends_dict\n    attrs = {\"get_response_time_percentile.return_value\": 1100}\n    env.stats.get = MagicMock(return_value=MagicMock(**attrs))\n    listeners._append_trend_data(env)\n    assert mock_logging.call_count == 7", "\n\ndef test_append_checks_data(mock_logging, example_checks_dict):\n    env = MagicMock()\n    listeners = GrasshopperListeners(environment=env)\n    listeners._append_checks_data(env)\n    mock_logging.assert_called_with(\"No checks data to report.\")\n    mock_logging.reset_mock()\n\n    env.stats.checks = example_checks_dict\n    listeners._append_checks_data(env)\n    assert mock_logging.call_count == 6", ""]}
{"filename": "tests/unit/conftest.py", "chunked_list": ["\"\"\"Module: Conftest.\"\"\"\nimport logging\nimport re\nfrom pathlib import Path\n\nimport pytest\n\npytest_plugins = [\"pytester\"]\n\nCONFTEST_TEMPLATE = \"\"\"", "\nCONFTEST_TEMPLATE = \"\"\"\n        import pytest\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n\n        # import the fixture(s) we need to patch\n        {patches}\n\n        # import the fixture we want to test\n        from grasshopper.lib.fixtures import {fixture_name}", "        # import the fixture we want to test\n        from grasshopper.lib.fixtures import {fixture_name}\n    \"\"\"\n\nPYFILE_TEMPLATE = \"\"\"\n        from grasshopper.lib.configuration.gh_configuration import GHConfiguration\n        from assertpy import assert_that\n\n        def test_grasshopper_fixture({fixture_name}):\n            assert_that({fixture_name}).is_instance_of(GHConfiguration)", "        def test_grasshopper_fixture({fixture_name}):\n            assert_that({fixture_name}).is_instance_of(GHConfiguration)\n            {validations}\n    \"\"\"\n\nPYFILE_ASSERT_EMPTY_CONFIG = (\n    \"assert_that({fixture_name}).is_equal_to(GHConfiguration())\"\n)\n\nPYFILE_ASSERT_EXPECTED_CONFIG = (", "\nPYFILE_ASSERT_EXPECTED_CONFIG = (\n    \"assert_that({fixture_name}).is_equal_to(GHConfiguration({expected}))\"\n)\n\n\n@pytest.fixture(scope=\"session\")\ndef current_global_defaults():\n    # Note the reason that we have a _separate_ dict here with the current defaults,\n    # is to be a cross check. If you were to grab the constant and do a calculation\n    # similar to fixture we are checking, both can be empty and the test would pass\n    defaults = {\n        \"slack_report_failures_only\": False,\n        \"shape\": \"Default\",\n        \"users\": 1.0,\n        \"runtime\": 120.0,\n        \"spawn_rate\": 1.0,\n        \"scenario_delay\": 0.0,\n        \"cleanup_s3\": True,\n        \"slack\": False,\n        \"influx\": False,\n        \"report_portal\": False,\n        \"rp_launch_name\": \"Grasshopper Performance Test Run | Launch name unknown\",\n    }\n    return defaults", "\n\nclass MockConfig(dict):\n    \"\"\"Mock config object that can be loaded with a dict instead of a Config.\"\"\"\n\n    def getoption(self, value):\n        \"\"\"Retrieve a value from the dictionary, the way a Config object supports.\"\"\"\n        return self.get(value)\n\n\ndef was_message_logged(\n    caplog,\n    target_message_re=None,\n    target_level=None,\n    target_index=None,\n    target_logger=None,\n):\n    \"\"\"Validate that the specified message was logged.\n\n    Validation is using only the criteria provided and skipping a check on any\n    attribute that is None.\n\n    Generally, parameter handling is meant to optimize the most common usage.\n\n    pass target_index=-1 to get the very last message logged\n    if you pass in a string for target_message_re, then will compile to re for you\n\n    \"\"\"\n    # if the user didn't specify any targets, return true if any records exist in caplog\n    if (\n        target_message_re is None\n        and target_level is None\n        and target_index is None\n        and target_level is None\n    ):\n        return len(caplog.record_tuples) > 0\n\n    # otherwise, see if any records match\n    if target_index:\n        # get the target_index record, make it into a list for the search part\n        record_tuples = [caplog.record_tuples[target_index]]\n    else:\n        # include all the records in caplog\n        record_tuples = caplog.record_tuples\n\n    # convert strings to a compiled regex\n    target_message_re = make_re(target_message_re)\n\n    matches = []  # collect the matching records\n    for record_tuple in record_tuples:\n        tuple_match = True\n        log, level, msg = record_tuple\n\n        # determine if this record matches\n        if target_logger:\n            tuple_match = tuple_match and log == target_logger\n        if target_level:\n            tuple_match = tuple_match and level == target_level\n        if target_message_re:\n            tuple_match = tuple_match and target_message_re.search(msg) is not None\n        if tuple_match:\n            matches.append(record_tuple)\n\n    return len(matches) > 0, matches", "\n\ndef was_message_logged(\n    caplog,\n    target_message_re=None,\n    target_level=None,\n    target_index=None,\n    target_logger=None,\n):\n    \"\"\"Validate that the specified message was logged.\n\n    Validation is using only the criteria provided and skipping a check on any\n    attribute that is None.\n\n    Generally, parameter handling is meant to optimize the most common usage.\n\n    pass target_index=-1 to get the very last message logged\n    if you pass in a string for target_message_re, then will compile to re for you\n\n    \"\"\"\n    # if the user didn't specify any targets, return true if any records exist in caplog\n    if (\n        target_message_re is None\n        and target_level is None\n        and target_index is None\n        and target_level is None\n    ):\n        return len(caplog.record_tuples) > 0\n\n    # otherwise, see if any records match\n    if target_index:\n        # get the target_index record, make it into a list for the search part\n        record_tuples = [caplog.record_tuples[target_index]]\n    else:\n        # include all the records in caplog\n        record_tuples = caplog.record_tuples\n\n    # convert strings to a compiled regex\n    target_message_re = make_re(target_message_re)\n\n    matches = []  # collect the matching records\n    for record_tuple in record_tuples:\n        tuple_match = True\n        log, level, msg = record_tuple\n\n        # determine if this record matches\n        if target_logger:\n            tuple_match = tuple_match and log == target_logger\n        if target_level:\n            tuple_match = tuple_match and level == target_level\n        if target_message_re:\n            tuple_match = tuple_match and target_message_re.search(msg) is not None\n        if tuple_match:\n            matches.append(record_tuple)\n\n    return len(matches) > 0, matches", "\n\ndef message_was_not_logged(\n    caplog,\n    target_message_re=None,\n    target_level=None,\n    target_index=None,\n    target_logger=None,\n):\n    \"\"\"Validate a message was _not_ logged using same matching as was_message_logged.\"\"\"\n    found, matches = was_message_logged(\n        caplog,\n        target_message_re=target_message_re,\n        target_level=target_level,\n        target_index=target_index,\n        target_logger=target_logger,\n    )\n    return not found, matches", "\n\ndef make_re(potential_re):\n    \"\"\"Convert a string to a compiled re, otherwise return the value unaltered.\"\"\"\n    if type(potential_re) == str:\n        potential_re = re.compile(potential_re, re.IGNORECASE)\n    return potential_re\n\n\ndef calculate_path(filename, subdir=\"../fixture_testing_data\"):\n    \"\"\"Calculate the absolute path to a file in the fixture_test_data directory.\"\"\"\n    path = Path(__file__).parent / Path(subdir) / filename\n\n    return path", "\ndef calculate_path(filename, subdir=\"../fixture_testing_data\"):\n    \"\"\"Calculate the absolute path to a file in the fixture_test_data directory.\"\"\"\n    path = Path(__file__).parent / Path(subdir) / filename\n\n    return path\n\n\ndef perform_fixture_test_with_optional_log_capture(\n    pytester,\n    outcomes={\"passed\": 1},\n    caplog=None,\n    target_messages=[],\n):\n    \"\"\"Perform the actual 'test' portion of a fixture test and validate the results.\n\n    Generally, parameter handling is meant to optimize the most common usage.\n\n    outcomes - the exact same values as you pass to pytester.assert_outcomes, see the\n    pytester documentation for more details.\n\n    caplog - Simply pass the value of the pytest supplied fixture called `caplog`,\n    which your test will need to put into its signature.bWill only do a log capture if\n    caplog is not None.\n\n    target_messages - list of messages you expect to see in the captured logging for\n    the test within a test. Only the non-None attributes are checked. Ignored if caplog\n    is None. If you pass a single dict message for target_messages, we will make it\n    into a list for you.\n\n    \"\"\"\n    # Step 1: Run the test within the test\n    if caplog:\n        with caplog.at_level(logging.DEBUG):\n            # run all tests with pytest\n            result = pytester.runpytest()\n    else:\n        result = pytester.runpytest()\n\n    # Step 2: Validate the outcomes from the test within the test\n    result.assert_outcomes(**outcomes)\n\n    # Step 3: Validate any log messages supplied\n    if caplog:\n        target_messages = convert_dict_to_list(target_messages)\n        for target_message in target_messages:\n            found, _ = was_message_logged(\n                caplog,\n                **target_message,\n            )\n            assert found\n\n    # return the result from pytester.runpytest() so that a test\n    # may perform additional validations not provided for here\n    return result", "def perform_fixture_test_with_optional_log_capture(\n    pytester,\n    outcomes={\"passed\": 1},\n    caplog=None,\n    target_messages=[],\n):\n    \"\"\"Perform the actual 'test' portion of a fixture test and validate the results.\n\n    Generally, parameter handling is meant to optimize the most common usage.\n\n    outcomes - the exact same values as you pass to pytester.assert_outcomes, see the\n    pytester documentation for more details.\n\n    caplog - Simply pass the value of the pytest supplied fixture called `caplog`,\n    which your test will need to put into its signature.bWill only do a log capture if\n    caplog is not None.\n\n    target_messages - list of messages you expect to see in the captured logging for\n    the test within a test. Only the non-None attributes are checked. Ignored if caplog\n    is None. If you pass a single dict message for target_messages, we will make it\n    into a list for you.\n\n    \"\"\"\n    # Step 1: Run the test within the test\n    if caplog:\n        with caplog.at_level(logging.DEBUG):\n            # run all tests with pytest\n            result = pytester.runpytest()\n    else:\n        result = pytester.runpytest()\n\n    # Step 2: Validate the outcomes from the test within the test\n    result.assert_outcomes(**outcomes)\n\n    # Step 3: Validate any log messages supplied\n    if caplog:\n        target_messages = convert_dict_to_list(target_messages)\n        for target_message in target_messages:\n            found, _ = was_message_logged(\n                caplog,\n                **target_message,\n            )\n            assert found\n\n    # return the result from pytester.runpytest() so that a test\n    # may perform additional validations not provided for here\n    return result", "\n\ndef convert_dict_to_list(target_messages):\n    \"\"\"Convert a single dictionary to a list of that single dictionary.\n\n    This is used to make the most common case (you only need to validate one message,\n    therefore only one message dict) less wordy and easier.\n\n    \"\"\"\n    if type(target_messages) == dict:\n        target_messages = [target_messages]\n\n    return target_messages", ""]}
{"filename": "tests/unit/test__fixture__cmdln_args.py", "chunked_list": ["from unittest.mock import patch\n\nfrom tests.unit.conftest import (\n    CONFTEST_TEMPLATE,\n    PYFILE_ASSERT_EMPTY_CONFIG,\n    PYFILE_ASSERT_EXPECTED_CONFIG,\n    PYFILE_TEMPLATE,\n    perform_fixture_test_with_optional_log_capture,\n)\n", ")\n\nfrom grasshopper.lib.configuration.gh_configuration import (  # noqa: N817\n    ConfigurationConstants as CC,\n)\n\nFIXTURE_UNDER_TEST = \"cmdln_args\"\n\n# Some comments on testing fixture cmdln_args.\n# The source of values for this fixture is pytest's request.config object. We are going", "# Some comments on testing fixture cmdln_args.\n# The source of values for this fixture is pytest's request.config object. We are going\n# on the assumption that this fixture is well tested by the pytest package and are only\n# checking what essentially are positive test cases for this collection.\n# Note, we are not using magic mock here because pytest's request.config object is very\n# hard to patch. See the definition of MockConfig (conftest) for more details.\n\n\ndef test__cmdln_args__happy(pytester):\n    \"\"\"Fixture should load any matching values from the command line.\"\"\"\n\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--spawn_rate\":.3})\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\"users\": 42, \"spawn_rate\": 0.3}\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "def test__cmdln_args__happy(pytester):\n    \"\"\"Fixture should load any matching values from the command line.\"\"\"\n\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--spawn_rate\":.3})\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\"users\": 42, \"spawn_rate\": 0.3}\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST,\n                expected=expected_config_args,\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__cmdln_args__empty_cmdln(pytester):\n    \"\"\"Fixture should return an empty config if command line is empty.\"\"\"\n    # note, we are _not_ using magic mock here because pytest's request.config\n    # object is very hard to patch. See the definition of MockConfig (conftest)\n    # for more details.\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig()\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\ndef test__cmdln_args__extra_args_coming_from_cmdln(pytester):\n    \"\"\"Fixture should load any attrs that match COMPLETE_ATTRS and ignore any other\n    attrs in the collection, such as pytest args or custom args that a consumer might\n    add in.\"\"\"\n\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--log_cli_level\": \"debug\"})\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    expected_config_args = {\"users\": 42}\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EXPECTED_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST, expected=expected_config_args\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__cmdln_args__empty_attrs(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig({\"--users\": 42, \"--spawn_rate\":.3})\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", "\n\n@patch.dict(CC.COMPLETE_ATTRS, clear=True)\ndef test__cmdln_args__empty_attrs_and_empty_cmdln(pytester):\n    \"\"\"Fixture should return an empty config if COMPLETE_ATTRS is empty.\"\"\"\n\n    patches = \"\"\"\n        from tests.unit.conftest import MockConfig\n\n        @pytest.fixture(scope=\"session\")\n        def request_config():\n            mock = MockConfig()\n            return mock\n        \"\"\"\n    pytester.makeconftest(\n        CONFTEST_TEMPLATE.format(fixture_name=FIXTURE_UNDER_TEST, patches=patches)\n    )\n    pytester.makepyfile(\n        PYFILE_TEMPLATE.format(\n            fixture_name=FIXTURE_UNDER_TEST,\n            validations=PYFILE_ASSERT_EMPTY_CONFIG.format(\n                fixture_name=FIXTURE_UNDER_TEST\n            ),\n        )\n    )\n\n    perform_fixture_test_with_optional_log_capture(pytester)", ""]}
{"filename": "example/test_example.py", "chunked_list": ["import logging\n\nfrom locust import between, task\n\nfrom grasshopper.lib.grasshopper import Grasshopper\nfrom grasshopper.lib.journeys.base_journey import BaseJourney\nfrom grasshopper.lib.util.utils import check\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass ExampleJourney(BaseJourney):\n    \"\"\"An example journey class with a simple task\"\"\"\n\n    # number of seconds to wait between each task\n    wait_time = between(min_wait=1, max_wait=4)\n\n    # `host` is automatically prepended to all request endpoint\n    # urls when using `self.client` requests. It is also set as the\n    # global \"environment\" tag in timeseries metrics\n    host = \"https://google.com\"\n\n    # lower precedence scenario_args dict, merged in on startup\n    defaults = {\n        \"foo\": \"bar\",\n    }\n\n    # a locust task, repeated over and over again until the test finishes\n    @task\n    def example_task(self):\n        \"\"\"a simple get google images HTTP request\"\"\"\n        logger.info(\n            f\"Beginning example task for VU {self.vu_number} with param `foo`=\"\n            f'`{self.scenario_args.get(\"foo\")}`'\n        )\n        # aggregate all metrics for this request under the name \"get google images\"\n        # if name is not specified, then the full url will be the name of the metric\n        response = self.client.get(\n            \"/imghp\", name=\"get google images\", context={\"extra\": \"tag\"}\n        )\n        logger.info(f\"google images responded with a {response.status_code}.\")\n        check(\n            \"google images responded with a 200\",\n            response.status_code == 200,\n            env=self.environment,\n        )", "\n\ndef test_run_example_journey(complete_configuration, example_configuration_values):\n    ExampleJourney.update_incoming_scenario_args(example_configuration_values)\n    ExampleJourney.update_incoming_scenario_args(complete_configuration)\n    locust_env = Grasshopper.launch_test(\n        ExampleJourney,\n        **complete_configuration,\n    )\n    return locust_env", ""]}
{"filename": "example/conftest.py", "chunked_list": ["\"\"\"A file which is used to establish pytest fixtures, plugins, hooks, etc.\"\"\"\nimport pytest\nfrom gevent import monkey\n\nmonkey.patch_all()\n# ^this part has to be done first in order to avoid errors with importing the requests\n# module\n\nfrom grasshopper.lib.configuration.gh_configuration import GHConfiguration  # noqa: E402\n", "from grasshopper.lib.configuration.gh_configuration import GHConfiguration  # noqa: E402\n\n\ndef pytest_addoption(parser):\n    \"\"\"Add pytest params.\"\"\"\n    # all grasshopper arguments (users, spawn_rate, etc.) will be automatically\n    # loaded via the locust_grasshopper pytest plugin\n    # add pytest options that are specific to your tests here\n    parser.addoption(\n        \"--foo\", action=\"store\", type=str, help=\"example parameter\", default=\"bar\"\n    )", "\n\n@pytest.fixture(scope=\"function\")\ndef example_configuration_values(request):  # noqa: F811\n    \"\"\"Load all the configuration values for this specific test (or suite).\n\n    These would be any custom command line values that your test/suite needs and\n    would roughly correspond to whatever arguments you added via pytest_addoption hook.\n\n    If you would like to use grasshopper configurations values in calculating these\n    values, then you can use the `complete_configuration` fixture to access those.\n\n    You can obviously return whatever you would like from this fixture, but we would\n    recommend that you return a GHConfiguration object, which is what all the\n    grasshopper configuration code returns (and a Journey is prepared to accept).\n\n    \"\"\"\n    config = GHConfiguration()  # an empty config object\n\n    # value defined by this conftest, specific to this particular test\n    config.update_single_key(\"foo\", request.config.getoption(\"foo\"))\n\n    return config", ""]}
{"filename": "src/__init__.py", "chunked_list": ["\"\"\"The source code for the grasshopper package.\"\"\"\n"]}
{"filename": "src/grasshopper/__init__.py", "chunked_list": ["\"\"\"Module: Grasshopper.\"\"\"\n"]}
{"filename": "src/grasshopper/lib/grasshopper.py", "chunked_list": ["\"\"\"Module: Grasshopper.\n\nThe Grasshopper class is the main entry point for accessing\ngrasshopper functionality.\n\n\"\"\"\nimport logging\nimport os\nimport signal\nfrom typing import Dict, List, Optional, Type, Union", "import signal\nfrom typing import Dict, List, Optional, Type, Union\n\nimport gevent\nimport locust\nfrom locust import LoadTestShape\nfrom locust.env import Environment\n\nfrom grasshopper.lib.journeys.base_journey import BaseJourney\nfrom grasshopper.lib.util.listeners import GrasshopperListeners", "from grasshopper.lib.journeys.base_journey import BaseJourney\nfrom grasshopper.lib.util.listeners import GrasshopperListeners\n\nlogger = logging.getLogger()\n\n\nclass Grasshopper:\n    \"\"\"Main entry point to access addins and extensions.\"\"\"\n\n    def __init__(self, global_configuration: dict = {}, **kwargs):\n        self.global_configuration = global_configuration\n        self.log()\n\n    def log(self) -> None:\n        \"\"\"Log all the configuration values.\"\"\"\n        logger.info(\"--- Grasshopper configuration ---\")\n\n        for k, v in self.global_configuration.items():\n            logger.info(f\"{k}: [{v}]\")\n\n        logger.info(\"--- /Grasshopper configuration ---\")\n\n    @property\n    def influx_configuration(self) -> dict[str, Optional[str]]:\n        \"\"\"Extract the influx related configuration items.\n\n        The InfluxDbSettings object should only get keys if there is a\n        value coming from the command line. We _could_ define\n        defaults here as well, but this somewhat breaks the\n        principle of separation of concern.\n\n        # TODO-DEPRECATED: move this code to the GHConfiguration object\n        \"\"\"\n        configuration = {}\n\n        host = self.global_configuration.get(\n            \"influx_host\", self.global_configuration.get(\"influx_host\")\n        )\n        if host:\n            configuration[\"influx_host\"] = host\n        port = self.global_configuration.get(\"influx_port\")\n        if port:\n            configuration[\"influx_port\"] = port\n        user = self.global_configuration.get(\"influx_user\")\n        if user:\n            configuration[\"user\"] = user\n        pwd = self.global_configuration.get(\"influx_pwd\")\n        if pwd:\n            configuration[\"pwd\"] = pwd\n\n        return configuration\n\n    @staticmethod\n    def launch_test(\n        weighted_user_classes: Union[\n            Type[BaseJourney], List[Type[BaseJourney]], Dict[Type[BaseJourney], float]\n        ],\n        **kwargs,\n    ) -> Environment:\n        \"\"\"\n        Parametrize launching of locust test.\n\n        Required parameters:\n        - weighted_user_classes: this represents the answer to the question \"what test\n        should I launch?\"\n\n        The situation with user classes and weights is actually fairly complicated.\n        In this context, the weight relates to how likely Locust is to pick that journey\n        for the next virtual user it will spawn. Once that virtual user is going with\n        journey, it will follow that journey's definition until the test terminates.\n\n        In the simple case of a single journey or a composite sequential journey, this\n        isn't that meaningful. In both cases, there is only one journey sent to Locust\n        and weights are interesting if there is only one choice.\n\n        You can supply:\n        1. a single user class (journey) - in this case weights, for journey selection\n        are ignored because Locust is only choosing from a set of 1\n        2. a list of user_classes - in this case, weights are supplied so that all\n        journeys have an equal chance of being selected\n        3. a dictionary where the user classes are keys and the values are the weights\n        - in this case, the weights come into play in selecting a journey\n\n        It is important to note that once a user journey is selected for a particular\n        vu, the definition of the journey (user class) comes into play. That journey\n        also may use the @task decorator or the tasks attribute with a TaskSet to\n        determine which __task__ is executed __each iteration__. An iteration is one\n        time through the taskloop where Locust picks a task, executes it and waits the\n        specified amount of time before starting the next iteration. The Shield team's\n        journeys to date have only defined a single task per user class, but this will\n        likely change as we build more complex journeys.\n\n        Optional parameters:\n            please see the documentation in grasshopper/pytest/commandline.py for a\n            complete list of supported parameters\n\n        TODO: this code is in need of a bit of refactor to pull out a few different\n        TODO: steps - the normalizing of the incoming user classes into one standard\n        TODO: format, processing the shape, setting up the Locust env & launching the\n        TODO: the test - primarily, it's getting a little long and complex but also\n        TODO: to have better separation of concerns\n        \"\"\"\n        if isinstance(weighted_user_classes, type):  # if it's a class\n            weighted_user_classes = {weighted_user_classes: 1}\n        elif isinstance(weighted_user_classes, list):\n            weighted_user_classes = {\n                user_class: 1 for user_class in weighted_user_classes\n            }\n\n        # Actually, Locust always takes a __list__ of user classes and additional\n        # information is set on each user class in this list to specify weight or\n        # fixed count. That portion, setting weights, is handled after we process any\n        # shape information below.\n        user_classes = list(weighted_user_classes.keys())\n\n        logger.debug(f\"Launch received kwargs: {kwargs}\")\n\n        env = Environment(user_classes=user_classes)\n        kwargs[\n            \"user_classes\"\n        ] = weighted_user_classes  # pass on the user classes as well\n\n        env.grasshopper = Grasshopper(global_configuration=kwargs)\n        env.create_local_runner()\n        env.runner.stats.reset_all()\n        gevent.spawn(locust.stats.stats_history, env.runner)\n        env.grasshopper_listeners = GrasshopperListeners(environment=env)\n\n        # env.shape_class is actually supplied a shape *instance*\n        # despite the attr name\n        env.shape_class = kwargs.get(\"shape_instance\")\n\n        # assign the weights to the individual user classes __after__ the shape has been\n        # processed because eventually, we will need to consult the shape to get the max\n        # number of virtual users\n        Grasshopper._assign_weights_to_user_classes(weighted_user_classes)\n        env.runner.start_shape()\n        gevent.spawn_later(\n            kwargs.get(\"runtime\"), lambda: os.kill(os.getpid(), signal.SIGINT)\n        )\n        env.runner.greenlet.join()\n\n        return env\n\n    @staticmethod\n    def _assign_weights_to_user_classes(weighted_user_classes):\n        for user_class, weight in weighted_user_classes.items():\n            user_class.weight = weight\n\n    @staticmethod\n    def load_shape(shape_name: str, **kwargs) -> LoadTestShape:\n        \"\"\"Return the instantiated shape instance given string shape name.\"\"\"\n        shape_name = shape_name.capitalize()  # to make sure it is capitalized\n        import grasshopper.lib.util.shapes as shapes\n\n        if shape_name in dir(shapes):\n            return getattr(shapes, shape_name)(**kwargs)\n        else:\n            raise ValueError(\n                f\"Shape {shape_name} does not exist in \"\n                f\"grasshopper.lib.util.shapes! Please check the spelling.\"\n            )", ""]}
{"filename": "src/grasshopper/lib/__init__.py", "chunked_list": ["\"\"\"Module: Lib.\"\"\"\n"]}
{"filename": "src/grasshopper/lib/fixtures/__init__.py", "chunked_list": ["\"\"\"Contents of the locust_grasshopper plugin which gets automatically loaded.\"\"\"\nimport logging\nimport os\nimport time\n\nimport pytest\nimport tagmatcher\nimport yaml\n\nfrom grasshopper.lib.configuration.gh_configuration import (", "\nfrom grasshopper.lib.configuration.gh_configuration import (\n    ConfigurationConstants,\n    GHConfiguration,\n)\nfrom grasshopper.lib.grasshopper import Grasshopper\nfrom grasshopper.lib.util.decorators import deprecate\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\ndef pytest_addoption(parser):\n    \"\"\"Add in the grasshopper specific cmdline args.\"\"\"\n    for attr_name, attr_definition in ConfigurationConstants.COMPLETE_ATTRS.items():\n        opts = attr_definition[\"opts\"]\n        option_attrs = attr_definition.get(\"attrs\", {})\n        parser.addoption(*opts, **option_attrs)\n", "\n\n# --------------------------------------- LEGACY --------------------------------------\n# legacy, keep for backwards compatibility\n@pytest.fixture(scope=\"function\")\n@deprecate(\"grasshopper_scenario_args fixture\", \"complete_configuration\")\ndef grasshopper_scenario_args():\n    \"\"\"a fixture for grasshopper journey args, which will later be set by\n    grasshopper_scenario_file_set_args.\"\"\"\n    return GHConfiguration()", "\n\n# legacy, keep for backwards compatibility\n@pytest.fixture(scope=\"function\")\n@deprecate(\"grasshopper_args fixture\", \"complete_configuration\")\ndef grasshopper_args(complete_configuration):\n    config = GHConfiguration(complete_configuration)\n    \"\"\"The public fixture to be used by grasshopper tests. This is after all\n    configuration has been set.\"\"\"\n    return config", "\n\n# ---------------------------- CONFIGURATION FIXTURES ---------------------------\n@pytest.fixture(scope=\"session\")\ndef global_defaults():\n    defaults = {\n        k: v.get(\"default\")\n        for k, v in ConfigurationConstants.COMPLETE_ATTRS.items()\n        if v.get(\"default\") is not None\n    }\n    config = GHConfiguration(**defaults)\n    logger.debug(f\"CONFIG FIXTURE: global_defaults {config}\")\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef grasshopper_config_file_args(request):\n    config = GHConfiguration()\n    try:\n        # TODO: should move yaml read to it's own method\n        path = request.getfixturevalue(\"grasshopper_config_file_path\")\n        with open(path, \"r\") as stream:\n            raw = yaml.safe_load(stream)\n\n        # transfer each section to GHConfiguration object, any other sections ignored\n        # TODO: should we load any other sections?\n        global_vals = raw.get(\"grasshopper\", {})\n        config.update(**global_vals)\n        test_run_vals = raw.get(\"test_run\", {})\n        config.update(**test_run_vals)\n        scenario_vals = raw.get(\"scenario\", {})\n        config.update(**scenario_vals)\n\n    except pytest.FixtureLookupError:\n        logger.warning(\n            \"CONFIG FIXTURE: Skipping loading from grasshopper configuration file \"\n            \"because fixture 'grasshopper_config_file_path` not found. You can safely \"\n            \"ignore this warning if you were not intending to use a grasshopper \"\n            \"configuration file.\"\n        )\n    except FileNotFoundError:\n        logger.warning(\n            f\"CONFIG FIXTURE: Skipping loading from grasshopper configuration file \"\n            f\"because {path} not found.\"\n        )\n    except (yaml.YAMLError, AttributeError) as e:\n        logger.warning(\n            f\"CONFIG FIXTURE: Unable to parse yaml file {path} with error \" f\"{e}.\"\n        )\n\n    logger.debug(f\"CONFIG FIXTURE: grasshopper_config_file {config}\")\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef env_var_prefix_key(request):\n    prefix = \"GH_\"\n    try:\n        prefix = request.getfixturevalue(\"configuration_prefix_key\")\n        if type(prefix) != str or (type(prefix) == str and len(prefix) == 0):\n            logger.warning(\n                f\"CONFIG FIXTURE: Fixture configuration_prefix_key may only be a non \"\n                f\"zero length str, returned value {prefix}, ignoring value.\"\n            )\n            prefix = \"GH_\"\n    except pytest.FixtureLookupError:\n        pass\n\n    logger.debug(f\"CONFIG FIXTURE: env_var_prefix_key {prefix}\")\n    return prefix", "\n\n@pytest.fixture(scope=\"session\")\ndef extra_env_var_keys(request):\n    keys = []\n    try:\n        keys = request.getfixturevalue(\"configuration_extra_env_var_keys\")\n        # only allow a list of strings for env_var_keys\n        if not type_check_list_of_strs(keys):\n            logger.warning(\n                f\"CONFIG FIXTURE: Fixture configuration_extra_env_var_keys may only \"\n                f\"return a list of strings, returned value {keys}, ignoring value.\"\n            )\n            keys = []\n    except pytest.FixtureLookupError:\n        pass\n\n    logger.debug(f\"CONFIG FIXTURE: extra_env_var_keys {keys}\")\n    return keys", "\n\n@pytest.fixture(scope=\"session\")\ndef env_var_args(env_var_prefix_key, extra_env_var_keys):\n    config = GHConfiguration()\n\n    for env_var_name, env_var_value in os.environ.items():\n        if (\n            env_var_name.lower() in ConfigurationConstants.COMPLETE_ATTRS.keys()\n            or env_var_name.startswith(env_var_prefix_key)\n            or env_var_name in extra_env_var_keys\n        ):\n            if env_var_name.startswith(env_var_prefix_key):\n                env_var_name = env_var_name.lstrip(env_var_prefix_key)\n\n            if len(env_var_name) > 0:\n                config.update_single_key(env_var_name.lower(), env_var_value)\n\n    logger.debug(f\"CONFIG FIXTURE: env_var_args {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef request_config(request):\n    \"\"\"Separate config so that we have something to patch during unit testing.\n\n    I could not find a way to patch the request object successfully using pytester.\n    Patching fixtures we have defined is easier and then we don't have worry about\n    how to sub in a config or request object that does all the other things correctly,\n    but with new command line arguments. Note that config is _not_ a dict, so you can\n    not just do patch.dict(...), which would be the natural way for this type of use\n    case.\n\n    \"\"\"\n    return request.config", "\n\n@pytest.fixture(scope=\"session\")\ndef cmdln_args(request_config):\n    config = GHConfiguration()\n\n    for attr_name, attr_definition in ConfigurationConstants.COMPLETE_ATTRS.items():\n        config.update_single_key(attr_name, request_config.getoption(f\"--{attr_name}\"))\n\n    logger.debug(f\"CONFIG FIXTURE: cmdln_args {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef pre_processed_args(\n    global_defaults, grasshopper_config_file_args, env_var_args, cmdln_args\n):\n    pre_config = GHConfiguration()\n    try:\n        # calculate a small dictionary of args that we need to in order to do the full\n        # merge of args first, determine if we currently have a scenario file & scenario\n        # name specified. if we don't have both, then we can skip merging in any values\n        # from scenario file. the collection code for when a yaml is specified will\n        # perform collection and call pytest.main again with the scenario file &\n        # scenario name\n        scenario_file = fetch_value_from_multiple_sources(\n            [cmdln_args, env_var_args, grasshopper_config_file_args, global_defaults],\n            \"scenario_file\",\n        )\n        pre_config.update_single_key(\"scenario_file\", scenario_file)\n\n        scenario_name = fetch_value_from_multiple_sources(\n            [cmdln_args, env_var_args, grasshopper_config_file_args, global_defaults],\n            \"scenario_name\",\n        )\n        pre_config.update_single_key(\"scenario_name\", scenario_name)\n\n    except Exception as e:\n        logger.error(\n            f\"CONFIG_FIXTURE: Uncaught exception in pre_processed_args fixture: \"\n            f\"{type(e).__name__} | {e}\"\n        )\n\n    logger.debug(f\"CONFIG FIXTURE: pre_processed_args {pre_config}\")\n    return pre_config", "\n\n@pytest.fixture(scope=\"session\")\ndef scenario_file_args(pre_processed_args):\n    config = GHConfiguration()\n\n    # if we don't have both scenario file and scenario name, then there are no values to\n    # load for this source; in the case of scenario collection, the collection code\n    # calls pytest.main again with the scenario and filename, prompting something to be\n    # loaded the 2nd time through the process of building the configuration values\n    scenario_file = pre_processed_args.get(\"scenario_file\")\n    scenario_name = pre_processed_args.get(\"scenario_name\")\n\n    if scenario_file and scenario_name:\n        try:\n            with open(scenario_file, \"r\") as stream:\n                raw = yaml.safe_load(stream)\n            scenario = raw.get(scenario_name)\n            config.update(scenario.get(\"grasshopper_args\", {}))\n            config.update(scenario.get(\"grasshopper_scenario_args\", {}))\n            config.update_single_key(\n                \"scenario_test_file_name\", scenario.get(\"test_file_name\")\n            )\n            config.update_single_key(\"scenario_tags\", scenario.get(\"tags\"))\n        except Exception as e:\n            logger.warning(\n                f\"CONFIG FIXTURE: Unexpected error loading scenario {scenario_name} \"\n                f\"from {scenario_file}: {type(e).__name__} | {e}\"\n            )\n\n    logger.debug(f\"CONFIG FIXTURE: scenario_file_args {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef merge_sources(\n    global_defaults,\n    grasshopper_config_file_args,\n    scenario_file_args,\n    env_var_args,\n    cmdln_args,\n):\n    config = GHConfiguration()\n\n    # order matters here, this is determining the variable precedence\n    try:\n        config.update(global_defaults)\n        config.update(grasshopper_config_file_args)\n        config.update(scenario_file_args)\n        config.update(env_var_args)\n        config.update(cmdln_args)\n    except Exception as e:\n        logger.error(\n            f\"CONFIG FIXTURE: Unexpected error in merge_sources: \"\n            f\"{type(e).__name__} | {e}\"\n        )\n        pass\n\n    logger.debug(f\"CONFIG FIXTURE: env_var_args {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef typecast(merge_sources):\n    config = GHConfiguration(merge_sources)\n\n    # get the collection of attrs that have a typecast func listed\n    attrs = {\n        k: v\n        for k, v in ConfigurationConstants.COMPLETE_ATTRS.items()\n        if v.get(\"typecast\") is not None\n    }\n\n    # apply the typecast lambda to the value\n    for k, v in attrs.items():\n        if config.get(k) is not None:\n            config[k] = v[\"typecast\"](config[k])\n\n    logger.debug(f\"CONFIG FIXTURE: typecast {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef process_shape(typecast):\n    config = GHConfiguration(typecast)\n    shape = config.get(\"shape\")\n\n    # TODO: keeping same logic here, but probably we should decide if an existing\n    # TODO: shape_instance should be overriden here??\n    if shape is not None:\n        # instantiate the shape and add to the args\n        shape_instance = Grasshopper.load_shape(shape_name=shape, **config)\n        config[\"shape_instance\"] = shape_instance\n\n        # a shape is passed all the configuration args, in case it wants to make\n        # decisions based on any of the values; in return the shape may override\n        # configuration values based on its calculations, within reason\n        # (must be in list of \"approved\" keys)\n\n        # get the overrides from the shape_instance\n        overrides = shape_instance.get_shape_overrides()\n\n        # filter out any that are not \"approved keys\"\n        overrides = {\n            k: v\n            for k, v in overrides.items()\n            if k in ConfigurationConstants.SHAPE_OVERRIDE_ATTR_NAMES\n        }\n\n        # add the overrides\n        config.update(overrides)\n    else:\n        logger.error(f\"CONFIG FIXTURE: Shape is missing from configuration {config}\")\n\n    logger.debug(f\"CONFIG FIXTURE: process_shape {config}\")\n\n    return config", "\n\n@pytest.fixture(scope=\"session\")\ndef complete_configuration(process_shape):\n    config = GHConfiguration(process_shape)\n    # TODO-DEPRECATED: Transfer the value from the deprecated arg to the new arg,\n    # TODO: so that we can gracefully transfer use to the new arg\n    # influxdb --> influx_host\n    config.update_single_key(\"influx_host\", config.get(\"influxdb\"))\n\n    logger.debug(f\"CONFIG FIXTURE: complete_configuration {config}\")\n\n    return config", "\n\n# -------------------------------- OTHER FIXTURES ------------------------------\n@pytest.fixture(scope=\"function\", autouse=True)\ndef do_scenario_delay(grasshopper_args):\n    \"\"\"Functionality to delay between each scenario run.\"\"\"\n    yield\n    delay = grasshopper_args.get(\"scenario_delay\")\n    if delay and delay > 0:\n        logger.info(f\"Waiting for {delay} seconds between scenarios...\")\n        time.sleep(delay)\n    else:\n        logger.debug(f\"Skipping delay of {delay} seconds between scenarios...\")", "\n\n# -------------------------------- YAML COLLECTION ------------------------------\ndef pytest_collect_file(parent, path):\n    \"\"\"Collect Yaml files for pytest.\"\"\"\n    logger.debug(f\"Performing collection for yaml {parent} {path}\")\n    allowed_yaml_exts = [\".yaml\", \".yml\"]\n    # if pytest is run via yaml, E.G. `pytest my_scenarios.yaml`, then return a YAML\n    # scenario file pytest node. This node will then kick off individual files\n    # that point to certain scenario names within that yaml. Once the individual\n    # files are kicked off, then we fall into the else case.\n    if path.ext.lower() in allowed_yaml_exts:\n        return YamlScenarioFile.from_parent(parent, fspath=path)\n    else:\n        return", "\n\nclass YamlScenarioFile(pytest.File):\n    \"\"\"The logic behind what to do when a Yaml file is specified in pytest.\"\"\"\n\n    def collect(self):\n        \"\"\"Collect the file, knowing the path via self.fspath.\"\"\"\n        # Third Party\n        import yaml\n\n        raw = yaml.safe_load(self.fspath.open())\n        valid_scenarios = _get_tagged_scenarios(\n            raw_yaml_dict=raw, config=self.config, fspath=self.fspath\n        )  # tag filter\n        for scenario_name, scenario_contents in valid_scenarios.items():\n            test_file_name = scenario_contents.get(\"test_file_name\")\n            if test_file_name:\n                yield Scenario.from_parent(\n                    self, name=scenario_name, spec=scenario_contents\n                )\n            else:\n                raise AttributeError(\n                    f\"The YAML scenario `{scenario_name}` \"\n                    f\"is missing the required `test_file_name` parameter\"\n                )", "\n\nclass Scenario(pytest.Item):\n    \"\"\"A pytest test that corresponds to a scenario within a scenario file.\"\"\"\n\n    def __init__(self, name, parent, spec):\n        \"\"\"Set the scenario properties in order to be run via runtest.\"\"\"\n        super().__init__(name, parent)\n        self.parent = parent\n\n        self.scenario_name = name\n        self.scenario_file = self.parent.fspath\n        self.test_file_name = spec.get(\"test_file_name\")\n\n    def runtest(self):\n        \"\"\"Run the pytest test/scenario.\"\"\"\n        args = (\n            [\n                self.test_file_name,\n                f\"--scenario_file={self.scenario_file}\",\n                f\"--scenario_name={self.scenario_name}\",\n            ]\n            + [\n                f\"--{option_name}={self.config.getoption(option_name)}\"\n                for option_name in _fetch_args(\n                    attr_names=ConfigurationConstants.COMPLETE_ATTRS.keys(),\n                    config=self.config,\n                )\n            ]\n            + [\n                extra_arg\n                for extra_arg in list(self.config.invocation_params.args)\n                if \"--\" in extra_arg\n            ]\n        )  # pass down any other params that were supplied when invoking pytest\n\n        # remove log-file args to avoid each test overwriting it\n        # remove error message skipping args which is sometimes passed in by the ide\n        ignore_args = [\"--log-file\", \"--no-header\", \"--no-summary\"]\n        args = [\n            arg\n            for arg in args\n            if not any([arg.startswith(ignore_arg) for ignore_arg in ignore_args])\n        ]\n        exit_code = pytest.main(args)\n        assert exit_code == pytest.ExitCode.OK\n\n    def repr_failure(self, excinfo):\n        \"\"\"Call this method when self.runtest() raises an exception.\"\"\"\n        if isinstance(excinfo.value, YamlError):\n            return \"\\n\".join(\n                [\n                    f\"Scenario `{self.name}` FAILED: \",\n                    \"{1!r}: {2!r}\".format(*excinfo.value.args),\n                ]\n            )\n\n    def reportinfo(self):\n        \"\"\"Return info about the scenario being run.\"\"\"\n        return self.fspath, 0, f\"Scenario: {self.name}\"", "\n\nclass YamlError(Exception):\n    \"\"\"Custom exception for error reporting.\"\"\"\n\n\n# ------------------------------------- HELPERS ------------------------------------\ndef _fetch_args(attr_names, config) -> dict:\n    args = {}\n    for arg in attr_names:\n        if arg in config.option.__dict__.keys() and config.getoption(f\"{arg}\"):\n            args[arg] = config.getoption(f\"{arg}\")\n    return args", "\n\ndef _get_tagged_scenarios(raw_yaml_dict, config, fspath) -> dict:\n    valid_scenarios = {}\n    tags_to_query_for = config.getoption(\"--tags\") or os.getenv(\"TAGS\")\n    if tags_to_query_for:\n        for scenario_name, scenario_contents in raw_yaml_dict.items():\n            tags_list = scenario_contents.get(\"tags\")\n\n            # protecting for the case where tags is specified as a key in the\n            # YAML, but there's no value pair\n            if not tags_list:\n                tags_list = []\n\n            # always include the scenario name as a tag, this gives an easy way to\n            # select a single scenario in test pipelines (you can also do the\n            # entirely different format of specifying the .py file, yaml file &\n            # scenario name, but this makes the script logic much more complex)\n            tags_list.append(scenario_name)\n            if tagmatcher.match(query_str=tags_to_query_for, tags=tags_list):\n                valid_scenarios[scenario_name] = scenario_contents\n        logging.info(\n            f\"Scenarios collected that match the specific tag query `\"\n            f\"{tags_to_query_for}`: \"\n            f\"{[scenario_name for scenario_name in valid_scenarios.keys()]}\"\n        )\n    else:\n        logging.warning(\n            f\"Since no tags param was specified, ALL scenarios in \"\n            f\"{fspath} will be run!\"\n        )\n        valid_scenarios = raw_yaml_dict\n\n    return valid_scenarios", "\n\ndef fetch_value_from_multiple_sources(sources, key):\n    \"\"\"Calculate a value from the given sources, using the list order as precedence.\n\n    Used by the pre-proccess fixture to fetch a few values that we need in order to do\n    the entire merge of values.\n\n    \"\"\"\n    value = None\n    for source in sources:\n        value = value or source.get(key)\n    return value", "\n\ndef type_check_list_of_strs(list_of_strs):\n    \"\"\"Return True if list of strings or [], false if anything else.\"\"\"\n    check_passed = False\n    if type(list_of_strs) == list:\n        all_strs = True\n        for s in list_of_strs:\n            all_strs = all_strs and type(s) == str\n        check_passed = all_strs\n    return check_passed", ""]}
{"filename": "src/grasshopper/lib/fixtures/grasshopper_constants.py", "chunked_list": ["\"\"\"The file for the GrasshopperConstants class.\"\"\"\n\n\nclass GrasshopperConstants:\n    \"\"\"Things that are always the same in grasshopper tests.\"\"\"\n\n    GRASSHOPPER_ATTR_NAMES = [\n        \"runtime\",\n        \"users\",\n        \"spawn_rate\",\n        \"shape\",\n        \"scenario_file\",\n        \"scenario_name\",\n        \"tags\",\n        \"influxdb\",  # legacy influx_host arg\n        \"influx_host\",\n        \"influx_port\",\n        \"influx_user\",\n        \"influx_pwd\",\n        \"shape_instance\",\n        \"scenario_delay\",\n        \"slack_webhook\",\n        \"slack_report_failures_only\",\n        \"cleanup_s3\",\n        \"rp_token\",\n        \"rp_launch_name\",\n        \"rp_project\",\n        \"rp_endpoint\",\n    ]\n    RUNTIME_DEFAULT = 120.0\n    USERS_DEFAULT = 1\n    SPAWN_RATE_DEFAULT = 1.0\n    SHAPE_DEFAULT = \"Default\"\n    SCENARIO_DELAY_DEFAULT = 0.0\n    THRESHOLD_PERCENTILE_DEFAULT = 0.9", ""]}
{"filename": "src/grasshopper/lib/reporting/reporter_extensions.py", "chunked_list": ["\"\"\"Module: ReporterExtensions.\"\"\"\n# Standard Library\nimport logging\nfrom typing import Any, Dict, List\n\nfrom grasshopper.lib.reporting.iextendedreporter import IExtendedReporter\n\nlogger = logging.getLogger()\n\n\nclass ReporterExtensions:\n    \"\"\"Class to hold the registration information for extended reporters (listeners).\n\n    Across this entire class, using `er` = extended reporter for space reasons\n    \"\"\"\n\n    EVENTS = [\n        \"event_pre_test\",\n        \"event_post_test\",\n        \"event_pre_suite\",\n        \"event_post_suite\",\n    ]\n    _ers: dict[str, IExtendedReporter] = []\n\n    @classmethod\n    def register_er(cls, er: IExtendedReporter) -> None:\n        \"\"\"Register an instance of an er.\"\"\"\n        try:\n            cls._ers[er.get_name()] = er\n        except Exception as e:\n            # intentionally catching all exceptions so that if there is a problem with\n            # an extended reporter it won't a) get registered & b) affect anything else\n            logger.warning(\n                f\"Error in [register_er] with reporter extension: {type(e).__name__} | \"\n                f\"{e}\"\n            )\n\n    @classmethod\n    def unregister_er(cls, er_name: str) -> None:\n        \"\"\"Unregister an instance, by key, of an er.\"\"\"\n        try:\n            del cls._ers[er_name]\n        except KeyError as e:\n            # ignore and log warning if asked to unregister something that isn't\n            # currently registered\n            logger.warning(e)\n\n    @classmethod\n    @property\n    def registrations(cls) -> Dict[str, IExtendedReporter]:\n        \"\"\"Return the list of currently registered ers.\"\"\"\n        return cls._ers\n\n    @classmethod\n    @property\n    def ers(cls) -> List[IExtendedReporter]:\n        return [v for k, v in cls._ers.items()]\n\n    @classmethod\n    def clear_registrations(cls) -> None:\n        \"\"\"Clear all registrations, mainly used to support unit testing.\"\"\"\n        cls._ers = {}\n\n    @classmethod\n    def _notify_event(cls, event: str, *args, **kwargs) -> None:\n        for er in cls.ers:\n            try:\n                logger.debug(\n                    f\"Calling event {event} on reporter extension {er.get_name()}\"\n                )\n                event_method = getattr(er, event)\n                event_method(*args, **kwargs)\n            except Exception as e:\n                # intentionally catching ALL exceptions so that a faulty extension will\n                # not ever affect other reporters or the main test process\n                logger.warning(\n                    f\"Error in event [{event}] on reporter extension [{er.get_name()}]:\"\n                    f\" {type(e).__name__} | {e}\"\n                )\n\n    @classmethod\n    def notify_pre_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ):\n        \"\"\"Call the pre-test event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_pre_test\", test_name, start_epoch, test_args=test_args, **kwargs\n        )\n\n    @classmethod\n    def notify_pre_suite(cls, suite_name, start_epoch, suite_args={}, **kwargs):\n        \"\"\"Call the pre-suite event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_pre_suite\", suite_name, start_epoch, suite_args=suite_args, **kwargs\n        )\n\n    @classmethod\n    def notify_post_suite(\n        cls, suite_name, start_epoch, end_epoch, suite_args={}, **kwargs\n    ):\n        \"\"\"Call the post-suite event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_post_suite\",\n            suite_name,\n            start_epoch,\n            end_epoch,\n            suite_args=suite_args,\n            **kwargs,\n        )\n\n    @classmethod\n    def notify_post_test(\n        cls,\n        test_name,\n        start_epoch,\n        end_epoch,\n        locust_env,\n        test_args={},\n        **kwargs,\n    ):\n        \"\"\"Call the post-test event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_post_test\",\n            test_name,\n            start_epoch,\n            end_epoch,\n            locust_env,\n            test_args=test_args,\n            **kwargs,\n        )", "\n\nclass ReporterExtensions:\n    \"\"\"Class to hold the registration information for extended reporters (listeners).\n\n    Across this entire class, using `er` = extended reporter for space reasons\n    \"\"\"\n\n    EVENTS = [\n        \"event_pre_test\",\n        \"event_post_test\",\n        \"event_pre_suite\",\n        \"event_post_suite\",\n    ]\n    _ers: dict[str, IExtendedReporter] = []\n\n    @classmethod\n    def register_er(cls, er: IExtendedReporter) -> None:\n        \"\"\"Register an instance of an er.\"\"\"\n        try:\n            cls._ers[er.get_name()] = er\n        except Exception as e:\n            # intentionally catching all exceptions so that if there is a problem with\n            # an extended reporter it won't a) get registered & b) affect anything else\n            logger.warning(\n                f\"Error in [register_er] with reporter extension: {type(e).__name__} | \"\n                f\"{e}\"\n            )\n\n    @classmethod\n    def unregister_er(cls, er_name: str) -> None:\n        \"\"\"Unregister an instance, by key, of an er.\"\"\"\n        try:\n            del cls._ers[er_name]\n        except KeyError as e:\n            # ignore and log warning if asked to unregister something that isn't\n            # currently registered\n            logger.warning(e)\n\n    @classmethod\n    @property\n    def registrations(cls) -> Dict[str, IExtendedReporter]:\n        \"\"\"Return the list of currently registered ers.\"\"\"\n        return cls._ers\n\n    @classmethod\n    @property\n    def ers(cls) -> List[IExtendedReporter]:\n        return [v for k, v in cls._ers.items()]\n\n    @classmethod\n    def clear_registrations(cls) -> None:\n        \"\"\"Clear all registrations, mainly used to support unit testing.\"\"\"\n        cls._ers = {}\n\n    @classmethod\n    def _notify_event(cls, event: str, *args, **kwargs) -> None:\n        for er in cls.ers:\n            try:\n                logger.debug(\n                    f\"Calling event {event} on reporter extension {er.get_name()}\"\n                )\n                event_method = getattr(er, event)\n                event_method(*args, **kwargs)\n            except Exception as e:\n                # intentionally catching ALL exceptions so that a faulty extension will\n                # not ever affect other reporters or the main test process\n                logger.warning(\n                    f\"Error in event [{event}] on reporter extension [{er.get_name()}]:\"\n                    f\" {type(e).__name__} | {e}\"\n                )\n\n    @classmethod\n    def notify_pre_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ):\n        \"\"\"Call the pre-test event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_pre_test\", test_name, start_epoch, test_args=test_args, **kwargs\n        )\n\n    @classmethod\n    def notify_pre_suite(cls, suite_name, start_epoch, suite_args={}, **kwargs):\n        \"\"\"Call the pre-suite event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_pre_suite\", suite_name, start_epoch, suite_args=suite_args, **kwargs\n        )\n\n    @classmethod\n    def notify_post_suite(\n        cls, suite_name, start_epoch, end_epoch, suite_args={}, **kwargs\n    ):\n        \"\"\"Call the post-suite event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_post_suite\",\n            suite_name,\n            start_epoch,\n            end_epoch,\n            suite_args=suite_args,\n            **kwargs,\n        )\n\n    @classmethod\n    def notify_post_test(\n        cls,\n        test_name,\n        start_epoch,\n        end_epoch,\n        locust_env,\n        test_args={},\n        **kwargs,\n    ):\n        \"\"\"Call the post-test event on all registered ers.\"\"\"\n        cls._notify_event(\n            \"event_post_test\",\n            test_name,\n            start_epoch,\n            end_epoch,\n            locust_env,\n            test_args=test_args,\n            **kwargs,\n        )", ""]}
{"filename": "src/grasshopper/lib/reporting/shared_reporting.py", "chunked_list": ["\"\"\"Module: Shared Reporting.\n\nContains a variety of methods that are used by grasshopper provided extended reporters.\nAlso intended to be optionally used by ers that grasshopper consumers create.\n\n\"\"\"\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom termcolor import colored\n", "from termcolor import colored\n\n\nclass SharedReporting:\n    \"\"\"Shared functionality that various extended reporters might want to use.\"\"\"\n\n    FORMAT_STRING_STANDARD_COLUMN_LAYOUT = \"{:<30} {:<10} {:<10} {:<10}\"\n\n    @staticmethod\n    def colorized_threshold_string(\n        trend_name: str,\n        threshold: Dict[str, Any],\n        format_string: str = FORMAT_STRING_STANDARD_COLUMN_LAYOUT,\n    ) -> Tuple[str, bool]:\n        \"\"\"Build a colorized string for a threshold (based on success).\"\"\"\n        formatted, any_errors_occurred = SharedReporting.plain_threshold_string(\n            trend_name, threshold, format_string\n        )\n        success = SharedReporting.get_threshold_status(threshold)\n        if success and not any_errors_occurred:\n            colored_string = colored(formatted, \"green\", attrs=[\"bold\"])\n        else:\n            colored_string = colored(formatted, \"red\", attrs=[\"bold\"])\n\n        return colored_string, any_errors_occurred\n\n    @staticmethod\n    def get_threshold_status(threshold: Dict[str, Any]) -> bool:\n        \"\"\"Perform a smart calculation of success for a single threshold.\"\"\"\n        try:\n            success = threshold.get(\"succeeded\") or False\n        except (AttributeError, KeyError):\n            success = False\n\n        return success\n\n    @staticmethod\n    def plain_threshold_string(\n        trend_name: str,\n        threshold: Dict[str, Any],\n        format_string: str = FORMAT_STRING_STANDARD_COLUMN_LAYOUT,\n    ) -> Tuple[str, bool]:\n        \"\"\"Generate a human friendly string for a threshold.\"\"\"\n        any_errors_occurred = False\n        try:\n            formatted = format_string.format(\n                trend_name,\n                int(threshold[\"percentile\"] * 100),\n                f'{threshold[\"less_than_in_ms\"]}ms',\n                f'{int(threshold[\"actual_value_in_ms\"])}ms',\n            )\n        except Exception as e:\n            # catch any kind of error (structural) and replace with a meaningful error\n            # line to get reported out to the destination\n            formatted = (\n                f\"UNABLE TO PROCESS THRESHOLD FOR TREND <{trend_name}>: \"\n                f\"{type(e).__name__} | {e}\"\n            )\n            any_errors_occurred = True\n        return formatted, any_errors_occurred\n\n    @staticmethod\n    def calculate_threshold_lines(\n        trends: Dict[str, Dict[str, Any]],\n        format_string: str = FORMAT_STRING_STANDARD_COLUMN_LAYOUT,\n        formatter: Callable[\n            [str, dict, Optional[str]], str\n        ] = plain_threshold_string.__func__,\n    ) -> List[str]:\n        \"\"\"Generate a list lines, one line per threshold.\n\n        Parameters\n        ----------\n            trends (dict): dictionary of trends, typically from the locust stats object\n            the key is the trendname, the value is thresh object\n\n        Optional Parameters\n        -------------------\n            format_string: string, possibly containing placeholders for runtime values\n            from the threshold object; the methods in this class pass the following\n            values to the `.format(...)` method\n                trend_name: str,\n                percentile: int,\n                less_than_in_ms: float,\n                actual_value_in_ms: float\n            formatter(Callable): method that will be used to format a single line for a\n            single threshold; the parameters that this method must accept are\n                trend_name: str,\n                threshold: dict,\n                optional format_string: str (see above for description of this format\n                string)\n            by default, it will use the method `plain_threshold_string` but you can\n            also see another example in `colorized_threshold_string` on this class\n\n        Returns\n        -------\n            list of lines (List[str]): List of lines calculated from the thresholds\n        \"\"\"\n        lines = []\n\n        try:\n            # filter the trends down to only those that have at least one threshold\n            # listed\n            filtered_trends = {k: v for (k, v) in trends.items() if \"thresholds\" in v}\n\n            # for each trend, get a string for each threshold & add to the list\n            for trend_name, trend_values in filtered_trends.items():\n                for threshold in trend_values[\"thresholds\"]:\n                    formatted, _ = formatter(trend_name, threshold, format_string)\n                    lines.append(formatted)\n        except AttributeError:\n            # ignore error and just return an empty list\n            pass\n\n        return lines", ""]}
{"filename": "src/grasshopper/lib/reporting/__init__.py", "chunked_list": ["\"\"\"Package: Reporting.\"\"\"\n"]}
{"filename": "src/grasshopper/lib/reporting/er_basic_console_reporter.py", "chunked_list": ["\"\"\"Module: ER Basic Console Reporter.\n\nCan be used by any grasshopper tests by having a fixture that registers this reporter.\nServes as an example implementation of the IExtendedReporter interface.\n\n\"\"\"\nimport logging\nfrom typing import Any, Dict\n\nfrom locust import env as locust_environment", "\nfrom locust import env as locust_environment\n\nfrom grasshopper.lib.reporting.iextendedreporter import IExtendedReporter\nfrom grasshopper.lib.reporting.shared_reporting import SharedReporting\n\nlogger = logging.getLogger()\nlogger.propagate = True\n\n\nclass ERBasicConsoleReporter(IExtendedReporter):\n    \"\"\"Handle writing basic information to the console for a test run.\"\"\"\n\n    LINE = \"-\" * 80\n    _name = \"ER Basic Console Reporter\"\n\n    @classmethod\n    def get_name(cls) -> str:\n        \"\"\"Retrieve the name for this er.\"\"\"\n        return cls._name\n\n    @classmethod\n    def set_name(cls, new_name: str) -> None:\n        \"\"\"Set the name for this er.\"\"\"\n        cls._name = new_name\n\n    @classmethod\n    def event_pre_suite(\n        cls,\n        suite_name: str,\n        start_epoch: float,\n        suite_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (pre)suite information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Starting Suite {suite_name} at {start_epoch} with the following \"\n            f\"arguments:\"\n        )\n        for k, v in suite_args.items():\n            logger.info(f\"{k}=={v}\")\n\n    @classmethod\n    def event_post_suite(\n        cls,\n        suite_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        suite_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (post)suite information to the console.\"\"\"\n        logger.info(f\"Suite {suite_name} complete\")\n        logger.info(cls.LINE)\n\n    @classmethod\n    def event_pre_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (pre)test information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Starting Test {test_name} at {start_epoch} with the following arguments:\"\n        )\n        for k, v in test_args.items():\n            logger.info(f\"{k}=={v}\")\n\n    @classmethod\n    def event_post_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        locust_environment: locust_environment = None,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (post)test information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Test {test_name} complete at {end_epoch} with the following results:\"\n        )\n        logger.info(f\"Epoch time stamp: {start_epoch} - {end_epoch}\")\n        cls._report_thresholds_to_console(locust_environment.stats.trends)\n        logger.info(cls.LINE)\n\n    @staticmethod\n    def _report_thresholds_to_console(trends: Dict[str, Dict[str, Any]]) -> None:\n        \"\"\"Print all the thresholds to the console in a pretty table.\"\"\"\n        logger.info(\"----------------------THRESHOLD REPORT-------------------------\")\n        logger.info(\n            SharedReporting.FORMAT_STRING_STANDARD_COLUMN_LAYOUT.format(\n                \"Trend_Name\", \"Percentile\", \"Limit\", \"Actual\"\n            )\n        )\n\n        lines = SharedReporting.calculate_threshold_lines(\n            trends, formatter=SharedReporting.colorized_threshold_string\n        )\n        for line in lines:\n            logger.info(line)\n\n        logger.info(\"--------------------END THRESHOLD REPORT-----------------------\")", "\n\nclass ERBasicConsoleReporter(IExtendedReporter):\n    \"\"\"Handle writing basic information to the console for a test run.\"\"\"\n\n    LINE = \"-\" * 80\n    _name = \"ER Basic Console Reporter\"\n\n    @classmethod\n    def get_name(cls) -> str:\n        \"\"\"Retrieve the name for this er.\"\"\"\n        return cls._name\n\n    @classmethod\n    def set_name(cls, new_name: str) -> None:\n        \"\"\"Set the name for this er.\"\"\"\n        cls._name = new_name\n\n    @classmethod\n    def event_pre_suite(\n        cls,\n        suite_name: str,\n        start_epoch: float,\n        suite_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (pre)suite information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Starting Suite {suite_name} at {start_epoch} with the following \"\n            f\"arguments:\"\n        )\n        for k, v in suite_args.items():\n            logger.info(f\"{k}=={v}\")\n\n    @classmethod\n    def event_post_suite(\n        cls,\n        suite_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        suite_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (post)suite information to the console.\"\"\"\n        logger.info(f\"Suite {suite_name} complete\")\n        logger.info(cls.LINE)\n\n    @classmethod\n    def event_pre_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (pre)test information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Starting Test {test_name} at {start_epoch} with the following arguments:\"\n        )\n        for k, v in test_args.items():\n            logger.info(f\"{k}=={v}\")\n\n    @classmethod\n    def event_post_test(\n        cls,\n        test_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        locust_environment: locust_environment = None,\n        test_args: Dict[str, Any] = {},\n        **kwargs,\n    ) -> None:\n        \"\"\"Report the grasshopper (post)test information to the console.\"\"\"\n        logger.info(cls.LINE)\n        logger.info(\n            f\"Test {test_name} complete at {end_epoch} with the following results:\"\n        )\n        logger.info(f\"Epoch time stamp: {start_epoch} - {end_epoch}\")\n        cls._report_thresholds_to_console(locust_environment.stats.trends)\n        logger.info(cls.LINE)\n\n    @staticmethod\n    def _report_thresholds_to_console(trends: Dict[str, Dict[str, Any]]) -> None:\n        \"\"\"Print all the thresholds to the console in a pretty table.\"\"\"\n        logger.info(\"----------------------THRESHOLD REPORT-------------------------\")\n        logger.info(\n            SharedReporting.FORMAT_STRING_STANDARD_COLUMN_LAYOUT.format(\n                \"Trend_Name\", \"Percentile\", \"Limit\", \"Actual\"\n            )\n        )\n\n        lines = SharedReporting.calculate_threshold_lines(\n            trends, formatter=SharedReporting.colorized_threshold_string\n        )\n        for line in lines:\n            logger.info(line)\n\n        logger.info(\"--------------------END THRESHOLD REPORT-----------------------\")", ""]}
{"filename": "src/grasshopper/lib/reporting/iextendedreporter.py", "chunked_list": ["\"\"\"Module: IExtendedReporter.\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nfrom locust import env as locust_environment\n\n\nclass IExtendedReporter(ABC):\n    \"\"\"Abstract class that defines the interface that all er classes must implement.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def get_name(cls) -> str:\n        \"\"\"Get the name of this extended reporter.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def set_name(cls, new_name) -> str:\n        \"\"\"Set the name for this extended reporter.\"\"\"\n\n    @abstractmethod\n    def event_pre_suite(\n        self, suite_name: str, start_epoch: float, suite_args: dict = {}, **kwargs\n    ):\n        \"\"\"Call by ReporterExtensions when a pre-suite event occurs.\"\"\"\n\n    @abstractmethod\n    def event_post_suite(\n        self,\n        suite_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        suite_args: dict = {},\n        **kwargs\n    ):\n        \"\"\"Call by ReporterExtensions when a post-test event occurs.\"\"\"\n\n    @abstractmethod\n    def event_pre_test(\n        self, test_name: str, start_epoch: float, test_args: dict = {}, **kwargs\n    ):\n        \"\"\"Call by ReporterExtensions when a pre-test event occurs.\"\"\"\n\n    @abstractmethod\n    def event_post_test(\n        self,\n        test_name: str,\n        start_epoch: float,\n        end_epoch: float,\n        locust_env: locust_environment = None,\n        test_args: dict = {},\n        **kwargs\n    ):\n        \"\"\"Call by ReporterExtensions when a post-test event occurs.\"\"\"", ""]}
{"filename": "src/grasshopper/lib/util/decorators.py", "chunked_list": ["\"\"\"Module: Decorators.\"\"\"\nimport functools\nimport logging\n\nlogger = logging.getLogger()\n\n\ndef deprecate(deprecated, redirect):\n    \"\"\"Log a deprecation message at the warning level before method execution.\n\n    We didn't add the [deprecation](https://pypi.org/project/deprecation/) package\n    because we only needed this one decorator right now. If we end up wanting to\n    add any additional functionality along these lines, we should add this package.\n    \"\"\"\n\n    def decorator_deprecate(func):\n        @functools.wraps(func)\n        def wrapper_deprecate(*args, **kwargs):\n            logger.warning(\n                f\"Method '{deprecated}' is deprecated, please use '{redirect}' instead.\"\n            )\n            value = func(*args, **kwargs)\n            return value\n\n        return wrapper_deprecate\n\n    return decorator_deprecate", ""]}
{"filename": "src/grasshopper/lib/util/check_constants.py", "chunked_list": ["\"\"\"Module: Check Constants.\"\"\"\n\n\nclass CheckConstants:\n    \"\"\"Check verdict constants.\"\"\"\n\n    VERDICT_ALL_PASSED = \"VERDICT_ALL_PASSED\"\n    # not 100% passed, but the failure rate is under the threshold specified\n    VERDICT_PASSED_RATE_OVER_THRESHOLD = \"VERDICT_PASSED_RATE_OVER_THRESHOLD\"\n    VERDICT_FAILED = \"VERDICT_FAILED\"", ""]}
{"filename": "src/grasshopper/lib/util/listeners.py", "chunked_list": ["\"\"\"Locust custom listeners.\n\nThe listeners module contains all the custom listeners that we have defined for Locust.\n\"\"\"\nimport logging\nfrom datetime import datetime\n\nfrom locust import events\nfrom locust.env import Environment\nfrom locust_influxdb_listener import InfluxDBListener, InfluxDBSettings", "from locust.env import Environment\nfrom locust_influxdb_listener import InfluxDBListener, InfluxDBSettings\n\nfrom grasshopper.lib.util.check_constants import CheckConstants\nfrom grasshopper.lib.util.utils import (\n    report_checks_to_console,\n    report_thresholds_to_console,\n)\n\nlogger = logging.getLogger()", "\nlogger = logging.getLogger()\n\n\nclass GrasshopperListeners:\n    \"\"\"All of the hooks used to report custom metrics/checks to dbs/the console.\"\"\"\n\n    influxdb_listener: InfluxDBListener = None\n    locust_environment: Environment\n\n    def __init__(self, environment: Environment):\n        \"\"\"Initialize for the given locust environment.\"\"\"\n        environment.events.test_start.add_listener(self.on_test_start)\n        environment.events.test_stop.add_listener(self.on_test_stop_append_metric_data)\n        self.locust_environment = environment\n\n    @events.test_start.add_listener\n    def on_test_start(self, environment: Environment, **_kwargs):\n        \"\"\"Create a listener for the test start event, starts an influxdb connection.\"\"\"\n        influx_configuration = environment.grasshopper.influx_configuration\n        influx_host = influx_configuration.get(\"influx_host\")\n\n        if influx_host:\n            logger.info(\n                f\"All collected metrics reported to influxdb host `{influx_host}`\"\n            )\n            influx_db_settings = InfluxDBSettings(\n                **influx_configuration,\n                database=\"locust\",\n            )\n            # start listener with the given configuration\n            self.influxdb_listener = InfluxDBListener(\n                env=environment, influxDbSettings=influx_db_settings\n            )\n        else:\n            logger.info(\n                \"InfluxDB host was not specified. Skipping influxdb listener \"\n                \"initialization...\"\n            )\n\n    @events.test_stop.add_listener\n    def on_test_stop_append_metric_data(self, environment, **_kwargs):\n        \"\"\"Create a listener which appends metrics to the environment.stats object.\"\"\"\n        try:\n            self._append_trend_data(environment)\n        except Exception as e:\n            logger.warning(\n                f\"Unexpected exception appending trend data to environment object: {e}\"\n            )\n        try:\n            self._append_checks_data(environment)\n        except Exception as e:\n            logger.warning(\n                f\"Unexpected exception appending trend data to environment object: {e}\"\n            )\n\n    def flush_check_to_dbs(self, check_name: str, check_passed: bool, extra_tags: dict):\n        \"\"\"Flush a check datapoint to whatever grasshopper dbs are being used.\"\"\"\n        environment_base_url = self.locust_environment.host\n        tags = {\"check_name\": check_name, \"environment\": environment_base_url}\n        if hasattr(self.locust_environment, \"extra_context\"):\n            tags.update(self.locust_environment.extra_context)\n        tags.update(extra_tags)\n        fields = {\"check_passed\": int(check_passed)}\n        time = datetime.utcnow()\n        points = [\n            {\n                \"measurement\": \"locust_checks\",\n                \"tags\": tags,\n                \"time\": time,\n                \"fields\": fields,\n            }\n        ]\n\n        if getattr(self, \"influxdb_listener\") is not None:\n            self.influxdb_listener.influxdb_client.write_points(points)\n\n    @staticmethod\n    def _append_trend_data(environment):\n        if (\n            hasattr(environment.stats, \"trends\")\n            and type(environment.stats.trends) is dict\n        ):\n            for trend_name, trend_values in environment.stats.trends.items():\n                for threshold_object in trend_values.get(\"thresholds\", []):\n                    threshold_object[\"actual_value_in_ms\"] = environment.stats.get(\n                        trend_name, threshold_object[\"http_method\"]\n                    ).get_response_time_percentile(threshold_object[\"percentile\"])\n                    # the threshold passes if it has a non-zero value (zero means that\n                    # the completion trigger never fired for this trend) - this\n                    # typically is a functional defect or test problem\n                    # AND the actual value is under the threshold set\n                    threshold_object[\"succeeded\"] = (\n                        threshold_object[\"actual_value_in_ms\"] > 0\n                        and threshold_object[\"actual_value_in_ms\"]\n                        <= threshold_object[\"less_than_in_ms\"]\n                    )\n\n            report_thresholds_to_console(environment.stats.trends)\n        else:\n            logger.info(\"No threshold data to report.\")\n\n    @staticmethod\n    def _append_checks_data(environment):\n        if (\n            hasattr(environment.stats, \"checks\")\n            and type(environment.stats.checks) is dict\n        ):\n            # mark all checks as passed or failed based on multiple criteria\n            for check_key, check_item in environment.stats.checks.items():\n                checks_passed = check_item.get(\"passed\", 0)\n                checks_total = check_item.get(\"total\", 0)\n\n                # add some calculated values, for convenience of consumers\n                check_item[\"percentage_passed\"] = (\n                    (checks_passed / checks_total) if checks_total > 0 else 1\n                )\n                check_item[\"percentage_passed_display\"] = round(\n                    check_item[\"percentage_passed\"] * 100, 2\n                )\n\n                # add a final \"verdict\" for this check\n                if checks_total == checks_passed:\n                    verdict = CheckConstants.VERDICT_ALL_PASSED\n                elif check_item[\"percentage_passed\"] >= check_item[\"warning_threshold\"]:\n                    verdict = CheckConstants.VERDICT_PASSED_RATE_OVER_THRESHOLD\n                else:\n                    verdict = CheckConstants.VERDICT_FAILED\n\n                check_item[\"verdict\"] = verdict\n\n            report_checks_to_console(environment.stats.checks)\n        else:\n            logger.info(\"No checks data to report.\")", ""]}
{"filename": "src/grasshopper/lib/util/metrics.py", "chunked_list": ["\"\"\"Module: Metrics.\n\nDecorators that help with tracking additional metrics for a locust test run.\nThe task decorator is meant to replace the existing locust @task decorator in order to\nincrement vu_iteration counters every time locust picks a task.\n\n\"\"\"\nimport functools\nimport logging\n", "import logging\n\nfrom locust import task as locusttask\n\nlogger = logging.getLogger()\n\n\ndef count_iterations(func):\n    \"\"\"Provide a wrapper that increments the vu_iteration count when a task is called.\n\n    Assumes that it is taking in a function that is called with the same signature\n    that tasks have, which is a self reference (always some subclass of HttpUser).\n    Note that I included *args and **kwargs for completeness, but they are not used\n    at this time by Locust.\n\n    Wrapper stores the vu_iteration count in the httpuser.environment.stats location, so\n    it is global across all virtual users.\n\n    It also increments the individual user's vu_iteration count, if the attribute\n    exists. Our BaseJourney class adds and initializes this attribute.\n\n    The functools.wraps is very important here because the way that Locust marks a task\n    to be collected on a test is by appending an attr called locust_task_weight to the\n    *method* object. Therefore, it is imperative that we transfer attrs from the\n    function being wrapped to the wrapper.\n\n    This decorator is not meant to be used directly, see the task decorator below.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(httpuser, *args, **kwargs):\n        httpuser.environment.stats.num_iterations += 1\n        if hasattr(httpuser, \"vu_iteration\"):\n            httpuser.vu_iteration += 1\n\n        logger.debug(\n            f\"{httpuser.client_id} Global vu_iteration count == \"\n            f\"{httpuser.environment.stats.num_iterations}\"\n        )\n        logger.debug(\n            f\"{httpuser.client_id} Per user vu_iteration count == \"\n            f\"{httpuser.__dict__.get('vu_iteration')}\"\n        )\n        return func(httpuser, *args, **kwargs)\n\n    return wrapper", "\n\ndef task(func):\n    \"\"\"Wrap Locust task decorator in order to count iterations.\n\n    Wrap the @task (both flavors - with and without arguments) decorator with our own\n    decorator that allows us to count iterations automatically without having to put\n    code in every task we create.\n\n    Some info on how this is working, since decorators are tricky...\n    1. Syntatic Sugar\n    ```python\n    @my_decorator\n    def foo(): pass\n    ```\n    is syntactic sugar for `my_decorator(foo)` and when foo is called, you can think of\n    that as `my_decorator(foo)()`\n    2. Stacked decorators are simply functional composition, that is\n    ```python\n    @d1\n    @d2\n    def foo(): pass\n    ```\n    is the same as `d1(d2(foo))` and when foo is called, you can think of that as\n    `d1(d2(foo))()`\n    3. The trickiest part of this is the support for using with and without arguments.\n    \"If a decorator function @invocation isn't passed any explicit arguments, it is\n    called with the function defined in the following def. If it is passed arguments,\n    then it is first called with them and then the result of that preliminary call\n    (which must itself also be a callable) is called with the function being defined.\n    Either way, the return value of the last or only call is bound to the defined\n    function name.\"\n    @d1(arg)\n    def foo(...): pass\n\n    The python interperter first calls d1 with arg and that must return a callable,\n    we'll call that c1.\n    Then it calls c1 with foo, which again returns a callable. This callable must\n    accept the same signature as foo and replaces the bound method object for foo.\n\n    In our case, for the new task method, in either case we want to apply both methods\n    (Locust's task and our count_iterations) to func. The difference is in the form it\n    is returned as.\n\n    Hopefully, that is all clear as mud!\n    \"\"\"\n\n    @functools.wraps(func)\n    def decorator_func(func):\n        return count_iterations((locusttask(weight))(func))\n\n    if callable(func):  # no arguments are being used, the parameter is the task method\n        return count_iterations(locusttask(func))\n    else:\n        # arguments are being used, so we need to return a callable to be used with the\n        # 2nd call that the interpreter will make in this situation\n        weight = func\n        return decorator_func", ""]}
{"filename": "src/grasshopper/lib/util/__init__.py", "chunked_list": ["\"\"\"These are the utility functions for the project.\"\"\"\n"]}
{"filename": "src/grasshopper/lib/util/utils.py", "chunked_list": ["\"\"\"Module: Utils.\n\nThese are the miscellaneous utility functions for the project that couldn't find\nanother place to live.\n\"\"\"\n\nimport logging\nimport sys\nimport time\nfrom datetime import datetime", "import time\nfrom datetime import datetime\n\nfrom termcolor import colored\n\nfrom grasshopper.lib.util.check_constants import CheckConstants\n\nlogger = logging.getLogger()\n\n\ndef custom_trend(trend_name: str, extra_tag_keys=[]):\n    \"\"\"Establish a custom trend for a function.\"\"\"\n\n    def calc_time_delta_and_report_metric(func):\n        def wrapper(journey_object, *args, **kwargs):\n            tags = {}\n            try:\n                environment = journey_object.environment\n                host = environment.host\n                test_parameters = journey_object.scenario_args\n                if hasattr(journey_object.environment, \"extra_context\"):\n                    tags.update(journey_object.environment.extra_context)\n            except AttributeError:\n                raise ReferenceError(\n                    \"The custom_trend decorator must be placed on a journey function \"\n                    \"which has a test parameters and host attributes defined.\"\n                )\n\n            start_time = datetime.now()\n            result = func(journey_object, *args, **kwargs)\n            end_time = datetime.now()\n            time_delta = end_time - start_time\n            tags.update(\n                {\n                    extra_tag_key: test_parameters.get(extra_tag_key)\n                    for extra_tag_key in extra_tag_keys\n                }\n            )\n            tags[\"environment\"] = host\n            environment.events.request.fire(\n                request_type=\"CUSTOM\",\n                name=trend_name,\n                response_time=round(time_delta.total_seconds() * 1000, 3),\n                response_length=0,\n                response=None,\n                context=tags,\n                exception=None,\n            )\n            return result\n\n        return wrapper\n\n    return calc_time_delta_and_report_metric", "\n\ndef custom_trend(trend_name: str, extra_tag_keys=[]):\n    \"\"\"Establish a custom trend for a function.\"\"\"\n\n    def calc_time_delta_and_report_metric(func):\n        def wrapper(journey_object, *args, **kwargs):\n            tags = {}\n            try:\n                environment = journey_object.environment\n                host = environment.host\n                test_parameters = journey_object.scenario_args\n                if hasattr(journey_object.environment, \"extra_context\"):\n                    tags.update(journey_object.environment.extra_context)\n            except AttributeError:\n                raise ReferenceError(\n                    \"The custom_trend decorator must be placed on a journey function \"\n                    \"which has a test parameters and host attributes defined.\"\n                )\n\n            start_time = datetime.now()\n            result = func(journey_object, *args, **kwargs)\n            end_time = datetime.now()\n            time_delta = end_time - start_time\n            tags.update(\n                {\n                    extra_tag_key: test_parameters.get(extra_tag_key)\n                    for extra_tag_key in extra_tag_keys\n                }\n            )\n            tags[\"environment\"] = host\n            environment.events.request.fire(\n                request_type=\"CUSTOM\",\n                name=trend_name,\n                response_time=round(time_delta.total_seconds() * 1000, 3),\n                response_length=0,\n                response=None,\n                context=tags,\n                exception=None,\n            )\n            return result\n\n        return wrapper\n\n    return calc_time_delta_and_report_metric", "\n\ndef current_method_name():\n    \"\"\"Get the name of the current method.\"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef highlight_print(text: str):\n    \"\"\"Print a string in bold with a highlighted color.\"\"\"\n    ctext = colored(text, \"green\", attrs=[\"bold\"])\n    logger.info(ctext)", "\n\ndef check(\n    check_name: str,\n    check_is_good: bool,\n    env,\n    tags={},\n    halt_on_failure=False,\n    msg_on_failure=None,\n    flexible_warning=0.95,\n):\n    \"\"\"\n    Do a soft assertion on some condition.\n\n    Required parameters:\n    - check_name: A name for the check\n    - check_is_good: A boolean expression that resolves to a boolean value\n      (example: request.body.data[0] == 1)\n    - env: The environment object (should be 'self.environment' in the locustfile)\n\n    Optional parameters:\n    - tags: an optional dictionary to add to timeseries metrics\n    - halt_on_failure: if set to True, stops test on a fail. Set to False by default\n    - flexible_warning: sets a threshold for whether the output is red or yellow.\n    (example 1: flexible_warning of 0.5, and 3/5 checks pass, output is yellow.)\n    (example 2: flexible_warning of 0.95, and 92/100 checks pass, output is red.)\n\n    TODO: this whole thing needs to be re-worked because we are calculating the\n    passing threshold in more than one place, some of which is in the slack formatter\n    plus the checks functionality should probably be it's own class instead of being\n    spread out across a couple of different locations\n    TODO: look at creating a checks class after the Grasshopper class has been merged\n\n    For now, we know that the slack formatter and report portal may sometimes have\n    slightly different calculations.\n\n    \"\"\"\n    check_is_good = bool(check_is_good)\n    check_object = {\n        \"passed\": 0,\n        \"failed\": 0,\n        \"total\": 0,\n        \"warning_threshold\": flexible_warning,\n    }\n\n    if hasattr(env.stats, \"checks\") and type(env.stats.checks) is dict:\n        if check_name not in env.stats.checks.keys():\n            env.stats.checks[check_name] = check_object\n    else:\n        env.stats.checks = {check_name: check_object}\n\n    if hasattr(env, \"grasshopper_listeners\"):\n        env.grasshopper_listeners.flush_check_to_dbs(\n            check_name=check_name, check_passed=check_is_good, extra_tags=tags\n        )\n    checks = env.stats.checks\n    checks[check_name][\"total\"] += 1\n\n    if check_is_good:\n        checks[check_name][\"passed\"] += 1\n    else:\n        checks[check_name][\"failed\"] += 1\n        logger.warning(f\"Check failed: {check_name}\")\n        if msg_on_failure:\n            logger.warning(f\"Failure message: {msg_on_failure}\")\n        if halt_on_failure:\n            logger.fatal(\"Check failed with halt_on_failure enabled. Stopping runner\")\n            env.runner.quit()", "\n\ndef report_thresholds_to_console(trend_dict):\n    \"\"\"Print all the thresholds to the console in a pretty table.\"\"\"\n    length = 32\n    logger.info(\"-\" * length + \" THRESHOLD REPORT \" + \"-\" * length)\n    logger.info(\n        \"{:<45} {:<10} {:<10} {:<10}\".format(\n            \"Trend_Name\", \"Percentile\", \"Limit\", \"Actual\"\n        )\n    )\n    for trend_name, trend_values in trend_dict.items():\n        if \"thresholds\" in trend_values:\n            for threshold_item in trend_values[\"thresholds\"]:\n                formatted_result_string = \"{:<45} {:<10} {:<10} {:<10}\".format(\n                    trend_name,\n                    int(threshold_item[\"percentile\"] * 100),\n                    f'{threshold_item[\"less_than_in_ms\"]}ms',\n                    f'{int(threshold_item[\"actual_value_in_ms\"])}ms',\n                )\n                if threshold_item[\"succeeded\"]:\n                    logger.info(\n                        colored(formatted_result_string, \"green\", attrs=[\"bold\"])\n                    )\n                else:\n                    logger.info(colored(formatted_result_string, \"red\", attrs=[\"bold\"]))\n    logger.info(\"-\" * 82)", "\n\ndef report_checks_to_console(checks_dict):\n    \"\"\"Print all the checks to the console in a pretty table.\"\"\"\n    color_map = {\n        CheckConstants.VERDICT_ALL_PASSED: \"green\",\n        CheckConstants.VERDICT_PASSED_RATE_OVER_THRESHOLD: \"yellow\",\n        CheckConstants.VERDICT_FAILED: \"red\",\n    }\n    length = 55\n    logger.info(\"-\" * length + \" CHECKS REPORT \" + \"-\" * length)\n    logger.info(\n        \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\n            \"Check_Name\", \"Passed\", \"Failed\", \"Total\", \"Percentage\"\n        )\n    )\n    for check_key, check_value in checks_dict.items():\n        passed, failed, total, percent = (\n            check_value[\"passed\"],\n            check_value[\"failed\"],\n            check_value[\"total\"],\n            check_value[\"percentage_passed_display\"],\n        )\n        result_string = \"{:<80} {:<10} {:<10} {:<10} {:<10}\".format(\n            check_key, passed, failed, total, percent\n        )\n        color_to_print = color_map.get(check_value[\"verdict\"]) or \"white\"\n        logger.info(colored(result_string, color_to_print, attrs=[\"bold\"]))\n    logger.info(\"-\" * 125)", "\n\ndef epoch_time():\n    \"\"\"Return the epoch time for the current moment ('now').\n\n    This is a convenience method, to avoid having to look up the calculation\n    of epoch time. It's also fine if you don't use it, it's not something I\n    imagine will change, like ever.\n    \"\"\"\n    return int(time.time() * 1000)", ""]}
{"filename": "src/grasshopper/lib/util/shapes.py", "chunked_list": ["\"\"\"Locust Custom Shapes.\n\nSometimes, a completely custom shaped load test is required that cannot be achieved by\nsimply setting or changing the user count and spawn rate. For example, one might want to\ngenerate a load spike or ramp up and down at custom times. By using a one of these\nclasses which extend LoadTestShape, we have full control over the user count and spawn\nrate at all times.\n\"\"\"\nimport logging\nimport json", "import logging\nimport json\n\nfrom locust import LoadTestShape\n\nlogger = logging.getLogger()\n\n\nclass Default(LoadTestShape):\n    \"\"\"Base shape for our all Load test shapes.\n\n    Replicate passing only runtime, spawn rate, users on the command line.\n    Also serves as the Base for the rest of our shapes, so that the launch method knows\n    how to instantiate. Note that all shapes now take in additional args in the\n    constructor, they are not obligated to do anything with them.\n    \"\"\"\n\n    DEFAULT_RUNTIME = 120\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.users = kwargs.get(\"users\") or 1\n        self.spawn_rate = kwargs.get(\"spawn_rate\") or 1\n        self.runtime = kwargs.get(\"runtime\") or self.DEFAULT_RUNTIME\n        self._configured_runtime = self.runtime\n\n    @property\n    def configured_runtime(self):\n        return self._configured_runtime\n\n    def get_shape_overrides(self):\n        \"\"\"Return a dict of values that this shape would like to override.\n\n        A shape definition takes precedence over the and may provide new values for\n        runtime, spawn_rate and users. Since this is the base implementation, we don't\n        have an overrides to provide.\n\n        \"\"\"\n        return {}\n\n    def tick(self):\n        \"\"\"Tell locust about the new values for users and spawn rate.\n\n        Called by locust about 1x/second, allowing the shape to adjust the values over\n        time or terminate the test.\n\n        Per the locust documentation, return None to terminate the test. When using\n        locust as a library, you must also set a different greenlet to send the quit\n        message when runtime has elapsed to (see the launch method).\n        \"\"\"\n        return self.users, self.spawn_rate", "class Default(LoadTestShape):\n    \"\"\"Base shape for our all Load test shapes.\n\n    Replicate passing only runtime, spawn rate, users on the command line.\n    Also serves as the Base for the rest of our shapes, so that the launch method knows\n    how to instantiate. Note that all shapes now take in additional args in the\n    constructor, they are not obligated to do anything with them.\n    \"\"\"\n\n    DEFAULT_RUNTIME = 120\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.users = kwargs.get(\"users\") or 1\n        self.spawn_rate = kwargs.get(\"spawn_rate\") or 1\n        self.runtime = kwargs.get(\"runtime\") or self.DEFAULT_RUNTIME\n        self._configured_runtime = self.runtime\n\n    @property\n    def configured_runtime(self):\n        return self._configured_runtime\n\n    def get_shape_overrides(self):\n        \"\"\"Return a dict of values that this shape would like to override.\n\n        A shape definition takes precedence over the and may provide new values for\n        runtime, spawn_rate and users. Since this is the base implementation, we don't\n        have an overrides to provide.\n\n        \"\"\"\n        return {}\n\n    def tick(self):\n        \"\"\"Tell locust about the new values for users and spawn rate.\n\n        Called by locust about 1x/second, allowing the shape to adjust the values over\n        time or terminate the test.\n\n        Per the locust documentation, return None to terminate the test. When using\n        locust as a library, you must also set a different greenlet to send the quit\n        message when runtime has elapsed to (see the launch method).\n        \"\"\"\n        return self.users, self.spawn_rate", "\n\nclass Smoke(Default):\n    \"\"\"Shape to run for smoke tests, use set values.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(runtime=60, users=1, spawn_rate=1)\n\n    def get_shape_overrides(self):\n        \"\"\"Return overrides from this shape.\n\n        Smoke is a shape that has hardcoded values, so we want to override whatever the\n        user might passed for the shape related values. Shape name always takes\n        precedence over the runtime, spawn_rate and users values.\n\n        \"\"\"\n        return {\"runtime\": 60, \"users\": 1, \"spawn_rate\": 1}", "\n\nclass Testingfixturesonly(Default):\n    \"\"\"Shape for testing fixtures, DO NOT USE!!.\"\"\"\n\n    testing_values = {\n        \"runtime\": 10,\n        \"users\": 20,\n        \"spawn_rate\": 0.004,\n        \"key_that_should_not_override\": \"some value\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(**self.testing_values)\n\n    def get_shape_overrides(self):\n        \"\"\"Return overrides from this shape.\n\n        Smoke is a shape that has hardcoded values, so we want to override whatever the\n        user might have passed for the shape related values. Shape always takes\n        precedence over the runtime, spawn_rate and users values.\n\n        \"\"\"\n        return self.testing_values", "\n\nclass Trend(Default):\n    \"\"\"Convenience shape with our standard trend parameters.\"\"\"\n\n    # for the trend shape, override the default to 10m\n    DEFAULT_RUNTIME = 600\n    USERS = 10\n    SPAWN_RATE = 0.1\n\n    def __init__(self, *args, **kwargs):\n        kwargs[\"users\"] = self.USERS\n        kwargs[\"spawn_rate\"] = self.SPAWN_RATE\n        # don't specify runtime here because mostly we want it\n        # to pick up the runtime from scenario yaml\n        super().__init__(*args, **kwargs)\n\n    def get_shape_overrides(self):\n        \"\"\"Return a dict of values that this shape would like to override.\"\"\"\n        return {\"users\": self.USERS, \"spawn_rate\": self.SPAWN_RATE}", "\n\nclass Stages(Default):  # noqa E501\n    \"\"\"\n    Stolen from this set of examples as part of the locust.io documentation.\n    https://github.com/locustio/locust/blob/master/examples/custom_shape/stages.py\n\n    Keyword arguments:\n        stages -- A list of dicts, each representing a stage with the following keys:\n            duration -- When this many seconds pass the test is advanced to the next\n            stage.\n            users -- Total user count\n            spawn_rate -- Number of users to start/stop per second\n            stop -- A boolean that can stop that test at a specific stage\n        stop_at_end -- Can be set to stop once all stages have run.\n\n    Most likely, you'd want to extend this class and only define a new stages attr.\n    \"\"\"\n\n    stages = [\n        {\"duration\": 60, \"users\": 1, \"spawn_rate\": 1},\n        {\"duration\": 120, \"users\": 2, \"spawn_rate\": 2},\n        {\"duration\": 240, \"users\": 3, \"spawn_rate\": 3},\n        {\"duration\": 300, \"users\": 4, \"spawn_rate\": 4},\n        {\"duration\": 360, \"users\": 3, \"spawn_rate\": 1},\n        {\"duration\": 420, \"users\": 1, \"spawn_rate\": 1, \"stop\": True},\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._configured_runtime = self.stages[-1].get(\"duration\", 0)\n\n    def tick(self):\n        \"\"\"Tell locust about the new values for users and spawn rate.\"\"\"\n        run_time = self.get_run_time()\n\n        logger.debug(f\"Tick: runtime=[{run_time}]\")\n\n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                tick_data = (stage[\"users\"], stage[\"spawn_rate\"])\n                return tick_data\n\n        return self.stages[-1][\"users\"], self.stages[-1][\"spawn_rate\"]", "\n\nclass Spike(Stages):\n    \"\"\"A spike shape that adds many users quickly, then cools down after.\n\n    Takes the following parameters from the command line\n    users = target number of users for spike stage\n    spawn_rate = how quickly to add the users during the spike stage\n    runtime = how long to make the *spike* stage, not that the cooldown stage gets\n    added on so the total length of the shape is runtime + cooldown_duration\n    cooldown_duration = how much time to take to get back to 1 user (uses same spawn\n    rate as the spike for now); if omitted, will use the runtime\n\n    \"\"\"\n\n    stages = [{\"duration\": 10, \"users\": 1, \"spawn_rate\": 1}]  # effectively a no-op\n\n    def __init__(self, *args, **kwargs):\n        users = kwargs.get(\"users\") or 1\n        spike_duration = kwargs.get(\"runtime\") or 600\n        spawn_rate = kwargs.get(\"spawn_rate\") or users / spike_duration\n        # checking kwargs still allows someone to create another shape that extends the\n        # this spike shape but with a different cooldown. if you use this shape\n        # directly, you will always get a 10m cooldown period\n        cooldown_duration = kwargs.get(\"cooldown_duration\") or 600\n        cooldown_spawn_rate = users / cooldown_duration\n        self.stages = [\n            {\"duration\": spike_duration, \"users\": users, \"spawn_rate\": spawn_rate},\n            {\n                \"duration\": spike_duration + cooldown_duration,\n                \"users\": 1,\n                \"spawn_rate\": cooldown_spawn_rate,\n            },\n        ]\n        super().__init__(*args, **kwargs)", "\nclass Customstages(Stages):  # noqa E501\n\n    \"\"\"Keyword arguments:\n        stages -- Takes keyword argument that is a json string\"\"\"\n\n    stages = [{\"duration\": 68, \"users\": 4, \"spawn_rate\": 0.5},\n              {\"duration\": 130, \"users\": 2, \"spawn_rate\": 1},\n              {\"duration\": 202, \"users\": 6, \"spawn_rate\": 0.5},\n              {\"duration\": 268, \"users\": 3, \"spawn_rate\": 1},\n              {\"duration\": 344, \"users\": 8, \"spawn_rate\": 0.5},\n              {\"duration\": 408, \"users\": 4, \"spawn_rate\": 1},\n              {\"duration\": 420, \"users\": 0, \"spawn_rate\": 1, \"stop\": True},\n              ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        try:\n            stages = json.loads(kwargs.get(\"stages\"))\n            self.stages = stages\n        except TypeError:\n            pass\n        self._configured_runtime = self.stages[-1].get(\"duration\", 0)", "\n"]}
{"filename": "src/grasshopper/lib/util/launch.py", "chunked_list": ["\"\"\"Module: Launch.\n\nThese are the miscellaneous launch functions for the project that couldn't find\nanother place to live.\n\"\"\"\nimport logging\n\nfrom locust.env import Environment\n\nfrom grasshopper.lib.grasshopper import Grasshopper", "\nfrom grasshopper.lib.grasshopper import Grasshopper\nfrom grasshopper.lib.util.decorators import deprecate\n\nlogger = logging.getLogger()\n\n\n@deprecate(\"launch_locust_test\", \"Grasshopper.launch_test\")\ndef launch_locust_test(user_classes, **kwargs) -> Environment:\n    \"\"\"Launch a locust test, deprecated method.\"\"\"\n    return Grasshopper.launch_test(user_classes, **kwargs)", "def launch_locust_test(user_classes, **kwargs) -> Environment:\n    \"\"\"Launch a locust test, deprecated method.\"\"\"\n    return Grasshopper.launch_test(user_classes, **kwargs)\n\n\n@deprecate(\"get_shape_instance\", \"Grasshopper.load_shape\")\ndef get_shape_instance(shape_name, **kwargs):\n    \"\"\"Get a shape instance, deprecated method.\"\"\"\n    return Grasshopper.load_shape(shape_name, **kwargs)\n", ""]}
{"filename": "src/grasshopper/lib/journeys/base_journey.py", "chunked_list": ["\"\"\"Module: BaseJourney.\n\nClass to hold all the common functionality that we added on top of Locust's HttpUser\nclass.\n\"\"\"\nimport logging\nimport signal\nfrom collections import abc\nfrom uuid import uuid4\n", "from uuid import uuid4\n\nimport gevent\nfrom locust import HttpUser\n\nimport grasshopper.lib.util.listeners  # noqa: F401\nfrom grasshopper.lib.fixtures.grasshopper_constants import GrasshopperConstants\n\n\nclass BaseJourney(HttpUser):\n    \"\"\"The base journey class for all other journey classes.\"\"\"\n\n    VUS_DICT = {}\n    host = \"\"\n    _incoming_test_parameters = {}\n    abstract = True\n    base_torn_down = False\n    defaults = {\"tags\": {}, \"thresholds\": {}}\n\n    @classmethod\n    @property\n    def incoming_scenario_args(cls):\n        return cls._incoming_test_parameters\n\n    @property\n    def scenario_args(self):\n        return self._test_parameters\n\n    @classmethod\n    def replace_incoming_scenario_args(cls, brand_new_args={}):\n        \"\"\"Replace the existing set of scenario_args with a new collection.\"\"\"\n        cls._incoming_test_parameters = brand_new_args\n\n    @classmethod\n    def update_incoming_scenario_args(cls, higher_precedence_args):\n        \"\"\"Add more values to scenario_args with higher precedence.\"\"\"\n        cls._incoming_test_parameters.update(higher_precedence_args)\n\n    @classmethod\n    def merge_incoming_scenario_args(cls, lower_precedence_args):\n        \"\"\"Add more values to scenario_args with lower precedence.\"\"\"\n        new_args = lower_precedence_args.copy()\n        new_args.update(cls._incoming_test_parameters)\n        cls.replace_incoming_scenario_args(new_args)\n\n    @classmethod\n    def reset_class_attributes(cls):\n        \"\"\"Reset the class level attributes to their starting state.\n\n        We are using class level attributes because most of the time we want the state\n        to be shared across all the instances of the class, since these values *should*\n        always be the same across the instances and performance should be better without\n        storing a copy for every instance.\n\n        But in order to support unit testing and possibly other scenarios, this method\n        is provided as a way to reset to the starting state.\n        \"\"\"\n        cls._incoming_test_parameters = {}\n        cls.defaults = {\"tags\": {}}\n        cls.host = \"\"\n        cls.abstract = True\n        cls.base_torn_down = False\n        BaseJourney.VUS_DICT = {}\n\n    @classmethod\n    def get_journey_object_given_vu_number(cls, vu_number):\n        \"\"\"Get the journey class if exists in the instance-level context, else None.\"\"\"\n        return cls.VUS_DICT.get(vu_number)\n\n    def on_start(self):\n        \"\"\"Initialize the journey, set tags, and then set the test parameters.\"\"\"\n        super().on_start()\n        self._merge_incoming_defaults_and_params()\n        self._test_parameters = self._incoming_test_parameters.copy()\n        self._set_base_teardown_listeners()\n        self.client_id = str(uuid4())\n        self._register_new_vu()\n        self._set_thresholds()\n        self.environment.host = self.scenario_args.get(\"target_url\", \"\") or self.host\n\n        # TODO: currently global iterations is stored in the environment stats object\n        # TODO: poke around to see if we can move it to a class attribute here\n        self.vu_iteration = 0\n\n    def _register_new_vu(self):\n        \"\"\"Increment the user count and return the new vu's number.\"\"\"\n        self.vu_number = len(BaseJourney.VUS_DICT) + 1\n        BaseJourney.VUS_DICT[self.vu_number] = self\n\n    def _set_base_teardown_listeners(self):\n        gevent.signal_handler(signal.SIGINT, self.teardown)  # ctrl-c teardown bind\n\n    def _merge_incoming_defaults_and_params(self):\n        self.merge_incoming_scenario_args(self.defaults)\n\n    def _set_thresholds(self):\n        self.environment.stats.trends = {}\n\n        # If parameters are not passed in that need to be set, set them to the defaults\n        self._check_for_threshold_parameters_and_set_thresholds(\n            scenario_args=self.scenario_args\n        )\n\n    def _check_for_threshold_parameters_and_set_thresholds(self, scenario_args):\n        thresholds_collection = scenario_args.get(\"thresholds\")\n        if thresholds_collection is None:\n            return\n        elif self._verify_thresholds_collection_shape(thresholds_collection):\n            for trend_name, threshold_values in thresholds_collection.items():\n                trend_name = str(trend_name)\n                thresh_object = {\n                    \"less_than_in_ms\": int(threshold_values.get(\"limit\")),\n                    \"actual_value_in_ms\": None,\n                    \"percentile\": float(\n                        threshold_values.get(\n                            \"percentile\",\n                            GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                        )\n                    ),\n                    \"succeeded\": None,\n                    \"http_method\": str(threshold_values.get(\"type\")).upper(),\n                }\n                if trend_name in self.environment.stats.trends:\n                    self.environment.stats.trends[trend_name][\"thresholds\"].append(\n                        thresh_object\n                    )\n                else:\n                    self.environment.stats.trends[trend_name] = {\n                        \"tags\": self.scenario_args.get(\"tags\", {}),\n                        \"thresholds\": [thresh_object],\n                    }\n        else:\n            logging.warning(\n                \"Skipping registering thresholds due to invalid \" \"thresholds shape...\"\n            )\n\n    @staticmethod\n    def _verify_thresholds_collection_shape(thresholds_collection):\n        valid_types = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"HEAD\", \"PATCH\", \"CUSTOM\"]\n        if not isinstance(thresholds_collection, abc.Mapping):\n            logging.warning(\n                f\"Thresholds object is of type {type(thresholds_collection)} \"\n                f\"but must be a mapping!\"\n            )\n            return False\n        for trend_name, threshold_values in thresholds_collection.items():\n            if (\n                threshold_values.get(\"type\") is None\n                or threshold_values.get(\"limit\") is None\n            ):\n                logging.warning(\n                    f\"Singular threshold object `{trend_name}` must have `type`\"\n                    f\"and `limit` fields defined.\"\n                )\n                return False\n            elif str(threshold_values.get(\"type\")).upper() not in valid_types:\n                logging.warning(\n                    f\"For threshold object {trend_name}, type \"\n                    f\"`{str(threshold_values.get('type')).upper()}` is \"\n                    f\"invalid. Must be one of {valid_types}.\"\n                )\n                return False\n            elif not str(threshold_values.get(\"limit\")).isnumeric():\n                logging.warning(\n                    f\"For threshold object {trend_name}, threshold limit of \"\n                    f\"`{threshold_values.get('limit')}` is invalid. \"\n                    f\"Must be numeric.\"\n                )\n                return False\n        return True\n\n    def teardown(self, *args, **kwargs):\n        \"\"\"\n        Tear down the journey and quit.\n\n        The proper way to do your own tear down when extending this class is like so:\n            def teardown(self, *args, **kwargs):\n                # do your teardown stuff here\n                super().teardown(*args, **kwargs)\n        \"\"\"\n        self.base_torn_down = True\n        stopped_vus = [vu.base_torn_down for vu in BaseJourney.VUS_DICT.values()]\n        for i in range(60):\n            if not all(stopped_vus):\n                gevent.sleep(1)\n            else:\n                break\n        BaseJourney.VUS_DICT = {}\n        self.environment.runner.quit()", "\nclass BaseJourney(HttpUser):\n    \"\"\"The base journey class for all other journey classes.\"\"\"\n\n    VUS_DICT = {}\n    host = \"\"\n    _incoming_test_parameters = {}\n    abstract = True\n    base_torn_down = False\n    defaults = {\"tags\": {}, \"thresholds\": {}}\n\n    @classmethod\n    @property\n    def incoming_scenario_args(cls):\n        return cls._incoming_test_parameters\n\n    @property\n    def scenario_args(self):\n        return self._test_parameters\n\n    @classmethod\n    def replace_incoming_scenario_args(cls, brand_new_args={}):\n        \"\"\"Replace the existing set of scenario_args with a new collection.\"\"\"\n        cls._incoming_test_parameters = brand_new_args\n\n    @classmethod\n    def update_incoming_scenario_args(cls, higher_precedence_args):\n        \"\"\"Add more values to scenario_args with higher precedence.\"\"\"\n        cls._incoming_test_parameters.update(higher_precedence_args)\n\n    @classmethod\n    def merge_incoming_scenario_args(cls, lower_precedence_args):\n        \"\"\"Add more values to scenario_args with lower precedence.\"\"\"\n        new_args = lower_precedence_args.copy()\n        new_args.update(cls._incoming_test_parameters)\n        cls.replace_incoming_scenario_args(new_args)\n\n    @classmethod\n    def reset_class_attributes(cls):\n        \"\"\"Reset the class level attributes to their starting state.\n\n        We are using class level attributes because most of the time we want the state\n        to be shared across all the instances of the class, since these values *should*\n        always be the same across the instances and performance should be better without\n        storing a copy for every instance.\n\n        But in order to support unit testing and possibly other scenarios, this method\n        is provided as a way to reset to the starting state.\n        \"\"\"\n        cls._incoming_test_parameters = {}\n        cls.defaults = {\"tags\": {}}\n        cls.host = \"\"\n        cls.abstract = True\n        cls.base_torn_down = False\n        BaseJourney.VUS_DICT = {}\n\n    @classmethod\n    def get_journey_object_given_vu_number(cls, vu_number):\n        \"\"\"Get the journey class if exists in the instance-level context, else None.\"\"\"\n        return cls.VUS_DICT.get(vu_number)\n\n    def on_start(self):\n        \"\"\"Initialize the journey, set tags, and then set the test parameters.\"\"\"\n        super().on_start()\n        self._merge_incoming_defaults_and_params()\n        self._test_parameters = self._incoming_test_parameters.copy()\n        self._set_base_teardown_listeners()\n        self.client_id = str(uuid4())\n        self._register_new_vu()\n        self._set_thresholds()\n        self.environment.host = self.scenario_args.get(\"target_url\", \"\") or self.host\n\n        # TODO: currently global iterations is stored in the environment stats object\n        # TODO: poke around to see if we can move it to a class attribute here\n        self.vu_iteration = 0\n\n    def _register_new_vu(self):\n        \"\"\"Increment the user count and return the new vu's number.\"\"\"\n        self.vu_number = len(BaseJourney.VUS_DICT) + 1\n        BaseJourney.VUS_DICT[self.vu_number] = self\n\n    def _set_base_teardown_listeners(self):\n        gevent.signal_handler(signal.SIGINT, self.teardown)  # ctrl-c teardown bind\n\n    def _merge_incoming_defaults_and_params(self):\n        self.merge_incoming_scenario_args(self.defaults)\n\n    def _set_thresholds(self):\n        self.environment.stats.trends = {}\n\n        # If parameters are not passed in that need to be set, set them to the defaults\n        self._check_for_threshold_parameters_and_set_thresholds(\n            scenario_args=self.scenario_args\n        )\n\n    def _check_for_threshold_parameters_and_set_thresholds(self, scenario_args):\n        thresholds_collection = scenario_args.get(\"thresholds\")\n        if thresholds_collection is None:\n            return\n        elif self._verify_thresholds_collection_shape(thresholds_collection):\n            for trend_name, threshold_values in thresholds_collection.items():\n                trend_name = str(trend_name)\n                thresh_object = {\n                    \"less_than_in_ms\": int(threshold_values.get(\"limit\")),\n                    \"actual_value_in_ms\": None,\n                    \"percentile\": float(\n                        threshold_values.get(\n                            \"percentile\",\n                            GrasshopperConstants.THRESHOLD_PERCENTILE_DEFAULT,\n                        )\n                    ),\n                    \"succeeded\": None,\n                    \"http_method\": str(threshold_values.get(\"type\")).upper(),\n                }\n                if trend_name in self.environment.stats.trends:\n                    self.environment.stats.trends[trend_name][\"thresholds\"].append(\n                        thresh_object\n                    )\n                else:\n                    self.environment.stats.trends[trend_name] = {\n                        \"tags\": self.scenario_args.get(\"tags\", {}),\n                        \"thresholds\": [thresh_object],\n                    }\n        else:\n            logging.warning(\n                \"Skipping registering thresholds due to invalid \" \"thresholds shape...\"\n            )\n\n    @staticmethod\n    def _verify_thresholds_collection_shape(thresholds_collection):\n        valid_types = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"HEAD\", \"PATCH\", \"CUSTOM\"]\n        if not isinstance(thresholds_collection, abc.Mapping):\n            logging.warning(\n                f\"Thresholds object is of type {type(thresholds_collection)} \"\n                f\"but must be a mapping!\"\n            )\n            return False\n        for trend_name, threshold_values in thresholds_collection.items():\n            if (\n                threshold_values.get(\"type\") is None\n                or threshold_values.get(\"limit\") is None\n            ):\n                logging.warning(\n                    f\"Singular threshold object `{trend_name}` must have `type`\"\n                    f\"and `limit` fields defined.\"\n                )\n                return False\n            elif str(threshold_values.get(\"type\")).upper() not in valid_types:\n                logging.warning(\n                    f\"For threshold object {trend_name}, type \"\n                    f\"`{str(threshold_values.get('type')).upper()}` is \"\n                    f\"invalid. Must be one of {valid_types}.\"\n                )\n                return False\n            elif not str(threshold_values.get(\"limit\")).isnumeric():\n                logging.warning(\n                    f\"For threshold object {trend_name}, threshold limit of \"\n                    f\"`{threshold_values.get('limit')}` is invalid. \"\n                    f\"Must be numeric.\"\n                )\n                return False\n        return True\n\n    def teardown(self, *args, **kwargs):\n        \"\"\"\n        Tear down the journey and quit.\n\n        The proper way to do your own tear down when extending this class is like so:\n            def teardown(self, *args, **kwargs):\n                # do your teardown stuff here\n                super().teardown(*args, **kwargs)\n        \"\"\"\n        self.base_torn_down = True\n        stopped_vus = [vu.base_torn_down for vu in BaseJourney.VUS_DICT.values()]\n        for i in range(60):\n            if not all(stopped_vus):\n                gevent.sleep(1)\n            else:\n                break\n        BaseJourney.VUS_DICT = {}\n        self.environment.runner.quit()", ""]}
{"filename": "src/grasshopper/lib/journeys/__init__.py", "chunked_list": ["\"\"\"Module: Journeys.\n\nJourneys represent a particular set of user actions.\n\"\"\"\n"]}
{"filename": "src/grasshopper/lib/configuration/__init__.py", "chunked_list": ["\"\"\"Module: Configuration.\n\nClasses and constants for managing grasshopper configuration capabilities.\n\n\"\"\"\n"]}
{"filename": "src/grasshopper/lib/configuration/gh_configuration.py", "chunked_list": ["\"\"\"Module: GHConfiguration.\n\nCode to support loading grasshopper configuration values:\n+ Store information about the grasshopper configuration values\n+ Class to carry configuration values (subclass of dict)\n+ Code for managing the loading process\n\n\"\"\"\nimport json\nimport logging", "import json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef typecast_dict(value):\n    \"\"\"Ensure that value is a dict (supports json strings) or log a warning.\"\"\"\n    new_value = value\n    if type(value) == str:\n        new_value = json.loads(value)\n    elif type(value) != dict:\n        logger.warning(\n            f\"Configuration value [{value}] of type [{type(value)}] not able to be \"\n            f\"cast to dictionary.\"\n        )\n\n    return new_value", "\n\ndef typecast_int(value):\n    \"\"\"Ensure value is an int or raise an error.\"\"\"\n    new_value = int(value)\n    return new_value\n\n\ndef typecast_float(value):\n    \"\"\"Ensure value is a float or raise an error.\"\"\"\n    new_value = float(value)\n    return new_value", "def typecast_float(value):\n    \"\"\"Ensure value is a float or raise an error.\"\"\"\n    new_value = float(value)\n    return new_value\n\n\ndef typecast_bool(value):\n    \"\"\"Ensure value is a bool or raise an error.\"\"\"\n    if type(value) == str:\n        new_value = value.lower() in [\"true\"]\n    else:\n        new_value = bool(value)\n\n    return new_value", "\n\nclass ConfigurationConstants:\n    \"\"\"Class to hold the _definition_ of all the configuration values available.\n\n    Different categories for the attrs are mainly to make it easier to understand the\n    effect and use of the different values. And to keep it kinda organized.\n\n    \"\"\"\n\n    # Grasshopper attrs that are typically set per test location (e.g. test repo)\n    GRASSHOPPER_ATTRS = {\n        \"influx_host\": {\n            \"opts\": [\"--influx_host\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Influx host ip address.\",\n            },\n        },\n        \"influxdb\": {  # TODO-DEPRECATED, USE INFLUX_HOST INSTEAD\n            \"opts\": [\"--influxdb\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Legacy argument name for influx_host.\",\n            },\n        },\n        \"influx_port\": {\n            \"opts\": [\"--influx_port\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Influx port for specified host.\",\n            },\n        },\n        \"influx_user\": {\n            \"opts\": [\"--influx_user\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Username to connect to the influx host.\",\n            },\n        },\n        \"influx_pwd\": {\n            \"opts\": [\"--influx_pwd\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Password to connect to the influx host.\",\n            },\n        },\n        \"slack_webhook\": {\n            \"opts\": [\"--slack_webhook\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Url of specified slack channel webhook. NYI.\",\n            },\n        },\n        \"slack_report_failures_only\": {\n            \"opts\": [\"--slack_report_failures_only\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": bool,\n                \"help\": \"True to only post to slack for a scenario when there are \"\n                \"failures in either the thresholds or checks.\",\n            },\n            \"typecast\": typecast_bool,\n            \"default\": False,\n        },\n        \"rp_token\": {\n            \"opts\": [\"--rp_token\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"API token for accessing report portal server. NYI. \"\n                \"programmatically.\",\n            },\n        },\n        \"rp_project\": {\n            \"opts\": [\"--rp_project\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Project ID for report portal project. NYI.\",\n            },\n        },\n        \"rp_endpoint\": {\n            \"opts\": [\"--rp_endpoint\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"API endpoint for the report portal server. NYI.\",\n            },\n        },\n    }\n\n    # Testrun attrs are typically set per test run and may change every time you\n    # perform a different test run\n    TESTRUN_ATTRS = {\n        \"shape\": {\n            \"opts\": [\"-S\", \"--shape\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Name of a shape class to use, Default shape is used if \"\n                \"nothing is specified.\",\n            },\n            \"default\": \"Default\",\n        },\n        \"spawn_instance\": {\n            \"opts\": [\"--spawn_instance\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Shape instance, used only by the scenario collector.\",\n            },\n        },\n        \"users\": {\n            \"opts\": [\"-U\", \"--users\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": int,\n                \"help\": \"Target number of users.\",\n            },\n            \"default\": 1.0,\n            \"typecast\": typecast_int,\n        },\n        \"runtime\": {\n            \"opts\": [\"--runtime\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": float,\n                \"help\": \"Runtime per scenario in milliseconds.\",\n            },\n            \"default\": 120.0,\n            \"typecast\": typecast_float,\n        },\n        \"spawn_rate\": {\n            \"opts\": [\"-R\", \"--spawn_rate\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": float,\n                \"help\": \"Rate at which new users are added, as expressed in \"\n                \"users/second.\",\n            },\n            \"default\": 1.0,\n            \"typecast\": typecast_float,\n        },\n        \"scenario_file\": {\n            \"opts\": [\"--scenario_file\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Path to a scenario yaml file.\",\n            },\n        },\n        \"scenario_name\": {\n            \"opts\": [\"--scenario_name\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Name (key) for a single scenario in the specified yaml file.\",\n            },\n        },\n        \"tags\": {\n            \"opts\": [\"--tags\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Tags to use for collection of scenarios from the specified \"\n                \"yaml file.\",\n            },\n        },\n        \"scenario_delay\": {\n            \"opts\": [\"--scenario_delay\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": float,\n                \"help\": \"Delay between scenarios in seconds.\",\n            },\n            \"default\": 0.0,\n            \"typecast\": typecast_float,\n        },\n        \"cleanup_s3\": {\n            \"opts\": [\"--cleanup_s3\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": bool,\n                \"help\": \"True to execute the cleanup s3 fixture.\",\n            },\n            \"default\": True,\n            \"typecast\": typecast_bool,\n        },\n        \"slack\": {\n            \"opts\": [\"--slack\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"True to post to slack. NYI.\",\n            },\n            \"default\": False,\n            \"typecast\": typecast_bool,\n        },\n        \"influx\": {\n            \"opts\": [\"--influx\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": bool,\n                \"help\": \"True to post to specified influx host. NYI.\",\n            },\n            \"typecast\": typecast_bool,\n            \"default\": False,\n        },\n        \"report_portal\": {\n            \"opts\": [\"--report_portal\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": bool,\n                \"help\": \"True to post results to report portal. NYI\",\n            },\n            \"typecast\": typecast_bool,\n            \"default\": False,\n        },\n        \"rp_launch_name\": {\n            \"opts\": [\"--rp_launch_name\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"help\": \"Base launch name to use when posting to report portal. NYI\",\n            },\n            \"default\": \"Grasshopper Performance Test Run | Launch name unknown\",\n        },\n    }\n\n    # Scenario attrs are typically set per scenario and usually are loaded from the\n    # scenario definition in a yaml file; However we don't enumerate all the possible\n    # values because that would be very hard to maintain. listed here are just the ones\n    # that we need to pull from the yaml and do something with (e.g. typecast)\n    SCENARIO_ATTRS = {\n        \"thresholds\": {\n            \"opts\": [\"--thresholds\"],\n            \"attrs\": {\n                \"action\": \"store\",\n                \"type\": str,\n                \"help\": \"Thresholds\",\n            },\n            \"typecast\": typecast_dict,\n        }\n    }\n\n    COMPLETE_ATTRS = {**GRASSHOPPER_ATTRS, **TESTRUN_ATTRS, **SCENARIO_ATTRS}\n\n    SHAPE_OVERRIDE_ATTR_NAMES = [\"runtime\", \"spawn_rate\", \"users\"]", "\n\nclass GHConfiguration(dict):\n    \"\"\"Collection to hold Grasshopper configuration values.\n\n    This serves 2 purposes right now:\n    1. it lets us identify if a given dict is a _configuration_ dict\n    2. it is a place that we can add a few convenience operations on top of all the\n    dict operations it inherits (more coming soon...)\n\n    \"\"\"\n\n    def update_single_key(self, key, value):\n        \"\"\"Update a single key in the underlying dict, convenience method.\"\"\"\n        if value is not None:\n            self[key] = value", ""]}
