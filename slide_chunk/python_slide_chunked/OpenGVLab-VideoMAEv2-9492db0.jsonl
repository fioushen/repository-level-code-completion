{"filename": "engine_for_finetuning.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport math\nimport os\nimport sys", "import os\nimport sys\nfrom multiprocessing import Pool\nfrom typing import Iterable, Optional\n\nimport numpy as np\nimport torch\nfrom scipy.special import softmax\nfrom timm.data import Mixup\nfrom timm.utils import ModelEma, accuracy", "from timm.data import Mixup\nfrom timm.utils import ModelEma, accuracy\n\nimport utils\n\n\ndef train_class_batch(model, samples, target, criterion):\n    outputs = model(samples)\n    loss = criterion(outputs, target)\n    return loss, outputs", "\n\ndef get_loss_scale_for_deepspeed(model):\n    optimizer = model.optimizer\n    return optimizer.loss_scale if hasattr(\n        optimizer, \"loss_scale\") else optimizer.cur_scale\n\n\ndef train_one_epoch(model: torch.nn.Module,\n                    criterion: torch.nn.Module,\n                    data_loader: Iterable,\n                    optimizer: torch.optim.Optimizer,\n                    device: torch.device,\n                    epoch: int,\n                    loss_scaler,\n                    max_norm: float = 0,\n                    model_ema: Optional[ModelEma] = None,\n                    mixup_fn: Optional[Mixup] = None,\n                    log_writer=None,\n                    start_steps=None,\n                    lr_schedule_values=None,\n                    wd_schedule_values=None,\n                    num_training_steps_per_epoch=None,\n                    update_freq=None):\n    model.train(True)\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\n        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter(\n        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 20\n\n    if loss_scaler is None:\n        model.zero_grad()\n        model.micro_steps = 0\n    else:\n        optimizer.zero_grad()\n\n    for data_iter_step, (samples, targets, _, _) in enumerate(\n            metric_logger.log_every(data_loader, print_freq, header)):\n        step = data_iter_step // update_freq\n        if step >= num_training_steps_per_epoch:\n            continue\n        it = start_steps + step  # global training iteration\n        # Update LR & WD for the first acc\n        if lr_schedule_values is not None or wd_schedule_values is not None and data_iter_step % update_freq == 0:\n            for i, param_group in enumerate(optimizer.param_groups):\n                if lr_schedule_values is not None:\n                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n                        \"lr_scale\"]\n                if wd_schedule_values is not None and param_group[\n                        \"weight_decay\"] > 0:\n                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\n        samples = samples.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if mixup_fn is not None:\n            # mixup handle 3th & 4th dimension\n            B, C, T, H, W = samples.shape\n            samples = samples.view(B, C * T, H, W)\n            samples, targets = mixup_fn(samples, targets)\n            samples = samples.view(B, C, T, H, W)\n\n        if loss_scaler is None:\n            samples = samples.half()\n            loss, output = train_class_batch(model, samples, targets,\n                                             criterion)\n        else:\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n                loss, output = train_class_batch(model, samples, targets,\n                                                 criterion)\n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            sys.exit(1)\n\n        if loss_scaler is None:\n            loss /= update_freq\n            model.backward(loss)\n            grad_norm = model.get_global_grad_norm()\n\n            model.step()\n\n            if (data_iter_step + 1) % update_freq == 0:\n                # Deepspeed will call step() & model.zero_grad() automatic\n                if model_ema is not None:\n                    model_ema.update(model)\n            loss_scale_value = get_loss_scale_for_deepspeed(model)\n        else:\n            # this attribute is added by timm on one optimizer (adahessian)\n            is_second_order = hasattr(\n                optimizer, 'is_second_order') and optimizer.is_second_order\n            loss /= update_freq\n            grad_norm = loss_scaler(\n                loss,\n                optimizer,\n                clip_grad=max_norm,\n                parameters=model.parameters(),\n                create_graph=is_second_order,\n                update_grad=(data_iter_step + 1) % update_freq == 0)\n            if (data_iter_step + 1) % update_freq == 0:\n                optimizer.zero_grad()\n                if model_ema is not None:\n                    model_ema.update(model)\n            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        if mixup_fn is None:\n            class_acc = (output.max(-1)[-1] == targets).float().mean()\n        else:\n            class_acc = None\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(class_acc=class_acc)\n        metric_logger.update(loss_scale=loss_scale_value)\n        min_lr = 10.\n        max_lr = 0.\n        for group in optimizer.param_groups:\n            min_lr = min(min_lr, group[\"lr\"])\n            max_lr = max(max_lr, group[\"lr\"])\n\n        metric_logger.update(lr=max_lr)\n        metric_logger.update(min_lr=min_lr)\n        weight_decay_value = None\n        for group in optimizer.param_groups:\n            if group[\"weight_decay\"] > 0:\n                weight_decay_value = group[\"weight_decay\"]\n        metric_logger.update(weight_decay=weight_decay_value)\n        metric_logger.update(grad_norm=grad_norm)\n\n        if log_writer is not None:\n            log_writer.update(loss=loss_value, head=\"loss\")\n            log_writer.update(class_acc=class_acc, head=\"loss\")\n            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n            log_writer.update(lr=max_lr, head=\"opt\")\n            log_writer.update(min_lr=min_lr, head=\"opt\")\n            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n\n            log_writer.set_step()\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", "def train_one_epoch(model: torch.nn.Module,\n                    criterion: torch.nn.Module,\n                    data_loader: Iterable,\n                    optimizer: torch.optim.Optimizer,\n                    device: torch.device,\n                    epoch: int,\n                    loss_scaler,\n                    max_norm: float = 0,\n                    model_ema: Optional[ModelEma] = None,\n                    mixup_fn: Optional[Mixup] = None,\n                    log_writer=None,\n                    start_steps=None,\n                    lr_schedule_values=None,\n                    wd_schedule_values=None,\n                    num_training_steps_per_epoch=None,\n                    update_freq=None):\n    model.train(True)\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\n        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter(\n        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 20\n\n    if loss_scaler is None:\n        model.zero_grad()\n        model.micro_steps = 0\n    else:\n        optimizer.zero_grad()\n\n    for data_iter_step, (samples, targets, _, _) in enumerate(\n            metric_logger.log_every(data_loader, print_freq, header)):\n        step = data_iter_step // update_freq\n        if step >= num_training_steps_per_epoch:\n            continue\n        it = start_steps + step  # global training iteration\n        # Update LR & WD for the first acc\n        if lr_schedule_values is not None or wd_schedule_values is not None and data_iter_step % update_freq == 0:\n            for i, param_group in enumerate(optimizer.param_groups):\n                if lr_schedule_values is not None:\n                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n                        \"lr_scale\"]\n                if wd_schedule_values is not None and param_group[\n                        \"weight_decay\"] > 0:\n                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\n        samples = samples.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if mixup_fn is not None:\n            # mixup handle 3th & 4th dimension\n            B, C, T, H, W = samples.shape\n            samples = samples.view(B, C * T, H, W)\n            samples, targets = mixup_fn(samples, targets)\n            samples = samples.view(B, C, T, H, W)\n\n        if loss_scaler is None:\n            samples = samples.half()\n            loss, output = train_class_batch(model, samples, targets,\n                                             criterion)\n        else:\n            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n                loss, output = train_class_batch(model, samples, targets,\n                                                 criterion)\n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            sys.exit(1)\n\n        if loss_scaler is None:\n            loss /= update_freq\n            model.backward(loss)\n            grad_norm = model.get_global_grad_norm()\n\n            model.step()\n\n            if (data_iter_step + 1) % update_freq == 0:\n                # Deepspeed will call step() & model.zero_grad() automatic\n                if model_ema is not None:\n                    model_ema.update(model)\n            loss_scale_value = get_loss_scale_for_deepspeed(model)\n        else:\n            # this attribute is added by timm on one optimizer (adahessian)\n            is_second_order = hasattr(\n                optimizer, 'is_second_order') and optimizer.is_second_order\n            loss /= update_freq\n            grad_norm = loss_scaler(\n                loss,\n                optimizer,\n                clip_grad=max_norm,\n                parameters=model.parameters(),\n                create_graph=is_second_order,\n                update_grad=(data_iter_step + 1) % update_freq == 0)\n            if (data_iter_step + 1) % update_freq == 0:\n                optimizer.zero_grad()\n                if model_ema is not None:\n                    model_ema.update(model)\n            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        if mixup_fn is None:\n            class_acc = (output.max(-1)[-1] == targets).float().mean()\n        else:\n            class_acc = None\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(class_acc=class_acc)\n        metric_logger.update(loss_scale=loss_scale_value)\n        min_lr = 10.\n        max_lr = 0.\n        for group in optimizer.param_groups:\n            min_lr = min(min_lr, group[\"lr\"])\n            max_lr = max(max_lr, group[\"lr\"])\n\n        metric_logger.update(lr=max_lr)\n        metric_logger.update(min_lr=min_lr)\n        weight_decay_value = None\n        for group in optimizer.param_groups:\n            if group[\"weight_decay\"] > 0:\n                weight_decay_value = group[\"weight_decay\"]\n        metric_logger.update(weight_decay=weight_decay_value)\n        metric_logger.update(grad_norm=grad_norm)\n\n        if log_writer is not None:\n            log_writer.update(loss=loss_value, head=\"loss\")\n            log_writer.update(class_acc=class_acc, head=\"loss\")\n            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n            log_writer.update(lr=max_lr, head=\"opt\")\n            log_writer.update(min_lr=min_lr, head=\"opt\")\n            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n\n            log_writer.set_step()\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", "\n\n@torch.no_grad()\ndef validation_one_epoch(data_loader, model, device):\n    criterion = torch.nn.CrossEntropyLoss()\n\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Val:'\n\n    # switch to evaluation mode\n    model.eval()\n\n    for batch in metric_logger.log_every(data_loader, 10, header):\n        images = batch[0]\n        target = batch[1]\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast():\n            output = model(images)\n            loss = criterion(output, target)\n\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        batch_size = images.shape[0]\n        metric_logger.update(loss=loss.item())\n        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\n        '* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n        .format(\n            top1=metric_logger.acc1,\n            top5=metric_logger.acc5,\n            losses=metric_logger.loss))\n\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", "\n\n@torch.no_grad()\ndef final_test(data_loader, model, device, file):\n    criterion = torch.nn.CrossEntropyLoss()\n\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    # switch to evaluation mode\n    model.eval()\n    final_result = []\n\n    for batch in metric_logger.log_every(data_loader, 10, header):\n        images = batch[0]\n        target = batch[1]\n        ids = batch[2]\n        chunk_nb = batch[3]\n        split_nb = batch[4]\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast():\n            output = model(images)\n            loss = criterion(output, target)\n\n        for i in range(output.size(0)):\n            string = \"{} {} {} {} {}\\n\".format(\n                ids[i], str(output.data[i].cpu().numpy().tolist()),\n                str(int(target[i].cpu().numpy())),\n                str(int(chunk_nb[i].cpu().numpy())),\n                str(int(split_nb[i].cpu().numpy())))\n            final_result.append(string)\n\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        batch_size = images.shape[0]\n        metric_logger.update(loss=loss.item())\n        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n\n    if not os.path.exists(file):\n        os.mknod(file)\n    with open(file, 'w') as f:\n        f.write(\"{}, {}\\n\".format(acc1, acc5))\n        for line in final_result:\n            f.write(line)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\n        '* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n        .format(\n            top1=metric_logger.acc1,\n            top5=metric_logger.acc5,\n            losses=metric_logger.loss))\n\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", "\n\ndef merge(eval_path, num_tasks, method='prob'):\n    assert method in ['prob', 'score']\n    dict_feats = {}\n    dict_label = {}\n    dict_pos = {}\n    print(\"Reading individual output files\")\n\n    for x in range(num_tasks):\n        file = os.path.join(eval_path, str(x) + '.txt')\n        lines = open(file, 'r').readlines()[1:]\n        for line in lines:\n            line = line.strip()\n            name = line.split('[')[0]\n            label = line.split(']')[1].split(' ')[1]\n            chunk_nb = line.split(']')[1].split(' ')[2]\n            split_nb = line.split(']')[1].split(' ')[3]\n            data = np.fromstring(\n                line.split('[')[1].split(']')[0], dtype=float, sep=',')\n            if name not in dict_feats:\n                dict_feats[name] = []\n                dict_label[name] = 0\n                dict_pos[name] = []\n            if chunk_nb + split_nb in dict_pos[name]:\n                continue\n            if method == 'prob':\n                dict_feats[name].append(softmax(data))\n            else:\n                dict_feats[name].append(data)\n            dict_pos[name].append(chunk_nb + split_nb)\n            dict_label[name] = label\n    print(\"Computing final results\")\n\n    input_lst = []\n    for i, item in enumerate(dict_feats):\n        input_lst.append([i, item, dict_feats[item], dict_label[item]])\n    p = Pool(64)\n    # [pred, top1, top5, label]\n    ans = p.map(compute_video, input_lst)\n    top1 = [x[1] for x in ans]\n    top5 = [x[2] for x in ans]\n    label = [x[3] for x in ans]\n    final_top1, final_top5 = np.mean(top1), np.mean(top5)\n\n    return final_top1 * 100, final_top5 * 100", "\n\ndef compute_video(lst):\n    i, video_id, data, label = lst\n    feat = [x for x in data]\n    feat = np.mean(feat, axis=0)\n    pred = np.argmax(feat)\n    top1 = (int(pred) == int(label)) * 1.0\n    top5 = (int(label) in np.argsort(-feat)[:5]) * 1.0\n    return [pred, top1, top5, int(label)]", ""]}
{"filename": "optim_factory.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport json\n\nimport torch", "\nimport torch\nfrom timm.optim.adafactor import Adafactor\nfrom timm.optim.adahessian import Adahessian\nfrom timm.optim.adamp import AdamP\nfrom timm.optim.lookahead import Lookahead\nfrom timm.optim.nadam import Nadam\nfrom timm.optim.novograd import NovoGrad\nfrom timm.optim.nvnovograd import NvNovoGrad\nfrom timm.optim.radam import RAdam", "from timm.optim.nvnovograd import NvNovoGrad\nfrom timm.optim.radam import RAdam\nfrom timm.optim.rmsprop_tf import RMSpropTF\nfrom timm.optim.sgdp import SGDP\nfrom torch import optim as optim\n\ntry:\n    from apex.optimizers import FusedAdam, FusedLAMB, FusedNovoGrad, FusedSGD\n    has_apex = True\nexcept ImportError:\n    has_apex = False", "\n\ndef get_num_layer_for_vit(var_name, num_max_layer):\n    if var_name in (\"cls_token\", \"mask_token\", \"pos_embed\"):\n        return 0\n    elif var_name.startswith(\"patch_embed\"):\n        return 0\n    elif var_name.startswith(\"rel_pos_bias\"):\n        return num_max_layer - 1\n    elif var_name.startswith(\"blocks\"):\n        layer_id = int(var_name.split('.')[1])\n        return layer_id + 1\n    else:\n        return num_max_layer - 1", "\n\nclass LayerDecayValueAssigner(object):\n\n    def __init__(self, values):\n        self.values = values\n\n    def get_scale(self, layer_id):\n        return self.values[layer_id]\n\n    def get_layer_id(self, var_name):\n        return get_num_layer_for_vit(var_name, len(self.values))", "\n\ndef get_parameter_groups(model,\n                         weight_decay=1e-5,\n                         skip_list=(),\n                         get_num_layer=None,\n                         get_layer_scale=None):\n    parameter_group_names = {}\n    parameter_group_vars = {}\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or name.endswith(\n                \".scale\") or name in skip_list:\n            group_name = \"no_decay\"\n            this_weight_decay = 0.\n        else:\n            group_name = \"decay\"\n            this_weight_decay = weight_decay\n        if get_num_layer is not None:\n            layer_id = get_num_layer(name)\n            group_name = \"layer_%d_%s\" % (layer_id, group_name)\n        else:\n            layer_id = None\n\n        if group_name not in parameter_group_names:\n            if get_layer_scale is not None:\n                scale = get_layer_scale(layer_id)\n            else:\n                scale = 1.\n\n            parameter_group_names[group_name] = {\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr_scale\": scale\n            }\n            parameter_group_vars[group_name] = {\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr_scale\": scale\n            }\n\n        parameter_group_vars[group_name][\"params\"].append(param)\n        parameter_group_names[group_name][\"params\"].append(name)\n    print(\"Param groups = %s\" % json.dumps(parameter_group_names, indent=2))\n    return list(parameter_group_vars.values())", "\n\ndef create_optimizer(args,\n                     model,\n                     get_num_layer=None,\n                     get_layer_scale=None,\n                     filter_bias_and_bn=True,\n                     skip_list=None):\n    opt_lower = args.opt.lower()\n    weight_decay = args.weight_decay\n    if weight_decay and filter_bias_and_bn:\n        skip = {}\n        if skip_list is not None:\n            skip = skip_list\n        elif hasattr(model, 'no_weight_decay'):\n            skip = model.no_weight_decay()\n        parameters = get_parameter_groups(model, weight_decay, skip,\n                                          get_num_layer, get_layer_scale)\n        weight_decay = 0.\n    else:\n        parameters = model.parameters()\n\n    if 'fused' in opt_lower:\n        assert has_apex and torch.cuda.is_available(\n        ), 'APEX and CUDA required for fused optimizers'\n\n    opt_args = dict(lr=args.lr, weight_decay=weight_decay)\n    if hasattr(args, 'opt_eps') and args.opt_eps is not None:\n        opt_args['eps'] = args.opt_eps\n    if hasattr(args, 'opt_betas') and args.opt_betas is not None:\n        opt_args['betas'] = args.opt_betas\n\n    print(\"optimizer settings:\", opt_args)\n\n    opt_split = opt_lower.split('_')\n    opt_lower = opt_split[-1]\n    if opt_lower == 'sgd' or opt_lower == 'nesterov':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(\n            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'momentum':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(\n            parameters, momentum=args.momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'adam':\n        optimizer = optim.Adam(parameters, **opt_args)\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, **opt_args)\n    elif opt_lower == 'nadam':\n        optimizer = Nadam(parameters, **opt_args)\n    elif opt_lower == 'radam':\n        optimizer = RAdam(parameters, **opt_args)\n    elif opt_lower == 'adamp':\n        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)\n    elif opt_lower == 'sgdp':\n        optimizer = SGDP(\n            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'adadelta':\n        optimizer = optim.Adadelta(parameters, **opt_args)\n    elif opt_lower == 'adafactor':\n        if not args.lr:\n            opt_args['lr'] = None\n        optimizer = Adafactor(parameters, **opt_args)\n    elif opt_lower == 'adahessian':\n        optimizer = Adahessian(parameters, **opt_args)\n    elif opt_lower == 'rmsprop':\n        optimizer = optim.RMSprop(\n            parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n    elif opt_lower == 'rmsproptf':\n        optimizer = RMSpropTF(\n            parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n    elif opt_lower == 'novograd':\n        optimizer = NovoGrad(parameters, **opt_args)\n    elif opt_lower == 'nvnovograd':\n        optimizer = NvNovoGrad(parameters, **opt_args)\n    elif opt_lower == 'fusedsgd':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(\n            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'fusedmomentum':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(\n            parameters, momentum=args.momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'fusedadam':\n        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)\n    elif opt_lower == 'fusedadamw':\n        optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)\n    elif opt_lower == 'fusedlamb':\n        optimizer = FusedLAMB(parameters, **opt_args)\n    elif opt_lower == 'fusednovograd':\n        opt_args.setdefault('betas', (0.95, 0.98))\n        optimizer = FusedNovoGrad(parameters, **opt_args)\n    else:\n        assert False and \"Invalid optimizer\"\n        raise ValueError\n\n    if len(opt_split) > 1:\n        if opt_split[0] == 'lookahead':\n            optimizer = Lookahead(optimizer)\n\n    return optimizer", ""]}
{"filename": "run_class_finetuning.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n\nimport argparse\nimport datetime", "import argparse\nimport datetime\nimport json\nimport os\nimport random\nimport time\nfrom collections import OrderedDict\nfrom functools import partial\nfrom pathlib import Path\n", "from pathlib import Path\n\nimport deepspeed\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom timm.data.mixup import Mixup\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.models import create_model\nfrom timm.utils import ModelEma", "from timm.models import create_model\nfrom timm.utils import ModelEma\n\n# NOTE: Do not comment `import models`, it is used to register models\nimport models  # noqa: F401\nimport utils\nfrom dataset import build_dataset\nfrom engine_for_finetuning import (\n    final_test,\n    merge,", "    final_test,\n    merge,\n    train_one_epoch,\n    validation_one_epoch,\n)\nfrom optim_factory import (\n    LayerDecayValueAssigner,\n    create_optimizer,\n    get_parameter_groups,\n)", "    get_parameter_groups,\n)\nfrom utils import NativeScalerWithGradNormCount as NativeScaler\nfrom utils import multiple_samples_collate\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        'VideoMAE fine-tuning and evaluation script for action classification',\n        add_help=False)\n    parser.add_argument('--batch_size', default=64, type=int)\n    parser.add_argument('--epochs', default=30, type=int)\n    parser.add_argument('--update_freq', default=1, type=int)\n    parser.add_argument('--save_ckpt_freq', default=100, type=int)\n\n    # Model parameters\n    parser.add_argument(\n        '--model',\n        default='vit_base_patch16_224',\n        type=str,\n        metavar='MODEL',\n        help='Name of model to train')\n    parser.add_argument('--tubelet_size', type=int, default=2)\n    parser.add_argument(\n        '--input_size', default=224, type=int, help='images input size')\n\n    parser.add_argument(\n        '--with_checkpoint', action='store_true', default=False)\n\n    parser.add_argument(\n        '--drop',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Dropout rate (default: 0.)')\n    parser.add_argument(\n        '--attn_drop_rate',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Attention dropout rate (default: 0.)')\n    parser.add_argument(\n        '--drop_path',\n        type=float,\n        default=0.1,\n        metavar='PCT',\n        help='Drop path rate (default: 0.1)')\n    parser.add_argument(\n        '--head_drop_rate',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='cls head dropout rate (default: 0.)')\n\n    parser.add_argument(\n        '--disable_eval_during_finetuning', action='store_true', default=False)\n\n    parser.add_argument('--model_ema', action='store_true', default=False)\n    parser.add_argument(\n        '--model_ema_decay', type=float, default=0.9999, help='')\n    parser.add_argument(\n        '--model_ema_force_cpu', action='store_true', default=False, help='')\n\n    # Optimizer parameters\n    parser.add_argument(\n        '--opt',\n        default='adamw',\n        type=str,\n        metavar='OPTIMIZER',\n        help='Optimizer (default: \"adamw\"')\n    parser.add_argument(\n        '--opt_eps',\n        default=1e-8,\n        type=float,\n        metavar='EPSILON',\n        help='Optimizer Epsilon (default: 1e-8)')\n    parser.add_argument(\n        '--opt_betas',\n        default=None,\n        type=float,\n        nargs='+',\n        metavar='BETA',\n        help='Optimizer Betas (default: None, use opt default)')\n    parser.add_argument(\n        '--clip_grad',\n        type=float,\n        default=None,\n        metavar='NORM',\n        help='Clip gradient norm (default: None, no clipping)')\n    parser.add_argument(\n        '--momentum',\n        type=float,\n        default=0.9,\n        metavar='M',\n        help='SGD momentum (default: 0.9)')\n    parser.add_argument(\n        '--weight_decay',\n        type=float,\n        default=0.05,\n        help='weight decay (default: 0.05)')\n    parser.add_argument(\n        '--weight_decay_end',\n        type=float,\n        default=None,\n        help=\"\"\"Final value of the\n        weight decay. We use a cosine schedule for WD and using a larger decay by\n        the end of training improves performance for ViTs.\"\"\")\n\n    parser.add_argument(\n        '--lr',\n        type=float,\n        default=1e-3,\n        metavar='LR',\n        help='learning rate (default: 1e-3)')\n    parser.add_argument('--layer_decay', type=float, default=0.75)\n\n    parser.add_argument(\n        '--warmup_lr',\n        type=float,\n        default=1e-8,\n        metavar='LR',\n        help='warmup learning rate (default: 1e-6)')\n    parser.add_argument(\n        '--min_lr',\n        type=float,\n        default=1e-6,\n        metavar='LR',\n        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\n    parser.add_argument(\n        '--warmup_epochs',\n        type=int,\n        default=5,\n        metavar='N',\n        help='epochs to warmup LR, if scheduler supports')\n    parser.add_argument(\n        '--warmup_steps',\n        type=int,\n        default=-1,\n        metavar='N',\n        help='num of steps to warmup LR, will overload warmup_epochs if set > 0'\n    )\n\n    # Augmentation parameters\n    parser.add_argument(\n        '--color_jitter',\n        type=float,\n        default=0.4,\n        metavar='PCT',\n        help='Color jitter factor (default: 0.4)')\n    parser.add_argument(\n        '--num_sample', type=int, default=2, help='Repeated_aug (default: 2)')\n    parser.add_argument(\n        '--aa',\n        type=str,\n        default='rand-m7-n4-mstd0.5-inc1',\n        metavar='NAME',\n        help=\n        'Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m7-n4-mstd0.5-inc1)'\n    ),\n    parser.add_argument(\n        '--smoothing',\n        type=float,\n        default=0.1,\n        help='Label smoothing (default: 0.1)')\n    parser.add_argument(\n        '--train_interpolation',\n        type=str,\n        default='bicubic',\n        help=\n        'Training interpolation (random, bilinear, bicubic default: \"bicubic\")'\n    )\n\n    # Evaluation parameters\n    parser.add_argument('--crop_pct', type=float, default=None)\n    parser.add_argument('--short_side_size', type=int, default=224)\n    parser.add_argument('--test_num_segment', type=int, default=10)\n    parser.add_argument('--test_num_crop', type=int, default=3)\n\n    # * Random Erase params\n    parser.add_argument(\n        '--reprob',\n        type=float,\n        default=0.25,\n        metavar='PCT',\n        help='Random erase prob (default: 0.25)')\n    parser.add_argument(\n        '--remode',\n        type=str,\n        default='pixel',\n        help='Random erase mode (default: \"pixel\")')\n    parser.add_argument(\n        '--recount',\n        type=int,\n        default=1,\n        help='Random erase count (default: 1)')\n    parser.add_argument(\n        '--resplit',\n        action='store_true',\n        default=False,\n        help='Do not random erase first (clean) augmentation split')\n\n    # * Mixup params\n    parser.add_argument(\n        '--mixup',\n        type=float,\n        default=0.8,\n        help='mixup alpha, mixup enabled if > 0.')\n    parser.add_argument(\n        '--cutmix',\n        type=float,\n        default=1.0,\n        help='cutmix alpha, cutmix enabled if > 0.')\n    parser.add_argument(\n        '--cutmix_minmax',\n        type=float,\n        nargs='+',\n        default=None,\n        help='cutmix min/max ratio, overrides alpha and enables cutmix if set')\n    parser.add_argument(\n        '--mixup_prob',\n        type=float,\n        default=1.0,\n        help=\n        'Probability of performing mixup or cutmix when either/both is enabled'\n    )\n    parser.add_argument(\n        '--mixup_switch_prob',\n        type=float,\n        default=0.5,\n        help=\n        'Probability of switching to cutmix when both mixup and cutmix enabled'\n    )\n    parser.add_argument(\n        '--mixup_mode',\n        type=str,\n        default='batch',\n        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"'\n    )\n\n    # * Finetuning params\n    parser.add_argument(\n        '--finetune', default='', help='finetune from checkpoint')\n    parser.add_argument('--model_key', default='model|module', type=str)\n    parser.add_argument('--model_prefix', default='', type=str)\n    parser.add_argument('--init_scale', default=0.001, type=float)\n    parser.add_argument('--use_mean_pooling', action='store_true')\n    parser.set_defaults(use_mean_pooling=True)\n    parser.add_argument(\n        '--use_cls', action='store_false', dest='use_mean_pooling')\n\n    # Dataset parameters\n    parser.add_argument(\n        '--data_path',\n        default='/your/data/path/',\n        type=str,\n        help='dataset path')\n    parser.add_argument(\n        '--data_root', default='', type=str, help='dataset path root')\n    parser.add_argument(\n        '--eval_data_path',\n        default=None,\n        type=str,\n        help='dataset path for evaluation')\n    parser.add_argument(\n        '--nb_classes',\n        default=400,\n        type=int,\n        help='number of the classification types')\n    parser.add_argument(\n        '--imagenet_default_mean_and_std', default=True, action='store_true')\n    parser.add_argument('--num_segments', type=int, default=1)\n    parser.add_argument('--num_frames', type=int, default=16)\n    parser.add_argument('--sampling_rate', type=int, default=4)\n    parser.add_argument('--sparse_sample', default=False, action='store_true')\n    parser.add_argument(\n        '--data_set',\n        default='Kinetics-400',\n        choices=[\n            'Kinetics-400', 'Kinetics-600', 'Kinetics-700', 'SSV2', 'UCF101',\n            'HMDB51', 'Diving48', 'Kinetics-710', 'MIT'\n        ],\n        type=str,\n        help='dataset')\n    parser.add_argument(\n        '--fname_tmpl',\n        default='img_{:05}.jpg',\n        type=str,\n        help='filename_tmpl for rawframe dataset')\n    parser.add_argument(\n        '--start_idx',\n        default=1,\n        type=int,\n        help='start_idx for rwaframe dataset')\n\n    parser.add_argument(\n        '--output_dir',\n        default='',\n        help='path where to save, empty for no saving')\n    parser.add_argument(\n        '--log_dir', default=None, help='path where to tensorboard log')\n    parser.add_argument(\n        '--device',\n        default='cuda',\n        help='device to use for training / testing')\n    parser.add_argument('--seed', default=0, type=int)\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\n    parser.add_argument('--auto_resume', action='store_true')\n    parser.add_argument(\n        '--no_auto_resume', action='store_false', dest='auto_resume')\n    parser.set_defaults(auto_resume=True)\n\n    parser.add_argument('--save_ckpt', action='store_true')\n    parser.add_argument(\n        '--no_save_ckpt', action='store_false', dest='save_ckpt')\n    parser.set_defaults(save_ckpt=True)\n\n    parser.add_argument(\n        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n    parser.add_argument(\n        '--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument(\n        '--validation', action='store_true', help='Perform validation only')\n    parser.add_argument(\n        '--dist_eval',\n        action='store_true',\n        default=False,\n        help='Enabling distributed evaluation')\n    parser.add_argument('--num_workers', default=10, type=int)\n    parser.add_argument(\n        '--pin_mem',\n        action='store_true',\n        help=\n        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'\n    )\n    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n    parser.set_defaults(pin_mem=True)\n\n    # distributed training parameters\n    parser.add_argument(\n        '--world_size',\n        default=1,\n        type=int,\n        help='number of distributed processes')\n    parser.add_argument('--local_rank', default=-1, type=int)\n    parser.add_argument('--dist_on_itp', action='store_true')\n    parser.add_argument(\n        '--dist_url',\n        default='env://',\n        help='url used to set up distributed training')\n\n    parser.add_argument(\n        '--enable_deepspeed', action='store_true', default=False)\n\n    known_args, _ = parser.parse_known_args()\n\n    if known_args.enable_deepspeed:\n        parser = deepspeed.add_config_arguments(parser)\n        ds_init = deepspeed.initialize\n    else:\n        ds_init = None\n\n    return parser.parse_args(), ds_init", "\n\ndef main(args, ds_init):\n    utils.init_distributed_mode(args)\n\n    if ds_init is not None:\n        utils.create_ds_config(args)\n\n    print(args)\n\n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    dataset_train, args.nb_classes = build_dataset(\n        is_train=True, test_mode=False, args=args)\n    if args.disable_eval_during_finetuning:\n        dataset_val = None\n    else:\n        dataset_val, _ = build_dataset(\n            is_train=False, test_mode=False, args=args)\n    dataset_test, _ = build_dataset(is_train=False, test_mode=True, args=args)\n\n    num_tasks = utils.get_world_size()\n    global_rank = utils.get_rank()\n    sampler_train = torch.utils.data.DistributedSampler(\n        dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True)\n    print(\"Sampler_train = %s\" % str(sampler_train))\n    if args.dist_eval:\n        if len(dataset_val) % num_tasks != 0:\n            print(\n                'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n                'This will slightly alter validation results as extra duplicate entries are added to achieve '\n                'equal num of samples per-process.')\n        sampler_val = torch.utils.data.DistributedSampler(\n            dataset_val,\n            num_replicas=num_tasks,\n            rank=global_rank,\n            shuffle=False)\n        sampler_test = torch.utils.data.DistributedSampler(\n            dataset_test,\n            num_replicas=num_tasks,\n            rank=global_rank,\n            shuffle=False)\n    else:\n        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\n    if global_rank == 0 and args.log_dir is not None:\n        os.makedirs(args.log_dir, exist_ok=True)\n        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n    else:\n        log_writer = None\n\n    if args.num_sample > 1:\n        collate_func = partial(multiple_samples_collate, fold=False)\n    else:\n        collate_func = None\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        sampler=sampler_train,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        pin_memory=args.pin_mem,\n        drop_last=True,\n        collate_fn=collate_func,\n        persistent_workers=True)\n\n    if dataset_val is not None:\n        data_loader_val = torch.utils.data.DataLoader(\n            dataset_val,\n            sampler=sampler_val,\n            batch_size=int(1.5 * args.batch_size),\n            num_workers=args.num_workers,\n            pin_memory=args.pin_mem,\n            drop_last=False,\n            persistent_workers=True)\n    else:\n        data_loader_val = None\n\n    if dataset_test is not None:\n        data_loader_test = torch.utils.data.DataLoader(\n            dataset_test,\n            sampler=sampler_test,\n            batch_size=args.batch_size,\n            num_workers=args.num_workers,\n            pin_memory=args.pin_mem,\n            drop_last=False,\n            persistent_workers=True)\n    else:\n        data_loader_test = None\n\n    mixup_fn = None\n    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n    if mixup_active:\n        print(\"Mixup is activated!\")\n        mixup_fn = Mixup(\n            mixup_alpha=args.mixup,\n            cutmix_alpha=args.cutmix,\n            cutmix_minmax=args.cutmix_minmax,\n            prob=args.mixup_prob,\n            switch_prob=args.mixup_switch_prob,\n            mode=args.mixup_mode,\n            label_smoothing=args.smoothing,\n            num_classes=args.nb_classes)\n\n    model = create_model(\n        args.model,\n        img_size=args.input_size,\n        pretrained=False,\n        num_classes=args.nb_classes,\n        all_frames=args.num_frames * args.num_segments,\n        tubelet_size=args.tubelet_size,\n        drop_rate=args.drop,\n        drop_path_rate=args.drop_path,\n        attn_drop_rate=args.attn_drop_rate,\n        head_drop_rate=args.head_drop_rate,\n        drop_block_rate=None,\n        use_mean_pooling=args.use_mean_pooling,\n        init_scale=args.init_scale,\n        with_cp=args.with_checkpoint,\n    )\n\n    patch_size = model.patch_embed.patch_size\n    print(\"Patch size = %s\" % str(patch_size))\n\n    args.window_size = (args.num_frames // args.tubelet_size,\n                        args.input_size // patch_size[0],\n                        args.input_size // patch_size[1])\n\n    args.patch_size = patch_size\n\n    if args.finetune:\n        if args.finetune.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.finetune, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.finetune, map_location='cpu')\n\n        print(\"Load ckpt from %s\" % args.finetune)\n        checkpoint_model = None\n        for model_key in args.model_key.split('|'):\n            if model_key in checkpoint:\n                checkpoint_model = checkpoint[model_key]\n                print(\"Load state_dict by model_key = %s\" % model_key)\n                break\n        if checkpoint_model is None:\n            checkpoint_model = checkpoint\n        for old_key in list(checkpoint_model.keys()):\n            if old_key.startswith('_orig_mod.'):\n                new_key = old_key[10:]\n                checkpoint_model[new_key] = checkpoint_model.pop(old_key)\n\n        state_dict = model.state_dict()\n        for k in ['head.weight', 'head.bias']:\n            if k in checkpoint_model and checkpoint_model[\n                    k].shape != state_dict[k].shape:\n                if checkpoint_model[k].shape[\n                        0] == 710 and args.data_set.startswith('Kinetics'):\n                    print(f'Convert K710 head to {args.data_set} head')\n                    if args.data_set == 'Kinetics-400':\n                        label_map_path = 'misc/label_710to400.json'\n                    elif args.data_set == 'Kinetics-600':\n                        label_map_path = 'misc/label_710to600.json'\n                    elif args.data_set == 'Kinetics-700':\n                        label_map_path = 'misc/label_710to700.json'\n\n                    label_map = json.load(open(label_map_path))\n                    checkpoint_model[k] = checkpoint_model[k][label_map]\n                else:\n                    print(f\"Removing key {k} from pretrained checkpoint\")\n                    del checkpoint_model[k]\n\n        all_keys = list(checkpoint_model.keys())\n        new_dict = OrderedDict()\n        for key in all_keys:\n            if key.startswith('backbone.'):\n                new_dict[key[9:]] = checkpoint_model[key]\n            elif key.startswith('encoder.'):\n                new_dict[key[8:]] = checkpoint_model[key]\n            else:\n                new_dict[key] = checkpoint_model[key]\n        checkpoint_model = new_dict\n\n        # interpolate position embedding\n        if 'pos_embed' in checkpoint_model:\n            pos_embed_checkpoint = checkpoint_model['pos_embed']\n            embedding_size = pos_embed_checkpoint.shape[-1]  # channel dim\n            num_patches = model.patch_embed.num_patches  #\n            num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # 0/1\n\n            # height (== width) for the checkpoint position embedding\n            orig_size = int(\n                ((pos_embed_checkpoint.shape[-2] - num_extra_tokens) //\n                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)\n            # height (== width) for the new position embedding\n            new_size = int(\n                (num_patches //\n                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)\n            # class_token and dist_token are kept unchanged\n            if orig_size != new_size:\n                print(\"Position interpolate from %dx%d to %dx%d\" %\n                      (orig_size, orig_size, new_size, new_size))\n                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n                # only the position tokens are interpolated\n                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n                # B, L, C -> BT, H, W, C -> BT, C, H, W\n                pos_tokens = pos_tokens.reshape(\n                    -1, args.num_frames // model.patch_embed.tubelet_size,\n                    orig_size, orig_size, embedding_size)\n                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size,\n                                                embedding_size).permute(\n                                                    0, 3, 1, 2)\n                pos_tokens = torch.nn.functional.interpolate(\n                    pos_tokens,\n                    size=(new_size, new_size),\n                    mode='bicubic',\n                    align_corners=False)\n                # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n                pos_tokens = pos_tokens.permute(0, 2, 3, 1).reshape(\n                    -1, args.num_frames // model.patch_embed.tubelet_size,\n                    new_size, new_size, embedding_size)\n                pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C\n                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n                checkpoint_model['pos_embed'] = new_pos_embed\n        elif args.input_size != 224:\n            pos_tokens = model.pos_embed\n            org_num_frames = 16\n            T = org_num_frames // args.tubelet_size\n            P = int((pos_tokens.shape[1] // T)**0.5)\n            C = pos_tokens.shape[2]\n            new_P = args.input_size // patch_size[0]\n            # B, L, C -> BT, H, W, C -> BT, C, H, W\n            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)\n            pos_tokens = pos_tokens.reshape(-1, P, P, C).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens,\n                size=(new_P, new_P),\n                mode='bicubic',\n                align_corners=False)\n            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n            pos_tokens = pos_tokens.permute(0, 2, 3,\n                                            1).reshape(-1, T, new_P, new_P, C)\n            pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C\n            model.pos_embed = pos_tokens  # update\n        if args.num_frames != 16:\n            org_num_frames = 16\n            T = org_num_frames // args.tubelet_size\n            pos_tokens = model.pos_embed\n            new_T = args.num_frames // args.tubelet_size\n            P = int((pos_tokens.shape[1] // T)**0.5)\n            C = pos_tokens.shape[2]\n            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 4,\n                                            1).reshape(-1, C, T)  # BHW,C,T\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=new_T, mode='linear')\n            pos_tokens = pos_tokens.reshape(1, P, P, C,\n                                            new_T).permute(0, 4, 1, 2, 3)\n            pos_tokens = pos_tokens.flatten(1, 3)\n            model.pos_embed = pos_tokens  # update\n\n        utils.load_state_dict(\n            model, checkpoint_model, prefix=args.model_prefix)\n\n    model.to(device)\n\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(\n            model,\n            decay=args.model_ema_decay,\n            device='cpu' if args.model_ema_force_cpu else '',\n            resume='')\n        print(\"Using EMA with decay = %.8f\" % args.model_ema_decay)\n\n    model_without_ddp = model\n    n_parameters = sum(p.numel() for p in model.parameters()\n                       if p.requires_grad)\n\n    print(\"Model = %s\" % str(model_without_ddp))\n    print('number of params:', n_parameters)\n\n    total_batch_size = args.batch_size * args.update_freq * num_tasks\n    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n    args.lr = args.lr * total_batch_size / 256\n    #########scale the lr#############\n    args.min_lr = args.min_lr * total_batch_size / 256\n    args.warmup_lr = args.warmup_lr * total_batch_size / 256\n    #########scale the lr#############\n    print(\"LR = %.8f\" % args.lr)\n    print(\"Batch size = %d\" % total_batch_size)\n    print(\"Update frequent = %d\" % args.update_freq)\n    print(\"Number of training examples = %d\" % len(dataset_train))\n    print(\"Number of training training per epoch = %d\" %\n          num_training_steps_per_epoch)\n\n    num_layers = model_without_ddp.get_num_layers()\n    if args.layer_decay < 1.0:\n        assigner = LayerDecayValueAssigner(\n            list(args.layer_decay**(num_layers + 1 - i)\n                 for i in range(num_layers + 2)))\n    else:\n        assigner = None\n\n    if assigner is not None:\n        print(\"Assigned values = %s\" % str(assigner.values))\n\n    skip_weight_decay_list = model.no_weight_decay()\n    print(\"Skip weight decay list: \", skip_weight_decay_list)\n\n    if args.enable_deepspeed:\n        loss_scaler = None\n        optimizer_params = get_parameter_groups(\n            model, args.weight_decay, skip_weight_decay_list,\n            assigner.get_layer_id if assigner is not None else None,\n            assigner.get_scale if assigner is not None else None)\n        model, optimizer, _, _ = ds_init(\n            args=args,\n            model=model,\n            model_parameters=optimizer_params,\n            dist_init_required=not args.distributed,\n        )\n\n        print(\"model.gradient_accumulation_steps() = %d\" %\n              model.gradient_accumulation_steps())\n        assert model.gradient_accumulation_steps() == args.update_freq\n    else:\n        if args.distributed:\n            model = torch.nn.parallel.DistributedDataParallel(\n                model, device_ids=[args.gpu], find_unused_parameters=False)\n            model_without_ddp = model.module\n\n        optimizer = create_optimizer(\n            args,\n            model_without_ddp,\n            skip_list=skip_weight_decay_list,\n            get_num_layer=assigner.get_layer_id\n            if assigner is not None else None,\n            get_layer_scale=assigner.get_scale\n            if assigner is not None else None)\n        loss_scaler = NativeScaler()\n\n    print(\"Use step level LR scheduler!\")\n    lr_schedule_values = utils.cosine_scheduler(\n        args.lr,\n        args.min_lr,\n        args.epochs,\n        num_training_steps_per_epoch,\n        warmup_epochs=args.warmup_epochs,\n        warmup_steps=args.warmup_steps,\n    )\n    if args.weight_decay_end is None:\n        args.weight_decay_end = args.weight_decay\n    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,\n                                                args.weight_decay_end,\n                                                args.epochs,\n                                                num_training_steps_per_epoch)\n    print(\"Max WD = %.7f, Min WD = %.7f\" %\n          (max(wd_schedule_values), min(wd_schedule_values)))\n\n    if mixup_fn is not None:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif args.smoothing > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    print(\"criterion = %s\" % str(criterion))\n\n    utils.auto_load_model(\n        args=args,\n        model=model,\n        model_without_ddp=model_without_ddp,\n        optimizer=optimizer,\n        loss_scaler=loss_scaler,\n        model_ema=model_ema)\n    if args.validation:\n        test_stats = validation_one_epoch(data_loader_val, model, device)\n        print(\n            f\"{len(dataset_val)} val images: Top-1 {test_stats['acc1']:.2f}%, Top-5 {test_stats['acc5']:.2f}%, loss {test_stats['loss']:.4f}\"\n        )\n        exit(0)\n\n    if args.eval:\n        preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')\n        test_stats = final_test(data_loader_test, model, device, preds_file)\n        torch.distributed.barrier()\n        if global_rank == 0:\n            print(\"Start merging results...\")\n            final_top1, final_top5 = merge(args.output_dir, num_tasks)\n            print(\n                f\"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%\"\n            )\n            log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}\n            if args.output_dir and utils.is_main_process():\n                with open(\n                        os.path.join(args.output_dir, \"log.txt\"),\n                        mode=\"a\",\n                        encoding=\"utf-8\") as f:\n                    f.write(json.dumps(log_stats) + \"\\n\")\n        exit(0)\n\n    print(f\"Start training for {args.epochs} epochs\")\n    start_time = time.time()\n    max_accuracy = 0.0\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            data_loader_train.sampler.set_epoch(epoch)\n        if log_writer is not None:\n            log_writer.set_step(epoch * num_training_steps_per_epoch *\n                                args.update_freq)\n        train_stats = train_one_epoch(\n            model,\n            criterion,\n            data_loader_train,\n            optimizer,\n            device,\n            epoch,\n            loss_scaler,\n            args.clip_grad,\n            model_ema,\n            mixup_fn,\n            log_writer=log_writer,\n            start_steps=epoch * num_training_steps_per_epoch,\n            lr_schedule_values=lr_schedule_values,\n            wd_schedule_values=wd_schedule_values,\n            num_training_steps_per_epoch=num_training_steps_per_epoch,\n            update_freq=args.update_freq,\n        )\n        if args.output_dir and args.save_ckpt:\n            _epoch = epoch + 1\n            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:\n                utils.save_model(\n                    args=args,\n                    model=model,\n                    model_without_ddp=model_without_ddp,\n                    optimizer=optimizer,\n                    loss_scaler=loss_scaler,\n                    epoch=epoch,\n                    model_ema=model_ema)\n        if data_loader_val is not None:\n            test_stats = validation_one_epoch(data_loader_val, model, device)\n            print(\n                f\"Accuracy of the network on the {len(dataset_val)} val images: {test_stats['acc1']:.2f}%\"\n            )\n            if max_accuracy < test_stats[\"acc1\"]:\n                max_accuracy = test_stats[\"acc1\"]\n                if args.output_dir and args.save_ckpt:\n                    utils.save_model(\n                        args=args,\n                        model=model,\n                        model_without_ddp=model_without_ddp,\n                        optimizer=optimizer,\n                        loss_scaler=loss_scaler,\n                        epoch=\"best\",\n                        model_ema=model_ema)\n\n            print(f'Max accuracy: {max_accuracy:.2f}%')\n            if log_writer is not None:\n                log_writer.update(\n                    val_acc1=test_stats['acc1'], head=\"perf\", step=epoch)\n                log_writer.update(\n                    val_acc5=test_stats['acc5'], head=\"perf\", step=epoch)\n                log_writer.update(\n                    val_loss=test_stats['loss'], head=\"perf\", step=epoch)\n\n            log_stats = {\n                **{f'train_{k}': v\n                   for k, v in train_stats.items()},\n                **{f'val_{k}': v\n                   for k, v in test_stats.items()}, 'epoch': epoch,\n                'n_parameters': n_parameters\n            }\n        else:\n            log_stats = {\n                **{f'train_{k}': v\n                   for k, v in train_stats.items()}, 'epoch': epoch,\n                'n_parameters': n_parameters\n            }\n        if args.output_dir and utils.is_main_process():\n            if log_writer is not None:\n                log_writer.flush()\n            with open(\n                    os.path.join(args.output_dir, \"log.txt\"),\n                    mode=\"a\",\n                    encoding=\"utf-8\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n    preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')\n    test_stats = final_test(data_loader_test, model, device, preds_file)\n    torch.distributed.barrier()\n\n    if global_rank == 0:\n        print(\"Start merging results...\")\n        final_top1, final_top5 = merge(args.output_dir, num_tasks)\n        print(\n            f\"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%\"\n        )\n        log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}\n        if args.output_dir and utils.is_main_process():\n            with open(\n                    os.path.join(args.output_dir, \"log.txt\"),\n                    mode=\"a\",\n                    encoding=\"utf-8\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str))", "\n\nif __name__ == '__main__':\n    opts, ds_init = get_args()\n    if opts.output_dir:\n        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)\n    main(opts, ds_init)\n"]}
{"filename": "engine_for_pretraining.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport math\nimport sys\nfrom typing import Iterable", "import sys\nfrom typing import Iterable\n\nimport torch\nfrom einops import rearrange\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nimport utils\n\n\ndef train_one_epoch(model: torch.nn.Module,\n                    data_loader: Iterable,\n                    optimizer: torch.optim.Optimizer,\n                    device: torch.device,\n                    epoch: int,\n                    loss_scaler,\n                    max_norm: float = 0,\n                    patch_size: int = 16,\n                    normlize_target: bool = True,\n                    log_writer=None,\n                    lr_scheduler=None,\n                    start_steps=None,\n                    lr_schedule_values=None,\n                    wd_schedule_values=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\n        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter(\n        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 20\n\n    for step, batch in enumerate(\n            metric_logger.log_every(data_loader, print_freq, header)):\n        # assign learning rate & weight decay for each step\n        it = start_steps + step  # global training iteration\n        if lr_schedule_values is not None or wd_schedule_values is not None:\n            for i, param_group in enumerate(optimizer.param_groups):\n                if lr_schedule_values is not None:\n                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n                        \"lr_scale\"]\n                if wd_schedule_values is not None and param_group[\n                        \"weight_decay\"] > 0:\n                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\n        # NOTE: When the decoder mask ratio is 0,\n        # in other words, when decoder masking is not used,\n        # decode_masked_pos = ~bool_masked_pos\n        images, bool_masked_pos, decode_masked_pos = batch\n\n        images = images.to(device, non_blocking=True)\n        bool_masked_pos = bool_masked_pos.to(\n            device, non_blocking=True).flatten(1).to(torch.bool)\n        decode_masked_pos = decode_masked_pos.to(\n            device, non_blocking=True).flatten(1).to(torch.bool)\n\n        with torch.no_grad():\n            # calculate the predict label\n            mean = torch.as_tensor(IMAGENET_DEFAULT_MEAN).to(device)[None, :,\n                                                                     None,\n                                                                     None,\n                                                                     None]\n            std = torch.as_tensor(IMAGENET_DEFAULT_STD).to(device)[None, :,\n                                                                   None, None,\n                                                                   None]\n            unnorm_images = images * std + mean  # in [0, 1]\n\n            if normlize_target:\n                images_squeeze = rearrange(\n                    unnorm_images,\n                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c',\n                    p0=2,\n                    p1=patch_size,\n                    p2=patch_size)\n                images_norm = (images_squeeze - images_squeeze.mean(\n                    dim=-2, keepdim=True)) / (\n                        images_squeeze.var(\n                            dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)\n                images_patch = rearrange(images_norm, 'b n p c -> b n (p c)')\n            else:\n                images_patch = rearrange(\n                    unnorm_images,\n                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)',\n                    p0=2,\n                    p1=patch_size,\n                    p2=patch_size)\n\n            B, N, C = images_patch.shape\n            labels = images_patch[~decode_masked_pos].reshape(B, -1, C)\n\n        if loss_scaler is None:\n            outputs = model(images, bool_masked_pos, decode_masked_pos)\n            loss = (outputs - labels)**2\n            loss = loss.mean(dim=-1)\n            cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(B, -1)\n            loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n        else:\n            with torch.cuda.amp.autocast():\n                outputs = model(images, bool_masked_pos, decode_masked_pos)\n                loss = (outputs - labels)**2\n                loss = loss.mean(dim=-1)\n                cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(\n                    B, -1)\n                loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            sys.exit(2)\n\n        optimizer.zero_grad()\n\n        if loss_scaler is None:\n            loss.backward()\n            if max_norm is None:\n                grad_norm = utils.get_grad_norm_(model.parameters())\n            else:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), max_norm)\n            optimizer.step()\n            loss_scale_value = 0\n        else:\n            # this attribute is added by timm on one optimizer (adahessian)\n            is_second_order = hasattr(\n                optimizer, 'is_second_order') and optimizer.is_second_order\n            grad_norm = loss_scaler(\n                loss,\n                optimizer,\n                clip_grad=max_norm,\n                parameters=model.parameters(),\n                create_graph=is_second_order)\n            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(loss_scale=loss_scale_value)\n        min_lr = 10.\n        max_lr = 0.\n        for group in optimizer.param_groups:\n            min_lr = min(min_lr, group[\"lr\"])\n            max_lr = max(max_lr, group[\"lr\"])\n\n        metric_logger.update(lr=max_lr)\n        metric_logger.update(min_lr=min_lr)\n        weight_decay_value = None\n        for group in optimizer.param_groups:\n            if group[\"weight_decay\"] > 0:\n                weight_decay_value = group[\"weight_decay\"]\n        metric_logger.update(weight_decay=weight_decay_value)\n        metric_logger.update(grad_norm=grad_norm)\n\n        if log_writer is not None:\n            log_writer.update(loss=loss_value, head=\"loss\")\n            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n            log_writer.update(lr=max_lr, head=\"opt\")\n            log_writer.update(min_lr=min_lr, head=\"opt\")\n            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n\n            log_writer.set_step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(start_steps + step)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", "\n\ndef train_one_epoch(model: torch.nn.Module,\n                    data_loader: Iterable,\n                    optimizer: torch.optim.Optimizer,\n                    device: torch.device,\n                    epoch: int,\n                    loss_scaler,\n                    max_norm: float = 0,\n                    patch_size: int = 16,\n                    normlize_target: bool = True,\n                    log_writer=None,\n                    lr_scheduler=None,\n                    start_steps=None,\n                    lr_schedule_values=None,\n                    wd_schedule_values=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\n        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter(\n        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 20\n\n    for step, batch in enumerate(\n            metric_logger.log_every(data_loader, print_freq, header)):\n        # assign learning rate & weight decay for each step\n        it = start_steps + step  # global training iteration\n        if lr_schedule_values is not None or wd_schedule_values is not None:\n            for i, param_group in enumerate(optimizer.param_groups):\n                if lr_schedule_values is not None:\n                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n                        \"lr_scale\"]\n                if wd_schedule_values is not None and param_group[\n                        \"weight_decay\"] > 0:\n                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\n        # NOTE: When the decoder mask ratio is 0,\n        # in other words, when decoder masking is not used,\n        # decode_masked_pos = ~bool_masked_pos\n        images, bool_masked_pos, decode_masked_pos = batch\n\n        images = images.to(device, non_blocking=True)\n        bool_masked_pos = bool_masked_pos.to(\n            device, non_blocking=True).flatten(1).to(torch.bool)\n        decode_masked_pos = decode_masked_pos.to(\n            device, non_blocking=True).flatten(1).to(torch.bool)\n\n        with torch.no_grad():\n            # calculate the predict label\n            mean = torch.as_tensor(IMAGENET_DEFAULT_MEAN).to(device)[None, :,\n                                                                     None,\n                                                                     None,\n                                                                     None]\n            std = torch.as_tensor(IMAGENET_DEFAULT_STD).to(device)[None, :,\n                                                                   None, None,\n                                                                   None]\n            unnorm_images = images * std + mean  # in [0, 1]\n\n            if normlize_target:\n                images_squeeze = rearrange(\n                    unnorm_images,\n                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c',\n                    p0=2,\n                    p1=patch_size,\n                    p2=patch_size)\n                images_norm = (images_squeeze - images_squeeze.mean(\n                    dim=-2, keepdim=True)) / (\n                        images_squeeze.var(\n                            dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)\n                images_patch = rearrange(images_norm, 'b n p c -> b n (p c)')\n            else:\n                images_patch = rearrange(\n                    unnorm_images,\n                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)',\n                    p0=2,\n                    p1=patch_size,\n                    p2=patch_size)\n\n            B, N, C = images_patch.shape\n            labels = images_patch[~decode_masked_pos].reshape(B, -1, C)\n\n        if loss_scaler is None:\n            outputs = model(images, bool_masked_pos, decode_masked_pos)\n            loss = (outputs - labels)**2\n            loss = loss.mean(dim=-1)\n            cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(B, -1)\n            loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n        else:\n            with torch.cuda.amp.autocast():\n                outputs = model(images, bool_masked_pos, decode_masked_pos)\n                loss = (outputs - labels)**2\n                loss = loss.mean(dim=-1)\n                cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(\n                    B, -1)\n                loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            sys.exit(2)\n\n        optimizer.zero_grad()\n\n        if loss_scaler is None:\n            loss.backward()\n            if max_norm is None:\n                grad_norm = utils.get_grad_norm_(model.parameters())\n            else:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), max_norm)\n            optimizer.step()\n            loss_scale_value = 0\n        else:\n            # this attribute is added by timm on one optimizer (adahessian)\n            is_second_order = hasattr(\n                optimizer, 'is_second_order') and optimizer.is_second_order\n            grad_norm = loss_scaler(\n                loss,\n                optimizer,\n                clip_grad=max_norm,\n                parameters=model.parameters(),\n                create_graph=is_second_order)\n            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\n        torch.cuda.synchronize()\n\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(loss_scale=loss_scale_value)\n        min_lr = 10.\n        max_lr = 0.\n        for group in optimizer.param_groups:\n            min_lr = min(min_lr, group[\"lr\"])\n            max_lr = max(max_lr, group[\"lr\"])\n\n        metric_logger.update(lr=max_lr)\n        metric_logger.update(min_lr=min_lr)\n        weight_decay_value = None\n        for group in optimizer.param_groups:\n            if group[\"weight_decay\"] > 0:\n                weight_decay_value = group[\"weight_decay\"]\n        metric_logger.update(weight_decay=weight_decay_value)\n        metric_logger.update(grad_norm=grad_norm)\n\n        if log_writer is not None:\n            log_writer.update(loss=loss_value, head=\"loss\")\n            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n            log_writer.update(lr=max_lr, head=\"opt\")\n            log_writer.update(min_lr=min_lr, head=\"opt\")\n            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n\n            log_writer.set_step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(start_steps + step)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}", ""]}
{"filename": "utils.py", "chunked_list": ["# --------------------------------------------------------\n# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport datetime\nimport io", "import datetime\nimport io\nimport json\nimport math\nimport os\nimport random\nimport subprocess\nimport time\nfrom collections import defaultdict, deque\nfrom pathlib import Path", "from collections import defaultdict, deque\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom tensorboardX import SummaryWriter\nfrom timm.utils import get_state_dict\nfrom torch import inf\nfrom torch.utils.data._utils.collate import default_collate", "from torch import inf\nfrom torch.utils.data._utils.collate import default_collate\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total],\n                         dtype=torch.float64,\n                         device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def min(self):\n        return min(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            min=self.min,\n            value=self.value)", "\n\nclass MetricLogger(object):\n\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\"{}: {}\".format(name, str(meter)))\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')\n        data_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        log_msg = [\n            header, '[{0' + space_fmt + '}/{1}]', 'eta: {eta}', '{meters}',\n            'time: {time}', 'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            log_msg.append('max mem: {memory:.0f}')\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time),\n                            memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))", "\n\nclass TensorboardLogger(object):\n\n    def __init__(self, log_dir):\n        self.writer = SummaryWriter(logdir=log_dir)\n        self.step = 0\n\n    def set_step(self, step=None):\n        if step is not None:\n            self.step = step\n        else:\n            self.step += 1\n\n    def update(self, head='scalar', step=None, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.writer.add_scalar(head + \"/\" + k, v,\n                                   self.step if step is None else step)\n\n    def flush(self):\n        self.writer.flush()", "\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\n\ndef _load_checkpoint_for_ema(model_ema, checkpoint):\n    \"\"\"\n    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n    \"\"\"\n    mem_file = io.BytesIO()\n    torch.save(checkpoint, mem_file)\n    mem_file.seek(0)\n    model_ema._load_checkpoint(mem_file)", "def _load_checkpoint_for_ema(model_ema, checkpoint):\n    \"\"\"\n    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n    \"\"\"\n    mem_file = io.BytesIO()\n    torch.save(checkpoint, mem_file)\n    mem_file.seek(0)\n    model_ema._load_checkpoint(mem_file)\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print", "\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print", "\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()", "\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()", "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)", "\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef init_distributed_mode(args):\n    if args.dist_on_itp:\n        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'],\n                                         os.environ['MASTER_PORT'])\n        os.environ['LOCAL_RANK'] = str(args.gpu)\n        os.environ['RANK'] = str(args.rank)\n        os.environ['WORLD_SIZE'] = str(args.world_size)\n        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = int(os.environ['SLURM_LOCALID'])\n        args.world_size = int(os.environ['SLURM_NTASKS'])\n        os.environ['RANK'] = str(args.rank)\n        os.environ['LOCAL_RANK'] = str(args.gpu)\n        os.environ['WORLD_SIZE'] = str(args.world_size)\n\n        node_list = os.environ['SLURM_NODELIST']\n        addr = subprocess.getoutput(\n            f'scontrol show hostname {node_list} | head -n1')\n        if 'MASTER_ADDR' not in os.environ:\n            os.environ['MASTER_ADDR'] = addr\n    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    else:\n        print('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n    print(\n        '| distributed init (rank {}): {}, gpu {}'.format(\n            args.rank, args.dist_url, args.gpu),\n        flush=True)\n    torch.distributed.init_process_group(\n        backend=args.dist_backend,\n        init_method=args.dist_url,\n        world_size=args.world_size,\n        rank=args.rank)\n    torch.cuda.empty_cache()\n    torch.distributed.barrier()\n    assert torch.distributed.is_initialized()\n    setup_for_distributed(args.rank == 0)", "\n\ndef load_state_dict(model,\n                    state_dict,\n                    prefix='',\n                    ignore_missing=\"relative_position_index\"):\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, '_metadata', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=''):\n        local_metadata = {} if metadata is None else metadata.get(\n            prefix[:-1], {})\n        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n                                     missing_keys, unexpected_keys, error_msgs)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(model, prefix=prefix)\n\n    warn_missing_keys = []\n    ignore_missing_keys = []\n    for key in missing_keys:\n        keep_flag = True\n        for ignore_key in ignore_missing.split('|'):\n            if ignore_key in key:\n                keep_flag = False\n                break\n        if keep_flag:\n            warn_missing_keys.append(key)\n        else:\n            ignore_missing_keys.append(key)\n\n    missing_keys = warn_missing_keys\n\n    if len(missing_keys) > 0:\n        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n            model.__class__.__name__, missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Weights from pretrained model not used in {}: {}\".format(\n            model.__class__.__name__, unexpected_keys))\n    if len(ignore_missing_keys) > 0:\n        print(\n            \"Ignored weights of {} not initialized from pretrained model: {}\".\n            format(model.__class__.__name__, ignore_missing_keys))\n    if len(error_msgs) > 0:\n        print('\\n'.join(error_msgs))", "\n\nclass NativeScalerWithGradNormCount:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(self,\n                 loss,\n                 optimizer,\n                 clip_grad=None,\n                 parameters=None,\n                 create_graph=False,\n                 update_grad=True):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None:\n                assert parameters is not None\n                self._scaler.unscale_(\n                    optimizer\n                )  # unscale the gradients of optimizer's assigned params in-place\n                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n            else:\n                self._scaler.unscale_(optimizer)\n                norm = get_grad_norm_(parameters)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n        else:\n            norm = None\n        return norm\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)", "\n\ndef get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == inf:\n        total_norm = max(p.grad.detach().abs().max().to(device)\n                         for p in parameters)\n    else:\n        total_norm = torch.norm(\n            torch.stack([\n                torch.norm(p.grad.detach(), norm_type).to(device)\n                for p in parameters\n            ]), norm_type)\n    return total_norm", "\n\ndef cosine_scheduler(base_value,\n                     final_value,\n                     epochs,\n                     niter_per_ep,\n                     warmup_epochs=0,\n                     start_warmup_value=0,\n                     warmup_steps=-1):\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    if warmup_steps > 0:\n        warmup_iters = warmup_steps\n    print(\"Set warmup steps = %d\" % warmup_iters)\n    if warmup_epochs > 0:\n        warmup_schedule = np.linspace(start_warmup_value, base_value,\n                                      warmup_iters)\n\n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = np.array([\n        final_value + 0.5 * (base_value - final_value) *\n        (1 + math.cos(math.pi * i / (len(iters)))) for i in iters\n    ])\n\n    schedule = np.concatenate((warmup_schedule, schedule))\n\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule", "\n\ndef save_model(args,\n               epoch,\n               model,\n               model_without_ddp,\n               optimizer,\n               loss_scaler,\n               model_ema=None):\n    output_dir = Path(args.output_dir)\n    epoch_name = str(epoch)\n    if loss_scaler is not None:\n        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n        for checkpoint_path in checkpoint_paths:\n            to_save = {\n                'model': model_without_ddp.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'epoch': epoch,\n                'scaler': loss_scaler.state_dict(),\n                'args': args,\n            }\n\n            if model_ema is not None:\n                to_save['model_ema'] = get_state_dict(model_ema)\n\n            save_on_master(to_save, checkpoint_path)\n    else:\n        client_state = {'epoch': epoch}\n        if model_ema is not None:\n            client_state['model_ema'] = get_state_dict(model_ema)\n        model.save_checkpoint(\n            save_dir=args.output_dir,\n            tag=\"checkpoint-%s\" % epoch_name,\n            client_state=client_state)", "\n\ndef auto_load_model(args,\n                    model,\n                    model_without_ddp,\n                    optimizer,\n                    loss_scaler,\n                    model_ema=None):\n    output_dir = Path(args.output_dir)\n    if loss_scaler is not None:\n        # torch.amp\n        if args.auto_resume and len(args.resume) == 0:\n            import glob\n            all_checkpoints = glob.glob(\n                os.path.join(output_dir, 'checkpoint-*.pth'))\n            latest_ckpt = -1\n            for ckpt in all_checkpoints:\n                t = ckpt.split('-')[-1].split('.')[0]\n                if t.isdigit():\n                    latest_ckpt = max(int(t), latest_ckpt)\n            if latest_ckpt >= 0:\n                args.resume = os.path.join(output_dir,\n                                           'checkpoint-%d.pth' % latest_ckpt)\n            print(\"Auto resume checkpoint: %s\" % args.resume)\n\n        if args.resume:\n            if args.resume.startswith('https'):\n                checkpoint = torch.hub.load_state_dict_from_url(\n                    args.resume, map_location='cpu', check_hash=True)\n            else:\n                checkpoint = torch.load(args.resume, map_location='cpu')\n            model_without_ddp.load_state_dict(checkpoint['model'])\n            print(\"Resume checkpoint %s\" % args.resume)\n            if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n                args.start_epoch = checkpoint['epoch'] + 1\n                if hasattr(args, 'model_ema') and args.model_ema:\n                    _load_checkpoint_for_ema(model_ema,\n                                             checkpoint['model_ema'])\n                if 'scaler' in checkpoint:\n                    loss_scaler.load_state_dict(checkpoint['scaler'])\n                print(\"With optim & sched!\")\n    else:\n        # deepspeed, only support '--auto_resume'.\n        if args.auto_resume:\n            import glob\n            all_checkpoints = glob.glob(\n                os.path.join(output_dir, 'checkpoint-*'))\n            latest_ckpt = -1\n            for ckpt in all_checkpoints:\n                t = ckpt.split('-')[-1].split('.')[0]\n                if t.isdigit():\n                    latest_ckpt = max(int(t), latest_ckpt)\n            if latest_ckpt >= 0:\n                args.resume = os.path.join(output_dir,\n                                           'checkpoint-%d' % latest_ckpt)\n                print(\"Auto resume checkpoint: %d\" % latest_ckpt)\n                _, client_states = model.load_checkpoint(\n                    args.output_dir, tag='checkpoint-%d' % latest_ckpt)\n                if 'epoch' in client_states:\n                    args.start_epoch = client_states['epoch'] + 1\n                if model_ema is not None:\n                    if args.model_ema:\n                        _load_checkpoint_for_ema(model_ema,\n                                                 client_states['model_ema'])", "\n\ndef create_ds_config(args):\n    args.deepspeed_config = os.path.join(args.output_dir,\n                                         \"deepspeed_config.json\")\n    with open(args.deepspeed_config, mode=\"w\") as writer:\n        ds_config = {\n            \"train_batch_size\":\n            args.batch_size * args.update_freq * get_world_size(),\n            \"train_micro_batch_size_per_gpu\":\n            args.batch_size,\n            \"steps_per_print\":\n            1000,\n            \"gradient_clipping\":\n            0.0 if args.clip_grad is None else args.clip_grad,\n            \"optimizer\": {\n                \"type\": \"Adam\",\n                \"adam_w_mode\": True,\n                \"params\": {\n                    \"lr\": args.lr,\n                    \"weight_decay\": args.weight_decay,\n                    \"bias_correction\": True,\n                    \"betas\": [0.9, 0.999],\n                    \"eps\": 1e-8\n                }\n            },\n            \"fp16\": {\n                \"enabled\": True,\n                \"loss_scale\": 0,\n                \"initial_scale_power\": 7,\n                \"loss_scale_window\": 128\n            }\n        }\n\n        writer.write(json.dumps(ds_config, indent=2))", "\n\ndef multiple_samples_collate(batch, fold=False):\n    \"\"\"\n    Collate function for repeated augmentation. Each instance in the batch has\n    more than one sample.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated data batch.\n    \"\"\"\n    inputs, labels, video_idx, extra_data = zip(*batch)\n    inputs = [item for sublist in inputs for item in sublist]\n    labels = [item for sublist in labels for item in sublist]\n    video_idx = [item for sublist in video_idx for item in sublist]\n    inputs, labels, video_idx, extra_data = (\n        default_collate(inputs),\n        default_collate(labels),\n        default_collate(video_idx),\n        default_collate(extra_data),\n    )\n    if fold:\n        return [inputs], labels, video_idx, extra_data\n    else:\n        return inputs, labels, video_idx, extra_data", "\n\ndef multiple_pretrain_samples_collate(batch, fold=False):\n    \"\"\"\n    Collate function for repeated augmentation. Each instance in the batch has\n    more than one sample.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated data batch.\n    \"\"\"\n    process_data, encoder_mask, decoder_mask = zip(*batch)\n\n    process_data = [item for sublist in process_data for item in sublist]\n    encoder_mask = [item for sublist in encoder_mask for item in sublist]\n    decoder_mask = [item for sublist in decoder_mask for item in sublist]\n    process_data, encoder_mask, decoder_mask = (\n        default_collate(process_data),\n        default_collate(encoder_mask),\n        default_collate(decoder_mask),\n    )\n    if fold:\n        return [process_data], encoder_mask, decoder_mask\n    else:\n        return process_data, encoder_mask, decoder_mask", ""]}
{"filename": "run_mae_pretraining.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n\nimport argparse\nimport datetime", "import argparse\nimport datetime\nimport json\nimport os\nimport random\nimport time\nfrom functools import partial\nfrom pathlib import Path\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom packaging import version\nfrom timm.models import create_model\n\n# NOTE: Do not comment `import models`, it is used to register models\nimport models  # noqa: F401\nimport utils", "import models  # noqa: F401\nimport utils\nfrom dataset import build_pretraining_dataset\nfrom engine_for_pretraining import train_one_epoch\nfrom optim_factory import create_optimizer\nfrom utils import NativeScalerWithGradNormCount as NativeScaler\nfrom utils import multiple_pretrain_samples_collate\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        'VideoMAE v2 pre-training script', add_help=False)\n    parser.add_argument('--batch_size', default=64, type=int)\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--save_ckpt_freq', default=50, type=int)\n\n    # Model parameters\n    parser.add_argument(\n        '--model',\n        default='pretrain_videomae_base_patch16_224',\n        type=str,\n        metavar='MODEL',\n        help='Name of model to train')\n    parser.add_argument('--tubelet_size', type=int, default=2)\n    parser.add_argument(\n        '--with_checkpoint', action='store_true', default=False)\n\n    parser.add_argument(\n        '--decoder_depth', default=4, type=int, help='depth of decoder')\n\n    parser.add_argument(\n        '--mask_type',\n        default='tube',\n        choices=['random', 'tube'],\n        type=str,\n        help='encoder masked strategy')\n    parser.add_argument(\n        '--decoder_mask_type',\n        default='run_cell',\n        choices=['random', 'run_cell'],\n        type=str,\n        help='decoder masked strategy')\n\n    parser.add_argument(\n        '--mask_ratio', default=0.9, type=float, help='mask ratio of encoder')\n    parser.add_argument(\n        '--decoder_mask_ratio',\n        default=0.0,\n        type=float,\n        help='mask ratio of decoder')\n\n    parser.add_argument(\n        '--input_size',\n        default=224,\n        type=int,\n        help='images input size for backbone')\n\n    parser.add_argument(\n        '--drop_path',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Drop path rate (default: 0.1)')\n\n    parser.add_argument(\n        '--normlize_target',\n        default=True,\n        type=bool,\n        help='normalized the target patch pixels')\n\n    # Optimizer parameters\n    parser.add_argument(\n        '--opt',\n        default='adamw',\n        type=str,\n        metavar='OPTIMIZER',\n        help='Optimizer (default: \"adamw\"')\n    parser.add_argument(\n        '--opt_eps',\n        default=1e-8,\n        type=float,\n        metavar='EPSILON',\n        help='Optimizer Epsilon (default: 1e-8)')\n    parser.add_argument(\n        '--opt_betas',\n        default=None,\n        type=float,\n        nargs='+',\n        metavar='BETA',\n        help='Optimizer Betas (default: None, use opt default)')\n    parser.add_argument(\n        '--clip_grad',\n        type=float,\n        default=None,\n        metavar='NORM',\n        help='Clip gradient norm (default: None, no clipping)')\n    parser.add_argument(\n        '--momentum',\n        type=float,\n        default=0.9,\n        metavar='M',\n        help='SGD momentum (default: 0.9)')\n    parser.add_argument(\n        '--weight_decay',\n        type=float,\n        default=0.05,\n        help='weight decay (default: 0.05)')\n    parser.add_argument(\n        '--weight_decay_end',\n        type=float,\n        default=None,\n        help=\"\"\"Final value of the\n        weight decay. We use a cosine schedule for WD. \n        (Set the same value with args.weight_decay to keep weight decay no change)\"\"\"\n    )\n\n    parser.add_argument(\n        '--lr',\n        type=float,\n        default=1.5e-4,\n        metavar='LR',\n        help='learning rate (default: 1.5e-4)')\n    parser.add_argument(\n        '--warmup_lr',\n        type=float,\n        default=1e-6,\n        metavar='LR',\n        help='warmup learning rate (default: 1e-6)')\n    parser.add_argument(\n        '--min_lr',\n        type=float,\n        default=1e-5,\n        metavar='LR',\n        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\n    parser.add_argument(\n        '--warmup_epochs',\n        type=int,\n        default=40,\n        metavar='N',\n        help='epochs to warmup LR, if scheduler supports')\n    parser.add_argument(\n        '--warmup_steps',\n        type=int,\n        default=-1,\n        metavar='N',\n        help='epochs to warmup LR, if scheduler supports')\n\n    # Augmentation parameters\n    parser.add_argument(\n        '--color_jitter',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Color jitter factor (default: 0.4)')\n    parser.add_argument(\n        '--train_interpolation',\n        type=str,\n        default='bicubic',\n        choices=['random', 'bilinear', 'bicubic'],\n        help='Training interpolation')\n\n    # * Finetuning params\n    parser.add_argument(\n        '--finetune', default='', help='finetune from checkpoint')\n\n    # Dataset parameters\n    parser.add_argument(\n        '--data_path',\n        default='/your/data/annotation/path',\n        type=str,\n        help='dataset path')\n    parser.add_argument(\n        '--data_root', default='', type=str, help='dataset path root')\n    parser.add_argument(\n        '--fname_tmpl',\n        default='img_{:05}.jpg',\n        type=str,\n        help='filename_tmpl for rawframe data')\n    parser.add_argument(\n        '--imagenet_default_mean_and_std', default=True, action='store_true')\n    parser.add_argument('--num_frames', type=int, default=16)\n    parser.add_argument('--sampling_rate', type=int, default=4)\n    parser.add_argument('--num_sample', type=int, default=1)\n    parser.add_argument(\n        '--output_dir',\n        default='',\n        help='path where to save, empty for no saving')\n    parser.add_argument(\n        '--log_dir', default=None, help='path where to tensorboard log')\n    parser.add_argument(\n        '--device',\n        default='cuda',\n        help='device to use for training / testing')\n    parser.add_argument('--seed', default=0, type=int)\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\n    parser.add_argument('--auto_resume', action='store_true')\n    parser.add_argument(\n        '--no_auto_resume', action='store_false', dest='auto_resume')\n    parser.set_defaults(auto_resume=True)\n\n    parser.add_argument(\n        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n    parser.add_argument('--num_workers', default=10, type=int)\n    parser.add_argument(\n        '--pin_mem',\n        action='store_true',\n        help=\n        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'\n    )\n    parser.add_argument(\n        '--no_pin_mem', action='store_false', dest='pin_mem', help='')\n    parser.set_defaults(pin_mem=True)\n\n    # distributed training parameters\n    parser.add_argument(\n        '--world_size',\n        default=1,\n        type=int,\n        help='number of distributed processes')\n    parser.add_argument('--local_rank', default=-1, type=int)\n    parser.add_argument('--dist_on_itp', action='store_true')\n    parser.add_argument(\n        '--dist_url',\n        default='env://',\n        help='url used to set up distributed training')\n\n    return parser.parse_args()", "\ndef get_args():\n    parser = argparse.ArgumentParser(\n        'VideoMAE v2 pre-training script', add_help=False)\n    parser.add_argument('--batch_size', default=64, type=int)\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--save_ckpt_freq', default=50, type=int)\n\n    # Model parameters\n    parser.add_argument(\n        '--model',\n        default='pretrain_videomae_base_patch16_224',\n        type=str,\n        metavar='MODEL',\n        help='Name of model to train')\n    parser.add_argument('--tubelet_size', type=int, default=2)\n    parser.add_argument(\n        '--with_checkpoint', action='store_true', default=False)\n\n    parser.add_argument(\n        '--decoder_depth', default=4, type=int, help='depth of decoder')\n\n    parser.add_argument(\n        '--mask_type',\n        default='tube',\n        choices=['random', 'tube'],\n        type=str,\n        help='encoder masked strategy')\n    parser.add_argument(\n        '--decoder_mask_type',\n        default='run_cell',\n        choices=['random', 'run_cell'],\n        type=str,\n        help='decoder masked strategy')\n\n    parser.add_argument(\n        '--mask_ratio', default=0.9, type=float, help='mask ratio of encoder')\n    parser.add_argument(\n        '--decoder_mask_ratio',\n        default=0.0,\n        type=float,\n        help='mask ratio of decoder')\n\n    parser.add_argument(\n        '--input_size',\n        default=224,\n        type=int,\n        help='images input size for backbone')\n\n    parser.add_argument(\n        '--drop_path',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Drop path rate (default: 0.1)')\n\n    parser.add_argument(\n        '--normlize_target',\n        default=True,\n        type=bool,\n        help='normalized the target patch pixels')\n\n    # Optimizer parameters\n    parser.add_argument(\n        '--opt',\n        default='adamw',\n        type=str,\n        metavar='OPTIMIZER',\n        help='Optimizer (default: \"adamw\"')\n    parser.add_argument(\n        '--opt_eps',\n        default=1e-8,\n        type=float,\n        metavar='EPSILON',\n        help='Optimizer Epsilon (default: 1e-8)')\n    parser.add_argument(\n        '--opt_betas',\n        default=None,\n        type=float,\n        nargs='+',\n        metavar='BETA',\n        help='Optimizer Betas (default: None, use opt default)')\n    parser.add_argument(\n        '--clip_grad',\n        type=float,\n        default=None,\n        metavar='NORM',\n        help='Clip gradient norm (default: None, no clipping)')\n    parser.add_argument(\n        '--momentum',\n        type=float,\n        default=0.9,\n        metavar='M',\n        help='SGD momentum (default: 0.9)')\n    parser.add_argument(\n        '--weight_decay',\n        type=float,\n        default=0.05,\n        help='weight decay (default: 0.05)')\n    parser.add_argument(\n        '--weight_decay_end',\n        type=float,\n        default=None,\n        help=\"\"\"Final value of the\n        weight decay. We use a cosine schedule for WD. \n        (Set the same value with args.weight_decay to keep weight decay no change)\"\"\"\n    )\n\n    parser.add_argument(\n        '--lr',\n        type=float,\n        default=1.5e-4,\n        metavar='LR',\n        help='learning rate (default: 1.5e-4)')\n    parser.add_argument(\n        '--warmup_lr',\n        type=float,\n        default=1e-6,\n        metavar='LR',\n        help='warmup learning rate (default: 1e-6)')\n    parser.add_argument(\n        '--min_lr',\n        type=float,\n        default=1e-5,\n        metavar='LR',\n        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\n    parser.add_argument(\n        '--warmup_epochs',\n        type=int,\n        default=40,\n        metavar='N',\n        help='epochs to warmup LR, if scheduler supports')\n    parser.add_argument(\n        '--warmup_steps',\n        type=int,\n        default=-1,\n        metavar='N',\n        help='epochs to warmup LR, if scheduler supports')\n\n    # Augmentation parameters\n    parser.add_argument(\n        '--color_jitter',\n        type=float,\n        default=0.0,\n        metavar='PCT',\n        help='Color jitter factor (default: 0.4)')\n    parser.add_argument(\n        '--train_interpolation',\n        type=str,\n        default='bicubic',\n        choices=['random', 'bilinear', 'bicubic'],\n        help='Training interpolation')\n\n    # * Finetuning params\n    parser.add_argument(\n        '--finetune', default='', help='finetune from checkpoint')\n\n    # Dataset parameters\n    parser.add_argument(\n        '--data_path',\n        default='/your/data/annotation/path',\n        type=str,\n        help='dataset path')\n    parser.add_argument(\n        '--data_root', default='', type=str, help='dataset path root')\n    parser.add_argument(\n        '--fname_tmpl',\n        default='img_{:05}.jpg',\n        type=str,\n        help='filename_tmpl for rawframe data')\n    parser.add_argument(\n        '--imagenet_default_mean_and_std', default=True, action='store_true')\n    parser.add_argument('--num_frames', type=int, default=16)\n    parser.add_argument('--sampling_rate', type=int, default=4)\n    parser.add_argument('--num_sample', type=int, default=1)\n    parser.add_argument(\n        '--output_dir',\n        default='',\n        help='path where to save, empty for no saving')\n    parser.add_argument(\n        '--log_dir', default=None, help='path where to tensorboard log')\n    parser.add_argument(\n        '--device',\n        default='cuda',\n        help='device to use for training / testing')\n    parser.add_argument('--seed', default=0, type=int)\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\n    parser.add_argument('--auto_resume', action='store_true')\n    parser.add_argument(\n        '--no_auto_resume', action='store_false', dest='auto_resume')\n    parser.set_defaults(auto_resume=True)\n\n    parser.add_argument(\n        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n    parser.add_argument('--num_workers', default=10, type=int)\n    parser.add_argument(\n        '--pin_mem',\n        action='store_true',\n        help=\n        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'\n    )\n    parser.add_argument(\n        '--no_pin_mem', action='store_false', dest='pin_mem', help='')\n    parser.set_defaults(pin_mem=True)\n\n    # distributed training parameters\n    parser.add_argument(\n        '--world_size',\n        default=1,\n        type=int,\n        help='number of distributed processes')\n    parser.add_argument('--local_rank', default=-1, type=int)\n    parser.add_argument('--dist_on_itp', action='store_true')\n    parser.add_argument(\n        '--dist_url',\n        default='env://',\n        help='url used to set up distributed training')\n\n    return parser.parse_args()", "\n\ndef get_model(args):\n    print(f\"Creating model: {args.model}\")\n    model = create_model(\n        args.model,\n        pretrained=False,\n        drop_path_rate=args.drop_path,\n        drop_block_rate=None,\n        all_frames=args.num_frames,\n        tubelet_size=args.tubelet_size,\n        decoder_depth=args.decoder_depth,\n        with_cp=args.with_checkpoint)\n\n    if version.parse(torch.__version__) > version.parse('1.13.1'):\n        torch.set_float32_matmul_precision('high')\n        model = torch.compile(model)\n\n    return model", "\n\ndef main(args):\n    utils.init_distributed_mode(args)\n\n    print(args)\n\n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    model = get_model(args)\n    patch_size = model.encoder.patch_embed.patch_size\n    print(\"Patch size = %s\" % str(patch_size))\n    args.window_size = (args.num_frames // args.tubelet_size,\n                        args.input_size // patch_size[0],\n                        args.input_size // patch_size[1])\n    args.patch_size = patch_size\n\n    # get dataset\n    dataset_train = build_pretraining_dataset(args)\n\n    num_tasks = utils.get_world_size()\n    global_rank = utils.get_rank()\n    sampler_rank = global_rank\n    total_batch_size = args.batch_size * num_tasks\n\n    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n\n    sampler_train = torch.utils.data.DistributedSampler(\n        dataset_train, num_replicas=num_tasks, rank=sampler_rank, shuffle=True)\n    print(\"Sampler_train = %s\" % str(sampler_train))\n\n    if global_rank == 0 and args.log_dir is not None:\n        os.makedirs(args.log_dir, exist_ok=True)\n        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n    else:\n        log_writer = None\n\n    if args.num_sample > 1:\n        collate_func = partial(multiple_pretrain_samples_collate, fold=False)\n    else:\n        collate_func = None\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        sampler=sampler_train,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        pin_memory=args.pin_mem,\n        drop_last=True,\n        collate_fn=collate_func,\n        worker_init_fn=utils.seed_worker,\n        persistent_workers=True)\n\n    if args.finetune:\n        checkpoint = torch.load(args.finetune, map_location='cpu')\n\n        print(\"Load ckpt from %s\" % args.finetune)\n        checkpoint_model = None\n        for model_key in ['model', 'module']:\n            if model_key in checkpoint:\n                checkpoint_model = checkpoint[model_key]\n                print(\"Load state_dict by model_key = %s\" % model_key)\n                break\n        if checkpoint_model is None:\n            checkpoint_model = checkpoint\n\n        utils.load_state_dict(model, checkpoint_model)\n\n    model.to(device)\n    model_without_ddp = model\n    n_parameters = sum(p.numel() for p in model.parameters()\n                       if p.requires_grad)\n\n    print(\"Model = %s\" % str(model_without_ddp))\n    print('number of params: {} M'.format(n_parameters / 1e6))\n\n    # scale the lr\n    args.lr = args.lr * total_batch_size / 256\n    args.min_lr = args.min_lr * total_batch_size / 256\n    args.warmup_lr = args.warmup_lr * total_batch_size / 256\n\n    print(\"LR = %.8f\" % args.lr)\n    print(\"Batch size = %d\" % total_batch_size)\n    print(\"Number of training steps = %d\" % num_training_steps_per_epoch)\n    print(\"Number of training examples per epoch = %d\" %\n          (total_batch_size * num_training_steps_per_epoch))\n\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.gpu], find_unused_parameters=False)\n        model_without_ddp = model.module\n\n    optimizer = create_optimizer(args, model_without_ddp)\n    loss_scaler = NativeScaler()\n\n    print(\"Use step level LR & WD scheduler!\")\n    lr_schedule_values = utils.cosine_scheduler(\n        args.lr,\n        args.min_lr,\n        args.epochs,\n        num_training_steps_per_epoch,\n        warmup_epochs=args.warmup_epochs,\n        warmup_steps=args.warmup_steps,\n    )\n    if args.weight_decay_end is None:\n        args.weight_decay_end = args.weight_decay\n    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,\n                                                args.weight_decay_end,\n                                                args.epochs,\n                                                num_training_steps_per_epoch)\n    print(\"Max WD = %.7f, Min WD = %.7f\" %\n          (max(wd_schedule_values), min(wd_schedule_values)))\n\n    utils.auto_load_model(\n        args=args,\n        model=model,\n        model_without_ddp=model_without_ddp,\n        optimizer=optimizer,\n        loss_scaler=loss_scaler)\n    torch.cuda.empty_cache()\n    print(f\"Start training for {args.epochs} epochs\")\n    start_time = time.time()\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            data_loader_train.sampler.set_epoch(epoch)\n        if log_writer is not None:\n            log_writer.set_step(epoch * num_training_steps_per_epoch)\n        train_stats = train_one_epoch(\n            model,\n            data_loader_train,\n            optimizer,\n            device,\n            epoch,\n            loss_scaler,\n            args.clip_grad,\n            log_writer=log_writer,\n            start_steps=epoch * num_training_steps_per_epoch,\n            lr_schedule_values=lr_schedule_values,\n            wd_schedule_values=wd_schedule_values,\n            patch_size=patch_size[0],\n            normlize_target=args.normlize_target)\n        if args.output_dir:\n            _epoch = epoch + 1\n            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:\n                utils.save_model(\n                    args=args,\n                    model=model,\n                    model_without_ddp=model_without_ddp,\n                    optimizer=optimizer,\n                    loss_scaler=loss_scaler,\n                    epoch=epoch)\n\n        log_stats = {\n            **{f'train_{k}': v\n               for k, v in train_stats.items()}, 'epoch': epoch,\n            'n_parameters': n_parameters\n        }\n\n        if args.output_dir and utils.is_main_process():\n            if log_writer is not None:\n                log_writer.flush()\n            with open(\n                    os.path.join(args.output_dir, \"log.txt\"),\n                    mode=\"a\",\n                    encoding=\"utf-8\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str))", "\n\nif __name__ == '__main__':\n    opts = get_args()\n    if opts.output_dir:\n        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)\n    main(opts)\n"]}
{"filename": "extract_tad_feature.py", "chunked_list": ["\"\"\"Extract features for temporal action detection datasets\"\"\"\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport torch\nfrom timm.models import create_model\nfrom torchvision import transforms\n", "from torchvision import transforms\n\n# NOTE: Do not comment `import models`, it is used to register models\nimport models  # noqa: F401\nfrom dataset.loader import get_video_loader\n\n\ndef to_normalized_float_tensor(vid):\n    return vid.permute(3, 0, 1, 2).to(torch.float32) / 255\n", "\n\n# NOTE: for those functions, which generally expect mini-batches, we keep them\n# as non-minibatch so that they are applied as if they were 4d (thus image).\n# this way, we only apply the transformation in the spatial domain\ndef resize(vid, size, interpolation='bilinear'):\n    # NOTE: using bilinear interpolation because we don't work on minibatches\n    # at this level\n    scale = None\n    if isinstance(size, int):\n        scale = float(size) / min(vid.shape[-2:])\n        size = None\n    return torch.nn.functional.interpolate(\n        vid,\n        size=size,\n        scale_factor=scale,\n        mode=interpolation,\n        align_corners=False)", "\n\nclass ToFloatTensorInZeroOne(object):\n\n    def __call__(self, vid):\n        return to_normalized_float_tensor(vid)\n\n\nclass Resize(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, vid):\n        return resize(vid, self.size)", "class Resize(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, vid):\n        return resize(vid, self.size)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        'Extract TAD features using the videomae model', add_help=False)\n\n    parser.add_argument(\n        '--data_set',\n        default='THUMOS14',\n        choices=['THUMOS14', 'FINEACTION'],\n        type=str,\n        help='dataset')\n\n    parser.add_argument(\n        '--data_path',\n        default='YOUR_PATH/thumos14_video',\n        type=str,\n        help='dataset path')\n    parser.add_argument(\n        '--save_path',\n        default='YOUR_PATH/thumos14_video/th14_vit_g_16_4',\n        type=str,\n        help='path for saving features')\n\n    parser.add_argument(\n        '--model',\n        default='vit_giant_patch14_224',\n        type=str,\n        metavar='MODEL',\n        help='Name of model')\n    parser.add_argument(\n        '--ckpt_path',\n        default='YOUR_PATH/vit_g_hyrbid_pt_1200e_k710_ft.pth',\n        help='load from checkpoint')\n\n    return parser.parse_args()", "\ndef get_args():\n    parser = argparse.ArgumentParser(\n        'Extract TAD features using the videomae model', add_help=False)\n\n    parser.add_argument(\n        '--data_set',\n        default='THUMOS14',\n        choices=['THUMOS14', 'FINEACTION'],\n        type=str,\n        help='dataset')\n\n    parser.add_argument(\n        '--data_path',\n        default='YOUR_PATH/thumos14_video',\n        type=str,\n        help='dataset path')\n    parser.add_argument(\n        '--save_path',\n        default='YOUR_PATH/thumos14_video/th14_vit_g_16_4',\n        type=str,\n        help='path for saving features')\n\n    parser.add_argument(\n        '--model',\n        default='vit_giant_patch14_224',\n        type=str,\n        metavar='MODEL',\n        help='Name of model')\n    parser.add_argument(\n        '--ckpt_path',\n        default='YOUR_PATH/vit_g_hyrbid_pt_1200e_k710_ft.pth',\n        help='load from checkpoint')\n\n    return parser.parse_args()", "\n\ndef get_start_idx_range(data_set):\n\n    def thumos14_range(num_frames):\n        return range(0, num_frames - 15, 4)\n\n    def fineaction_range(num_frames):\n        return range(0, num_frames - 15, 16)\n\n    if data_set == 'THUMOS14':\n        return thumos14_range\n    elif data_set == 'FINEACTION':\n        return fineaction_range\n    else:\n        raise NotImplementedError()", "\n\ndef extract_feature(args):\n    # preparation\n    if not os.path.exists(args.save_path):\n        os.makedirs(args.save_path)\n    video_loader = get_video_loader()\n    start_idx_range = get_start_idx_range(args.data_set)\n    transform = transforms.Compose(\n        [ToFloatTensorInZeroOne(),\n         Resize((224, 224))])\n\n    # get video path\n    vid_list = os.listdir(args.data_path)\n    random.shuffle(vid_list)\n\n    # get model & load ckpt\n    model = create_model(\n        args.model,\n        img_size=224,\n        pretrained=False,\n        num_classes=710,\n        all_frames=16,\n        tubelet_size=2,\n        drop_path_rate=0.3,\n        use_mean_pooling=True)\n    ckpt = torch.load(args.ckpt_path, map_location='cpu')\n    for model_key in ['model', 'module']:\n        if model_key in ckpt:\n            ckpt = ckpt[model_key]\n            break\n    model.load_state_dict(ckpt)\n    model.eval()\n    model.cuda()\n\n    # extract feature\n    num_videos = len(vid_list)\n    for idx, vid_name in enumerate(vid_list):\n        url = os.path.join(args.save_path, vid_name.split('.')[0] + '.npy')\n        if os.path.exists(url):\n            continue\n\n        video_path = os.path.join(args.data_path, vid_name)\n        vr = video_loader(video_path)\n\n        feature_list = []\n        for start_idx in start_idx_range(len(vr)):\n            data = vr.get_batch(np.arange(start_idx, start_idx + 16)).asnumpy()\n            frame = torch.from_numpy(data)  # torch.Size([16, 566, 320, 3])\n            frame_q = transform(frame)  # torch.Size([3, 16, 224, 224])\n            input_data = frame_q.unsqueeze(0).cuda()\n\n            with torch.no_grad():\n                feature = model.forward_features(input_data)\n                feature_list.append(feature.cpu().numpy())\n\n        # [N, C]\n        np.save(url, np.vstack(feature_list))\n        print(f'[{idx} / {num_videos}]: save feature on {url}')", "\n\nif __name__ == '__main__':\n    args = get_args()\n    extract_feature(args)\n"]}
{"filename": "dataset/masking_generator.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport numpy as np\n\n\nclass Cell():\n\n    def __init__(self, num_masks, num_patches):\n        self.num_masks = num_masks\n        self.num_patches = num_patches\n        self.size = num_masks + num_patches\n        self.queue = np.hstack([np.ones(num_masks), np.zeros(num_patches)])\n        self.queue_ptr = 0\n\n    def set_ptr(self, pos=-1):\n        self.queue_ptr = np.random.randint(self.size) if pos < 0 else pos\n\n    def get_cell(self):\n        cell_idx = (np.arange(self.size) + self.queue_ptr) % self.size\n        return self.queue[cell_idx]\n\n    def run_cell(self):\n        self.queue_ptr += 1", "\n\nclass Cell():\n\n    def __init__(self, num_masks, num_patches):\n        self.num_masks = num_masks\n        self.num_patches = num_patches\n        self.size = num_masks + num_patches\n        self.queue = np.hstack([np.ones(num_masks), np.zeros(num_patches)])\n        self.queue_ptr = 0\n\n    def set_ptr(self, pos=-1):\n        self.queue_ptr = np.random.randint(self.size) if pos < 0 else pos\n\n    def get_cell(self):\n        cell_idx = (np.arange(self.size) + self.queue_ptr) % self.size\n        return self.queue[cell_idx]\n\n    def run_cell(self):\n        self.queue_ptr += 1", "\n\nclass RandomMaskingGenerator:\n\n    def __init__(self, input_size, mask_ratio):\n        if not isinstance(input_size, tuple):\n            input_size = (input_size, ) * 3\n\n        self.frames, self.height, self.width = input_size\n\n        self.num_patches = self.frames * self.height * self.width  # 8x14x14\n        self.num_mask = int(mask_ratio * self.num_patches)\n\n    def __repr__(self):\n        repr_str = \"Mask: total patches {}, mask patches {}\".format(\n            self.num_patches, self.num_mask)\n        return repr_str\n\n    def __call__(self):\n        mask = np.hstack([\n            np.zeros(self.num_patches - self.num_mask),\n            np.ones(self.num_mask),\n        ])\n        np.random.shuffle(mask)\n        return mask  # [196*8]", "\n\nclass TubeMaskingGenerator:\n\n    def __init__(self, input_size, mask_ratio):\n        self.frames, self.height, self.width = input_size\n        self.num_patches_per_frame = self.height * self.width  # 14x14\n        self.total_patches = self.frames * self.num_patches_per_frame\n        self.num_masks_per_frame = int(mask_ratio * self.num_patches_per_frame)\n        self.total_masks = self.frames * self.num_masks_per_frame\n\n    def __repr__(self):\n        repr_str = \"Tube Masking: total patches {}, mask patches {}\".format(\n            self.total_patches, self.total_masks)\n        return repr_str\n\n    def __call__(self):\n        mask_per_frame = np.hstack([\n            np.zeros(self.num_patches_per_frame - self.num_masks_per_frame),\n            np.ones(self.num_masks_per_frame),\n        ])\n        np.random.shuffle(mask_per_frame)\n        mask = np.tile(mask_per_frame, (self.frames, 1))\n        return mask  # [196*8]", "\n\nclass RunningCellMaskingGenerator:\n\n    def __init__(self, input_size, mask_ratio=0.5):\n        self.frames, self.height, self.width = input_size\n        self.mask_ratio = mask_ratio\n\n        num_masks_per_cell = int(4 * self.mask_ratio)\n        assert 0 < num_masks_per_cell < 4\n        num_patches_per_cell = 4 - num_masks_per_cell\n\n        self.cell = Cell(num_masks_per_cell, num_patches_per_cell)\n        self.cell_size = self.cell.size\n\n        mask_list = []\n        for ptr_pos in range(self.cell_size):\n            self.cell.set_ptr(ptr_pos)\n            mask = []\n            for _ in range(self.frames):\n                self.cell.run_cell()\n                mask_unit = self.cell.get_cell().reshape(2, 2)\n                mask_map = np.tile(mask_unit,\n                                   [self.height // 2, self.width // 2])\n                mask.append(mask_map.flatten())\n            mask = np.stack(mask, axis=0)\n            mask_list.append(mask)\n        self.all_mask_maps = np.stack(mask_list, axis=0)\n\n    def __repr__(self):\n        repr_str = f\"Running Cell Masking with mask ratio {self.mask_ratio}\"\n        return repr_str\n\n    def __call__(self):\n        mask = self.all_mask_maps[np.random.randint(self.cell_size)]\n        return np.copy(mask)", ""]}
{"filename": "dataset/loader.py", "chunked_list": ["import io\n\nimport cv2\nimport numpy as np\nfrom decord import VideoReader, cpu\n\ntry:\n    from petrel_client.client import Client\n    petrel_backend_imported = True\nexcept (ImportError, ModuleNotFoundError):\n    petrel_backend_imported = False", "\n\ndef get_video_loader(use_petrel_backend: bool = True,\n                     enable_mc: bool = True,\n                     conf_path: str = None):\n    if petrel_backend_imported and use_petrel_backend:\n        _client = Client(conf_path=conf_path, enable_mc=enable_mc)\n    else:\n        _client = None\n\n    def _loader(video_path):\n        if _client is not None and 's3:' in video_path:\n            video_path = io.BytesIO(_client.get(video_path))\n\n        vr = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n        return vr\n\n    return _loader", "\n\ndef get_image_loader(use_petrel_backend: bool = True,\n                     enable_mc: bool = True,\n                     conf_path: str = None):\n    if petrel_backend_imported and use_petrel_backend:\n        _client = Client(conf_path=conf_path, enable_mc=enable_mc)\n    else:\n        _client = None\n\n    def _loader(frame_path):\n        if _client is not None and 's3:' in frame_path:\n            img_bytes = _client.get(frame_path)\n        else:\n            with open(frame_path, 'rb') as f:\n                img_bytes = f.read()\n\n        img_np = np.frombuffer(img_bytes, np.uint8)\n        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n        cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n        return img\n\n    return _loader", ""]}
{"filename": "dataset/transforms.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport math\nimport numbers\nimport random", "import numbers\nimport random\nimport warnings\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as F\nfrom PIL import Image, ImageOps\n", "from PIL import Image, ImageOps\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img", "\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype)", "\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n    Image.HAMMING: 'PIL.Image.HAMMING',\n    Image.BOX: 'PIL.Image.BOX',\n}", "    Image.BOX: 'PIL.Image.BOX',\n}\n\n\ndef _pil_interp(method):\n    if method == 'bicubic':\n        return Image.BICUBIC\n    elif method == 'lanczos':\n        return Image.LANCZOS\n    elif method == 'hamming':\n        return Image.HAMMING\n    else:\n        # default bilinear, do we want to allow nearest?\n        return Image.BILINEAR", "\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandomResizedCropAndInterpolationWithTwoPic:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self,\n                 size,\n                 second_size=None,\n                 scale=(0.08, 1.0),\n                 ratio=(3. / 4., 4. / 3.),\n                 interpolation='bilinear',\n                 second_interpolation='lanczos'):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if second_size is not None:\n            if isinstance(second_size, tuple):\n                self.second_size = second_size\n            else:\n                self.second_size = (second_size, second_size)\n        else:\n            self.second_size = None\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(\"range should be of kind (min, max)\")\n\n        if interpolation == 'random':\n            self.interpolation = _RANDOM_INTERPOLATION\n        else:\n            self.interpolation = _pil_interp(interpolation)\n        self.second_interpolation = _pil_interp(second_interpolation)\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if in_ratio < min(ratio):\n            w = img.size[0]\n            h = int(round(w / min(ratio)))\n        elif in_ratio > max(ratio):\n            h = img.size[1]\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolation = random.choice(self.interpolation)\n        else:\n            interpolation = self.interpolation\n        if self.second_size is None:\n            return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n        else:\n            return F.resized_crop(img, i, j, h, w, self.size,\n                                  interpolation), F.resized_crop(\n                                      img, i, j, h, w, self.second_size,\n                                      self.second_interpolation)\n\n    def __repr__(self):\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolate_str = ' '.join(\n                [_pil_interpolation_to_str[x] for x in self.interpolation])\n        else:\n            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += ', scale={0}'.format(\n            tuple(round(s, 4) for s in self.scale))\n        format_string += ', ratio={0}'.format(\n            tuple(round(r, 4) for r in self.ratio))\n        format_string += ', interpolation={0}'.format(interpolate_str)\n        if self.second_size is not None:\n            format_string += ', second_size={0}'.format(self.second_size)\n            format_string += ', second_interpolation={0}'.format(\n                _pil_interpolation_to_str[self.second_interpolation])\n        format_string += ')'\n        return format_string", "\n\nclass GroupRandomCrop(object):\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n\n        w, h = img_group[0].size\n        th, tw = self.size\n\n        out_images = list()\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n\n        for img in img_group:\n            assert (img.size[0] == w and img.size[1] == h)\n            if w == tw and h == th:\n                out_images.append(img)\n            else:\n                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n\n        return (out_images, label)", "\n\nclass GroupCenterCrop(object):\n\n    def __init__(self, size):\n        self.worker = torchvision.transforms.CenterCrop(size)\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n        return ([self.worker(img) for img in img_group], label)", "\n\nclass GroupRandomHorizontalFlip(object):\n    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    \"\"\"\n\n    def __init__(self, selective_flip=True, is_flow=False):\n        self.is_flow = is_flow\n        self.class_LeftRight = [86, 87, 93, 94, 166, 167\n                                ] if selective_flip else []\n\n    def __call__(self, img_tuple, is_flow=False):\n        img_group, label = img_tuple\n        v = random.random()\n        if (label not in self.class_LeftRight) and v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(ret), 2):\n                    ret[i] = ImageOps.invert(\n                        ret[i])  # invert flow pixel values when flipping\n            return (ret, label)\n        else:\n            return img_tuple", "\n\nclass GroupNormalize(object):\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor_tuple):\n        tensor, label = tensor_tuple\n        rep_mean = self.mean * (tensor.size()[0] // len(self.mean))\n        rep_std = self.std * (tensor.size()[0] // len(self.std))\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, rep_mean, rep_std):\n            t.sub_(m).div_(s)\n\n        return (tensor, label)", "\n\nclass GroupGrayScale(object):\n\n    def __init__(self, size):\n        self.worker = torchvision.transforms.Grayscale(size)\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n        return ([self.worker(img) for img in img_group], label)", "\n\nclass GroupScale(object):\n    \"\"\" Rescales the input PIL.Image to the given 'size'.\n    'size' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.worker = torchvision.transforms.Resize(size, interpolation)\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n        return ([self.worker(img) for img in img_group], label)", "\n\nclass GroupOverSample(object):\n\n    def __init__(self, crop_size, scale_size=None):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (\n            crop_size, crop_size)\n\n        if scale_size is not None:\n            self.scale_worker = GroupScale(scale_size)\n        else:\n            self.scale_worker = None\n\n    def __call__(self, img_tuple):\n        if self.scale_worker is not None:\n            img_tuple = self.scale_worker(img_tuple)\n\n        img_group, label = img_tuple\n\n        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n\n        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h,\n                                                      crop_w, crop_h)\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\n                if img.mode == 'L' and i % 2 == 0:\n                    flip_group.append(ImageOps.invert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return (oversample_group, label)", "\n\nclass GroupFullResSample(object):\n\n    def __init__(self, crop_size, scale_size=None, flip=True):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (\n            crop_size, crop_size)\n\n        if scale_size is not None:\n            self.scale_worker = GroupScale(scale_size)\n        else:\n            self.scale_worker = None\n        self.flip = flip\n\n    def __call__(self, img_tuple):\n\n        if self.scale_worker is not None:\n            img_tuple = self.scale_worker(img_tuple)\n\n        img_group, label = img_tuple\n        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        offsets = list()\n        offsets.append((0 * w_step, 2 * h_step))  # left\n        offsets.append((4 * w_step, 2 * h_step))  # right\n        offsets.append((2 * w_step, 2 * h_step))  # center\n\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                if self.flip:\n                    flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\n                    if img.mode == 'L' and i % 2 == 0:\n                        flip_group.append(ImageOps.invert(flip_crop))\n                    else:\n                        flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return (oversample_group, label)", "\n\nclass GroupMultiScaleCrop(object):\n\n    def __init__(self,\n                 input_size,\n                 scales=None,\n                 max_distort=1,\n                 fix_crop=True,\n                 more_fix_crop=True):\n        self.scales = scales if scales is not None else [1, .875, .75, .66]\n        self.max_distort = max_distort\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.input_size = input_size if not isinstance(input_size, int) else [\n            input_size, input_size\n        ]\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n\n        im_size = img_group[0].size\n\n        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n        crop_img_group = [\n            img.crop(\n                (offset_w, offset_h, offset_w + crop_w, offset_h + crop_h))\n            for img in img_group\n        ]\n        ret_img_group = [\n            img.resize((self.input_size[0], self.input_size[1]),\n                       self.interpolation) for img in crop_img_group\n        ]\n        return (ret_img_group, label)\n\n    def _sample_crop_size(self, im_size):\n        image_w, image_h = im_size[0], im_size[1]\n\n        # find a crop size\n        base_size = min(image_w, image_h)\n        crop_sizes = [int(base_size * x) for x in self.scales]\n        crop_h = [\n            self.input_size[1] if abs(x - self.input_size[1]) < 3 else x\n            for x in crop_sizes\n        ]\n        crop_w = [\n            self.input_size[0] if abs(x - self.input_size[0]) < 3 else x\n            for x in crop_sizes\n        ]\n\n        pairs = []\n        for i, h in enumerate(crop_h):\n            for j, w in enumerate(crop_w):\n                if abs(i - j) <= self.max_distort:\n                    pairs.append((w, h))\n\n        crop_pair = random.choice(pairs)\n        if not self.fix_crop:\n            w_offset = random.randint(0, image_w - crop_pair[0])\n            h_offset = random.randint(0, image_h - crop_pair[1])\n        else:\n            w_offset, h_offset = self._sample_fix_offset(\n                image_w, image_h, crop_pair[0], crop_pair[1])\n\n        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\n    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h,\n                                       crop_w, crop_h)\n        return random.choice(offsets)\n\n    @staticmethod\n    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        ret = list()\n        ret.append((0, 0))  # upper left\n        ret.append((4 * w_step, 0))  # upper right\n        ret.append((0, 4 * h_step))  # lower left\n        ret.append((4 * w_step, 4 * h_step))  # lower right\n        ret.append((2 * w_step, 2 * h_step))  # center\n\n        if more_fix_crop:\n            ret.append((0, 2 * h_step))  # center left\n            ret.append((4 * w_step, 2 * h_step))  # center right\n            ret.append((2 * w_step, 4 * h_step))  # lower center\n            ret.append((2 * w_step, 0 * h_step))  # upper center\n\n            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\n        return ret", "\n\nclass GroupRandomSizedCrop(object):\n    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n\n        for attempt in range(10):\n            area = img_group[0].size[0] * img_group[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n                x1 = random.randint(0, img_group[0].size[0] - w)\n                y1 = random.randint(0, img_group[0].size[1] - h)\n                found = True\n                break\n        else:\n            found = False\n            x1 = 0\n            y1 = 0\n\n        if found:\n            out_group = list()\n            for img in img_group:\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n                out_group.append(\n                    img.resize((self.size, self.size), self.interpolation))\n            return out_group\n        else:\n            # Fallback\n            scale = GroupScale(self.size, interpolation=self.interpolation)\n            crop = GroupRandomCrop(self.size)\n            return crop(scale(img_group))", "\n\nclass Stack(object):\n\n    def __init__(self, roll=False):\n        self.roll = roll\n\n    def __call__(self, img_tuple):\n        img_group, label = img_tuple\n\n        if img_group[0].mode == 'L':\n            return (np.concatenate([np.expand_dims(x, 2) for x in img_group],\n                                   axis=2), label)\n        elif img_group[0].mode == 'RGB':\n            if self.roll:\n                return (np.concatenate(\n                    [np.array(x)[:, :, ::-1] for x in img_group],\n                    axis=2), label)\n            else:\n                return (np.concatenate(img_group, axis=2), label)", "\n\nclass ToTorchFormatTensor(object):\n    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n\n    def __init__(self, div=True):\n        self.div = div\n\n    def __call__(self, pic_tuple):\n        pic, label = pic_tuple\n\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n        else:\n            # handle PIL Image\n            img = torch.as_tensor(pic.tobytes(), dtype=torch.uint8)\n            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n            # put it from HWC to CHW format\n            # yikes, this transpose takes 80% of the loading time/CPU\n            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        return (img.float().div(255.) if self.div else img.float(), label)", "\n\nclass IdentityTransform(object):\n\n    def __call__(self, data):\n        return data\n"]}
{"filename": "dataset/video_transforms.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport math\nimport numbers\nimport random\n\nimport numpy as np\nimport PIL\nimport torch", "import PIL\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as F\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom . import functional as FF\nfrom .rand_augment import rand_augment_transform\nfrom .random_erasing import RandomErasing", "from .rand_augment import rand_augment_transform\nfrom .random_erasing import RandomErasing\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: \"PIL.Image.NEAREST\",\n    Image.BILINEAR: \"PIL.Image.BILINEAR\",\n    Image.BICUBIC: \"PIL.Image.BICUBIC\",\n    Image.LANCZOS: \"PIL.Image.LANCZOS\",\n    Image.HAMMING: \"PIL.Image.HAMMING\",\n    Image.BOX: \"PIL.Image.BOX\",", "    Image.HAMMING: \"PIL.Image.HAMMING\",\n    Image.BOX: \"PIL.Image.BOX\",\n}\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\ndef _pil_interp(method):\n    if method == \"bicubic\":\n        return Image.BICUBIC\n    elif method == \"lanczos\":\n        return Image.LANCZOS\n    elif method == \"hamming\":\n        return Image.HAMMING\n    else:\n        return Image.BILINEAR", "\n\ndef random_short_side_scale_jitter(images,\n                                   min_size,\n                                   max_size,\n                                   boxes=None,\n                                   inverse_uniform_sampling=False):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        min_size (int): the minimal size to scale the frames.\n        max_size (int): the maximal size to scale the frames.\n        boxes (ndarray): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        inverse_uniform_sampling (bool): if True, sample uniformly in\n            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the\n            scale. If False, take a uniform sample from [min_scale, max_scale].\n    Returns:\n        (tensor): the scaled images with dimension of\n            `num frames` x `channel` x `new height` x `new width`.\n        (ndarray or None): the scaled boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    if inverse_uniform_sampling:\n        size = int(\n            round(1.0 / np.random.uniform(1.0 / max_size, 1.0 / min_size)))\n    else:\n        size = int(round(np.random.uniform(min_size, max_size)))\n\n    height = images.shape[2]\n    width = images.shape[3]\n    if (width <= height and width == size) or (height <= width\n                                               and height == size):\n        return images, boxes\n    new_width = size\n    new_height = size\n    if width < height:\n        new_height = int(math.floor((float(height) / width) * size))\n        if boxes is not None:\n            boxes = boxes * float(new_height) / height\n    else:\n        new_width = int(math.floor((float(width) / height) * size))\n        if boxes is not None:\n            boxes = boxes * float(new_width) / width\n\n    return (\n        torch.nn.functional.interpolate(\n            images,\n            size=(new_height, new_width),\n            mode=\"bilinear\",\n            align_corners=False,\n        ),\n        boxes,\n    )", "\n\ndef crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    cropped_boxes = boxes.copy()\n    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\n    return cropped_boxes", "\n\ndef random_crop(images, size, boxes=None):\n    \"\"\"\n    Perform random spatial crop on the given images and corresponding boxes.\n    Args:\n        images (tensor): images to perform random crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): the size of height and width to crop on the image.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        cropped (tensor): cropped images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    if images.shape[2] == size and images.shape[3] == size:\n        return images\n    height = images.shape[2]\n    width = images.shape[3]\n    y_offset = 0\n    if height > size:\n        y_offset = int(np.random.randint(0, height - size))\n    x_offset = 0\n    if width > size:\n        x_offset = int(np.random.randint(0, width - size))\n    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]\n\n    cropped_boxes = (\n        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)\n\n    return cropped, cropped_boxes", "\n\ndef horizontal_flip(prob, images, boxes=None):\n    \"\"\"\n    Perform horizontal flip on the given images and corresponding boxes.\n    Args:\n        prob (float): probility to flip the images.\n        images (tensor): images to perform horizontal flip, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:\n        images (tensor): images with dimension of\n            `num frames` x `channel` x `height` x `width`.\n        flipped_boxes (ndarray or None): the flipped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    if boxes is None:\n        flipped_boxes = None\n    else:\n        flipped_boxes = boxes.copy()\n\n    if np.random.uniform() < prob:\n        images = images.flip((-1))\n\n        if len(images.shape) == 3:\n            width = images.shape[2]\n        elif len(images.shape) == 4:\n            width = images.shape[3]\n        else:\n            raise NotImplementedError(\"Dimension does not supported\")\n        if boxes is not None:\n            flipped_boxes[:, [0, 2]] = width - boxes[:, [2, 0]] - 1\n\n    return images, flipped_boxes", "\n\ndef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n        scale_size (int): optinal. If not None, resize the images to scale_size before\n            performing any crop.\n    Returns:\n        cropped (tensor): images with dimension of\n            `num frames` x `channel` x `size` x `size`.\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    assert spatial_idx in [0, 1, 2]\n    ndim = len(images.shape)\n    if ndim == 3:\n        images = images.unsqueeze(0)\n    height = images.shape[2]\n    width = images.shape[3]\n\n    if scale_size is not None:\n        if width <= height:\n            width, height = scale_size, int(height / width * scale_size)\n        else:\n            width, height = int(width / height * scale_size), scale_size\n        images = torch.nn.functional.interpolate(\n            images,\n            size=(height, width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    y_offset = int(math.ceil((height - size) / 2))\n    x_offset = int(math.ceil((width - size) / 2))\n\n    if height > width:\n        if spatial_idx == 0:\n            y_offset = 0\n        elif spatial_idx == 2:\n            y_offset = height - size\n    else:\n        if spatial_idx == 0:\n            x_offset = 0\n        elif spatial_idx == 2:\n            x_offset = width - size\n    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]\n    cropped_boxes = (\n        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)\n    if ndim == 3:\n        cropped = cropped.squeeze(0)\n    return cropped, cropped_boxes", "\n\ndef clip_boxes_to_image(boxes, height, width):\n    \"\"\"\n    Clip an array of boxes to an image with the given height and width.\n    Args:\n        boxes (ndarray): bounding boxes to perform clipping.\n            Dimension is `num boxes` x 4.\n        height (int): given image height.\n        width (int): given image width.\n    Returns:\n        clipped_boxes (ndarray): the clipped boxes with dimension of\n            `num boxes` x 4.\n    \"\"\"\n    clipped_boxes = boxes.copy()\n    clipped_boxes[:, [0, 2]] = np.minimum(width - 1.0,\n                                          np.maximum(0.0, boxes[:, [0, 2]]))\n    clipped_boxes[:, [1, 3]] = np.minimum(height - 1.0,\n                                          np.maximum(0.0, boxes[:, [1, 3]]))\n    return clipped_boxes", "\n\ndef blend(images1, images2, alpha):\n    \"\"\"\n    Blend two images with a given weight alpha.\n    Args:\n        images1 (tensor): the first images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        images2 (tensor): the second images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alpha (float): the blending weight.\n    Returns:\n        (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    return images1 * alpha + images2 * (1 - alpha)", "\n\ndef grayscale(images):\n    \"\"\"\n    Get the grayscale for the input images. The channels of images should be\n    in order BGR.\n    Args:\n        images (tensor): the input images for getting grayscale. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    # R -> 0.299, G -> 0.587, B -> 0.114.\n    img_gray = torch.tensor(images)\n    gray_channel = (0.299 * images[:, 2] + 0.587 * images[:, 1] +\n                    0.114 * images[:, 0])\n    img_gray[:, 0] = gray_channel\n    img_gray[:, 1] = gray_channel\n    img_gray[:, 2] = gray_channel\n    return img_gray", "\n\ndef color_jitter(images, img_brightness=0, img_contrast=0, img_saturation=0):\n    \"\"\"\n    Perfrom a color jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n\n    jitter = []\n    if img_brightness != 0:\n        jitter.append(\"brightness\")\n    if img_contrast != 0:\n        jitter.append(\"contrast\")\n    if img_saturation != 0:\n        jitter.append(\"saturation\")\n\n    if len(jitter) > 0:\n        order = np.random.permutation(np.arange(len(jitter)))\n        for idx in range(0, len(jitter)):\n            if jitter[order[idx]] == \"brightness\":\n                images = brightness_jitter(img_brightness, images)\n            elif jitter[order[idx]] == \"contrast\":\n                images = contrast_jitter(img_contrast, images)\n            elif jitter[order[idx]] == \"saturation\":\n                images = saturation_jitter(img_saturation, images)\n    return images", "\n\ndef brightness_jitter(var, images):\n    \"\"\"\n    Perfrom brightness jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for brightness.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)\n\n    img_bright = torch.zeros(images.shape)\n    images = blend(images, img_bright, alpha)\n    return images", "\n\ndef contrast_jitter(var, images):\n    \"\"\"\n    Perfrom contrast jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for contrast.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)\n\n    img_gray = grayscale(images)\n    img_gray[:] = torch.mean(img_gray, dim=(1, 2, 3), keepdim=True)\n    images = blend(images, img_gray, alpha)\n    return images", "\n\ndef saturation_jitter(var, images):\n    \"\"\"\n    Perfrom saturation jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for saturation.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)\n    img_gray = grayscale(images)\n    images = blend(images, img_gray, alpha)\n\n    return images", "\n\ndef lighting_jitter(images, alphastd, eigval, eigvec):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given images.\n    Args:\n        images (tensor): images to perform lighting jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:\n        out_images (tensor): the jittered images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    if alphastd == 0:\n        return images\n    # generate alpha1, alpha2, alpha3.\n    alpha = np.random.normal(0, alphastd, size=(1, 3))\n    eig_vec = np.array(eigvec)\n    eig_val = np.reshape(eigval, (1, 3))\n    rgb = np.sum(\n        eig_vec * np.repeat(alpha, 3, axis=0) * np.repeat(eig_val, 3, axis=0),\n        axis=1,\n    )\n    out_images = torch.zeros_like(images)\n    if len(images.shape) == 3:\n        # C H W\n        channel_dim = 0\n    elif len(images.shape) == 4:\n        # T C H W\n        channel_dim = 1\n    else:\n        raise NotImplementedError(f\"Unsupported dimension {len(images.shape)}\")\n\n    for idx in range(images.shape[channel_dim]):\n        # C H W\n        if len(images.shape) == 3:\n            out_images[idx] = images[idx] + rgb[2 - idx]\n        # T C H W\n        elif len(images.shape) == 4:\n            out_images[:, idx] = images[:, idx] + rgb[2 - idx]\n        else:\n            raise NotImplementedError(\n                f\"Unsupported dimension {len(images.shape)}\")\n\n    return out_images", "\n\ndef color_normalization(images, mean, stddev):\n    \"\"\"\n    Perform color nomration on the given images.\n    Args:\n        images (tensor): images to perform color normalization. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        mean (list): mean values for normalization.\n        stddev (list): standard deviations for normalization.\n\n    Returns:\n        out_images (tensor): the noramlized images, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n    \"\"\"\n    if len(images.shape) == 3:\n        assert (\n            len(mean) == images.shape[0]), \"channel mean not computed properly\"\n        assert (len(stddev) == images.shape[0]\n                ), \"channel stddev not computed properly\"\n    elif len(images.shape) == 4:\n        assert (\n            len(mean) == images.shape[1]), \"channel mean not computed properly\"\n        assert (len(stddev) == images.shape[1]\n                ), \"channel stddev not computed properly\"\n    else:\n        raise NotImplementedError(f\"Unsupported dimension {len(images.shape)}\")\n\n    out_images = torch.zeros_like(images)\n    for idx in range(len(mean)):\n        # C H W\n        if len(images.shape) == 3:\n            out_images[idx] = (images[idx] - mean[idx]) / stddev[idx]\n        elif len(images.shape) == 4:\n            out_images[:, idx] = (images[:, idx] - mean[idx]) / stddev[idx]\n        else:\n            raise NotImplementedError(\n                f\"Unsupported dimension {len(images.shape)}\")\n    return out_images", "\n\ndef _get_param_spatial_crop(scale,\n                            ratio,\n                            height,\n                            width,\n                            num_repeat=10,\n                            log_scale=True,\n                            switch_hw=False):\n    \"\"\"\n    Given scale, ratio, height and width, return sampled coordinates of the videos.\n    \"\"\"\n    for _ in range(num_repeat):\n        area = height * width\n        target_area = random.uniform(*scale) * area\n        if log_scale:\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n        else:\n            aspect_ratio = random.uniform(*ratio)\n\n        w = int(round(math.sqrt(target_area * aspect_ratio)))\n        h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n        if np.random.uniform() < 0.5 and switch_hw:\n            w, h = h, w\n\n        if 0 < w <= width and 0 < h <= height:\n            i = random.randint(0, height - h)\n            j = random.randint(0, width - w)\n            return i, j, h, w\n\n    # Fallback to central crop\n    in_ratio = float(width) / float(height)\n    if in_ratio < min(ratio):\n        w = width\n        h = int(round(w / min(ratio)))\n    elif in_ratio > max(ratio):\n        h = height\n        w = int(round(h * max(ratio)))\n    else:  # whole image\n        w = width\n        h = height\n    i = (height - h) // 2\n    j = (width - w) // 2\n    return i, j, h, w", "\n\ndef random_resized_crop(\n        images,\n        target_height,\n        target_width,\n        scale=(0.8, 1.0),\n        ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    Crop the given images to random size and aspect ratio. A crop of random\n    size (default: of 0.08 to 1.0) of the original size and a random aspect\n    ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This\n    crop is finally resized to given size. This is popularly used to train the\n    Inception networks.\n\n    Args:\n        images: Images to perform resizing and cropping.\n        target_height: Desired height after cropping.\n        target_width: Desired width after cropping.\n        scale: Scale range of Inception-style area based random resizing.\n        ratio: Aspect ratio range of Inception-style area based random resizing.\n    \"\"\"\n\n    height = images.shape[2]\n    width = images.shape[3]\n\n    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)\n    cropped = images[:, :, i:i + h, j:j + w]\n    return torch.nn.functional.interpolate(\n        cropped,\n        size=(target_height, target_width),\n        mode=\"bilinear\",\n        align_corners=False,\n    )", "\n\ndef random_resized_crop_with_shift(\n        images,\n        target_height,\n        target_width,\n        scale=(0.8, 1.0),\n        ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    This is similar to random_resized_crop. However, it samples two different\n    boxes (for cropping) for the first and last frame. It then linearly\n    interpolates the two boxes for other frames.\n\n    Args:\n        images: Images to perform resizing and cropping.\n        target_height: Desired height after cropping.\n        target_width: Desired width after cropping.\n        scale: Scale range of Inception-style area based random resizing.\n        ratio: Aspect ratio range of Inception-style area based random resizing.\n    \"\"\"\n    t = images.shape[1]\n    height = images.shape[2]\n    width = images.shape[3]\n\n    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)\n    i_, j_, h_, w_ = _get_param_spatial_crop(scale, ratio, height, width)\n    i_s = [int(i) for i in torch.linspace(i, i_, steps=t).tolist()]\n    j_s = [int(i) for i in torch.linspace(j, j_, steps=t).tolist()]\n    h_s = [int(i) for i in torch.linspace(h, h_, steps=t).tolist()]\n    w_s = [int(i) for i in torch.linspace(w, w_, steps=t).tolist()]\n    out = torch.zeros((3, t, target_height, target_width))\n    for ind in range(t):\n        out[:, ind:ind + 1, :, :] = torch.nn.functional.interpolate(\n            images[:, ind:ind + 1, i_s[ind]:i_s[ind] + h_s[ind],\n                   j_s[ind]:j_s[ind] + w_s[ind], ],\n            size=(target_height, target_width),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n    return out", "\n\ndef create_random_augment(\n    input_size,\n    auto_augment=None,\n    interpolation=\"bilinear\",\n):\n    \"\"\"\n    Get video randaug transform.\n\n    Args:\n        input_size: The size of the input video in tuple.\n        auto_augment: Parameters for randaug. An example:\n            \"rand-m7-n4-mstd0.5-inc1\" (m is the magnitude and n is the number\n            of operations to apply).\n        interpolation: Interpolation method.\n    \"\"\"\n    if isinstance(input_size, tuple):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if auto_augment:\n        assert isinstance(auto_augment, str)\n        if isinstance(img_size, tuple):\n            img_size_min = min(img_size)\n        else:\n            img_size_min = img_size\n        aa_params = {\"translate_const\": int(img_size_min * 0.45)}\n        if interpolation and interpolation != \"random\":\n            aa_params[\"interpolation\"] = _pil_interp(interpolation)\n        if auto_augment.startswith(\"rand\"):\n            return transforms.Compose(\n                [rand_augment_transform(auto_augment, aa_params)])\n    raise NotImplementedError", "\n\ndef random_sized_crop_img(\n        im,\n        size,\n        jitter_scale=(0.08, 1.0),\n        jitter_aspect=(3.0 / 4.0, 4.0 / 3.0),\n        max_iter=10,\n):\n    \"\"\"\n    Performs Inception-style cropping (used for training).\n    \"\"\"\n    assert (len(\n        im.shape) == 3), \"Currently only support image for random_sized_crop\"\n    h, w = im.shape[1:3]\n    i, j, h, w = _get_param_spatial_crop(\n        scale=jitter_scale,\n        ratio=jitter_aspect,\n        height=h,\n        width=w,\n        num_repeat=max_iter,\n        log_scale=False,\n        switch_hw=True,\n    )\n    cropped = im[:, i:i + h, j:j + w]\n    return torch.nn.functional.interpolate(\n        cropped.unsqueeze(0),\n        size=(size, size),\n        mode=\"bilinear\",\n        align_corners=False,\n    ).squeeze(0)", "\n\n# The following code are modified based on timm lib, we will replace the following\n# contents with dependency from PyTorchVideo.\n# https://github.com/facebookresearch/pytorchvideo\nclass RandomResizedCropAndInterpolation:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(\n            self,\n            size,\n            scale=(0.08, 1.0),\n            ratio=(3.0 / 4.0, 4.0 / 3.0),\n            interpolation=\"bilinear\",\n    ):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            print(\"range should be of kind (min, max)\")\n\n        if interpolation == \"random\":\n            self.interpolation = _RANDOM_INTERPOLATION\n        else:\n            self.interpolation = _pil_interp(interpolation)\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for _ in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if in_ratio < min(ratio):\n            w = img.size[0]\n            h = int(round(w / min(ratio)))\n        elif in_ratio > max(ratio):\n            h = img.size[1]\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolation = random.choice(self.interpolation)\n        else:\n            interpolation = self.interpolation\n        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n\n    def __repr__(self):\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolate_str = \" \".join(\n                [_pil_interpolation_to_str[x] for x in self.interpolation])\n        else:\n            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + \"(size={0}\".format(self.size)\n        format_string += \", scale={0}\".format(\n            tuple(round(s, 4) for s in self.scale))\n        format_string += \", ratio={0}\".format(\n            tuple(round(r, 4) for r in self.ratio))\n        format_string += \", interpolation={0})\".format(interpolate_str)\n        return format_string", "\n\ndef transforms_imagenet_train(\n    img_size=224,\n    scale=None,\n    ratio=None,\n    hflip=0.5,\n    vflip=0.0,\n    color_jitter=0.4,\n    auto_augment=None,\n    interpolation=\"random\",\n    use_prefetcher=False,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    re_prob=0.0,\n    re_mode=\"const\",\n    re_count=1,\n    re_num_splits=0,\n    separate=False,\n):\n    \"\"\"\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the 'clean' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    \"\"\"\n    if isinstance(img_size, tuple):\n        img_size = img_size[-2:]\n    else:\n        img_size = img_size\n\n    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range\n    ratio = tuple(ratio\n                  or (3.0 / 4.0, 4.0 / 3.0))  # default imagenet ratio range\n    primary_tfl = [\n        RandomResizedCropAndInterpolation(\n            img_size, scale=scale, ratio=ratio, interpolation=interpolation)\n    ]\n    if hflip > 0.0:\n        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]\n    if vflip > 0.0:\n        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]\n\n    secondary_tfl = []\n    if auto_augment:\n        assert isinstance(auto_augment, str)\n        if isinstance(img_size, tuple):\n            img_size_min = min(img_size)\n        else:\n            img_size_min = img_size\n        aa_params = dict(\n            translate_const=int(img_size_min * 0.45),\n            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n        )\n        if interpolation and interpolation != \"random\":\n            aa_params[\"interpolation\"] = _pil_interp(interpolation)\n        if auto_augment.startswith(\"rand\"):\n            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n        elif auto_augment.startswith(\"augmix\"):\n            raise NotImplementedError(\"Augmix not implemented\")\n        else:\n            raise NotImplementedError(\"Auto aug not implemented\")\n    elif color_jitter is not None:\n        # color jitter is enabled when not using AA\n        if isinstance(color_jitter, (list, tuple)):\n            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n            # or 4 if also augmenting hue\n            assert len(color_jitter) in (3, 4)\n        else:\n            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue\n            color_jitter = (float(color_jitter), ) * 3\n        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n\n    final_tfl = []\n    final_tfl += [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=torch.tensor(mean), std=torch.tensor(std)),\n    ]\n    if re_prob > 0.0:\n        final_tfl.append(\n            RandomErasing(\n                re_prob,\n                mode=re_mode,\n                max_count=re_count,\n                num_splits=re_num_splits,\n                device=\"cpu\",\n                cube=False,\n            ))\n\n    if separate:\n        return (\n            transforms.Compose(primary_tfl),\n            transforms.Compose(secondary_tfl),\n            transforms.Compose(final_tfl),\n        )\n    else:\n        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)", "\n\n############################################################################################################\n############################################################################################################\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms\n    Args:\n    transforms (list of ``Transform`` objects): list of transforms\n    to compose\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, clip):\n        for t in self.transforms:\n            clip = t(clip)\n        return clip", "\n\nclass RandomHorizontalFlip(object):\n    \"\"\"Horizontally flip the list of given images randomly\n    with a probability 0.5\n    \"\"\"\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n        Returns:\n        PIL.Image or numpy.ndarray: Randomly flipped clip\n        \"\"\"\n        if random.random() < 0.5:\n            if isinstance(clip[0], np.ndarray):\n                return [np.fliplr(img) for img in clip]\n            elif isinstance(clip[0], PIL.Image.Image):\n                return [\n                    img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip\n                ]\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                                ' but got list of {0}'.format(type(clip[0])))\n        return clip", "\n\nclass RandomResize(object):\n    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n    The larger the original image is, the more times it takes to\n    interpolate\n    Args:\n    interpolation (str): Can be one of 'nearest', 'bilinear'\n    defaults to nearest\n    size (tuple): (widht, height)\n    \"\"\"\n\n    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):\n        self.ratio = ratio\n        self.interpolation = interpolation\n\n    def __call__(self, clip):\n        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])\n\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n\n        new_w = int(im_w * scaling_factor)\n        new_h = int(im_h * scaling_factor)\n        new_size = (new_w, new_h)\n        resized = FF.resize_clip(\n            clip, new_size, interpolation=self.interpolation)\n        return resized", "\n\nclass Resize(object):\n    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n    The larger the original image is, the more times it takes to\n    interpolate\n    Args:\n    interpolation (str): Can be one of 'nearest', 'bilinear'\n    defaults to nearest\n    size (tuple): (widht, height)\n    \"\"\"\n\n    def __init__(self, size, interpolation='nearest'):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, clip):\n        resized = FF.resize_clip(\n            clip, self.size, interpolation=self.interpolation)\n        return resized", "\n\nclass RandomCrop(object):\n    \"\"\"Extract random crop at the same location for a list of images\n    Args:\n    size (sequence or int): Desired output size for the\n    crop in format (h, w)\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            size = (size, size)\n\n        self.size = size\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        h, w = self.size\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        if w > im_w or h > im_h:\n            error_msg = (\n                'Initial image size should be larger then '\n                'cropped size but got cropped sizes : ({w}, {h}) while '\n                'initial image is ({im_w}, {im_h})'.format(\n                    im_w=im_w, im_h=im_h, w=w, h=h))\n            raise ValueError(error_msg)\n\n        x1 = random.randint(0, im_w - w)\n        y1 = random.randint(0, im_h - h)\n        cropped = FF.crop_clip(clip, y1, x1, h, w)\n\n        return cropped", "\n\nclass ThreeCrop(object):\n    \"\"\"Extract random crop at the same location for a list of images\n    Args:\n    size (sequence or int): Desired output size for the\n    crop in format (h, w)\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            size = (size, size)\n\n        self.size = size\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        h, w = self.size\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        if w != im_w and h != im_h:\n            clip = FF.resize_clip(clip, self.size, interpolation=\"bilinear\")\n            im_h, im_w, im_c = clip[0].shape\n\n        step = np.max((np.max((im_w, im_h)) - self.size[0]) // 2, 0)\n        cropped = []\n        for i in range(3):\n            if (im_h > self.size[0]):\n                x1 = 0\n                y1 = i * step\n                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))\n            else:\n                x1 = i * step\n                y1 = 0\n                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))\n        return cropped", "\n\nclass RandomRotation(object):\n    \"\"\"Rotate entire clip randomly by a random angle within\n    given bounds\n    Args:\n    degrees (sequence or int): Range of degrees to select from\n    If degrees is a number instead of sequence like (min, max),\n    the range of degrees, will be (-degrees, +degrees).\n    \"\"\"\n\n    def __init__(self, degrees):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError('If degrees is a single number,'\n                                 'must be positive')\n            degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError('If degrees is a sequence,'\n                                 'it must be of len 2.')\n\n        self.degrees = degrees\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        import skimage\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        if isinstance(clip[0], np.ndarray):\n            rotated = [skimage.transform.rotate(img, angle) for img in clip]\n        elif isinstance(clip[0], PIL.Image.Image):\n            rotated = [img.rotate(angle) for img in clip]\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n\n        return rotated", "\n\nclass CenterCrop(object):\n    \"\"\"Extract center crop at the same location for a list of images\n    Args:\n    size (sequence or int): Desired output size for the\n    crop in format (h, w)\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            size = (size, size)\n\n        self.size = size\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"\n        h, w = self.size\n        if isinstance(clip[0], np.ndarray):\n            im_h, im_w, im_c = clip[0].shape\n        elif isinstance(clip[0], PIL.Image.Image):\n            im_w, im_h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        if w > im_w or h > im_h:\n            error_msg = (\n                'Initial image size should be larger then '\n                'cropped size but got cropped sizes : ({w}, {h}) while '\n                'initial image is ({im_w}, {im_h})'.format(\n                    im_w=im_w, im_h=im_h, w=w, h=h))\n            raise ValueError(error_msg)\n\n        x1 = int(round((im_w - w) / 2.))\n        y1 = int(round((im_h - h) / 2.))\n        cropped = FF.crop_clip(clip, y1, x1, h, w)\n\n        return cropped", "\n\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip\n    Args:\n    brightness (float): How much to jitter brightness. brightness_factor\n    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n    contrast (float): How much to jitter contrast. contrast_factor\n    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n    saturation (float): How much to jitter saturation. saturation_factor\n    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n    [-hue, hue]. Should be >=0 and <= 0.5.\n    \"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        self.saturation = saturation\n        self.hue = hue\n\n    def get_params(self, brightness, contrast, saturation, hue):\n        if brightness > 0:\n            brightness_factor = random.uniform(\n                max(0, 1 - brightness), 1 + brightness)\n        else:\n            brightness_factor = None\n\n        if contrast > 0:\n            contrast_factor = random.uniform(\n                max(0, 1 - contrast), 1 + contrast)\n        else:\n            contrast_factor = None\n\n        if saturation > 0:\n            saturation_factor = random.uniform(\n                max(0, 1 - saturation), 1 + saturation)\n        else:\n            saturation_factor = None\n\n        if hue > 0:\n            hue_factor = random.uniform(-hue, hue)\n        else:\n            hue_factor = None\n        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n        clip (list): list of PIL.Image\n        Returns:\n        list PIL.Image : list of transformed PIL.Image\n        \"\"\"\n        if isinstance(clip[0], np.ndarray):\n            raise TypeError(\n                'Color jitter not yet implemented for numpy arrays')\n        elif isinstance(clip[0], PIL.Image.Image):\n            brightness, contrast, saturation, hue = self.get_params(\n                self.brightness, self.contrast, self.saturation, self.hue)\n\n            # Create img transform function sequence\n            img_transforms = []\n            if brightness is not None:\n                img_transforms.append(\n                    lambda img: torchvision.transforms.functional.\n                    adjust_brightness(img, brightness))\n            if saturation is not None:\n                img_transforms.append(\n                    lambda img: torchvision.transforms.functional.\n                    adjust_saturation(img, saturation))\n            if hue is not None:\n                img_transforms.append(lambda img: torchvision.transforms.\n                                      functional.adjust_hue(img, hue))\n            if contrast is not None:\n                img_transforms.append(\n                    lambda img: torchvision.transforms.functional.\n                    adjust_contrast(img, contrast))\n            random.shuffle(img_transforms)\n\n            # Apply to all images\n            jittered_clip = []\n            for img in clip:\n                for func in img_transforms:\n                    jittered_img = func(img)\n                jittered_clip.append(jittered_img)\n\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n        return jittered_clip", "\n\nclass Normalize(object):\n    \"\"\"Normalize a clip with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    .. note::\n        This transform acts out of place, i.e., it does not mutates the input tensor.\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, clip):\n        \"\"\"\n        Args:\n            clip (Tensor): Tensor clip of size (T, C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized Tensor clip.\n        \"\"\"\n        return FF.normalize(clip, self.mean, self.std)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(\n            self.mean, self.std)", ""]}
{"filename": "dataset/rand_augment.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nAutoAugment, RandAugment, and AugMix for PyTorch\nThis code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA", "This code implements the searched ImageNet policies with various tweaks and\nimprovements and does not include any of the search code. AA and RA\nImplementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\nAugMix adapted from:\n    https://github.com/google-research/augmix\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data\n    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection", "    https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection\n    https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation...\n    https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and\n    Uncertainty https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"", "Hacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport math\nimport random\nimport re\n\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps", "import PIL\nfrom PIL import Image, ImageEnhance, ImageOps\n\n_PIL_VER = tuple([int(x) for x in PIL.__version__.split(\".\")[:2]])\n\n_FILL = (128, 128, 128)\n\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0", "# augmentation scheme.\n_MAX_LEVEL = 10.0\n\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n", "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n", "\n\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")\n    kwargs[\"resample\"] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0),\n                         **kwargs)", "def shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0),\n                         **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0),\n                         **kwargs)", "\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),\n                         **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),\n                         **kwargs)", "\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),\n                         **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),\n                         **kwargs)", "def translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),\n                         **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),\n                         **kwargs)", "\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0],\n            -rotn_center[1] - post_trans[1],\n            matrix,\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\"resample\"])", "\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)", "\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img", "\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img", "\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)", "def contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)", "def brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v", "def _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level, )", "\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return ((level / _MAX_LEVEL) * 1.8 + 0.1, )\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend\n    # range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * 0.9\n    level = 1.0 + _randomly_negate(level)\n    return (level, )", "\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return (level, )\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level, )", "\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\"translate_const\"]\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return (level, )\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level, )", "def _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\"translate_pct\", 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return (level, )\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4), )", "def _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4), )\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return (4 - _posterize_level_to_arg(level, hparams)[0], )", "\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 4) + 4, )\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256), )", "\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return (int((level / _MAX_LEVEL) * 256), )\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return (256 - _solarize_level_to_arg(level, _hparams)[0], )", "\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return (int((level / _MAX_LEVEL) * 110), )\n\n\nLEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,", "    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,", "    \"Solarize\": _solarize_level_to_arg,\n    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n    \"SolarizeAdd\": _solarize_add_level_to_arg,\n    \"Color\": _enhance_level_to_arg,\n    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n    \"Contrast\": _enhance_level_to_arg,\n    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n    \"Brightness\": _enhance_level_to_arg,\n    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,", "    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"Sharpness\": _enhance_level_to_arg,\n    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n    \"ShearX\": _shear_level_to_arg,\n    \"ShearY\": _shear_level_to_arg,\n    \"TranslateX\": _translate_abs_level_to_arg,\n    \"TranslateY\": _translate_abs_level_to_arg,\n    \"TranslateXRel\": _translate_rel_level_to_arg,\n    \"TranslateYRel\": _translate_rel_level_to_arg,\n}", "    \"TranslateYRel\": _translate_rel_level_to_arg,\n}\n\nNAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,", "    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,\n    \"SolarizeAdd\": solarize_add,\n    \"Color\": color,\n    \"ColorIncreasing\": color,\n    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,", "    \"Contrast\": contrast,\n    \"ContrastIncreasing\": contrast,\n    \"Brightness\": brightness,\n    \"BrightnessIncreasing\": brightness,\n    \"Sharpness\": sharpness,\n    \"SharpnessIncreasing\": sharpness,\n    \"ShearX\": shear_x,\n    \"ShearY\": shear_y,\n    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,", "    \"TranslateX\": translate_x_abs,\n    \"TranslateY\": translate_y_abs,\n    \"TranslateXRel\": translate_x_rel,\n    \"TranslateYRel\": translate_y_rel,\n}\n\n\nclass AugmentOp:\n    \"\"\"\n    Apply for video.\n    \"\"\"\n\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = {\n            \"fillcolor\":\n            hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n            \"resample\":\n            hparams[\"interpolation\"]\n            if \"interpolation\" in hparams else _RANDOM_INTERPOLATION,\n        }\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        # NOTE This is my own hack, being tested, not in papers or reference impls.\n        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\n    def __call__(self, img_list):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img_list\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = (\n            self.level_fn(magnitude, self.hparams)\n            if self.level_fn is not None else ())\n\n        if isinstance(img_list, list):\n            return [\n                self.aug_fn(img, *level_args, **self.kwargs)\n                for img in img_list\n            ]\n        else:\n            return self.aug_fn(img_list, *level_args, **self.kwargs)", "\n\n_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",", "    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",\n    \"Brightness\",\n    \"Sharpness\",\n    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",", "    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",", "    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",\n    \"BrightnessIncreasing\",\n    \"SharpnessIncreasing\",\n    \"ShearX\",\n    \"ShearY\",", "    \"ShearX\",\n    \"ShearY\",\n    \"TranslateXRel\",\n    \"TranslateYRel\",\n]\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,", "_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,", "    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,\n    \"SolarizeAdd\": 0.005,\n    \"Contrast\": 0.005,\n    \"Brightness\": 0.005,\n    \"Equalize\": 0.005,\n    \"Posterize\": 0,\n    \"Invert\": 0,\n}\n", "}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs", "\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]\n", "\n\nclass RandAugment:\n\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img", "\n\ndef rand_augment_transform(config_str, hparams):\n    \"\"\"\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n\n    Create a RandAugment transform\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        'mstd' -  float std deviation of magnitude noise applied\n        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    magnitude = _MAX_LEVEL  # default to _MAX_LEVEL for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    weight_idx = None  # default to no probability weights for op choice\n    transforms = _RAND_TRANSFORMS\n    config = config_str.split(\"-\")\n    assert config[0] == \"rand\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        elif key == \"inc\":\n            if bool(val):\n                transforms = _RAND_INCREASING_TRANSFORMS\n        elif key == \"m\":\n            magnitude = int(val)\n        elif key == \"n\":\n            num_layers = int(val)\n        elif key == \"w\":\n            weight_idx = int(val)\n        else:\n            assert NotImplementedError\n    ra_ops = rand_augment_ops(\n        magnitude=magnitude, hparams=hparams, transforms=transforms)\n    choice_weights = (None if weight_idx is None else\n                      _select_rand_weights(weight_idx))\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)", ""]}
{"filename": "dataset/random_erasing.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\"\"\"\nThis implementation is based on\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\npulished under an Apache License 2.0.\n\nCOMMENT FROM ORIGINAL:\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman", "Copyright Zhun Zhong & Liang Zheng\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport random\n\nimport torch\n\n\ndef _get_pixels(per_pixel,\n                rand_color,\n                patch_size,\n                dtype=torch.float32,\n                device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype,\n                           device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)", "\ndef _get_pixels(per_pixel,\n                rand_color,\n                patch_size,\n                dtype=torch.float32,\n                device=\"cuda\"):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype,\n                           device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)", "\n\nclass RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        probability=0.5,\n        min_area=0.02,\n        max_area=1 / 3,\n        min_aspect=0.3,\n        max_aspect=None,\n        mode=\"const\",\n        min_count=1,\n        max_count=None,\n        num_splits=0,\n        device=\"cuda\",\n        cube=True,\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        self.cube = cube\n        if mode == \"rand\":\n            self.rand_color = True  # per block random normal\n        elif mode == \"pixel\":\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \"const\"\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count if self.min_count == self.max_count else\n            random.randint(self.min_count, self.max_count))\n        for _ in range(count):\n            for _ in range(10):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area /\n                    count)\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top:top + h, left:left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def _erase_cube(\n        self,\n        img,\n        batch_start,\n        batch_size,\n        chan,\n        img_h,\n        img_w,\n        dtype,\n    ):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = (\n            self.min_count if self.min_count == self.max_count else\n            random.randint(self.min_count, self.max_count))\n        for _ in range(count):\n            for _ in range(100):\n                target_area = (\n                    random.uniform(self.min_area, self.max_area) * area /\n                    count)\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    for i in range(batch_start, batch_size):\n                        img_instance = img[i]\n                        img_instance[:, top:top + h,\n                                     left:left + w] = _get_pixels(\n                                         self.per_pixel,\n                                         self.rand_color,\n                                         (chan, h, w),\n                                         dtype=dtype,\n                                         device=self.device,\n                                     )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = (\n                batch_size // self.num_splits if self.num_splits > 1 else 0)\n            if self.cube:\n                self._erase_cube(\n                    input,\n                    batch_start,\n                    batch_size,\n                    chan,\n                    img_h,\n                    img_w,\n                    input.dtype,\n                )\n            else:\n                for i in range(batch_start, batch_size):\n                    self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input", ""]}
{"filename": "dataset/pretrain_datasets.py", "chunked_list": ["import os\nimport random\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom .loader import get_image_loader, get_video_loader\nfrom .masking_generator import (", "from .loader import get_image_loader, get_video_loader\nfrom .masking_generator import (\n    RunningCellMaskingGenerator,\n    TubeMaskingGenerator,\n)\nfrom .transforms import (\n    GroupMultiScaleCrop,\n    GroupNormalize,\n    Stack,\n    ToTorchFormatTensor,", "    Stack,\n    ToTorchFormatTensor,\n)\n\n\nclass DataAugmentationForVideoMAEv2(object):\n\n    def __init__(self, args):\n        self.input_mean = [0.485, 0.456, 0.406]\n        self.input_std = [0.229, 0.224, 0.225]\n        div = True\n        roll = False\n        normalize = GroupNormalize(self.input_mean, self.input_std)\n        self.train_augmentation = GroupMultiScaleCrop(args.input_size,\n                                                      [1, .875, .75, .66])\n        self.transform = transforms.Compose([\n            self.train_augmentation,\n            Stack(roll=roll),\n            ToTorchFormatTensor(div=div),\n            normalize,\n        ])\n        if args.mask_type == 'tube':\n            self.encoder_mask_map_generator = TubeMaskingGenerator(\n                args.window_size, args.mask_ratio)\n        else:\n            raise NotImplementedError(\n                'Unsupported encoder masking strategy type.')\n        if args.decoder_mask_ratio > 0.:\n            if args.decoder_mask_type == 'run_cell':\n                self.decoder_mask_map_generator = RunningCellMaskingGenerator(\n                    args.window_size, args.decoder_mask_ratio)\n            else:\n                raise NotImplementedError(\n                    'Unsupported decoder masking strategy type.')\n\n    def __call__(self, images):\n        process_data, _ = self.transform(images)\n        encoder_mask_map = self.encoder_mask_map_generator()\n        if hasattr(self, 'decoder_mask_map_generator'):\n            decoder_mask_map = self.decoder_mask_map_generator()\n        else:\n            decoder_mask_map = 1 - encoder_mask_map\n        return process_data, encoder_mask_map, decoder_mask_map\n\n    def __repr__(self):\n        repr = \"(DataAugmentationForVideoMAEv2,\\n\"\n        repr += \"  transform = %s,\\n\" % str(self.transform)\n        repr += \"  Encoder Masking Generator = %s,\\n\" % str(\n            self.encoder_mask_map_generator)\n        if hasattr(self, 'decoder_mask_map_generator'):\n            repr += \"  Decoder Masking Generator = %s,\\n\" % str(\n                self.decoder_mask_map_generator)\n        else:\n            repr += \"  Do not use decoder masking,\\n\"\n        repr += \")\"\n        return repr", "\n\nclass HybridVideoMAE(torch.utils.data.Dataset):\n    \"\"\"Load your own videomae pretraining dataset.\n    Parameters\n    ----------\n    root : str, required.\n        Path to the root folder storing the dataset.\n    setting : str, required.\n        A text file describing the dataset, each line per video sample.\n        There are four items in each line:\n        (1) video path; (2) start_idx, (3) total frames and (4) video label.\n        for pre-train video data\n            total frames < 0, start_idx and video label meaningless\n        for pre-train rawframe data\n            video label meaningless\n    train : bool, default True.\n        Whether to load the training or validation set.\n    test_mode : bool, default False.\n        Whether to perform evaluation on the test set.\n        Usually there is three-crop or ten-crop evaluation strategy involved.\n    name_pattern : str, default 'img_{:05}.jpg'.\n        The naming pattern of the decoded video frames.\n        For example, img_00012.jpg.\n    video_ext : str, default 'mp4'.\n        If video_loader is set to True, please specify the video format accordinly.\n    is_color : bool, default True.\n        Whether the loaded image is color or grayscale.\n    modality : str, default 'rgb'.\n        Input modalities, we support only rgb video frames for now.\n        Will add support for rgb difference image and optical flow image later.\n    num_segments : int, default 1.\n        Number of segments to evenly divide the video into clips.\n        A useful technique to obtain global video-level information.\n        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.\n    num_crop : int, default 1.\n        Number of crops for each image. default is 1.\n        Common choices are three crops and ten crops during evaluation.\n    new_length : int, default 1.\n        The length of input video clip. Default is a single image, but it can be multiple video frames.\n        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.\n    new_step : int, default 1.\n        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.\n        new_step=2 means we will extract a video clip of every other frame.\n    transform : function, default None.\n        A function that takes data and label and transforms them.\n    temporal_jitter : bool, default False.\n        Whether to temporally jitter if new_step > 1.\n    lazy_init : bool, default False.\n        If set to True, build a dataset instance without loading any dataset.\n    num_sample : int, default 1.\n        Number of sampled views for Repeated Augmentation.\n    \"\"\"\n\n    def __init__(self,\n                 root,\n                 setting,\n                 train=True,\n                 test_mode=False,\n                 name_pattern='img_{:05}.jpg',\n                 video_ext='mp4',\n                 is_color=True,\n                 modality='rgb',\n                 num_segments=1,\n                 num_crop=1,\n                 new_length=1,\n                 new_step=1,\n                 transform=None,\n                 temporal_jitter=False,\n                 lazy_init=False,\n                 num_sample=1):\n\n        super(HybridVideoMAE, self).__init__()\n        self.root = root\n        self.setting = setting\n        self.train = train\n        self.test_mode = test_mode\n        self.is_color = is_color\n        self.modality = modality\n        self.num_segments = num_segments\n        self.num_crop = num_crop\n        self.new_length = new_length\n        self.new_step = new_step\n        self.skip_length = self.new_length * self.new_step\n        self.temporal_jitter = temporal_jitter\n        self.name_pattern = name_pattern\n        self.video_ext = video_ext\n        self.transform = transform\n        self.lazy_init = lazy_init\n        self.num_sample = num_sample\n\n        # NOTE:\n        # for hybrid train\n        # different frame naming formats are used for different datasets\n        # should MODIFY the fname_tmpl to your own situation\n        self.ava_fname_tmpl = 'image_{:06}.jpg'\n        self.ssv2_fname_tmpl = 'img_{:05}.jpg'\n\n        # NOTE:\n        # we set sampling_rate = 2 for ssv2\n        # thus being consistent with the fine-tuning stage\n        # Note that the ssv2 we use is decoded to frames at 12 fps;\n        # if decoded at 24 fps, the sample interval should be 4.\n        self.ssv2_skip_length = self.new_length * 2\n        self.orig_skip_length = self.skip_length\n\n        self.video_loader = get_video_loader()\n        self.image_loader = get_image_loader()\n\n        if not self.lazy_init:\n            self.clips = self._make_dataset(root, setting)\n            if len(self.clips) == 0:\n                raise (\n                    RuntimeError(\"Found 0 video clips in subfolders of: \" +\n                                 root + \"\\n\"\n                                 \"Check your data directory (opt.data-dir).\"))\n\n    def __getitem__(self, index):\n        try:\n            video_name, start_idx, total_frame = self.clips[index]\n            self.skip_length = self.orig_skip_length\n\n            if total_frame < 0:\n                decord_vr = self.video_loader(video_name)\n                duration = len(decord_vr)\n\n                segment_indices, skip_offsets = self._sample_train_indices(\n                    duration)\n                frame_id_list = self.get_frame_id_list(duration,\n                                                       segment_indices,\n                                                       skip_offsets)\n                video_data = decord_vr.get_batch(frame_id_list).asnumpy()\n                images = [\n                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')\n                    for vid, _ in enumerate(frame_id_list)\n                ]\n\n            else:\n                # ssv2 & ava & other rawframe dataset\n                if 'SomethingV2' in video_name:\n                    self.skip_length = self.ssv2_skip_length\n                    fname_tmpl = self.ssv2_fname_tmpl\n                elif 'AVA2.2' in video_name:\n                    fname_tmpl = self.ava_fname_tmpl\n                else:\n                    fname_tmpl = self.name_pattern\n\n                segment_indices, skip_offsets = self._sample_train_indices(\n                    total_frame)\n                frame_id_list = self.get_frame_id_list(total_frame,\n                                                       segment_indices,\n                                                       skip_offsets)\n\n                images = []\n                for idx in frame_id_list:\n                    frame_fname = os.path.join(\n                        video_name, fname_tmpl.format(idx + start_idx))\n                    img = self.image_loader(frame_fname)\n                    img = Image.fromarray(img)\n                    images.append(img)\n\n        except Exception as e:\n            print(\"Failed to load video from {} with error {}\".format(\n                video_name, e))\n            index = random.randint(0, len(self.clips) - 1)\n            return self.__getitem__(index)\n\n        if self.num_sample > 1:\n            process_data_list = []\n            encoder_mask_list = []\n            decoder_mask_list = []\n            for _ in range(self.num_sample):\n                process_data, encoder_mask, decoder_mask = self.transform(\n                    (images, None))\n                process_data = process_data.view(\n                    (self.new_length, 3) + process_data.size()[-2:]).transpose(\n                        0, 1)\n                process_data_list.append(process_data)\n                encoder_mask_list.append(encoder_mask)\n                decoder_mask_list.append(decoder_mask)\n            return process_data_list, encoder_mask_list, decoder_mask_list\n        else:\n            process_data, encoder_mask, decoder_mask = self.transform(\n                (images, None))\n            # T*C,H,W -> T,C,H,W -> C,T,H,W\n            process_data = process_data.view(\n                (self.new_length, 3) + process_data.size()[-2:]).transpose(\n                    0, 1)\n            return process_data, encoder_mask, decoder_mask\n\n    def __len__(self):\n        return len(self.clips)\n\n    def _make_dataset(self, root, setting):\n        if not os.path.exists(setting):\n            raise (RuntimeError(\n                \"Setting file %s doesn't exist. Check opt.train-list and opt.val-list. \"\n                % (setting)))\n        clips = []\n        with open(setting) as split_f:\n            data = split_f.readlines()\n            for line in data:\n                line_info = line.split(' ')\n                # line format: video_path, video_duration, video_label\n                if len(line_info) < 2:\n                    raise (RuntimeError(\n                        'Video input format is not correct, missing one or more element. %s'\n                        % line))\n                clip_path = os.path.join(root, line_info[0])\n                start_idx = int(line_info[1])\n                total_frame = int(line_info[2])\n                item = (clip_path, start_idx, total_frame)\n                clips.append(item)\n        return clips\n\n    def _sample_train_indices(self, num_frames):\n        average_duration = (num_frames - self.skip_length +\n                            1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(\n                list(range(self.num_segments)), average_duration)\n            offsets = offsets + np.random.randint(\n                average_duration, size=self.num_segments)\n        elif num_frames > max(self.num_segments, self.skip_length):\n            offsets = np.sort(\n                np.random.randint(\n                    num_frames - self.skip_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments, ))\n\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.skip_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.skip_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def get_frame_id_list(self, duration, indices, skip_offsets):\n        frame_id_list = []\n        for seg_ind in indices:\n            offset = int(seg_ind)\n            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):\n                if offset + skip_offsets[i] <= duration:\n                    frame_id = offset + skip_offsets[i] - 1\n                else:\n                    frame_id = offset - 1\n                frame_id_list.append(frame_id)\n                if offset + self.new_step < duration:\n                    offset += self.new_step\n        return frame_id_list", "\n\nclass VideoMAE(torch.utils.data.Dataset):\n    \"\"\"Load your own videomae pretraining dataset.\n    Parameters\n    ----------\n    root : str, required.\n        Path to the root folder storing the dataset.\n    setting : str, required.\n        A text file describing the dataset, each line per video sample.\n        There are four items in each line:\n        (1) video path; (2) start_idx, (3) total frames and (4) video label.\n        for pre-train video data\n            total frames < 0, start_idx and video label meaningless\n        for pre-train rawframe data\n            video label meaningless\n    train : bool, default True.\n        Whether to load the training or validation set.\n    test_mode : bool, default False.\n        Whether to perform evaluation on the test set.\n        Usually there is three-crop or ten-crop evaluation strategy involved.\n    name_pattern : str, default 'img_{:05}.jpg'.\n        The naming pattern of the decoded video frames.\n        For example, img_00012.jpg.\n    video_ext : str, default 'mp4'.\n        If video_loader is set to True, please specify the video format accordinly.\n    is_color : bool, default True.\n        Whether the loaded image is color or grayscale.\n    modality : str, default 'rgb'.\n        Input modalities, we support only rgb video frames for now.\n        Will add support for rgb difference image and optical flow image later.\n    num_segments : int, default 1.\n        Number of segments to evenly divide the video into clips.\n        A useful technique to obtain global video-level information.\n        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.\n    num_crop : int, default 1.\n        Number of crops for each image. default is 1.\n        Common choices are three crops and ten crops during evaluation.\n    new_length : int, default 1.\n        The length of input video clip. Default is a single image, but it can be multiple video frames.\n        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.\n    new_step : int, default 1.\n        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.\n        new_step=2 means we will extract a video clip of every other frame.\n    transform : function, default None.\n        A function that takes data and label and transforms them.\n    temporal_jitter : bool, default False.\n        Whether to temporally jitter if new_step > 1.\n    lazy_init : bool, default False.\n        If set to True, build a dataset instance without loading any dataset.\n    num_sample : int, default 1.\n        Number of sampled views for Repeated Augmentation.\n    \"\"\"\n\n    def __init__(self,\n                 root,\n                 setting,\n                 train=True,\n                 test_mode=False,\n                 name_pattern='img_{:05}.jpg',\n                 video_ext='mp4',\n                 is_color=True,\n                 modality='rgb',\n                 num_segments=1,\n                 num_crop=1,\n                 new_length=1,\n                 new_step=1,\n                 transform=None,\n                 temporal_jitter=False,\n                 lazy_init=False,\n                 num_sample=1):\n\n        super(VideoMAE, self).__init__()\n        self.root = root\n        self.setting = setting\n        self.train = train\n        self.test_mode = test_mode\n        self.is_color = is_color\n        self.modality = modality\n        self.num_segments = num_segments\n        self.num_crop = num_crop\n        self.new_length = new_length\n        self.new_step = new_step\n        self.skip_length = self.new_length * self.new_step\n        self.temporal_jitter = temporal_jitter\n        self.name_pattern = name_pattern\n        self.video_ext = video_ext\n        self.transform = transform\n        self.lazy_init = lazy_init\n        self.num_sample = num_sample\n\n        self.video_loader = get_video_loader()\n        self.image_loader = get_image_loader()\n\n        if not self.lazy_init:\n            self.clips = self._make_dataset(root, setting)\n            if len(self.clips) == 0:\n                raise (\n                    RuntimeError(\"Found 0 video clips in subfolders of: \" +\n                                 root + \"\\n\"\n                                 \"Check your data directory (opt.data-dir).\"))\n\n    def __getitem__(self, index):\n        try:\n            video_name, start_idx, total_frame = self.clips[index]\n            if total_frame < 0:  # load video\n                decord_vr = self.video_loader(video_name)\n                duration = len(decord_vr)\n\n                segment_indices, skip_offsets = self._sample_train_indices(\n                    duration)\n                frame_id_list = self.get_frame_id_list(duration,\n                                                       segment_indices,\n                                                       skip_offsets)\n                video_data = decord_vr.get_batch(frame_id_list).asnumpy()\n                images = [\n                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')\n                    for vid, _ in enumerate(frame_id_list)\n                ]\n            else:  # load frames\n                segment_indices, skip_offsets = self._sample_train_indices(\n                    total_frame)\n                frame_id_list = self.get_frame_id_list(total_frame,\n                                                       segment_indices,\n                                                       skip_offsets)\n\n                images = []\n                for idx in frame_id_list:\n                    frame_fname = os.path.join(\n                        video_name, self.name_pattern.format(idx + start_idx))\n                    img = self.image_loader(frame_fname)\n                    img = Image.fromarray(img)\n                    images.append(img)\n\n        except Exception as e:\n            print(\"Failed to load video from {} with error {}\".format(\n                video_name, e))\n            index = random.randint(0, len(self.clips) - 1)\n            return self.__getitem__(index)\n\n        if self.num_sample > 1:\n            process_data_list = []\n            encoder_mask_list = []\n            decoder_mask_list = []\n            for _ in range(self.num_sample):\n                process_data, encoder_mask, decoder_mask = self.transform(\n                    (images, None))\n                process_data = process_data.view(\n                    (self.new_length, 3) + process_data.size()[-2:]).transpose(\n                        0, 1)\n                process_data_list.append(process_data)\n                encoder_mask_list.append(encoder_mask)\n                decoder_mask_list.append(decoder_mask)\n            return process_data_list, encoder_mask_list, decoder_mask_list\n        else:\n            process_data, encoder_mask, decoder_mask = self.transform(\n                (images, None))\n            # T*C,H,W -> T,C,H,W -> C,T,H,W\n            process_data = process_data.view(\n                (self.new_length, 3) + process_data.size()[-2:]).transpose(\n                    0, 1)\n            return process_data, encoder_mask, decoder_mask\n\n    def __len__(self):\n        return len(self.clips)\n\n    def _make_dataset(self, root, setting):\n        if not os.path.exists(setting):\n            raise (RuntimeError(\n                \"Setting file %s doesn't exist. Check opt.train-list and opt.val-list. \"\n                % (setting)))\n        clips = []\n        with open(setting) as split_f:\n            data = split_f.readlines()\n            for line in data:\n                line_info = line.split(' ')\n                # line format: video_path, start_idx, total_frames\n                if len(line_info) < 3:\n                    raise (RuntimeError(\n                        'Video input format is not correct, missing one or more element. %s'\n                        % line))\n                clip_path = os.path.join(root, line_info[0])\n                start_idx = int(line_info[1])\n                total_frame = int(line_info[2])\n                item = (clip_path, start_idx, total_frame)\n                clips.append(item)\n        return clips\n\n    def _sample_train_indices(self, num_frames):\n        average_duration = (num_frames - self.skip_length +\n                            1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(\n                list(range(self.num_segments)), average_duration)\n            offsets = offsets + np.random.randint(\n                average_duration, size=self.num_segments)\n        elif num_frames > max(self.num_segments, self.skip_length):\n            offsets = np.sort(\n                np.random.randint(\n                    num_frames - self.skip_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments, ))\n\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.skip_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.skip_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def get_frame_id_list(self, duration, indices, skip_offsets):\n        frame_id_list = []\n        for seg_ind in indices:\n            offset = int(seg_ind)\n            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):\n                if offset + skip_offsets[i] <= duration:\n                    frame_id = offset + skip_offsets[i] - 1\n                else:\n                    frame_id = offset - 1\n                frame_id_list.append(frame_id)\n                if offset + self.new_step < duration:\n                    offset += self.new_step\n        return frame_id_list", ""]}
{"filename": "dataset/__init__.py", "chunked_list": ["from .build import build_dataset, build_pretraining_dataset\n\n__all__ = ['build_dataset', 'build_pretraining_dataset']\n"]}
{"filename": "dataset/volume_transforms.py", "chunked_list": ["import numpy as np\nimport torch\nfrom PIL import Image\n\n\ndef convert_img(img):\n    \"\"\"Converts (H, W, C) numpy.ndarray to (C, W, H) format\n    \"\"\"\n    if len(img.shape) == 3:\n        img = img.transpose(2, 0, 1)\n    if len(img.shape) == 2:\n        img = np.expand_dims(img, 0)\n    return img", "\n\nclass ClipToTensor(object):\n    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n    \"\"\"\n\n    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n        self.channel_nb = channel_nb\n        self.div_255 = div_255\n        self.numpy = numpy\n\n    def __call__(self, clip):\n        \"\"\"\n        Args: clip (list of numpy.ndarray): clip (list of images)\n        to be converted to tensor.\n        \"\"\"\n        # Retrieve shape\n        if isinstance(clip[0], np.ndarray):\n            h, w, ch = clip[0].shape\n            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n                ch)\n        elif isinstance(clip[0], Image.Image):\n            w, h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n            but got list of {0}'.format(type(clip[0])))\n\n        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n\n        # Convert\n        for img_idx, img in enumerate(clip):\n            if isinstance(img, np.ndarray):\n                pass\n            elif isinstance(img, Image.Image):\n                img = np.array(img, copy=False)\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n                but got list of {0}'.format(type(clip[0])))\n            img = convert_img(img)\n            np_clip[:, img_idx, :, :] = img\n        if self.numpy:\n            if self.div_255:\n                np_clip = np_clip / 255.0\n            return np_clip\n\n        else:\n            tensor_clip = torch.from_numpy(np_clip)\n\n            if not isinstance(tensor_clip, torch.FloatTensor):\n                tensor_clip = tensor_clip.float()\n            if self.div_255:\n                tensor_clip = torch.div(tensor_clip, 255)\n            return tensor_clip", "\n\n# Note this norms data to -1/1\nclass ClipToTensor_K(object):\n    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n    \"\"\"\n\n    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n        self.channel_nb = channel_nb\n        self.div_255 = div_255\n        self.numpy = numpy\n\n    def __call__(self, clip):\n        \"\"\"\n        Args: clip (list of numpy.ndarray): clip (list of images)\n        to be converted to tensor.\n        \"\"\"\n        # Retrieve shape\n        if isinstance(clip[0], np.ndarray):\n            h, w, ch = clip[0].shape\n            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n                ch)\n        elif isinstance(clip[0], Image.Image):\n            w, h = clip[0].size\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n            but got list of {0}'.format(type(clip[0])))\n\n        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n\n        # Convert\n        for img_idx, img in enumerate(clip):\n            if isinstance(img, np.ndarray):\n                pass\n            elif isinstance(img, Image.Image):\n                img = np.array(img, copy=False)\n            else:\n                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n                but got list of {0}'.format(type(clip[0])))\n            img = convert_img(img)\n            np_clip[:, img_idx, :, :] = img\n        if self.numpy:\n            if self.div_255:\n                np_clip = (np_clip - 127.5) / 127.5\n            return np_clip\n\n        else:\n            tensor_clip = torch.from_numpy(np_clip)\n\n            if not isinstance(tensor_clip, torch.FloatTensor):\n                tensor_clip = tensor_clip.float()\n            if self.div_255:\n                tensor_clip = torch.div(torch.sub(tensor_clip, 127.5), 127.5)\n            return tensor_clip", "\n\nclass ToTensor(object):\n    \"\"\"Converts numpy array to tensor\n    \"\"\"\n\n    def __call__(self, array):\n        tensor = torch.from_numpy(array)\n        return tensor\n", ""]}
{"filename": "dataset/build.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport os\n\nfrom .datasets import RawFrameClsDataset, VideoClsDataset", "\nfrom .datasets import RawFrameClsDataset, VideoClsDataset\nfrom .pretrain_datasets import (  # noqa: F401\n    DataAugmentationForVideoMAEv2, HybridVideoMAE, VideoMAE,\n)\n\n\ndef build_pretraining_dataset(args):\n    transform = DataAugmentationForVideoMAEv2(args)\n    dataset = VideoMAE(\n        root=args.data_root,\n        setting=args.data_path,\n        train=True,\n        test_mode=False,\n        name_pattern=args.fname_tmpl,\n        video_ext='mp4',\n        is_color=True,\n        modality='rgb',\n        num_segments=1,\n        num_crop=1,\n        new_length=args.num_frames,\n        new_step=args.sampling_rate,\n        transform=transform,\n        temporal_jitter=False,\n        lazy_init=False,\n        num_sample=args.num_sample)\n    print(\"Data Aug = %s\" % str(transform))\n    return dataset", "\n\ndef build_dataset(is_train, test_mode, args):\n    if is_train:\n        mode = 'train'\n        anno_path = os.path.join(args.data_path, 'train.csv')\n    elif test_mode:\n        mode = 'test'\n        anno_path = os.path.join(args.data_path, 'val.csv')\n    else:\n        mode = 'validation'\n        anno_path = os.path.join(args.data_path, 'val.csv')\n\n    if args.data_set == 'Kinetics-400':\n        if not args.sparse_sample:\n            dataset = VideoClsDataset(\n                anno_path=anno_path,\n                data_root=args.data_root,\n                mode=mode,\n                clip_len=args.num_frames,\n                frame_sample_rate=args.sampling_rate,\n                num_segment=1,\n                test_num_segment=args.test_num_segment,\n                test_num_crop=args.test_num_crop,\n                num_crop=1 if not test_mode else 3,\n                keep_aspect_ratio=True,\n                crop_size=args.input_size,\n                short_side_size=args.short_side_size,\n                new_height=256,\n                new_width=320,\n                sparse_sample=False,\n                args=args)\n        else:\n            dataset = VideoClsDataset(\n                anno_path=anno_path,\n                data_root=args.data_root,\n                mode=mode,\n                clip_len=1,\n                frame_sample_rate=1,\n                num_segment=args.num_frames,\n                test_num_segment=args.test_num_segment,\n                test_num_crop=args.test_num_crop,\n                num_crop=1 if not test_mode else 3,\n                keep_aspect_ratio=True,\n                crop_size=args.input_size,\n                short_side_size=args.short_side_size,\n                new_height=256,\n                new_width=320,\n                sparse_sample=True,\n                args=args)\n        nb_classes = 400\n\n    elif args.data_set == 'Kinetics-600':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 600\n\n    elif args.data_set == 'Kinetics-700':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 700\n\n    elif args.data_set == 'Kinetics-710':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 710\n\n    elif args.data_set == 'SSV2':\n        dataset = RawFrameClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=1,\n            num_segment=args.num_frames,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            filename_tmpl=args.fname_tmpl,\n            start_idx=args.start_idx,\n            args=args)\n\n        nb_classes = 174\n\n    elif args.data_set == 'UCF101':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 101\n\n    elif args.data_set == 'HMDB51':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 51\n\n    elif args.data_set == 'Diving48':\n        dataset = VideoClsDataset(\n            anno_path=anno_path,\n            data_root=args.data_root,\n            mode=mode,\n            clip_len=args.num_frames,\n            frame_sample_rate=args.sampling_rate,\n            num_segment=1,\n            test_num_segment=args.test_num_segment,\n            test_num_crop=args.test_num_crop,\n            num_crop=1 if not test_mode else 3,\n            keep_aspect_ratio=True,\n            crop_size=args.input_size,\n            short_side_size=args.short_side_size,\n            new_height=256,\n            new_width=320,\n            args=args)\n        nb_classes = 48\n    elif args.data_set == 'MIT':\n        if not args.sparse_sample:\n            dataset = VideoClsDataset(\n                anno_path=anno_path,\n                data_root=args.data_root,\n                mode=mode,\n                clip_len=args.num_frames,\n                frame_sample_rate=args.sampling_rate,\n                num_segment=1,\n                test_num_segment=args.test_num_segment,\n                test_num_crop=args.test_num_crop,\n                num_crop=1 if not test_mode else 3,\n                keep_aspect_ratio=True,\n                crop_size=args.input_size,\n                short_side_size=args.short_side_size,\n                new_height=256,\n                new_width=320,\n                sparse_sample=False,\n                args=args)\n        else:\n            dataset = VideoClsDataset(\n                anno_path=anno_path,\n                data_root=args.data_root,\n                mode=mode,\n                clip_len=1,\n                frame_sample_rate=1,\n                num_segment=args.num_frames,\n                test_num_segment=args.test_num_segment,\n                test_num_crop=args.test_num_crop,\n                num_crop=1 if not test_mode else 3,\n                keep_aspect_ratio=True,\n                crop_size=args.input_size,\n                short_side_size=args.short_side_size,\n                new_height=256,\n                new_width=320,\n                sparse_sample=True,\n                args=args)\n        nb_classes = 339\n    else:\n        raise NotImplementedError('Unsupported Dataset')\n\n    assert nb_classes == args.nb_classes\n    print(\"Number of the class = %d\" % args.nb_classes)\n\n    return dataset, nb_classes", ""]}
{"filename": "dataset/datasets.py", "chunked_list": ["# pylint: disable=line-too-long,too-many-lines,missing-docstring\nimport os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n", "from torchvision import transforms\n\nfrom . import video_transforms, volume_transforms\nfrom .loader import get_image_loader, get_video_loader\nfrom .random_erasing import RandomErasing\n\n\nclass VideoClsDataset(Dataset):\n    \"\"\"Load your own video classification dataset.\"\"\"\n\n    def __init__(self,\n                 anno_path,\n                 data_root='',\n                 mode='train',\n                 clip_len=8,\n                 frame_sample_rate=2,\n                 crop_size=224,\n                 short_side_size=256,\n                 new_height=256,\n                 new_width=340,\n                 keep_aspect_ratio=True,\n                 num_segment=1,\n                 num_crop=1,\n                 test_num_segment=10,\n                 test_num_crop=3,\n                 sparse_sample=False,\n                 args=None):\n        self.anno_path = anno_path\n        self.data_root = data_root\n        self.mode = mode\n        self.clip_len = clip_len\n        self.frame_sample_rate = frame_sample_rate\n        self.crop_size = crop_size\n        self.short_side_size = short_side_size\n        self.new_height = new_height\n        self.new_width = new_width\n        self.keep_aspect_ratio = keep_aspect_ratio\n        self.num_segment = num_segment\n        self.test_num_segment = test_num_segment\n        self.num_crop = num_crop\n        self.test_num_crop = test_num_crop\n        self.sparse_sample = sparse_sample\n        self.args = args\n        self.aug = False\n        self.rand_erase = False\n\n        if self.mode in ['train']:\n            self.aug = True\n            if self.args.reprob > 0:\n                self.rand_erase = True\n\n        self.video_loader = get_video_loader()\n\n        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')\n        self.dataset_samples = list(\n            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))\n        self.label_array = list(cleaned.values[:, 1])\n\n        if (mode == 'train'):\n            pass\n\n        elif (mode == 'validation'):\n            self.data_transform = video_transforms.Compose([\n                video_transforms.Resize(\n                    self.short_side_size, interpolation='bilinear'),\n                video_transforms.CenterCrop(\n                    size=(self.crop_size, self.crop_size)),\n                volume_transforms.ClipToTensor(),\n                video_transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        elif mode == 'test':\n            self.data_resize = video_transforms.Compose([\n                video_transforms.Resize(\n                    size=(short_side_size), interpolation='bilinear')\n            ])\n            self.data_transform = video_transforms.Compose([\n                volume_transforms.ClipToTensor(),\n                video_transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            self.test_seg = []\n            self.test_dataset = []\n            self.test_label_array = []\n            for ck in range(self.test_num_segment):\n                for cp in range(self.test_num_crop):\n                    for idx in range(len(self.label_array)):\n                        sample_label = self.label_array[idx]\n                        self.test_label_array.append(sample_label)\n                        self.test_dataset.append(self.dataset_samples[idx])\n                        self.test_seg.append((ck, cp))\n\n    def __getitem__(self, index):\n        if self.mode == 'train':\n            args = self.args\n            scale_t = 1\n\n            sample = self.dataset_samples[index]\n            # T H W C\n            buffer = self.load_video(sample, sample_rate_scale=scale_t)\n            if len(buffer) == 0:\n                while len(buffer) == 0:\n                    warnings.warn(\n                        \"video {} not correctly loaded during training\".format(\n                            sample))\n                    index = np.random.randint(self.__len__())\n                    sample = self.dataset_samples[index]\n                    buffer = self.load_video(sample, sample_rate_scale=scale_t)\n\n            if args.num_sample > 1:\n                frame_list = []\n                label_list = []\n                index_list = []\n                for _ in range(args.num_sample):\n                    new_frames = self._aug_frame(buffer, args)\n                    label = self.label_array[index]\n                    frame_list.append(new_frames)\n                    label_list.append(label)\n                    index_list.append(index)\n                return frame_list, label_list, index_list, {}\n            else:\n                buffer = self._aug_frame(buffer, args)\n\n            return buffer, self.label_array[index], index, {}\n\n        elif self.mode == 'validation':\n            sample = self.dataset_samples[index]\n            buffer = self.load_video(sample)\n            if len(buffer) == 0:\n                while len(buffer) == 0:\n                    warnings.warn(\n                        \"video {} not correctly loaded during validation\".\n                        format(sample))\n                    index = np.random.randint(self.__len__())\n                    sample = self.dataset_samples[index]\n                    buffer = self.load_video(sample)\n            buffer = self.data_transform(buffer)\n            return buffer, self.label_array[index], sample.split(\n                \"/\")[-1].split(\".\")[0]\n\n        elif self.mode == 'test':\n            sample = self.test_dataset[index]\n            chunk_nb, split_nb = self.test_seg[index]\n            buffer = self.load_video(sample)\n\n            while len(buffer) == 0:\n                warnings.warn(\n                    \"video {}, temporal {}, spatial {} not found during testing\"\n                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))\n                index = np.random.randint(self.__len__())\n                sample = self.test_dataset[index]\n                chunk_nb, split_nb = self.test_seg[index]\n                buffer = self.load_video(sample)\n\n            buffer = self.data_resize(buffer)\n            if isinstance(buffer, list):\n                buffer = np.stack(buffer, 0)\n\n            if self.sparse_sample:\n                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n                                      self.short_side_size) / (\n                                          self.test_num_crop - 1)\n                temporal_start = chunk_nb\n                spatial_start = int(split_nb * spatial_step)\n                if buffer.shape[1] >= buffer.shape[2]:\n                    buffer = buffer[temporal_start::self.test_num_segment,\n                                    spatial_start:spatial_start +\n                                    self.short_side_size, :, :]\n                else:\n                    buffer = buffer[temporal_start::self.test_num_segment, :,\n                                    spatial_start:spatial_start +\n                                    self.short_side_size, :]\n            else:\n                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n                                      self.short_side_size) / (\n                                          self.test_num_crop - 1)\n                temporal_step = max(\n                    1.0 * (buffer.shape[0] - self.clip_len) /\n                    (self.test_num_segment - 1), 0)\n                temporal_start = int(chunk_nb * temporal_step)\n                spatial_start = int(split_nb * spatial_step)\n                if buffer.shape[1] >= buffer.shape[2]:\n                    buffer = buffer[temporal_start:temporal_start +\n                                    self.clip_len,\n                                    spatial_start:spatial_start +\n                                    self.short_side_size, :, :]\n                else:\n                    buffer = buffer[temporal_start:temporal_start +\n                                    self.clip_len, :,\n                                    spatial_start:spatial_start +\n                                    self.short_side_size, :]\n\n            buffer = self.data_transform(buffer)\n            return buffer, self.test_label_array[index], sample.split(\n                \"/\")[-1].split(\".\")[0], chunk_nb, split_nb\n        else:\n            raise NameError('mode {} unkown'.format(self.mode))\n\n    def _aug_frame(self, buffer, args):\n        aug_transform = video_transforms.create_random_augment(\n            input_size=(self.crop_size, self.crop_size),\n            auto_augment=args.aa,\n            interpolation=args.train_interpolation,\n        )\n\n        buffer = [transforms.ToPILImage()(frame) for frame in buffer]\n\n        buffer = aug_transform(buffer)\n\n        buffer = [transforms.ToTensor()(img) for img in buffer]\n        buffer = torch.stack(buffer)  # T C H W\n        buffer = buffer.permute(0, 2, 3, 1)  # T H W C\n\n        # T H W C\n        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],\n                                  [0.229, 0.224, 0.225])\n        # T H W C -> C T H W.\n        buffer = buffer.permute(3, 0, 1, 2)\n        # Perform data augmentation.\n        scl, asp = (\n            [0.08, 1.0],\n            [0.75, 1.3333],\n        )\n\n        buffer = spatial_sampling(\n            buffer,\n            spatial_idx=-1,\n            min_scale=256,\n            max_scale=320,\n            # crop_size=224,\n            crop_size=args.input_size,\n            random_horizontal_flip=False if args.data_set == 'SSV2' else True,\n            inverse_uniform_sampling=False,\n            aspect_ratio=asp,\n            scale=scl,\n            motion_shift=False)\n\n        if self.rand_erase:\n            erase_transform = RandomErasing(\n                args.reprob,\n                mode=args.remode,\n                max_count=args.recount,\n                num_splits=args.recount,\n                device=\"cpu\",\n            )\n            buffer = buffer.permute(1, 0, 2, 3)  # C T H W -> T C H W\n            buffer = erase_transform(buffer)\n            buffer = buffer.permute(1, 0, 2, 3)  # T C H W -> C T H W\n\n        return buffer\n\n    def load_video(self, sample, sample_rate_scale=1):\n        fname = sample\n\n        try:\n            vr = self.video_loader(fname)\n        except Exception as e:\n            print(f\"Failed to load video from {fname} with error {e}!\")\n            return []\n\n        length = len(vr)\n\n        if self.mode == 'test':\n            if self.sparse_sample:\n                tick = length / float(self.num_segment)\n                all_index = []\n                for t_seg in range(self.test_num_segment):\n                    tmp_index = [\n                        int(t_seg * tick / self.test_num_segment + tick * x)\n                        for x in range(self.num_segment)\n                    ]\n                    all_index.extend(tmp_index)\n                all_index = list(np.sort(np.array(all_index)))\n            else:\n                all_index = [\n                    x for x in range(0, length, self.frame_sample_rate)\n                ]\n                while len(all_index) < self.clip_len:\n                    all_index.append(all_index[-1])\n\n            vr.seek(0)\n            buffer = vr.get_batch(all_index).asnumpy()\n            return buffer\n\n        # handle temporal segments\n        converted_len = int(self.clip_len * self.frame_sample_rate)\n        seg_len = length // self.num_segment\n\n        all_index = []\n        for i in range(self.num_segment):\n            if seg_len <= converted_len:\n                index = np.linspace(\n                    0, seg_len, num=seg_len // self.frame_sample_rate)\n                index = np.concatenate(\n                    (index,\n                     np.ones(self.clip_len - seg_len // self.frame_sample_rate)\n                     * seg_len))\n                index = np.clip(index, 0, seg_len - 1).astype(np.int64)\n            else:\n                if self.mode == 'validation':\n                    end_idx = (converted_len + seg_len) // 2\n                else:\n                    end_idx = np.random.randint(converted_len, seg_len)\n                str_idx = end_idx - converted_len\n                index = np.linspace(str_idx, end_idx, num=self.clip_len)\n                index = np.clip(index, str_idx, end_idx - 1).astype(np.int64)\n            index = index + i * seg_len\n            all_index.extend(list(index))\n\n        all_index = all_index[::int(sample_rate_scale)]\n        vr.seek(0)\n        buffer = vr.get_batch(all_index).asnumpy()\n        return buffer\n\n    def __len__(self):\n        if self.mode != 'test':\n            return len(self.dataset_samples)\n        else:\n            return len(self.test_dataset)", "\n\nclass RawFrameClsDataset(Dataset):\n    \"\"\"Load your own raw frame classification dataset.\"\"\"\n\n    def __init__(self,\n                 anno_path,\n                 data_root,\n                 mode='train',\n                 clip_len=8,\n                 crop_size=224,\n                 short_side_size=256,\n                 new_height=256,\n                 new_width=340,\n                 keep_aspect_ratio=True,\n                 num_segment=1,\n                 num_crop=1,\n                 test_num_segment=10,\n                 test_num_crop=3,\n                 filename_tmpl='img_{:05}.jpg',\n                 start_idx=1,\n                 args=None):\n        self.anno_path = anno_path\n        self.data_root = data_root\n        self.mode = mode\n        self.clip_len = clip_len\n        self.crop_size = crop_size\n        self.short_side_size = short_side_size\n        self.new_height = new_height\n        self.new_width = new_width\n        self.keep_aspect_ratio = keep_aspect_ratio\n        self.num_segment = num_segment\n        self.test_num_segment = test_num_segment\n        self.num_crop = num_crop\n        self.test_num_crop = test_num_crop\n        self.filename_tmpl = filename_tmpl\n        self.start_idx = start_idx\n        self.args = args\n        self.aug = False\n        self.rand_erase = False\n\n        if self.mode in ['train']:\n            self.aug = True\n            if self.args.reprob > 0:\n                self.rand_erase = True\n\n        self.image_loader = get_image_loader()\n\n        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')\n        self.dataset_samples = list(\n            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))\n        self.total_frames = list(cleaned.values[:, 1])\n        self.label_array = list(cleaned.values[:, -1])\n\n        if (mode == 'train'):\n            pass\n\n        elif (mode == 'validation'):\n            self.data_transform = video_transforms.Compose([\n                video_transforms.Resize(\n                    self.short_side_size, interpolation='bilinear'),\n                video_transforms.CenterCrop(\n                    size=(self.crop_size, self.crop_size)),\n                volume_transforms.ClipToTensor(),\n                video_transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        elif mode == 'test':\n            self.data_resize = video_transforms.Compose([\n                video_transforms.Resize(\n                    size=(short_side_size), interpolation='bilinear')\n            ])\n            self.data_transform = video_transforms.Compose([\n                volume_transforms.ClipToTensor(),\n                video_transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            self.test_seg = []\n            self.test_dataset = []\n            self.test_total_frames = []\n            self.test_label_array = []\n            for ck in range(self.test_num_segment):\n                for cp in range(self.test_num_crop):\n                    for idx in range(len(self.label_array)):\n                        self.test_seg.append((ck, cp))\n                        self.test_dataset.append(self.dataset_samples[idx])\n                        self.test_total_frames.append(self.total_frames[idx])\n                        self.test_label_array.append(self.label_array[idx])\n\n    def __getitem__(self, index):\n        if self.mode == 'train':\n            args = self.args\n            scale_t = 1\n\n            sample = self.dataset_samples[index]\n            total_frame = self.total_frames[index]\n            buffer = self.load_frame(\n                sample, total_frame, sample_rate_scale=scale_t)  # T H W C\n            if len(buffer) == 0:\n                while len(buffer) == 0:\n                    warnings.warn(\n                        \"video {} not correctly loaded during training\".format(\n                            sample))\n                    index = np.random.randint(self.__len__())\n                    sample = self.dataset_samples[index]\n                    total_frame = self.total_frames[index]\n                    buffer = self.load_frame(\n                        sample, total_frame, sample_rate_scale=scale_t)\n\n            if args.num_sample > 1:\n                frame_list = []\n                label_list = []\n                index_list = []\n                for _ in range(args.num_sample):\n                    new_frames = self._aug_frame(buffer, args)\n                    label = self.label_array[index]\n                    frame_list.append(new_frames)\n                    label_list.append(label)\n                    index_list.append(index)\n                return frame_list, label_list, index_list, {}\n            else:\n                buffer = self._aug_frame(buffer, args)\n\n            return buffer, self.label_array[index], index, {}\n\n        elif self.mode == 'validation':\n            sample = self.dataset_samples[index]\n            total_frame = self.total_frames[index]\n            buffer = self.load_frame(sample, total_frame)\n            if len(buffer) == 0:\n                while len(buffer) == 0:\n                    warnings.warn(\n                        \"video {} not correctly loaded during validation\".\n                        format(sample))\n                    index = np.random.randint(self.__len__())\n                    sample = self.dataset_samples[index]\n                    buffer = self.load_frame(sample, total_frame)\n            buffer = self.data_transform(buffer)\n            return buffer, self.label_array[index], sample.split(\n                \"/\")[-1].split(\".\")[0]\n\n        elif self.mode == 'test':\n            sample = self.test_dataset[index]\n            total_frame = self.test_total_frames[index]\n            chunk_nb, split_nb = self.test_seg[index]\n            buffer = self.load_frame(sample, total_frame)\n\n            while len(buffer) == 0:\n                warnings.warn(\n                    \"video {}, temporal {}, spatial {} not found during testing\"\n                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))\n                index = np.random.randint(self.__len__())\n                sample = self.test_dataset[index]\n                total_frame = self.test_total_frames[index]\n                chunk_nb, split_nb = self.test_seg[index]\n                buffer = self.load_frame(sample, total_frame)\n\n            buffer = self.data_resize(buffer)\n            if isinstance(buffer, list):\n                buffer = np.stack(buffer, 0)\n\n            spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n                                  self.short_side_size) / (\n                                      self.test_num_crop - 1)\n            temporal_start = chunk_nb\n            spatial_start = int(split_nb * spatial_step)\n            if buffer.shape[1] >= buffer.shape[2]:\n                buffer = buffer[temporal_start::self.test_num_segment,\n                                spatial_start:spatial_start +\n                                self.short_side_size, :, :]\n            else:\n                buffer = buffer[temporal_start::self.test_num_segment, :,\n                                spatial_start:spatial_start +\n                                self.short_side_size, :]\n\n            buffer = self.data_transform(buffer)\n            return buffer, self.test_label_array[index], sample.split(\n                \"/\")[-1].split(\".\")[0], chunk_nb, split_nb\n        else:\n            raise NameError('mode {} unkown'.format(self.mode))\n\n    def _aug_frame(self, buffer, args):\n        aug_transform = video_transforms.create_random_augment(\n            input_size=(self.crop_size, self.crop_size),\n            auto_augment=args.aa,\n            interpolation=args.train_interpolation,\n        )\n\n        buffer = [transforms.ToPILImage()(frame) for frame in buffer]\n\n        buffer = aug_transform(buffer)\n\n        buffer = [transforms.ToTensor()(img) for img in buffer]\n        buffer = torch.stack(buffer)  # T C H W\n        buffer = buffer.permute(0, 2, 3, 1)  # T H W C\n\n        # T H W C\n        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],\n                                  [0.229, 0.224, 0.225])\n        # T H W C -> C T H W.\n        buffer = buffer.permute(3, 0, 1, 2)\n        # Perform data augmentation.\n        scl, asp = (\n            [0.08, 1.0],\n            [0.75, 1.3333],\n        )\n\n        buffer = spatial_sampling(\n            buffer,\n            spatial_idx=-1,\n            min_scale=256,\n            max_scale=320,\n            crop_size=self.crop_size,\n            random_horizontal_flip=False if args.data_set == 'SSV2' else True,\n            inverse_uniform_sampling=False,\n            aspect_ratio=asp,\n            scale=scl,\n            motion_shift=False)\n\n        if self.rand_erase:\n            erase_transform = RandomErasing(\n                args.reprob,\n                mode=args.remode,\n                max_count=args.recount,\n                num_splits=args.recount,\n                device=\"cpu\",\n            )\n            buffer = buffer.permute(1, 0, 2, 3)\n            buffer = erase_transform(buffer)\n            buffer = buffer.permute(1, 0, 2, 3)\n\n        return buffer\n\n    def load_frame(self, sample, num_frames, sample_rate_scale=1):\n        \"\"\"Load video content using Decord\"\"\"\n        fname = sample\n\n        if self.mode == 'test':\n            tick = num_frames / float(self.num_segment)\n            all_index = []\n            for t_seg in range(self.test_num_segment):\n                tmp_index = [\n                    int(t_seg * tick / self.test_num_segment + tick * x)\n                    for x in range(self.num_segment)\n                ]\n                all_index.extend(tmp_index)\n            all_index = list(np.sort(np.array(all_index) + self.start_idx))\n            imgs = []\n            for idx in all_index:\n                frame_fname = os.path.join(fname,\n                                           self.filename_tmpl.format(idx))\n                img = self.image_loader(frame_fname)\n                imgs.append(img)\n            buffer = np.array(imgs)\n            return buffer\n\n        # handle temporal segments\n        average_duration = num_frames // self.num_segment\n        all_index = []\n        if average_duration > 0:\n            if self.mode == 'validation':\n                all_index = list(\n                    np.multiply(\n                        list(range(self.num_segment)), average_duration) +\n                    np.ones(self.num_segment, dtype=int) *\n                    (average_duration // 2))\n            else:\n                all_index = list(\n                    np.multiply(\n                        list(range(self.num_segment)), average_duration) +\n                    np.random.randint(average_duration, size=self.num_segment))\n        elif num_frames > self.num_segment:\n            if self.mode == 'validation':\n                all_index = list(range(self.num_segment))\n            else:\n                all_index = list(\n                    np.sort(\n                        np.random.randint(num_frames, size=self.num_segment)))\n        else:\n            all_index = [0] * (self.num_segment - num_frames) + list(\n                range(num_frames))\n        all_index = list(np.array(all_index) + self.start_idx)\n        imgs = []\n        for idx in all_index:\n            frame_fname = os.path.join(fname, self.filename_tmpl.format(idx))\n            img = self.image_loader(frame_fname)\n            imgs.append(img)\n        buffer = np.array(imgs)\n        return buffer\n\n    def __len__(self):\n        if self.mode != 'test':\n            return len(self.dataset_samples)\n        else:\n            return len(self.test_dataset)", "\n\ndef spatial_sampling(\n    frames,\n    spatial_idx=-1,\n    min_scale=256,\n    max_scale=320,\n    crop_size=224,\n    random_horizontal_flip=True,\n    inverse_uniform_sampling=False,\n    aspect_ratio=None,\n    scale=None,\n    motion_shift=False,\n):\n    \"\"\"\n    Perform spatial sampling on the given video frames. If spatial_idx is\n    -1, perform random scale, random crop, and random flip on the given\n    frames. If spatial_idx is 0, 1, or 2, perform spatial uniform sampling\n    with the given spatial_idx.\n    Args:\n        frames (tensor): frames of images sampled from the video. The\n            dimension is `num frames` x `height` x `width` x `channel`.\n        spatial_idx (int): if -1, perform random spatial sampling. If 0, 1,\n            or 2, perform left, center, right crop if width is larger than\n            height, and perform top, center, buttom crop if height is larger\n            than width.\n        min_scale (int): the minimal size of scaling.\n        max_scale (int): the maximal size of scaling.\n        crop_size (int): the size of height and width used to crop the\n            frames.\n        inverse_uniform_sampling (bool): if True, sample uniformly in\n            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the\n            scale. If False, take a uniform sample from [min_scale,\n            max_scale].\n        aspect_ratio (list): Aspect ratio range for resizing.\n        scale (list): Scale range for resizing.\n        motion_shift (bool): Whether to apply motion shift for resizing.\n    Returns:\n        frames (tensor): spatially sampled frames.\n    \"\"\"\n    assert spatial_idx in [-1, 0, 1, 2]\n    if spatial_idx == -1:\n        if aspect_ratio is None and scale is None:\n            frames, _ = video_transforms.random_short_side_scale_jitter(\n                images=frames,\n                min_size=min_scale,\n                max_size=max_scale,\n                inverse_uniform_sampling=inverse_uniform_sampling,\n            )\n            frames, _ = video_transforms.random_crop(frames, crop_size)\n        else:\n            transform_func = (\n                video_transforms.random_resized_crop_with_shift\n                if motion_shift else video_transforms.random_resized_crop)\n            frames = transform_func(\n                images=frames,\n                target_height=crop_size,\n                target_width=crop_size,\n                scale=scale,\n                ratio=aspect_ratio,\n            )\n        if random_horizontal_flip:\n            frames, _ = video_transforms.horizontal_flip(0.5, frames)\n    else:\n        # The testing is deterministic and no jitter should be performed.\n        # min_scale, max_scale, and crop_size are expect to be the same.\n        assert len({min_scale, max_scale, crop_size}) == 1\n        frames, _ = video_transforms.random_short_side_scale_jitter(\n            frames, min_scale, max_scale)\n        frames, _ = video_transforms.uniform_crop(frames, crop_size,\n                                                  spatial_idx)\n    return frames", "\n\ndef tensor_normalize(tensor, mean, std):\n    \"\"\"\n    Normalize a given tensor by subtracting the mean and dividing the std.\n    Args:\n        tensor (tensor): tensor to normalize.\n        mean (tensor or list): mean value to subtract.\n        std (tensor or list): std to divide.\n    \"\"\"\n    if tensor.dtype == torch.uint8:\n        tensor = tensor.float()\n        tensor = tensor / 255.0\n    if type(mean) == list:\n        mean = torch.tensor(mean)\n    if type(std) == list:\n        std = torch.tensor(std)\n    tensor = tensor - mean\n    tensor = tensor / std\n    return tensor", ""]}
{"filename": "dataset/functional.py", "chunked_list": ["import numbers\n\nimport cv2\nimport numpy as np\nimport PIL\nimport torch\n\n\ndef _is_tensor_clip(clip):\n    return torch.is_tensor(clip) and clip.ndimension() == 4", "def _is_tensor_clip(clip):\n    return torch.is_tensor(clip) and clip.ndimension() == 4\n\n\ndef crop_clip(clip, min_h, min_w, h, w):\n    if isinstance(clip[0], np.ndarray):\n        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]\n\n    elif isinstance(clip[0], PIL.Image.Image):\n        cropped = [\n            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip\n        ]\n    else:\n        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                        'but got list of {0}'.format(type(clip[0])))\n    return cropped", "\n\ndef resize_clip(clip, size, interpolation='bilinear'):\n    if isinstance(clip[0], np.ndarray):\n        if isinstance(size, numbers.Number):\n            im_h, im_w, im_c = clip[0].shape\n            # Min spatial dim already matches minimal size\n            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n                                                   and im_h == size):\n                return clip\n            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n            size = (new_w, new_h)\n        else:\n            size = size[0], size[1]\n        if interpolation == 'bilinear':\n            np_inter = cv2.INTER_LINEAR\n        else:\n            np_inter = cv2.INTER_NEAREST\n        scaled = [\n            cv2.resize(img, size, interpolation=np_inter) for img in clip\n        ]\n    elif isinstance(clip[0], PIL.Image.Image):\n        if isinstance(size, numbers.Number):\n            im_w, im_h = clip[0].size\n            # Min spatial dim already matches minimal size\n            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n                                                   and im_h == size):\n                return clip\n            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n            size = (new_w, new_h)\n        else:\n            size = size[1], size[0]\n        if interpolation == 'bilinear':\n            pil_inter = PIL.Image.BILINEAR\n        else:\n            pil_inter = PIL.Image.NEAREST\n        scaled = [img.resize(size, pil_inter) for img in clip]\n    else:\n        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                        'but got list of {0}'.format(type(clip[0])))\n    return scaled", "\n\ndef get_resize_sizes(im_h, im_w, size):\n    if im_w < im_h:\n        ow = size\n        oh = int(size * im_h / im_w)\n    else:\n        oh = size\n        ow = int(size * im_w / im_h)\n    return oh, ow", "\n\ndef normalize(clip, mean, std, inplace=False):\n    if not _is_tensor_clip(clip):\n        raise TypeError('tensor is not a torch clip.')\n\n    if not inplace:\n        clip = clip.clone()\n\n    dtype = clip.dtype\n    mean = torch.as_tensor(mean, dtype=dtype, device=clip.device)\n    std = torch.as_tensor(std, dtype=dtype, device=clip.device)\n    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n\n    return clip", ""]}
{"filename": "models/modeling_finetune.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nfrom functools import partial\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 400,\n        'input_size': (3, 224, 224),\n        'pool_size': None,\n        'crop_pct': .9,\n        'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5),\n        'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }", "\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 400,\n        'input_size': (3, 224, 224),\n        'pool_size': None,\n        'crop_pct': .9,\n        'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5),\n        'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }", "\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)", "\n\nclass Mlp(nn.Module):\n\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x", "\n\nclass CosAttention(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.,\n                 proj_drop=0.,\n                 attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # self.scale = qk_scale or head_dim**-0.5\n        # DO NOT RENAME [self.scale] (for no weight decay)\n        if qk_scale is None:\n            self.scale = nn.Parameter(\n                torch.log(10 * torch.ones((num_heads, 1, 1))),\n                requires_grad=True)\n        else:\n            self.scale = qk_scale\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat(\n                (self.q_bias,\n                 torch.zeros_like(self.v_bias,\n                                  requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[\n            2]  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (\n            F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n\n        # torch.log(torch.tensor(1. / 0.01)) = 4.6052\n        logit_scale = torch.clamp(self.scale, max=4.6052).exp()\n\n        attn = attn * logit_scale\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x", "\n\nclass Attention(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.,\n                 proj_drop=0.,\n                 attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat(\n                (self.q_bias,\n                 torch.zeros_like(self.v_bias,\n                                  requires_grad=False), self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[\n            2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 num_heads,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 init_values=None,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 attn_head_dim=None,\n                 cos_attn=False):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if cos_attn:\n            self.attn = CosAttention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                proj_drop=drop,\n                attn_head_dim=attn_head_dim)\n        else:\n            self.attn = Attention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                proj_drop=drop,\n                attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop)\n\n        if init_values > 0:\n            self.gamma_1 = nn.Parameter(\n                init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(\n                init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x", "\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_chans=3,\n                 embed_dim=768,\n                 num_frames=16,\n                 tubelet_size=2):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_spatial_patches = (img_size[0] // patch_size[0]) * (\n            img_size[1] // patch_size[1])\n        num_patches = num_spatial_patches * (num_frames // tubelet_size)\n\n        self.img_size = img_size\n        self.tubelet_size = tubelet_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.proj = nn.Conv3d(\n            in_channels=in_chans,\n            out_channels=embed_dim,\n            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n            stride=(self.tubelet_size, patch_size[0], patch_size[1]))\n\n    def forward(self, x, **kwargs):\n        B, C, T, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[\n            1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        # b, c, l -> b, l, c\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x", "\n\n# sin-cos position encoding\n# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\ndef get_sinusoid_encoding_table(n_position, d_hid):\n    ''' Sinusoid position encoding table '''\n\n    # TODO: make it with torch instead of numpy\n    def get_position_angle_vec(position):\n        return [\n            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n            for hid_j in range(d_hid)\n        ]\n\n    sinusoid_table = np.array(\n        [get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    return torch.tensor(\n        sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)", "\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_chans=3,\n                 num_classes=1000,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 head_drop_rate=0.,\n                 norm_layer=nn.LayerNorm,\n                 init_values=0.,\n                 use_learnable_pos_emb=False,\n                 init_scale=0.,\n                 all_frames=16,\n                 tubelet_size=2,\n                 use_mean_pooling=True,\n                 with_cp=False,\n                 cos_attn=False):\n        super().__init__()\n        self.num_classes = num_classes\n        # num_features for consistency with other models\n        self.num_features = self.embed_dim = embed_dim\n        self.tubelet_size = tubelet_size\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            num_frames=all_frames,\n            tubelet_size=tubelet_size)\n        num_patches = self.patch_embed.num_patches\n        self.with_cp = with_cp\n\n        if use_learnable_pos_emb:\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, num_patches, embed_dim))\n        else:\n            # sine-cosine positional embeddings is on the way\n            self.pos_embed = get_sinusoid_encoding_table(\n                num_patches, embed_dim)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n               ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n                cos_attn=cos_attn) for i in range(depth)\n        ])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(\n            embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head_dropout = nn.Dropout(head_drop_rate)\n        self.head = nn.Linear(\n            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if use_learnable_pos_emb:\n            trunc_normal_(self.pos_embed, std=.02)\n\n        self.apply(self._init_weights)\n\n        self.head.weight.data.mul_(init_scale)\n        self.head.bias.data.mul_(init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(\n            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.size(0)\n\n        x = self.patch_embed(x)\n\n        if self.pos_embed is not None:\n            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(\n                x.device).clone().detach()\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            if self.with_cp:\n                x = cp.checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n        if self.fc_norm is not None:\n            return self.fc_norm(x.mean(1))\n        else:\n            return self.norm(x[:, 0])\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head_dropout(x)\n        x = self.head(x)\n        return x", "\n\n@register_model\ndef vit_small_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\n\n@register_model\ndef vit_base_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\n\n@register_model\ndef vit_large_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\n\n@register_model\ndef vit_huge_patch16_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=16,\n        embed_dim=1280,\n        depth=32,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\n\n@register_model\ndef vit_giant_patch14_224(pretrained=False, **kwargs):\n    model = VisionTransformer(\n        patch_size=14,\n        embed_dim=1408,\n        depth=40,\n        num_heads=16,\n        mlp_ratio=48 / 11,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model", ""]}
{"filename": "models/__init__.py", "chunked_list": ["from .modeling_finetune import (\n    vit_base_patch16_224,\n    vit_giant_patch14_224,\n    vit_huge_patch16_224,\n    vit_large_patch16_224,\n    vit_small_patch16_224,\n)\nfrom .modeling_pretrain import (\n    pretrain_videomae_base_patch16_224,\n    pretrain_videomae_giant_patch14_224,", "    pretrain_videomae_base_patch16_224,\n    pretrain_videomae_giant_patch14_224,\n    pretrain_videomae_huge_patch16_224,\n    pretrain_videomae_large_patch16_224,\n    pretrain_videomae_small_patch16_224,\n)\n\n__all__ = [\n    'pretrain_videomae_small_patch16_224',\n    'pretrain_videomae_base_patch16_224',", "    'pretrain_videomae_small_patch16_224',\n    'pretrain_videomae_base_patch16_224',\n    'pretrain_videomae_large_patch16_224',\n    'pretrain_videomae_huge_patch16_224',\n    'pretrain_videomae_giant_patch14_224',\n    'vit_small_patch16_224',\n    'vit_base_patch16_224',\n    'vit_large_patch16_224',\n    'vit_huge_patch16_224',\n    'vit_giant_patch14_224',", "    'vit_huge_patch16_224',\n    'vit_giant_patch14_224',\n]"]}
{"filename": "models/modeling_pretrain.py", "chunked_list": ["# --------------------------------------------------------\n# Based on BEiT, timm, DINO and DeiT code bases\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nfrom functools import partial\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom timm.models.layers import trunc_normal_ as __call_trunc_normal_\nfrom timm.models.registry import register_model\n\nfrom .modeling_finetune import (\n    Block,\n    PatchEmbed,", "    Block,\n    PatchEmbed,\n    _cfg,\n    get_sinusoid_encoding_table,\n)\n\n\ndef trunc_normal_(tensor, mean=0., std=1.):\n    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n", "\n\nclass PretrainVisionTransformerEncoder(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_chans=3,\n                 num_classes=0,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 norm_layer=nn.LayerNorm,\n                 init_values=None,\n                 tubelet_size=2,\n                 use_learnable_pos_emb=False,\n                 with_cp=False,\n                 all_frames=16,\n                 cos_attn=False):\n        super().__init__()\n        self.num_classes = num_classes\n        # num_features for consistency with other models\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            num_frames=all_frames,\n            tubelet_size=tubelet_size)\n        num_patches = self.patch_embed.num_patches\n        self.with_cp = with_cp\n\n        if use_learnable_pos_emb:\n            self.pos_embed = nn.Parameter(\n                torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            # sine-cosine positional embeddings\n            self.pos_embed = get_sinusoid_encoding_table(\n                num_patches, embed_dim)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n               ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n                cos_attn=cos_attn) for i in range(depth)\n        ])\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Linear(\n            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if use_learnable_pos_emb:\n            trunc_normal_(self.pos_embed, std=.02)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(\n            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, mask):\n        x = self.patch_embed(x)\n\n        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n\n        B, _, C = x.shape\n        x_vis = x[~mask].reshape(B, -1, C)  # ~mask means visible\n\n        for blk in self.blocks:\n            if self.with_cp:\n                x_vis = cp.checkpoint(blk, x_vis)\n            else:\n                x_vis = blk(x_vis)\n\n        x_vis = self.norm(x_vis)\n        return x_vis\n\n    def forward(self, x, mask):\n        x = self.forward_features(x, mask)\n        x = self.head(x)\n        return x", "\n\nclass PretrainVisionTransformerDecoder(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 patch_size=16,\n                 num_classes=768,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 norm_layer=nn.LayerNorm,\n                 init_values=None,\n                 num_patches=196,\n                 tubelet_size=2,\n                 with_cp=False,\n                 cos_attn=False):\n        super().__init__()\n        self.num_classes = num_classes\n        assert num_classes == 3 * tubelet_size * patch_size**2\n        # num_features for consistency with other models\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_size = patch_size\n        self.with_cp = with_cp\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n               ]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n                cos_attn=cos_attn) for i in range(depth)\n        ])\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Linear(\n            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(\n            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward(self, x, return_token_num):\n        for blk in self.blocks:\n            if self.with_cp:\n                x = cp.checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n        if return_token_num > 0:\n            # only return the mask tokens predict pixels\n            x = self.head(self.norm(x[:, -return_token_num:]))\n        else:\n            # [B, N, 3*16^2]\n            x = self.head(self.norm(x))\n        return x", "\n\nclass PretrainVisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        encoder_in_chans=3,\n        encoder_num_classes=0,\n        encoder_embed_dim=768,\n        encoder_depth=12,\n        encoder_num_heads=12,\n        decoder_num_classes=1536,  # decoder_num_classes=768\n        decoder_embed_dim=512,\n        decoder_depth=8,\n        decoder_num_heads=8,\n        mlp_ratio=4.,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.,\n        attn_drop_rate=0.,\n        drop_path_rate=0.,\n        norm_layer=nn.LayerNorm,\n        init_values=0.,\n        use_learnable_pos_emb=False,\n        tubelet_size=2,\n        num_classes=0,  # avoid the error from create_fn in timm\n        in_chans=0,  # avoid the error from create_fn in timm\n        with_cp=False,\n        all_frames=16,\n        cos_attn=False,\n    ):\n        super().__init__()\n        self.encoder = PretrainVisionTransformerEncoder(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=encoder_in_chans,\n            num_classes=encoder_num_classes,\n            embed_dim=encoder_embed_dim,\n            depth=encoder_depth,\n            num_heads=encoder_num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate,\n            norm_layer=norm_layer,\n            init_values=init_values,\n            tubelet_size=tubelet_size,\n            use_learnable_pos_emb=use_learnable_pos_emb,\n            with_cp=with_cp,\n            all_frames=all_frames,\n            cos_attn=cos_attn)\n\n        self.decoder = PretrainVisionTransformerDecoder(\n            patch_size=patch_size,\n            num_patches=self.encoder.patch_embed.num_patches,\n            num_classes=decoder_num_classes,\n            embed_dim=decoder_embed_dim,\n            depth=decoder_depth,\n            num_heads=decoder_num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate,\n            norm_layer=norm_layer,\n            init_values=init_values,\n            tubelet_size=tubelet_size,\n            with_cp=with_cp,\n            cos_attn=cos_attn)\n\n        self.encoder_to_decoder = nn.Linear(\n            encoder_embed_dim, decoder_embed_dim, bias=False)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\n        self.pos_embed = get_sinusoid_encoding_table(\n            self.encoder.patch_embed.num_patches, decoder_embed_dim)\n\n        trunc_normal_(self.mask_token, std=.02)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token', 'mask_token'}\n\n    def forward(self, x, mask, decode_mask=None):\n        decode_vis = mask if decode_mask is None else ~decode_mask\n\n        x_vis = self.encoder(x, mask)  # [B, N_vis, C_e]\n        x_vis = self.encoder_to_decoder(x_vis)  # [B, N_vis, C_d]\n        B, N_vis, C = x_vis.shape\n\n        # we don't unshuffle the correct visible token order,\n        # but shuffle the pos embedding accorddingly.\n        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(\n            x.device).clone().detach()\n        pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)\n        pos_emd_mask = expand_pos_embed[decode_vis].reshape(B, -1, C)\n\n        # [B, N, C_d]\n        x_full = torch.cat(\n            [x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1)\n        # NOTE: if N_mask==0, the shape of x is [B, N_mask, 3 * 16 * 16]\n        x = self.decoder(x_full, pos_emd_mask.shape[1])\n\n        return x", "\n\n@register_model\ndef pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=16,\n        encoder_embed_dim=384,\n        encoder_depth=12,\n        encoder_num_heads=6,\n        encoder_num_classes=0,\n        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n        decoder_embed_dim=192,\n        decoder_num_heads=3,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n    return model", "\n\n@register_model\ndef pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=16,\n        encoder_embed_dim=768,\n        encoder_depth=12,\n        encoder_num_heads=12,\n        encoder_num_classes=0,\n        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n        decoder_embed_dim=384,\n        decoder_num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n    return model", "\n\n@register_model\ndef pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=16,\n        encoder_embed_dim=1024,\n        encoder_depth=24,\n        encoder_num_heads=16,\n        encoder_num_classes=0,\n        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n        decoder_embed_dim=512,\n        decoder_num_heads=8,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n    return model", "\n\n@register_model\ndef pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=16,\n        encoder_embed_dim=1280,\n        encoder_depth=32,\n        encoder_num_heads=16,\n        encoder_num_classes=0,\n        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n        decoder_embed_dim=512,\n        decoder_num_heads=8,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n    return model", "\n\n@register_model\ndef pretrain_videomae_giant_patch14_224(pretrained=False, **kwargs):\n    model = PretrainVisionTransformer(\n        img_size=224,\n        patch_size=14,\n        encoder_embed_dim=1408,\n        encoder_depth=40,\n        encoder_num_heads=16,\n        encoder_num_classes=0,\n        decoder_num_classes=1176,  # 14 * 14 * 3 * 2,\n        decoder_embed_dim=512,\n        decoder_num_heads=8,\n        mlp_ratio=48 / 11,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs)\n    model.default_cfg = _cfg()\n    if pretrained:\n        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n    return model", ""]}
