{"filename": "example.py", "chunked_list": ["from src.linkedin.api import API\nfrom src.linkedin.user import User\nfrom rich import print\nimport os\n\nos.environ[\n    \"LINKEDIN_TOKEN\"\n] = \"AQUsFm3ywriXg7JvgFCXW3uSsnI5lmvhq1cmzbrN0FRQK1FRRbdkxIo7w9SpQ09fb4hxWoO22mJb8_6Llox5PuBxpE8GpweThQy0xqAwcD2Ni8LDFo4q0i_0RG3THsBVDn_WgXRryzvIW0vlCuM2aUQmsvPzQR2fHjqSYb-k5n_nOgA6Ak7SRQ1lcF92yoN-R22M_ovaVXxfhK0MpHIKEe89BJRaSIdA_TPIsBAjO3gQiY2KLz2cZyDV1JXSW5ukeEK-nD2eROv10rWFL-3J4gWiIBw886J-PdArTZquhEyEGUV056ggAtLnevO8gjveNasJ912Rw8Qmc0ZRvoTK5c0ros56ZQ\"\n\napi = API.from_env()", "\napi = API.from_env()\n\nuser = User()\nres = user.create_post(\n    \"test from my custom python APIs with two images\",\n    images=[\n        (\"/home/zuppif/Documents/LinkedInGPT/grogu.jpg\", \"grogu\"),\n        (\"/home/zuppif/Documents/LinkedInGPT/grogu_2.png\", \"grogu2\"),\n    ],", "        (\"/home/zuppif/Documents/LinkedInGPT/grogu_2.png\", \"grogu2\"),\n    ],\n)\n"]}
{"filename": "gurus/linkedin_ai.py", "chunked_list": ["import sys\n# lazy to make a package\nsys.path.append(\".\")\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom pathlib import Path\n\nfrom langchain.chat_models import ChatOpenAI", "\nfrom langchain.chat_models import ChatOpenAI\n\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nfrom src.guru import Guru\nfrom src.actions.linkedIn import PostOnLinkedInAction\nfrom src.storages import SQLiteStorage", "from src.actions.linkedIn import PostOnLinkedInAction\nfrom src.storages import SQLiteStorage\nfrom src.content_providers import TrendingAIPapersProvider\nfrom src.confirmations.input_confirmation import input_confirmation\n\nprompt = PromptTemplate(\n    input_variables=[\"content\", \"bot_name\"],\n    template=Path(\"prompts/guru.prompt\").read_text(),\n)\n", ")\n\nllm = ChatOpenAI(temperature=0)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nguru = Guru(\n    name=\"Leonardo\",\n    content_provider=TrendingAIPapersProvider(),\n    storage=SQLiteStorage(),", "    content_provider=TrendingAIPapersProvider(),\n    storage=SQLiteStorage(),\n    action=PostOnLinkedInAction(),\n    confirmation=input_confirmation,\n    llm_chain=chain\n)\n\nguru.run()"]}
{"filename": "experiments/bot.py", "chunked_list": ["import json\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nfrom pathlib import Path\n\nfrom langchain.agents import Tool", "\nfrom langchain.agents import Tool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom rich import print\n\nfrom db import insert_post\nfrom linkedin.user import User\nfrom logger import logger\nfrom tools.linkedIn import write_linkedin_post", "from logger import logger\nfrom tools.linkedIn import write_linkedin_post\nfrom tools.papers import get_a_trending_paper_for_a_post\n\nuser = User()\n\nmemory = ConversationBufferWindowMemory(\n    memory_key=\"chat_history\", k=5, return_messages=True\n)\n", ")\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"paper\", \"bot_name\"],\n    template=Path(\"prompts/bot.prompt\").read_text(),\n)\n", ")\n\nllm = ChatOpenAI(temperature=0)\n\npaper = get_a_trending_paper_for_a_post(only_from_db=True)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\ncontent = chain.run({\"paper\": json.dumps(paper), \"bot_name\": \"Leonardo\"})\nprint(paper)", "content = chain.run({\"paper\": json.dumps(paper), \"bot_name\": \"Leonardo\"})\nprint(paper)\nprint(content)\n\nconfirmation = input(\"Proceed? [y/n]:\")\nif confirmation == \"y\":\n    print(\"Writing...\")\n    write_linkedin_post(user, content, media_url=paper[\"media\"])\n", ""]}
{"filename": "experiments/agent.py", "chunked_list": ["import os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\nimport json\nimport random\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import List", "from pathlib import Path\nfrom typing import List\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.agents import Tool, initialize_agent, load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.experimental import AutoGPT\nfrom langchain.memory import (ConversationBufferMemory,\n                              ConversationBufferWindowMemory)", "from langchain.memory import (ConversationBufferMemory,\n                              ConversationBufferWindowMemory)\nfrom langchain.tools.file_management.read import ReadFileTool\nfrom langchain.tools.file_management.write import WriteFileTool\nfrom langchain.utilities import (ArxivAPIWrapper, GoogleSearchAPIWrapper,\n                                 TextRequestsWrapper)\nfrom rich import print\n\nfrom db import Paper\nfrom linkedin.user import User", "from db import Paper\nfrom linkedin.user import User\n\nuser = User()\n\n\ndef get_paper_info(uid: str, *args, **kwargs):\n    response = requests.get(f\"https://paperswithcode.com{uid}\")\n    result = {}\n    if response.status_code == 200:", "    result = {}\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paper_abstract_div = soup.select_one(\".paper-abstract\")\n        # Extract the abstract\n        abstract = paper_abstract_div.find(\"p\").text.strip()\n        # Extract the arXiv URL\n        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\n        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}", "\n        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\n    else:\n        print(\n            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n        )\n\n    return result\n", "    return result\n\n\ndef write_linkedin_post(content: str, media_url: str):\n    file_path = f\"media.{Path(media_url).suffix}\"\n    with open(file_path, \"wb\") as f:\n        f.write(requests.get(media_url).content)\n    user.create_post(\n        content\n        + \"\\n#opensource #llms #datascience #machinelearning #programming #ai #ds #python #deeplearning #nlp\",", "        content\n        + \"\\n#opensource #llms #datascience #machinelearning #programming #ai #ds #python #deeplearning #nlp\",\n        [(file_path, \"media\")],\n    )\n    return \"Post created! Good job!\"\n\n\ndef get_latest_papers(*args, **kwargs) -> List[Paper]:\n    response = requests.get(\"https://paperswithcode.com/\")\n    results = []", "    response = requests.get(\"https://paperswithcode.com/\")\n    results = []\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        for row in soup.select(\n            \".infinite-container .row.infinite-item.item.paper-card\"\n        ):\n            paper_dict = {\n                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),", "                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n                \"media\": row.select_one(\".item-image\")[\"style\"]\n                .split(\"('\")[1]\n                .split(\"')\")[0],\n                \"tags\": [\n                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n                ],\n                \"stars\": int(\n                    row.select_one(\".entity-stars .badge\")", "                \"stars\": int(\n                    row.select_one(\".entity-stars .badge\")\n                    .get_text(strip=True)\n                    .split(\" \")[0]\n                    .replace(\",\", \"\")\n                ),\n                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n            }\n            paper_dict = paper_dict", "            }\n            paper_dict = paper_dict\n            results.append({**paper_dict, **get_paper_info(paper_dict[\"uid\"])})\n        else:\n            print(\"No div element with the specified class was found\")\n    else:\n        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n\n    return results\n", "    return results\n\n\ndef get_a_trending_paper(*args, **kwargs) -> Paper:\n    papers = get_latest_papers()\n    print(papers)\n    paper = random.choice(papers)\n    return paper\n\n", "\n\n# write_linkedin_post(\n#     \"foo\",\n#     \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/3ef6f4ee-4b0b-4654-8bcb-9f0f1299261f.jpg\",\n# )\n\n# print(get_a_trending_paper())\n\ntools = [", "\ntools = [\n    Tool(\n        name=\"get_a_trending_paper\",\n        func=get_a_trending_paper,\n        description=\"Use this to find a new and trending AI paper, it returns a JSON with information about a paper.\",\n    ),\n    #     # Tool(\n    #     #     name=\"get_paper_info\",\n    #     #     func=get_paper_info,", "    #     #     name=\"get_paper_info\",\n    #     #     func=get_paper_info,\n    #     #     description=\"Use this tool to get the abstract and the arxiv link from a `uid`. It will return a JSON. You need to pass the `uid`\",\n    #     # ),\n    # Tool(\n    #     name=\"write LinkedIn post\",\n    #     func=write_linkedin_post,\n    #     description=\"Use it to write a LinkedIn post, input is the post's content and an image url.\",\n    # ),\n]", "    # ),\n]\n\nmemory = ConversationBufferWindowMemory(\n    memory_key=\"chat_history\", k=10, return_messages=True\n)\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n", "from langchain.chains import LLMChain\n\nprompt = PromptTemplate(\n    input_variables=[\"paper\"],\n    template=Path(\"goal_agent.prompt\").read_text(),\n)\n\n\nllm = ChatOpenAI(temperature=0)\n", "llm = ChatOpenAI(temperature=0)\n\npaper = get_a_trending_paper()\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\ncontent = chain.run(json.dumps(paper))\nprint(paper)\nprint(content)\n# write_linkedin_post(content, paper['media'])", "print(content)\n# write_linkedin_post(content, paper['media'])\n\n# conversational_agent = initialize_agent(\n#     agent=\"chat-conversational-react-description\",\n#     tools=tools,\n#     llm=ChatOpenAI(temperature=0),\n#     verbose=True,\n#     max_iterations=10,\n#     early_stopping_method=\"generate\",", "#     max_iterations=10,\n#     early_stopping_method=\"generate\",\n#     memory=memory,\n# )\n\n\n# print(conversational_agent.run(Path(\"goal_agent.prompt\").read_text()))\n# {\n#     'title': 'Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond',\n#     'subtitle': 'This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.',", "#     'title': 'Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond',\n#     'subtitle': 'This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.',\n#     'media': 'https://production-media.paperswithcode.com/thumbnails/papergithubrepo/16d0f450-e5e7-4471-9a8b-42727da19551.gif',\n#     'tags': [],\n#     'stars': 2156,\n#     'github_link': 'https://github.com/mooler0410/llmspracticalguide',\n#     'uid': '/paper/harnessing-the-power-of-llms-in-practice-a',\n#     'abstract': 'This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. \n# Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, \n# such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to ", "# Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, \n# such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to \n# understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive\n# guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \n# \\\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.',\n#     'arxiv_link': 'https://arxiv.org/pdf/2304.13712v2.pdf'\n# }\nUnlock the Power of Large Language Models in NLP Tasks \ud83e\udd16\ud83d\udcdd\n\n", "\n\nThis paper provides a practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. Key takeaways include: \n\n\u25aa\ufe0f Understanding the influence of pre-training data, training data, and test data on LLMs \n\u25aa\ufe0f Detailed discussion of use and non-use cases of LLMs for various NLP tasks \n\u25aa\ufe0f Importance of data and specific challenges associated with each NLP task \n\u25aa\ufe0f Impact of spurious biases on LLMs and other essential considerations \n\n\ud83d\udd17 GitHub: https://github.com/mooler0410/llmspracticalguide", "\n\ud83d\udd17 GitHub: https://github.com/mooler0410/llmspracticalguide\n\ud83d\udd17 Arvix: https://arxiv.org/pdf/2304.13712v2.pdf"]}
{"filename": "experiments/db.py", "chunked_list": ["import sqlite3\nfrom typing import List, TypedDict\n\n\"\"\"\nBasically some poor man CRUD functions to store papers and post inside a sqlite db.\n\"\"\"\n\n\nclass Paper(TypedDict):\n    uid: str\n    title: str\n    subtitle: str\n    abstract: str\n    media: str\n    tags: List[str]\n    stars: int\n    github_link: str\n    arxiv_link: str", "class Paper(TypedDict):\n    uid: str\n    title: str\n    subtitle: str\n    abstract: str\n    media: str\n    tags: List[str]\n    stars: int\n    github_link: str\n    arxiv_link: str", "\n\nclass Post(TypedDict):\n    paper_uid: str\n    text: str\n    media: str\n\n\ndef create_connection() -> sqlite3.Connection:\n    return sqlite3.connect(\"agent.db\")", "def create_connection() -> sqlite3.Connection:\n    return sqlite3.connect(\"agent.db\")\n\n\ndef row_to_dict(cursor, row):\n    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}\n\n\ndef create_table_papers(conn: sqlite3.Connection):\n    cur = conn.cursor()\n    cur.execute(\n        \"\"\"CREATE TABLE IF NOT EXISTS papers (\n                                    uid TEXT PRIMARY KEY,\n                                    title TEXT NOT NULL,\n                                    subtitle TEXT,\n                                    abstract TEXT,\n                                    media TEXT,\n                                    tags TEXT,\n                                    stars INTEGER,\n                                    github_link TEXT,\n                                    arxiv_link TEXT\n                                );\"\"\"\n    )", "def create_table_papers(conn: sqlite3.Connection):\n    cur = conn.cursor()\n    cur.execute(\n        \"\"\"CREATE TABLE IF NOT EXISTS papers (\n                                    uid TEXT PRIMARY KEY,\n                                    title TEXT NOT NULL,\n                                    subtitle TEXT,\n                                    abstract TEXT,\n                                    media TEXT,\n                                    tags TEXT,\n                                    stars INTEGER,\n                                    github_link TEXT,\n                                    arxiv_link TEXT\n                                );\"\"\"\n    )", "\n\ndef delete_paper(conn: sqlite3.Connection, uid: str):\n    sql = \"DELETE FROM papers WHERE uid=?\"\n    cur = conn.cursor()\n    cur.execute(sql, (uid,))\n    conn.commit()\n\n\ndef insert_paper(conn: sqlite3.Connection, paper: Paper):\n    sql = \"\"\"INSERT OR IGNORE INTO papers(uid, title, subtitle, abstract, media, tags, stars, github_link, arxiv_link)\n             VALUES(:uid, :title, :subtitle, :abstract, :media, :tags, :stars, :github_link, :arxiv_link)\"\"\"\n    cur = conn.cursor()\n    cur.execute(\n        sql,\n        {\n            \"uid\": paper[\"uid\"],\n            \"title\": paper[\"title\"],\n            \"subtitle\": paper[\"subtitle\"],\n            \"abstract\": paper[\"abstract\"],\n            \"media\": paper[\"media\"],\n            \"tags\": \",\".join(paper[\"tags\"]),\n            \"stars\": paper[\"stars\"],\n            \"github_link\": paper[\"github_link\"],\n            \"arxiv_link\": paper[\"arxiv_link\"],\n        },\n    )\n    conn.commit()\n    return cur.lastrowid", "\ndef insert_paper(conn: sqlite3.Connection, paper: Paper):\n    sql = \"\"\"INSERT OR IGNORE INTO papers(uid, title, subtitle, abstract, media, tags, stars, github_link, arxiv_link)\n             VALUES(:uid, :title, :subtitle, :abstract, :media, :tags, :stars, :github_link, :arxiv_link)\"\"\"\n    cur = conn.cursor()\n    cur.execute(\n        sql,\n        {\n            \"uid\": paper[\"uid\"],\n            \"title\": paper[\"title\"],\n            \"subtitle\": paper[\"subtitle\"],\n            \"abstract\": paper[\"abstract\"],\n            \"media\": paper[\"media\"],\n            \"tags\": \",\".join(paper[\"tags\"]),\n            \"stars\": paper[\"stars\"],\n            \"github_link\": paper[\"github_link\"],\n            \"arxiv_link\": paper[\"arxiv_link\"],\n        },\n    )\n    conn.commit()\n    return cur.lastrowid", "\n\ndef get_all_papers(conn: sqlite3.Connection) -> List[Paper]:\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM papers\")\n    rows = cur.fetchall()\n    return [row_to_dict(cur, row) for row in rows]\n\n\ndef get_paper_by_uid(conn: sqlite3.Connection, uid: str) -> Paper:\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM papers WHERE uid=?\", (uid,))\n    row = cur.fetchone()\n    if row:\n        return row_to_dict(cur, row)\n    return None", "\ndef get_paper_by_uid(conn: sqlite3.Connection, uid: str) -> Paper:\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM papers WHERE uid=?\", (uid,))\n    row = cur.fetchone()\n    if row:\n        return row_to_dict(cur, row)\n    return None\n\n\ndef create_post_table(conn: sqlite3.Connection):\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS posts (\n            paper_uid TEXT PRIMARY KEY,\n            text TEXT,\n            media TEXT\n        )\n    \"\"\"\n    )\n    conn.commit()", "\n\ndef create_post_table(conn: sqlite3.Connection):\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS posts (\n            paper_uid TEXT PRIMARY KEY,\n            text TEXT,\n            media TEXT\n        )\n    \"\"\"\n    )\n    conn.commit()", "\n\ndef insert_post(conn: sqlite3.Connection, post: Post):\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        INSERT INTO posts (paper_uid, text, media) VALUES (?, ?, ?)\n    \"\"\",\n        (post[\"paper_uid\"], post[\"text\"], post[\"media\"]),\n    )\n    conn.commit()", "\n\ndef get_all_posts(conn: sqlite3.Connection) -> List[Post]:\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT * FROM posts\")\n    rows = cursor.fetchall()\n    return [row_to_dict(cursor, row) for row in rows]\n\n\ndef get_all_papers_without_a_post(conn: sqlite3.Connection) -> List[Paper]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT papers.* FROM papers\n        LEFT JOIN posts ON papers.uid = posts.paper_uid\n        WHERE posts.paper_uid IS NULL\n    \"\"\"\n    )\n    rows = cursor.fetchall()\n    return [row_to_dict(cursor, row) for row in rows]", "\ndef get_all_papers_without_a_post(conn: sqlite3.Connection) -> List[Paper]:\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT papers.* FROM papers\n        LEFT JOIN posts ON papers.uid = posts.paper_uid\n        WHERE posts.paper_uid IS NULL\n    \"\"\"\n    )\n    rows = cursor.fetchall()\n    return [row_to_dict(cursor, row) for row in rows]", "\n\ndef init_db() -> sqlite3.Connection:\n    conn = create_connection()\n    create_table_papers(conn)\n    create_post_table(conn)\n    return conn\n\n\nif __name__ == \"__main__\":\n    conn = create_connection()\n    create_table_papers(conn)\n    create_post_table(conn)\n\n    insert_paper(\n        conn,\n        Paper(\n            uid=\"foo1\",\n            title=\"FOO1\",\n            subtitle=\"baa\",\n            abstract=\"asddsa\",\n            media=\"/media.png\",\n            tags=[\"foo\"],\n            stars=10,\n            github_link=\"github/foo\",\n            arxiv_link=\"arxiv/foo\",\n        ),\n    )\n    insert_paper(\n        conn,\n        Paper(\n            uid=\"foo2\",\n            title=\"FOO2\",\n            subtitle=\"baa\",\n            abstract=\"asddsa\",\n            media=\"/media.png\",\n            tags=[\"foo\"],\n            stars=10,\n            github_link=\"github/foo\",\n            arxiv_link=\"arxiv/foo\",\n        ),\n    )\n    papers = get_all_papers(conn)\n\n    post = Post(paper_uid=papers[0][\"uid\"], text=\"a post\", media=\"example.com\")\n    insert_post(conn, post)\n    print(get_all_posts(conn))\n    print(get_all_papers_without_a_post(conn))", "\nif __name__ == \"__main__\":\n    conn = create_connection()\n    create_table_papers(conn)\n    create_post_table(conn)\n\n    insert_paper(\n        conn,\n        Paper(\n            uid=\"foo1\",\n            title=\"FOO1\",\n            subtitle=\"baa\",\n            abstract=\"asddsa\",\n            media=\"/media.png\",\n            tags=[\"foo\"],\n            stars=10,\n            github_link=\"github/foo\",\n            arxiv_link=\"arxiv/foo\",\n        ),\n    )\n    insert_paper(\n        conn,\n        Paper(\n            uid=\"foo2\",\n            title=\"FOO2\",\n            subtitle=\"baa\",\n            abstract=\"asddsa\",\n            media=\"/media.png\",\n            tags=[\"foo\"],\n            stars=10,\n            github_link=\"github/foo\",\n            arxiv_link=\"arxiv/foo\",\n        ),\n    )\n    papers = get_all_papers(conn)\n\n    post = Post(paper_uid=papers[0][\"uid\"], text=\"a post\", media=\"example.com\")\n    insert_post(conn, post)\n    print(get_all_posts(conn))\n    print(get_all_papers_without_a_post(conn))", ""]}
{"filename": "experiments/auto_gpt.py", "chunked_list": ["import os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import List", "from pathlib import Path\nfrom typing import List\n\nimport faiss\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.agents import Tool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.embeddings import OpenAIEmbeddings", "from langchain.docstore import InMemoryDocstore\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.experimental import AutoGPT\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.vectorstores import FAISS\nfrom rich import print\nfrom functools import cache\nfrom db import Paper\nfrom linkedin.user import User\n", "from linkedin.user import User\n\nuser = User()\n\n\ndef get_paper_info(uid: str, *args, **kwargs):\n    response = requests.get(f\"https://paperswithcode.com{uid}\")\n    result = {}\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paper_abstract_div = soup.select_one(\".paper-abstract\")\n        # Extract the abstract\n        abstract = paper_abstract_div.find(\"p\").text.strip()\n        # Extract the arXiv URL\n        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\n        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\n    else:\n        print(\n            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n        )\n\n    return result", "\n\ndef write_linkedin_post(content: str, media_url: str):\n    file_path = f\"media.{Path(media_url).suffix}\"\n    with open(file_path, \"wb\") as f:\n        f.write(requests.get(media_url).content)\n    user.create_post(\n        content\n        + \"\\n#opensource #llms #datascience #machinelearning #programming #ai #ds #python #deeplearning #nlp\",\n        [(file_path, \"media\")],\n    )\n    return \"Post created! Good job!\"", "\n\ndef get_latest_papers(*args, **kwargs) -> List[Paper]:\n    response = requests.get(\"https://paperswithcode.com/\")\n    results = []\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        for row in soup.select(\n            \".infinite-container .row.infinite-item.item.paper-card\"\n        ):\n            paper_dict = {\n                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n                \"media\": row.select_one(\".item-image\")[\"style\"]\n                .split(\"('\")[1]\n                .split(\"')\")[0],\n                \"tags\": [\n                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n                ],\n                \"stars\": int(\n                    row.select_one(\".entity-stars .badge\")\n                    .get_text(strip=True)\n                    .split(\" \")[0]\n                    .replace(\",\", \"\")\n                ),\n                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n            }\n            paper_dict = paper_dict\n            results.append({**paper_dict, **get_paper_info(paper_dict[\"uid\"])})\n        else:\n            print(\"No div element with the specified class was found\")\n    else:\n        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n\n    return results", "\n@cache\ndef get_a_trending_paper(*args, **kwargs):\n    papers = get_latest_papers()\n    print(papers)\n    paper = random.choice(papers)\n    return json.dumps(paper)\n\n\n# write_linkedin_post(", "\n# write_linkedin_post(\n#     \"foo\",\n#     \"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/3ef6f4ee-4b0b-4654-8bcb-9f0f1299261f.jpg\",\n# )\n\n# print(get_a_trending_paper())\n\ntools = [\n    Tool(", "tools = [\n    Tool(\n        name=\"get_a_trending_paper\",\n        func=get_a_trending_paper,\n        description=\"Use this to find a new and trending AI paper, it returns a JSON with information about a paper.\",\n    ),\n    #     # Tool(\n    #     #     name=\"get_paper_info\",\n    #     #     func=get_paper_info,\n    #     #     description=\"Use this tool to get the abstract and the arxiv link from a `uid`. It will return a JSON. You need to pass the `uid`\",", "    #     #     func=get_paper_info,\n    #     #     description=\"Use this tool to get the abstract and the arxiv link from a `uid`. It will return a JSON. You need to pass the `uid`\",\n    #     # ),\n    Tool(\n        name=\"write LinkedIn post\",\n        func=write_linkedin_post,\n        description=\"Use it to write a LinkedIn post, inputs are `content`, the post text and `media_url`, a link to a media from the paper. \",\n    ),\n]\n", "]\n\nmemory = ConversationBufferWindowMemory(\n    memory_key=\"chat_history\", k=3, return_messages=True\n)\n\n# Define your embedding model\nembeddings_model = OpenAIEmbeddings()\n# Initialize the vectorstore as empty\n", "# Initialize the vectorstore as empty\n\nembedding_size = 1536\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n\nagent = AutoGPT.from_llm_and_tools(\n    ai_name=\"Jacob\",\n    ai_role=\"Assistant\",\n    tools=tools,", "    ai_role=\"Assistant\",\n    tools=tools,\n    llm=ChatOpenAI(temperature=0),\n    memory=vectorstore.as_retriever(),\n)\n# Set verbose to be true\n# agent.chain.verbose = True\n\nagent.run([Path(\"goal.prompt\").read_text()])\n", "agent.run([Path(\"goal.prompt\").read_text()])\n"]}
{"filename": "src/types.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Any, Dict, Optional\n\n\n@dataclass\nclass Content:\n    uid: str\n    data: Dict[str, Any]\n    created: Optional[bool] = False\n", "\n\n@dataclass\nclass GeneratedContent:\n    text: str\n    media_url: Optional[str] = None\n"]}
{"filename": "src/logger.py", "chunked_list": ["import logging\nimport os\n\nfrom rich.logging import RichHandler\n\nlogger = logging.getLogger(\"linkedInGPT\")\nlogger.setLevel(os.environ.get(\"LOG_LEVEL\", logging.INFO))\nlogger.addHandler(RichHandler())\n", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/guru.py", "chunked_list": ["import random\nfrom typing import Callable, List, Optional\n\nfrom langchain import LLMChain\n\nfrom src.actions import Action\nfrom src.content_providers import ContentProvider\nfrom src.storages import Storage\nfrom src.types import Content, GeneratedContent\n", "from src.types import Content, GeneratedContent\n\nfrom src.logger import logger\n\ndef random_content_selection_strategy(contents: List[Content]) -> Content:\n    return random.choice(contents)\n\n\nclass Guru:\n    def __init__(\n        self,\n        name: str,\n        content_provider: ContentProvider,\n        storage: Storage,\n        action: Action,\n        confirmation: Callable[[Callable], bool],\n        llm_chain: LLMChain,\n        content_selection_strategy: Optional[Callable[[List[Content]], Content]] = None,\n    ) -> None:\n        self.name = name\n        self.content_provider = content_provider\n        self.storage = storage\n        self.action = action\n        self.confirmation = confirmation\n        self.llm_chain = llm_chain\n        self.content_selection_strategy = (\n            random_content_selection_strategy\n            if content_selection_strategy is None\n            else content_selection_strategy\n        )\n\n    def run(self):\n        contents = self.content_provider.get_contents()\n        list(map(lambda content: self.storage.store(content), contents))\n        contents = self.storage.get_all(created=False)\n        logger.info(contents)\n        content = self.content_selection_strategy(contents)\n        generated_text = self.llm_chain.run({\"content\": content.__dict__, \"bot_name\": self.name})\n        logger.info(f\"Generated text for content:\\n{generated_text}\")\n        if self.confirmation(self.run):\n            logger.info(f\"Running action {self.action}\")\n            # [TODO] here I know in my setup what 'media' will be inside content because I will get it from paperswithcode\n            generated_content = GeneratedContent(generated_text, media_url=content.data['media'])\n            self.action(generated_content)\n            content.created = True\n            self.storage.update(content)\n            logger.info(\"Done!\")", "class Guru:\n    def __init__(\n        self,\n        name: str,\n        content_provider: ContentProvider,\n        storage: Storage,\n        action: Action,\n        confirmation: Callable[[Callable], bool],\n        llm_chain: LLMChain,\n        content_selection_strategy: Optional[Callable[[List[Content]], Content]] = None,\n    ) -> None:\n        self.name = name\n        self.content_provider = content_provider\n        self.storage = storage\n        self.action = action\n        self.confirmation = confirmation\n        self.llm_chain = llm_chain\n        self.content_selection_strategy = (\n            random_content_selection_strategy\n            if content_selection_strategy is None\n            else content_selection_strategy\n        )\n\n    def run(self):\n        contents = self.content_provider.get_contents()\n        list(map(lambda content: self.storage.store(content), contents))\n        contents = self.storage.get_all(created=False)\n        logger.info(contents)\n        content = self.content_selection_strategy(contents)\n        generated_text = self.llm_chain.run({\"content\": content.__dict__, \"bot_name\": self.name})\n        logger.info(f\"Generated text for content:\\n{generated_text}\")\n        if self.confirmation(self.run):\n            logger.info(f\"Running action {self.action}\")\n            # [TODO] here I know in my setup what 'media' will be inside content because I will get it from paperswithcode\n            generated_content = GeneratedContent(generated_text, media_url=content.data['media'])\n            self.action(generated_content)\n            content.created = True\n            self.storage.update(content)\n            logger.info(\"Done!\")", ""]}
{"filename": "src/actions/base.py", "chunked_list": ["from typing import Any\n\nfrom src.types import GeneratedContent\n\n\nclass Action:\n    def __call__(self, content: GeneratedContent) -> Any:\n        pass\n", ""]}
{"filename": "src/actions/__init__.py", "chunked_list": ["from .base import Action\nfrom .linkedIn import PostOnLinkedInAction\n"]}
{"filename": "src/actions/linkedIn.py", "chunked_list": ["from pathlib import Path\nfrom typing import Any\n\nimport requests\n\nfrom linkedin_python import User\nfrom src.types import GeneratedContent\n\nfrom .base import Action\n", "from .base import Action\n\n\nclass PostOnLinkedInAction(Action):\n    def __init__(self) -> None:\n        self.user = User()\n\n    def __call__(self, content: GeneratedContent):\n        if content.media_url is not None:\n            media_url = content.media_url\n            file_path = f\"media.{Path(media_url).suffix}\"\n            with open(file_path, \"wb\") as f:\n                f.write(requests.get(media_url).content)\n            self.user.create_post(\n                content.text,\n                [(file_path, \"media\")],\n            )\n        else:\n            self.user.create_post(content.text)", ""]}
{"filename": "src/storages/sqlite.py", "chunked_list": ["import json\nimport sqlite3\nfrom typing import Dict, List, Optional, TypedDict\n\nfrom src.types import Content\n\nfrom .base import Storage\n\n\nclass SQLiteStorage(Storage):\n    def __init__(self, name: str = \"content.db\"):\n        self.conn = sqlite3.connect(\"content.db\")\n        self._init_table()\n\n    def _init_table(self):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS content (\n            uid TEXT PRIMARY KEY,\n            data TEXT,\n            created INTEGER\n        )\n        \"\"\"\n        )\n        self.conn.commit()\n\n    def store(self, content: Content):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        INSERT OR IGNORE INTO content (uid, data, created) VALUES (?, ?, ?)\n        \"\"\",\n            (content.uid, json.dumps(content.data), int(content.created)),\n        )\n        self.conn.commit()\n\n    def update(self, updated_content: Content):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        UPDATE content\n        SET data = ?, created = ?\n        WHERE uid = ?\n        \"\"\",\n            (\n                json.dumps(updated_content.data),\n                int(updated_content.created),\n                updated_content.uid,\n            ),\n        )\n        self.conn.commit()\n\n    def get_all(self, created: Optional[bool] = None) -> List[Content]:\n        cursor = self.conn.cursor()\n\n        if created is None:\n            cursor.execute(\"SELECT uid, data, created FROM content\")\n        else:\n            cursor.execute(\n                \"SELECT uid, data, created FROM content WHERE created=?\",\n                (int(created),),\n            )\n\n        results = cursor.fetchall()\n\n        contents = [\n            Content(uid=row[0], data=json.loads(row[1]), created=bool(row[2]))\n            for row in results\n        ]\n\n        return contents\n\n    def close(self):\n        self.conn.close", "\nclass SQLiteStorage(Storage):\n    def __init__(self, name: str = \"content.db\"):\n        self.conn = sqlite3.connect(\"content.db\")\n        self._init_table()\n\n    def _init_table(self):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        CREATE TABLE IF NOT EXISTS content (\n            uid TEXT PRIMARY KEY,\n            data TEXT,\n            created INTEGER\n        )\n        \"\"\"\n        )\n        self.conn.commit()\n\n    def store(self, content: Content):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        INSERT OR IGNORE INTO content (uid, data, created) VALUES (?, ?, ?)\n        \"\"\",\n            (content.uid, json.dumps(content.data), int(content.created)),\n        )\n        self.conn.commit()\n\n    def update(self, updated_content: Content):\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"\"\"\n        UPDATE content\n        SET data = ?, created = ?\n        WHERE uid = ?\n        \"\"\",\n            (\n                json.dumps(updated_content.data),\n                int(updated_content.created),\n                updated_content.uid,\n            ),\n        )\n        self.conn.commit()\n\n    def get_all(self, created: Optional[bool] = None) -> List[Content]:\n        cursor = self.conn.cursor()\n\n        if created is None:\n            cursor.execute(\"SELECT uid, data, created FROM content\")\n        else:\n            cursor.execute(\n                \"SELECT uid, data, created FROM content WHERE created=?\",\n                (int(created),),\n            )\n\n        results = cursor.fetchall()\n\n        contents = [\n            Content(uid=row[0], data=json.loads(row[1]), created=bool(row[2]))\n            for row in results\n        ]\n\n        return contents\n\n    def close(self):\n        self.conn.close", ""]}
{"filename": "src/storages/base.py", "chunked_list": ["from typing import List\n\nfrom src.types import Content\n\n\nclass Storage:\n    def store(self, content: Content):\n        raise NotImplemented\n\n    def update(self, content: Content):\n        raise NotImplemented\n\n    def get_all(self) -> List[Content]:\n        raise NotImplemented\n\n    def close(self):\n        raise NotImplemented", ""]}
{"filename": "src/storages/__init__.py", "chunked_list": ["from .base import Storage\nfrom .sqlite import SQLiteStorage\n"]}
{"filename": "src/content_providers/base.py", "chunked_list": ["from typing import List\n\nfrom src.types import Content\n\n\nclass ContentProvider:\n    def get_contents(self) -> List[Content]:\n        raise NotImplemented\n", ""]}
{"filename": "src/content_providers/__init__.py", "chunked_list": ["from .ai_papers.provider import TrendingAIPapersProvider\nfrom .base import ContentProvider\n"]}
{"filename": "src/content_providers/ai_papers/__init__.py", "chunked_list": [""]}
{"filename": "src/content_providers/ai_papers/papers_with_code.py", "chunked_list": ["from random import choice\nfrom typing import Dict, List\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom src.logger import logger\nfrom src.types import Content\n\n\ndef get_paper_info_from_papers_with_code(uid: str, *args, **kwargs) -> Dict:\n    \"\"\"\n    Fetching the paper little summary page on papers with code, e.g.\n    https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos\n\n    This function returns a dictionary with `abtract` and `arxiv_link`.\n    \"\"\"\n    response = requests.get(f\"https://paperswithcode.com{uid}\")\n    result = {}\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paper_abstract_div = soup.select_one(\".paper-abstract\")\n        # Extract the abstract\n        abstract = paper_abstract_div.find(\"p\").text.strip()\n        # Extract the arXiv URL\n        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\n        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\n    else:\n        logger.warning(\n            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n        )\n\n    return result", "\n\ndef get_paper_info_from_papers_with_code(uid: str, *args, **kwargs) -> Dict:\n    \"\"\"\n    Fetching the paper little summary page on papers with code, e.g.\n    https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos\n\n    This function returns a dictionary with `abtract` and `arxiv_link`.\n    \"\"\"\n    response = requests.get(f\"https://paperswithcode.com{uid}\")\n    result = {}\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paper_abstract_div = soup.select_one(\".paper-abstract\")\n        # Extract the abstract\n        abstract = paper_abstract_div.find(\"p\").text.strip()\n        # Extract the arXiv URL\n        arxiv_url = paper_abstract_div.find(\"a\", class_=\"badge badge-light\")[\"href\"]\n\n        result = {\"abstract\": abstract, \"arxiv_link\": arxiv_url}\n\n    else:\n        logger.warning(\n            f\"Failed to fetch https://paperswithcode.com{uid}. Status code: {response.status_code}\"\n        )\n\n    return result", "\n\ndef get_latest_papers_from_papers_with_code(*args, **kwargs) -> List[Content]:\n    logger.info(\"Getting papers from https://paperswithcode.com/\")\n    response = requests.get(\"https://paperswithcode.com/\")\n    results: List[Content] = []\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        for row in soup.select(\n            \".infinite-container .row.infinite-item.item.paper-card\"\n        ):\n            paper_dict = {\n                \"title\": row.select_one(\"h1 a\").get_text(strip=True),\n                \"subtitle\": row.select_one(\".item-strip-abstract\").get_text(strip=True),\n                \"media\": row.select_one(\".item-image\")[\"style\"]\n                .split(\"('\")[1]\n                .split(\"')\")[0],\n                \"tags\": [\n                    a.get_text(strip=True) for a in row.select(\".badge-primary a\")\n                ],\n                \"stars\": int(\n                    row.select_one(\".entity-stars .badge\")\n                    .get_text(strip=True)\n                    .split(\" \")[0]\n                    .replace(\",\", \"\")\n                ),\n                \"github_link\": row.select_one(\".item-github-link a\")[\"href\"],\n                \"uid\": row.select_one(\"h1 a\")[\"href\"],\n            }\n            paper_dict = paper_dict\n            results.append(\n                Content(\n                    paper_dict[\"uid\"],\n                    data={\n                        **paper_dict,\n                        **get_paper_info_from_papers_with_code(paper_dict[\"uid\"]),\n                    },\n                )\n            )\n        else:\n            logger.warning(\"Was not able to scrape the html for one row.\")\n    else:\n        logger.warning(\n            f\"Failed to fetch the webpage. Status code: {response.status_code}.\"\n        )\n\n    return results", ""]}
{"filename": "src/content_providers/ai_papers/provider.py", "chunked_list": ["from typing import List\n\nfrom src.types import Content\n\nfrom ..base import ContentProvider\nfrom .papers_with_code import get_latest_papers_from_papers_with_code\n\n\nclass TrendingAIPapersProvider(ContentProvider):\n    def get_contents(self) -> List[Content]:\n        return get_latest_papers_from_papers_with_code()", "class TrendingAIPapersProvider(ContentProvider):\n    def get_contents(self) -> List[Content]:\n        return get_latest_papers_from_papers_with_code()\n"]}
{"filename": "src/linkedin/schemas.py", "chunked_list": ["from typing import Dict, List, Optional, Tuple, TypedDict\n\n\nclass ProfilePicture(TypedDict):\n    displayImage: str\n\n\nclass LocalizedName(TypedDict):\n    it_IT: str\n", "\n\nclass PreferredLocale(TypedDict):\n    country: str\n    language: str\n\n\nclass Name(TypedDict):\n    localized: LocalizedName\n    preferredLocale: PreferredLocale", "\n\nclass UserProfile(TypedDict):\n    localizedLastName: str\n    profilePicture: ProfilePicture\n    firstName: Name\n    lastName: Name\n    id: str\n    localizedFirstName: str\n", "\n\nclass ServiceRelationship(TypedDict):\n    relationshipType: str\n    identifier: str\n\n\nclass RegisterUploadRequest(TypedDict):\n    recipes: List[str]\n    owner: str\n    serviceRelationships: List[ServiceRelationship]", "\n\nclass RegisterUploadBody(TypedDict):\n    registerUploadRequest: RegisterUploadRequest\n\n\nclass UploadHttpRequest(TypedDict):\n    uploadUrl: str\n    headers: Dict[str, str]\n", "\n\nclass UploadMechanism(TypedDict):\n    com_linkedin_digitalmedia_uploading_MediaUploadHttpRequest: UploadHttpRequest\n\n\nclass Value(TypedDict):\n    mediaArtifact: str\n    uploadMechanism: UploadMechanism\n    asset: str\n    assetRealTimeTopic: str", "\n\nclass RegisterUploadResponse(TypedDict):\n    value: Value\n\n\nclass Text(TypedDict):\n    text: str\n\n\nclass Title(TypedDict):\n    text: str", "\n\nclass Title(TypedDict):\n    text: str\n\n\nclass Description(TypedDict):\n    text: str\n\n\nclass Media(TypedDict):\n    status: str\n    description: Description\n    media: str\n    title: Title", "\n\nclass Media(TypedDict):\n    status: str\n    description: Description\n    media: str\n    title: Title\n\n\nclass ShareCommentary(TypedDict):\n    text: str", "\nclass ShareCommentary(TypedDict):\n    text: str\n\n\nclass ShareContent(TypedDict):\n    shareCommentary: ShareCommentary\n    shareMediaCategory: str\n    media: Optional[List[Media]]\n", "\n\nclass SpecificContent(TypedDict):\n    com_linkedin_ugc_ShareContent: ShareContent\n\n\nclass MemberNetworkVisibility(TypedDict):\n    com_linkedin_ugc_MemberNetworkVisibility: str\n\n\nclass CreatePostBody(TypedDict):\n    author: str\n    lifecycleState: str\n    specificContent: SpecificContent\n    visibility: MemberNetworkVisibility", "\n\nclass CreatePostBody(TypedDict):\n    author: str\n    lifecycleState: str\n    specificContent: SpecificContent\n    visibility: MemberNetworkVisibility\n\n\nclass CreatePostResponse(TypedDict):\n    id: str", "\nclass CreatePostResponse(TypedDict):\n    id: str\n"]}
{"filename": "src/linkedin/api.py", "chunked_list": ["import os\nfrom functools import cache\nfrom typing import Dict, TypedDict\n\nimport requests\n\nfrom .schemas import *\n\n\nclass API:\n    def __init__(self, token: str):\n        self.token = token\n        self.session = requests.Session()\n        self._set_headers()\n        self._set_hooks()\n\n    def _set_headers(self):\n        headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n        self.session.headers = headers\n\n    def _set_hooks(self):\n        self.session.hooks = {\n            \"response\": lambda r, *args, **kwargs: r.raise_for_status()\n        }\n\n    @cache\n    def get_me(self) -> UserProfile:\n        return self.session.get(\"https://api.linkedin.com/v2/me\").json()\n\n    def create_post(self, body: Dict) -> CreatePostResponse:\n        return self.session.post(\"https://api.linkedin.com/v2/ugcPosts\", json=body)\n\n    def register_upload(self, body: RegisterUploadBody) -> RegisterUploadResponse:\n        return self.session.post(\n            \"https://api.linkedin.com/v2/assets?action=registerUpload\", json=body\n        ).json()\n\n    def upload_image(self, file_path: str, upload_url: str) -> requests.Response:\n        with open(file_path, \"rb\") as file:\n            response = requests.put(\n                upload_url, data=file, headers={\"Authorization\": f\"Bearer {self.token}\"}\n            )\n        return response\n\n    @classmethod\n    def from_env(cls):\n        return cls(os.environ[\"LINKEDIN_TOKEN\"])", "\nclass API:\n    def __init__(self, token: str):\n        self.token = token\n        self.session = requests.Session()\n        self._set_headers()\n        self._set_hooks()\n\n    def _set_headers(self):\n        headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n        self.session.headers = headers\n\n    def _set_hooks(self):\n        self.session.hooks = {\n            \"response\": lambda r, *args, **kwargs: r.raise_for_status()\n        }\n\n    @cache\n    def get_me(self) -> UserProfile:\n        return self.session.get(\"https://api.linkedin.com/v2/me\").json()\n\n    def create_post(self, body: Dict) -> CreatePostResponse:\n        return self.session.post(\"https://api.linkedin.com/v2/ugcPosts\", json=body)\n\n    def register_upload(self, body: RegisterUploadBody) -> RegisterUploadResponse:\n        return self.session.post(\n            \"https://api.linkedin.com/v2/assets?action=registerUpload\", json=body\n        ).json()\n\n    def upload_image(self, file_path: str, upload_url: str) -> requests.Response:\n        with open(file_path, \"rb\") as file:\n            response = requests.put(\n                upload_url, data=file, headers={\"Authorization\": f\"Bearer {self.token}\"}\n            )\n        return response\n\n    @classmethod\n    def from_env(cls):\n        return cls(os.environ[\"LINKEDIN_TOKEN\"])", ""]}
{"filename": "src/linkedin/__init__.py", "chunked_list": [""]}
{"filename": "src/linkedin/user.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import List, Optional, Tuple\n\nfrom requests import Response\nfrom rich import print\n\nfrom .api import API\nfrom .schemas import *\n\n\nclass User:\n    def __init__(self, api: API = None):\n        self._api = API.from_env() if api is None else api\n        self._me = self._api.get_me()\n\n    def _register_and_upload_image(self, image_path: str) -> str:\n        author = f\"urn:li:person:{self._me['id']}\"\n        register_upload_body = RegisterUploadBody(\n            registerUploadRequest=RegisterUploadRequest(\n                recipes=[\"urn:li:digitalmediaRecipe:feedshare-image\"],\n                owner=author,\n                serviceRelationships=[\n                    ServiceRelationship(\n                        relationshipType=\"OWNER\",\n                        identifier=\"urn:li:userGeneratedContent\",\n                    )\n                ],\n            )\n        )\n        response = self._api.register_upload(register_upload_body)\n        # [NOTE] LinkedIn apis have '.' in the name making it impossible to use TypeDicts\n        upload_url = response[\"value\"][\"uploadMechanism\"][\n            \"com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest\"\n        ][\"uploadUrl\"]\n        asset_id = response[\"value\"][\"asset\"]\n        response = self._api.upload_image(image_path, upload_url)\n\n        return asset_id\n\n    def create_post(\n        self, text: str, images: Optional[List[Tuple[str, str]]] = None\n    ) -> bool:\n        \"\"\"To create a post with the LinkedIn Apis we need to\n         - (if we have an image): register the image upload, store the returned uploadUrl\n         - (if we have an image): upload the image\n         - send the post, (if we have an image) add the correct image to the `media` request body\n\n        Args:\n            text (str): _description_\n            image_path (Optional[str], optional): _description_. Defaults to None.\n\n        Returns:\n            bool: _description_\n        \"\"\"\n        shareMediaCategory = \"NONE\" if images is None else \"IMAGE\"\n        author = f\"urn:li:person:{self._me['id']}\"\n        media: List[Media] = []\n\n        if images is not None:\n            for image_path, image_description in images:\n                asset_id = self._register_and_upload_image(image_path)\n                media.append(\n                    Media(\n                        status=\"READY\",\n                        description=Description(text=image_description),\n                        media=asset_id,\n                        title=Title(text=\"Image Title\"),\n                    )\n                )\n        create_post_body = {\n            \"author\": author,\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\"text\": text},\n                    \"shareMediaCategory\": shareMediaCategory,\n                    \"media\": media,\n                }\n            },\n            \"visibility\": {\"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"},\n        }\n\n        return self._api.create_post(create_post_body).json()", "\n\nclass User:\n    def __init__(self, api: API = None):\n        self._api = API.from_env() if api is None else api\n        self._me = self._api.get_me()\n\n    def _register_and_upload_image(self, image_path: str) -> str:\n        author = f\"urn:li:person:{self._me['id']}\"\n        register_upload_body = RegisterUploadBody(\n            registerUploadRequest=RegisterUploadRequest(\n                recipes=[\"urn:li:digitalmediaRecipe:feedshare-image\"],\n                owner=author,\n                serviceRelationships=[\n                    ServiceRelationship(\n                        relationshipType=\"OWNER\",\n                        identifier=\"urn:li:userGeneratedContent\",\n                    )\n                ],\n            )\n        )\n        response = self._api.register_upload(register_upload_body)\n        # [NOTE] LinkedIn apis have '.' in the name making it impossible to use TypeDicts\n        upload_url = response[\"value\"][\"uploadMechanism\"][\n            \"com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest\"\n        ][\"uploadUrl\"]\n        asset_id = response[\"value\"][\"asset\"]\n        response = self._api.upload_image(image_path, upload_url)\n\n        return asset_id\n\n    def create_post(\n        self, text: str, images: Optional[List[Tuple[str, str]]] = None\n    ) -> bool:\n        \"\"\"To create a post with the LinkedIn Apis we need to\n         - (if we have an image): register the image upload, store the returned uploadUrl\n         - (if we have an image): upload the image\n         - send the post, (if we have an image) add the correct image to the `media` request body\n\n        Args:\n            text (str): _description_\n            image_path (Optional[str], optional): _description_. Defaults to None.\n\n        Returns:\n            bool: _description_\n        \"\"\"\n        shareMediaCategory = \"NONE\" if images is None else \"IMAGE\"\n        author = f\"urn:li:person:{self._me['id']}\"\n        media: List[Media] = []\n\n        if images is not None:\n            for image_path, image_description in images:\n                asset_id = self._register_and_upload_image(image_path)\n                media.append(\n                    Media(\n                        status=\"READY\",\n                        description=Description(text=image_description),\n                        media=asset_id,\n                        title=Title(text=\"Image Title\"),\n                    )\n                )\n        create_post_body = {\n            \"author\": author,\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\"text\": text},\n                    \"shareMediaCategory\": shareMediaCategory,\n                    \"media\": media,\n                }\n            },\n            \"visibility\": {\"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"},\n        }\n\n        return self._api.create_post(create_post_body).json()", ""]}
{"filename": "src/confirmations/input_confirmation.py", "chunked_list": ["from typing import Callable\n\nfrom src.actions import Action\nfrom src.types import GeneratedContent\n\n\ndef input_confirmation(repeat_func: Callable) -> bool:\n    while True:\n        user_input = input(\"Proceed [yes/no/repeat]\").lower().strip()\n        match user_input:", "        user_input = input(\"Proceed [yes/no/repeat]\").lower().strip()\n        match user_input:\n            case \"yes\":\n                return True\n            case \"no\":\n                return False\n            case \"repeat\":\n                return repeat_func()"]}
{"filename": "src/confirmations/__init__.py", "chunked_list": [""]}
