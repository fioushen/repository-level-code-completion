{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"Installation of the tritondse module.\"\"\"\n\nimport sys\nfrom setuptools import setup, find_packages\n\n\nwith open(\"README.md\") as f:\n    README = f.read()", "\nwith open(\"README.md\") as f:\n    README = f.read()\n\n\nsetup(\n    name=\"tritondse\",\n    version=\"0.1.8\",\n    description=\"A library of Dynamic Symbolic Exploration based the Triton library\",\n    packages=find_packages(),", "    description=\"A library of Dynamic Symbolic Exploration based the Triton library\",\n    packages=find_packages(),\n    long_description=README,\n    long_description_content_type='text/markdown',\n    url=\"https://github.com/quarkslab/tritondse\",\n    project_urls={\n        \"Documentation\": \"https://quarkslab.github.io/tritondse/\",\n        \"Bug Tracker\": \"https://github.com/quarkslab/tritondse/issues\",\n        \"Source\": \"https://github.com/quarkslab/tritondse\"\n    },", "        \"Source\": \"https://github.com/quarkslab/tritondse\"\n    },\n    setup_requires=[],\n    install_requires=[\n        \"triton-library\",\n        \"lief\",\n        \"pyQBDI\",\n        \"cle\",\n        \"quokka-project\",\n        \"enum_tools\"", "        \"quokka-project\",\n        \"enum_tools\"\n    ],\n    tests_require=[],\n    license=\"AGPL-3.0\",\n    author=\"Quarkslab\",\n    classifiers=[\n        'Topic :: Security',\n        'Environment :: Console',\n        'Operating System :: OS Independent',", "        'Environment :: Console',\n        'Operating System :: OS Independent',\n    ],\n    test_suite=\"\",\n    scripts=[]\n)\n"]}
{"filename": "doc/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the", "# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n# sys.path.insert(0, os.path.abspath('..'))\n\n# -- Project information -----------------------------------------------------\n", "# -- Project information -----------------------------------------------------\n\nproject = 'TritonDSE'\ncopyright = '2022, Quarkslab'\nauthor = 'Quarkslab'\n\n# The full version, including alpha/beta/rc tags\nrelease = '0.1'\n\n", "\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.todo',", "    'sphinx.ext.autodoc',\n    'sphinx.ext.todo',\n    'sphinx.ext.viewcode',\n    'breathe',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.githubpages',\n    \"nbsphinx\",\n    \"enum_tools.autoenum\"\n]\n", "]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n", "exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n", "html_theme = \"sphinx_rtd_theme\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['figs']\n\nautodoc_default_flags = ['members', 'inherited-members']\n\nautoclass_content = \"both\"  # Comment class with both class docstring and __init__ docstring", "\nautoclass_content = \"both\"  # Comment class with both class docstring and __init__ docstring\n\nautodoc_typehints = \"signature\"\n\nautodoc_type_aliases = {\n    'PathLike': 'tritondse.types.PathLike',\n    'Addr': 'tritondse.types.Addr',\n    'rAddr': 'tritondse.types.rAddr',\n    'BitSize': 'tritondse.types.BitSize',", "    'rAddr': 'tritondse.types.rAddr',\n    'BitSize': 'tritondse.types.BitSize',\n    'ByteSize': 'tritondse.types.ByteSize',\n    'Input': 'tritondse.types.Input',\n    'PathHash': 'tritondse.types.PathHash',\n    'AddrCallback': 'tritondse.callbacks.AddrCallback',\n    'InstrCallback': 'tritondse.callbacks.InstrCallback',\n    'MemReadCallback': 'tritondse.callbacks.MemReadCallback',\n    'MemWriteCallback': 'tritondse.callbacks.MemWriteCallback',\n    'NewInputCallback': 'tritondse.callbacks.NewInputCallback',", "    'MemWriteCallback': 'tritondse.callbacks.MemWriteCallback',\n    'NewInputCallback': 'tritondse.callbacks.NewInputCallback',\n    'RegReadCallback': 'tritondse.callbacks.RegReadCallback',\n    'RegWriteCallback': 'tritondse.callbacks.RegWriteCallback',\n    'RtnCallback': 'tritondse.callbacks.RtnCallback',\n    'SymExCallback': 'tritondse.callbacks.SymExCallback',\n    'ThreadCallback': 'tritondse.callbacks.ThreadCallback',\n}\n\n# For internationalization", "\n# For internationalization\nlocale_dirs = ['locale/']   # path is example but recommended.\ngettext_compact = False     # optional.\n\n\n# For intersphinx\nintersphinx_mapping = {'python': ('https://docs.python.org/3', None),\n                       'lief': ('https://lief.quarkslab.com/doc/latest/', None)}\n", "                       'lief': ('https://lief.quarkslab.com/doc/latest/', None)}\n"]}
{"filename": "doc/practicals/solution_json_parser.py", "chunked_list": ["#!/usr/bin/env python\nfrom tritondse import SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, Workspace, Seed, \\\n                      SeedFormat, Loader, MonolithicLoader, CompositeData, Addr, SolverStatus, Architecture, \\\n                      LoadableSegment, Perm, SeedStatus, SmtSolver\nfrom tritondse.sanitizers import NullDerefSanitizer\nimport struct\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n", "logging.basicConfig(level=logging.INFO)\n\n# Memory mapping\nSTACK_ADDR  = 0x1000000\nSTACK_SIZE  = 1024*6\nSTRUC_ADDR  = 0x3000000\nBUFFER_ADDR = 0x2000000\nBASE_ADDRESS= 0x8000000\nUSER_CB     = 0x40000000\n", "USER_CB     = 0x40000000\n\n# Addresses\nENTRY_POINT = 0x81dc46e\nEXIT_POINT  = 0x81dc472\nSTUB_ADDR1  = 0x81d1bf0\nSTUB_ADDR2  = 0x81d1252\n\n\ndef pre_inst(se: SymbolicExecutor, state: ProcessState, inst):\n    if se.trace_offset > 40000:\n        se.seed.status = SeedStatus.HANG\n        se.abort()", "\ndef pre_inst(se: SymbolicExecutor, state: ProcessState, inst):\n    if se.trace_offset > 40000:\n        se.seed.status = SeedStatus.HANG\n        se.abort()\n    #print(f\"[{se.trace_offset}] {inst.getAddress():#08x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"[{se.uid}] seed:{se.seed.hash} ({repr(se.seed.content.variables['buffer'][:100])}) => {se.seed.status.name}   [exitcode:{se.exitcode}]\")\n\ndef fptr_stub(exec: SymbolicExecutor, pstate: ProcessState, addr: int):\n    print(f\"fptr_stub addr : {addr:#x}\")\n    if addr == STUB_ADDR1:\n        pstate.cpu.r0 = 1\n    elif addr == STUB_ADDR2:\n        pstate.cpu.r0 = 0\n    pstate.cpu.program_counter += 2\n    exec.skip_instruction()", "\ndef fptr_stub(exec: SymbolicExecutor, pstate: ProcessState, addr: int):\n    print(f\"fptr_stub addr : {addr:#x}\")\n    if addr == STUB_ADDR1:\n        pstate.cpu.r0 = 1\n    elif addr == STUB_ADDR2:\n        pstate.cpu.r0 = 0\n    pstate.cpu.program_counter += 2\n    exec.skip_instruction()\n\ndef hook_start(exec: SymbolicExecutor, pstate: ProcessState):\n    buffer = pstate.get_argument_value(0)\n    length = pstate.get_argument_value(1)\n    JSON_ctx = pstate.get_argument_value(2)\n\n    exec.inject_symbolic_variable_memory(buffer, \"buffer\", exec.seed.content.variables[\"buffer\"])\n    exec.inject_symbolic_variable_memory(JSON_ctx, \"JSON_ctx\", exec.seed.content.variables[\"JSON_ctx\"])\n\n    # Take the length of the buffer (which is not meant to change)\n    pstate.cpu.r1 = len(exec.seed.content.variables['buffer'])", "\ndef hook_start(exec: SymbolicExecutor, pstate: ProcessState):\n    buffer = pstate.get_argument_value(0)\n    length = pstate.get_argument_value(1)\n    JSON_ctx = pstate.get_argument_value(2)\n\n    exec.inject_symbolic_variable_memory(buffer, \"buffer\", exec.seed.content.variables[\"buffer\"])\n    exec.inject_symbolic_variable_memory(JSON_ctx, \"JSON_ctx\", exec.seed.content.variables[\"JSON_ctx\"])\n\n    # Take the length of the buffer (which is not meant to change)\n    pstate.cpu.r1 = len(exec.seed.content.variables['buffer'])", "\nconf = Config(skip_unsupported_import=True, seed_format=SeedFormat.COMPOSITE, smt_solver=SmtSolver.Z3)\n\nraw_firmware = Path(\"./bugged_json_parser.bin\").read_bytes()\n\nldr = MonolithicLoader(Architecture.ARM32,\n                       cpustate = {\"pc\": ENTRY_POINT, \n                                   \"r0\": BUFFER_ADDR,\n                                   \"r2\": STRUC_ADDR,\n                                   \"sp\": STACK_ADDR+STACK_SIZE},", "                                   \"r2\": STRUC_ADDR,\n                                   \"sp\": STACK_ADDR+STACK_SIZE},\n                       set_thumb=True,\n                       maps = [LoadableSegment(BASE_ADDRESS, len(raw_firmware), Perm.R|Perm.X, content=raw_firmware, name=\"bugged_json_parser\"), \n                               LoadableSegment(BUFFER_ADDR, 40, Perm.R|Perm.W, name=\"input\"),\n                               LoadableSegment(STRUC_ADDR, 512, Perm.R|Perm.W, name=\"JSON_ctx\"),\n                               LoadableSegment(USER_CB, 1000, Perm.R|Perm.X, name=\"user_cb\"),\n                               LoadableSegment(STACK_ADDR, STACK_SIZE, Perm.R|Perm.W, name=\"[stack]\")\n                       ])\n", "                       ])\n\nworkspace = Workspace(\"ws\")\n\ndse = SymbolicExplorator(conf, ldr, executor_stop_at=EXIT_POINT, workspace=workspace)\n\nseed = Seed(CompositeData(variables={\n                                    \"buffer\": b\"A\"*40,\n                                    \"JSON_ctx\": b\"\\x00\"*128}))#struct.pack(\"<I\", USER_CB)*128}))\ndse.add_input_seed(seed)", "                                    \"JSON_ctx\": b\"\\x00\"*128}))#struct.pack(\"<I\", USER_CB)*128}))\ndse.add_input_seed(seed)\n\n# dse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_pre_execution_callback(hook_start)\n\ndse.callback_manager.register_pre_instruction_callback(pre_inst)\n\ndse.callback_manager.register_pre_addr_callback(STUB_ADDR1, fptr_stub)", "\ndse.callback_manager.register_pre_addr_callback(STUB_ADDR1, fptr_stub)\ndse.callback_manager.register_pre_addr_callback(STUB_ADDR2, fptr_stub)\n\ndse.explore()\n\n# seed = Seed(CompositeData(variables={\n#                                     \"buffer\" : b\"A\"*40,\n#                                     \"JSON_ctx\": b\"\\x00\"*128}))#struct.pack(\"<I\", USER_CB)*128}))\n", "#                                     \"JSON_ctx\": b\"\\x00\"*128}))#struct.pack(\"<I\", USER_CB)*128}))\n\n# dse = SymbolicExecutor(conf, seed, workspace=workspace)\n# dse.load(ldr)\n\n# # dse.callback_manager.register_probe(NullDerefSanitizer())\n# dse.cbm.register_post_execution_callback(post_exec_hook)\n# dse.cbm.register_pre_execution_callback(hook_start)\n\n# dse.cbm.register_pre_instruction_callback(pre_inst)", "\n# dse.cbm.register_pre_instruction_callback(pre_inst)\n\n# dse.cbm.register_pre_addr_callback(STUB_ADDR1, fptr_stub)\n# dse.cbm.register_pre_addr_callback(STUB_ADDR2, fptr_stub)\n\n# dse.run(stop_at=EXIT_POINT)\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve2.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed, SeedFormat, Loader, CompositeData\nfrom tritondse.types import Addr, SolverStatus, Architecture, ArchMode\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\nonce_flag = False\n\ndef trace_inst(exec: SymbolicExecutor, pstate: ProcessState, inst: Instruction):\n    print(f\"[tid:{inst.getThreadId()}] 0x{inst.getAddress():x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")", "\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef memory_read_callback(se: SymbolicExecutor, pstate: ProcessState, addr):\n    global once_flag\n    if once_flag: return\n    read_address = addr.getAddress()\n    inst_address = pstate.read_register(pstate.registers.rip)\n    if inst_address == 0x11c6:\n        rax_sym = pstate.read_symbolic_register(pstate.registers.rax)\n        rax = pstate.read_register(pstate.registers.rax)\n        rbp = pstate.read_register(pstate.registers.rbp)\n        target = rbp + rax * 4 - 0x20\n\n        if not pstate.is_register_symbolic(pstate.registers.rax):\n            print(\"rax not symbolic\")\n            return\n\n        lea = addr.getLeaAst()\n        if lea == None: return\n        print(f\"argv[1] = {se.seed.content} Target = {hex(target)}\")\n        exp = lea != target\n        status, model = pstate.solve(exp)\n        while status == SolverStatus.SAT:\n            new_seed = se.mk_new_seed_from_model(model)\n            se.enqueue_seed(new_seed)\n            target = pstate.evaluate_expression_model(lea, model)\n            var_values = pstate.get_expression_variable_values_model(rax_sym, model)\n            for var, value in var_values.items():\n                print(f\"{var}: {chr(value)} Target = {hex(target)}\")\n            exp = pstate.actx.land([exp, lea != target])\n            status, model = pstate.solve(exp)\n        once_flag = True", "\np = Program(\"./2\")\nconf = Config(\\\n    skip_unsupported_import=True, \\\n    seed_format=SeedFormat.COMPOSITE)\n\ndse = SymbolicExplorator(conf, p)\n\ncomposite_data = CompositeData(argv=[b\"./1\", b\"AZ\\nERAZER\"])\ndse.add_input_seed(composite_data)", "composite_data = CompositeData(argv=[b\"./1\", b\"AZ\\nERAZER\"])\ndse.add_input_seed(composite_data)\n\ndse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_memory_read_callback(memory_read_callback)\n#dse.callback_manager.register_pre_instruction_callback(trace_inst)\n\ndse.explore()\n", "dse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve4.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed, SeedFormat, CompositeData\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\nonce_flag = False\n\ndef trace_inst(exec: SymbolicExecutor, pstate: ProcessState, inst: Instruction):\n    print(f\"[tid:{inst.getThreadId()}] 0x{inst.getAddress():x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")", "\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef hook_strlen(se: SymbolicExecutor, pstate: ProcessState, routine: str, addr: int):\n    global once_flag\n    if once_flag: return\n\n    # Get arguments\n    s = pstate.get_argument_value(0)\n    ast = pstate.actx\n\n    def rec(res, s, deep, maxdeep):\n        if deep == maxdeep:\n            return res\n        cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n        res  = ast.ite(cell == 0x00, ast.bv(deep, 64), rec(res, s, deep + 1, maxdeep))\n        return res\n\n    sze = 20\n    res = ast.bv(sze, 64)\n    res = rec(res, s, 0, sze)\n\n    pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n    # Manual state coverage of strlen(s) \n    exp = res != sze\n    status, model = pstate.solve(exp)\n    while status == SolverStatus.SAT:\n        sze = pstate.evaluate_expression_model(res, model)\n        new_seed = se.mk_new_seed_from_model(model)\n        print(f\"new_seed : {new_seed.content}\")\n\n        se.enqueue_seed(new_seed)\n        var_values = pstate.get_expression_variable_values_model(res, model)\n        exp = pstate.actx.land([exp, res != sze])\n        status, model = pstate.solve(exp)\n\n    once_flag = True\n    return res", "\n\np = Program(\"./4\")\ndse = SymbolicExplorator(Config(skip_unsupported_import=True,\\\n        seed_format=SeedFormat.COMPOSITE), p)\n\ndse.add_input_seed(Seed(CompositeData(argv=[b\"./4\", b\"AAAAAA\"])))\n\ndse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)", "dse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_pre_imported_routine_callback(\"strlen\", hook_strlen)\n#dse.callback_manager.register_post_instruction_callback(trace_inst)\n\ndse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve1.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed, SeedFormat, CompositeData\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\n\nfrom tritondse.routines import rtn_atoi\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef hook_fread(exec: SymbolicExecutor, pstate: ProcessState, routine: str, addr: int):\n    # We hook fread to symbolize what is being read\n    arg = pstate.get_argument_value(0)\n    sizeof = pstate.get_argument_value(2)\n    exec.inject_symbolic_input(arg, exec.seed)\n    print(\"Symbolizing {} bytes at {}\".format(hex(sizeof), hex(arg)))\n    s = pstate.memory.read_string(arg)\n    print(f\"fread: {s}\")\n    return 0", "\ndef hook_fread(exec: SymbolicExecutor, pstate: ProcessState, routine: str, addr: int):\n    # We hook fread to symbolize what is being read\n    arg = pstate.get_argument_value(0)\n    sizeof = pstate.get_argument_value(2)\n    exec.inject_symbolic_input(arg, exec.seed)\n    print(\"Symbolizing {} bytes at {}\".format(hex(sizeof), hex(arg)))\n    s = pstate.memory.read_string(arg)\n    print(f\"fread: {s}\")\n    return 0", "\ndef hook_sscanf4(exec: SymbolicExecutor, pstate: ProcessState, routine: str, addr: int):\n    # sscanf(buffer, \"%d\", &j) is treated as j = atoi(buffer)\n    ast = pstate.actx\n    addr_j = pstate.get_argument_value(2)\n    arg = pstate.get_argument_value(0)\n    int_str = pstate.memory.read_string(arg)\n\n    cells = {i: pstate.read_symbolic_memory_byte(arg+i).getAst() for i in range(10)}\n\n    def multiply(ast, cells, index):\n        n = ast.bv(0, 32)\n        for i in range(index):\n            n = n * 10 + (ast.zx(24, cells[i]) - 0x30)\n        return n\n\n    res = ast.ite(\n              ast.lnot(ast.land([cells[0] >= 0x30, cells[0] <= 0x39])),\n              multiply(ast, cells, 0),\n              ast.ite(\n                  ast.lnot(ast.land([cells[1] >= 0x30, cells[1] <= 0x39])),\n                  multiply(ast, cells, 1),\n                  ast.ite(\n                      ast.lnot(ast.land([cells[2] >= 0x30, cells[2] <= 0x39])),\n                      multiply(ast, cells, 2),\n                      ast.ite(\n                          ast.lnot(ast.land([cells[3] >= 0x30, cells[3] <= 0x39])),\n                          multiply(ast, cells, 3),\n                          ast.ite(\n                              ast.lnot(ast.land([cells[4] >= 0x30, cells[4] <= 0x39])),\n                              multiply(ast, cells, 4),\n                              ast.ite(\n                                  ast.lnot(ast.land([cells[5] >= 0x30, cells[5] <= 0x39])),\n                                  multiply(ast, cells, 5),\n                                  ast.ite(\n                                      ast.lnot(ast.land([cells[6] >= 0x30, cells[6] <= 0x39])),\n                                      multiply(ast, cells, 6),\n                                      ast.ite(\n                                          ast.lnot(ast.land([cells[7] >= 0x30, cells[7] <= 0x39])),\n                                          multiply(ast, cells, 7),\n                                          ast.ite(\n                                              ast.lnot(ast.land([cells[8] >= 0x30, cells[8] <= 0x39])),\n                                              multiply(ast, cells, 8),\n                                              ast.ite(\n                                                  ast.lnot(ast.land([cells[9] >= 0x30, cells[9] <= 0x39])),\n                                                  multiply(ast, cells, 9),\n                                                  multiply(ast, cells, 9)\n                                              )\n                                          )\n                                      )\n                                  )\n                              )\n                          )\n                      )\n                  )\n              )\n          )\n    res = ast.sx(32, res)\n\n    pstate.write_symbolic_memory_int(addr_j, 8, res)\n\n    try:\n        i = int(int_str)\n        constraint = res == i\n        pstate.push_constraint(constraint)\n    except:\n        print(\"Failed to convert to int\")\n\n    return res", "\np = Program(\"./1\")\ndse = SymbolicExplorator(Config(\\\n        skip_unsupported_import=True,\\\n        seed_format=SeedFormat.COMPOSITE), p)\n\ndse.add_input_seed(Seed(CompositeData(files={\"stdin\": b\"AZERZAER\", \"tmp.covpro\": b\"AZERAEZR\"})))\n\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_probe(NullDerefSanitizer())", "dse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_probe(NullDerefSanitizer())\n#dse.callback_manager.register_post_imported_routine_callback(\"fread\", hook_fread)\ndse.callback_manager.register_pre_imported_routine_callback(\"__isoc99_sscanf\", hook_sscanf4)\n\ndse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve0.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed, SeedFormat, CompositeData\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\ndef trace_inst(exec: SymbolicExecutor, pstate: ProcessState, inst: Instruction):\n    print(f\"[tid:{inst.getThreadId()}] 0x{inst.getAddress():x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})\")", "def post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})\")\n\np = Program(\"./7\")\ndse = SymbolicExplorator(Config(\\\n        skip_unsupported_import=True,\\\n        seed_format=SeedFormat.COMPOSITE), p)\n\ndse.add_input_seed(Seed(CompositeData(argv=[b\"./7\", b\"XXXX\"], files={\"stdin\": b\"ZZZZ\"})))\ndse.callback_manager.register_probe(NullDerefSanitizer())", "dse.add_input_seed(Seed(CompositeData(argv=[b\"./7\", b\"XXXX\"], files={\"stdin\": b\"ZZZZ\"})))\ndse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\n#dse.callback_manager.register_post_instruction_callback(trace_inst)\n\ndse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve3.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed, SeedFormat, CompositeData\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\nonce_flag_write = False\nonce_flag_read = False\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")", "def post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef memory_read_callback(se: SymbolicExecutor, pstate: ProcessState, addr):\n    global once_flag_read\n    if once_flag_read: return\n    read_address = addr.getAddress()\n    inst_address = pstate.read_register(pstate.registers.rip)\n    lea = addr.getLeaAst()\n    if lea == None: return\n    #print(f\"inst: {hex(inst_address)} read: {hex(read_address)}\")\n    if inst_address == 0x1234:\n        print(lea)\n        rax_sym = pstate.read_symbolic_register(pstate.registers.rax)\n        rax = pstate.read_register(pstate.registers.rax)\n        rbp = pstate.read_register(pstate.registers.rbp)\n        target = rbp + rax * 4 - 0x80\n\n        if not pstate.is_register_symbolic(pstate.registers.rax):\n            print(\"rax not symbolic\")\n            return\n\n        print(f\"argv[1] = {se.seed.content} Target = {hex(target)}\")\n        exp = lea != target\n        status, model = pstate.solve(exp)\n        while status == SolverStatus.SAT:\n            new_seed = se.mk_new_seed_from_model(model)\n            se.enqueue_seed(new_seed)\n            target = pstate.evaluate_expression_model(lea, model)\n            var_values = pstate.get_expression_variable_values_model(rax_sym, model)\n            for var, value in var_values.items():\n                print(f\"{var}: {chr(value)} Target = {hex(target)}\")\n            exp = pstate.actx.land([exp, lea != target])\n            status, model = pstate.solve(exp)\n        once_flag_read = True", "\n#   0010120f 89 54 85 80     MOV        dword ptr [RBP + RAX*0x4 + -0x80],EDX\ndef memory_write_callback(se: SymbolicExecutor, pstate: ProcessState, addr, value):\n    global once_flag_write\n    if once_flag_write: return\n    read_address = addr.getAddress()\n    inst_address = pstate.read_register(pstate.registers.rip)\n    lea = addr.getLeaAst()\n    if lea == None: return\n    #print(f\"inst: {hex(inst_address)} write {hex(value)} to {hex(read_address)}\")\n    if inst_address == 0x120f:\n        rax_sym = pstate.read_symbolic_register(pstate.registers.rax)\n        rax = pstate.read_register(pstate.registers.rax)\n        rbp = pstate.read_register(pstate.registers.rbp)\n        target = rbp + rax * 4 - 0x80\n\n        if not pstate.is_register_symbolic(pstate.registers.rax):\n            print(\"rax not symbolic\")\n            return\n\n        print(f\"argv[1] = {se.seed.content} Target = {hex(target)}\")\n        exp = lea != target\n        status, model = pstate.solve(exp)\n        while status == SolverStatus.SAT:\n            new_seed = se.mk_new_seed_from_model(model)\n            se.enqueue_seed(new_seed)\n            target = pstate.evaluate_expression_model(lea, model)\n            var_values = pstate.get_expression_variable_values_model(rax_sym, model)\n            for var, value in var_values.items():\n                print(f\"{var}: {chr(value)} Target = {hex(target)}\")\n            exp = pstate.actx.land([exp, lea != target])\n            status, model = pstate.solve(exp)\n        once_flag_write = True", "\np = Program(\"./3\")\n\ndse = SymbolicExplorator(Config(\\\n        skip_unsupported_import=True,\\\n        seed_format=SeedFormat.COMPOSITE), p)\n\ndse.add_input_seed(Seed(CompositeData(files={\"stdin\": b\"AZERAZER\"})))\n\ndse.callback_manager.register_probe(NullDerefSanitizer())", "\ndse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_memory_read_callback(memory_read_callback)\ndse.callback_manager.register_memory_write_callback(memory_write_callback)\n\ndse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve5.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\nimport logging\n\nbuffers_len_g = dict() # buffer_address : buffer_len\n\nclass StrncpySanitizer(ProbeInterface):\n    # TODO handle strncpy into buff + offset\n    # TODO handle symbolic n \n\n    def __init__(self):\n        super(StrncpySanitizer, self).__init__()\n        self._add_callback(CbType.PRE_RTN, self.strncpy_check, 'strncpy')\n\n    def strncpy_check(self, se: SymbolicExecutor, pstate: ProcessState, rtn_name: str, addr: Addr):\n        buffer_addr = se.pstate.get_argument_value(0)\n        if buffer_addr not in buffers_len_g:\n            return \n\n        buffer_len = buffers_len_g[buffer_addr]\n        n = se.pstate.get_argument_value(2)\n        n_sym = se.pstate.get_argument_symbolic(2)\n\n        if n > buffer_len:\n            logging.critical(f\"Found overflowing strncpy buf: {hex(buffer_addr)} bufsize: {buffer_len} copysize: {n}\")\n\n            # Generate input to trigger the overflow\n            s = pstate.get_argument_value(1)\n            ast = pstate.actx\n\n            def rec(res, s, deep, maxdeep):\n                if deep == maxdeep:\n                    return res\n                cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n                res  = ast.ite(cell == 0x00, ast.bv(deep, 64), rec(res, s, deep + 1, maxdeep))\n                return res\n\n            sze = len(pstate.memory.read_string(s))\n            res = ast.bv(sze, 64)\n            res = rec(res, s, 0, sze)\n\n            pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n            # Manual state coverage of strlen(s) \n            exp = res > n\n            print(\"a\")\n            status, model = pstate.solve(exp)\n            while status == SolverStatus.SAT:\n                print(\"b\")\n                sze = pstate.evaluate_expression_model(res, model)\n                new_seed = se.mk_new_seed_from_model(model)\n                #print(f\"new_seed: {new_seed.content} len = {hex(sze)}\")\n\n                se.enqueue_seed(new_seed)\n                var_values = pstate.get_expression_variable_values_model(res, model)\n                exp = pstate.actx.land([exp, res != sze])\n                status, model = pstate.solve(exp)\n            return\n\n        if n_sym.isSymbolized():\n            const = n_sym > buffer_len\n            st, model = pstate.solve(const)\n            if st == SolverStatus.SAT:\n                new_seed = se.mk_new_seed_from_model(model)\n                new_seed.status = SeedStatus.CRASH\n                se.enqueue_seed(new_seed) ", "\nclass StrncpySanitizer(ProbeInterface):\n    # TODO handle strncpy into buff + offset\n    # TODO handle symbolic n \n\n    def __init__(self):\n        super(StrncpySanitizer, self).__init__()\n        self._add_callback(CbType.PRE_RTN, self.strncpy_check, 'strncpy')\n\n    def strncpy_check(self, se: SymbolicExecutor, pstate: ProcessState, rtn_name: str, addr: Addr):\n        buffer_addr = se.pstate.get_argument_value(0)\n        if buffer_addr not in buffers_len_g:\n            return \n\n        buffer_len = buffers_len_g[buffer_addr]\n        n = se.pstate.get_argument_value(2)\n        n_sym = se.pstate.get_argument_symbolic(2)\n\n        if n > buffer_len:\n            logging.critical(f\"Found overflowing strncpy buf: {hex(buffer_addr)} bufsize: {buffer_len} copysize: {n}\")\n\n            # Generate input to trigger the overflow\n            s = pstate.get_argument_value(1)\n            ast = pstate.actx\n\n            def rec(res, s, deep, maxdeep):\n                if deep == maxdeep:\n                    return res\n                cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n                res  = ast.ite(cell == 0x00, ast.bv(deep, 64), rec(res, s, deep + 1, maxdeep))\n                return res\n\n            sze = len(pstate.memory.read_string(s))\n            res = ast.bv(sze, 64)\n            res = rec(res, s, 0, sze)\n\n            pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n            # Manual state coverage of strlen(s) \n            exp = res > n\n            print(\"a\")\n            status, model = pstate.solve(exp)\n            while status == SolverStatus.SAT:\n                print(\"b\")\n                sze = pstate.evaluate_expression_model(res, model)\n                new_seed = se.mk_new_seed_from_model(model)\n                #print(f\"new_seed: {new_seed.content} len = {hex(sze)}\")\n\n                se.enqueue_seed(new_seed)\n                var_values = pstate.get_expression_variable_values_model(res, model)\n                exp = pstate.actx.land([exp, res != sze])\n                status, model = pstate.solve(exp)\n            return\n\n        if n_sym.isSymbolized():\n            const = n_sym > buffer_len\n            st, model = pstate.solve(const)\n            if st == SolverStatus.SAT:\n                new_seed = se.mk_new_seed_from_model(model)\n                new_seed.status = SeedStatus.CRASH\n                se.enqueue_seed(new_seed) ", "\ndef trace_inst(exec: SymbolicExecutor, pstate: ProcessState, inst: Instruction):\n    print(f\"[tid:{inst.getThreadId()}] 0x{inst.getAddress():x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef hook_alert_placeholder(exec: SymbolicExecutor, pstate: ProcessState, addr: int):\n    buffer_len = pstate.get_argument_value(2)\n    buffer_addr = pstate.get_argument_value(3)\n    buffers_len_g[buffer_addr] = buffer_len", "\np = Program(\"./5\")\nalert_placeholder_addr = p.find_function_addr(\"__alert_placeholder\")\ndse = SymbolicExplorator(Config(skip_unsupported_import=True), p)\n\n#dse.add_input_seed(Seed(b\"AZERAZAZERA\"))\ndse.add_input_seed(Seed(b\"AZER\"))\n\ndse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)", "dse.callback_manager.register_probe(NullDerefSanitizer())\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\ndse.callback_manager.register_pre_addr_callback(alert_placeholder_addr, hook_alert_placeholder)\ndse.callback_manager.register_probe(StrncpySanitizer())\n\ndse.explore()\n"]}
{"filename": "doc/practicals/solutions_toy_examples/solve6.py", "chunked_list": ["from tritondse import ProbeInterface, SymbolicExecutor, Config, Program, SymbolicExplorator, ProcessState, CbType, SeedStatus, Seed\nfrom tritondse.types import Addr, SolverStatus, Architecture\nfrom tritondse.sanitizers import NullDerefSanitizer\nfrom triton import Instruction\n\nimport logging\n\nbuffers_len_g = dict() # buffer_address : buffer_len\n\nclass StrncpySanitizer(ProbeInterface):\n    # TODO handle strncpy into buff + offset\n\n    def __init__(self):\n        super(StrncpySanitizer, self).__init__()\n        self._add_callback(CbType.PRE_RTN, self.strncpy_check, 'strncpy')\n\n    def strncpy_check(self, se: SymbolicExecutor, pstate: ProcessState, rtn_name: str, addr: Addr):\n        buffer_addr = se.pstate.get_argument_value(0)\n        if buffer_addr not in buffers_len_g:\n            return \n\n        buffer_len = buffers_len_g[buffer_addr]\n        n = se.pstate.get_argument_value(2)\n        n_sym = se.pstate.get_argument_symbolic(2)\n\n        if n > buffer_len:\n            logging.critical(f\"Found overflowing strncpy buf: {hex(buffer_addr)} bufsize: {buffer_len} copysize: {n}\")\n\n            # Generate input to trigger the overflow\n            s = pstate.get_argument_value(1)\n            ast = pstate.actx\n\n            def rec(res, s, deep, maxdeep):\n                if deep == maxdeep:\n                    return res\n                cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n                res  = ast.ite(cell == 0x00, ast.bv(deep, 64), rec(res, s, deep + 1, maxdeep))\n                return res\n\n            sze = len(pstate.memory.read_string(s))\n            res = ast.bv(sze, 64)\n            res = rec(res, s, 0, sze)\n\n            pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n            # Manual state coverage of strlen(s) \n            exp = res > n\n            status, model = pstate.solve(exp)\n            while status == SolverStatus.SAT:\n                sze = pstate.evaluate_expression_model(res, model)\n                new_seed = se.mk_new_seed_from_model(model)\n                #print(f\"new_seed: {new_seed.content} len = {hex(sze)}\")\n\n                se.enqueue_seed(new_seed)\n                var_values = pstate.get_expression_variable_values_model(res, model)\n                exp = pstate.actx.land([exp, res != sze])\n                status, model = pstate.solve(exp)\n            return\n\n        # If n is symbolic, we try to make if bigger than buffer_len\n        if n_sym.isSymbolized():\n            const = n_sym.getAst() > buffer_len\n            st, model = pstate.solve(const)\n            if st == SolverStatus.SAT:\n                new_seed = se.mk_new_seed_from_model(model)\n                #new_seed.status = SeedStatus.CRASH\n                se.enqueue_seed(new_seed) ", "\nclass StrncpySanitizer(ProbeInterface):\n    # TODO handle strncpy into buff + offset\n\n    def __init__(self):\n        super(StrncpySanitizer, self).__init__()\n        self._add_callback(CbType.PRE_RTN, self.strncpy_check, 'strncpy')\n\n    def strncpy_check(self, se: SymbolicExecutor, pstate: ProcessState, rtn_name: str, addr: Addr):\n        buffer_addr = se.pstate.get_argument_value(0)\n        if buffer_addr not in buffers_len_g:\n            return \n\n        buffer_len = buffers_len_g[buffer_addr]\n        n = se.pstate.get_argument_value(2)\n        n_sym = se.pstate.get_argument_symbolic(2)\n\n        if n > buffer_len:\n            logging.critical(f\"Found overflowing strncpy buf: {hex(buffer_addr)} bufsize: {buffer_len} copysize: {n}\")\n\n            # Generate input to trigger the overflow\n            s = pstate.get_argument_value(1)\n            ast = pstate.actx\n\n            def rec(res, s, deep, maxdeep):\n                if deep == maxdeep:\n                    return res\n                cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n                res  = ast.ite(cell == 0x00, ast.bv(deep, 64), rec(res, s, deep + 1, maxdeep))\n                return res\n\n            sze = len(pstate.memory.read_string(s))\n            res = ast.bv(sze, 64)\n            res = rec(res, s, 0, sze)\n\n            pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n            # Manual state coverage of strlen(s) \n            exp = res > n\n            status, model = pstate.solve(exp)\n            while status == SolverStatus.SAT:\n                sze = pstate.evaluate_expression_model(res, model)\n                new_seed = se.mk_new_seed_from_model(model)\n                #print(f\"new_seed: {new_seed.content} len = {hex(sze)}\")\n\n                se.enqueue_seed(new_seed)\n                var_values = pstate.get_expression_variable_values_model(res, model)\n                exp = pstate.actx.land([exp, res != sze])\n                status, model = pstate.solve(exp)\n            return\n\n        # If n is symbolic, we try to make if bigger than buffer_len\n        if n_sym.isSymbolized():\n            const = n_sym.getAst() > buffer_len\n            st, model = pstate.solve(const)\n            if st == SolverStatus.SAT:\n                new_seed = se.mk_new_seed_from_model(model)\n                #new_seed.status = SeedStatus.CRASH\n                se.enqueue_seed(new_seed) ", "\ndef trace_inst(exec: SymbolicExecutor, pstate: ProcessState, inst: Instruction):\n    print(f\"[tid:{inst.getThreadId()}] 0x{inst.getAddress():x}: {inst.getDisassembly()}\")\n\ndef post_exec_hook(se: SymbolicExecutor, state: ProcessState):\n    print(f\"seed:{se.seed.hash} ({repr(se.seed.content)})   [exitcode:{se.exitcode}]\")\n\ndef hook_alert_placeholder(exec: SymbolicExecutor, pstate: ProcessState, addr: int):\n    buffer_len = pstate.get_argument_value(2)\n    buffer_addr = pstate.get_argument_value(3)\n    buffers_len_g[buffer_addr] = buffer_len", "\np = Program(\"./6\")\nalert_placeholder_addr = p.find_function_addr(\"__alert_placeholder\")\ndse = SymbolicExplorator(Config(symbolize_argv=True, skip_unsupported_import=True), p)\n\ndse.add_input_seed(Seed(b\"./6\\x00AZERAZER\\x00AZERAZER\"))\n\ndse.callback_manager.register_post_execution_callback(post_exec_hook)\n#dse.callback_manager.register_post_instruction_callback(trace_inst)\ndse.callback_manager.register_pre_addr_callback(alert_placeholder_addr, hook_alert_placeholder)", "#dse.callback_manager.register_post_instruction_callback(trace_inst)\ndse.callback_manager.register_pre_addr_callback(alert_placeholder_addr, hook_alert_placeholder)\ndse.callback_manager.register_probe(StrncpySanitizer())\ndse.callback_manager.register_probe(NullDerefSanitizer())\n\ndse.explore()\n"]}
{"filename": "tritondse/worklist.py", "chunked_list": ["# built-in imports\nimport json\nfrom typing import Optional\n\n# Local imports\nfrom tritondse.seed import Seed\nfrom tritondse.coverage import GlobalCoverage\nfrom tritondse.workspace import Workspace\nimport tritondse.logging\n", "import tritondse.logging\n\nlogger = tritondse.logging.get(\"seedmanager\")\n\n\nclass SeedScheduler:\n    \"\"\"\n    Abstract class for all seed selection strategies.\n    This class provides the base methods that all\n    subclasses should implement to be compliant with\n    the interface.\n    \"\"\"\n\n    def has_seed_remaining(self) -> bool:\n        \"\"\"\n        Returns true if there are still seeds to be processed in the scheduler\n\n        :returns: true if there are seeds to process\n        \"\"\"\n        raise NotImplementedError()\n\n    def add(self, seed: Seed) -> None:\n        \"\"\"\n        Add a new seed in the scheduler\n\n        :param seed: Seed to add in the scheduler\n        :type seed: Seed\n        \"\"\"\n        raise NotImplementedError()\n\n    def update_worklist(self, coverage: GlobalCoverage) -> None:\n        \"\"\"\n        Call after every execution.\n        That function might help the scheduler with some of its internal states.\n        For instance the scheduler is keep somes seed meant to cover an address\n        which is now covered, it can just drop these seeds.\n\n        :param coverage: global coverage of the exploration\n        :type coverage: GlobalCoverage\n        \"\"\"\n        raise NotImplementedError()\n\n    def can_solve_models(self) -> bool:\n        \"\"\"\n        Function called by the seed manager to know if it can\n        start negating branches to discover new paths. Some seed\n        scheduler might want to run concretely all inputs first\n        before starting negating branches.\n\n        :return: true if the :py:obj:`SeedManager` can negate branches\n        \"\"\"\n        raise NotImplementedError()\n\n    def pick(self) -> Optional[Seed]:\n        \"\"\"\n        Return the next seed to execute.\n\n        :returns: seed to execute\n        :rtype: Seed\n        \"\"\"\n        raise NotImplementedError()\n\n    def post_execution(self) -> None:\n        \"\"\"\n        Called at the end of each execution after the generation of new seeds through SMT.\n        Last thing called before starting the next iteration.\n        \"\"\"\n        pass\n\n    def post_exploration(self, workspace: Workspace) -> None:\n        \"\"\"\n        Called at the end of the exploration to perform\n        some clean-up or anything else.\n        \"\"\"\n        pass", "\n\nclass WorklistAddressToSet(SeedScheduler):\n    \"\"\"\n    This worklist classifies seeds by addresses. We map a seed X to an\n    address Y, if the seed X has been generated to reach the address Y.\n    When the method pick() is called, seeds covering a new address 'Y'\n    are selected first. Otherwise anyone is taken.\n    \"\"\"\n    def __init__(self, manager: 'SeedManager'):\n        self.manager = manager\n        self.cov = None\n        self.worklist = dict() # {CovItem: set(Seed)}\n\n    def __len__(self) -> int:\n        \"\"\" Number of pending seeds to execute \"\"\"\n        count = 0\n        for k in list(self.worklist):  # Copy in list to avoid race-condition\n            v = self.worklist[k]\n            count += len(v)\n        return count\n\n    def has_seed_remaining(self) -> bool:\n        \"\"\" Returns true if there are still seeds in the worklist \"\"\"\n        return len(self) != 0\n\n    def add(self, seed: Seed) -> None:\n        \"\"\" Add a seed to the worklist \"\"\"\n        for obj in seed.coverage_objectives:\n            if obj in self.worklist:\n                self.worklist[obj].add(seed)\n            else:\n                self.worklist[obj] = {seed}\n\n    def update_worklist(self, coverage: GlobalCoverage) -> None:\n        \"\"\" Update the coverage state of the woklist with the global one \"\"\"\n        self.cov = coverage\n\n    def can_solve_models(self) -> bool:\n        \"\"\"\n        Always true.\n        This strategy always allows solving branches. As a consequence\n        it might try to solve a branch already covered in a seed not run yet.\n        But this enables iterating a seed only once.\n\n        :returns: True\n        \"\"\"\n        return True\n\n    def pick(self) -> Optional[Seed]:\n        \"\"\" Return the next seed to execute\n\n        :returns: next seed to execute (first one covering new addresses, otherwise any other)\n        :rtype: Seed\n        \"\"\"\n        seed_picked = None\n        item_picked = None\n        to_remove = set()\n\n        for k, v in self.worklist.items():\n            # If the set is empty remove the entry\n            if not len(v):\n                to_remove.add(k)\n                continue\n\n            # If the address has never been executed, return the seed\n            if not self.cov.is_covered(k):\n                seed_picked = v.pop()\n                item_picked = k\n                if not len(v):\n                    to_remove.add(k)\n                break\n\n        # If all adresses has been executed, just pick a random seed\n        if not seed_picked:\n            for k, v in self.worklist.items():\n                if v:\n                    seed_picked = v.pop()\n                    item_picked = k\n                    if not len(v):\n                        to_remove.add(k)\n                    break\n\n        # If we did not have a seed again the worklist is definitely empty\n        if not seed_picked:\n            return None\n\n        # Pop the seed from all worklist[X] where it is\n        for obj in seed_picked.coverage_objectives:\n            if obj != item_picked:   # already poped it from item_picked thus only pop the other\n                if obj in self.worklist and seed_picked in self.worklist[obj]: \n                    self.worklist[obj].remove(seed_picked)\n                    if not self.worklist[obj]:\n                        to_remove.add(obj)\n\n        # Garbage the worklist\n        for i in to_remove:\n            self.worklist.pop(i)\n\n        return seed_picked", "\n\nclass WorklistRand(SeedScheduler):\n    \"\"\"\n    Trivial strategy that returns any Seed without any classification.\n    It uses a Set for insertion and pop (which is random) for picking seeds.\n    \"\"\"\n    def __init__(self, manager: 'SeedManager'):\n        self.worklist = set() # set(Seed)\n\n    def __len__(self) -> int:\n        \"\"\" Number of pending seeds to execute \"\"\"\n        return len(self.worklist)\n\n    def has_seed_remaining(self) -> bool:\n        \"\"\" Returns true if there are still seeds in the worklist \"\"\"\n        return len(self) != 0\n\n    def add(self, seed: Seed) -> None:\n        \"\"\" Add a seed to the worklist\n\n        :param seed: Seed to add to this rand scheduler\n        :type seed: Seed\n        \"\"\"\n        self.worklist.add(seed)\n\n    def update_worklist(self, coverage: GlobalCoverage) -> None:\n        \"\"\" Update the coverage state of the worklist with the global one \"\"\"\n        self.cov = coverage\n\n    def can_solve_models(self) -> bool:\n        \"\"\" Always true \"\"\"\n        return True\n\n    def pick(self) -> Optional[Seed]:\n        \"\"\"\n        Return the next seed to execute. The method pop() removes a random element\n        from the set and returns the removed element. Unlike, a stack a\n        random element is popped off the set.\n\n        :returns: next seed to executre\n        :rtype: Seed\n        \"\"\"\n        return self.worklist.pop() if self.worklist else None", "\n\nclass FreshSeedPrioritizerWorklist(SeedScheduler):\n    \"\"\"\n    Strategy that first execute all seeds without negating branches\n    in order to get the most updated coverage and which then re-run\n    all relevant seeds to negate their branches.\n\n    This worklist works as follow:\n        - return first fresh seeds first to get them executed (to improve coverage)\n        - keep the seed in the worklist up until it gets dropped or thoroughtly processed\n        - if no fresh seed is available, iterates seed that will generate coverage\n    \"\"\"\n    def __init__(self, manager: 'SeedManager'):\n        self.manager = manager\n        self.fresh = []       # Seed never processed (list to make sure we can pop first one received)\n        self.worklist = dict() # CovItem -> set(Seed)\n\n    def __len__(self) -> int:\n        \"\"\" Number of pending seeds to execute \"\"\"\n        s = set()\n        for seeds in list(self.worklist.values()):\n            s.update(seeds)\n        return len(self.fresh) + len(s)\n\n    def has_seed_remaining(self) -> bool:\n        \"\"\" Returns true if there are still seeds in the worklist \"\"\"\n        return len(self) != 0\n\n    def add(self, seed: Seed) -> None:\n        \"\"\" Add a seed to the worklist\n\n        :param seed: seed to add to the scheduler\n        :type seed: Seed\n        \"\"\"\n        if seed.coverage_objectives:  # If the seed already have coverage objectives\n            for item in seed.coverage_objectives:  # Add it in our worklist\n                if item in self.worklist:\n                    self.worklist[item].add(seed)\n                else:\n                    self.worklist[item] = {seed}\n            # seed.coverage_objectives.clear()  # Flush the objectives\n        else:  # Otherwise it is fresh\n            self.fresh.append(seed)\n\n    def update_worklist(self, coverage: GlobalCoverage) -> None:\n        \"\"\" Update the coverage state of the worklist with the global one \"\"\"\n        # Iterate the worklist to see if some items have now been covered\n        # and are thus not interesting anymore\n        to_remove = [x for x in self.worklist if coverage.is_covered(x)]\n\n        for item in to_remove:\n            for seed in self.worklist.pop(item):\n                seed.coverage_objectives.remove(item)\n                if not seed.coverage_objectives:  # The seed cannot improve the coverage of anything\n                    self.manager.drop_seed(seed)\n\n    def can_solve_models(self) -> bool:\n        \"\"\"\n        Returns True if there are no \"fresh\" seeds to execute.\n\n        :returns: True if all fresh seeds have been executed.\n        \"\"\"\n        return not self.fresh\n\n    def pick(self) -> Optional[Seed]:\n        \"\"\" Return the next seed to execute \"\"\"\n        # Pop first fresh seed\n        if self.fresh:\n            return self.fresh.pop(0)  # Return first item as it is the older\n\n        # Then pop seed meant to crash\n        if ... in self.worklist:  # If we have specific seeds (mostly generated by sanitizers)\n            it = self.worklist[...].pop()\n            if not self.worklist[...]:  # Remove the key if now empty\n                self.worklist.pop(...)\n            return it\n\n        if not self.worklist:\n            return None\n\n        # Then pop traditional coverage seeds\n        k = list(self.worklist.keys())[0]      # arbitrary covitem\n        seed = self.worklist[k].pop()          # remove first seed inside\n        for it in seed.coverage_objectives:    # Remove the seed from all worklist[x]\n            if it != k:                        # we already popped the item from k\n                self.worklist[it].remove(seed) # remove the seed from that covitem set\n            if not self.worklist[it]:          # remove the whole covitem if empty\n                self.worklist.pop(it)\n        return seed\n\n    def post_execution(self) -> None:\n        \"\"\"\n        Solely used to show intermediate statistics\n        \"\"\"\n        logger.info(f\"Seed Scheduler: worklist:{len(self)} Coverage objectives:{len(self.worklist)}  (fresh:{len(self.fresh)})\")\n\n    def post_exploration(self, workspace: Workspace) -> None:\n        \"\"\"\n        At the end of the execution, print the worklist to know\n        its state before exit.\n        \"\"\"\n        workspace.save_metadata_file(\"coverage_objectives.json\", json.dumps(list(self.worklist.keys())))", ""]}
{"filename": "tritondse/exception.py", "chunked_list": ["class SkipInstructionException(Exception):\n    \"\"\"\n    Exception to raise in a PRE callback to skip the evaluation\n    of the current instruction. It will thus force a SymbolicExecutor\n    to fetch the next instruction. Thus the user have to update the\n    RIP of the ProcessState currently being executed.\n    \"\"\"\n    pass\n\n\nclass AbortExecutionException(Exception):\n    \"\"\"\n    Exception to rais in a callback to stop current SymbolicExecutor.\n    The user should be careful to set the status of the current seed\n    being executed.\n    \"\"\"\n    pass", "\n\nclass AbortExecutionException(Exception):\n    \"\"\"\n    Exception to rais in a callback to stop current SymbolicExecutor.\n    The user should be careful to set the status of the current seed\n    being executed.\n    \"\"\"\n    pass\n", "\n\nclass StopExplorationException(Exception):\n    \"\"\"\n    Exception to raise in a callback to stop the whole exploration of\n    the program. It is caught by SymbolicExplorator.\n    \"\"\"\n    pass\n\n\nclass AllocatorException(Exception):\n    \"\"\"\n    Class used to represent an heap allocator exception.\n    This exception can be raised in the following conditions:\n\n    * trying to allocate data which overflow heap size\n    * trying to free a pointer already freed\n    * trying to free a non-allocated pointer\n    \"\"\"\n    def __init__(self, message):\n        super(Exception, self).__init__(message)", "\n\nclass AllocatorException(Exception):\n    \"\"\"\n    Class used to represent an heap allocator exception.\n    This exception can be raised in the following conditions:\n\n    * trying to allocate data which overflow heap size\n    * trying to free a pointer already freed\n    * trying to free a non-allocated pointer\n    \"\"\"\n    def __init__(self, message):\n        super(Exception, self).__init__(message)", ""]}
{"filename": "tritondse/symbolic_explorator.py", "chunked_list": ["import time\nimport threading\nimport gc\nfrom enum import Enum\nfrom typing import Union, Type\nfrom pathlib import Path\nimport stat\nimport enum_tools.documentation\n\nfrom tritondse.config            import Config", "\nfrom tritondse.config            import Config\nfrom tritondse.process_state     import ProcessState\nfrom tritondse.loaders           import Loader\nfrom tritondse.seed              import Seed\nfrom tritondse.symbolic_executor import SymbolicExecutor\nfrom tritondse.workspace         import Workspace\nfrom tritondse.coverage          import GlobalCoverage\nfrom tritondse.types             import Addr\nfrom tritondse.exception         import StopExplorationException", "from tritondse.types             import Addr\nfrom tritondse.exception         import StopExplorationException\nfrom tritondse.seeds_manager import SeedManager\nfrom tritondse.worklist import SeedScheduler\nfrom tritondse.callbacks import CallbackManager\nimport tritondse.logging\n\n\nlogger = tritondse.logging.get(\"explorator\")\n", "logger = tritondse.logging.get(\"explorator\")\n\n\n@enum_tools.documentation.document_enum\nclass ExplorationStatus(Enum):\n    \"\"\" Enum representing the current state of the exploration \"\"\"\n    NOT_RUNNING = 0  # doc: The explorator has not been started yet\n    RUNNING     = 1  # doc: The explorator is running and performing exploration\n    IDLE        = 2  # doc: The explorator is idle, it has run, but is out of inputs to process\n    STOPPED     = 3  # doc: The explorator has been stopped by the user\n    TERMINATED  = 4  # doc: The explorator has stopped normally, reached its goal (user-defined)", "\n\nclass SymbolicExplorator(object):\n    \"\"\"\n    Symbolic Exploration. This class is in charge of iterating\n    executions with the different seeds available in the workspace\n    and generated along the way.\n    \"\"\"\n    def __init__(self, config: Config, loader: Loader = None, workspace: Workspace = None, executor_stop_at: Addr = None, seed_scheduler_class: Type[SeedScheduler] = None):\n        self.loader: Loader = loader  #: Program being analyzed\n        self.config: Config = config  #: Configuration file\n        self.cbm: CallbackManager = CallbackManager()  #: CallbackManager to register callbacks\n        self._stop = False\n        self.ts: int = time.time()  #: Timestamp (object instanciation)\n        self.uid_counter: int = 0\n        self.status: ExplorationStatus = ExplorationStatus.NOT_RUNNING  #: status of the execution\n        self._executor_stop_at = executor_stop_at\n\n        # Initialize the workspace\n        if workspace:\n            self.workspace: Workspace = workspace  #: exploration workspace\n        else:\n            self.workspace: Workspace = Workspace(self.config.workspace)  #: workspace object\n            self.workspace.initialize(flush=False)\n\n        # Save the configuration in the workspace\n        self.workspace.save_file(\"config.json\", self.config.to_json())\n\n        # Save the binary in the workspace if not already done\n        if self.loader:\n            bin_path = self.workspace.get_binary_directory() / self.loader.bin_path.name\n            if not bin_path.exists():  # If the program is not yet present\n                self.workspace.save_file(bin_path, self.loader.bin_path.read_bytes())\n                self.loader.bin_path = bin_path  # Patch its official new location\n                bin_path.chmod(stat.S_IRWXU)  # Make it executable\n\n        # Configure logfile in workspace\n        tritondse.logging.enable_to_file(logger.level, self.workspace.logfile_path)\n\n        # Initialize coverage\n        self.coverage: GlobalCoverage = GlobalCoverage(self.config.coverage_strategy, self.config.branch_solving_strategy)\n        \"\"\" GlobalCoverage object holding information about the global coverage.\n        *(not really meant to be manipulated by the user)*\n        \"\"\"\n        # Load workspace global coverage if any\n        cov = self.workspace.get_metadata_file_path(GlobalCoverage.COVERAGE_FILE)\n        if cov.exists():\n            self.coverage = GlobalCoverage.from_file(cov)\n\n        # Initialize the seed manager\n        self.seeds_manager: SeedManager = SeedManager(self.coverage, self.workspace, self.config.smt_queries_limit, callback_manager=self.cbm, seed_scheduler_class=seed_scheduler_class)\n        \"\"\" Manager of seed, holding all seeds related data and various statistics \"\"\"\n\n        # running executors (for debugging purposes)\n        self.current_executor: SymbolicExecutor = None  #: last symbolic executor executed\n\n        # General purpose attributes\n        self._exec_count = 0\n        self._total_emulation_time = 0\n\n    @property\n    def total_emulation_time(self) -> float:\n        \"\"\" Represent total emulation time. This include all callbacks execution\n        but not the SMT solving time (performed at the end). \"\"\"\n        return self._total_emulation_time\n\n    @property\n    def callback_manager(self) -> CallbackManager:\n        \"\"\"\n        CallbackManager global instance that will be transmitted to\n        all :py:obj:`SymbolicExecutor`.\n\n        :rtype: CallbackManager\n        \"\"\"\n        return self.cbm\n\n    @property\n    def execution_count(self) -> int:\n        \"\"\"\n        Get the number of execution performed.\n\n        :return: number of execution performed\n        :rtype: int\n        \"\"\"\n        return self._exec_count\n\n    def __time_delta(self):\n        return time.time() - self.ts\n\n    def _worker(self, seed, uid):\n        \"\"\" Worker thread \"\"\"\n        logger.info(f'Pick-up seed: {seed.filename} (fresh: {seed.is_fresh()})')\n\n        if self.config.exploration_timeout and self.__time_delta() >= self.config.exploration_timeout:\n            logger.info('Exploration timeout')\n            self.stop_exploration()\n            return\n\n        # Execute the binary with seeds\n        cbs = None if self.cbm.is_empty() else self.cbm.fork()\n        logger.info(f\"Initialize ProcessState with thread scheduling: {self.config.thread_scheduling}\")\n        execution = SymbolicExecutor(self.config, seed=seed, workspace=self.workspace, uid=uid, callbacks=cbs)\n        if self.loader:  # If doing the exploration from a program\n            execution.load(self.loader)\n        else:\n            execution.load_process(ProcessState())\n        self.current_executor = execution\n\n        # increment exec_count\n        self._exec_count += 1\n\n        ts = time.time()\n        try:\n            execution.run(self._executor_stop_at)\n            expl_ts = time.time() - ts\n        except StopExplorationException:\n            expl_ts = time.time() - ts\n            logger.info(\"Exploration interrupted (coverage not integrated)\")\n            self.stop_exploration()\n\n        if self.config.exploration_limit and (uid+1) >= self.config.exploration_limit:\n            logger.info('Exploration limit reached')\n            self.stop_exploration()\n\n        # Some analysis in post execution\n        solve_time = self.seeds_manager.post_execution(execution, seed, not self._stop)\n        self._total_emulation_time += expl_ts\n\n        logger.info(f\"Emulation: {self._fmt_secs(expl_ts)} | Solving: {self._fmt_secs(solve_time)} | Elapsed: {self._fmt_secs(self.__time_delta())}\\n\")\n\n    def step(self) -> None:\n        \"\"\"\n        Perform a single exploration step. That means it execute\n        a single :py:obj:`SymbolicExecutor`. Then it gives the hand\n        back to the user.\n        \"\"\"\n        # Take an input\n        seed = self.seeds_manager.pick_seed()\n\n        # If we don't have any new seed to process just switch exploration to idle\n        if seed is None:\n            logger.info(\"worklist of seed to process is empty\")\n            self.status = ExplorationStatus.IDLE\n            return\n\n        # Iterate the callback to be called at each steps\n        for cb in self.cbm.get_exploration_step_callbacks():\n            cb(self)\n\n        # Execution into a thread\n        t = threading.Thread(\n            name='\\033[0;%dm[exec:%08d]\\033[0m' % ((31 + (self.uid_counter % 4)), self.uid_counter),\n            target=self._worker,\n            args=[seed, self.uid_counter],\n            daemon=True\n        )\n        t.start()\n        self.uid_counter += 1\n\n        while True:\n            t.join(0.001)\n            if not t.is_alive():\n                break\n\n    def explore(self) -> ExplorationStatus:\n        \"\"\"\n        Start the symbolic exploration. That function\n        holds until the exploration is interrupted or finished.\n\n        :returns: the status of the exploration\n        :rtype: ExplorationStatus\n        \"\"\"\n        self.status = ExplorationStatus.RUNNING\n\n        try:\n            while self.seeds_manager.seeds_available() and not self._stop:\n                gc.collect()\n                self.step()\n\n            if self.status == ExplorationStatus.RUNNING:\n                if not self.seeds_manager.seeds_available():\n                    logger.info(\"exploration step done (no new seed available)\")\n                    self.status = ExplorationStatus.IDLE\n                else:\n                    logger.warning(f'should not exit step() in RUNNING state (stop? {self._stop}, seeds available? {self.seeds_manager.seeds_available()})')\n\n        except KeyboardInterrupt:\n            logger.warning(\"keyboard interrupt, stop symbolic exploration\")\n            self.stop_exploration()\n\n        self.post_exploration()\n        logger.info(f\"Total time of the exploration: {self._fmt_secs(self.__time_delta())}\")\n\n        if self.status == ExplorationStatus.IDLE:\n            logger.info(\"Execution IDLE no seeds to execute\")\n\n        return self.status\n\n    def add_input_seed(self, seed: Union[bytes, Seed]) -> None:\n        \"\"\"\n        Add the given bytes or Seed object as input for the exploration.\n\n        :param seed: input seed to add in the pending inputs to process\n        :type seed: Union[bytes, Seed]\n        \"\"\"\n        seed = seed if isinstance(seed, Seed) else Seed(seed)\n        self.seeds_manager.add_new_seed(seed)\n\n    def stop_exploration(self) -> None:\n        \"\"\" Interrupt the exploration \"\"\"\n        self.status = ExplorationStatus.STOPPED\n        self._stop = True\n\n    def terminate_exploration(self) -> None:\n        \"\"\" Terminate exploration with status terminated (normal shutdown) \"\"\"\n        self.status = ExplorationStatus.TERMINATED\n        self._stop = True\n\n    @staticmethod\n    def _fmt_secs(seconds) -> str:\n        m, s = divmod(seconds, 60)\n        h, m = divmod(m, 60)\n        return (f\"{int(h)}h\" if h else '')+f\"{int(m)}m{int(s)}s\"\n\n    def post_exploration(self) -> None:\n        \"\"\" Perform  all calls to post exploration functions\"\"\"\n        self.seeds_manager.post_exploration()\n        self.coverage.post_exploration(self.workspace)", ""]}
{"filename": "tritondse/sanitizers.py", "chunked_list": ["from __future__ import annotations\n\nfrom triton import Instruction\nfrom tritondse.callbacks import CbType, ProbeInterface\nfrom tritondse.seed import Seed, SeedStatus\nfrom tritondse.types import Architecture, Addr, Tuple, SolverStatus\nfrom tritondse import SymbolicExecutor, ProcessState\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"sanitizers\")", "\nlogger = tritondse.logging.get(\"sanitizers\")\n\n\n\ndef mk_new_crashing_seed(se, model) -> Seed:\n    \"\"\"\n    This function is used by every sanitizers to dump the model found in order\n    to trigger a bug into the crash directory.\n\n    :return: A fresh Seed\n    \"\"\"\n    new_input = bytearray(se.seed.content)\n    for k, v in model.items():\n        new_input[k] = v.getValue()\n    # Don't tag the seed as CRASH before executing it.\n    # At this stage, we do not know if the seed will really make the\n    # program crash or not.\n    return Seed(bytes(new_input))", "\n\nclass UAFSanitizer(ProbeInterface):\n    \"\"\"\n    Use-After-Free Sanitizer.\n    It is able to detect UaF and double-free. It works by hooking\n    all memory read/write if it points to the heap in a freed area\n    then the Use-After-Free is detected. It also hooks the free\n    routine to detect double-free.\n    \"\"\"\n    def __init__(self):\n        super(UAFSanitizer, self).__init__()\n        self._add_callback(CbType.MEMORY_READ, self._memory_read)\n        self._add_callback(CbType.MEMORY_WRITE, self._memory_write)\n        self._add_callback(CbType.PRE_RTN, self._free_routine, 'free')\n\n    @staticmethod\n    def check(se: SymbolicExecutor, pstate: ProcessState, ptr: Addr, description: str = None) -> bool:\n        \"\"\"\n        Checks whether the given ``ptr`` is symptomatic of a Use-After-Free by querying\n        various methods of :py:obj:`tritondse.heap_allocator.HeapAllocator`.\n\n        :param se: symbolic executor\n        :type se: SymbolicExecutor\n        :param pstate: process state\n        :type pstate: ProcessState\n        :param ptr: pointer address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :param description: description string printed in logger if an issue is detected\n        :return: True if the bug is present\n        \"\"\"\n        if pstate.is_heap_ptr(ptr) and pstate.heap_allocator.is_ptr_freed(ptr):\n            if description:\n                logger.critical(description)\n            se.seed.status = SeedStatus.CRASH\n            pstate.stop = True\n            return True\n        return False\n\n    @staticmethod\n    def _memory_read(se, pstate, mem):\n        return UAFSanitizer.check(se, pstate, mem.getAddress(), f'UAF detected at {mem}')\n\n    @staticmethod\n    def _memory_write(se, pstate, mem, value):\n        return UAFSanitizer.check(se, pstate, mem.getAddress(), f'UAF detected at {mem}')\n\n    @staticmethod\n    def _free_routine(se, pstate, name, addr):\n        ptr = se.pstate.get_argument_value(0)\n        return UAFSanitizer.check(se, pstate, ptr, f'Double free detected at {addr:#x}')", "\n\nclass NullDerefSanitizer(ProbeInterface):\n    \"\"\"\n    Null Dereference Sanitizer.\n    Simply checks if any memory read or write is performed at address 0.\n    If so an error is raised.\n    \"\"\"\n    def __init__(self):\n        super(NullDerefSanitizer, self).__init__()\n        self._add_callback(CbType.MEMORY_READ, self._memory_read)\n        self._add_callback(CbType.MEMORY_WRITE, self._memory_write)\n\n    @staticmethod\n    def check(se: SymbolicExecutor, pstate: ProcessState, ptr: Addr, description: str = None) -> bool:\n        \"\"\"\n        Checks that the ``ptr`` given is basically not 0.\n\n        :param se: symbolic executor\n        :type se: SymbolicExecutor\n        :param pstate: process state\n        :type pstate: ProcessState\n        :param ptr: pointer address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :param description: description string printed in logger if an issue is detected\n        :return: True if the bug is present\n        \"\"\"\n\n         # The execution has not started yet\n        if pstate.current_instruction is None:\n            return False\n\n        # FIXME: Takes so much time...\n        #if access_ast is not None and access_ast.isSymbolized():\n        #    model = pstate.tt_ctx.getModel(access_ast == 0)\n        #    if model:\n        #        logging.warning(f'Potential null deref when reading at {mem}')\n        #        crash_seed = mk_new_crashing_seed(se, model)\n        #        se.workspace.save_seed(crash_seed)\n        #        se.seed.status = SeedStatus.OK_DONE\n        #        # Do not abort, just continue the execution\n        #if access_ast is not None and access_ast.evaluate() == 0:\n\n        # FIXME: Ici on rajoute 16 car nous avons un probl\u00e8me si une instruction se situe\n        # en fin de page mapp\u00e9e. Lors du fetching des opcodes, nous fetchons 16 bytes car\n        # nous ne connaissons pas la taille d'une instruction, ici, en fetchant en fin de\n        # page on d\u00e9clenche ce sanitizer...\n\n        # FIXME: Why do we call is_valid_memory_mapping ? It is not a \"Null Deref vulnerability\", it is more a segmentation error\n        if ptr == 0 or (pstate.memory.segmentation_enabled and not pstate.memory.is_mapped(ptr)):\n            if description:\n                logger.critical(description)\n            se.seed.status = SeedStatus.CRASH\n            pstate.stop = True\n            return True\n\n        return False\n\n    @staticmethod\n    def _memory_read(se, pstate, mem):\n        return NullDerefSanitizer.check(se, pstate, mem.getAddress(), f'Invalid memory access when reading at {mem} from {pstate.current_instruction}')\n\n    @staticmethod\n    def _memory_write(se, pstate, mem, value):\n        return NullDerefSanitizer.check(se, pstate, mem.getAddress(), f'Invalid memory access when writting at {mem} from {pstate.current_instruction}')", "\n\nclass FormatStringSanitizer(ProbeInterface):\n    \"\"\"\n    Format String Sanitizer.\n    This probes hooks standard libc functions like 'printf', 'fprintf', 'sprintf',\n    'dprintf', 'snprintf' and if one of them is triggered it checks the format string.\n    If the format string is symbolic then it is user controlled. A warning is shown\n    but the execution not interrupted. However the sanitizer tries through SMT to\n    generate format strings with many '%s'. If satisfiable a new input is generated\n    which will then be added to inputs to process. That subsequent input might lead\n    to a crash.\n    \"\"\"\n    def __init__(self):\n        super(FormatStringSanitizer, self).__init__()\n        self._add_callback(CbType.PRE_RTN,  self._xprintf_arg0_routine, 'printf')\n        self._add_callback(CbType.PRE_RTN, self._xprintf_arg1_routine, 'fprintf')\n        self._add_callback(CbType.PRE_RTN, self._xprintf_arg1_routine, 'sprintf')\n        self._add_callback(CbType.PRE_RTN, self._xprintf_arg1_routine, 'dprintf')\n        self._add_callback(CbType.PRE_RTN, self._xprintf_arg1_routine, 'snprintf')\n\n    @staticmethod\n    def check(se, pstate, fmt_ptr, extra_data: Tuple[str, Addr] = None):\n        \"\"\"\n        Checks that the format string at ``fmt_ptr`` does not contain\n        symbolic bytes. If so shows an alert and tries to generate new\n        inputs with as many '%s' as possible.\n\n        :param se: symbolic executor\n        :type se: SymbolicExecutor\n        :param pstate: process state\n        :type pstate: ProcessState\n        :param fmt_ptr: pointer address to check\n        :type fmt_ptr: :py:obj:`tritondse.types.Addr`\n        :param extra_data: additionnal infos given by the callbacks on routines (indicating function address)\n        :type extra_data: Tuple[str, :py:obj:`tritondse.types.Addr`]\n        :return: True if the bug is present\n        \"\"\"\n        symbolic_cells = []\n\n        # Count the number of cells which is symbolic\n        cur_ptr = fmt_ptr\n        while se.pstate.memory.read_uchar(cur_ptr):  # while different from 0\n            if se.pstate.is_memory_symbolic(cur_ptr, 1):\n                symbolic_cells.append(cur_ptr)\n            cur_ptr += 1\n\n        if symbolic_cells:\n            extra = f\"(function {extra_data[0]}@{extra_data[1]:#x})\" if extra_data else \"\"\n            logger.warning(f'Potential format string of length {len(symbolic_cells)} on {fmt_ptr:x} {extra}')\n            se.seed.status = SeedStatus.OK_DONE\n            pp_seeds = []\n            nopp_seeds = []\n\n            for i in range(int(len(symbolic_cells) / 2)):\n                # FIXME: Does not check that cell1 and cell2 are contiguous\n                cell1 = pstate.read_symbolic_memory_byte(symbolic_cells.pop(0)).getAst()\n                cell2 = pstate.read_symbolic_memory_byte(symbolic_cells.pop(0)).getAst()\n\n                # Try to solve once with the path predicate\n                st, model = pstate.solve([cell1 == ord('%'), cell2 == ord('s')], with_pp=True)\n                if st == SolverStatus.SAT and model:\n                    pp_seeds.append(mk_new_crashing_seed(se, model))\n\n                # Try once again without the path predicate (may be incorrect but help to discover bug)\n                st, model = pstate.solve_no_pp([cell1 == ord('%'), cell2 == ord('s')])\n                if st == SolverStatus.SAT and model:\n                    pp_seeds.append(mk_new_crashing_seed(se, model))\n\n            # If found some seeds\n            if pp_seeds:\n                s = pp_seeds[-1]\n                se.enqueue_seed(s)  # Only keep last seed\n                logger.warning(f'Found model that might lead to a crash: {s.hash} (with path predicate)')\n            if nopp_seeds:\n                s = nopp_seeds[-1]\n                se.enqueue_seed(s)  # Only keep last seed\n                logger.warning(f'Found model that might lead to a crash: {s.hash} (without path predicate)')\n\n            # Do not stop the execution, just continue the execution\n            pstate.stop = False\n            return True\n        return False\n\n    @staticmethod\n    def _xprintf_arg0_routine(se, pstate, name, addr):\n        string_ptr = se.pstate.get_argument_value(0)\n        FormatStringSanitizer.check(se, pstate, string_ptr, (name, addr))\n\n    @staticmethod\n    def _xprintf_arg1_routine(se, pstate, name, addr):\n        string_ptr = se.pstate.get_argument_value(1)\n        FormatStringSanitizer.check(se, pstate, string_ptr, (name, addr))", "\n\nclass IntegerOverflowSanitizer(ProbeInterface):\n    \"\"\"\n    Integer Overflow Sanitizer.\n    This probe checks on every instructions that the overflow\n    flag is not set. If so mark the input as a crashing input.\n    If not, but the value is symbolic, via SMT solving to make\n    it to be set (and thus to overflow). If possible generates\n    a new input to be executed.\n    \"\"\"\n    def __init__(self):\n        super(IntegerOverflowSanitizer, self).__init__()\n        self._add_callback(CbType.POST_INST, self.check)\n\n    @staticmethod\n    def check(se: SymbolicExecutor, pstate: ProcessState, instruction: Instruction) -> bool:\n        \"\"\"\n        The entry point of the sanitizer. This function check if a bug is present\n\n        :param se: symbolic executor\n        :type se: SymbolicExecutor\n        :param pstate: process state\n        :type pstate: ProcessState\n        :param instruction: Instruction that has just been executed\n        :type instruction: `Instruction <https://triton.quarkslab.com/documentation/doxygen/py_Instruction_page.html>`_\n        :return: True if the bug is present\n        \"\"\"\n        # This probe is only available for X86_64 and AARCH64\n        assert(pstate.architecture == Architecture.X86_64 or pstate.architecture == Architecture.AARCH64)\n\n        rf = (pstate.registers.of if pstate.architecture == Architecture.X86_64 else pstate.registers.v)\n\n        if pstate.read_register(rf):\n            logger.warning(f'Integer overflow at {instruction}')\n            # FIXME: What if it's normal behavior?\n            se.seed.status = SeedStatus.CRASH\n            return True\n\n        else:  # if no overflow took place check if symbolic and if so, if it can be 1\n            if pstate.is_register_symbolic(rf):\n                sym_flag = pstate.read_symbolic_register(rf)\n                _, model = pstate.solve_no_pp(sym_flag.getAst() == 1)\n                if model:\n                    logger.warning(f'Potential integer overflow at {instruction}')\n                    crash_seed = mk_new_crashing_seed(se, model)\n                    se.enqueue_seed(crash_seed)\n                    return True\n\n        return False", ""]}
{"filename": "tritondse/arch.py", "chunked_list": ["# built-in modules\nimport platform\nfrom collections import namedtuple\n\n# third-party module\nfrom triton import OPCODE, TritonContext\n\n# local imports\nfrom tritondse.types import Architecture, Addr\n", "from tritondse.types import Architecture, Addr\n\nArch = namedtuple(\"Arch\", \"ret_reg pc_reg bp_reg sp_reg sys_reg reg_args halt_inst syscall_inst\")\n\nARCHS = {\n    Architecture.X86:     Arch('eax', 'eip', 'ebp', 'esp', 'eax',\n                               [],\n                               OPCODE.X86.HLT,\n                               [OPCODE.X86.SYSCALL, OPCODE.X86.SYSENTER]),  # ignore int 80\n    Architecture.X86_64:  Arch('rax', 'rip', 'rbp', 'rsp', 'rax',", "                               [OPCODE.X86.SYSCALL, OPCODE.X86.SYSENTER]),  # ignore int 80\n    Architecture.X86_64:  Arch('rax', 'rip', 'rbp', 'rsp', 'rax',\n                               ['rdi', 'rsi', 'rdx', 'rcx', 'r8', 'r9'],\n                               OPCODE.X86.HLT,\n                               [OPCODE.X86.SYSCALL, OPCODE.X86.SYSENTER]),  # ignore int 80\n    Architecture.AARCH64: Arch('x0', 'pc', 'sp', 'sp', 'x8',\n                               ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7'],\n                               OPCODE.AARCH64.HLT,\n                               [OPCODE.AARCH64.SVC]),\n    Architecture.ARM32:   Arch('r0', 'pc', 'r11', 'sp', 'r7',", "                               [OPCODE.AARCH64.SVC]),\n    Architecture.ARM32:   Arch('r0', 'pc', 'r11', 'sp', 'r7',\n                               ['r0', 'r1', 'r2', 'r3'],\n                               OPCODE.ARM32.HLT,\n                               [OPCODE.ARM32.SVC])\n}\n\n\nclass CpuState(dict):\n    \"\"\"\n    Thin wrapper on a TritonContext, to allow accessing\n    and modifying registers in a Pythonic way. It also\n    abstract base, stack, and program counter for architecture\n    agnostic operations. This class performs all actions\n    on the TritonContext, and does not hold any information.\n    It is just acting as a proxy\n\n    .. note:: This class adds dynamically attributes corresponding\n              to register. Thus attributes will vary from an architecture\n              to the other.\n\n    >>> cpu.rax\n    12\n    >>> cpu.rax += 1\n    >>> cpu.rax\n    13\n\n    No data is stored, all operations are performed on the\n    TritonContext:\n\n    >>> cpu.__ctx.getConcreteRegisterValue(cpu.rsp)\n    0x7ff6540\n    >>> cpu.stack_pointer += 8\n    >>> cpu.__ctx.getConcreteRegisterValue(cpu.rsp)\n    0x7ff6548\n\n    .. note:: The user is not meant to instanciate it manually, and must\n              use it through :py:obj:`ProcessState`.\n    \"\"\"\n\n    def __init__(self, ctx: TritonContext, arch_info: Arch):\n        super(CpuState, self).__init__()\n        self.__ctx = ctx\n        self.__archinfo = arch_info\n        for r in ctx.getAllRegisters():\n            self[r.getName()] = r\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        Return the concrete value of a given register name\n\n        :param name: The name of the register\n        :return: the concrete value of a given register\n        \"\"\"\n        if name in self:\n            return self.__ctx.getConcreteRegisterValue(self[name])\n        else:\n            super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: int):\n        \"\"\"\n        Set a concrete value to a given register name\n\n        :param name: The name of the register\n        :param value: The concrete value to set\n        \"\"\"\n        if name in self:\n            self.__ctx.setConcreteRegisterValue(self[name], value)\n        else:\n            super().__setattr__(name, value)\n\n    @property\n    def program_counter(self) -> int:\n        \"\"\"\n        :return: The value of the program counter (RIP for x86, PC for ARM ..)\n        :rtype: int\n        \"\"\"\n        return getattr(self, self.__archinfo.pc_reg)\n\n    @program_counter.setter\n    def program_counter(self, value: int) -> None:\n        \"\"\"\n        Set a value to the program counter\n\n        :param value: Value to set\n        :type value: int\n        \"\"\"\n        setattr(self, self.__archinfo.pc_reg, value)\n\n    @property\n    def base_pointer(self) -> int:\n        \"\"\"\n        :return: The value of the base pointer register\n        \"\"\"\n        return getattr(self, self.__archinfo.bp_reg)\n\n    @base_pointer.setter\n    def base_pointer(self, value: int) -> None:\n        \"\"\"\n        Set a value to the base pointer register\n\n        :param value: Value to set\n        :return: None\n        \"\"\"\n        setattr(self, self.__archinfo.bp_reg, value)\n\n    @property\n    def stack_pointer(self) -> int:\n        \"\"\"\n        :return: The value of the stack pointer register\n        \"\"\"\n        return getattr(self, self.__archinfo.sp_reg)\n\n    @stack_pointer.setter\n    def stack_pointer(self, value: int) -> None:\n        \"\"\"\n        Set a value to the stack pointer register\n\n        :param value: Value to set\n        :type value: int\n        \"\"\"\n        setattr(self, self.__archinfo.sp_reg, value)", "class CpuState(dict):\n    \"\"\"\n    Thin wrapper on a TritonContext, to allow accessing\n    and modifying registers in a Pythonic way. It also\n    abstract base, stack, and program counter for architecture\n    agnostic operations. This class performs all actions\n    on the TritonContext, and does not hold any information.\n    It is just acting as a proxy\n\n    .. note:: This class adds dynamically attributes corresponding\n              to register. Thus attributes will vary from an architecture\n              to the other.\n\n    >>> cpu.rax\n    12\n    >>> cpu.rax += 1\n    >>> cpu.rax\n    13\n\n    No data is stored, all operations are performed on the\n    TritonContext:\n\n    >>> cpu.__ctx.getConcreteRegisterValue(cpu.rsp)\n    0x7ff6540\n    >>> cpu.stack_pointer += 8\n    >>> cpu.__ctx.getConcreteRegisterValue(cpu.rsp)\n    0x7ff6548\n\n    .. note:: The user is not meant to instanciate it manually, and must\n              use it through :py:obj:`ProcessState`.\n    \"\"\"\n\n    def __init__(self, ctx: TritonContext, arch_info: Arch):\n        super(CpuState, self).__init__()\n        self.__ctx = ctx\n        self.__archinfo = arch_info\n        for r in ctx.getAllRegisters():\n            self[r.getName()] = r\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        Return the concrete value of a given register name\n\n        :param name: The name of the register\n        :return: the concrete value of a given register\n        \"\"\"\n        if name in self:\n            return self.__ctx.getConcreteRegisterValue(self[name])\n        else:\n            super().__getattr__(name)\n\n    def __setattr__(self, name: str, value: int):\n        \"\"\"\n        Set a concrete value to a given register name\n\n        :param name: The name of the register\n        :param value: The concrete value to set\n        \"\"\"\n        if name in self:\n            self.__ctx.setConcreteRegisterValue(self[name], value)\n        else:\n            super().__setattr__(name, value)\n\n    @property\n    def program_counter(self) -> int:\n        \"\"\"\n        :return: The value of the program counter (RIP for x86, PC for ARM ..)\n        :rtype: int\n        \"\"\"\n        return getattr(self, self.__archinfo.pc_reg)\n\n    @program_counter.setter\n    def program_counter(self, value: int) -> None:\n        \"\"\"\n        Set a value to the program counter\n\n        :param value: Value to set\n        :type value: int\n        \"\"\"\n        setattr(self, self.__archinfo.pc_reg, value)\n\n    @property\n    def base_pointer(self) -> int:\n        \"\"\"\n        :return: The value of the base pointer register\n        \"\"\"\n        return getattr(self, self.__archinfo.bp_reg)\n\n    @base_pointer.setter\n    def base_pointer(self, value: int) -> None:\n        \"\"\"\n        Set a value to the base pointer register\n\n        :param value: Value to set\n        :return: None\n        \"\"\"\n        setattr(self, self.__archinfo.bp_reg, value)\n\n    @property\n    def stack_pointer(self) -> int:\n        \"\"\"\n        :return: The value of the stack pointer register\n        \"\"\"\n        return getattr(self, self.__archinfo.sp_reg)\n\n    @stack_pointer.setter\n    def stack_pointer(self, value: int) -> None:\n        \"\"\"\n        Set a value to the stack pointer register\n\n        :param value: Value to set\n        :type value: int\n        \"\"\"\n        setattr(self, self.__archinfo.sp_reg, value)", "\n\ndef local_architecture() -> Architecture:\n    \"\"\"\n    Returns the architecture of the local machine.\n\n    :return: local architecture\n    \"\"\"\n    arch_m = {\"i386\": Architecture.X86,\n              \"x86_64\": Architecture.X86_64,\n              \"armv7l\": Architecture.ARM32,\n              \"aarch64\": Architecture.AARCH64}\n    return arch_m[platform.machine()]", ""]}
{"filename": "tritondse/routines.py", "chunked_list": ["import io\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport time\n\nfrom tritondse.types import Architecture, FileDesc\nfrom tritondse.seed import SeedFormat, SeedStatus, Seed", "from tritondse.types import Architecture, FileDesc\nfrom tritondse.seed import SeedFormat, SeedStatus, Seed\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"routines\")\n\nNULL_PTR = 0\n\n\n\ndef rtn_ctype_b_loc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __ctype_b_loc behavior.\n    \"\"\"\n    logger.debug('__ctype_b_loc hooked')\n\n    ctype  = b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"  # must point here\n    ctype += b\"\\x02\\x00\\x03\\x20\\x02\\x20\\x02\\x20\\x02\\x20\\x02\\x20\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x01\\x60\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\"\n    ctype += b\"\\x08\\xd8\\x08\\xd8\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x02\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n\n    # Allocate on heap enough to make the table to fit\n    alloc_size = 2*pstate.ptr_size + len(ctype)\n    base_ctype = pstate.heap_allocator.alloc(alloc_size)\n\n    ctype_table_offset = base_ctype + (pstate.ptr_size * 2)\n    otable_offset = ctype_table_offset + 256\n\n    pstate.memory.write_ptr(base_ctype, otable_offset)\n    pstate.memory.write_ptr(base_ctype+pstate.ptr_size, 0)\n\n    # FIXME: On pourrait la renvoyer qu'une seule fois ou la charger au demarage direct dans pstate\n    pstate.memory.write(ctype_table_offset, ctype)\n\n    return base_ctype", "\n\ndef rtn_ctype_b_loc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __ctype_b_loc behavior.\n    \"\"\"\n    logger.debug('__ctype_b_loc hooked')\n\n    ctype  = b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"  # must point here\n    ctype += b\"\\x02\\x00\\x03\\x20\\x02\\x20\\x02\\x20\\x02\\x20\\x02\\x20\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\\x02\\x00\"\n    ctype += b\"\\x01\\x60\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\\x08\\xd8\"\n    ctype += b\"\\x08\\xd8\\x08\\xd8\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xd5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x08\\xc5\"\n    ctype += b\"\\x08\\xc5\\x08\\xc5\\x08\\xc5\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\"\n    ctype += b\"\\x04\\xc0\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xd6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x08\\xc6\"\n    ctype += b\"\\x08\\xc6\\x08\\xc6\\x08\\xc6\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x04\\xc0\\x02\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n    ctype += b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n\n    # Allocate on heap enough to make the table to fit\n    alloc_size = 2*pstate.ptr_size + len(ctype)\n    base_ctype = pstate.heap_allocator.alloc(alloc_size)\n\n    ctype_table_offset = base_ctype + (pstate.ptr_size * 2)\n    otable_offset = ctype_table_offset + 256\n\n    pstate.memory.write_ptr(base_ctype, otable_offset)\n    pstate.memory.write_ptr(base_ctype+pstate.ptr_size, 0)\n\n    # FIXME: On pourrait la renvoyer qu'une seule fois ou la charger au demarage direct dans pstate\n    pstate.memory.write(ctype_table_offset, ctype)\n\n    return base_ctype", "\ndef rtn_ctype_toupper_loc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    # FIXME: Not sure about the array and where to place the pointer \n    # https://codebrowser.dev/glibc/glibc/locale/C-ctype.c.html\n    \"\"\"\n    The __ctype_toupper_loc behavior.\n    \"\"\"\n    logger.debug('__ctype_toupper_loc hooked')\n\n    ctype  = b\"\\x80\\x81\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x8b\\x8c\\x8d\\x8e\\x8f\"\n    ctype += b\"\\x90\\x91\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\x9b\\x9c\\x9d\\x9e\\x9f\"\n    ctype += b\"\\xa0\\xa1\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xab\\xac\\xad\\xae\\xaf\"\n    ctype += b\"\\xb0\\xb1\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xbb\\xbc\\xbd\\xbe\\xbf\"\n    ctype += b\"\\xc0\\xc1\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xcb\\xcc\\xcd\\xce\\xcf\"\n    ctype += b\"\\xd0\\xd1\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xdb\\xdc\\xdd\\xde\\xdf\"\n    ctype += b\"\\xe0\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xeb\\xec\\xed\\xee\\xef\"\n    ctype += b\"\\xf0\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xfb\\xfc\\xfd\\xfe\\xff\\xff\\xff\\xff\"\n    ctype += b\"\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\"\n    ctype += b\"\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\"\n    ctype += b\"\\x20\\x21\\x22\\x23\\x24\\x25\\x26\\x27\\x28\\x29\\x2a\\x2b\\x2c\\x2d\\x2e\\x2f\"\n    ctype += b\"\\x30\\x31\\x32\\x33\\x34\\x35\\x36\\x37\\x38\\x39\\x3a\\x3b\\x3c\\x3d\\x3e\\x3f\"\n    ctype += b\"\\x40\\x41\\x42\\x43\\x44\\x45\\x46\\x47\\x48\\x49\\x4a\\x4b\\x4c\\x4d\\x4e\\x4f\"\n    ctype += b\"\\x50\\x51\\x52\\x53\\x54\\x55\\x56\\x57\\x58\\x59\\x5a\\x5b\\x5c\\x5d\\x5e\\x5f\"\n    ctype += b\"\\x60\\x41\\x42\\x43\\x44\\x45\\x46\\x47\\x48\\x49\\x4a\\x4b\\x4c\\x4d\\x4e\\x4f\"\n    ctype += b\"\\x50\\x51\\x52\\x53\\x54\\x55\\x56\\x57\\x58\\x59\\x5a\\x7b\\x7c\\x7d\\x7e\\x7f\"\n    ctype += b\"\\x80\\x81\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x8b\\x8c\\x8d\\x8e\\x8f\"\n    ctype += b\"\\x90\\x91\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\x9b\\x9c\\x9d\\x9e\\x9f\"\n    ctype += b\"\\xa0\\xa1\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xab\\xac\\xad\\xae\\xaf\"\n    ctype += b\"\\xb0\\xb1\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xbb\\xbc\\xbd\\xbe\\xbf\"\n    ctype += b\"\\xc0\\xc1\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xcb\\xcc\\xcd\\xce\\xcf\"\n    ctype += b\"\\xd0\\xd1\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xdb\\xdc\\xdd\\xde\\xdf\"\n    ctype += b\"\\xe0\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xeb\\xec\\xed\\xee\\xef\"\n    ctype += b\"\\xf0\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xfb\\xfc\\xfd\\xfe\\xff\"\n\n    # Allocate on heap enough to make the table to fit\n    alloc_size = 2*pstate.ptr_size + len(ctype)\n    base_ctype = pstate.heap_allocator.alloc(alloc_size)\n\n    ctype_table_offset = base_ctype + (pstate.ptr_size * 2)\n    otable_offset = ctype_table_offset + 256\n\n    pstate.memory.write_ptr(base_ctype, otable_offset)\n    pstate.memory.write_ptr(base_ctype+pstate.ptr_size, 0)\n\n    # FIXME: On pourrait la renvoyer qu'une seule fois ou la charger au demarage direct dans pstate\n    pstate.memory.write(ctype_table_offset, ctype)\n\n    return base_ctype", "\n\ndef rtn_errno_location(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __errno_location behavior.\n    \"\"\"\n    logger.debug('__errno_location hooked')\n\n    # Errno is a int* ptr, initialize it to zero\n    # We consider it is located in the [extern] segment\n    # Thus the process must have one of this map\n    segs = pstate.memory.find_map(pstate.EXTERN_SEG)\n    if segs:\n        map = segs[0]\n        ERRNO = map.start + map.size - 4  # Point is last int of the mapping\n    else:\n        assert False\n    pstate.memory.write_dword(ERRNO, 0)\n\n    return ERRNO", "\n\ndef rtn_libc_start_main(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __libc_start_main behavior.\n    \"\"\"\n    logger.debug('__libc_start_main hooked')\n\n    # Get arguments\n    main = pstate.get_argument_value(0)\n\n    # WARNING: Dirty trick to make sure to jump on main after\n    # the emulation of that stub\n    if pstate.architecture == Architecture.AARCH64:\n        pstate.cpu.x30 = main\n    elif pstate.architecture in [Architecture.X86_64, Architecture.X86]:\n        # Push the return value to jump into the main() function\n        pstate.push_stack_value(main)\n    else:\n        assert False\n\n    # Define concrete value of argc (from either the seed or the program_argv)\n    if se.config.is_format_raw():\n        # Cannot provide argv in RAW seeds\n        argc = len(se.config.program_argv)\n    else: # SeedFormat.COMPOSITE\n        argc = len(se.seed.content.argv) if se.seed.content.argv else len(se.config.program_argv)\n\n    if pstate.architecture == Architecture.X86:\n        # Because of the \"Dirty trick\" described above, we RET to main instead of CALLing it.\n        # Because of that, the arguments end up 1 slot off on the stack\n        pstate.write_argument_value(0 + 1, argc)\n    else:\n        pstate.write_argument_value(0, argc)\n    logger.debug(f\"argc = {argc}\")\n\n    # Define argv\n    addrs = list()\n\n    if se.config.is_format_composite() and se.seed.content.argv: # Use the seed provided (and ignore config.program_argv !!)\n        argvs = se.seed.content.argv\n        src = 'seed'\n    else:  # use the config argv\n        argvs = [x.encode(\"latin-1\") for x in se.config.program_argv]  # Convert it from str to bytes\n        src = 'config'\n\n    # Compute the allocation size: size of strings, + all \\x00 + all pointers\n    size = sum(len(x) for x in argvs)+len(argvs)+len(argvs)*pstate.ptr_size\n    if size == 0:  # Falback on a single pointer that will hold not even be initialized\n        size = pstate.ptr_size\n\n    # We put the ARGV stuff on the heap even though its normally on stack\n    base = pstate.heap_allocator.alloc(size)\n\n    for i, arg in enumerate(argvs):\n        addrs.append(base)\n        pstate.memory.write(base, arg + b'\\x00')\n\n        if se.config.is_format_composite() and se.seed.content.argv: # Use the seed provided (and ignore config.program_argv !!)\n            # Symbolize the argv string\n            se.inject_symbolic_argv_memory(base, i, arg)\n            # FIXME: Shall add a constraint on every char to be != \\x00\n\n        logger.debug(f\"({src}) argv[{i}] = {repr(pstate.memory.read(base, len(arg)))}\")\n        base += len(arg) + 1\n\n\n    # NOTE: the array of pointers will be after the string themselves\n    b_argv = base\n    for addr in addrs:\n        pstate.memory.write_ptr(base, addr)\n        base += pstate.ptr_size\n\n    # Concrete value\n    if pstate.architecture == Architecture.X86:\n        # Because of the \"Dirty trick\" described above, we RET to main instead of CALLing it.\n        # Because of that, the arguments end up 1 slot off on the stack\n        pstate.write_argument_value(1 + 1, b_argv)\n    else:\n        pstate.write_argument_value(1, b_argv)\n    return None", "\n\ndef rtn_stack_chk_fail(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __stack_chk_fail behavior.\n    \"\"\"\n    logger.debug('__stack_chk_fail hooked')\n    logger.critical('*** stack smashing detected ***: terminated')\n    se.seed.status = SeedStatus.CRASH\n    pstate.stop = True", "\n\n# int __xstat(int ver, const char* path, struct stat* stat_buf);\ndef rtn_xstat(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __xstat behavior.\n    \"\"\"\n    logger.debug('__xstat hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # int ver\n    arg1 = pstate.get_argument_value(1)  # const char* path\n    arg2 = pstate.get_argument_value(2)  # struct stat* stat_buf\n\n    if os.path.isfile(pstate.memory.read_string(arg1)):\n        stat = os.stat(pstate.memory.read_string(arg1))\n        pstate.memory.write_qword(arg2 + 0x00, stat.st_dev)\n        pstate.memory.write_qword(arg2 + 0x08, stat.st_ino)\n        pstate.memory.write_qword(arg2 + 0x10, stat.st_nlink)\n        pstate.memory.write_dword(arg2 + 0x18, stat.st_mode)\n        pstate.memory.write_dword(arg2 + 0x1c, stat.st_uid)\n        pstate.memory.write_dword(arg2 + 0x20, stat.st_gid)\n        pstate.memory.write_dword(arg2 + 0x24, 0)\n        pstate.memory.write_qword(arg2 + 0x28, stat.st_rdev)\n        pstate.memory.write_qword(arg2 + 0x30, stat.st_size)\n        pstate.memory.write_qword(arg2 + 0x38, stat.st_blksize)\n        pstate.memory.write_qword(arg2 + 0x40, stat.st_blocks)\n        pstate.memory.write_qword(arg2 + 0x48, 0)\n        pstate.memory.write_qword(arg2 + 0x50, 0)\n        pstate.memory.write_qword(arg2 + 0x58, 0)\n        pstate.memory.write_qword(arg2 + 0x60, 0)\n        pstate.memory.write_qword(arg2 + 0x68, 0)\n        pstate.memory.write_qword(arg2 + 0x70, 0)\n        pstate.memory.write_qword(arg2 + 0x78, 0)\n        pstate.memory.write_qword(arg2 + 0x80, 0)\n        pstate.memory.write_qword(arg2 + 0x88, 0)\n        return 0\n\n    return pstate.minus_one", "\n\ndef rtn_abort(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"::\n\n        void abort(void);\n\n    Mark the input seed as OK and stop execution.\n\n    [`Man Page <https://man7.org/linux/man-pages/man3/abort.3.html>`_]\n    \"\"\"\n    logger.debug('abort hooked')\n    se.seed.status = SeedStatus.OK_DONE\n    pstate.stop = True", "\n\n# int atoi(const char *nptr);\ndef rtn_atoi(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"::\n\n        int atoi(const char *nptr);\n\n    **Description**: The atoi() function converts the initial portion of the string\n    pointed to by nptr to int.  The behavior is the same as\n\n    Concrete: /\n\n    Symbolic: Represent the return value symbolically with 10 nested if\n    to represent the value.\n\n    .. warning:: The function does not support all possibles representation\n                 of an integer. It does not support negative integer nor values\n                 prefixed by spaces.\n\n    [`Man Page <https://man7.org/linux/man-pages/man3/abort.3.html>`_]\n\n    :return: Symbolic value of the integer base on the symbolic string ``nptr``\n    \"\"\"\n    logger.debug('atoi hooked')\n\n    ast = pstate.actx\n    arg = pstate.get_argument_value(0)\n\n    # FIXME: On ne concretize pas correctement la taille de la chaine\n\n    cells = {i: pstate.read_symbolic_memory_byte(arg+i).getAst() for i in range(10)}\n\n    # FIXME: Does not support negative value and all other corner cases.\n    # \"000000000012372183762173\"\n    # \"         98273483274\"\n    # \"-123123\"\n    # \"18273213\"\n\n    def multiply(ast, cells, index):\n        n = ast.bv(0, 32)\n        for i in range(index):\n            n = n * 10 + (ast.zx(24, cells[i]) - 0x30)\n        return n\n\n    res = ast.ite(\n              ast.lnot(ast.land([cells[0] >= 0x30, cells[0] <= 0x39])),\n              multiply(ast, cells, 0),\n              ast.ite(\n                  ast.lnot(ast.land([cells[1] >= 0x30, cells[1] <= 0x39])),\n                  multiply(ast, cells, 1),\n                  ast.ite(\n                      ast.lnot(ast.land([cells[2] >= 0x30, cells[2] <= 0x39])),\n                      multiply(ast, cells, 2),\n                      ast.ite(\n                          ast.lnot(ast.land([cells[3] >= 0x30, cells[3] <= 0x39])),\n                          multiply(ast, cells, 3),\n                          ast.ite(\n                              ast.lnot(ast.land([cells[4] >= 0x30, cells[4] <= 0x39])),\n                              multiply(ast, cells, 4),\n                              ast.ite(\n                                  ast.lnot(ast.land([cells[5] >= 0x30, cells[5] <= 0x39])),\n                                  multiply(ast, cells, 5),\n                                  ast.ite(\n                                      ast.lnot(ast.land([cells[6] >= 0x30, cells[6] <= 0x39])),\n                                      multiply(ast, cells, 6),\n                                      ast.ite(\n                                          ast.lnot(ast.land([cells[7] >= 0x30, cells[7] <= 0x39])),\n                                          multiply(ast, cells, 7),\n                                          ast.ite(\n                                              ast.lnot(ast.land([cells[8] >= 0x30, cells[8] <= 0x39])),\n                                              multiply(ast, cells, 8),\n                                              ast.ite(\n                                                  ast.lnot(ast.land([cells[9] >= 0x30, cells[9] <= 0x39])),\n                                                  multiply(ast, cells, 9),\n                                                  multiply(ast, cells, 9)\n                                              )\n                                          )\n                                      )\n                                  )\n                              )\n                          )\n                      )\n                  )\n              )\n          )\n    res = ast.sx(32, res)\n\n    return res", "\n\n# void *calloc(size_t nmemb, size_t size);\ndef rtn_calloc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The calloc behavior.\n    \"\"\"\n    logger.debug('calloc hooked')\n\n    # Get arguments\n    nmemb = pstate.get_argument_value(0)\n    size  = pstate.get_argument_value(1)\n\n    # We use nmemb and size as concret values\n    pstate.concretize_argument(0)  # will be concretized with nmemb value\n    pstate.concretize_argument(1)  # will be concretized with size value\n\n    if nmemb == 0 or size == 0:\n        ptr = NULL_PTR\n    else:\n        ptr = pstate.heap_allocator.alloc(nmemb * size)\n        # Once the ptr allocated, the memory area must be filled with zero\n        for index in range(nmemb * size):\n            pstate.write_symbolic_memory_byte(ptr+index, pstate.actx.bv(0, 8))\n\n    # Return value\n    return ptr", "\n\n# int clock_gettime(clockid_t clockid, struct timespec *tp);\ndef rtn_clock_gettime(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The clock_gettime behavior.\n    \"\"\"\n    logger.debug('clock_gettime hooked')\n\n    # Get arguments\n    clockid = pstate.get_argument_value(0)\n    tp      = pstate.get_argument_value(1)\n\n    # We use tp as concret value\n    pstate.concretize_argument(1)\n\n    # FIXME: We can return something logic\n    if tp == 0:\n        return pstate.minus_one\n\n    if pstate.time_inc_coefficient:\n        t = pstate.time\n    else:\n        t = time.time()\n\n    pstate.memory.write_ptr(tp, int(t))\n    pstate.memory.write_ptr(tp+pstate.ptr_size, int(t * 1000000))\n\n    # Return value\n    return 0", "\n\ndef rtn_exit(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The exit behavior.\n    \"\"\"\n    logger.debug('exit hooked')\n    arg = pstate.get_argument_value(0)\n    pstate.stop = True\n    return arg", "\n\ndef rtn_fclose(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fclose behavior.\n    \"\"\"\n    logger.debug('fclose hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0) # fd\n\n    # We use fd as concret value\n    pstate.concretize_argument(0)\n\n    if pstate.file_descriptor_exists(arg0):\n        pstate.close_file_descriptor(arg0)\n    else:\n        return pstate.minus_one\n\n    # Return value\n    return 0", "\n\ndef rtn_fseek(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fseek behavior.\n    \"\"\"\n\n    #define SEEK_SET    0   /* set file offset to offset */\n    #define SEEK_CUR    1   /* set file offset to current plus offset */\n    #define SEEK_END    2   /* set file offset to EOF plus offset */\n    logger.debug('fseek hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n    arg2 = pstate.get_argument_value(2)\n\n    if arg2 not in [0, 1, 2]:\n        return pstate.minus_one\n        # TODO: set errno to: -22 # EINVAL\n\n    if pstate.file_descriptor_exists(arg0):\n        desc = pstate.get_file_descriptor(arg0)\n\n        if desc.fd.seekable():\n            r = desc.fd.seek(arg1, arg2)\n            return r\n        else:\n            return -1\n            # TODO: set errno to: 29 # ESPIPE\n    else:\n        return -1", "\n\ndef rtn_ftell(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The ftell behavior.\n    \"\"\"\n\n    logger.debug('ftell hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n\n    if pstate.file_descriptor_exists(arg0):\n        desc = pstate.get_file_descriptor(arg0)\n\n        if desc.fd.seekable():\n            return desc.fd.tell()\n        else:\n            return -1", "            # TODO: set errno -22 EINVAL\n\n\n# char *fgets(char *s, int size, FILE *stream);\ndef rtn_fgets(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fgets behavior.\n    \"\"\"\n    logger.debug('fgets hooked')\n\n    # Get arguments\n    buff, buff_ast = pstate.get_full_argument(0)\n    size, size_ast = pstate.get_full_argument(1)\n    fd = pstate.get_argument_value(2)\n    # We use fd as concret value\n    pstate.concretize_argument(2)\n\n    if pstate.file_descriptor_exists(fd):\n        filedesc = pstate.get_file_descriptor(fd)\n        offset = filedesc.offset\n        data = filedesc.fgets(size)\n        data_no_trail = data[:-1]  # not \\x00 terminated\n\n        if filedesc.is_input_fd(): # Reading into input\n            # if we started from empty seed simulate reading `size` amount of data\n            if se.seed.is_raw() and se.seed.is_bootstrap_seed() and not data_no_trail:\n                data = b'\\x00' * size\n\n            if len(data_no_trail) == size:  # if `size` limited the fgets its an indirect constraint\n                pstate.push_constraint(size_ast.getAst() == size)\n\n            se.inject_symbolic_file_memory(buff, filedesc.name, data, offset)\n            logger.debug(f\"fgets() in {filedesc.name} = {repr(data)}\")\n        else:\n            pstate.concretize_argument(1)\n            pstate.memory.write(buff, data)\n\n        return buff_ast if data_no_trail else NULL_PTR\n    else:\n        logger.warning(f'File descriptor ({fd}) not found')\n        return NULL_PTR", "\n\n# fopen(const char *pathname, const char *mode);\ndef rtn_fopen(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fopen behavior.\n    \"\"\"\n    logger.debug('fopen hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # const char *pathname\n    arg1 = pstate.get_argument_value(1)  # const char *mode\n    arg0s = pstate.memory.read_string(arg0)\n    arg1s = pstate.memory.read_string(arg1)\n\n    # Concretize the whole path name\n    pstate.concretize_memory_bytes(arg0, len(arg0s)+1)  # Concretize the whole string + \\0\n\n    # We use mode as concrete value\n    pstate.concretize_argument(1)\n\n    if se.seed.is_file_defined(arg0s):\n        logger.info(f\"opening an input file: {arg0s}\")\n        # Program is opening an input\n        data = se.seed.get_file_input(arg0s)\n        filedesc = pstate.create_file_descriptor(arg0s, io.BytesIO(data))\n        return filedesc.id\n    else:\n        # Try to open it as a regular file\n        try:\n            fd = open(arg0s, arg1s)\n            filedesc = pstate.create_file_descriptor(arg0s, fd)\n            return filedesc.id\n        except Exception as e:\n            logger.debug(f\"Failed to open {arg0s} {e}\")\n            return NULL_PTR", "\n\ndef rtn_fprintf(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fprintf behavior.\n    \"\"\"\n    logger.debug('fprintf hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n\n    # FIXME: ARM64\n    # FIXME: pushPathConstraint\n\n    arg1f = pstate.get_format_string(arg1)\n    nbArgs = arg1f.count(\"{\")\n    args = pstate.get_format_arguments(arg1, [pstate.get_argument_value(x) for x in range(2, nbArgs+2)])\n    try:\n        s = arg1f.format(*args)\n    except:\n        # FIXME: Les chars UTF8 peuvent foutre le bordel. Voir avec ground-truth/07.input\n        logger.warning('Something wrong, probably UTF-8 string')\n        s = \"\"\n\n    if pstate.file_descriptor_exists(arg0):\n        fdesc = pstate.get_file_descriptor(arg0)\n        if arg0 not in [1, 2] or (arg0 == 1 and se.config.pipe_stdout) or (arg0 == 2 and se.config.pipe_stderr):\n            fdesc.fd.write(s)\n            fdesc.fd.flush()\n    else:\n        return 0\n\n    # Return value\n    return len(s)", "\n\ndef rtn___fprintf_chk(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __fprintf_chk behavior.\n    \"\"\"\n    logger.debug('__fprintf_chk hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    flag = pstate.get_argument_value(1)\n    arg1 = pstate.get_argument_value(2)\n\n    # FIXME: ARM64\n    # FIXME: pushPathConstraint\n\n    arg1f = pstate.get_format_string(arg1)\n    nbArgs = arg1f.count(\"{\")\n    args = pstate.get_format_arguments(arg1, [pstate.get_argument_value(x) for x in range(3, nbArgs+2)])\n    try:\n        s = arg1f.format(*args)\n    except:\n        # FIXME: Les chars UTF8 peuvent foutre le bordel. Voir avec ground-truth/07.input\n        logger.warning('Something wrong, probably UTF-8 string')\n        s = \"\"\n\n    if pstate.file_descriptor_exists(arg0):\n        fdesc = pstate.get_file_descriptor(arg0)\n        if arg0 not in [1, 2] or (arg0 == 1 and se.config.pipe_stdout) or (arg0 == 2 and se.config.pipe_stderr):\n            fdesc.fd.write(s)\n            fdesc.fd.flush()\n    else:\n        return 0\n\n    # Return value\n    return len(s)", "\n\n# fputc(int c, FILE *stream);\ndef rtn_fputc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fputc behavior.\n    \"\"\"\n    logger.debug('fputc hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n\n    pstate.concretize_argument(0)\n    pstate.concretize_argument(1)\n\n    if pstate.file_descriptor_exists(arg1):\n        fdesc = pstate.get_file_descriptor(arg1)\n        if arg1 == 0:\n            return 0\n        elif (arg1 == 1 and se.config.pipe_stdout) or (arg1 == 2 and se.config.pipe_stderr):\n            fdesc.fd.write(chr(arg0))\n            fdesc.fd.flush()\n        elif arg1 not in [0, 2]:\n            fdesc.fd.write(chr(arg0))\n        return 1\n    else:\n        return 0", "\n\n\ndef rtn_fputs(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fputs behavior.\n    \"\"\"\n    logger.debug('fputs hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n\n    pstate.concretize_argument(0)\n    pstate.concretize_argument(1)\n\n    s = pstate.memory.read_string(arg0)\n\n    # FIXME: What if the fd is coming from the memory (fmemopen) ?\n\n    if pstate.file_descriptor_exists(arg1):\n        fdesc = pstate.get_file_descriptor(arg1)\n        if arg1 == 0:\n            return 0\n        elif arg1 == 1:\n            if se.config.pipe_stdout:\n                fdesc.fd.write(s)\n                fdesc.fd.flush()\n        elif arg1 == 2:\n            if se.config.pipe_stderr:\n                fdesc.fd.write(s)\n                fdesc.fd.flush()\n        else:\n            fdesc.fd.write(s)\n    else:\n        return 0\n\n    # Return value\n    return len(s)", "\n\ndef rtn_fread(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fread behavior.\n    \"\"\"\n    logger.debug('fread hooked')\n\n    # Get arguments\n    ptr = pstate.get_argument_value(0) # ptr\n    size_t, size_ast = pstate.get_full_argument(1) # size\n    nmemb = pstate.get_argument_value(2) # nmemb\n    fd = pstate.get_argument_value(3) # stream\n    size = size_t * nmemb\n\n    # FIXME: pushPathConstraint\n\n    if pstate.file_descriptor_exists(fd):\n        filedesc = pstate.get_file_descriptor(fd)\n        offset = filedesc.offset\n        data = filedesc.read(size)\n\n        if filedesc.is_input_fd(): # Reading into input\n            # if we started from empty seed simulate reading `size` amount of data\n            if se.seed.is_raw() and se.seed.is_bootstrap_seed() and not data:\n                data = b'\\x00' * size\n\n            if len(data) == size:  # if `size` limited the fgets its an indirect constraint\n                pstate.push_constraint(size_ast.getAst() == size)\n\n            se.inject_symbolic_file_memory(ptr, filedesc.name, data, offset)\n            logger.debug(f\"read in {filedesc.name} = {repr(data)}\")\n        else:\n            pstate.concretize_argument(2)\n            pstate.memory.write(ptr, data)\n\n        return int(len(data)/size_t) if size_t else 0  # number of items read\n    else:\n        logger.warning(f'File descriptor ({fd}) not found')\n        return 0", "\n\ndef rtn_free(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The free behavior.\n    \"\"\"\n    logger.debug('free hooked')\n\n    # Get arguments\n    ptr = pstate.get_argument_value(0)\n    if ptr == 0: # free(NULL) is a nop\n        return None\n    pstate.heap_allocator.free(ptr)\n\n    return None", "\n\ndef rtn_fwrite(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The fwrite behavior.\n    \"\"\"\n    logger.debug('fwrite hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n    arg2 = pstate.get_argument_value(2)\n    arg3 = pstate.get_argument_value(3)\n    size = arg1 * arg2\n    data = pstate.memory.read(arg0, size)\n\n    if pstate.file_descriptor_exists(arg3):\n        fdesc = pstate.get_file_descriptor(arg3)\n        if arg3 == 0:\n            return 0\n        elif arg3 == 1:\n            if se.config.pipe_stdout:\n                fdesc.fd.buffer.write(data)\n                fdesc.fd.flush()\n        elif arg3 == 2:\n            if se.config.pipe_stderr:\n                fdesc.fd.buffer.write(data)\n                fdesc.fd.flush()\n        else:\n            fdesc.fd.write(data)\n    else:\n        return 0\n\n    # Return value\n    return size", "\n\ndef rtn_write(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The write behavior.\n    \"\"\"\n    logger.debug('write hooked')\n\n    # Get arguments\n    fd = pstate.get_argument_value(0)\n    buf = pstate.get_argument_value(1)\n    size = pstate.get_argument_value(2)\n    data = pstate.memory.read(buf, size)\n\n    if pstate.file_descriptor_exists(fd):\n        fdesc = pstate.get_file_descriptor(fd)\n        if fd == 0:\n            return 0\n        elif fd == 1:\n            if se.config.pipe_stdout:\n                fdesc.fd.buffer.write(data)\n                fdesc.fd.flush()\n        elif fd == 2:\n            if se.config.pipe_stderr:\n                fdesc.fd.buffer.write(data)\n                fdesc.fd.flush()\n        else:\n            fdesc.fd.write(data)\n    else:\n        return 0\n\n    # Return value\n    return size", "\n\ndef rtn_gettimeofday(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The gettimeofday behavior.\n    \"\"\"\n    logger.debug('gettimeofday hooked')\n\n    # Get arguments\n    tv = pstate.get_argument_value(0)\n    tz = pstate.get_argument_value(1)\n\n    if tv == 0:\n        return pstate.minus_one\n\n    if pstate.time_inc_coefficient:\n        t = pstate.time\n    else:\n        t = time.time()\n\n    s = pstate.ptr_size\n    pstate.memory.write_ptr(tv, int(t))\n    pstate.memory.write_ptr(tv+pstate.ptr_size, int(t * 1000000))\n\n    # Return value\n    return 0", "\n\ndef rtn_malloc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The malloc behavior.\n    \"\"\"\n    logger.debug('malloc hooked')\n\n    # Get arguments\n    size = pstate.get_argument_value(0)\n    ptr = pstate.heap_allocator.alloc(size)\n\n    # Return value\n    return ptr", "\n\ndef rtn_open(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The open behavior.\n    \"\"\"\n    logger.debug('open hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # const char *pathname\n    flags = pstate.get_argument_value(1)  # int flags\n    mode = pstate.get_argument_value(2)  # we ignore it\n    arg0s = pstate.memory.read_string(arg0)\n\n    # Concretize the whole path name\n    pstate.concretize_memory_bytes(arg0, len(arg0s)+1)  # Concretize the whole string + \\0\n\n    # We use flags as concrete value\n    pstate.concretize_argument(1)\n\n    # Use the flags to open the file in the write mode.\n    mode = \"\"\n    if (flags & 0xFF) == 0x00:   # O_RDONLY\n        mode = \"r\"\n    elif (flags & 0xFF) == 0x01: # O_WRONLY\n        mode = \"w\"\n    elif (flags & 0xFF) == 0x02: # O_RDWR\n        mode = \"r+\"\n\n    if flags & 0x0100: # O_CREAT\n        mode += \"x\"\n    if flags & 0x0200: # O_APPEND\n        mode = \"a\"  # replace completely value\n\n    # enforce using binary mode for open\n    mode += \"b\"\n\n    if se.seed.is_file_defined(arg0s) and \"r\" in mode:  # input file and opened in reading\n        logger.info(f\"opening an input file: {arg0s}\")\n        # Program is opening an input\n        data = se.seed.get_file_input(arg0s)\n        filedesc = pstate.create_file_descriptor(arg0s, io.BytesIO(data))\n        return filedesc.id\n    else:\n        # Try to open it as a regular file\n        try:\n            fd = open(arg0s, mode)  # use the mode here\n            filedesc = pstate.create_file_descriptor(arg0s, fd)\n            return filedesc.id\n        except Exception as e:\n            logger.debug(f\"Failed to open {arg0s} {e}\")\n            return pstate.minus_one", "\ndef rtn_realloc(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The realloc behavior.\n    \"\"\"\n    logger.debug('realloc hooked')\n\n    # Get arguments\n    oldptr = pstate.get_argument_value(0)\n    size = pstate.get_argument_value(1)\n\n    if oldptr == 0:\n        # malloc behaviour\n        ptr = pstate.heap_allocator.alloc(size)\n        return ptr\n\n    ptr = pstate.heap_allocator.alloc(size)\n    if ptr == 0: return ptr\n\n    if ptr not in pstate.heap_allocator.alloc_pool:\n        logger.warning(\"Invalid ptr passed to realloc\")\n        pstate.heap_allocator.free(ptr) # This will raise an error\n\n    old_memmap = pstate.heap_allocator.alloc_pool[oldptr]\n    old_size = old_memmap.size\n    size_to_copy = min(size, old_size)\n\n    #data = pstate.memory.read(oldptr, size_to_copy)\n    #pstate.memory.write(ptr, data)\n\n    # Copy bytes symbolically\n    for index in range(size_to_copy):\n        sym_c = pstate.read_symbolic_memory_byte(oldptr+index)\n        pstate.write_symbolic_memory_byte(ptr+index, sym_c)\n\n    pstate.heap_allocator.free(oldptr)\n\n    return ptr", "\n\ndef rtn_memcmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The memcmp behavior.\n    \"\"\"\n    logger.debug('memcmp hooked')\n\n    s1 = pstate.get_argument_value(0)\n    s2 = pstate.get_argument_value(1)\n    size = pstate.get_argument_value(2)\n\n    ptr_bit_size = pstate.ptr_bit_size\n\n    ast = pstate.actx\n    res = ast.bv(0, ptr_bit_size)\n\n    # We constrain the logical value of size\n    pstate.concretize_argument(2)\n\n    for index in range(size):\n        cells1 = pstate.read_symbolic_memory_byte(s1+index).getAst()\n        cells2 = pstate.read_symbolic_memory_byte(s2+index).getAst()\n        res = res + ast.ite(\n                        cells1 == cells2,\n                        ast.bv(0, ptr_bit_size),\n                        ast.bv(1, ptr_bit_size)\n                    )\n\n    return res", "\n\ndef rtn_memcpy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The memcpy behavior.\n    \"\"\"\n    logger.debug('memcpy hooked')\n\n    dst, dst_ast = pstate.get_full_argument(0)\n    src = pstate.get_argument_value(1)\n    cnt = pstate.get_argument_value(2)\n\n    # We constrain the logical value of size\n    pstate.concretize_argument(2)\n\n    for index in range(cnt):\n        # Read symbolic src value and copy symbolically in dst\n        sym_src = pstate.read_symbolic_memory_byte(src+index)\n        pstate.write_symbolic_memory_byte(dst+index, sym_src)\n\n    return dst_ast", "\n\ndef rtn_mempcpy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The mempcpy behavior.\n    \"\"\"\n    logger.debug('mempcpy hooked')\n\n    dst, dst_ast = pstate.get_full_argument(0)\n    src = pstate.get_argument_value(1)\n    cnt = pstate.get_argument_value(2)\n\n    # We constrain the logical value of size\n    pstate.concretize_argument(2)\n\n    for index in range(cnt):\n        # Read symbolic src value and copy symbolically in dst\n        sym_src = pstate.read_symbolic_memory_byte(src+index)\n        pstate.write_symbolic_memory_byte(dst+index, sym_src)\n\n    return dst + cnt", "\n\ndef rtn_memmem(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The memmem behavior.\n    \"\"\"\n    logger.debug('memmem hooked')\n\n    haystack    = pstate.get_argument_value(0)  # const void*\n    haystacklen = pstate.get_argument_value(1)  # size_t\n    needle      = pstate.get_argument_value(2)  # const void *\n    needlelen   = pstate.get_argument_value(3)  # size_t\n\n    s1 = pstate.memory.read(haystack, haystacklen)  # haystack\n    s2 = pstate.memory.read(needle, needlelen)      # needle\n\n    offset = s1.find(s2)\n    if offset == -1:\n        #FIXME: faut s'assurer que le marquer dans le string\n        return NULL_PTR\n\n    for i, c in enumerate(s2):\n        c1 = pstate.read_symbolic_memory_byte(haystack+offset+i)\n        c2 = pstate.read_symbolic_memory_byte(needle+i)\n        pstate.push_constraint(c1.getAst() == c2.getAst())\n\n    # FIXME: \u00e0 reflechir si on doit contraindre offset ou pas\n\n    # faut s'assurer que le marquer est bien pr\u00e9sent \u00e0 l'offset trouv\u00e9\n    return haystack + offset", "\n\ndef rtn_memmove(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The memmove behavior.\n    \"\"\"\n    logger.debug('memmove hooked')\n\n    dst, dst_ast = pstate.get_full_argument(0)\n    src = pstate.get_argument_value(1)\n    cnt = pstate.get_argument_value(2)\n\n    # We constrain the logical value of cnt\n    pstate.concretize_argument(2)\n\n    # TODO: What if cnt is symbolic ?\n    for index in range(cnt):\n        # Read symbolic src value and copy symbolically in dst\n        sym_src = pstate.read_symbolic_memory_byte(src+index)\n        pstate.write_symbolic_memory_byte(dst+index, sym_src)\n\n    return dst_ast", "\n\ndef rtn_memset(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The memset behavior.\n    \"\"\"\n    logger.debug('memset hooked')\n\n    dst, dst_ast = pstate.get_full_argument(0)\n    src, src_ast = pstate.get_full_argument(1)\n    size = pstate.get_argument_value(2)\n\n    # We constrain the logical value of size\n    pstate.concretize_argument(2)\n\n    sym_cell = pstate.actx.extract(7, 0, src_ast.getAst())\n\n    # TODO: What if size is symbolic ?\n    for index in range(size):\n        pstate.write_symbolic_memory_byte(dst+index, sym_cell)\n\n    return dst_ast", "\n\ndef rtn_printf(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The printf behavior.\n    \"\"\"\n    logger.debug('printf hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n\n    arg0f = pstate.get_format_string(arg0)\n    nbArgs = arg0f.count(\"{\")\n    args = pstate.get_format_arguments(arg0, [pstate.get_argument_value(x) for x in range(1, nbArgs+1)])\n    try:\n        s = arg0f.format(*args)\n    except:\n        # FIXME: Les chars UTF8 peuvent foutre le bordel. Voir avec ground-truth/07.input\n        logger.warning('Something wrong, probably UTF-8 string')\n        s = \"\"\n\n    if se.config.pipe_stdout:\n        stdout = pstate.get_file_descriptor(1)\n        stdout.fd.write(s)\n        stdout.fd.flush()\n\n    # Return value\n    return len(s)", "\n\ndef rtn_pthread_create(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_create behavior.\n    \"\"\"\n    logger.debug('pthread_create hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0) # pthread_t *thread\n    arg1 = pstate.get_argument_value(1) # const pthread_attr_t *attr\n    arg2 = pstate.get_argument_value(2) # void *(*start_routine) (void *)\n    arg3 = pstate.get_argument_value(3) # void *arg\n\n    th = pstate.spawn_new_thread(arg2, arg3)\n\n    # Save out the thread id\n    pstate.memory.write_ptr(arg0, th.tid)\n\n    # Return value\n    return 0", "\n\ndef rtn_pthread_exit(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_exit behavior.\n    \"\"\"\n    logger.debug('pthread_exit hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n\n    # Kill the thread\n    pstate.current_thread.kill()\n\n    # FIXME: I guess the thread exiting never return, so should not continue\n    # FIXME: iterating instructions\n\n    # Return value\n    return None", "\n\ndef rtn_pthread_join(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_join behavior.\n    \"\"\"\n    logger.debug('pthread_join hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)\n    arg1 = pstate.get_argument_value(1)\n\n    if arg0 in pstate.threads:\n        pstate.current_thread.join_thread(arg0)\n        logger.info(f\"Thread id {pstate.current_thread.tid} joined thread id {arg0}\")\n    else:\n        pstate.current_thread.cancel_join()\n        logger.debug(f\"Thread id {arg0} already destroyed\")\n\n    # Return value\n    return 0", "\n\ndef rtn_pthread_mutex_destroy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_mutex_destroy behavior.\n    \"\"\"\n    logger.debug('pthread_mutex_destroy hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # pthread_mutex_t *restrict mutex\n    pstate.memory.write_ptr(arg0, pstate.PTHREAD_MUTEX_INIT_MAGIC)\n\n    return 0", "\n\ndef rtn_pthread_mutex_init(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_mutex_init behavior.\n    \"\"\"\n    logger.debug('pthread_mutex_init hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # pthread_mutex_t *restrict mutex\n    arg1 = pstate.get_argument_value(1)  # const pthread_mutexattr_t *restrict attr)\n\n    pstate.memory.write_ptr(arg0, pstate.PTHREAD_MUTEX_INIT_MAGIC)\n\n    return 0", "\n\ndef rtn_pthread_mutex_lock(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_mutex_lock behavior.\n    \"\"\"\n    logger.debug('pthread_mutex_lock hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # pthread_mutex_t *mutex\n    mutex = pstate.memory.read_ptr(arg0)  # deref pointer and read a uint64 int\n\n    # If the thread has been initialized and unused, define the tid has lock\n    if mutex == pstate.PTHREAD_MUTEX_INIT_MAGIC:\n        logger.debug('mutex unlocked')\n        pstate.memory.write_ptr(arg0, pstate.current_thread.tid)\n\n    # The mutex is locked and we are not allowed to continue the execution\n    elif mutex != pstate.current_thread.tid:\n        logger.debug('mutex locked')\n        pstate.mutex_locked = True\n\n    # Return value\n    return 0", "\n\ndef rtn_pthread_mutex_unlock(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The pthread_mutex_unlock behavior.\n    \"\"\"\n    logger.debug('pthread_mutex_unlock hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # pthread_mutex_t *mutex\n\n    pstate.memory.write_ptr(arg0, pstate.PTHREAD_MUTEX_INIT_MAGIC)\n\n    # Return value\n    return 0", "\n\ndef rtn_puts(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The puts behavior.\n    \"\"\"\n    logger.debug('puts hooked')\n\n    arg0 = pstate.get_string_argument(0)\n\n    # Get arguments\n    if se.config.pipe_stdout:  # Only perform printing if pipe_stdout activated\n        sys.stdout.write(arg0 + '\\n')\n        sys.stdout.flush()\n\n    # Return value\n    return len(arg0) + 1", "\n\ndef rtn_rand(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The rand behavior.\n    \"\"\"\n    logger.debug('rand hooked')\n    return random.randrange(0, 0xffffffff)\n\n\ndef rtn_read(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The read behavior.\n    \"\"\"\n    logger.debug('read hooked')\n\n    # Get arguments\n    fd   = pstate.get_argument_value(0)\n    buff = pstate.get_argument_value(1)\n    size, size_ast = pstate.get_full_argument(2)\n\n    if size_ast.isSymbolized():\n        logger.warning(f'Reading from the file descriptor ({fd}) with a symbolic size')\n\n    pstate.concretize_argument(0)\n\n    if pstate.file_descriptor_exists(fd):\n        filedesc = pstate.get_file_descriptor(fd)\n        offset = filedesc.offset\n        data = filedesc.read(size)\n\n        if filedesc.is_input_fd(): # Reading into input\n            # if we started from empty seed simulate reading `size` amount of data\n            if se.seed.is_raw() and se.seed.is_bootstrap_seed() and not data:\n                data = b'\\x00' * size\n\n            if len(data) == size:  # if `size` limited the fgets its an indirect constraint\n                pstate.push_constraint(size_ast.getAst() == size)\n\n            se.inject_symbolic_file_memory(buff, filedesc.name, data, offset)\n            logger.debug(f\"read in (input) {filedesc.name} = {repr(data)}\")\n        else:\n            pstate.concretize_argument(2)\n            pstate.memory.write(buff, data)\n\n        return len(data)\n    else:\n        logger.warning(f'File descriptor ({fd}) not found')\n        return pstate.minus_one  # todo: set errno", "\n\ndef rtn_read(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The read behavior.\n    \"\"\"\n    logger.debug('read hooked')\n\n    # Get arguments\n    fd   = pstate.get_argument_value(0)\n    buff = pstate.get_argument_value(1)\n    size, size_ast = pstate.get_full_argument(2)\n\n    if size_ast.isSymbolized():\n        logger.warning(f'Reading from the file descriptor ({fd}) with a symbolic size')\n\n    pstate.concretize_argument(0)\n\n    if pstate.file_descriptor_exists(fd):\n        filedesc = pstate.get_file_descriptor(fd)\n        offset = filedesc.offset\n        data = filedesc.read(size)\n\n        if filedesc.is_input_fd(): # Reading into input\n            # if we started from empty seed simulate reading `size` amount of data\n            if se.seed.is_raw() and se.seed.is_bootstrap_seed() and not data:\n                data = b'\\x00' * size\n\n            if len(data) == size:  # if `size` limited the fgets its an indirect constraint\n                pstate.push_constraint(size_ast.getAst() == size)\n\n            se.inject_symbolic_file_memory(buff, filedesc.name, data, offset)\n            logger.debug(f\"read in (input) {filedesc.name} = {repr(data)}\")\n        else:\n            pstate.concretize_argument(2)\n            pstate.memory.write(buff, data)\n\n        return len(data)\n    else:\n        logger.warning(f'File descriptor ({fd}) not found')\n        return pstate.minus_one  # todo: set errno", "\n\ndef rtn_getchar(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The getchar behavior.\n    \"\"\"\n    logger.debug('getchar hooked')\n\n    # Get arguments\n    filedesc = pstate.get_file_descriptor(0) # stdin\n    offset = filedesc.offset\n\n    data = filedesc.read(1)\n    if data:\n        if filedesc.is_input_fd():  # Reading into input\n            data = ord(data) # convert to integer\n            se.inject_symbolic_file_register(pstate.return_register, filedesc.name, data, offset)\n            logger.debug(f\"read in {filedesc.name} = {repr(data)}\")\n        return data\n    else:\n        return pstate.minus_one", "\n\ndef rtn_sem_destroy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_destroy behavior.\n    \"\"\"\n    logger.debug('sem_destroy hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n\n    # Destroy the semaphore with the value\n    pstate.memory.write_ptr(arg0, 0)\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_getvalue(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_getvalue behavior.\n    \"\"\"\n    logger.debug('sem_getvalue hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n    arg1 = pstate.get_argument_value(1)  # int *sval\n\n    value = pstate.memory.read_ptr(arg0)  # deref pointer\n\n    # Set the semaphore's value into the output\n    pstate.memory.write_dword(arg1, value)  # WARNING: read uint64 to uint32\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_init(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_init behavior.\n    \"\"\"\n    logger.debug('sem_init hooked')\n\n    # Get arguments\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n    arg1 = pstate.get_argument_value(1)  # int pshared\n    arg2 = pstate.get_argument_value(2)  # unsigned int value\n\n    # Init the semaphore with the value\n    pstate.memory.write_ptr(arg0, arg2)\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_post(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_post behavior.\n    \"\"\"\n    logger.debug('sem_post hooked')\n\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n\n    # increments (unlocks) the semaphore pointed to by sem\n    value = pstate.memory.read_ptr(arg0)\n    pstate.memory.write_ptr(arg0, value + 1)\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_timedwait(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_timedwait behavior.\n    \"\"\"\n    logger.debug('sem_timedwait hooked')\n\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n    arg1 = pstate.get_argument_value(1)  # const struct timespec *abs_timeout\n\n    # sem_timedwait() is the same as sem_wait(), except that abs_timeout specifies a limit\n    # on the amount of time that the call should block if the decrement cannot be immediately\n    # performed. The abs_timeout argument points to a structure that specifies an absolute\n    # timeout in seconds and nanoseconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).\n    # This structure is defined as follows:\n    #\n    #     struct timespec {\n    #         time_t tv_sec;      /* Seconds */\n    #         long   tv_nsec;     /* Nanoseconds [0 .. 999999999] */\n    #     };\n    #\n    # If the timeout has already expired by the time of the call, and the semaphore could not be\n    # locked immediately, then sem_timedwait() fails with a timeout error (errno set to ETIMEDOUT).\n    #\n    # If  the operation can be performed immediately, then sem_timedwait() never fails with a\n    # timeout error, regardless of the value of abs_timeout.  Furthermore, the validity of\n    # abs_timeout is not checked in this case.\n\n    # TODO: Take into account the abs_timeout argument\n    value = pstate.memory.read_ptr(arg0)\n    if value > 0:\n        logger.debug('semaphore still not locked')\n        pstate.memory.write_ptr(arg0, value - 1)\n        pstate.semaphore_locked = False\n    else:\n        logger.debug('semaphore locked')\n        pstate.semaphore_locked = True\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_trywait(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_trywait behavior.\n    \"\"\"\n    logger.debug('sem_trywait hooked')\n\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n\n    # sem_trywait()  is  the  same as sem_wait(), except that if the decrement\n    # cannot be immediately performed, then call returns an error (errno set to\n    # EAGAIN) instead of blocking.\n    value = pstate.memory.read_ptr(arg0)\n    if value > 0:\n        logger.debug('semaphore still not locked')\n        pstate.memory.write_ptr(arg0, value - 1)\n        pstate.semaphore_locked = False\n    else:\n        logger.debug('semaphore locked but continue')\n        pstate.semaphore_locked = False\n\n        # Setting errno to EAGAIN (3406)\n        segs = pstate.memory.find_map(pstate.EXTERN_SEG)\n        if segs:\n            map = segs[0]\n            ERRNO = map.start + map.size - 4  # Point is last int of the mapping\n            pstate.memory.write_dword(ERRNO, 3406)\n        else:\n            assert False\n\n        # Return -1\n        return pstate.minus_one\n\n    # Return success\n    return 0", "\n\ndef rtn_sem_wait(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sem_wait behavior.\n    \"\"\"\n    logger.debug('sem_wait hooked')\n\n    arg0 = pstate.get_argument_value(0)  # sem_t *sem\n\n    # decrements (locks) the semaphore pointed to by sem. If the semaphore's value\n    # is greater than zero, then the decrement proceeds, and the function returns,\n    # immediately. If the semaphore currently has the value zero, then the call blocks\n    # until either it becomes possible to perform the decrement (i.e., the semaphore\n    # value rises above zero).\n    value = pstate.memory.read_ptr(arg0)\n    if value > 0:\n        logger.debug('semaphore still not locked')\n        pstate.memory.write_ptr(arg0, value - 1)\n        pstate.semaphore_locked = False\n    else:\n        logger.debug('semaphore locked')\n        pstate.semaphore_locked = True\n\n    # Return success\n    return 0", "\n\ndef rtn_sleep(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sleep behavior.\n    \"\"\"\n    logger.debug('sleep hooked')\n\n    # Get arguments\n    if not se.config.skip_sleep_routine:\n        t = pstate.get_argument_value(0)\n        time.sleep(t)\n\n    # Return value\n    return 0", "\n\ndef rtn_sprintf(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The sprintf behavior.\n    \"\"\"\n    logger.debug('sprintf hooked')\n\n    # Get arguments\n    buff = pstate.get_argument_value(0)\n    arg0 = pstate.get_argument_value(1)\n\n    try:\n        arg0f = pstate.get_format_string(arg0)\n        nbArgs = arg0f.count(\"{\")\n        args = pstate.get_format_arguments(arg0, [pstate.get_argument_value(x) for x in range(2, nbArgs+2)])\n        s = arg0f.format(*args)\n    except:\n        # FIXME: Les chars UTF8 peuvent foutre le bordel. Voir avec ground-truth/07.input\n        logger.warning('Something wrong, probably UTF-8 string')\n        s = \"\"\n\n    # FIXME: todo\n\n    # FIXME: THIS SEEMS NOT OK\n    # for index, c in enumerate(s):\n    #     pstate.tt_ctx.concretizeMemory(buff + index)\n    #     pstate.tt_ctx.setConcreteMemoryValue(buff + index, ord(c))\n    #     pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(buff + index, 1)) == ord(c))\n    #\n    # # including the terminating null byte ('\\0')\n    # pstate.tt_ctx.setConcreteMemoryValue(buff + len(s), 0x00)\n    # pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(buff + len(s), 1)) == 0x00)\n\n    return len(s)", "\n\ndef rtn_strcasecmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strcasecmp behavior.\n    \"\"\"\n    logger.debug('strcasecmp hooked')\n\n    s1 = pstate.get_argument_value(0)\n    s2 = pstate.get_argument_value(1)\n    size = min(len(pstate.memory.read_string(s1)), len(pstate.memory.read_string(s2)) + 1)\n\n    #s = s1 if len(pstate.memory.read_string(s1)) < len(pstate.memory.read_string(s2)) else s2\n    #for i in range(size):\n    #    pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s1 + i, CPUSIZE.BYTE)) != 0x00)\n    #    pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s2 + i, CPUSIZE.BYTE)) != 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s + size, CPUSIZE.BYTE)) == 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s1 + len(pstate.memory.read_string(s1)), CPUSIZE.BYTE)) == 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s2 + len(pstate.memory.read_string(s2)), CPUSIZE.BYTE)) == 0x00)\n\n    # FIXME: Il y a des truc chelou avec le +1 et le logic ci-dessous\n\n    ptr_bit_size = pstate.ptr_bit_size\n    ast = pstate.actx\n    res = ast.bv(0, pstate.ptr_bit_size)\n    for index in range(size):\n        cells1 = pstate.read_symbolic_memory_byte(s1 + index).getAst()\n        cells2 = pstate.read_symbolic_memory_byte(s2 + index).getAst()\n        cells1 = ast.ite(ast.land([cells1 >= ord('a'), cells1 <= ord('z')]), cells1 - 32, cells1) # upper case\n        cells2 = ast.ite(ast.land([cells2 >= ord('a'), cells2 <= ord('z')]), cells2 - 32, cells2) # upper case\n        res = res + ast.ite(cells1 == cells2, ast.bv(0, ptr_bit_size), ast.bv(1, ptr_bit_size))\n\n    return res", "\n\ndef rtn_strchr(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strchr behavior.\n    \"\"\"\n    logger.debug('strchr hooked')\n\n    string = pstate.get_argument_value(0)\n    char   = pstate.get_argument_value(1)\n    ast    = pstate.actx\n    ptr_bit_size = pstate.ptr_bit_size\n\n    def rec(res, deep, maxdeep):\n        if deep == maxdeep:\n            return res\n        cell = pstate.read_symbolic_memory_byte(string + deep).getAst()\n        res  = ast.ite(cell == (char & 0xff), ast.bv(string + deep, ptr_bit_size), rec(res, deep + 1, maxdeep))\n        return res\n\n    sze = len(pstate.memory.read_string(string))\n    res = rec(ast.bv(0, ptr_bit_size), 0, sze)\n\n    for i, c in enumerate(pstate.memory.read_string(string)):\n        pstate.push_constraint(pstate.read_symbolic_memory_byte(string+i).getAst() != 0x00)\n    pstate.push_constraint(pstate.read_symbolic_memory_byte(string+sze).getAst() == 0x00)\n\n    return res", "\n\ndef rtn_strcmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strcmp behavior.\n    \"\"\"\n    logger.debug('strcmp hooked')\n\n    s1 = pstate.get_argument_value(0)\n    s2 = pstate.get_argument_value(1)\n    size = min(len(pstate.memory.read_string(s1)), len(pstate.memory.read_string(s2))) + 1\n    \n    #s = s1 if len(pstate.memory.read_string(s1)) <= len(pstate.memory.read_string(s2)) else s2\n    #for i in range(size):\n    #    pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s1 + i, CPUSIZE.BYTE)) != 0x00)\n    #    pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s2 + i, CPUSIZE.BYTE)) != 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s + size, CPUSIZE.BYTE)) == 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s1 + len(pstate.memory.read_string(s1)), CPUSIZE.BYTE)) == 0x00)\n    #pstate.tt_ctx.pushPathConstraint(pstate.tt_ctx.getMemoryAst(MemoryAccess(s2 + len(pstate.memory.read_string(s2)), CPUSIZE.BYTE)) == 0x00)\n\n    # FIXME: Il y a des truc chelou avec le +1 et le logic ci-dessous\n\n    ptr_bit_size = pstate.ptr_bit_size\n    ast = pstate.actx\n    res = ast.bv(0, ptr_bit_size)\n    for index in range(size):\n        cells1 = pstate.read_symbolic_memory_byte(s1 + index).getAst()\n        cells2 = pstate.read_symbolic_memory_byte(s2 + index).getAst()\n        res = res + ast.ite(cells1 == cells2, ast.bv(0, ptr_bit_size), ast.bv(1, ptr_bit_size))\n\n    return res", "\n\ndef rtn_strcpy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strcpy behavior.\n    \"\"\"\n    logger.debug('strcpy hooked')\n\n    dst  = pstate.get_argument_value(0)\n    src  = pstate.get_argument_value(1)\n    src_str = pstate.memory.read_string(src)\n    size = len(src_str)\n\n    # constraint src buff to be != \\00 and last one to be \\00 (indirectly concretize length)\n    for i, c in enumerate(src_str):\n        pstate.push_constraint(pstate.read_symbolic_memory_byte(src + i).getAst() != 0x00)\n    pstate.push_constraint(pstate.read_symbolic_memory_byte(src + size).getAst() == 0x00)\n\n    # Copy symbolically bytes (including \\00)\n    for index in range(size+1):\n        sym_c = pstate.read_symbolic_memory_byte(src+index)\n        pstate.write_symbolic_memory_byte(dst+index, sym_c)\n\n    return dst", "\n\ndef rtn_strdup(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strdup behavior.\n    \"\"\"\n    logger.debug('strdup hooked')\n\n    s  = pstate.get_argument_value(0)\n    s_str = pstate.memory.read_string(s)\n    size = len(s_str)\n\n    #print(f\"strdup s={s:#x} s_str={s_str} size={size}\")\n\n    # constrain src buff to be != \\00 and last one to be \\00 (indirectly concretize length)\n    for i, c in enumerate(s_str):\n        pstate.push_constraint(pstate.read_symbolic_memory_byte(s + i).getAst() != 0x00)\n    pstate.push_constraint(pstate.read_symbolic_memory_byte(s + size).getAst() == 0x00)\n\n\n    # Malloc a chunk\n    ptr = pstate.heap_allocator.alloc(size + 1)\n\n    # Copy symbolically bytes (including \\00)\n    for index in range(size+1):\n        sym_c = pstate.read_symbolic_memory_byte(s+index)\n        pstate.write_symbolic_memory_byte(ptr+index, sym_c)\n\n    return ptr", "\n\ndef rtn_strerror(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strerror behavior.\n\n    :param se: The current symbolic execution instance\n    :param pstate: The current process state\n    :return: a concrete value\n    \"\"\"\n    logger.debug('strerror hooked')\n\n    sys_errlist = [\n        b\"Success\",\n        b\"Operation not permitted\",\n        b\"No such file or directory\",\n        b\"No such process\",\n        b\"Interrupted system call\",\n        b\"Input/output error\",\n        b\"No such device or address\",\n        b\"Argument list too long\",\n        b\"Exec format error\",\n        b\"Bad file descriptor\",\n        b\"No child processes\",\n        b\"Resource temporarily unavailable\",\n        b\"Cannot allocate memory\",\n        b\"Permission denied\",\n        b\"Bad address\",\n        b\"Block device required\",\n        b\"Device or resource busy\",\n        b\"File exists\",\n        b\"Invalid cross-device link\",\n        b\"No such device\",\n        b\"Not a directory\",\n        b\"Is a directory\",\n        b\"Invalid argument\",\n        b\"Too many open files in system\",\n        b\"Too many open files\",\n        b\"Inappropriate ioctl for device\",\n        b\"Text file busy\",\n        b\"File too large\",\n        b\"No space left on device\",\n        b\"Illegal seek\",\n        b\"Read-only file system\",\n        b\"Too many links\",\n        b\"Broken pipe\",\n        b\"Numerical argument out of domain\",\n        b\"Numerical result out of range\",\n        b\"Resource deadlock avoided\",\n        b\"File name too long\",\n        b\"No locks available\",\n        b\"Function not implemented\",\n        b\"Directory not empty\",\n        b\"Too many levels of symbolic links\",\n        None,\n        b\"No message of desired type\",\n        b\"Identifier removed\",\n        b\"Channel number out of range\",\n        b\"Level 2 not synchronized\",\n        b\"Level 3 halted\",\n        b\"Level 3 reset\",\n        b\"Link number out of range\",\n        b\"Protocol driver not attached\",\n        b\"No CSI structure available\",\n        b\"Level 2 halted\",\n        b\"Invalid exchange\",\n        b\"Invalid request descriptor\",\n        b\"Exchange full\",\n        b\"No anode\",\n        b\"Invalid request code\",\n        b\"Invalid slot\",\n        None,\n        b\"Bad font file format\",\n        b\"Device not a stream\",\n        b\"No data available\",\n        b\"Timer expired\",\n        b\"Out of streams resources\",\n        b\"Machine is not on the network\",\n        b\"Package not installed\",\n        b\"Object is remote\",\n        b\"Link has been severed\",\n        b\"Advertise error\",\n        b\"Srmount error\",\n        b\"Communication error on send\",\n        b\"Protocol error\",\n        b\"Multihop attempted\",\n        b\"RFS specific error\",\n        b\"Bad message\",\n        b\"Value too large for defined data type\",\n        b\"Name not unique on network\",\n        b\"File descriptor in bad state\",\n        b\"Remote address changed\",\n        b\"Can not access a needed shared library\",\n        b\"Accessing a corrupted shared library\",\n        b\".lib section in a.out corrupted\",\n        b\"Attempting to link in too many shared libraries\",\n        b\"Cannot exec a shared library directly\",\n        b\"Invalid or incomplete multibyte or wide character\",\n        b\"Interrupted system call should be restarted\",\n        b\"Streams pipe error\",\n        b\"Too many users\",\n        b\"Socket operation on non-socket\",\n        b\"Destination address required\",\n        b\"Message too long\",\n        b\"Protocol wrong type for socket\",\n        b\"Protocol not available\",\n        b\"Protocol not supported\",\n        b\"Socket type not supported\",\n        b\"Operation not supported\",\n        b\"Protocol family not supported\",\n        b\"Address family not supported by protocol\",\n        b\"Address already in use\",\n        b\"Cannot assign requested address\",\n        b\"Network is down\",\n        b\"Network is unreachable\",\n        b\"Network dropped connection on reset\",\n        b\"Software caused connection abort\",\n        b\"Connection reset by peer\",\n        b\"No buffer space available\",\n        b\"Transport endpoint is already connected\",\n        b\"Transport endpoint is not connected\",\n        b\"Cannot send after transport endpoint shutdown\",\n        b\"Too many references: cannot splice\",\n        b\"Connection timed out\",\n        b\"Connection refused\",\n        b\"Host is down\",\n        b\"No route to host\",\n        b\"Operation already in progress\",\n        b\"Operation now in progress\",\n        b\"Stale NFS file handle\",\n        b\"Structure needs cleaning\",\n        b\"Not a XENIX named type file\",\n        b\"No XENIX semaphores available\",\n        b\"Is a named type file\",\n        b\"Remote I/O error\",\n        b\"Disk quota exceeded\",\n        b\"No medium found\",\n        b\"Wrong medium type\",\n        b\"Operation canceled\",\n        b\"Required key not available\",\n        b\"Key has expired\",\n        b\"Key has been revoked\",\n        b\"Key was rejected by service\",\n        b\"Owner died\",\n        b\"State not recoverable\"\n    ]\n\n    # Get arguments\n    errnum = pstate.get_argument_value(0)\n    try:\n        str = sys_errlist[errnum]\n    except:\n        # invalid errnum\n        str = b'Error'\n\n    # TODO: We allocate the string at every hit of this function with a\n    # potential memory leak. We should allocate the sys_errlist only once\n    # and then refer to this table instead of allocate string.\n    ptr = pstate.heap_allocator.alloc(len(str) + 1)\n    pstate.memory.write(ptr, str + b'\\0')\n\n    return ptr", "\n\ndef rtn_strlen(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strlen behavior.\n    \"\"\"\n    logger.debug('strlen hooked')\n\n    ptr_bit_size = pstate.ptr_bit_size\n\n    # Get arguments\n    s = pstate.get_argument_value(0)\n    ast = pstate.actx\n\n    # FIXME: Not so sure its is really the strlen semantic\n    def rec(res, s, deep, maxdeep):\n        if deep == maxdeep:\n            return res\n        cell = pstate.read_symbolic_memory_byte(s+deep).getAst()\n        res  = ast.ite(cell == 0x00, ast.bv(deep, ptr_bit_size), rec(res, s, deep + 1, maxdeep))\n        return res\n\n    sze = len(pstate.memory.read_string(s))\n    res = ast.bv(sze, ptr_bit_size)\n    res = rec(res, s, 0, sze)\n\n    # FIXME: That routine should do something like below to be SOUND !\n    # for i, c in enumerate(pstate.memory.read_string(src)):\n    #     pstate.push_constraint(pstate.read_symbolic_memory_byte(src + i) != 0x00)\n    # pstate.push_constraint(pstate.read_symbolic_memory_byte(src + size) == 0x00)\n\n    pstate.push_constraint(pstate.read_symbolic_memory_byte(s+sze).getAst() == 0x00)\n\n    return res", "\n\ndef rtn_strncasecmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strncasecmp behavior.\n    \"\"\"\n    logger.debug('strncasecmp hooked')\n\n    s1 = pstate.get_argument_value(0)\n    s2 = pstate.get_argument_value(1)\n    sz = pstate.get_argument_value(2)\n    maxlen = min(sz, min(len(pstate.memory.read_string(s1)), len(pstate.memory.read_string(s2))) + 1)\n\n    ptr_bit_size = pstate.ptr_bit_size\n\n    ast = pstate.actx\n    res = ast.bv(0, ptr_bit_size)\n    for index in range(maxlen):\n        cells1 = pstate.read_symbolic_memory_byte(s1 + index).getAst()\n        cells2 = pstate.read_symbolic_memory_byte(s2 + index).getAst()\n        cells1 = ast.ite(ast.land([cells1 >= ord('a'), cells1 <= ord('z')]), cells1 - 32, cells1) # upper case\n        cells2 = ast.ite(ast.land([cells2 >= ord('a'), cells2 <= ord('z')]), cells2 - 32, cells2) # upper case\n        res = res + ast.ite(cells1 == cells2, ast.bv(0, ptr_bit_size), ast.bv(1, ptr_bit_size))\n\n    return res", "\n\ndef rtn_strncmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strncmp behavior.\n    \"\"\"\n    logger.debug('strncmp hooked')\n\n    s1 = pstate.get_argument_value(0)\n    s2 = pstate.get_argument_value(1)\n    sz = pstate.get_argument_value(2)\n    maxlen = min(sz, min(len(pstate.memory.read_string(s1)), len(pstate.memory.read_string(s2))) + 1)\n\n    ptr_bit_size = pstate.ptr_bit_size\n\n    ast = pstate.actx\n    res = ast.bv(0, ptr_bit_size)\n    for index in range(maxlen):\n        cells1 = pstate.read_symbolic_memory_byte(s1 + index).getAst()\n        cells2 = pstate.read_symbolic_memory_byte(s2 + index).getAst()\n        res = res + ast.ite(cells1 == cells2, ast.bv(0, ptr_bit_size), ast.bv(1, ptr_bit_size))\n\n    return res", "\n\ndef rtn_strncpy(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strncpy behavior.\n    \"\"\"\n    logger.debug('strncpy hooked')\n\n    dst = pstate.get_argument_value(0)\n    src = pstate.get_argument_value(1)\n    cnt = pstate.get_argument_value(2)\n\n    pstate.concretize_argument(2)\n\n    for index in range(cnt):\n        src_sym = pstate.read_symbolic_memory_byte(src+index)\n        pstate.write_symbolic_memory_byte(dst+index, src_sym)\n\n        if src_sym.getAst().evaluate() == 0:\n            pstate.push_constraint(src_sym.getAst() == 0x00)\n            break\n        else:\n            pstate.push_constraint(src_sym.getAst() != 0x00)\n\n    return dst", "\n\ndef rtn_strtok_r(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strtok_r behavior.\n    \"\"\"\n    logger.debug('strtok_r hooked')\n\n    string  = pstate.get_argument_value(0)\n    delim   = pstate.get_argument_value(1)\n    saveptr = pstate.get_argument_value(2)\n    saveMem = pstate.memory.read_ptr(saveptr)\n\n    if string == 0:\n        string = saveMem\n\n    d = pstate.memory.read_string(delim)\n    s = pstate.memory.read_string(string)\n\n    tokens = re.split('[' + re.escape(d) + ']', s)\n\n    # TODO: Make it symbolic\n    for token in tokens:\n        if token:\n            offset = s.find(token)\n            # Init the \\0 at the delimiter position\n            node = pstate.read_symbolic_memory_byte(string + offset + len(token)).getAst()\n            try:\n                pstate.push_constraint(pstate.actx.lor([node == ord(c) for c in d]))\n            except:  # dafuck is that?\n                pstate.push_constraint(node == ord(d))\n\n            # Token must not contain delimiters\n            for index, char in enumerate(token):\n                node = pstate.read_symbolic_memory_byte(string + offset + index).getAst()\n                for delim in d:\n                    pstate.push_constraint(node != ord(delim))\n\n            pstate.memory.write_char(string + offset + len(token), 0)\n            # Save the pointer\n            pstate.memory.write_ptr(saveptr, string + offset + len(token) + 1)\n            # Return the token\n            return string + offset\n\n    return NULL_PTR", "\n\ndef rtn_strtoul(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The strtoul behavior.\n    \"\"\"\n    logger.debug('strtoul hooked')\n\n    nptr   = pstate.get_argument_value(0)\n    nptrs  = pstate.get_string_argument(0)\n    endptr = pstate.get_argument_value(1)\n    base   = pstate.get_argument_value(2)\n\n    for i, c in enumerate(nptrs):\n        pstate.push_constraint(pstate.read_symbolic_memory_byte(nptr+i).getAst() == ord(c))\n\n    pstate.concretize_argument(2)  # Concretize base\n\n    try:\n        return int(nptrs, base)\n    except:\n        return 0xffffffff", "\n\ndef rtn_getenv(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The getenv behavior.\n    \"\"\"\n    # TODO\n    name = pstate.get_argument_value(0)\n\n    if name == 0:\n        return NULL_PTR\n\n    environ_name = pstate.memory.read_string(name)\n    logger.warning(f\"Target called getenv({environ_name})\")\n    host_env_val = os.getenv(environ_name)\n    return host_env_val if host_env_val is not None else 0", "\n\n#def rtn_tolower(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n#    # TODO\n#    \"\"\"\n#    The tolower behavior.\n#    \"\"\"\n#    ptr_bit_size = pstate.ptr_bit_size\n#    ast = pstate.actx\n#    arg_sym = pstate.get_argument_symbolic(0)", "#    ast = pstate.actx\n#    arg_sym = pstate.get_argument_symbolic(0)\n#    return rdi_sym.getAst() - 0x20\n\ndef rtn_isspace(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The isspace behavior.\n    \"\"\"\n    ptr_bit_size = pstate.ptr_bit_size\n    ast = pstate.actx\n    arg_sym = pstate.get_argument_symbolic(0)\n\n    exp = arg_sym.getAst() == 0x20\n    exp = ast.lor([exp, arg_sym.getAst() == 0xa])\n    exp = ast.lor([exp, arg_sym.getAst() == 0x9])\n    exp = ast.lor([exp, arg_sym.getAst() == 0xc])\n    exp = ast.lor([exp, arg_sym.getAst() == 0xd])\n    res =  ast.ite(exp, ast.bv(0, ptr_bit_size), ast.bv(1, ptr_bit_size))\n    return res", "\n\ndef rtn_assert_fail(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The __assert_fail behavior.\n    \"\"\"\n    msg = pstate.get_argument_value(0)\n\n    msg = pstate.memory.read_string(msg)\n    logger.warning(f\"__assert_fail called : {msg}\")\n\n    # Write 1 as return value of the program\n    pstate.write_register(pstate.return_register, 1)\n    se.abort()", "\ndef rtn_setlocale(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The setlocale behavior.\n    \"\"\"\n    logger.debug('setlocale hooked')\n\n    category = pstate.get_argument_value(0)\n    locale   = pstate.get_argument_value(1)\n\n    if locale != 0:\n        logger.warning(f\"Attempt to modify Locale. Currently not supported.\")\n        return 0\n\n    # This is a bit hacky but we just store the LOCALEs in the [extern] segment\n    segs = pstate.memory.find_map(pstate.EXTERN_SEG)\n    if segs:\n        map = segs[0]\n        LC_ALL = map.start + map.size - 0x20 # Point to the end of seg. But keep in mind LC_ALL is at end - 4.\n    else:\n        assert False\n    print(f\"selocale writing at {LC_ALL:#x}\")\n\n    if category == 0:\n        pstate.memory.write(LC_ALL, b\"en_US.UTF-8\\x00\")\n    else:\n        logger.warning(f\"setlocale called with unsupported category={category}.\")\n\n    return LC_ALL", "\n\ndef rtn__setjmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The _setjmp behavior. \n    \"\"\"\n    # TODO\n    logger.warning(\"hooked _setjmp\")\n    return 0\n", "\n\ndef rtn_longjmp(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    \"\"\"\n    The longjmp behavior.\n    \"\"\"\n    # NOTE All the programs tested so far used `longjmp` as an error handling mechanism, right\n    # before exiting. This is why, `longjmp` is currently considered an exit condition. \n    # TODO Real implementation\n    logger.debug('longjmp hooked')\n    pstate.stop = True", "\ndef rtn_atexit(se: 'SymbolicExecutor', pstate: 'ProcessState'):\n    return 0\n\n\nSUPPORTED_ROUTINES = {\n    # TODO:\n    #   - tolower\n    #   - toupper\n    '__assert_fail':           rtn_assert_fail,", "    #   - toupper\n    '__assert_fail':           rtn_assert_fail,\n    '__ctype_b_loc':           rtn_ctype_b_loc,\n    '__ctype_toupper_loc':     rtn_ctype_toupper_loc,\n    '__errno_location':        rtn_errno_location,\n    '__fprintf_chk':           rtn___fprintf_chk,\n    '__libc_start_main':       rtn_libc_start_main,\n    '__stack_chk_fail':        rtn_stack_chk_fail,\n    '__xstat':                 rtn_xstat,\n    'abort':                   rtn_abort,", "    '__xstat':                 rtn_xstat,\n    'abort':                   rtn_abort,\n    \"atexit\":                  rtn_atexit,\n    \"__cxa_atexit\":            rtn_atexit,\n    'atoi':                    rtn_atoi,\n    'calloc':                  rtn_calloc,\n    'clock_gettime':           rtn_clock_gettime,\n    'exit':                    rtn_exit,\n    'fclose':                  rtn_fclose,\n    'fgets':                   rtn_fgets,", "    'fclose':                  rtn_fclose,\n    'fgets':                   rtn_fgets,\n    'fopen':                   rtn_fopen,\n    'fprintf':                 rtn_fprintf,\n    'fputc':                   rtn_fputc,\n    'fputs':                   rtn_fputs,\n    'fread':                   rtn_fread,\n    'free':                    rtn_free,\n    'fwrite':                  rtn_fwrite,\n    'gettimeofday':            rtn_gettimeofday,", "    'fwrite':                  rtn_fwrite,\n    'gettimeofday':            rtn_gettimeofday,\n    'malloc':                  rtn_malloc,\n    'memcmp':                  rtn_memcmp,\n    'memcpy':                  rtn_memcpy,\n    'memmem':                  rtn_memmem,\n    'memmove':                 rtn_memmove,\n    'memset':                  rtn_memset,\n    'open':                    rtn_open,\n    'printf':                  rtn_printf,", "    'open':                    rtn_open,\n    'printf':                  rtn_printf,\n    'pthread_create':          rtn_pthread_create,\n    'pthread_exit':            rtn_pthread_exit,\n    'pthread_join':            rtn_pthread_join,\n    'pthread_mutex_destroy':   rtn_pthread_mutex_destroy,\n    'pthread_mutex_init':      rtn_pthread_mutex_init,\n    'pthread_mutex_lock':      rtn_pthread_mutex_lock,\n    'pthread_mutex_unlock':    rtn_pthread_mutex_unlock,\n    'puts':                    rtn_puts,", "    'pthread_mutex_unlock':    rtn_pthread_mutex_unlock,\n    'puts':                    rtn_puts,\n    'rand':                    rtn_rand,\n    'read':                    rtn_read,\n    'sem_destroy':             rtn_sem_destroy,\n    'sem_getvalue':            rtn_sem_getvalue,\n    'sem_init':                rtn_sem_init,\n    'sem_post':                rtn_sem_post,\n    'sem_timedwait':           rtn_sem_timedwait,\n    'sem_trywait':             rtn_sem_trywait,", "    'sem_timedwait':           rtn_sem_timedwait,\n    'sem_trywait':             rtn_sem_trywait,\n    'sem_wait':                rtn_sem_wait,\n    'sleep':                   rtn_sleep,\n    'sprintf':                 rtn_sprintf,\n    'strcasecmp':              rtn_strcasecmp,\n    'strchr':                  rtn_strchr,\n    'strcmp':                  rtn_strcmp,\n    'strcpy':                  rtn_strcpy,\n    'strerror':                rtn_strerror,", "    'strcpy':                  rtn_strcpy,\n    'strerror':                rtn_strerror,\n    'strlen':                  rtn_strlen,\n    'strncasecmp':             rtn_strncasecmp,\n    'strncmp':                 rtn_strncmp,\n    'strncpy':                 rtn_strncpy,\n    'strtok_r':                rtn_strtok_r,\n    'strtoul':                 rtn_strtoul,\n\n    'write':                   rtn_write,", "\n    'write':                   rtn_write,\n    'getenv':                  rtn_getenv,\n    'fseek':                   rtn_fseek,\n    'ftell':                   rtn_ftell,\n\n    '_setjmp':                 rtn__setjmp,\n    'longjmp':                 rtn_longjmp,\n    'realloc':                 rtn_realloc,\n    'setlocale':               rtn_setlocale,", "    'realloc':                 rtn_realloc,\n    'setlocale':               rtn_setlocale,\n    'strdup':                  rtn_strdup,\n    'mempcpy':                 rtn_mempcpy,\n    '__mempcpy':               rtn_mempcpy,\n    'getchar':                 rtn_getchar,\n\n    'isspace':                 rtn_isspace,\n    #'tolower':                 rtn_tolower,\n}", "    #'tolower':                 rtn_tolower,\n}\n\n\nSUPORTED_GVARIABLES = {\n    '__stack_chk_guard':    0xdead,\n    'stderr':               0x0002,\n    'stdin':                0x0000,\n    'stdout':               0x0001,\n}", "    'stdout':               0x0001,\n}\n\n"]}
{"filename": "tritondse/types.py", "chunked_list": ["from __future__ import annotations\n\nimport sys\nfrom enum import IntEnum, Enum, auto, IntFlag\nfrom pathlib import Path\nfrom triton import ARCH, SOLVER_STATE, SOLVER\nfrom typing import Union, TypeVar, Tuple, Type\nimport io\nimport enum_tools.documentation\nfrom dataclasses import dataclass", "import enum_tools.documentation\nfrom dataclasses import dataclass\n\nPathLike = Union[str, Path]\n\"\"\"Type representing file either as a file, either as a Path object\"\"\"\n\nAddr = int\n\"\"\"Integer representing an address\"\"\"\n\nrAddr = int", "\nrAddr = int\n\"\"\"Integer representing a relative address\"\"\"\n\nBitSize = int\n\"\"\"Integer representing a value in bits\"\"\"\n\nByteSize = int\n\"\"\"Integer representing a value in bytes\"\"\"\n", "\"\"\"Integer representing a value in bytes\"\"\"\n\nInput = bytes\n\"\"\" Type representing an Input (which is bytes) \"\"\"\n\nRegister = TypeVar('Register')\n\"\"\"Register identifier as used by Triton *(not the Register object itself)*\"\"\"\n\nRegisters = TypeVar('Registers')\n\"\"\"Set of registers as used by Triton\"\"\"", "Registers = TypeVar('Registers')\n\"\"\"Set of registers as used by Triton\"\"\"\n\nPathConstraint = TypeVar('PathConstraint')\n\"\"\" `PathConstraint <https://triton.quarkslab.com/documentation/doxygen/py_PathConstraint_page.html>`_ object as returned by Triton\"\"\"\n\nAstNode = TypeVar('AstNode')\n\"\"\" SMT logic formula as returned by Triton (`AstNode <https://triton.quarkslab.com/documentation/doxygen/py_AstNode_page.html>`_) \"\"\"\n\nModel = TypeVar('Model')", "\nModel = TypeVar('Model')\n\"\"\" Solver `Model <https://triton.quarkslab.com/documentation/doxygen/py_SolverModel_page.html>`_ as returned by Triton \"\"\"\n\nExpression = TypeVar('Expression')\n\"\"\" Symbolic Expression as returned by Triton (`SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_) \"\"\"\n\nSymbolicVariable = TypeVar('SymbolicVariable')\n\"\"\" Symbolic Variable as returned by Triton (`SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicVariable_page.html>`_) \"\"\"\n", "\"\"\" Symbolic Variable as returned by Triton (`SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicVariable_page.html>`_) \"\"\"\n\nEdge = Tuple[Addr, Addr]\n\"\"\" Type representing a edge in the program \"\"\"\n\nPathHash = str\n\"\"\"Type representing the hash of path to uniquely identify any path \"\"\"\n\n\n@enum_tools.documentation.document_enum\nclass SymExType(str, Enum):\n    \"\"\"\n    Symobolic Expression type enum. (internal usage only)\n    \"\"\"\n\n    CONDITIONAL_JMP = 'cond-jcc'  # doc: symbolic expression is a conditional jump\n    DYNAMIC_JMP = 'dyn-jmp'       # doc: symbolic expression is a dynamic jump\n    SYMBOLIC_READ = 'sym-read'    # doc: symbolic expression is a symbolic memory read\n    SYMBOLIC_WRITE = 'sym-write'  # doc: symbolic expression is a symbolic memory write", "\n@enum_tools.documentation.document_enum\nclass SymExType(str, Enum):\n    \"\"\"\n    Symobolic Expression type enum. (internal usage only)\n    \"\"\"\n\n    CONDITIONAL_JMP = 'cond-jcc'  # doc: symbolic expression is a conditional jump\n    DYNAMIC_JMP = 'dyn-jmp'       # doc: symbolic expression is a dynamic jump\n    SYMBOLIC_READ = 'sym-read'    # doc: symbolic expression is a symbolic memory read\n    SYMBOLIC_WRITE = 'sym-write'  # doc: symbolic expression is a symbolic memory write", "\n\nif sys.version_info.minor >= 8:\n    from typing import TypedDict\n\n    class PathBranch(TypedDict):\n        \"\"\"\n        Typed dictionnary describing the branch information\n        returned by Triton (with getBranchConstraints())\n        \"\"\"\n        isTaken: bool\n        srcAddr: Addr\n        dstAddr: Addr\n        constraint: AstNode\nelse:\n    PathBranch = TypeVar('PathBranch')\n    \"\"\" PathBranchobject as returned by Triton.\n    Thus it is a dictionnary with the keys:\n    \"\"\"", "\n\n@enum_tools.documentation.document_enum\nclass Architecture(IntEnum):\n    \"\"\"\n    Common architecture Enum fully compatible with Triton\n    `ARCH <https://triton.quarkslab.com/documentation/doxygen/py_ARCH_page.html>`_\n    \"\"\"\n    AARCH64 = ARCH.AARCH64  # doc: Aarch64 architecture\n    ARM32 = ARCH.ARM32      # doc: ARM architecture (32 bits)\n    X86 = ARCH.X86          # doc: x86 architecture (32 bits)\n    X86_64 = ARCH.X86_64    # doc: x86-64 architecture (64 bits)", "\n@enum_tools.documentation.document_enum\nclass ArchMode(IntFlag):\n    \"\"\"\n    Various architecture specific modes that can be enabled or disabledd.\n    (meant to be fullfilled)\n    \"\"\"\n    THUMB = 1   # doc: set thumb mode for ARM32 architecture\n\n", "\n\n@enum_tools.documentation.document_enum\nclass Platform(IntEnum):\n    \"\"\"\n    Platform associated to a binary\n    \"\"\"\n    LINUX = auto()    # doc: Linux platform\n    WINDOWS = auto()  # doc: Windows platform\n    MACOS = auto()    # doc: Mac OS platform\n    ANDROID = auto()  # doc: Android platform\n    IOS = auto()      # doc: IOS platform", "\n@enum_tools.documentation.document_enum\nclass SmtSolver(IntEnum):\n    \"\"\" Common SMT Solver Enum fully compatible with Triton \"\"\"\n    Z3 = SOLVER.Z3              # doc: Z3 SMT solver\n    BITWUZLA = SOLVER.BITWUZLA  # doc: bitwuzla solver\n\n\n@enum_tools.documentation.document_enum\nclass SolverStatus(IntEnum):\n    \"\"\" Common Solver Enum fully compatible with Triton ARCH \"\"\"\n    SAT     = SOLVER_STATE.SAT      # doc: Formula is satisfiable (SAT)\n    UNSAT   = SOLVER_STATE.UNSAT    # doc: Formula is unsatisfiable (UNSAT)\n    TIMEOUT = SOLVER_STATE.TIMEOUT  # doc: Formula solving did timeout\n    UNKNOWN = SOLVER_STATE.UNKNOWN  # doc: Formula solving failed", "@enum_tools.documentation.document_enum\nclass SolverStatus(IntEnum):\n    \"\"\" Common Solver Enum fully compatible with Triton ARCH \"\"\"\n    SAT     = SOLVER_STATE.SAT      # doc: Formula is satisfiable (SAT)\n    UNSAT   = SOLVER_STATE.UNSAT    # doc: Formula is unsatisfiable (UNSAT)\n    TIMEOUT = SOLVER_STATE.TIMEOUT  # doc: Formula solving did timeout\n    UNKNOWN = SOLVER_STATE.UNKNOWN  # doc: Formula solving failed\n\n\n@enum_tools.documentation.document_enum\nclass Perm(IntFlag):\n    \"\"\"\n    Flags encoding permissions. Used for memory pages.\n    They can be combined as flags. e.g:\n\n    .. code-block:: python\n\n        rw = Perm.R | Perm.W\n    \"\"\"\n    R = 4  # doc: Read\n    W = 2  # doc: Write\n    X = 1  # doc: Execute", "\n@enum_tools.documentation.document_enum\nclass Perm(IntFlag):\n    \"\"\"\n    Flags encoding permissions. Used for memory pages.\n    They can be combined as flags. e.g:\n\n    .. code-block:: python\n\n        rw = Perm.R | Perm.W\n    \"\"\"\n    R = 4  # doc: Read\n    W = 2  # doc: Write\n    X = 1  # doc: Execute", "\n\n@enum_tools.documentation.document_enum\nclass Endian(IntEnum):\n    \"\"\"\n    Endianess of the binary.\n    \"\"\"\n    LITTLE = 1  # doc: Little-endian\n    BIG = 2     # doc: Big-endian\n", "\n\n@dataclass\nclass FileDesc:\n    \"\"\"\n    Type representing a file descriptor\n    \"\"\"\n    \"\"\" The target program's file descriptor \"\"\"\n    id: int\n    \"\"\" Name of the file \"\"\"\n    name: str\n    \"\"\" The python file stream object \"\"\"\n    fd: io.IOBase\n\n    @property\n    def offset(self) -> int:\n        return self.fd.tell()\n\n    def is_real_fd(self) -> bool:\n        return isinstance(self.fd, io.TextIOWrapper)\n\n    def is_input_fd(self) -> bool:\n        return isinstance(self.fd, io.BytesIO)\n\n    def fgets(self, max_size: int) -> bytes:\n        s = b\"\"\n        for i in range(max_size):\n            c = self.fd.read(1)\n            if not c:  # EOF\n                break\n            c = c if isinstance(c, bytes) else c.encode()\n            s += c\n            if c == b\"\\x00\":\n                return s\n            elif c == b\"\\n\":\n                break\n        # If get there read max_size\n        return s+b\"\\x00\"\n\n    def read(self, size: int) -> bytes:\n        data = self.fd.read(size)\n        return data if isinstance(data, bytes) else data.encode()", ""]}
{"filename": "tritondse/config.py", "chunked_list": ["import logging\nimport json\nfrom enum import Enum, IntFlag\nfrom pathlib import Path\nfrom typing import List\nfrom functools import reduce\n\n# triton-based libraries\nfrom tritondse.coverage import CoverageStrategy, BranchSolvingStrategy\nfrom tritondse.types import SmtSolver", "from tritondse.coverage import CoverageStrategy, BranchSolvingStrategy\nfrom tritondse.types import SmtSolver\nfrom tritondse.seed import SeedFormat\n\n\nclass Config(object):\n    \"\"\"\n    Data class holding tritondse configuration\n    parameter\n    \"\"\"\n\n    def __init__(self,\n                 seed_format: SeedFormat = SeedFormat.RAW,\n                 pipe_stdout: bool = False,\n                 pipe_stderr: bool = False,\n                 skip_sleep_routine: bool = False,\n                 smt_solver: SmtSolver = SmtSolver.Z3,\n                 smt_timeout: int = 5000,\n                 execution_timeout: int = 0,\n                 exploration_timeout: int = 0,\n                 exploration_limit: int = 0,\n                 thread_scheduling: int = 200,\n                 smt_queries_limit: int = 1200,\n                 smt_enumeration_limit: int = 40,\n                 coverage_strategy: CoverageStrategy = CoverageStrategy.BLOCK,\n                 branch_solving_strategy: BranchSolvingStrategy = BranchSolvingStrategy.FIRST_LAST_NOT_COVERED,\n                 workspace: str = \"\",\n                 program_argv: List[str] = None,\n                 time_inc_coefficient: float = 0.00001,\n                 skip_unsupported_import: bool = False,\n                 skip_unsupported_instruction: bool = False,\n                 memory_segmentation: bool = True):\n\n        self.seed_format: SeedFormat = seed_format\n        \"\"\" Seed type is either Raw (raw bytes) or Composite (more expressive).\n            See seeds.py for more information on each format.\n        \"\"\"\n        \n        self.pipe_stdout: bool = pipe_stdout\n        \"\"\" Pipe the program stdout to Python's stdout. *(default: False)*\"\"\"\n\n        self.pipe_stderr: bool = pipe_stderr\n        \"\"\" Pipe the program stderr to Python's stderr *(default: False)*\"\"\"\n\n        self.skip_sleep_routine: bool = skip_sleep_routine\n        \"\"\" Whether to emulate sleeps routine or not *(default: False)*\"\"\"\n\n        self.smt_solver: SmtSolver = smt_solver\n        \"\"\" SMT solver to perform queries solving \"\"\"\n\n        self.smt_timeout: int = smt_timeout\n        \"\"\" Timeout for a single SMT query in milliseconds *(default: 10)*\"\"\"\n\n        self.execution_timeout: int = execution_timeout\n        \"\"\" Timeout of a single execution. If it is triggered the associated\n        input file is marked as 'hanging'. In seconds, 0 means unlimited *(default: 0)*\"\"\"\n\n        self.exploration_timeout: int = exploration_timeout\n        \"\"\" Overall timeout of the exploration in seconds. 0 means unlimited *(default: 0)* \"\"\"\n\n        self.exploration_limit: int = exploration_limit\n        \"\"\" Number of execution iterations. 0 means unlimited *(default: 0)*\"\"\"\n\n        self.thread_scheduling: int = thread_scheduling\n        \"\"\" Number of instructions to execute before switching to the next thread.\n        At the moment all threads are scheduled in a round-robin manner *(default: 200)*\"\"\"\n\n        self.smt_queries_limit: int = smt_queries_limit\n        \"\"\" Limit of SMT queries to perform for a single execution *(default: 1200)*\"\"\"\n\n        self.smt_enumeration_limit: int = smt_enumeration_limit\n        \"\"\" Limit of model values retrieved when enumerating a dynamic jump or symbolic memory accesses\"\"\"\n\n        self.coverage_strategy: CoverageStrategy = coverage_strategy\n        \"\"\" Coverage strategy to apply for the whole exploration, default: :py:obj:`CoverageStrategy.BLOCK`\"\"\"\n\n        self.branch_solving_strategy: BranchSolvingStrategy = branch_solving_strategy\n        \"\"\" Branch solving strategy to apply for a single execution. For a given non-covered\n        branch allows changing whether we try to solve it at all occurences or more seldomly.\n        default: :py:obj:`BranchSolvingStrategy.FIRST_LAST_NOT_COVERED`\n        \"\"\"\n\n        self.workspace: str = workspace\n        \"\"\" Workspace directory to use. *(default: 'workspace')* \"\"\"\n\n        self.program_argv: List[str] = [] if program_argv is None else program_argv\n        \"\"\" Concrete program argument as given on the command line.\"\"\"\n\n        self.time_inc_coefficient: float = time_inc_coefficient\n        \"\"\" Time increment coefficient at each instruction to provide a deterministic\n        behavior when calling time functions (e.g gettimeofday(), clock_gettime(), ...).\n        For example, if 0.0001 is defined, each instruction will increment the time representation\n        of the execution by 100us. *(default: 0.00001)*\n        \"\"\"\n\n        self.skip_unsupported_import: bool = skip_unsupported_import\n        \"\"\" Whether or not to stop the emulation when hitting a external\n        call to a function that is not supported.\n        \"\"\"\n\n        self.skip_unsupported_instruction: bool = skip_unsupported_instruction\n        \"\"\" Whether or not to stop the emulation when hitting an instruction\n        for which the semantic is not defined.\n        \"\"\"\n\n        self.memory_segmentation: bool = memory_segmentation\n        \"\"\" This option defines whether or not memory segmentation is enforced.\n        If activated all memory accesses must belong to a mapped memory area.\n        \"\"\"\n\n        self.custom = {}\n        \"\"\"\n        Custom carrier field enabling user to add parameters of their own.\n        \"\"\"\n\n    def __str__(self):\n        return \"\\n\".join(f\"{k.ljust(23)}= {v}\" for k, v in self.__dict__.items())\n\n    def to_file(self, file: str) -> None:\n        \"\"\"\n        Save the current configuration to a file\n\n        :param file: The path name\n        \"\"\"\n        with open(file, \"w\") as f:\n            f.write(self.to_json())\n\n    @staticmethod\n    def from_file(file: str) -> 'Config':\n        \"\"\"\n        Load a configuration from a file to a new instance of Config\n\n        :param file: The path name\n        :return: A fresh instance of Config\n        \"\"\"\n        raw = Path(file).read_text()\n        return Config.from_json(raw)\n\n    @staticmethod\n    def from_json(s: str) -> 'Config':\n        \"\"\"\n        Load a configuration from a json input to a new instance of Config\n\n        :param s: The JSON text\n        :return: A fresh instance of Config\n        \"\"\"\n        data = json.loads(s)\n        c = Config()\n        for k, v in data.items():\n            if hasattr(c, k):\n                mapping = {\"coverage_strategy\": CoverageStrategy, \"smt_solver\": SmtSolver,\n                           \"seed_format\": SeedFormat}\n                if k in mapping:\n                    v = mapping[k][v]\n                elif k == \"branch_solving_strategy\":\n                    v = reduce(lambda acc, x: BranchSolvingStrategy[x] | acc, v, 0)\n                setattr(c, k, v)\n            else:\n                logging.warning(f\"config unknown parameter: {k}\")\n        return c\n\n    def to_json(self) -> str:\n        \"\"\"\n        Convert the current configuration to a json output\n\n        :return: JSON text\n        \"\"\"\n        def to_str_list(value):\n            return [x.name for x in list(BranchSolvingStrategy) if x in value]\n        d = {}\n        for k, v in self.__dict__.items():\n            if isinstance(v, IntFlag):\n                d[k] = to_str_list(v)\n            elif isinstance(v, Enum):\n                d[k] = v.name\n            else:\n                d[k] = v\n        return json.dumps(d, indent=2)\n\n    def is_format_composite(self) -> bool:\n        \"\"\" Return true if the seed format is composite\"\"\"\n        return self.seed_format == SeedFormat.COMPOSITE\n\n    def is_format_raw(self) -> bool:\n        \"\"\" Return true if the seed format is raw \"\"\"\n        return self.seed_format == SeedFormat.RAW", ""]}
{"filename": "tritondse/workspace.py", "chunked_list": ["# built-in imports\nfrom __future__ import annotations\nimport shutil\nfrom pathlib import Path\nfrom typing import Generator, Optional, Union\nimport time\n\n# local imports\nfrom tritondse.types import PathLike\nfrom tritondse.seed import Seed, SeedStatus", "from tritondse.types import PathLike\nfrom tritondse.seed import Seed, SeedStatus\nimport tritondse.logging\n\nlogger = tritondse.logging.get()\n\n\nclass Workspace(object):\n    \"\"\"\n    Class to abstract the file tree of the current exploration workspace.\n    A user willing to save additional files in the workspace is invited\n    to do it from the workspace API as it somehow abstract the exact\n    location of it.\n    \"\"\"\n\n    DEFAULT_WORKSPACE = \"/tmp/triton_workspace\"\n    CORPUS_DIR = \"corpus\"\n    CRASH_DIR = \"crashes\"\n    HANG_DIR = \"hangs\"\n    WORKLIST_DIR = \"worklist\"\n    METADATA_DIR = \"metadata\"\n    BIN_DIR = \"bin\"\n    LOG_FILE = \"tritondse.log\"\n\n    def __init__(self, root_dir: PathLike):\n        \"\"\"\n        :param root_dir: Root directory of the workspace. Created if not existing\n        :type root_dir: :py:obj:`tritondse.types.PathLike`\n        \"\"\"\n        if not root_dir:  # If no workspace was provided create a unique temporary one\n            self.root_dir = Path(self.DEFAULT_WORKSPACE) / str(time.time()).replace(\".\", \"\")\n            self.root_dir.mkdir(parents=True)\n        else:\n            self.root_dir = Path(root_dir)\n            if not self.root_dir.exists():  # Create the directory in case it was not existing\n                self.root_dir.mkdir(parents=True)\n                self.initialize()\n        self.root_dir: Path = self.root_dir.resolve()  #: root directory of the Workspace\n\n    def initialize(self, flush: bool = False) -> None:\n        \"\"\"\n        Initialize the workspace by creating all required subfolders\n        if not already existing.\n\n        :param flush: if True deletes all files contained in the workspace\n        :type flush: bool\n        \"\"\"\n\n        for dir in (self.root_dir / x for x in [self.CORPUS_DIR, self.CRASH_DIR, self.HANG_DIR, self.WORKLIST_DIR, self.METADATA_DIR, self.BIN_DIR]):\n            if not dir.exists():\n                logger.debug(f\"Creating the {dir} directory\")\n                dir.mkdir(parents=True)\n            else:\n                if flush:\n                    shutil.rmtree(dir)\n                    dir.mkdir()\n\n    def get_metadata_file(self, name: str) -> Optional[str]:\n        \"\"\"\n        Read a metadata file from the workspace on disk.\n        Data is read as a string. If the given file does not\n        exists, None is returned\n\n        :param name: file name (can also be a path)\n        :type name: str\n        :returns: File content as string if existing\n        :rtype: Optional[str]\n        \"\"\"\n        p = (self.root_dir / self.METADATA_DIR) / name\n        if p.exists():\n            return p.read_text()\n        else:\n            return None\n\n    def get_metadata_file_path(self, name: str) -> Path:\n        \"\"\"\n        Get a file path in the workspace directory that the user\n        can write into. Might be called for the user to write on\n        its own the file content. If name is a file tree, all parent\n        directories are created.\n\n        :param name: filename wanted\n        :type name: str\n        :return: absolute filepath (regardless of whether it exists or not)\n        \"\"\"\n        p = (self.root_dir / self.METADATA_DIR) / name\n        if not p.parent.exists():\n            p.parent.mkdir(parents=True)\n        return p\n\n    def get_binary_directory(self) -> Path:\n        \"\"\"\n        Get the directory containing the executable (and its dependencies).\n        :return: Path of the directory\n        \"\"\"\n        return self.root_dir / self.BIN_DIR\n\n    def save_metadata_file(self, name: str, content: Union[str, bytes]) -> None:\n        \"\"\"\n        Save ``content`` in a file ``name`` in the metadata directory.\n        The name should be a file name not a path.\n\n        :param name: file name\n        :type name: str\n        :param content: content of the file to write\n        :type content: Union[str, bytes]\n        \"\"\"\n        p = (self.root_dir / self.METADATA_DIR) / name\n        if isinstance(content, str):\n            p.write_text(content)\n        else:\n            p.write_bytes(content)\n\n    def _iter_seeds(self, directory: str, st: SeedStatus) -> Generator[Seed, None, None]:\n        \"\"\" Iterate over seeds \"\"\"\n        for file in (self.root_dir/directory).glob(\"*.cov\"):\n            yield Seed.from_file(file, st)\n\n    def iter_corpus(self) -> Generator[Seed, None, None]:\n        \"\"\"\n        Iterate over the corpus files as Seed object.\n\n        :returns: generator of Seed object\n        :rtype: Generator[Seed, None, None]\n        \"\"\"\n        yield from self._iter_seeds(self.CORPUS_DIR, SeedStatus.OK_DONE)\n\n    def iter_crashes(self) -> Generator[Seed, None, None]:\n        \"\"\"\n        Iterate over the crashes files as Seed object.\n\n        :returns: generator of Seed object\n        :rtype: Generator[Seed, None, None]\n        \"\"\"\n        yield from self._iter_seeds(self.CRASH_DIR, SeedStatus.CRASH)\n\n    def iter_hangs(self) -> Generator[Seed, None, None]:\n        \"\"\"\n        Iterate over the hang files as Seed object.\n\n        :returns: generator of Seed object\n        :rtype: Generator[Seed, None, None]\n        \"\"\"\n        yield from self._iter_seeds(self.HANG_DIR, SeedStatus.HANG)\n\n    def iter_worklist(self) -> Generator[Seed, None, None]:\n        \"\"\"\n        Iterate over the worklist files as Seed object.\n        Worklist are all the pending seeds\n\n        :returns: generator of Seed object\n        :rtype: Generator[Seed, None, None]\n        \"\"\"\n        yield from self._iter_seeds(self.WORKLIST_DIR, SeedStatus.NEW)\n\n    def save_seed(self, seed: Seed) -> None:\n        \"\"\"\n        Save the current seed in the workspace directory matching its status.\n\n        :param seed: Seed to save\n        :type seed: Seed\n        \"\"\"\n        mapper = {SeedStatus.NEW: self.WORKLIST_DIR,\n                  SeedStatus.OK_DONE: self.CORPUS_DIR,\n                  SeedStatus.HANG: self.HANG_DIR,\n                  SeedStatus.CRASH: self.CRASH_DIR}\n        p = (self.root_dir / mapper[seed.status]) / seed.filename\n        p.write_bytes(bytes(seed))\n\n    def update_seed_location(self, seed: Seed) -> None:\n        \"\"\"\n        Move a worklist seed to its final location according to its (new) status.\n        Typically used to move a seed from pending ones to corpus or crash once it\n        is fully consumed.\n\n        :param seed: seed to move\n        :type seed: Seed\n        \"\"\"\n        old_p = (self.root_dir / self.WORKLIST_DIR) / seed.filename\n        try:\n            old_p.unlink()  # Remove the seed from the worklist\n        except:\n            logger.warning(f\"seed {seed} unlink failed\")\n            pass  # FIXME: Not meant to get here\n        self.save_seed(seed)\n\n    def save_file(self, rel_path: PathLike, content: Union[str, bytes], override: bool = False):\n        \"\"\"\n        Save a, arbitrary file in the workspace by providing the relative path\n        of the file. If ``override`` is True, erase the previous file if any.\n\n        :param rel_path: relative path of the file\n        :type rel_path: :py:obj:`tritondse.types.PathLike`\n        :param content: content to write\n        :type content: Union[str, bytes]\n        :param override: whether to override or not an existing file\n        :type override: bool\n        \"\"\"\n        p = self.root_dir / rel_path\n        p.parent.mkdir(parents=True, exist_ok=True)\n        if not p.exists() or override:\n            if isinstance(content, str):\n                p.write_text(content)\n            elif isinstance(content, bytes):\n                p.write_bytes(content)\n            else:\n                assert False\n\n    @property\n    def logfile_path(self):\n        return self.root_dir / self.LOG_FILE", ""]}
{"filename": "tritondse/thread_context.py", "chunked_list": ["from enum import Enum, auto\nimport enum_tools.documentation\nfrom triton import TritonContext\n\n\n@enum_tools.documentation.document_enum\nclass ThreadState(Enum):\n    RUNNING = auto()  # doc: Normal state\n    DEAD = auto()     # doc: State after pthread_exit\n    JOINING = auto()  # doc: State after pthread_join\n    LOCKED = auto()   # doc: State after a pthread_lock & co", "\n\nclass ThreadContext(object):\n    \"\"\"\n    Thread data structure holding all information related to it.\n    Purposely used to save registers and to restore them in a\n    TritonContext.\n    \"\"\"\n\n    def __init__(self, tid: int):\n        \"\"\"\n        :param tid: thread id\n        \"\"\"\n        self.cregs = dict()          # context of concrete registers\n        self.sregs = dict()          # context of symbolic registers\n        self._join_th_id = None      # joined thread id\n        self.tid = tid               # the thread id\n        self.count = 0               # Number of instructions executed until scheduling\n        self.state = ThreadState.RUNNING\n        # FIXME: Keep the thread_scheduling and automated the reset on restore\n\n    def save(self, tt_ctx: TritonContext) -> None:\n        \"\"\"\n        Save the current thread state from the current execution.\n        That implies keeping a reference on symbolic and concrete\n        registers.\n\n        :param tt_ctx: current TritonContext to save\n        :type tt_ctx: `TritonContext <https://triton.quarkslab.com/documentation/doxygen/py_TritonContext_page.html>`_\n        \"\"\"\n        # Save symbolic registers\n        self.sregs = tt_ctx.getSymbolicRegisters()\n        # Save concrete registers\n        for r in tt_ctx.getParentRegisters():\n            self.cregs.update({r.getId(): tt_ctx.getConcreteRegisterValue(r)})\n\n    def restore(self, tt_ctx: TritonContext) -> None:\n        \"\"\"\n        Restore a thread state in the given TritonContext\n\n        :param tt_ctx: context in which to restor the current thread state\n        :type tt_ctx: `TritonContext <https://triton.quarkslab.com/documentation/doxygen/py_TritonContext_page.html>`_\n        \"\"\"\n        # Restore concrete registers\n        for rid, v in self.cregs.items():\n            tt_ctx.setConcreteRegisterValue(tt_ctx.getRegister(rid), v)\n        # Restore symbolic registers\n        for rid, e in self.sregs.items():\n            tt_ctx.assignSymbolicExpressionToRegister(e, tt_ctx.getRegister(rid))\n\n    def kill(self) -> None:\n        \"\"\"\n        Kill the current thread. Called when exiting the thread.\n\n        :return:\n        \"\"\"\n        self.state = ThreadState.DEAD\n\n    def is_dead(self) -> bool:\n        \"\"\"\n        Returns whether the thread is killed or not\n\n        :return: boolean indicating if the thread is dead or not\n        \"\"\"\n        return self.state == ThreadState.DEAD\n\n    def join_thread(self, th_id: int) -> None:\n        \"\"\"\n        Put the thread in a join state where waits for\n        another thread.\n\n        :param th_id: id of the thread to join\n        :return: None\n        \"\"\"\n        self._join_th_id = th_id\n        self.state = ThreadState.JOINING\n\n    def is_waiting_to_join(self) -> bool:\n        \"\"\"\n        Checks whether the thread is waiting to join\n        another one.\n\n        :return: boolean on whether it waits for another thread\n        \"\"\"\n        return self.state == ThreadState.JOINING\n\n    def cancel_join(self) -> None:\n        \"\"\"\n        Cancel a join operation.\n\n        :return: None\n        \"\"\"\n        self._join_th_id = None\n        self.state = ThreadState.RUNNING\n\n    def is_main_thread(self) -> bool:\n        \"\"\"\n        Returns whether or not it is the main thread\n        (namely its id is 0)\n\n        :return: bool\n        \"\"\"\n        return self.tid == 0\n\n    def is_running(self) -> bool:\n        \"\"\"\n        Return if the thread is properly running or not.\n\n        :return: True if the thread is running\n        \"\"\"\n        return self.state == ThreadState.RUNNING", ""]}
{"filename": "tritondse/seed.py", "chunked_list": ["import hashlib\nimport base64\nimport json\nfrom enum import Enum\nfrom pathlib import Path\nfrom tritondse.types import PathLike, SymExType\nfrom typing import List, Dict, Union, Optional\nfrom dataclasses import dataclass, field\nimport enum_tools.documentation\n", "import enum_tools.documentation\n\n\n@enum_tools.documentation.document_enum\nclass SeedStatus(Enum):\n    \"\"\"\n    Seed status enum.\n    Enables giving a status to a seed during its execution.\n    At the end of a :py:obj:`SymbolicExecutor` run one of these\n    status must have set to the seed.\n    \"\"\"\n    NEW = 0      # doc: The input seed is new (has not been executed yet)\n    OK_DONE = 1  # doc: The input seed has been executed and terminated correctly\n    CRASH = 2    # doc: The input seed crashed in some ways\n    HANG = 3     # doc: The input seed made the program to hang", "\n\n@enum_tools.documentation.document_enum\nclass SeedFormat(Enum):\n    \"\"\"\n    Seed format enum\n    Raw seeds are just bytes Seed(b\"AAAAA\\x00BBBBB\")\n    Composite can describe how to inject the input more precisely \n    \"\"\"\n    RAW = 0        # doc: plain bytes input seed\n    COMPOSITE = 1  # doc: complex input object", "\n\n@dataclass(frozen=True)\nclass CompositeData:\n    argv: List[bytes] = field(default_factory=list)\n    \"list of argv values\"\n    files: Dict[str, bytes] = field(default_factory=dict)\n    \"dictionnary of files and the associated content (stdin is one of them)\"\n    variables: Dict[str, bytes] = field(default_factory=dict)\n    \"user defined variables, that the use must take care to inject at right location\"\n\n    def _to_json(self):\n        data = {\n            'argv': [base64.b64encode(v).decode() for v in self.argv],\n            'files': {k: (base64.b64encode(v).decode() if isinstance(v, bytes) else v) for k, v in self.files.items()},\n            'variables': {k: (base64.b64encode(v).decode() if isinstance(v, bytes) else v) for k, v in self.variables.items()},\n        }\n        return json.dumps(data, indent=2)\n\n    def __bytes__(self) -> str:\n        \"\"\"\n        Serialize data into a json string.\n\n        :return: JSON serialized data\n        \"\"\"\n        return self._to_json().encode()\n\n    @staticmethod\n    def from_dict(json_data: dict) -> 'CompositeData':\n        \"\"\"\n        Convert dict data into a :py:obj:`CompositeData` object.\n\n        :param json_data: json data\n        :return: new object instance\n        \"\"\"\n        argv = [base64.b64decode(v) for v in json_data['argv']]\n        files = {k: (base64.b64decode(v) if isinstance(v, str) else v) for k, v in json_data['files'].items()}\n        variables = {k: (base64.b64decode(v) if isinstance(v, str) else v) for k, v in json_data['variables'].items()}\n        return CompositeData(argv=argv, files=files, variables=variables)\n\n    def __hash__(self):\n        return hash(bytes(self))", "\n\nclass Seed(object):\n    \"\"\"\n    Seed input.\n    Holds the bytes buffer of the content a status after execution\n    but also some metadata of code portions it is meant to cover.\n    \"\"\"\n    def __init__(self, content: Union[bytes, CompositeData] = bytes(), status=SeedStatus.NEW):\n        \"\"\"\n        :param content: content of the input. By default is b\"\" *(and is thus considered as a bootstrap seed)*\n        :type content: bytes\n        :param status: status of the seed if already known\n        :type status: SeedStatus\n        \"\"\"\n        self.content = content\n        self.coverage_objectives = set()  # set of coverage items that the seed is meant to cover\n        self.meta_fname = []\n        self.target = None                # CovItem informational field indicate the item the seed was generated for\n        self._status = status\n        self._type = SeedFormat.COMPOSITE if isinstance(content, CompositeData) else SeedFormat.RAW\n\n    def is_composite(self) -> bool:\n        \"\"\"Returns wether the seed is a composite seed or not. \"\"\"\n        return self._type == SeedFormat.COMPOSITE\n\n    def is_raw(self) -> bool:\n        \"\"\"Returns wether the seed is a raw seed or not. \"\"\"\n        return self._type == SeedFormat.RAW\n\n    def is_bootstrap_seed(self) -> bool:\n        \"\"\"\n        A bootstrap seed is an empty seed (b\"\"). It will received a\n        specific processing in the engine as its size will be automatically\n        adapted to the size read (in stdin for instance)\n\n        :return: true if the seed is a bootstrap seed\n        \"\"\"\n        return self.content == b\"\"\n\n    def is_fresh(self) -> bool:\n        \"\"\"\n        A fresh seed is never been executed. Its is recognizable\n        as it does not contain any coverage objectives.\n\n        :return: True if the seed has never been executed\n        \"\"\"\n        return not self.coverage_objectives\n\n    @property\n    def status(self) -> SeedStatus:\n        \"\"\"\n        Status of the seed.\n\n        :rtype: SeedStatus\n        \"\"\"\n        return self._status\n\n    @property\n    def format(self) -> SeedFormat:\n        \"\"\"\n        Format of the seed.\n\n        :rtype: SeedFormat\n        \"\"\"\n        return self._type\n\n    @status.setter\n    def status(self, value: SeedStatus) -> None:\n        \"\"\" Sets the status of the seed \"\"\"\n        self._status = value\n\n    def is_status_set(self) -> bool:\n        \"\"\" Checks whether a status has already been assigned to the seed. \"\"\"\n        return self.status != SeedStatus.NEW\n\n    def __len__(self) -> int:\n        \"\"\"\n        Size of the content of the seed.\n\n        :rtype: int\n        \"\"\"\n        return len(bytes(self.content))\n\n    def __eq__(self, other) -> bool:\n        \"\"\"\n        Equality check based on content.\n\n        :returns: true if content of both seeds are equal \"\"\"\n        return self.content == other.content\n\n    def bytes(self) -> bytes:\n        return bytes(self)\n\n    def __bytes__(self) -> bytes:\n        \"\"\"\n        Return a representation of the seed's content in bytes.\n\n        :rtype: bytes\n        \"\"\"\n        return bytes(self.content)\n\n    def __hash__(self):\n        \"\"\"\n        Seed hash function overriden to base itself on content.\n        That enable storing seed in dictionnaries directly based\n        on their content to discriminate them.\n\n        :rtype: int\n        \"\"\"\n        return hash(self.content)\n\n    @property\n    def hash(self) -> str:\n        \"\"\"\n        MD5 hash of the seed content\n\n        :rtype: str\n        \"\"\"\n        m = hashlib.md5(bytes(self))\n        return m.hexdigest()\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        Size of the seed content in bytes\n\n        :rtype: int\n        \"\"\"\n        return len(bytes(self))\n\n    @property\n    def filename(self):\n        \"\"\"\n        Standardized filename based on hash and size.\n        That does not mean the file exists or anything.\n\n        :returns: formatted intended filename of the seed\n        :rtype: str\n        \"\"\"\n        return f\"{self.hash}_{self.size:04x}_{'_'.join(self.meta_fname)}.tritondse.cov\"\n\n    @staticmethod\n    def from_bytes(raw_seed: bytes, status: SeedStatus = SeedStatus.NEW) -> 'Seed':\n        \"\"\"\n        Parse a seed from its byte representation. If its a composite one\n        it will parse the bytes as JSON and create the CompositeData accordingly.\n\n        :param raw_seed: bytes: raw bytes of the seed\n        :param status: status of the seed if any, otherwise :py:obj:`SeedStatus.NEW`\n        :type status: SeedStatus\n\n        :returns: fresh seed instance\n        :rtype: Seed\n        \"\"\"\n        try:\n            data = json.loads(raw_seed)\n\n            if not isinstance(data, dict):  # it might happen that files contains only digit which is a valid JSON\n                return Seed(raw_seed, status)\n\n            if 'files' in data and 'argv' in data:\n                return Seed(CompositeData.from_dict(data), status)\n            else:  # Else still consider file as raw bytes\n                return Seed(raw_seed, status)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            return Seed(raw_seed, status)\n\n    @staticmethod\n    def from_file(path: PathLike, status: SeedStatus = SeedStatus.NEW) -> 'Seed':\n        \"\"\"\n        Read a seed from a file. The status can optionally given\n        as it cannot be determined from the file.\n\n        :param path: seed path\n        :type path: :py:obj:`tritondse.types.PathLike`\n        :param status: status of the seed if any, otherwise :py:obj:`SeedStatus.NEW`\n        :type status: SeedStatus\n\n        :returns: fresh seed instance\n        :rtype: Seed\n        \"\"\"\n        raw = Path(path).read_bytes()\n\n        seed = Seed.from_bytes(raw, status)\n\n        # Parse filename to extract back metadata if any\n        name = Path(path).name\n        if name.endswith(\".tritondse.cov\"):\n            name = name.replace(\".tritondse.cov\", \"\")\n        metas = name.split(\"_\")\n        if len(metas) >= 4:\n            seed.meta_fname = metas[3:]\n\n        return seed\n\n    # Utility function for composite seeds\n    def is_file_defined(self, name: str) -> bool:\n        if self.is_composite():\n            return name in self.content.files\n        else:\n            return False\n\n    def get_file_input(self, name: str) -> bytes:\n        \"\"\"\n        Return the bytes associated to a given file within\n        a composite seed.\n\n        :raise KeyError: if the name cannot be found in the seed.\n        :param name: name of the file to retrieve\n        :return: bytes of the file content\n        \"\"\"\n        return self.content.files[name]", ""]}
{"filename": "tritondse/qbdi_trace.py", "chunked_list": ["# This script is used by pyqbdipreload to generate a json file that can be parsed with CoverageSingleRun.from_json\n# This needs to be fast which is why we cannot import tritondse and generate the CoverageSingleRun directly \n# (`import tritondse` adds ~0.3 s to the execution time of the script in my experience).\n\n# built-in modules\nimport atexit\nimport bisect\nimport ctypes\nimport ctypes.util\nimport json", "import ctypes.util\nimport json\nimport lief\nimport os\nimport sys\nimport time\n\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Dict", "from dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Dict\n\n# Third-party modules\nimport pyqbdi\n\n\nsetjmp_data = {}\nlibdl = None\n", "libdl = None\n\n\nclass Dl_info(ctypes.Structure):\n    _fields_ = [\n        ('dli_fname', ctypes.c_char_p),\n        ('dli_fbase', ctypes.c_void_p),\n        ('dli_sname', ctypes.c_char_p),\n        ('dli_saddr', ctypes.c_void_p)\n    ]", "\n\ndef dladdr(addr):\n    res = Dl_info()\n    libdl.dladdr(ctypes.cast(addr, ctypes.c_void_p), ctypes.byref(res))\n\n    return res.dli_sname\n\n\ndef is_symbol(name, addr):\n    sname = dladdr(addr)\n    sname = sname.decode() if sname else \"\"\n\n    return sname == name", "\ndef is_symbol(name, addr):\n    sname = dladdr(addr)\n    sname = sname.decode() if sname else \"\"\n\n    return sname == name\n\n\ndef hook_post_setjmp(vm, state, gpr, fpr, data):\n    setjmp_data[data]['rip'] = gpr.rip\n\n    if setjmp_data[data]['setjmp_cbk_id'] is not None:\n        vm.deleteInstrumentation(setjmp_data[data]['setjmp_cbk_id'])\n        setjmp_data[data]['setjmp_cbk_id'] = None\n\n    return pyqbdi.CONTINUE", "def hook_post_setjmp(vm, state, gpr, fpr, data):\n    setjmp_data[data]['rip'] = gpr.rip\n\n    if setjmp_data[data]['setjmp_cbk_id'] is not None:\n        vm.deleteInstrumentation(setjmp_data[data]['setjmp_cbk_id'])\n        setjmp_data[data]['setjmp_cbk_id'] = None\n\n    return pyqbdi.CONTINUE\n\n\ndef hook_post_longjmp(vm, state, gpr, fpr, data):\n    if data in setjmp_data:\n        gpr.rip = setjmp_data[data]['rip']\n    else:\n        print(f'[FATAL] longjmp arg ({data:x}) not found!')\n        sys.exit(-1)\n\n    if setjmp_data[data]['longjmp_cbk_id'] is not None:\n        vm.deleteInstrumentation(setjmp_data[data]['longjmp_cbk_id'])\n        setjmp_data[data]['longjmp_cbk_id'] = None\n\n    return pyqbdi.CONTINUE", "\n\ndef hook_post_longjmp(vm, state, gpr, fpr, data):\n    if data in setjmp_data:\n        gpr.rip = setjmp_data[data]['rip']\n    else:\n        print(f'[FATAL] longjmp arg ({data:x}) not found!')\n        sys.exit(-1)\n\n    if setjmp_data[data]['longjmp_cbk_id'] is not None:\n        vm.deleteInstrumentation(setjmp_data[data]['longjmp_cbk_id'])\n        setjmp_data[data]['longjmp_cbk_id'] = None\n\n    return pyqbdi.CONTINUE", "\n\ndef handle_exec_transfer_call(vm, state, gpr, fpr, data):\n    if is_symbol(\"_setjmp\", gpr.rip):\n        arg1 = gpr.rdi      # get env argument.\n\n        setjmp_data[arg1] = {}\n        setjmp_data[arg1]['setjmp_cbk_id'] = vm.addVMEventCB(pyqbdi.EXEC_TRANSFER_RETURN, hook_post_setjmp, arg1)\n    elif is_symbol(\"longjmp\", gpr.rip):\n        arg1 = gpr.rdi      # get env argument.\n\n        setjmp_data[arg1]['longjmp_cbk_id'] = vm.addVMEventCB(pyqbdi.EXEC_TRANSFER_RETURN, hook_post_longjmp, arg1)\n\n    return pyqbdi.CONTINUE", "\n\n@dataclass\nclass CoverageTrace:\n    strategy: str  # BLOCK, EDGE, etc..\n    covered_instructions: Counter\n    covered_items: List[Tuple[int, int, Optional[int]]]\n    modules: Dict[str, int]\n    trace: List[int]\n", "\n\n@dataclass\nclass CoverageData:\n    strategy: str  # BLOCK, EDGE, etc..\n    branch_data: Optional[Tuple[int, int, int, bool, bool]]  # (temporary data): branch pc, true-branch, false-branch, is_taken, is_dynamic\n    trace: CoverageTrace\n    modules_base: List[int]\n    pie: bool\n    dump_trace: bool\n\n    def to_relative(self, addr: int) -> int:\n        if self.pie:\n            return addr - self.modules_base[bisect.bisect_right(self.modules_base, addr)-1]\n        else:\n            return addr", "\n\ndef get_modules() -> Dict[str, int]:\n    \"\"\" Retrieve modules base address to remove ASLR. \"\"\"\n    modules = {}\n    for m in pyqbdi.getCurrentProcessMaps(True):\n        if m.name in modules:\n            modules[m.name] = min(m.range[0], modules[m.name])\n        else:\n            modules[m.name] = m.range[0]\n        # print(f\"{m.name}: {m.range}, {m.permission}\")\n    return modules", "\n\ndef get_module_bases() -> List[int]:\n    \"\"\" Retrieve modules base address to remove ASLR. \"\"\"\n    return sorted(get_modules().values())\n\n\ndef write_coverage(covdata: CoverageData, output_file: str):\n    \"\"\"Write coverage into a file.\n    \"\"\"\n    data = {\n        \"coverage_strategy\": covdata.trace.strategy,\n        \"covered_instructions\": covdata.trace.covered_instructions,\n        \"covered_items\": covdata.trace.covered_items,\n        \"trace\": covdata.trace.trace,\n        \"modules_base\": covdata.trace.modules\n    }\n    with open(output_file, \"w\") as fd:\n        json.dump(data, fd)", "\n\ndef register_instruction_coverage(vm, gpr, fpr, data: CoverageData):\n    # inst_analysis = vm.getInstAnalysis(type=pyqbdi.AnalysisType.ANALYSIS_INSTRUCTION)\n\n    # Save instruction covered\n    rel_rip = data.to_relative(gpr.rip)\n    data.trace.covered_instructions[rel_rip] += 1  # change to be portable\n\n    if data.dump_trace:\n        # Also save the trace\n        data.trace.trace.append(rel_rip)\n\n    return pyqbdi.CONTINUE", "\n\ndef register_basic_block_coverage(vm, evt, gpr, fpr, data: CoverageData):\n    addr = evt.basicBlockStart\n\n    # Process branch data in case there is one pending.\n    if data.branch_data:\n        # Unpack branch data.\n        branch_addr, true_branch_addr, false_branch_addr, is_taken, is_dynamic = data.branch_data\n        br_a, true_a, false_a = data.to_relative(branch_addr), data.to_relative(true_branch_addr), data.to_relative(false_branch_addr)\n\n        # Check if the branch was taken.\n        taken_a, not_taken_a = (true_a, false_a) if bool(addr == true_branch_addr) else (false_a, true_a)\n\n        if is_dynamic:\n            data.trace.covered_items.append((br_a, taken_a, None))\n        else:\n            data.trace.covered_items.append((br_a, taken_a, not_taken_a))\n\n        # Clear branch data for next occurrence.\n        data.branch_data = None\n    else:\n        pass\n        # FIXME: There is a problem in that script is_dynamic can never be true !\n        # FIXME: as it's never set to true. I feel like the else here is the case where its dynamic?\n\n    return pyqbdi.CONTINUE", "\n\ndef register_branch_coverage(vm, gpr, fpr, data):\n    inst_analysis = vm.getInstAnalysis(type=pyqbdi.AnalysisType.ANALYSIS_INSTRUCTION | pyqbdi.AnalysisType.ANALYSIS_OPERANDS)\n\n    operand = inst_analysis.operands[0]\n\n    branch_addr = inst_analysis.address\n    false_branch_addr = inst_analysis.address + inst_analysis.instSize\n\n    if operand.type == pyqbdi.OperandType.OPERAND_IMM:\n        # FIXME: Isn't it assuming the jump is relative ?\n        true_branch_addr = inst_analysis.address + inst_analysis.instSize + ctypes.c_longlong(operand.value).value\n    else:\n        raise Exception('Invalid operand type')\n\n    # Save current branch data\n    data.branch_data = (branch_addr, true_branch_addr, false_branch_addr, None, False)\n\n    return pyqbdi.CONTINUE", "\n\ndef pyqbdipreload_on_run(vm, start, stop):\n    global libdl\n\n    s = time.time()\n\n    # Load dl library.\n    libdl_path = ctypes.util.find_library('dl')\n    if libdl_path is None:\n        raise Exception('Unable to found dl library')\n    libdl = ctypes.cdll.LoadLibrary(libdl_path)\n    libdl.dladdr.argtypes = (ctypes.c_void_p, ctypes.POINTER(Dl_info))\n\n    # Read parameters.\n    strat = os.getenv('PYQBDIPRELOAD_COVERAGE_STRATEGY', 'BLOCK')\n    output = os.getenv('PYQBDIPRELOAD_OUTPUT_FILEPATH', 'a.cov')\n    bool_trace = os.getenv('PYQBDIPRELOAD_DUMP_TRACE', 'False')\n    bool_trace = True if bool_trace in ['true', 'True'] else False\n\n    mods = get_modules()\n    base_addresses = sorted(get_modules().values())\n    covtrace = CoverageTrace(strat, Counter(), [], mods, [])\n\n    # Open binary in LIEF to check if PIE or not\n    p = lief.parse(sys.argv[0])\n\n    coverage_data = CoverageData(strat, None, covtrace, base_addresses, p.is_pie, bool_trace)\n\n    # Remove all instrumented modules except the main one.\n    vm.removeAllInstrumentedRanges()\n    vm.addInstrumentedModuleFromAddr(start)\n\n    if coverage_data.strategy == 'BLOCK' or bool_trace:\n        # Add callback on instruction execution.\n        vm.addCodeCB(pyqbdi.PREINST, register_instruction_coverage, coverage_data)\n\n    # Add callback on basic block entry.\n    vm.addVMEventCB(pyqbdi.BASIC_BLOCK_ENTRY, register_basic_block_coverage, coverage_data)\n\n    # Add callback on the JCC mnemonic.\n    vm.addMnemonicCB('JCC', pyqbdi.InstPosition.POSTINST, register_branch_coverage, coverage_data)\n\n    # Write coverage on exit.\n    # TODO This does not work with bins that crash.\n    atexit.register(write_coverage, coverage_data, output)\n\n    # Add callback for handling calls to functions setjmp and longjmp.\n    vm.addVMEventCB(pyqbdi.EXEC_TRANSFER_CALL, handle_exec_transfer_call, None)\n\n    # Run program.\n    print(\"Run start\")\n    vm.run(start, stop)\n    print(f\"Run finished: {time.time() - s:.02f}s\")", ""]}
{"filename": "tritondse/process_state.py", "chunked_list": ["# built-ins\nfrom __future__ import annotations\n\nimport io\nimport struct\nimport sys\nimport time\nfrom typing import Union, Callable, Tuple, Optional, List, Dict\n\n", "\n\n# third-party\n# import z3  # For direct value enumeration\nfrom triton import TritonContext, MemoryAccess, CALLBACK, CPUSIZE, Instruction, MODE, AST_NODE, SOLVER, EXCEPTION\n\n# local imports\nfrom tritondse.thread_context import ThreadContext\nfrom tritondse.heap_allocator import HeapAllocator\nfrom tritondse.types import Architecture, Addr, ByteSize, BitSize, PathConstraint, Register, Expression, \\", "from tritondse.heap_allocator import HeapAllocator\nfrom tritondse.types import Architecture, Addr, ByteSize, BitSize, PathConstraint, Register, Expression, \\\n                            AstNode, Registers, SolverStatus, Model, SymbolicVariable, ArchMode, Perm, FileDesc, Endian\nfrom tritondse.arch import ARCHS, CpuState\nfrom tritondse.loaders import Loader\nfrom tritondse.memory import Memory, MemoryAccessViolation\nimport tritondse.logging\n\nlogger = tritondse.logging.get()\n", "logger = tritondse.logging.get()\n\n\nclass ProcessState(object):\n    \"\"\"\n    Current process state. This class keeps all the runtime related to a running\n    process, namely current, instruction, thread, memory maps, file descriptors etc.\n    It also wraps Triton execution and thus hold its context. At the top of this,\n    it provides a user-friendly API to access data in both the concrete and symbolic\n    state of Triton.\n    \"\"\"\n\n    STACK_SEG = \"[stack]\"\n    EXTERN_SEG = \"[extern]\"\n\n    def __init__(self, endianness: Endian = Endian.LITTLE, time_inc_coefficient: float = 0.0001):\n        \"\"\"\n        :param endianness: Endianness to consider\n        :param time_inc_coefficient: Time coefficient to represent execution time of an\n                                     instruction see: :py:attr:`tritondse.Config.time_inc_coefficient`\n        \"\"\"\n        # EXTERN_BASE is a \"fake\" memory area (not mapped) that will\n        # which addresses will be used for external symbols\n        self.EXTERN_FUNC_BASE = 0x01000000  # Not PLT but a dummy address space containing pointers to external symbols\n\n        # This range will be dynamically allocated\n        # upon request.\n        self.BASE_HEAP = 0x10000000\n        self.END_HEAP = 0x6fffffff\n\n        # The Triton's context\n        self.tt_ctx = TritonContext()\n        \"\"\"TritonContext object\"\"\"\n        self.actx: 'AstContext' = self.tt_ctx.getAstContext()\n        \"\"\"\n        Triton `AstContext <https://triton-library.github.io/documentation/doxygen/py_AstContext_page.html>`_\n        enabling crafting logical expressions to be solved by SMT\n        \"\"\"\n\n        # Cpu object wrapping registers values\n        self.cpu: CpuState = None  #: CpuState holding concrete values of registers *(initialized when calling load)*\n        self._archinfo = None\n\n        # Memory object\n        self.memory: Memory = Memory(self.tt_ctx, endianness)\n        \"\"\"Memory object associated with the ProcessState \"\"\"\n\n        # Used to define that the process must exist\n        self.stop = False\n\n        # Signals table used by raise(), signal(), etc.\n        #self.signals_table = dict()\n\n        # Dynamic symbols name -> addr (where they are mapped)\n        self.dynamic_symbol_table: Dict[str, Tuple[Addr, bool]] = {}\n        \"\"\"Dictionnary of dynamic symbols as retrieved during the loading\"\"\"\n\n        # File descriptors table used by fopen(), fprintf(), etc.\n        self._fd_table = {\n            0: FileDesc(0, \"stdin\", sys.stdin),\n            1: FileDesc(1, \"stdout\", sys.stdout),\n            2: FileDesc(2, \"stderr\", sys.stderr),\n        }\n        # Unique file id incrementation\n        self._fd_id = len(self._fd_table)\n\n        # Allocation information used by malloc()\n        self.heap_allocator: HeapAllocator = HeapAllocator(self.BASE_HEAP, self.END_HEAP, self.memory)\n        \"\"\"Allocator providing alloc, free primitives atop the Memory object\"\"\"\n\n        # Unique thread id incrementation\n        self._utid = 0\n\n        # Current thread id\n        self._tid = self._utid\n\n        # Threads contexts\n        self._threads = {\n            self._tid: ThreadContext(self._tid)\n        }\n\n        # Thread mutext init magic number\n        self.PTHREAD_MUTEX_INIT_MAGIC = 0xdead\n\n        # Mutex and semaphore\n        self.mutex_locked = False\n        self.semaphore_locked = False\n\n        # The time when the ProcessState is instancied.\n        # It's used to provide a deterministic behavior when calling functions\n        # like gettimeofday(), clock_gettime(), etc.\n        self.time = time.time()\n\n        # Configuration values\n        self.endianness = endianness  #: Current endianness\n        self.time_inc_coefficient = time_inc_coefficient\n\n        # Runtime temporary variables\n        self.__pcs_updated = False\n\n        # The current instruction executed\n        self.__current_inst = None\n\n        # The memory mapping of the program ({vaddr_s : vaddr_e})\n        self.__program_segments_mapping = {}\n\n    @property\n    def threads(self) -> List[ThreadContext]:\n        \"\"\"\n        Gives a list of all threads currently active.\n\n        :return:\n        \"\"\"\n        return list(self._threads.values())\n\n    @property\n    def current_thread(self) -> ThreadContext:\n        \"\"\"\n        Gives the current thread selected.\n\n        :return: current thread\n        :rtype: ThreadContext\n        \"\"\"\n        return self._threads[self._tid]\n\n    def switch_thread(self, thread: ThreadContext) -> bool:\n        \"\"\"\n        Change the current thread to the one given in parameter.\n        Thus save the current context, and restore the one of the\n        thread given in parameter. It also resets the counter of\n        the thread restored. If the current_thread is dead, it will\n        also remove it !\n\n        :param thread: thread to restore ThreadContext\n        :return: True if the switch worked fine\n        \"\"\"\n        assert (thread.tid in self._threads)\n\n        try:\n            if self.current_thread.is_dead():\n                del self._threads[self._tid]\n                # TODO: Finding all other threads joining it / (or locked by it ?) to unlock them\n            else:  # Do a normal switch\n                # Reset the counter and save its context\n                self.current_thread.save(self.tt_ctx)\n\n            # Schedule to the next thread\n            thread.count = 0  # Reset the counter\n            thread.restore(self.tt_ctx)\n            self._tid = thread.tid\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error while doing context switch: {e}\")\n            return False\n\n    def spawn_new_thread(self, new_pc: Addr, args: Addr) -> ThreadContext:\n        \"\"\"\n        Create a new thread in the process state. Parameters are the\n        new program counter and a pointer to arguments to provide the thread.\n\n        :param new_pc: new program counter (function to execution)\n        :param args: arguments\n        :return: thread context newly created\n        \"\"\"\n        tid = self._get_unique_thread_id()\n        thread = ThreadContext(tid)\n        thread.save(self.tt_ctx)\n\n        # Concretize pc, bp, sp, and first (argument)\n        regs = [self.program_counter_register, self.stack_pointer_register, self.base_pointer_register, self._get_argument_register(0)]\n        for reg in regs:\n            if reg.getId() in thread.sregs:\n                del thread.sregs[reg.getId()]\n\n        thread.cregs[self.program_counter_register.getId()] = new_pc  # set new pc\n        thread.cregs[self._get_argument_register(0).getId()] = args   # set args pointer\n        stack = self.memory.map_from_name(self.STACK_SEG)\n        thread.cregs[self.base_pointer_register.getId()] = ((stack.start+stack.size) - ((1 << 28) * tid))\n        thread.cregs[self.stack_pointer_register.getId()] = ((stack.start+stack.size) - ((1 << 28) * tid))\n\n        # Add the thread in the pool of threads\n        self._threads[tid] = thread\n        return thread\n\n    def set_triton_mode(self, mode: MODE, value: int = True) -> None:\n        \"\"\"\n        Set the given mode in the TritonContext.\n\n        :param mode: mode to set in triton context\n        :param value: value to set (default True)\n        \"\"\"\n        self.tt_ctx.setMode(mode, value)\n\n    def set_thumb(self, enable: bool) -> None:\n        \"\"\"\n        Set thumb mode activated in the TritonContext. The mode will automatically\n        be switched during execution, but at initialization this method enable\n        activating it / disabling it. (Disabled be default)\n\n        :param enable: bool: Wether or not to active thumb\n        \"\"\"\n        self.tt_ctx.setThumb(enable)\n\n    def set_solver_timeout(self, timeout: int) -> None:\n        \"\"\"\n        Set the timeout for all subsequent queries.\n\n        :param timeout: timeout in milliseconds\n        \"\"\"\n        self.tt_ctx.setSolverTimeout(timeout)\n\n    def set_solver(self, solver: Union[str, SOLVER]) -> None:\n        \"\"\"\n        Set the SMT solver to use in the background.\n\n        :param solver: Solver to use\n        \"\"\"\n        if isinstance(solver, str):\n            solver = getattr(SOLVER, solver.upper(), SOLVER.Z3)\n        self.tt_ctx.setSolver(solver)\n\n    def _get_unique_thread_id(self) -> int:\n        \"\"\"\n        Return a new unique thread id. Used by thread related functions when spawning a new thread.\n\n        :returns: new thread identifier\n        \"\"\"\n        self._utid += 1\n        return self._utid\n\n    def create_file_descriptor(self, name: str, file: io.IOBase) -> FileDesc:\n        \"\"\"\n        Create a new file descriptor out of a name.\n\n        :param name: name of the file\n        :param file: object to read from\n        :return: FileDesc object\n        \"\"\"\n        new_fd_id = self._fd_id\n        self._fd_id += 1\n        filedesc = FileDesc(id=new_fd_id, name=name, fd=file)\n        self._fd_table[new_fd_id] = filedesc\n        return filedesc\n\n    def close_file_descriptor(self, id: int) -> None:\n        \"\"\"\n        Close the given file descriptor id.\n\n        :param id: id of the file descriptor\n        :return: None\n        \"\"\"\n        filedesc = self._fd_table.pop(id)\n        if isinstance(filedesc.fd, io.IOBase):\n            filedesc.fd.close()\n\n    def get_file_descriptor(self, id: int) -> FileDesc:\n        \"\"\"\n        Get the given file descriptor.\n\n        :raise KeyError: if the file descriptor is not found\n        :param id: id of the file descriptor\n        :return: FileDesc object\n        \"\"\"\n        return self._fd_table[id]\n\n    def file_descriptor_exists(self, id: int) -> bool:\n        \"\"\"\n        Returns whether the file descriptor has been defined or not.\n\n        :param id: id of the file descriptor\n        :return: True if the id is found\n        \"\"\"\n        return bool(id in self._fd_table)\n\n    @property\n    def architecture(self) -> Architecture:\n        \"\"\"\n        Architecture of the current process state\n\n        :return: Architecture set\n        \"\"\"\n        return Architecture(self.tt_ctx.getArchitecture())\n\n    @architecture.setter\n    def architecture(self, arch: Architecture) -> None:\n        \"\"\"\n        Set the architecture of the process state.\n        Internal set it in the TritonContext\n\n        :param arch: Architecture to set\n        \"\"\"\n        self.tt_ctx.setArchitecture(arch)\n\n    @property\n    def ptr_size(self) -> ByteSize:\n        \"\"\"\n        Size of a pointer in bytes\n\n        :rtype: :py:obj:`tritondse.types.ByteSize`\n        \"\"\"\n        return self.tt_ctx.getGprSize()\n\n    @property\n    def ptr_bit_size(self) -> BitSize:\n        \"\"\"\n        Size of a pointer in bits\n\n        :rtype: :py:obj:`tritondse.types.BitSize`\n        \"\"\"\n        return self.tt_ctx.getGprBitSize()\n\n    @property\n    def minus_one(self) -> int:\n        \"\"\"\n        Value -1 according to the architecture size (32 or 64 bits)\n\n        :return: -1 as an unsigned Python integer\n        \"\"\"\n        return (1 << self.ptr_bit_size) - 1\n\n    @property\n    def registers(self) -> Registers:\n        \"\"\"\n        All registers according to the current architecture defined.\n        The object returned is the TritonContext.register object.\n\n        :rtype: :py:obj:`tritondse.types.Registers`\n        \"\"\"\n        return self.tt_ctx.registers\n\n    @property\n    def return_register(self) -> Register:\n        \"\"\"\n        Return the appropriate return register according to the arch.\n\n        :rtype: :py:obj:`tritondse.types.Register`\n        \"\"\"\n        return getattr(self.registers, self._archinfo.ret_reg)\n\n    @property\n    def program_counter_register(self) -> Register:\n        \"\"\"\n        Return the appropriate pc register according to the arch.\n\n        :rtype: :py:obj:`tritondse.types.Register`\n        \"\"\"\n        return getattr(self.registers, self._archinfo.pc_reg)\n\n    @property\n    def base_pointer_register(self) -> Register:\n        \"\"\"\n        Return the appropriate base pointer register according to the arch.\n\n        :rtype: :py:obj:`tritondse.types.Register`\n        \"\"\"\n        return getattr(self.registers, self._archinfo.bp_reg)\n\n    @property\n    def stack_pointer_register(self) -> Register:\n        \"\"\"\n        Return the appropriate stack pointer register according to the arch.\n\n        :rtype: :py:obj:`tritondse.types.Register`\n        \"\"\"\n        return getattr(self.registers, self._archinfo.sp_reg)\n\n    @property\n    def _syscall_register(self) -> Register:\n        \"\"\" Return the appropriate syscall id register according to the arch \"\"\"\n        return getattr(self.registers, self._archinfo.sys_reg)\n\n    def _get_argument_register(self, i: int) -> Register:\n        \"\"\"\n        Return the appropriate register according to the arch.\n\n        :raise: IndexError If the index is out of arguments bound\n        :return: Register\n        \"\"\"\n        return getattr(self.registers, self._archinfo.reg_args[i])\n\n    def initialize_context(self, arch: Architecture):\n        \"\"\"\n        Initialize the context with the given architecture\n\n        .. todo:: Protecting that function\n\n        :param arch: The architecture to initialize\n        :type arch: Architecture\n        :return: None\n        \"\"\"\n        self.architecture = arch\n        self._archinfo = ARCHS[self.architecture]\n        self.cpu = CpuState(self.tt_ctx, self._archinfo)\n\n    def unpack_integer(self, data: bytes, size: int) -> int:\n        \"\"\"\n        Unpack the given bytes into into integer value respecting\n        size given and endianness.\n\n        :param data: bytes data to unpack\n        :param size: size in bits of data to unpack\n        :return: integer value unpacked\n        \"\"\"\n        s = \"<\" if self.endianness == Endian.LITTLE else \">\"\n        tab = {8: 'B', 16: 'H', 32: 'I', 64: 'Q'}\n        s += tab[size]\n        return struct.unpack(s, data)[0]\n\n    def pack_integer(self, value: int, size: int) -> bytes:\n        \"\"\"\n        Unpack the given bytes into into integer value respecting\n        size given and endianness.\n\n        :param data: bytes data to unpack\n        :param size: size in bits of data to unpack\n        :return: integer value packed as bytes\n        \"\"\"\n        s = \"<\" if self.endianness == Endian.LITTLE else \">\"\n        tab = {8: 'B', 16: 'H', 32: 'I', 64: 'Q'}\n        s += tab[size]\n        return struct.pack(s, value)\n\n    def read_register(self, register: Union[str, Register]) -> int:\n        \"\"\"\n        Read the current concrete value of the given register.\n\n        :param register: string of the register or Register object\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :return: Integer value\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register  # if str transform to reg\n        return self.tt_ctx.getConcreteRegisterValue(reg)\n\n    def write_register(self, register: Union[str, Register], value: int) -> None:\n        \"\"\"\n        Read the current concrete value of the given register.\n\n        :param register: string of the register or Register object\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :param value: integer value to assign in the register\n        :type value: int\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register  # if str transform to reg\n        return self.tt_ctx.setConcreteRegisterValue(reg, value)\n\n    def register_triton_callback(self, cb_type: CALLBACK, callback: Callable) -> None:\n        \"\"\"\n        Register the given ``callback`` as triton callback to hook memory/registers\n        read/writes.\n\n        :param cb_type: Callback enum type as defined by Triton\n        :type cb_type: `CALLBACK <https://triton.quarkslab.com/documentation/doxygen/py_CALLBACK_page.html>`_\n        :param callback: routines to call on the given event\n        \"\"\"\n        self.tt_ctx.addCallback(cb_type, callback)\n\n    def clear_triton_callbacks(self) -> None:\n        \"\"\"\n        Remove all registered callbacks in triton.\n        \"\"\"\n        self.tt_ctx.clearCallbacks()\n\n    def is_heap_ptr(self, ptr: Addr) -> bool:\n        \"\"\"\n        Check whether a given address is pointing in the heap area.\n\n        :param ptr: Address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :return: True if pointer points to the heap area *(allocated or not)*.\n        \"\"\"\n        if self.BASE_HEAP <= ptr < self.END_HEAP:\n            return True\n        return False\n\n    def is_syscall(self) -> bool:\n        \"\"\"\n        Check whether the current instrution fetched is a syscall or not.\n        \"\"\"\n        return bool(self.current_instruction.getType() in self._archinfo.syscall_inst)\n\n    def fetch_instruction(self, address: Addr = None, set_as_current: bool = True, disable_callbacks: bool = True) -> Instruction:\n        \"\"\"\n        Fetch the instruction at the given address. If no address\n        is specified the current program counter one is used.\n\n        :raise MemoryAccessViolation: If the instruction cannot be fetched in the memory.\n\n        :param address: address where to get the instruction from\n        :param set_as_current: set as the current instruction in the process state\n        :param disable_callbacks: whether memory callbacks should be disabled to fetch memory bytes\n        :return: instruction disassembled\n        \"\"\"\n        if address is None:\n            address = self.cpu.program_counter\n        with self.memory.without_segmentation(disable_callbacks=disable_callbacks):\n            data = self.memory.read(address, 16)\n        i = Instruction(address, data)\n        i.setThreadId(self.current_thread.tid)\n        self.tt_ctx.disassembly(i)  # This needs to be done before using i.getSize()\n                                    # otherwise, i.getSize() will always be 16\n\n        if self.memory.segmentation_enabled:\n            map = self.memory.get_map(address, i.getSize())\n            if map is None:\n                raise MemoryAccessViolation(address, Perm.X, memory_not_mapped=True)\n            if Perm.X not in map.perm:  # Note: in this model we can execute code in non-readable pages\n                raise MemoryAccessViolation(address, Perm.X, map_perm=map.perm, perm_error=True)\n        if set_as_current:\n            self.__current_inst = i\n        return i\n\n    def process_instruction(self, instruction: Instruction) -> bool:\n        \"\"\"\n        Process the given triton instruction on this process state.\n\n        :param instruction: Triton Instruction object\n        :type instruction: `Instruction <https://triton.quarkslab.com/documentation/doxygen/py_Instruction_page.html>`_\n        :return: True if the processing of the instruction succeeded (False otherwise)\n        \"\"\"\n        self.__pcs_updated = False\n        __len_pcs = self.tt_ctx.getPathPredicateSize()\n\n        if not instruction.getDisassembly():  # If the insrtuction has not been disassembled\n            self.tt_ctx.disassembly(instruction)\n\n        self.__current_inst = instruction\n        ret = self.tt_ctx.buildSemantics(instruction)\n\n        # Simulate that the time of an executed instruction is time_inc_coefficient.\n        # For example, if time_inc_coefficient is 0.0001, it means that an instruction\n        # takes 100us to be executed. Used to provide a deterministic behavior when\n        # calling time functions (e.g gettimeofday(), clock_gettime(), ...).\n        self.time += self.time_inc_coefficient\n\n        if self.tt_ctx.getPathPredicateSize() > __len_pcs:\n            self.__pcs_updated = True\n\n        return ret == EXCEPTION.NO_FAULT\n\n    @property\n    def path_predicate_size(self) -> int:\n        \"\"\"\n        Get the size of the path predicate (conjonction\n        of all branches and additionnals constraints added)\n\n        :return: size of the predicate\n        \"\"\"\n        return self.tt_ctx.getPathPredicateSize()\n\n    def is_path_predicate_updated(self) -> bool:\n        \"\"\" Return whether or not the path predicate has been updated \"\"\"\n        return self.__pcs_updated\n\n    @property\n    def last_branch_constraint(self) -> PathConstraint:\n        \"\"\"\n        Return the last PathConstraint object added in the path predicate.\n        Should be called after :py:meth:`is_path_predicate_updated`.\n\n        :raise IndexError: if the path predicate is empty\n        :return: the path constraint object as returned by Triton\n        :rtype: `PathConstraint <https://triton.quarkslab.com/documentation/doxygen/py_PathConstraint_page.html>`_\n        \"\"\"\n        return self.tt_ctx.getPathConstraints()[-1]\n\n    @property\n    def current_instruction(self) -> Optional[Instruction]:\n        \"\"\"\n        The current instruction being executed. *(None if not set yet)*\n\n        :rtype: Optional[`Instruction <https://triton.quarkslab.com/documentation/doxygen/py_Instruction_page.html>`_]\n        \"\"\"\n        return self.__current_inst\n\n    def is_register_symbolic(self, register: Union[str, Register]) -> bool:\n        \"\"\"\n        Check whether the register is symbolic or not.\n\n        :param register: register string, or Register object\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :return: True if the register is symbolic\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register\n        return self.tt_ctx.getRegisterAst(reg).isSymbolized()\n\n    def read_symbolic_register(self, register: Union[str, Register]) -> Expression:\n        \"\"\"\n        Get the symbolic expression associated with the given register.\n\n        :param register: register string, or Register object\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :return: SymbolicExpression of the register as returned by Triton\n        :rtype: `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register  # if str transform to reg\n        sym_reg = self.tt_ctx.getSymbolicRegister(reg)\n\n        if sym_reg is None or reg.getBitSize() != sym_reg.getAst().getBitvectorSize():\n            return self.tt_ctx.newSymbolicExpression(self.tt_ctx.getRegisterAst(reg))\n        else:\n            return sym_reg\n\n    def write_symbolic_register(self, register: Union[str, Register], expr: Union[AstNode, Expression], comment: str = \"\") -> None:\n        \"\"\"\n        Assign the given symbolic expression to the register. The given expression can either be a SMT AST node\n        or directly an Expression (SymbolicExpression).\n\n        :param register: register identifier (str or Register)\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :param expr: expression to assign (`AstNode <https://triton.quarkslab.com/documentation/doxygen/py_AstNode_page.html>`_\n               or `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_)\n        :param comment: Comment to add on the symbolic expression created\n        :type comment: str\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register  # if str transform to reg\n        exp = expr if hasattr(expr, \"getAst\") else self.tt_ctx.newSymbolicExpression(expr, f\"assign {reg.getName()}: {comment}\")\n        self.write_register(reg, exp.getAst().evaluate())  # Update concrete state to keep sync\n        self.tt_ctx.assignSymbolicExpressionToRegister(exp, reg)\n\n    def read_symbolic_memory_int(self, addr: Addr, size: ByteSize) -> Expression:\n        \"\"\"\n        Return a new Symbolic Expression representing the whole memory range given in parameter.\n        That function should not be used on big memory chunks.\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: memory size in bytes\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :raise RuntimeError: If the size if not aligned\n        :return: Symbolic Expression associated with the memory\n        :rtype: `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_\n        \"\"\"\n        if size == 1:\n            return self.read_symbolic_memory_byte(addr)\n        elif size in [2, 4, 8, 16, 32, 64]:\n            ast = self.tt_ctx.getMemoryAst(MemoryAccess(addr, size))\n            return self.tt_ctx.newSymbolicExpression(ast)\n        else:\n            raise RuntimeError(\"size should be aligned [1, 2, 4, 8, 16, 32, 64] (bytes)\")\n\n    def read_symbolic_memory_byte(self, addr: Addr) -> Expression:\n        \"\"\"\n        Thin wrapper to retrieve the symbolic expression of a single bytes in memory.\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :return: Symbolic Expression associated with the memory\n        :rtype: `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_\n        \"\"\"\n        res = self.tt_ctx.getSymbolicMemory(addr)\n        if res is None:\n            return self.tt_ctx.newSymbolicExpression(self.tt_ctx.getMemoryAst(MemoryAccess(addr, 1)))\n        else:\n            return res\n\n    def read_symbolic_memory_bytes(self, addr: Addr, size: ByteSize) -> Expression:\n        \"\"\"\n        Return a new Symbolic Expression representing the whole memory range given in parameter.\n        That function should not be used on big memory chunks.\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: memory size in bytes\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :return: Symbolic Expression associated with the memory\n        :rtype: `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_\n        \"\"\"\n        if size == 1:\n            return self.read_symbolic_memory_byte(addr)\n        else:  # Need to create a per-byte expression with concat\n            asts = [self.tt_ctx.getMemoryAst(MemoryAccess(addr+i, CPUSIZE.BYTE)) for i in range(size)]\n            concat_expr = self.actx.concat(asts)\n            return self.tt_ctx.newSymbolicExpression(concat_expr)\n\n    def write_symbolic_memory_int(self, addr: Addr, size: ByteSize, expr: Union[AstNode, Expression]) -> None:\n        \"\"\"\n        Assign the given symbolic expression representing an integer to the given address.\n        That function should not be used on big memory chunks.\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: memory size in bytes\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :param expr: expression to assign (`AstNode <https://triton.quarkslab.com/documentation/doxygen/py_AstNode_page.html>`_\n                     or `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_)\n        :raise RuntimeError: if the size if not aligned\n        \"\"\"\n        expr = expr if hasattr(expr, \"getAst\") else self.tt_ctx.newSymbolicExpression(expr, f\"assign memory\")\n        if size in [1, 2, 4, 8, 16, 32, 64]:\n            self.tt_ctx.setConcreteMemoryValue(MemoryAccess(addr, size), expr.getAst().evaluate())  # To keep the concrete state synchronized\n            self.tt_ctx.assignSymbolicExpressionToMemory(expr, MemoryAccess(addr, size))\n        else:\n            raise RuntimeError(\"size should be aligned [1, 2, 4, 8, 16, 32, 64] (bytes)\")\n\n    def write_symbolic_memory_byte(self, addr: Addr, expr: Union[AstNode, Expression]) -> None:\n        \"\"\"\n        Set a single bytes symbolic at the given address\n\n        .. NOTE: We purposefully not provide a way to assign in memory a symbolic expression of\n           arbitrary size as it would imply doing many extract on the given expression. For buffer\n           you should do it in a per-byte manner with this method.\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param expr: byte expression to assign (`AstNode <https://triton.quarkslab.com/documentation/doxygen/py_AstNode_page.html>`_\n                     or `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_)\n        \"\"\"\n        expr = expr if hasattr(expr, \"getAst\") else self.tt_ctx.newSymbolicExpression(expr, f\"assign memory\")\n        ast = expr.getAst()\n        assert ast.getBitvectorSize() == 8\n        self.tt_ctx.setConcreteMemoryValue(MemoryAccess(addr, CPUSIZE.BYTE), ast.evaluate())  # Keep concrete state synced\n        self.tt_ctx.assignSymbolicExpressionToMemory(expr, MemoryAccess(addr, CPUSIZE.BYTE))\n\n    def is_memory_symbolic(self, addr: Addr, size: ByteSize) -> bool:\n        \"\"\"\n        Iterate the symbolic memory and returns whether or not at least one byte of the buffer\n        is symbolic\n\n        :param addr: Memory address\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: size of the memory range to check\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :return: True if at least one byte of the memory is symbolic, false otherwise\n        \"\"\"\n        for i in range(addr, addr+size):\n            if self.tt_ctx.isMemorySymbolized(MemoryAccess(i, 1)):\n                return True\n        return False\n\n    def push_constraint(self, constraint: AstNode, comment: str = \"\") -> None:\n        \"\"\"\n        Thin wrapper on the triton context underneath to add a path constraint.\n\n        :param constraint: Constraint expression to add\n        :type constraint: `AstNode <https://triton.quarkslab.com/documentation/doxygen/py_AstNode_page.html>`_\n        :param comment: String comment to attach to the constraint\n        :type comment: str\n        \"\"\"\n        self.tt_ctx.pushPathConstraint(constraint, comment)\n\n    def get_path_constraints(self) -> List[PathConstraint]:\n        \"\"\"\n        Get the list of all path constraints set in the Triton context.\n\n        :return: list of constraints\n        \"\"\"\n        return self.tt_ctx.getPathConstraints()\n\n    def concretize_register(self, register: Union[str, Register]) -> None:\n        \"\"\"\n        Concretize the given register with its current concrete value.\n        **This operation is sound** as it will also add a path constraint\n        to enforce that the symbolic register value is equal to its concrete\n        value.\n\n        :param register: Register identifier (str or Register)\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register\n        if self.tt_ctx.isRegisterSymbolized(reg):\n            value = self.read_register(reg)\n            self.push_constraint(self.read_symbolic_register(reg).getAst() == value)\n        # Else do not even push the constraint\n\n    def concretize_memory_bytes(self, addr: Addr, size: ByteSize) -> None:\n        \"\"\"\n        Concretize the given memory with its current concrete value.\n        **This operation is sound** and allows restraining the memory\n        value to its constant value.\n\n        :param addr: Address to concretize\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: Size of the integer to concretize\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        \"\"\"\n        data = self.memory.read(addr, size)\n        if self.is_memory_symbolic(addr, size):\n            self.push_constraint(self.read_symbolic_memory_bytes(addr, size).getAst() == data)\n        # else do not even push the constraint\n\n    def concretize_memory_int(self, addr: Addr, size: ByteSize) -> None:\n        \"\"\"\n        Concretize the given memory with its current concrete value.\n        **This operation is sound** and allows restraining the memory\n        value to its constant value.\n\n        :param addr: Address to concretize\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: Size of the integer to concretize\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        \"\"\"\n        value = self.memory.read_uint(addr, size)\n        if self.tt_ctx.isMemorySymbolized(MemoryAccess(addr, size)):\n            self.push_constraint(self.read_symbolic_memory_int(addr, size).getAst() == value)\n        # else do not even push the constraint\n\n    def concretize_argument(self, index: int) -> None:\n        \"\"\"\n        Concretize the given function parameter following the calling convention\n        of the architecture.\n\n        :param index: Argument index\n        :type index: int\n        \"\"\"\n        try:\n            self.concretize_register(self._get_argument_register(index))\n        except IndexError:\n            len_args = len(self._archinfo.reg_args)\n            addr = self.cpu.stack_pointer + self.ptr_size + ((index-len_args) * self.ptr_size)  # Retrieve stack address\n            self.concretize_memory_int(addr, self.ptr_size)                     # Concretize the value at this addr\n\n    def write_argument_value(self, i: int, val: int) -> None:\n        \"\"\"\n        Write the parameter index with the given value. It will take in account\n        whether the argument is in a register or the stack.\n\n        :param i: Ith argument of the function\n        :param val: integer value of the parameter\n        :return: None\n        \"\"\"\n        try:\n            return self.write_register(self._get_argument_register(i), val)\n        except IndexError:\n            len_args = len(self._archinfo.reg_args)\n            return self.write_stack_value(i-len_args, val, offset=1)\n\n    def get_argument_value(self, i: int) -> int:\n        \"\"\"\n        Get the integer value of parameters following the call convention.\n        The value originate either from a register or the stack depending\n        on the ith argument requested.\n\n        :param i: Ith argument of the function\n        :type i: int\n        :return: integer value of the parameter\n        :rtype: int\n        \"\"\"\n        try:\n            return self.read_register(self._get_argument_register(i))\n        except IndexError:\n            len_args = len(self._archinfo.reg_args)\n            return self.get_stack_value(i-len_args, offset=1)\n\n    def get_argument_symbolic(self, i: int) -> Expression:\n        \"\"\"\n        Return the symbolic expression associated with the given ith parameter.\n\n        :param i: Ith function parameter\n        :return: Symbolic expression associated\n        :rtype: `SymbolicExpression <https://triton.quarkslab.com/documentation/doxygen/py_SymbolicExpression_page.html>`_\n        \"\"\"\n        try:\n            return self.read_symbolic_register(self._get_argument_register(i))\n        except IndexError:\n            len_args = len(self._archinfo.reg_args)\n            addr = self.cpu.stack_pointer + ((i-len_args) * self.ptr_size)\n            return self.read_symbolic_memory_int(addr, self.ptr_size)\n\n    def get_full_argument(self, i: int) -> Tuple[int, Expression]:\n        \"\"\"\n        Get both the concrete argument value along with its symbolic expression.\n\n        :return: Tuple containing concrete value and symbolic expression\n        \"\"\"\n        return self.get_argument_value(i), self.get_argument_symbolic(i)\n\n    def get_string_argument(self, idx: int) -> str:\n        \"\"\"Read a string for which address is a function parameter.\n        The function first get the argument value, and then dereference\n        the string located at that address.\n\n        :param idx: argument index\n        :type idx: int\n        :returns: memory string\n        :rtype: str\n        \"\"\"\n        return self.memory.read_string(self.get_argument_value(idx))\n\n    def get_format_string(self, addr: Addr) -> str:\n        \"\"\"\n        Returns a formatted string in Python format from a format string\n        located in memory at ``addr``.\n\n        :param addr: Address to concretize\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :rtype: str\n        \"\"\"\n        return self.memory.read_string(addr)                                             \\\n               .replace(\"%s\", \"{}\").replace(\"%d\", \"{}\").replace(\"%#02x\", \"{:#02x}\")     \\\n               .replace(\"%#x\", \"{:#x}\").replace(\"%x\", \"{:x}\").replace(\"%02X\", \"{:02X}\") \\\n               .replace(\"%c\", \"{:c}\").replace(\"%02x\", \"{:02x}\").replace(\"%ld\", \"{}\")    \\\n               .replace(\"%*s\", \"\").replace(\"%lX\", \"{:X}\").replace(\"%08x\", \"{:08x}\")     \\\n               .replace(\"%u\", \"{}\").replace(\"%lu\", \"{}\").replace(\"%zu\", \"{}\")           \\\n               .replace(\"%02u\", \"{:02d}\").replace(\"%03u\", \"{:03d}\")                     \\\n               .replace(\"%03d\", \"{:03d}\").replace(\"%p\", \"{:#x}\").replace(\"%i\", \"{}\")\n\n    def get_format_arguments(self, fmt_addr: Addr, args: List[int]) -> List[Union[int, str]]:\n        \"\"\"\n        Read the format string at ``fmt_addr``. For each format item\n        which are strings, dereference that associated string and replaces it\n        in ``args``.\n\n        :param fmt_addr: Address to concretize\n        :type fmt_addr: :py:obj:`tritondse.types.Addr`\n        :param args: Parameters associated with the format string\n        :type args: List[int]\n        :rtype: List[Union[int, str]]\n        \"\"\"\n        # FIXME: Modifies inplace args (which is not very nice)\n        s_str = self.memory.read_string(fmt_addr)\n        postString = [i for i, x in enumerate([i for i, c in enumerate(s_str) if c == '%']) if s_str[x+1] == \"s\"]\n        for p in postString:\n            args[p] = self.memory.read_string(args[p])\n            args[p] = args[p].encode(\"latin-1\").decode()\n        return args\n\n    def get_stack_value(self, index: int, offset: int = 0) -> int:\n        \"\"\"\n        Returns the value at the ith position further in the stack\n\n        :param index: The index position from the top of the stack\n        :type index: int\n        :param offset: An integer value offset to apply to stack address\n        :type offset: int\n        :return: the value got\n        :return: the value got\n        :rtype: int\n        \"\"\"\n        addr = self.cpu.stack_pointer + (offset * self.ptr_size) + (index * self.ptr_size)\n        return self.memory.read_uint(addr, self.ptr_size)\n\n    def write_stack_value(self, index: int, value: int, offset: int = 0) -> None:\n        \"\"\"\n        Write the given value on the stack at the given index relative to the current\n        stack pointer. The index value can be positive to write further down the stack\n        or negative to write upward.\n\n        :param index: The index position from the top of the stack\n        :type index: int\n        :param value: Integer value to write on the stack\n        :type value: int\n        :param offset: Add an optional ith item offset to add to stack value (not a size)\n        :type offset: int\n        :return: the value got\n        :rtype: int\n        \"\"\"\n        addr = self.cpu.stack_pointer + (offset * self.ptr_size) + (index * self.ptr_size)\n        self.memory.write_int(addr, value, self.ptr_size)\n\n    def pop_stack_value(self) -> int:\n        \"\"\"\n        Pop a stack value, and the re-increment the stack pointer value.\n        This operation is fully concrete.\n\n        :return: int\n        \"\"\"\n        val = self.memory.read_ptr(self.cpu.stack_pointer)\n        self.cpu.stack_pointer += self.ptr_size\n        return val\n\n    def push_stack_value(self, value: int) -> None:\n        \"\"\"\n        Push a stack value. It then decreement the stack pointer value.\n\n        :param value: The value to push\n        \"\"\"\n        self.memory.write_ptr(self.cpu.stack_pointer-self.ptr_size, value)\n        self.cpu.stack_pointer -= self.ptr_size\n\n    def is_halt_instruction(self) -> bool:\n        \"\"\"\n        Check if the the current instruction is corresponding to an 'halt' instruction\n        in the target architecture.\n\n        :returns: Return true if on halt instruction architecture independent\n        \"\"\"\n        halt_opc = self._archinfo.halt_inst\n        return self.__current_inst.getType() == halt_opc\n\n    def solve(self, constraint: Union[AstNode, List[AstNode]], with_pp: bool = True) -> Tuple[SolverStatus, Model]:\n        \"\"\"\n        Solve the given constraint one the current symbolic state and returns both\n        a Solver status and a model. If not SAT the model returned is empty. Argument\n        ``with_pp`` enables checking the constraint taking in account the path predicate.\n\n        :param constraint: AstNode or list of AstNodes constraints to solve\n        :param with_pp: whether to take in account path predicate\n        :return: tuple of status and model\n        \"\"\"\n        if with_pp:\n            cst = constraint if isinstance(constraint, list) else [constraint]\n            final_cst = self.actx.land([self.tt_ctx.getPathPredicate()]+cst)\n        else:\n            final_cst = self.actx.land(constraint) if isinstance(constraint, list) else constraint\n\n        model, status, _ = self.tt_ctx.getModel(final_cst, status=True)\n        return SolverStatus(status), model\n\n    def solve_no_pp(self, constraint: Union[AstNode, List[AstNode]]) -> Tuple[SolverStatus, Model]:\n        \"\"\"\n        Helper function that solve a constraint forcing not to use\n        the path predicate.\n\n        .. warning:: Solving a query without the path predicate gives theoretically\n                     unsound results.\n\n        :param constraint: AstNode constraint to solve\n        :return: tuple of status and model\n        \"\"\"\n        return self.solve(constraint, with_pp=False)\n\n    def symbolize_register(self, register: Union[str, Register], alias: str = None) -> SymbolicVariable:\n        \"\"\"\n        Symbolize the given register. This a proxy for the symbolizeRegister\n        Triton function.\n\n        :param register: string of the register or Register object\n        :type register: Union[str, :py:obj:`tritondse.types.Register`]\n        :param alias: alias name to give to the symbolic variable\n        :type alias: str\n        :return: Triton Symbolic variable created\n        \"\"\"\n        reg = getattr(self.tt_ctx.registers, register) if isinstance(register, str) else register  # if str get reg\n        if alias:\n            var = self.tt_ctx.symbolizeRegister(reg, alias)\n        else:\n            var = self.tt_ctx.symbolizeRegister(reg)\n        return var\n\n    def symbolize_memory_byte(self, addr: Addr, alias: str = None) -> SymbolicVariable:\n        \"\"\"\n        Symbolize the given memory cell. Returns the associated\n        SymbolicVariable\n\n        :param addr: Address to symbolize\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param alias: alias to give the variable\n        :return: newly created symbolic variable\n        :rtype: :py:obj:`tritondse.types.SymbolicVariable`\n        \"\"\"\n        if alias:\n            return self.tt_ctx.symbolizeMemory(MemoryAccess(addr, CPUSIZE.BYTE), alias)\n        else:\n            return self.tt_ctx.symbolizeMemory(MemoryAccess(addr, CPUSIZE.BYTE))\n\n    def symbolize_memory_bytes(self, addr: Addr, size: ByteSize, alias_prefix: str = None, offset: int = 0) -> List[SymbolicVariable]:\n        \"\"\"\n        Symbolize a range of memory addresses. Can optionally provide an alias\n        prefix.\n\n        :param addr: Address at which to read data\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: Number of bytes to symbolize\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :param alias_prefix: prefix name to give the variable\n        :type alias_prefix: str\n        :return: list of Symbolic variables created\n        :rtype: List[:py:obj:`tritondse.types.SymbolicVariable`]\n        \"\"\"\n        if alias_prefix:\n            return [self.symbolize_memory_byte(addr+i, alias_prefix+f\"[{i+offset}]\") for i in range(size)]\n        else:\n            return [self.symbolize_memory_byte(addr+i) for i in range(size)]\n\n    def get_expression_variable_values_model(self, exp: Union[AstNode, Expression], model: Model) -> Dict[SymbolicVariable: int]:\n        \"\"\"\n        Given a symbolic expression and a model, returns the valuation\n        of all variables involved in the expression.\n\n        :param exp: Symbolic Expression to look into\n        :param model: Model generated by the solver\n        :return: dictionnary of symbolic variables and their associated value (as int)\n        \"\"\"\n        ast = exp.getAst() if hasattr(exp, \"getAst\") else exp\n        ast_vars = self.actx.search(ast, AST_NODE.VARIABLE)\n        sym_vars = [x.getSymbolicVariable() for x in ast_vars]\n        final_dict = {}\n        for avar, svar in zip(ast_vars, sym_vars):\n            if svar.getId() in model:\n                final_dict[svar] = model[svar.getId()].getValue()\n            else:\n                final_dict[svar] = avar.evaluate()\n        return final_dict\n\n    def evaluate_expression_model(self, exp: Union[AstNode, Expression], model: Model) -> int:\n        \"\"\"\n        Evaluate the given expression on the given model. The value returned is the\n        integer value corresponding to the bitvector evaluation of the expression.\n\n        :param exp: Symbolic Expression to evaluate\n        :param model: Model generated by the solver\n        :return: result of the evaluation\n        \"\"\"\n        ast = exp.getAst() if hasattr(exp, \"getAst\") else exp\n\n        vars = self.get_expression_variable_values_model(ast, model)\n\n        backup = {}\n        for var, value in vars.items():\n            backup[var] = self.tt_ctx.getConcreteVariableValue(var)\n            self.tt_ctx.setConcreteVariableValue(var, value)\n        final_value = ast.evaluate()\n        for var in vars.keys():\n            self.tt_ctx.setConcreteVariableValue(var, backup[var])\n        return final_value\n\n    # def enumerate_expression_value(self, exp: Union[AstNode, Expression], constraints: List[AstNode], values_blacklist: List[int], limit: int):\n    #     # Written for when it will work\n    #     solver = z3.SolverFor(\"QF_BV\")\n    #     ast = exp.getAst() if hasattr(exp, \"getAst\") else exp\n    #     z3ast = self.actx.tritonToZ3(ast)\n    #\n    #     solver.add([self.actx.tritonToZ3(x) for x in constraints])\n    #     solver.add([z3ast != x for x in values_blacklist])\n    #\n    #     values = []  # retrieved values\n    #\n    #     while limit:\n    #         res = solver.check()\n    #         if res == z3.sat:\n    #             model = solver.model()\n    #             new_val = model.eval(z3ast)\n    #             values.append(new_val)\n    #             solver.add(z3ast != new_val)\n    #         else:\n    #             return values\n    #         limit -= 1\n    #     return values\n\n    def solve_enumerate_expression(self, exp: Union[AstNode, Expression], constraints: List[AstNode], values_blacklist: List[int], limit: int) -> List[Tuple[Model, int]]:\n        # Written for when it will work\n        ast = exp.getAst() if hasattr(exp, \"getAst\") else exp\n\n        constraint = self.actx.land(constraints + [ast != x for x in values_blacklist])\n\n        result = []\n        while limit:\n            status, model = self.solve(constraint, with_pp=False)\n            if status == SolverStatus.SAT:\n                new_val = self.evaluate_expression_model(ast, model)\n                result.append((model, new_val))\n                constraint = self.actx.land([constraint, ast != new_val])\n            else:\n                return result\n            limit -= 1\n        return result\n\n    @staticmethod\n    def from_loader(loader: Loader) -> 'ProcessState':\n        pstate = ProcessState(loader.endianness)\n\n        # Initialize the architecture of the processstate\n        pstate.initialize_context(loader.architecture)\n\n        # Set the program counter to points to entrypoint\n        pstate.cpu.program_counter = loader.entry_point\n\n        # Disable segmentation to map segments\n        with pstate.memory.without_segmentation():\n            # Load memory areas in memory\n            for i, seg in enumerate(loader.memory_segments()):\n                if not seg.size and not seg.content:\n                    logger.warning(f\"A segment have to provide either a size or a content {seg.name} (skipped)\")\n                    continue\n                size = len(seg.content) if seg.content else seg.size\n                logger.debug(f\"Loading 0x{seg.address:#08x} - {seg.address+size:#08x} size={size:#x}\")\n                pstate.memory.map(seg.address, size, seg.perms, seg.name)\n                if seg.content:\n                    pstate.memory.write(seg.address, seg.content)\n\n        # Apply dynamic relocations\n        cur_linkage_address = pstate.EXTERN_FUNC_BASE\n\n        # Disable segmentation\n        with pstate.memory.without_segmentation():\n            # Link imported functions in EXTERN_FUNC_BASE\n            for fname, rel_addr in loader.imported_functions_relocations():\n                logger.debug(f\"Hooking {fname} at {rel_addr:#x}\")\n\n                # If we already linked this function (because another library uses it) we reuse the same\n                # linkage address.\n                if fname in pstate.dynamic_symbol_table:\n                    (linkage_address, _) = pstate.dynamic_symbol_table[fname] \n                    logger.debug(f\"Already added. {fname} at {rel_addr:#x} linkage_addr={linkage_address:#x}\")\n                    pstate.memory.write_ptr(rel_addr, linkage_address)\n\n                else:\n                    # Add symbol in dynamic_symbol_table\n                    pstate.dynamic_symbol_table[fname] = (cur_linkage_address, True)\n\n                    # Apply relocation to our custom address in process memory\n                    pstate.memory.write_ptr(rel_addr, cur_linkage_address)\n                    # Increment linkage address number\n                    cur_linkage_address += pstate.ptr_size\n\n        # Try initializing stack registers if a stack is present in maps\n        # Map the stack\n        try:\n            stack = pstate.memory.map_from_name(pstate.STACK_SEG)\n            alloc = 1 * pstate.ptr_size\n            pstate.write_register(pstate.base_pointer_register, stack.start+stack.size-alloc) # Pointing right-out of the stack\n            pstate.write_register(pstate.stack_pointer_register, stack.start+stack.size-alloc)\n        except AssertionError:\n            logger.warning(\"no stack segment has been created by the loader\")\n\n        # Search for a map to settle foreign symbols\n        segs = pstate.memory.find_map(pstate.EXTERN_SEG)\n        if segs:\n            symb_base = segs[0].start\n\n            # Link imported symbols\n            for sname, rel_addr in loader.imported_variable_symbols_relocations():\n                logger.debug(f\"Hooking {sname} at {rel_addr:#x}\")\n\n                if pstate.architecture == Architecture.X86_64:  # HACK: Keep rel_addr to directly write symbol on it\n                    # Add symbol in dynamic_symbol_table\n                    pstate.dynamic_symbol_table[sname] = (rel_addr, False)\n                    #pstate.memory.write_ptr(rel_addr, cur_linkage_address)  # Do not write anything as symbolic executor will do it\n                else:\n                    # Add symbol in dynamic_symbol_table\n                    pstate.dynamic_symbol_table[sname] = (symb_base, False)\n                    pstate.memory.write_ptr(rel_addr, symb_base)\n\n                symb_base += pstate.ptr_size\n\n        for reg_name in pstate.cpu:\n            if reg_name in loader.cpustate:\n                setattr(pstate.cpu, reg_name, loader.cpustate[reg_name])\n\n        if loader.arch_mode: # If the processor's mode is provided\n            if loader.arch_mode == ArchMode.THUMB:\n                pstate.set_thumb(True)\n        return pstate", ""]}
{"filename": "tritondse/__init__.py", "chunked_list": ["from .config                import Config\nfrom .loaders.program       import Program\nfrom .loaders.quokkaprogram import QuokkaProgram\nfrom .loaders.cle_loader    import CleLoader\nfrom .loaders.loader        import Loader, MonolithicLoader, LoadableSegment\nfrom .process_state         import ProcessState\nfrom .coverage              import CoverageStrategy, BranchSolvingStrategy, CoverageSingleRun, GlobalCoverage\nfrom .symbolic_executor     import SymbolicExecutor\nfrom .symbolic_explorator   import SymbolicExplorator, ExplorationStatus\nfrom .seed                  import Seed, SeedStatus, SeedFormat, CompositeData", "from .symbolic_explorator   import SymbolicExplorator, ExplorationStatus\nfrom .seed                  import Seed, SeedStatus, SeedFormat, CompositeData\nfrom .sanitizers            import CbType, ProbeInterface, UAFSanitizer, NullDerefSanitizer, FormatStringSanitizer, IntegerOverflowSanitizer\nfrom .types                 import *\nfrom .workspace             import Workspace\nfrom .memory                import Memory, MemoryAccessViolation, MapOverlapException, MemMap\nfrom .exception import AllocatorException, SkipInstructionException, StopExplorationException, AbortExecutionException\n\nfrom triton import VERSION\n", "from triton import VERSION\n\nTRITON_VERSION = f\"v{VERSION.MAJOR}.{VERSION.MINOR}\"\n"]}
{"filename": "tritondse/symbolic_executor.py", "chunked_list": ["# built-in imports\nimport io\nimport time\nimport os\n\nif os.name == 'posix':\n    import resource\n\nfrom typing import Optional, Union, List, NoReturn, Dict, Type\n", "from typing import Optional, Union, List, NoReturn, Dict, Type\n\n# third party imports\nfrom triton import MODE, Instruction, CPUSIZE, MemoryAccess, CALLBACK\n\n# local imports\nfrom tritondse.config import Config\nfrom tritondse.coverage import CoverageSingleRun, BranchSolvingStrategy\nfrom tritondse.process_state import ProcessState\nfrom tritondse.loaders import Loader", "from tritondse.process_state import ProcessState\nfrom tritondse.loaders import Loader\nfrom tritondse.seed import Seed, SeedStatus, SeedFormat, CompositeData\nfrom tritondse.types import Expression, Architecture, Addr, Model, SymbolicVariable, Register\nfrom tritondse.routines import SUPPORTED_ROUTINES, SUPORTED_GVARIABLES\nfrom tritondse.callbacks import CallbackManager\nfrom tritondse.workspace import Workspace\nfrom tritondse.heap_allocator import AllocatorException\nfrom tritondse.thread_context import ThreadContext\nfrom tritondse.exception import AbortExecutionException, SkipInstructionException, StopExplorationException", "from tritondse.thread_context import ThreadContext\nfrom tritondse.exception import AbortExecutionException, SkipInstructionException, StopExplorationException\nfrom tritondse.memory import MemoryAccessViolation, Perm\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"executor\")\n\n\nclass SymbolicExecutor(object):\n    \"\"\"\n    Single Program Execution Class.\n    That module, is in charge of performing the process loading from the given\n    program.\n    \"\"\"\n\n    def __init__(self, config: Config, seed: Seed = Seed(), workspace: Workspace = None, uid=0, callbacks=None):\n        \"\"\"\n        :param config: configuration file to use\n        :type config: Config\n        :param seed: input file to inject either in stdin or argv (optional)\n        :type seed: Seed\n        :param workspace: Workspace to use. If None it will be instanciated\n        :type workspace: Optional[Workspace]\n        :param uid: Unique ID. Given by :py:obj:`SymbolicExplorator` to identify uniquely executions\n        :type uid: int\n        :param callbacks: callbacks to bind on this execution before running *(instanciated if empty !)*\n        :type callbacks: CallbackManager\n        \"\"\"\n        self.config: Config = config      #: Configuration file used\n        self.loader: Type[Loader] = None  #: Loader used to run the code\n\n        self.pstate: ProcessState = None  #: ProcessState\n\n        self.workspace: Workspace = workspace  #: Current workspace\n        if self.workspace is None:\n            self.workspace = Workspace(config.workspace)\n\n        self.seed: Seed = seed  #: The current seed used for the execution\n\n        # Override config if there is a mismatch between seed format and config file\n        if seed.format != self.config.seed_format:\n            logger.warning(f\"seed format {seed.format} mismatch config {config.seed_format} (override config)\")\n            self.config.seed_format = seed.format\n\n        self.symbolic_seed = self._init_symbolic_seed(seed) #: symbolic seed (same structure than Seed but with symbols)\n\n        self.coverage: CoverageSingleRun = CoverageSingleRun(self.config.coverage_strategy)  #: Coverage of the execution\n        self.rtn_table = dict()   # Addr -> Tuple[fname, routine]\n        self.uid: int = uid       #: Unique identifier meant to unique accross Exploration instances\n        self.start_time: int = 0  #: start time of the process\n        self.end_time: int = 0    #: end time of the process\n\n        # create callback object if not provided as argument, and bind callbacks to the current process state\n        self.cbm: CallbackManager = callbacks if callbacks is not None else CallbackManager()\n        \"\"\"callback manager\"\"\"\n\n        # List of new seeds filled during the execution and flushed by explorator\n        self._pending_seeds = []\n        self._run_to_target = None\n\n        self.trace_offset: int = 0  #: counter of instructions executed\n\n        self.previous_pc: int = 0  #: previous program counter executed\n        self.current_pc = 0        #: current program counter\n\n        self.debug_pp = False\n\n        self._in_processing = False  # use to know if we are currently processing an instruction\n\n        # TODO: Here we load the binary each time we run an execution (via ELFLoader). We can\n        #       avoid this (and so gain in speed) if a TritonContext could be forked from a\n        #       state. See: https://github.com/JonathanSalwan/Triton/issues/532\n\n    def _init_symbolic_seed(self, seed: Seed) -> Union[list, CompositeData]:\n        if seed.is_raw():\n            return [None]*len(seed.content)\n        else:  # is composite\n            argv = [[None]*len(a) for a in seed.content.argv]\n            files = {k: [None]*len(v) for k, v in seed.content.files.items()}\n            variables = {k: [None]*(1 if isinstance(v, int) else len(v)) for k, v in seed.content.variables.items()}\n            return CompositeData(argv=argv, files=files, variables=variables)\n\n    def load(self, loader: Loader) -> None:\n        \"\"\"\n        Use the given loader to initialize the ProcessState.\n        It overrides the current ProcessState if any.\n\n        :param loader: Loader describing how to load\n        :return: None\n        \"\"\"\n\n        # Initialize the process_state architecture (at this point arch is sure to be supported)\n        self.loader = loader\n        logger.debug(f\"Loading program {self.loader.name} [{self.loader.architecture}]\")\n        self.pstate = ProcessState.from_loader(loader)\n        self._map_dynamic_symbols()\n        self._load_seed_process_state(self.pstate, self.seed)\n\n    def load_process(self, pstate: ProcessState) -> None:\n        \"\"\"\n        Load the given process state. Do nothing but\n        setting the internal ProcessState.\n\n        :param pstate: PrcoessState to set\n        \"\"\"\n        self.pstate = pstate\n        self._load_seed_process_state(self.pstate, self.seed)\n\n    @staticmethod\n    def _load_seed_process_state(pstate: ProcessState, seed: Seed) -> None:\n        if seed.is_raw():\n            data = seed.content\n        else:  # is composite\n            if seed.is_file_defined(\"stdin\"):\n                data = seed.get_file_input(\"stdin\")\n            else:\n                return\n        filedesc = pstate.get_file_descriptor(0)\n        filedesc.fd = io.BytesIO(data)\n\n    @property\n    def execution_time(self) -> int:\n        \"\"\"\n        Time taken for the execution in seconds\n\n        .. warning:: Only relevant at the end of the execution\n\n        :return: execution time (in s)\n        \"\"\"\n        return self.end_time - self.start_time\n\n    @property\n    def pending_seeds(self) -> List[Seed]:\n        \"\"\"\n        List of pending seeds gathered during execution.\n\n        .. warning:: Only relevant at the end of execution\n\n        :returns: list of new seeds generated\n        :rtype: List[Seed]\n        \"\"\"\n        return self._pending_seeds\n\n    def enqueue_seed(self, seed: Seed) -> None:\n        \"\"\"\n        Add a seed to the queue of seed to be executed in later iterations.\n        This function is meant to be used by user callbacks.\n\n        :param seed: Seed to be added\n        :type seed: Seed\n        \"\"\"\n        self._pending_seeds.append(seed)\n\n    @property\n    def callback_manager(self) -> CallbackManager:\n        \"\"\"\n        Get the callback manager associated with the execution.\n\n        :rtype: CallbackManager\"\"\"\n        return self.cbm\n\n    def is_seed_injected(self) -> bool:\n        \"\"\"\n        Get whether or not the seed has been injected.\n\n        :return: True if the seed has already been inserted\n        \"\"\"\n        if self.config.is_format_raw():\n            return bool(self.symbolic_seed)\n        elif self.config.is_format_composite():\n            # Namely has one of the various input been injected or not\n            return bool(self.symbolic_seed.content.files) or bool(self.symbolic_seed.content.variables)\n        else:\n            assert False\n\n    def _configure_pstate(self) -> None:\n        #for mode in [MODE.ALIGNED_MEMORY, MODE.AST_OPTIMIZATIONS, MODE.CONSTANT_FOLDING, MODE.ONLY_ON_SYMBOLIZED]:\n        for mode in [MODE.ONLY_ON_SYMBOLIZED]:\n            self.pstate.set_triton_mode(mode, True)\n        logger.info(f\"configure pstate: time_inc:{self.config.time_inc_coefficient}  solver:{self.config.smt_solver.name}  timeout:{self.config.smt_timeout}\")\n        self.pstate.time_inc_coefficient = self.config.time_inc_coefficient\n        self.pstate.set_solver_timeout(self.config.smt_timeout)\n        self.pstate.set_solver(self.config.smt_solver)\n\n    def _fetch_next_thread(self, threads: List[ThreadContext]) -> Optional[ThreadContext]:\n        \"\"\"\n        Given a list of threads, returns the next to execute. Iterating\n        threads in a round-robin style picking the next item in the list.\n\n        :param threads: list of threads\n        :return: thread context\n        \"\"\"\n        cur_idx = threads.index(self.pstate.current_thread)\n\n        tmp_list = threads[cur_idx+1:]+threads[:cur_idx]  # rotate list (and exclude current_item)\n        for th in tmp_list:\n            if th.is_running():\n                return th  # Return the first thread that is properly running\n        return None\n\n    def __schedule_thread(self) -> None:\n        threads_list = self.pstate.threads\n\n        if len(threads_list) == 1:  # If there is only one thread no need to schedule another thread\n            return\n\n        if self.pstate.current_thread.count > self.config.thread_scheduling:\n            # Select the next thread to execute\n            next_th = self._fetch_next_thread(threads_list)\n\n            if next_th:  # We found another thread to schedule\n\n                # Call all callbacks related to threads\n                for cb in self.cbm.get_context_switch_callback():\n                    cb(self, self.pstate, self.pstate.current_thread)\n\n                # Save current context and restore new thread context (+kill current if dead)\n                self.pstate.switch_thread(next_th)\n\n            else:  # There are other thread but not other one is available (thus keep current one)\n                self.pstate.current_thread.count = 0  # Reset its counter\n\n        else:\n            # Increment the instruction counter of the thread (a bit in advance but it does not matter)\n            self.pstate.current_thread.count += 1\n\n    def _symbolic_mem_callback(self, se: 'SymbolicExecutor', ps: ProcessState, mem: MemoryAccess, *args):\n        tgt_addr = mem.getAddress()\n        lea_ast = mem.getLeaAst()\n        if lea_ast is None:\n            return\n        if lea_ast.isSymbolized():\n            s = \"write\" if bool(args) else \"read\"\n            pc = self.pstate.cpu.program_counter\n            logger.debug(f\"symbolic {s} at 0x{pc:x}: target: 0x{tgt_addr:x} [{lea_ast}]\")\n            self.pstate.push_constraint(lea_ast == tgt_addr, f\"sym-{s}:{self.trace_offset}:{pc}\")\n\n    def emulate(self):\n        while not self.pstate.stop and self.pstate.threads:\n            if not self.step():\n                break\n        if not self.seed.is_status_set():  # Set a status if it has not already been done\n            self.seed.status = SeedStatus.OK_DONE\n        return\n\n\n    def step(self) -> bool:\n        \"\"\"\n        Perform a single instruction step. Returns whether the emulation can\n        continue or we have to stop.\n        \"\"\"\n        try:\n            # Schedule thread if it's time\n            self.__schedule_thread()\n\n            if not self.pstate.current_thread.is_running():\n                logger.warning(f\"After scheduling current thread is not running (probably in a deadlock state)\")\n                return False  # Were not able to find a suitable thread thus exit emulation\n\n            # Fetch program counter (of the thread selected), at this point the current thread should be running!\n            self.current_pc = self.pstate.cpu.program_counter  # should normally be already set but still.\n\n            if self.current_pc == self._run_to_target:  # Hit the location we wanted to reach\n                return False\n\n            if self.current_pc == 0:\n                logger.error(f\"PC=0, is it normal ? (stop)\")\n                return False\n\n            if self.pstate.memory.segmentation_enabled:\n                if not self.pstate.memory.has_ever_been_written(self.current_pc, CPUSIZE.BYTE):\n                    logger.error(f\"Instruction not mapped: 0x{self.current_pc:x}\")\n                    return False\n\n            instruction = self.pstate.fetch_instruction()\n            opcode = instruction.getOpcode()\n            mnemonic = instruction.getType()\n\n            try:\n                # Trigger pre-address callback\n                pre_cbs, post_cbs = self.cbm.get_address_callbacks(self.current_pc)\n                for cb in pre_cbs:\n                    cb(self, self.pstate, self.current_pc)\n\n                # Trigger pre-opcode callback\n                pre_opcode, post_opcode = self.cbm.get_opcode_callbacks(opcode)\n                for cb in pre_opcode:\n                    cb(self, self.pstate, opcode)\n\n                # Trigger pre-mnemonic callback\n                pre_mnemonic, post_mnemonic = self.cbm.get_mnemonic_callbacks(mnemonic)\n                for cb in pre_mnemonic:\n                    cb(self, self.pstate, mnemonic)\n\n                # Trigger pre-instruction callback\n                pre_insts, post_insts = self.cbm.get_instruction_callbacks()\n                for cb in pre_insts:\n                    cb(self, self.pstate, instruction)\n            except SkipInstructionException as _:\n                return True\n\n            if self.pstate.is_syscall():\n                logger.warning(f\"execute syscall instruction {self.pstate.read_register(self.pstate._syscall_register)}\")\n\n            # Process\n            prev_pc = self.current_pc\n            self._in_processing = True\n            if not self.pstate.process_instruction(instruction):\n                if self.pstate.is_halt_instruction():\n                    logger.info(f\"hit {str(instruction)} instruction stop.\")\n                else:\n                    logger.error('Instruction not supported: %s' % (str(instruction)))\n\n                if self.config.skip_unsupported_instruction:\n                    self.pstate.cpu.program_counter += instruction.getSize() # try to jump over the instruction\n                else:\n                    return False  # stop emulation\n            self._in_processing = False\n            # increment trace offset\n            self.trace_offset += 1\n\n            # update previous program counters\n            self.previous_pc = prev_pc\n            self.current_pc = self.pstate.cpu.program_counter  # current_pc becomes new instruction pointer\n\n            # Update the coverage of the execution\n            self.coverage.add_covered_address(self.previous_pc)\n\n            # Update coverage send it the last PathConstraint object if one was added\n            if self.pstate.is_path_predicate_updated():\n                path_constraint = self.pstate.last_branch_constraint\n\n                if path_constraint.isMultipleBranches():\n                    branches = path_constraint.getBranchConstraints()\n                    if len(branches) != 2:\n                        logger.error(\"Branching condition has more than two branches\")\n                    taken, not_taken = branches if branches[0]['isTaken'] else branches[::-1]\n                    taken_addr, not_taken_addr = taken['dstAddr'], not_taken['dstAddr']\n\n                    for cb in self.cbm.get_on_branch_covered_callback():\n                        cb(self, self.pstate, (self.previous_pc, taken_addr))\n\n                    self.coverage.add_covered_branch(self.previous_pc, taken_addr, not_taken_addr)\n\n                else:  # It is normally a dynamic jump or symbolic memory read/write\n                    cmt = path_constraint.getComment()\n                    if cmt.startswith(\"sym-read\") or cmt.startswith(\"sym-write\"):\n                        pass\n                        # NOTE: At the moment it does not seems suitable to count r/w pointers\n                        # as part of the coverage. So does not have an influence on covered/not_covered.\n                    else:\n                        logger.warning(f\"New dynamic jump covered at: {self.previous_pc:08x}\")\n                        path_constraint.setComment(f\"dyn-jmp:{self.trace_offset}:{self.previous_pc}\")\n                        self.coverage.add_covered_dynamic_branch(self.previous_pc, self.current_pc)\n\n            # Trigger post-opcode callback\n            for cb in post_opcode:\n                cb(self, self.pstate, opcode)\n\n            # Trigger post-mnemonic callback\n            for cb in post_mnemonic:\n                cb(self, self.pstate, mnemonic)\n\n            # Trigger post-instruction callback\n            for cb in post_insts:\n                cb(self, self.pstate, instruction)\n\n            # Trigger post-address callbacks\n            for cb in post_cbs:\n                cb(self, self.pstate, self.previous_pc)\n\n            # Simulate routines\n            try:\n                self._routines_handler(instruction)\n            except AllocatorException as e:\n                logger.info(f'An exception has been raised: {e}')\n                self.seed.status = SeedStatus.CRASH\n                return False\n\n            # Check timeout of the execution\n            if self.config.execution_timeout and (time.time() - self.start_time) >= self.config.execution_timeout:\n                logger.info('Timeout of an execution reached')\n                self.seed.status = SeedStatus.HANG\n                return False\n            return True\n        except AbortExecutionException as e:\n            return False\n        except MemoryAccessViolation as e:\n            logger.warning(f\"Memory violation: {str(e)}\")\n\n            # Call all the callbacks on the memory violations\n            for cb in self.callback_manager.get_memory_violation_callbacks():\n                cb(self, self.pstate, e)\n\n            # Assign the seed the status of crash\n            if not self.seed.is_status_set():\n                self.seed.status = SeedStatus.CRASH\n            return False\n\n\n    def __handle_external_return(self, routine_name: str, ret_val: Optional[Union[int, Expression]]) -> None:\n        \"\"\" Symbolize or concretize return values of external functions \"\"\"\n        if ret_val is not None:\n            reg = self.pstate.return_register\n            if isinstance(ret_val, int): # Write its concrete value\n                self.pstate.write_register(reg, ret_val)\n            else:  # It should be a logic expression\n                self.pstate.write_symbolic_register(reg, ret_val, f\"(routine {routine_name}\")\n\n    def _routines_handler(self, instruction: Instruction):\n        \"\"\"\n        This function handle external routines calls. When the .plt jmp on an external\n        address, we call the appropriate Python routine and setup the returned value\n        which may be concrete or symbolic.\n\n        :param instruction: The current instruction executed\n        :return: None\n        \"\"\"\n        pc = self.pstate.cpu.program_counter\n        if pc in self.rtn_table:\n            routine_name, routine = self.rtn_table[pc]\n            logger.debug(f\"Enter external routine: {routine_name}\")\n\n            # Trigger pre-address callback\n            pre_cbs, post_cbs = self.cbm.get_imported_routine_callbacks(routine_name)\n\n            ret_val = None\n            for cb in pre_cbs:\n                ret = cb(self, self.pstate, routine_name, pc)\n                if ret is not None:  # if the callback return a value the function behavior will be skipped\n                    ret_val = ret\n                    break  # Set the ret val and break\n\n            if ret_val is None:  # If no ret_val has been set by any callback function call the supported routine\n                # Emulate the routine and the return value\n                ret_val = routine(self, self.pstate)\n\n            self.__handle_external_return(routine_name, ret_val)\n\n            # Trigger post-address callbacks\n            for cb in post_cbs:\n                cb(self, self.pstate, routine_name, pc)\n\n            # Do not continue the execution if we are in a locked mutex\n            if self.pstate.mutex_locked:\n                self.pstate.mutex_locked = False\n                self.pstate.cpu.program_counter = instruction.getAddress()\n                # It's locked, so switch to another thread\n                self.pstate.current_thread.count = self.config.thread_scheduling+1\n                return\n\n            # Do not continue the execution if we are in a locked semaphore\n            if self.pstate.semaphore_locked:\n                self.pstate.semaphore_locked = False\n                self.pstate.cpu.program_counter = instruction.getAddress()\n                # It's locked, so switch to another thread\n                self.pstate.current_thread.count = self.config.thread_scheduling+1\n                return\n\n            if self.pstate.architecture == Architecture.AARCH64:\n                # Get the return address\n                ret_addr = self.pstate.read_register('x30')\n            elif self.pstate.architecture in [Architecture.X86, Architecture.X86_64]:\n                # Get the return address and restore RSP (simulate RET)\n                ret_addr = self.pstate.pop_stack_value()\n            else:\n                raise Exception(\"Architecture not supported\")\n\n            # Hijack RIP to skip the call\n            self.pstate.cpu.program_counter = ret_addr\n\n    def _map_dynamic_symbols(self) -> None:\n        \"\"\"\n        Apply dynamic relocations of imported functions and imported symbols\n        regardless of the architecture or executable format\n        .. FIXME: This function does not apply all possible relocations\n        :return: None\n        \"\"\"\n        for symbol, (addr, is_func) in self.pstate.dynamic_symbol_table.items():\n\n            if symbol in SUPPORTED_ROUTINES:  # if the routine name is supported\n                # Add link to the routine and got tables\n                self.rtn_table[addr] = (symbol, SUPPORTED_ROUTINES[symbol])\n\n            elif symbol in SUPORTED_GVARIABLES:\n                # if self.pstate.architecture == Architecture.X86_64:\n                self.pstate.memory.write_ptr(addr, SUPORTED_GVARIABLES[symbol])  # write directly at addr\n                # elif self.pstate.architecture == Architecture.AARCH64:\n                #     val = self.pstate.memory.read_ptr(addr)\n                #     self.pstate.memory.write_ptr(val, SUPORTED_GVARIABLES[symbol])\n\n            else:  # the symbol is not supported\n                if self.uid == 0:  # print warning if first uid (so that it get printed once)\n                    logger.warning(f\"symbol {symbol} imported but unsupported\")\n                if is_func:\n                    # Add link to a default stub function\n                    self.rtn_table[addr] = (symbol, self.__default_stub)\n                else:\n                    pass # do nothing on unsupported symbols\n\n    def __default_stub(self, se: 'SymbolicExecutor', pstate: ProcessState):\n        rtn_name, _ = self.rtn_table[pstate.cpu.program_counter]\n        logger.warning(f\"calling {rtn_name} which is unsupported\")\n        if self.config.skip_unsupported_import:\n            return None  # Like if function did nothing\n        else:\n            self.abort()\n\n    def abort(self) -> NoReturn:\n        \"\"\"\n        Abort the current execution. It works by raising\n        an exception which is caught by the emulation function\n        that takes care of returning appropriately afterward.\n\n        :raise AbortExecutionException: to abort execution from anywhere\n        \"\"\"\n        raise AbortExecutionException('Execution aborted')\n\n    def skip_instruction(self) -> NoReturn:\n        \"\"\"\n        Skip the current instruction before it gets executed. It is only\n        relevant to call it from pre-inst or pre-addr callbacks.\n\n        :raise SkipInstructionException: to skip the current instruction\n        \"\"\"\n        raise SkipInstructionException(\"Skip instruction\")\n\n    def stop_exploration(self) -> NoReturn:\n        \"\"\"\n        Function to call to stop the whole exploration of\n        the program. It raises an exception which is caught by SymbolicExplorator.\n\n        :raise StopExplorationException: to stop the exploration\n        \"\"\"\n        raise StopExplorationException(\"Stop exploration\")\n\n    def emulation_init(self) -> bool:\n        if self.pstate is None:\n            logger.error(f\"ProcessState is None (have you called \\\"load\\\"?\")\n            return False\n\n        self.start_time = time.time()\n\n        # Configure memory segmentation using configuration\n        self.pstate.memory.set_segmentation(self.config.memory_segmentation)\n        if self.config.memory_segmentation:\n            self.cbm.register_memory_read_callback(self._mem_accesses_callback)\n            self.cbm.register_memory_write_callback(self._mem_accesses_callback)\n\n        # Register memory callbacks in case we activated covering mem access\n        if BranchSolvingStrategy.COVER_SYM_READ in self.config.branch_solving_strategy:\n            self.cbm.register_memory_read_callback(self._symbolic_mem_callback)\n        if BranchSolvingStrategy.COVER_SYM_WRITE in self.config.branch_solving_strategy:\n            self.cbm.register_memory_write_callback(self._symbolic_mem_callback)\n\n        # bind dbm callbacks on the process state (newly initialized)\n        self.cbm.bind_to(self)  # bind call\n\n        # Let's emulate the binary from the entry point\n        logger.info('Starting emulation')\n\n        # Get pre/post callbacks on execution\n        pre_cb, post_cb = self.cbm.get_execution_callbacks()\n        # Iterate through all pre exec callbacks\n        for cb in pre_cb:\n            cb(self, self.pstate)\n\n        # Call it here to make sure in case of \"load_process\" the use has properly instanciated the architecture\n        self._configure_pstate()\n        return True\n\n    def run(self, stop_at: Addr = None) -> None:\n        \"\"\"\n        Execute the program.\n\n        If the :py:attr:`tritondse.Config.execution_timeout` is not set\n        the execution might hang forever if the program does.\n\n        :param stop_at: Address where to stop (if necessary)\n        :return: None\n        \"\"\"\n        if stop_at:\n            self._run_to_target = stop_at\n\n        # Call init steps\n        if not self.emulation_init():\n            return\n\n        # Run until reaching a stopping condition\n        self.emulate()\n\n        # Call termination steps\n        self.emulation_deinit()\n\n    def emulation_deinit(self):\n        _, post_cb = self.cbm.get_execution_callbacks()\n        # Iterate through post exec callbacks\n        for cb in post_cb:\n            cb(self, self.pstate)\n\n        self.end_time = time.time()\n\n        # IMPORTANT The next call is necessary otherwise there is a memory\n        #           leak issues.\n        # Unbind callbacks from the current symbolic executor instance.\n        self.cbm.unbind()\n\n        # NOTE Unregister callbacks registered at the beginning of the function.\n        #      This is necessary because we currently have a circular dependency\n        #      between this class and the callback manager. Note that we create\n        #      that circular dependency indirectly when we set the callback to\n        #      a method of this class (_mem_accesses_callback and\n        #      _symbolic_mem_callback).\n        if self.config.memory_segmentation:\n            self.cbm.unregister_callback(self._mem_accesses_callback)\n\n        if BranchSolvingStrategy.COVER_SYM_READ in self.config.branch_solving_strategy:\n            self.cbm.unregister_callback(self._symbolic_mem_callback)\n        if BranchSolvingStrategy.COVER_SYM_WRITE in self.config.branch_solving_strategy:\n            self.cbm.unregister_callback(self._symbolic_mem_callback)\n\n        logger.info(f\"Emulation done [ret:{self.pstate.read_register(self.pstate.return_register):x}]  (time:{self.execution_time:.02f}s)\")\n        logger.info(f\"Instructions executed: {self.coverage.total_instruction_executed}  symbolic branches: {self.pstate.path_predicate_size}\")\n        logger.info(f\"Memory usage: {self.mem_usage_str()}\")\n\n    def _mem_accesses_callback(self, se: 'SymbolicExecutor', ps: ProcessState, mem: MemoryAccess, *args):\n        \"\"\"\n        This callback is used to ensure memory accesses performed by side-effect of instructions\n        semantic correctly checks memory segmentation. Thus we only do the check during the processing\n        of an instruction.\n        \"\"\"\n        if ps.memory.segmentation_enabled and self._in_processing:\n            perm = Perm.W if bool(args) else Perm.R\n            addr = mem.getAddress()\n            size = mem.getSize()\n            map = ps.memory.get_map(addr, size)  # It raises\n            if map is None:\n                raise MemoryAccessViolation(addr, perm, memory_not_mapped=True)\n            else:\n                if perm not in map.perm:\n                    raise MemoryAccessViolation(addr, perm, map_perm=map.perm, perm_error=True)\n\n    @property\n    def exitcode(self) -> int:\n        \"\"\" Exit code value of the process. The value\n        is simply the concrete value of the register\n        marked as return_register (rax, on x86, r0 on ARM..)\n        \"\"\"\n        return self.pstate.read_register(self.pstate.return_register) & 0xFF\n\n    @staticmethod\n    def mem_usage_str() -> str:\n        \"\"\"\n        Debug function to track memory consumption of an execution (not\n        implemented on Windows).\n        \"\"\"\n        if os.name == \"posix\":\n            size, resident, shared, _, _, _, _ = (int(x) for x in open(f\"/proc/{os.getpid()}/statm\").read().split(\" \"))\n            resident = resident * resource.getpagesize()\n            units = [(float(1024), \"Kb\"), (float(1024 **2), \"Mb\"), (float(1024 **3), \"Gb\")]\n            for unit, s in units[::-1]:\n                if resident / unit < 1:\n                    continue\n                else:  # We are on the right unit\n                    return \"%.2f%s\" % (resident/unit, s)\n            return \"%dB\" % resident\n        else:\n          return \"N/A\"\n\n    def mk_new_seed_from_model(self, model: Model) -> Seed:\n        \"\"\"\n        Creates a new seed from the given SMT model.\n\n        :param model: SMT model\n        :return: new seed object\n        \"\"\"\n        def repl_bytearray(concrete, symbolic):\n            for i, sv in enumerate(symbolic):  # Enumerate symvars associated with each bytes\n                if sv is not None:\n                    if sv.getId() in model:  # If solver provided a new value for the symvar\n                        value = model[sv.getId()].getValue()\n                        concrete[i] = value # Replace it in the bytearray\n            return concrete\n\n        if self.config.is_format_raw(): # RAW seed. => symbolize_stdin\n            content = bytes(repl_bytearray(bytearray(self.seed.content), self.symbolic_seed))\n\n        elif self.config.is_format_composite():\n            # NOTE will have to update this if more things are added to CompositeData\n            new_files, new_vars = {}, {}\n\n            # Handle argv (its meant to be here)\n            args = [bytearray(x) for x in self.seed.content.argv]\n            new_argv = [bytes(repl_bytearray(c, s)) for c, s in zip(args, self.symbolic_seed.argv)]\n\n            # Handle stdin and files\n            # If the seed provides the content of files (#NOTE stdin is treated as a file)\n            new_files = {}\n            for k, c in self.seed.content.files.items():\n                if k in self.symbolic_seed.files:\n                    new_files[k] = bytes(repl_bytearray(bytearray(c), self.symbolic_seed.files[k]))\n                else:\n                    new_files[k] = c  # keep the current value in the seed\n\n            # Handle variables, if the seed provides some\n            new_variables = {}\n            for k, c in self.seed.content.variables.items():\n                if k in self.symbolic_seed.variables:\n                    conc = bytearray(c) if isinstance(c, bytes) else [c]\n                    new_vals = repl_bytearray(conc, self.symbolic_seed.variables[k])\n                    new_variables[k] = bytes(new_vals) if isinstance(c, bytes) else new_vals[0]  # new variables are either bytes or int\n                else:\n                    new_variables[k] = c  # If it has not been injected keep the current concrete value\n\n            content = CompositeData(new_argv, new_files, new_variables)\n        else:\n            assert False\n\n        # Calling callback if user defined one\n        new_seed = Seed(content)\n        for cb in self.cbm.get_new_input_callback():\n            cont = cb(self, self.pstate, new_seed)\n            if cont:\n                # if the callback return a new input continue with that one\n                new_seed = cont\n        # Return the\n        return new_seed\n\n    def inject_symbolic_argv_memory(self, addr: Addr, index: int, value: bytes) -> None:\n        \"\"\"\n        Inject the ith item of argv in memory.\n        To be used only with composite seeds and only if seed have a symbolic argv\n\n        :param addr: address where to inject the argv[ith]\n        :param index: ith argv item\n        :param value: value of the item\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), f\"argv[{index}]\") # Symbolize bytes\n        self.symbolic_seed.argv[index] = sym_vars # Add symbolic variables to symbolic seed\n\n    def inject_symbolic_file_memory(self, addr: Addr, name: str, value: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic file (or part of it) in memory.\n\n        :param addr: address where to inject the file bytes\n        :param name: name of the file in the composite seed\n        :param value: bytes content of the file\n        :param offset: offset within the file (for partial file injection)\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), name, offset) # Symbolize bytes\n        sym_seed = self.symbolic_seed.files[name] if self.seed.is_composite() else self.symbolic_seed\n        sym_seed[offset:offset+len(value)] = sym_vars # Add symbolic variables to symbolic seed\n        # FIXME: Handle if reading twice same input bytes !\n\n    def inject_symbolic_variable_memory(self, addr: Addr, name: str, value: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic variable in memory.\n\n        :param addr: address where to inject the variable\n        :param name: name of the variable in the composite seed\n        :param value: value of the variable\n        :param offset: offset within the variable (for partial variable injection)\n        :return:\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), name, offset)  # Symbolize bytes\n        self.symbolic_seed.variables[name][offset:offset+len(value)-1] = sym_vars # Add symbolic variables to symbolic seed\n        # FIXME: Handle if reading twice same input bytes !\n\n    def inject_symbolic_file_register(self, reg: Union[str, Register], name: str, value: int, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic file (or part of it) into a register.\n        The value has to be an integer.\n\n        :param reg: register identifier\n        :param name: name of the file in the composite seed\n        :param value: integer value\n        :param offset: offset within the file\n        \"\"\"\n        if reg.getSize != 1:\n            logger.error(\"can't call inject_symbolic_file_register with regsiter larger than 1!\")\n            return\n        self.pstate.write_register(reg, value)  # Write concrete value in register\n        sym_vars = self.pstate.symbolize_register(reg, f\"{name}[{offset}]\")  # Symbolize bytes\n        sym_seed = self.symbolic_seed.files[name] if self.seed.is_composite() else self.symbolic_seed\n        sym_seed[offset] = sym_vars  # Add symbolic variables to symbolic seed\n\n    def inject_symbolic_variable_register(self, reg: Union[str, Register], name: str, value: int) -> None:\n        \"\"\"\n        Inject a symbolic variable (or part of it) in a register.\n        The value has to be an integer.\n\n        :param reg: register identifier\n        :param name: name of the variable\n        :param value: integer value\n        \"\"\"\n        if not self.seed.is_composite():\n            logger.warning(\"cannot use inject_symbolic_variable_register on raw seeds!\")\n            return\n\n        if isinstance(value, int):\n            self.pstate.write_register(reg, value)                         # write concrete value in register\n            sym_var = self.pstate.symbolize_register(reg, f\"{name}[{0}]\")  # symbolize value\n            self.symbolic_seed.variables[name][0] = sym_var               # add the symbolic variables to symbolic seed\n        else:  # meant to be bytes\n            logger.warning(\"variable injected in registers have to be integer values\")\n\n    def inject_symbolic_raw_input(self, addr: Addr, data: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject the input in memory. This injection method should\n        be used for RAW seed type.\n\n        :param addr: address where to inject input.\n        :param data: content of the seed\n        :param offset: offset within the content of the seed.\n        \"\"\"\n        if self.seed.is_composite():\n            logger.warning(\"inject_symbolic_memory must not be used with composite seeds !\")\n        else:\n            self.inject_symbolic_file_memory(addr, \"input\", data, offset)", "class SymbolicExecutor(object):\n    \"\"\"\n    Single Program Execution Class.\n    That module, is in charge of performing the process loading from the given\n    program.\n    \"\"\"\n\n    def __init__(self, config: Config, seed: Seed = Seed(), workspace: Workspace = None, uid=0, callbacks=None):\n        \"\"\"\n        :param config: configuration file to use\n        :type config: Config\n        :param seed: input file to inject either in stdin or argv (optional)\n        :type seed: Seed\n        :param workspace: Workspace to use. If None it will be instanciated\n        :type workspace: Optional[Workspace]\n        :param uid: Unique ID. Given by :py:obj:`SymbolicExplorator` to identify uniquely executions\n        :type uid: int\n        :param callbacks: callbacks to bind on this execution before running *(instanciated if empty !)*\n        :type callbacks: CallbackManager\n        \"\"\"\n        self.config: Config = config      #: Configuration file used\n        self.loader: Type[Loader] = None  #: Loader used to run the code\n\n        self.pstate: ProcessState = None  #: ProcessState\n\n        self.workspace: Workspace = workspace  #: Current workspace\n        if self.workspace is None:\n            self.workspace = Workspace(config.workspace)\n\n        self.seed: Seed = seed  #: The current seed used for the execution\n\n        # Override config if there is a mismatch between seed format and config file\n        if seed.format != self.config.seed_format:\n            logger.warning(f\"seed format {seed.format} mismatch config {config.seed_format} (override config)\")\n            self.config.seed_format = seed.format\n\n        self.symbolic_seed = self._init_symbolic_seed(seed) #: symbolic seed (same structure than Seed but with symbols)\n\n        self.coverage: CoverageSingleRun = CoverageSingleRun(self.config.coverage_strategy)  #: Coverage of the execution\n        self.rtn_table = dict()   # Addr -> Tuple[fname, routine]\n        self.uid: int = uid       #: Unique identifier meant to unique accross Exploration instances\n        self.start_time: int = 0  #: start time of the process\n        self.end_time: int = 0    #: end time of the process\n\n        # create callback object if not provided as argument, and bind callbacks to the current process state\n        self.cbm: CallbackManager = callbacks if callbacks is not None else CallbackManager()\n        \"\"\"callback manager\"\"\"\n\n        # List of new seeds filled during the execution and flushed by explorator\n        self._pending_seeds = []\n        self._run_to_target = None\n\n        self.trace_offset: int = 0  #: counter of instructions executed\n\n        self.previous_pc: int = 0  #: previous program counter executed\n        self.current_pc = 0        #: current program counter\n\n        self.debug_pp = False\n\n        self._in_processing = False  # use to know if we are currently processing an instruction\n\n        # TODO: Here we load the binary each time we run an execution (via ELFLoader). We can\n        #       avoid this (and so gain in speed) if a TritonContext could be forked from a\n        #       state. See: https://github.com/JonathanSalwan/Triton/issues/532\n\n    def _init_symbolic_seed(self, seed: Seed) -> Union[list, CompositeData]:\n        if seed.is_raw():\n            return [None]*len(seed.content)\n        else:  # is composite\n            argv = [[None]*len(a) for a in seed.content.argv]\n            files = {k: [None]*len(v) for k, v in seed.content.files.items()}\n            variables = {k: [None]*(1 if isinstance(v, int) else len(v)) for k, v in seed.content.variables.items()}\n            return CompositeData(argv=argv, files=files, variables=variables)\n\n    def load(self, loader: Loader) -> None:\n        \"\"\"\n        Use the given loader to initialize the ProcessState.\n        It overrides the current ProcessState if any.\n\n        :param loader: Loader describing how to load\n        :return: None\n        \"\"\"\n\n        # Initialize the process_state architecture (at this point arch is sure to be supported)\n        self.loader = loader\n        logger.debug(f\"Loading program {self.loader.name} [{self.loader.architecture}]\")\n        self.pstate = ProcessState.from_loader(loader)\n        self._map_dynamic_symbols()\n        self._load_seed_process_state(self.pstate, self.seed)\n\n    def load_process(self, pstate: ProcessState) -> None:\n        \"\"\"\n        Load the given process state. Do nothing but\n        setting the internal ProcessState.\n\n        :param pstate: PrcoessState to set\n        \"\"\"\n        self.pstate = pstate\n        self._load_seed_process_state(self.pstate, self.seed)\n\n    @staticmethod\n    def _load_seed_process_state(pstate: ProcessState, seed: Seed) -> None:\n        if seed.is_raw():\n            data = seed.content\n        else:  # is composite\n            if seed.is_file_defined(\"stdin\"):\n                data = seed.get_file_input(\"stdin\")\n            else:\n                return\n        filedesc = pstate.get_file_descriptor(0)\n        filedesc.fd = io.BytesIO(data)\n\n    @property\n    def execution_time(self) -> int:\n        \"\"\"\n        Time taken for the execution in seconds\n\n        .. warning:: Only relevant at the end of the execution\n\n        :return: execution time (in s)\n        \"\"\"\n        return self.end_time - self.start_time\n\n    @property\n    def pending_seeds(self) -> List[Seed]:\n        \"\"\"\n        List of pending seeds gathered during execution.\n\n        .. warning:: Only relevant at the end of execution\n\n        :returns: list of new seeds generated\n        :rtype: List[Seed]\n        \"\"\"\n        return self._pending_seeds\n\n    def enqueue_seed(self, seed: Seed) -> None:\n        \"\"\"\n        Add a seed to the queue of seed to be executed in later iterations.\n        This function is meant to be used by user callbacks.\n\n        :param seed: Seed to be added\n        :type seed: Seed\n        \"\"\"\n        self._pending_seeds.append(seed)\n\n    @property\n    def callback_manager(self) -> CallbackManager:\n        \"\"\"\n        Get the callback manager associated with the execution.\n\n        :rtype: CallbackManager\"\"\"\n        return self.cbm\n\n    def is_seed_injected(self) -> bool:\n        \"\"\"\n        Get whether or not the seed has been injected.\n\n        :return: True if the seed has already been inserted\n        \"\"\"\n        if self.config.is_format_raw():\n            return bool(self.symbolic_seed)\n        elif self.config.is_format_composite():\n            # Namely has one of the various input been injected or not\n            return bool(self.symbolic_seed.content.files) or bool(self.symbolic_seed.content.variables)\n        else:\n            assert False\n\n    def _configure_pstate(self) -> None:\n        #for mode in [MODE.ALIGNED_MEMORY, MODE.AST_OPTIMIZATIONS, MODE.CONSTANT_FOLDING, MODE.ONLY_ON_SYMBOLIZED]:\n        for mode in [MODE.ONLY_ON_SYMBOLIZED]:\n            self.pstate.set_triton_mode(mode, True)\n        logger.info(f\"configure pstate: time_inc:{self.config.time_inc_coefficient}  solver:{self.config.smt_solver.name}  timeout:{self.config.smt_timeout}\")\n        self.pstate.time_inc_coefficient = self.config.time_inc_coefficient\n        self.pstate.set_solver_timeout(self.config.smt_timeout)\n        self.pstate.set_solver(self.config.smt_solver)\n\n    def _fetch_next_thread(self, threads: List[ThreadContext]) -> Optional[ThreadContext]:\n        \"\"\"\n        Given a list of threads, returns the next to execute. Iterating\n        threads in a round-robin style picking the next item in the list.\n\n        :param threads: list of threads\n        :return: thread context\n        \"\"\"\n        cur_idx = threads.index(self.pstate.current_thread)\n\n        tmp_list = threads[cur_idx+1:]+threads[:cur_idx]  # rotate list (and exclude current_item)\n        for th in tmp_list:\n            if th.is_running():\n                return th  # Return the first thread that is properly running\n        return None\n\n    def __schedule_thread(self) -> None:\n        threads_list = self.pstate.threads\n\n        if len(threads_list) == 1:  # If there is only one thread no need to schedule another thread\n            return\n\n        if self.pstate.current_thread.count > self.config.thread_scheduling:\n            # Select the next thread to execute\n            next_th = self._fetch_next_thread(threads_list)\n\n            if next_th:  # We found another thread to schedule\n\n                # Call all callbacks related to threads\n                for cb in self.cbm.get_context_switch_callback():\n                    cb(self, self.pstate, self.pstate.current_thread)\n\n                # Save current context and restore new thread context (+kill current if dead)\n                self.pstate.switch_thread(next_th)\n\n            else:  # There are other thread but not other one is available (thus keep current one)\n                self.pstate.current_thread.count = 0  # Reset its counter\n\n        else:\n            # Increment the instruction counter of the thread (a bit in advance but it does not matter)\n            self.pstate.current_thread.count += 1\n\n    def _symbolic_mem_callback(self, se: 'SymbolicExecutor', ps: ProcessState, mem: MemoryAccess, *args):\n        tgt_addr = mem.getAddress()\n        lea_ast = mem.getLeaAst()\n        if lea_ast is None:\n            return\n        if lea_ast.isSymbolized():\n            s = \"write\" if bool(args) else \"read\"\n            pc = self.pstate.cpu.program_counter\n            logger.debug(f\"symbolic {s} at 0x{pc:x}: target: 0x{tgt_addr:x} [{lea_ast}]\")\n            self.pstate.push_constraint(lea_ast == tgt_addr, f\"sym-{s}:{self.trace_offset}:{pc}\")\n\n    def emulate(self):\n        while not self.pstate.stop and self.pstate.threads:\n            if not self.step():\n                break\n        if not self.seed.is_status_set():  # Set a status if it has not already been done\n            self.seed.status = SeedStatus.OK_DONE\n        return\n\n\n    def step(self) -> bool:\n        \"\"\"\n        Perform a single instruction step. Returns whether the emulation can\n        continue or we have to stop.\n        \"\"\"\n        try:\n            # Schedule thread if it's time\n            self.__schedule_thread()\n\n            if not self.pstate.current_thread.is_running():\n                logger.warning(f\"After scheduling current thread is not running (probably in a deadlock state)\")\n                return False  # Were not able to find a suitable thread thus exit emulation\n\n            # Fetch program counter (of the thread selected), at this point the current thread should be running!\n            self.current_pc = self.pstate.cpu.program_counter  # should normally be already set but still.\n\n            if self.current_pc == self._run_to_target:  # Hit the location we wanted to reach\n                return False\n\n            if self.current_pc == 0:\n                logger.error(f\"PC=0, is it normal ? (stop)\")\n                return False\n\n            if self.pstate.memory.segmentation_enabled:\n                if not self.pstate.memory.has_ever_been_written(self.current_pc, CPUSIZE.BYTE):\n                    logger.error(f\"Instruction not mapped: 0x{self.current_pc:x}\")\n                    return False\n\n            instruction = self.pstate.fetch_instruction()\n            opcode = instruction.getOpcode()\n            mnemonic = instruction.getType()\n\n            try:\n                # Trigger pre-address callback\n                pre_cbs, post_cbs = self.cbm.get_address_callbacks(self.current_pc)\n                for cb in pre_cbs:\n                    cb(self, self.pstate, self.current_pc)\n\n                # Trigger pre-opcode callback\n                pre_opcode, post_opcode = self.cbm.get_opcode_callbacks(opcode)\n                for cb in pre_opcode:\n                    cb(self, self.pstate, opcode)\n\n                # Trigger pre-mnemonic callback\n                pre_mnemonic, post_mnemonic = self.cbm.get_mnemonic_callbacks(mnemonic)\n                for cb in pre_mnemonic:\n                    cb(self, self.pstate, mnemonic)\n\n                # Trigger pre-instruction callback\n                pre_insts, post_insts = self.cbm.get_instruction_callbacks()\n                for cb in pre_insts:\n                    cb(self, self.pstate, instruction)\n            except SkipInstructionException as _:\n                return True\n\n            if self.pstate.is_syscall():\n                logger.warning(f\"execute syscall instruction {self.pstate.read_register(self.pstate._syscall_register)}\")\n\n            # Process\n            prev_pc = self.current_pc\n            self._in_processing = True\n            if not self.pstate.process_instruction(instruction):\n                if self.pstate.is_halt_instruction():\n                    logger.info(f\"hit {str(instruction)} instruction stop.\")\n                else:\n                    logger.error('Instruction not supported: %s' % (str(instruction)))\n\n                if self.config.skip_unsupported_instruction:\n                    self.pstate.cpu.program_counter += instruction.getSize() # try to jump over the instruction\n                else:\n                    return False  # stop emulation\n            self._in_processing = False\n            # increment trace offset\n            self.trace_offset += 1\n\n            # update previous program counters\n            self.previous_pc = prev_pc\n            self.current_pc = self.pstate.cpu.program_counter  # current_pc becomes new instruction pointer\n\n            # Update the coverage of the execution\n            self.coverage.add_covered_address(self.previous_pc)\n\n            # Update coverage send it the last PathConstraint object if one was added\n            if self.pstate.is_path_predicate_updated():\n                path_constraint = self.pstate.last_branch_constraint\n\n                if path_constraint.isMultipleBranches():\n                    branches = path_constraint.getBranchConstraints()\n                    if len(branches) != 2:\n                        logger.error(\"Branching condition has more than two branches\")\n                    taken, not_taken = branches if branches[0]['isTaken'] else branches[::-1]\n                    taken_addr, not_taken_addr = taken['dstAddr'], not_taken['dstAddr']\n\n                    for cb in self.cbm.get_on_branch_covered_callback():\n                        cb(self, self.pstate, (self.previous_pc, taken_addr))\n\n                    self.coverage.add_covered_branch(self.previous_pc, taken_addr, not_taken_addr)\n\n                else:  # It is normally a dynamic jump or symbolic memory read/write\n                    cmt = path_constraint.getComment()\n                    if cmt.startswith(\"sym-read\") or cmt.startswith(\"sym-write\"):\n                        pass\n                        # NOTE: At the moment it does not seems suitable to count r/w pointers\n                        # as part of the coverage. So does not have an influence on covered/not_covered.\n                    else:\n                        logger.warning(f\"New dynamic jump covered at: {self.previous_pc:08x}\")\n                        path_constraint.setComment(f\"dyn-jmp:{self.trace_offset}:{self.previous_pc}\")\n                        self.coverage.add_covered_dynamic_branch(self.previous_pc, self.current_pc)\n\n            # Trigger post-opcode callback\n            for cb in post_opcode:\n                cb(self, self.pstate, opcode)\n\n            # Trigger post-mnemonic callback\n            for cb in post_mnemonic:\n                cb(self, self.pstate, mnemonic)\n\n            # Trigger post-instruction callback\n            for cb in post_insts:\n                cb(self, self.pstate, instruction)\n\n            # Trigger post-address callbacks\n            for cb in post_cbs:\n                cb(self, self.pstate, self.previous_pc)\n\n            # Simulate routines\n            try:\n                self._routines_handler(instruction)\n            except AllocatorException as e:\n                logger.info(f'An exception has been raised: {e}')\n                self.seed.status = SeedStatus.CRASH\n                return False\n\n            # Check timeout of the execution\n            if self.config.execution_timeout and (time.time() - self.start_time) >= self.config.execution_timeout:\n                logger.info('Timeout of an execution reached')\n                self.seed.status = SeedStatus.HANG\n                return False\n            return True\n        except AbortExecutionException as e:\n            return False\n        except MemoryAccessViolation as e:\n            logger.warning(f\"Memory violation: {str(e)}\")\n\n            # Call all the callbacks on the memory violations\n            for cb in self.callback_manager.get_memory_violation_callbacks():\n                cb(self, self.pstate, e)\n\n            # Assign the seed the status of crash\n            if not self.seed.is_status_set():\n                self.seed.status = SeedStatus.CRASH\n            return False\n\n\n    def __handle_external_return(self, routine_name: str, ret_val: Optional[Union[int, Expression]]) -> None:\n        \"\"\" Symbolize or concretize return values of external functions \"\"\"\n        if ret_val is not None:\n            reg = self.pstate.return_register\n            if isinstance(ret_val, int): # Write its concrete value\n                self.pstate.write_register(reg, ret_val)\n            else:  # It should be a logic expression\n                self.pstate.write_symbolic_register(reg, ret_val, f\"(routine {routine_name}\")\n\n    def _routines_handler(self, instruction: Instruction):\n        \"\"\"\n        This function handle external routines calls. When the .plt jmp on an external\n        address, we call the appropriate Python routine and setup the returned value\n        which may be concrete or symbolic.\n\n        :param instruction: The current instruction executed\n        :return: None\n        \"\"\"\n        pc = self.pstate.cpu.program_counter\n        if pc in self.rtn_table:\n            routine_name, routine = self.rtn_table[pc]\n            logger.debug(f\"Enter external routine: {routine_name}\")\n\n            # Trigger pre-address callback\n            pre_cbs, post_cbs = self.cbm.get_imported_routine_callbacks(routine_name)\n\n            ret_val = None\n            for cb in pre_cbs:\n                ret = cb(self, self.pstate, routine_name, pc)\n                if ret is not None:  # if the callback return a value the function behavior will be skipped\n                    ret_val = ret\n                    break  # Set the ret val and break\n\n            if ret_val is None:  # If no ret_val has been set by any callback function call the supported routine\n                # Emulate the routine and the return value\n                ret_val = routine(self, self.pstate)\n\n            self.__handle_external_return(routine_name, ret_val)\n\n            # Trigger post-address callbacks\n            for cb in post_cbs:\n                cb(self, self.pstate, routine_name, pc)\n\n            # Do not continue the execution if we are in a locked mutex\n            if self.pstate.mutex_locked:\n                self.pstate.mutex_locked = False\n                self.pstate.cpu.program_counter = instruction.getAddress()\n                # It's locked, so switch to another thread\n                self.pstate.current_thread.count = self.config.thread_scheduling+1\n                return\n\n            # Do not continue the execution if we are in a locked semaphore\n            if self.pstate.semaphore_locked:\n                self.pstate.semaphore_locked = False\n                self.pstate.cpu.program_counter = instruction.getAddress()\n                # It's locked, so switch to another thread\n                self.pstate.current_thread.count = self.config.thread_scheduling+1\n                return\n\n            if self.pstate.architecture == Architecture.AARCH64:\n                # Get the return address\n                ret_addr = self.pstate.read_register('x30')\n            elif self.pstate.architecture in [Architecture.X86, Architecture.X86_64]:\n                # Get the return address and restore RSP (simulate RET)\n                ret_addr = self.pstate.pop_stack_value()\n            else:\n                raise Exception(\"Architecture not supported\")\n\n            # Hijack RIP to skip the call\n            self.pstate.cpu.program_counter = ret_addr\n\n    def _map_dynamic_symbols(self) -> None:\n        \"\"\"\n        Apply dynamic relocations of imported functions and imported symbols\n        regardless of the architecture or executable format\n        .. FIXME: This function does not apply all possible relocations\n        :return: None\n        \"\"\"\n        for symbol, (addr, is_func) in self.pstate.dynamic_symbol_table.items():\n\n            if symbol in SUPPORTED_ROUTINES:  # if the routine name is supported\n                # Add link to the routine and got tables\n                self.rtn_table[addr] = (symbol, SUPPORTED_ROUTINES[symbol])\n\n            elif symbol in SUPORTED_GVARIABLES:\n                # if self.pstate.architecture == Architecture.X86_64:\n                self.pstate.memory.write_ptr(addr, SUPORTED_GVARIABLES[symbol])  # write directly at addr\n                # elif self.pstate.architecture == Architecture.AARCH64:\n                #     val = self.pstate.memory.read_ptr(addr)\n                #     self.pstate.memory.write_ptr(val, SUPORTED_GVARIABLES[symbol])\n\n            else:  # the symbol is not supported\n                if self.uid == 0:  # print warning if first uid (so that it get printed once)\n                    logger.warning(f\"symbol {symbol} imported but unsupported\")\n                if is_func:\n                    # Add link to a default stub function\n                    self.rtn_table[addr] = (symbol, self.__default_stub)\n                else:\n                    pass # do nothing on unsupported symbols\n\n    def __default_stub(self, se: 'SymbolicExecutor', pstate: ProcessState):\n        rtn_name, _ = self.rtn_table[pstate.cpu.program_counter]\n        logger.warning(f\"calling {rtn_name} which is unsupported\")\n        if self.config.skip_unsupported_import:\n            return None  # Like if function did nothing\n        else:\n            self.abort()\n\n    def abort(self) -> NoReturn:\n        \"\"\"\n        Abort the current execution. It works by raising\n        an exception which is caught by the emulation function\n        that takes care of returning appropriately afterward.\n\n        :raise AbortExecutionException: to abort execution from anywhere\n        \"\"\"\n        raise AbortExecutionException('Execution aborted')\n\n    def skip_instruction(self) -> NoReturn:\n        \"\"\"\n        Skip the current instruction before it gets executed. It is only\n        relevant to call it from pre-inst or pre-addr callbacks.\n\n        :raise SkipInstructionException: to skip the current instruction\n        \"\"\"\n        raise SkipInstructionException(\"Skip instruction\")\n\n    def stop_exploration(self) -> NoReturn:\n        \"\"\"\n        Function to call to stop the whole exploration of\n        the program. It raises an exception which is caught by SymbolicExplorator.\n\n        :raise StopExplorationException: to stop the exploration\n        \"\"\"\n        raise StopExplorationException(\"Stop exploration\")\n\n    def emulation_init(self) -> bool:\n        if self.pstate is None:\n            logger.error(f\"ProcessState is None (have you called \\\"load\\\"?\")\n            return False\n\n        self.start_time = time.time()\n\n        # Configure memory segmentation using configuration\n        self.pstate.memory.set_segmentation(self.config.memory_segmentation)\n        if self.config.memory_segmentation:\n            self.cbm.register_memory_read_callback(self._mem_accesses_callback)\n            self.cbm.register_memory_write_callback(self._mem_accesses_callback)\n\n        # Register memory callbacks in case we activated covering mem access\n        if BranchSolvingStrategy.COVER_SYM_READ in self.config.branch_solving_strategy:\n            self.cbm.register_memory_read_callback(self._symbolic_mem_callback)\n        if BranchSolvingStrategy.COVER_SYM_WRITE in self.config.branch_solving_strategy:\n            self.cbm.register_memory_write_callback(self._symbolic_mem_callback)\n\n        # bind dbm callbacks on the process state (newly initialized)\n        self.cbm.bind_to(self)  # bind call\n\n        # Let's emulate the binary from the entry point\n        logger.info('Starting emulation')\n\n        # Get pre/post callbacks on execution\n        pre_cb, post_cb = self.cbm.get_execution_callbacks()\n        # Iterate through all pre exec callbacks\n        for cb in pre_cb:\n            cb(self, self.pstate)\n\n        # Call it here to make sure in case of \"load_process\" the use has properly instanciated the architecture\n        self._configure_pstate()\n        return True\n\n    def run(self, stop_at: Addr = None) -> None:\n        \"\"\"\n        Execute the program.\n\n        If the :py:attr:`tritondse.Config.execution_timeout` is not set\n        the execution might hang forever if the program does.\n\n        :param stop_at: Address where to stop (if necessary)\n        :return: None\n        \"\"\"\n        if stop_at:\n            self._run_to_target = stop_at\n\n        # Call init steps\n        if not self.emulation_init():\n            return\n\n        # Run until reaching a stopping condition\n        self.emulate()\n\n        # Call termination steps\n        self.emulation_deinit()\n\n    def emulation_deinit(self):\n        _, post_cb = self.cbm.get_execution_callbacks()\n        # Iterate through post exec callbacks\n        for cb in post_cb:\n            cb(self, self.pstate)\n\n        self.end_time = time.time()\n\n        # IMPORTANT The next call is necessary otherwise there is a memory\n        #           leak issues.\n        # Unbind callbacks from the current symbolic executor instance.\n        self.cbm.unbind()\n\n        # NOTE Unregister callbacks registered at the beginning of the function.\n        #      This is necessary because we currently have a circular dependency\n        #      between this class and the callback manager. Note that we create\n        #      that circular dependency indirectly when we set the callback to\n        #      a method of this class (_mem_accesses_callback and\n        #      _symbolic_mem_callback).\n        if self.config.memory_segmentation:\n            self.cbm.unregister_callback(self._mem_accesses_callback)\n\n        if BranchSolvingStrategy.COVER_SYM_READ in self.config.branch_solving_strategy:\n            self.cbm.unregister_callback(self._symbolic_mem_callback)\n        if BranchSolvingStrategy.COVER_SYM_WRITE in self.config.branch_solving_strategy:\n            self.cbm.unregister_callback(self._symbolic_mem_callback)\n\n        logger.info(f\"Emulation done [ret:{self.pstate.read_register(self.pstate.return_register):x}]  (time:{self.execution_time:.02f}s)\")\n        logger.info(f\"Instructions executed: {self.coverage.total_instruction_executed}  symbolic branches: {self.pstate.path_predicate_size}\")\n        logger.info(f\"Memory usage: {self.mem_usage_str()}\")\n\n    def _mem_accesses_callback(self, se: 'SymbolicExecutor', ps: ProcessState, mem: MemoryAccess, *args):\n        \"\"\"\n        This callback is used to ensure memory accesses performed by side-effect of instructions\n        semantic correctly checks memory segmentation. Thus we only do the check during the processing\n        of an instruction.\n        \"\"\"\n        if ps.memory.segmentation_enabled and self._in_processing:\n            perm = Perm.W if bool(args) else Perm.R\n            addr = mem.getAddress()\n            size = mem.getSize()\n            map = ps.memory.get_map(addr, size)  # It raises\n            if map is None:\n                raise MemoryAccessViolation(addr, perm, memory_not_mapped=True)\n            else:\n                if perm not in map.perm:\n                    raise MemoryAccessViolation(addr, perm, map_perm=map.perm, perm_error=True)\n\n    @property\n    def exitcode(self) -> int:\n        \"\"\" Exit code value of the process. The value\n        is simply the concrete value of the register\n        marked as return_register (rax, on x86, r0 on ARM..)\n        \"\"\"\n        return self.pstate.read_register(self.pstate.return_register) & 0xFF\n\n    @staticmethod\n    def mem_usage_str() -> str:\n        \"\"\"\n        Debug function to track memory consumption of an execution (not\n        implemented on Windows).\n        \"\"\"\n        if os.name == \"posix\":\n            size, resident, shared, _, _, _, _ = (int(x) for x in open(f\"/proc/{os.getpid()}/statm\").read().split(\" \"))\n            resident = resident * resource.getpagesize()\n            units = [(float(1024), \"Kb\"), (float(1024 **2), \"Mb\"), (float(1024 **3), \"Gb\")]\n            for unit, s in units[::-1]:\n                if resident / unit < 1:\n                    continue\n                else:  # We are on the right unit\n                    return \"%.2f%s\" % (resident/unit, s)\n            return \"%dB\" % resident\n        else:\n          return \"N/A\"\n\n    def mk_new_seed_from_model(self, model: Model) -> Seed:\n        \"\"\"\n        Creates a new seed from the given SMT model.\n\n        :param model: SMT model\n        :return: new seed object\n        \"\"\"\n        def repl_bytearray(concrete, symbolic):\n            for i, sv in enumerate(symbolic):  # Enumerate symvars associated with each bytes\n                if sv is not None:\n                    if sv.getId() in model:  # If solver provided a new value for the symvar\n                        value = model[sv.getId()].getValue()\n                        concrete[i] = value # Replace it in the bytearray\n            return concrete\n\n        if self.config.is_format_raw(): # RAW seed. => symbolize_stdin\n            content = bytes(repl_bytearray(bytearray(self.seed.content), self.symbolic_seed))\n\n        elif self.config.is_format_composite():\n            # NOTE will have to update this if more things are added to CompositeData\n            new_files, new_vars = {}, {}\n\n            # Handle argv (its meant to be here)\n            args = [bytearray(x) for x in self.seed.content.argv]\n            new_argv = [bytes(repl_bytearray(c, s)) for c, s in zip(args, self.symbolic_seed.argv)]\n\n            # Handle stdin and files\n            # If the seed provides the content of files (#NOTE stdin is treated as a file)\n            new_files = {}\n            for k, c in self.seed.content.files.items():\n                if k in self.symbolic_seed.files:\n                    new_files[k] = bytes(repl_bytearray(bytearray(c), self.symbolic_seed.files[k]))\n                else:\n                    new_files[k] = c  # keep the current value in the seed\n\n            # Handle variables, if the seed provides some\n            new_variables = {}\n            for k, c in self.seed.content.variables.items():\n                if k in self.symbolic_seed.variables:\n                    conc = bytearray(c) if isinstance(c, bytes) else [c]\n                    new_vals = repl_bytearray(conc, self.symbolic_seed.variables[k])\n                    new_variables[k] = bytes(new_vals) if isinstance(c, bytes) else new_vals[0]  # new variables are either bytes or int\n                else:\n                    new_variables[k] = c  # If it has not been injected keep the current concrete value\n\n            content = CompositeData(new_argv, new_files, new_variables)\n        else:\n            assert False\n\n        # Calling callback if user defined one\n        new_seed = Seed(content)\n        for cb in self.cbm.get_new_input_callback():\n            cont = cb(self, self.pstate, new_seed)\n            if cont:\n                # if the callback return a new input continue with that one\n                new_seed = cont\n        # Return the\n        return new_seed\n\n    def inject_symbolic_argv_memory(self, addr: Addr, index: int, value: bytes) -> None:\n        \"\"\"\n        Inject the ith item of argv in memory.\n        To be used only with composite seeds and only if seed have a symbolic argv\n\n        :param addr: address where to inject the argv[ith]\n        :param index: ith argv item\n        :param value: value of the item\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), f\"argv[{index}]\") # Symbolize bytes\n        self.symbolic_seed.argv[index] = sym_vars # Add symbolic variables to symbolic seed\n\n    def inject_symbolic_file_memory(self, addr: Addr, name: str, value: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic file (or part of it) in memory.\n\n        :param addr: address where to inject the file bytes\n        :param name: name of the file in the composite seed\n        :param value: bytes content of the file\n        :param offset: offset within the file (for partial file injection)\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), name, offset) # Symbolize bytes\n        sym_seed = self.symbolic_seed.files[name] if self.seed.is_composite() else self.symbolic_seed\n        sym_seed[offset:offset+len(value)] = sym_vars # Add symbolic variables to symbolic seed\n        # FIXME: Handle if reading twice same input bytes !\n\n    def inject_symbolic_variable_memory(self, addr: Addr, name: str, value: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic variable in memory.\n\n        :param addr: address where to inject the variable\n        :param name: name of the variable in the composite seed\n        :param value: value of the variable\n        :param offset: offset within the variable (for partial variable injection)\n        :return:\n        \"\"\"\n        self.pstate.memory.write(addr, value)  # Write concrete bytes in memory\n        sym_vars = self.pstate.symbolize_memory_bytes(addr, len(value), name, offset)  # Symbolize bytes\n        self.symbolic_seed.variables[name][offset:offset+len(value)-1] = sym_vars # Add symbolic variables to symbolic seed\n        # FIXME: Handle if reading twice same input bytes !\n\n    def inject_symbolic_file_register(self, reg: Union[str, Register], name: str, value: int, offset: int = 0) -> None:\n        \"\"\"\n        Inject a symbolic file (or part of it) into a register.\n        The value has to be an integer.\n\n        :param reg: register identifier\n        :param name: name of the file in the composite seed\n        :param value: integer value\n        :param offset: offset within the file\n        \"\"\"\n        if reg.getSize != 1:\n            logger.error(\"can't call inject_symbolic_file_register with regsiter larger than 1!\")\n            return\n        self.pstate.write_register(reg, value)  # Write concrete value in register\n        sym_vars = self.pstate.symbolize_register(reg, f\"{name}[{offset}]\")  # Symbolize bytes\n        sym_seed = self.symbolic_seed.files[name] if self.seed.is_composite() else self.symbolic_seed\n        sym_seed[offset] = sym_vars  # Add symbolic variables to symbolic seed\n\n    def inject_symbolic_variable_register(self, reg: Union[str, Register], name: str, value: int) -> None:\n        \"\"\"\n        Inject a symbolic variable (or part of it) in a register.\n        The value has to be an integer.\n\n        :param reg: register identifier\n        :param name: name of the variable\n        :param value: integer value\n        \"\"\"\n        if not self.seed.is_composite():\n            logger.warning(\"cannot use inject_symbolic_variable_register on raw seeds!\")\n            return\n\n        if isinstance(value, int):\n            self.pstate.write_register(reg, value)                         # write concrete value in register\n            sym_var = self.pstate.symbolize_register(reg, f\"{name}[{0}]\")  # symbolize value\n            self.symbolic_seed.variables[name][0] = sym_var               # add the symbolic variables to symbolic seed\n        else:  # meant to be bytes\n            logger.warning(\"variable injected in registers have to be integer values\")\n\n    def inject_symbolic_raw_input(self, addr: Addr, data: bytes, offset: int = 0) -> None:\n        \"\"\"\n        Inject the input in memory. This injection method should\n        be used for RAW seed type.\n\n        :param addr: address where to inject input.\n        :param data: content of the seed\n        :param offset: offset within the content of the seed.\n        \"\"\"\n        if self.seed.is_composite():\n            logger.warning(\"inject_symbolic_memory must not be used with composite seeds !\")\n        else:\n            self.inject_symbolic_file_memory(addr, \"input\", data, offset)", ""]}
{"filename": "tritondse/coverage.py", "chunked_list": ["# built-in imports\nfrom __future__ import annotations\nimport hashlib\nimport struct\nfrom pathlib import Path\nfrom typing import List, Generator, Tuple, Set, Union, Dict, Optional\nfrom collections import Counter\nfrom enum import IntFlag, Enum, auto\nimport pickle\nimport enum_tools.documentation", "import pickle\nimport enum_tools.documentation\n\n# third-party imports\nfrom triton import AST_NODE\n\n# local imports\nfrom tritondse.types import Addr, PathConstraint, PathBranch, SolverStatus, PathHash, Edge, SymExType\nimport tritondse.logging\n", "import tritondse.logging\n\nlogger = tritondse.logging.get()\n\nCovItem = Union[Addr, Edge, PathHash, Tuple[PathHash, Edge]]\n\"\"\"\nVariant type representing a coverage item.\nIt can be:\n\n* an address :py:obj:`tritondse.types.Addr` for block coverage", "\n* an address :py:obj:`tritondse.types.Addr` for block coverage\n* an edge :py:obj:`tritondse.types.Edge` for edge coverage\n* a string :py:obj:`tritondse.types.PathHash` for path coverage\n* a tuple of both a Pathhash and an edge\n\"\"\"\n\n@enum_tools.documentation.document_enum\nclass CoverageStrategy(str, Enum):\n    \"\"\"\n    Coverage strategy (metric) enum.\n    This enum will change whether a given branch have\n    to be solved or not.\n    \"\"\"\n\n    BLOCK = \"block\"   # doc: block coverage, only tracks new basic blocks covered\n    EDGE = \"edge\"     # doc: edge coverage, tracks CFGs edges covered\n    PATH = \"path\"     # doc: tracks any new path covered\n    PREFIXED_EDGE = \"PREFIXED_EDGE\"  # doc: edge coverage but also taking in account path prefix)", "class CoverageStrategy(str, Enum):\n    \"\"\"\n    Coverage strategy (metric) enum.\n    This enum will change whether a given branch have\n    to be solved or not.\n    \"\"\"\n\n    BLOCK = \"block\"   # doc: block coverage, only tracks new basic blocks covered\n    EDGE = \"edge\"     # doc: edge coverage, tracks CFGs edges covered\n    PATH = \"path\"     # doc: tracks any new path covered\n    PREFIXED_EDGE = \"PREFIXED_EDGE\"  # doc: edge coverage but also taking in account path prefix)", "\n\n@enum_tools.documentation.document_enum\nclass BranchSolvingStrategy(IntFlag):\n    \"\"\"\n    Branch strategy enumerate.\n    It defines the manner with which branches are checked with SMT\n    on a single trace, namely a :py:obj:`CoverageSingleRun`. For a\n    given branch that has not been covered strategies are:\n\n    * ``ALL_NOT_COVERED``: check by SMT all occurences\n    * ``FIRST_LAST_NOT_COVERED``: check only the first and last occurence in the trace\n    \"\"\"\n    ALL_NOT_COVERED = auto()         # doc: check by SMT all occurences of a given branch (true by default)\n    FIRST_LAST_NOT_COVERED = auto()  # doc: check by SMT the first and last occurence of a given branch\n    UNSAT_ONCE = auto()              # doc: if a branch is UNSAT do not try solving it again\n    TIMEOUT_ONCE = auto()            # doc: if a branch is TIMEOUT do not try solving it again\n    TIMEOUT_ALWAYS = auto()          # doc: always try solving again a TIMEOUT branch (incompatible with :py:enum:mem:`TIMEOUT_ONCE`\n    COVER_SYM_DYNJUMP = auto()       # doc: try covering dynamic jumps on a symbolic register or memory value\n    COVER_SYM_READ = auto()          # doc: try enumerating values for symbolic reads\n    COVER_SYM_WRITE = auto()         # doc: try enumerating values for symbolic writes\n    SOUND_MEM_ACCESS = auto()        # doc: enables adding a constraint when using a symbolic read/write or jump\n    MANUAL = auto()                  # doc: disable automatic branch solving after an execution (has to be done manually in callbacks)", "\n\nclass CoverageSingleRun(object):\n    \"\"\"\n    Coverage produced by a **Single Execution**\n    Depending on the strategy given to the constructor\n    it stores different data.\n    \"\"\"\n\n    def __init__(self, strategy: CoverageStrategy):\n        \"\"\"\n        :param strategy: Strategy to employ\n        :type strategy: CoverageStrategy\n        \"\"\"\n        self.strategy: CoverageStrategy = strategy  #: Coverage strategy\n\n        # For instruction coverage\n        self.covered_instructions: Dict[Addr, int] = Counter()\n        \"\"\" Instruction coverage. Counter for code coverage) \"\"\"\n\n        self.covered_items: Dict[CovItem, int] = Counter()\n        \"\"\" Stores covered items whatever they are \"\"\"\n        self.not_covered_items: Set[CovItem] = set()\n        self._not_covered_items_mirror: Dict[CovItem, List[str]] = {}  # solely used for prefixed-edge\n        \"\"\" CovItems not covered in the trace. It thus represent what can be\n        covered by the trace (input). We call it coverage objectives.\"\"\"\n\n        # For path coverage\n        self._current_path: List[Addr] = []\n        \"\"\" List of addresses forming the path currently being taken \"\"\"\n        self._current_path_hash = hashlib.md5()\n\n    def add_covered_address(self, address: Addr) -> None:\n        \"\"\"\n        Add an instruction address covered.\n        *(Called by :py:obj:`SymbolicExecutor` for each\n        instruction executed)*\n\n        :param address: The address of the instruction\n        :type address: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n        self.covered_instructions[address] += 1\n\n    def add_covered_dynamic_branch(self, source: Addr, target: Addr) -> None:\n        \"\"\"\n        Add a dynamic branch covered. The branch will be encoded according to the\n        coverage strategy.\n\n        :param source: Address of the dynamic jump\n        :param target: Target address on which the jump is performed\n        :return:\n        \"\"\"\n        if self.strategy == CoverageStrategy.BLOCK:\n            pass # Target address will be covered anyway\n\n        if self.strategy == CoverageStrategy.EDGE:\n            self.covered_items[(source, target)] += 1\n            self.not_covered_items.discard((source, target))    # Remove it from non-taken if it was inside\n\n        if self.strategy == CoverageStrategy.PATH:\n            self._current_path.append(target)\n            self._current_path_hash.update(struct.pack(\"<Q\", target))\n            self.covered_items[self._current_path_hash.hexdigest()] += 1\n\n        if self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            # Add covered as covered\n            self.covered_items[(\"\", (source, target))] += 1\n            # update the current path hash etc\n            self._current_path.append(target)\n            self._current_path_hash.update(struct.pack(\"<Q\", target))\n\n    def add_covered_branch(self, program_counter: Addr, taken_addr: Addr, not_taken_addr: Addr) -> None:\n        \"\"\"\n        Add a branch to our covered branches list. Each branch is encoded according\n        to the coverage strategy. For code coverage, the branch encoding is the\n        address of the instruction. For edge coverage, the branch encoding is the\n        tupe (src address, dst address). For path coverage, the branch encoding\n        is the MD5 of the conjunction of all taken branch addresses.\n\n        :param program_counter: The address taken in by the branch\n        :type program_counter: :py:obj:`tritondse.types.Addr`\n        :param taken_addr: Target address of branch taken\n        :type taken_addr: Addr\n        :param not_taken_addr: Target address of branch **not** taken\n        :type not_taken_addr: Addr\n        \"\"\"\n\n        if self.strategy == CoverageStrategy.BLOCK:\n            self.covered_items[taken_addr] += 1\n            self.not_covered_items.discard(taken_addr)    # remove address from non-covered if inside\n            if not_taken_addr not in self.covered_items:  # Keep the address that has not been covered (and could have)\n                self.not_covered_items.add(not_taken_addr)\n\n        if self.strategy == CoverageStrategy.EDGE:\n            taken_tuple, not_taken_tuple = (program_counter, taken_addr), (program_counter, not_taken_addr)\n            self.covered_items[taken_tuple] += 1\n            self.not_covered_items.discard(taken_tuple)    # Remove it from non-taken if it was inside\n            if not_taken_tuple not in self.covered_items:  # Add the not taken tuple in non-covered\n                self.not_covered_items.add(not_taken_tuple)\n\n        if self.strategy == CoverageStrategy.PATH:\n            self._current_path.append(taken_addr)\n\n            # Compute the hash of the not taken path and add it to non-covered paths\n            not_taken_path_hash = self._current_path_hash.copy()\n            not_taken_path_hash.update(struct.pack('<Q', not_taken_addr))\n            self.not_covered_items.add(not_taken_path_hash.hexdigest())\n\n            # Update the current path hash and add it to hashes\n            self._current_path_hash.update(struct.pack(\"<Q\", taken_addr))\n            self.covered_items[self._current_path_hash.hexdigest()] += 1\n\n        if self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            taken_tuple, not_taken_tuple = (program_counter, taken_addr), (program_counter, not_taken_addr)\n            _, not_taken = (self._current_path_hash.hexdigest(), taken_tuple), (self._current_path_hash.hexdigest(), not_taken_tuple)\n            gtaken, gnot_taken = (\"\", taken_tuple), (\"\", not_taken_tuple)\n\n            # Add covered as covered\n            self.covered_items[gtaken] += 1\n\n            # Find all items in not_covered that have this edge\n            if taken_tuple in self._not_covered_items_mirror:               # if a not_covered match this edge\n                for prefix in self._not_covered_items_mirror[taken_tuple]:  # iterate all the prefixes\n                    self.not_covered_items.discard((prefix, taken_tuple))   # and discard them\n                self._not_covered_items_mirror.pop(taken_tuple)             # finally discard the entry\n\n            # look if not_taken edge not in covered\n            if gnot_taken not in self.covered_items:\n                self.not_covered_items.add(not_taken)\n                if not_taken[1] not in self._not_covered_items_mirror:\n                    self._not_covered_items_mirror[not_taken[1]] = [not_taken[0]]\n                else:\n                    self._not_covered_items_mirror[not_taken[1]].append(not_taken[0])\n\n            # update the current path hash etc\n            self._current_path.append(taken_addr)\n            self._current_path_hash.update(struct.pack(\"<Q\", taken_addr))\n\n    @property\n    def unique_instruction_covered(self) -> int:\n        \"\"\"\n        :return: The number of unique instructions covered\n        \"\"\"\n        return len(self.covered_instructions)\n\n    @property\n    def unique_covitem_covered(self) -> int:\n        \"\"\"\n        :return: The number of unique edges covered\n        \"\"\"\n        return len(self.covered_items)\n\n    @property\n    def total_instruction_executed(self) -> int:\n        \"\"\"\n        :return: The number of total instruction executed\n        \"\"\"\n        return sum(self.covered_instructions.values())\n\n    def post_execution(self) -> None:\n        \"\"\"\n        Function is called after each execution\n        for post processing or clean-up. *(Not\n        doing anythin at the moment)*\n        \"\"\"\n        pass\n\n    def is_covered(self, item: CovItem) -> bool:\n        \"\"\"\n        Return whether the item has been covered or not.\n        **The item should match the strategy**\n\n        :param item: An address, an edge or a path\n        :type item: CovItem\n        :return: bool\n        \"\"\"\n        if self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            try:\n                return ('', item[1]) in self.covered_items\n            except TypeError:  # in case of ellipsis\n                return False\n        else:\n            return item in self.covered_items\n\n    def pp_item(self, covitem: CovItem) -> str:\n        \"\"\"\n        Pretty print a CovItem according the coverage strategy\n\n        :param covitem: An address, an edge or a path\n        :return: str\n        \"\"\"\n        if self.strategy == CoverageStrategy.BLOCK:\n            return f\"0x{covitem:08x}\"\n        elif self.strategy == CoverageStrategy.EDGE:\n            return f\"(0x{covitem[0]:08x}-0x{covitem[1]:08x})\"\n        elif self.strategy == CoverageStrategy.PATH:\n            return covitem[:10]  # already a hash str\n        elif self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            return f\"({covitem[0][:6]}_0x{covitem[1][0]:08x}-0x{covitem[1][1]:08x})\"\n\n    def difference(self, other: CoverageSingleRun) -> Set[CovItem]:\n        if self.strategy == other.strategy:\n            return self.covered_items.keys() - other.covered_items.keys()\n        else:\n            logger.error(\"Trying to make difference of coverage with different strategies\")\n            return set()\n\n    def __sub__(self, other) -> Set[CovItem]:\n        return self.difference(other)", "\n\nclass GlobalCoverage(CoverageSingleRun):\n    \"\"\"\n    Global Coverage.\n    Represent the overall coverage of the exploration.\n    It is filled by iteratively call merge with the\n    :py:obj:`CoverageSingleRun` objects created during\n    exploration.\n    \"\"\"\n\n    COVERAGE_FILE = \"coverage.json\"\n\n    def __init__(self, strategy: CoverageStrategy, branch_strategy: BranchSolvingStrategy):\n        \"\"\"\n        :param strategy: Coverage strategy to use\n        :type strategy: CoverageStrategy\n        :param branch_strategy: Branch checking strategies\n        :type branch_strategy: BranchSolvingStrategy\n        \"\"\"\n        super().__init__(strategy)\n        self.branch_strategy = branch_strategy\n\n        # Keep pending items to be covered (code, edge, path)\n        self.pending_coverage: Set[CovItem] = set()\n        \"\"\" Set of pending coverage items. These are items for which a branch\n        as already been solved and \n        \"\"\"\n\n        self.uncoverable_items: Dict[CovItem, SolverStatus] = {}\n        \"\"\" CovItems that are determined not to be coverable. \"\"\"\n\n        self.covered_symbolic_pointers: Set[Addr] = set()\n        \"\"\" Set of addresses for which pointers have been enumerated \"\"\"\n\n    def iter_new_paths(self, path_constraints: List[PathConstraint]) -> Generator[Tuple[SymExType, List[PathConstraint], PathBranch, CovItem, int], Optional[SolverStatus], None]:\n        \"\"\"\n        The function iterate the given path predicate and yield PatchConstraint to\n        consider as-is and PathBranch representing the new branch to take. It acts\n        as a black-box so that the SeedManager does not have to know what strategy\n        is being used under the hood. From an implementation perspective the goal\n        of the function is to manipulate the path WITHOUT doing any SMT related things.\n\n        :param path_constraints: list of path constraint to iterate\n        :return: generator of path constraint and branches to solve. The first tuple\n                 item is a list of PathConstraint to add in the path predicate and the second\n                 is the branch to solve (but not to keep in path predicate)\n        \"\"\"\n        if BranchSolvingStrategy.MANUAL in self.branch_strategy:\n            logger.info(f'Branch solving strategy set to MANUAL.')\n            return\n\n        pending_csts = []\n        current_hash = hashlib.md5()  # Current path hash for PATH coverage\n\n        # NOTE: When we arrive here the CoverageSingleRun associated with the path_constraints\n        # has already been merge. Thus covered, pending etc do include ones of the CoverageSingleRuns\n\n        not_covered_items = self._get_items_trace(path_constraints)  # Map of CovItem -> [idx1, idx2, ..., .] (occurence in list)\n        # is_ok_with_branch_strategy = lambda covitem, idx: True if self.strategy == CoverageStrategy.PATH else (idx in occurence_map[covitem])\n\n        # Re-iterate through all path constraints to solve them concretely (with knowledge of what is beyond in the trace)\n        for i, pc in enumerate(path_constraints):\n            if pc.isMultipleBranches():     # If there is a condition\n                for branch in pc.getBranchConstraints():  # Get all branches\n                    # Get the constraint of the branch which has not been taken.\n                    if not branch['isTaken']:\n                        covitem = self._get_covitem(current_hash, branch)\n                        generic_covitem = ('', covitem[1]) if self.strategy == CoverageStrategy.PREFIXED_EDGE else covitem\n                        #print(f\"Covitem: {covitem}: {covitem not in self.covered_items} | {covitem not in self.pending_coverage} | {covitem not in self.uncoverable_items} | {i in not_covered_items.get(covitem, [])} | {i} | {not_covered_items.get(covitem)}\")\n\n                        # Not covered in: previous runs | yet to be covered by a seed already SAT | not uncoverable | parts of items to solve\n                        if generic_covitem not in self.covered_items and \\\n                           generic_covitem not in self.pending_coverage and \\\n                           covitem not in self.uncoverable_items and \\\n                           i in not_covered_items.get(covitem, []):\n\n                            # Send the branch to solve to the function iterating\n                            res = yield SymExType.CONDITIONAL_JMP, pending_csts, branch, covitem, i\n\n                            # If path SAT add it to pending coverage\n                            if res == SolverStatus.SAT:\n                                self.pending_coverage.add(generic_covitem)\n\n                            elif res == SolverStatus.UNSAT:\n                                if BranchSolvingStrategy.UNSAT_ONCE in self.branch_strategy:\n                                    self.uncoverable_items[covitem] = res\n                                elif self.strategy in [CoverageStrategy.PATH, CoverageStrategy.PREFIXED_EDGE]:\n                                    self.uncoverable_items[covitem] = res  # paths, and prefixed-edge ensure to be unique thus drop if unsat\n\n                            elif res == SolverStatus.TIMEOUT:\n                                if BranchSolvingStrategy.TIMEOUT_ONCE in self.branch_strategy:\n                                    self.uncoverable_items[covitem] = res\n\n                            elif res == SolverStatus.UNKNOWN:\n                                pass\n\n                            else: # status == None\n                                logger.debug(f'Branch skipped!')\n\n                            pending_csts = []  # reset pending constraint added\n\n                    else:\n                        pass  # Branch was taken do nothing\n\n                # Add it the path predicate constraints and update current path hash\n                pending_csts.append(pc)\n                current_hash.update(struct.pack(\"<Q\", pc.getTakenAddress()))\n\n            else:\n                cmt = pc.getComment()\n\n                if (cmt.startswith(\"dyn-jmp\") and BranchSolvingStrategy.COVER_SYM_DYNJUMP in self.branch_strategy) or \\\n                   (cmt.startswith(\"sym-read\") and BranchSolvingStrategy.COVER_SYM_READ in self.branch_strategy) or \\\n                   (cmt.startswith(\"sym-write\") and BranchSolvingStrategy.COVER_SYM_WRITE in self.branch_strategy):\n                    typ, offset, addr = cmt.split(\":\")\n                    typ = SymExType(typ)\n                    offset, addr = int(offset), int(addr)\n                    if addr not in self.covered_symbolic_pointers:  # if the address pointer has never been covered\n                        pred = pc.getTakenPredicate()\n                        if pred.getType() == AST_NODE.EQUAL:\n                            p1, p2 = pred.getChildren()\n                            if p2.getType() == AST_NODE.BV:\n                                logger.info(f\"Try to enumerate value {offset}:0x{addr:02x}: {p1}\")\n                                res = yield typ, pending_csts, p1, (addr, p2.evaluate()), i\n                                self.covered_symbolic_pointers.add(addr)  # add the pointer in covered regardless of result\n                            else:\n                                logger.warning(f\"memory constraint unexpected pattern: {pred}\")\n                        else:\n                            logger.warning(f\"memory constraint unexpected pattern: {pred}\")\n\n                    if BranchSolvingStrategy.SOUND_MEM_ACCESS in self.branch_strategy:\n                        pending_csts.append(pc)  # if sound add the mem dereference as a constraint in path predicate\n                        # NOTE: in both case the branch is not taken in account in the current_path_hash\n\n                else:  # Routines, or user-defined constraints thus add it all the time.\n                    pending_csts.append(pc)\n\n    def _get_covitem(self, path_hash, branch: PathBranch) -> CovItem:\n        src, dst = branch['srcAddr'], branch['dstAddr']\n\n        # Check if the target is new with regards to the strategy\n        if self.strategy == CoverageStrategy.BLOCK:\n            return dst\n        elif self.strategy == CoverageStrategy.EDGE:\n            return src, dst\n        elif self.strategy == CoverageStrategy.PATH:\n            # Have to fork the hash of the current pc for each branch we want to revert\n            forked_hash = path_hash.copy()\n            forked_hash.update(struct.pack(\"<Q\", dst))\n            return forked_hash.hexdigest()\n        elif self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            return path_hash.hexdigest(), (src, dst)\n        else:\n            assert False\n\n    def _get_items_trace(self, path_constraints: List[PathConstraint]) -> Dict[CovItem, List[int]]:\n        \"\"\"\n        Iterate the all trace and retrieve all covered and not covered CovItem. For non covered one\n        it filter instances to check.\n        \"\"\"\n        not_covered = {}\n        current_hash = hashlib.md5()  # Current path hash for PATH coverage\n        for i, pc in enumerate(path_constraints):\n            if pc.isMultipleBranches():     # If there is a condition\n                for branch in pc.getBranchConstraints():  # Get all branches\n                    if not branch['isTaken']:\n                        covitem = self._get_covitem(current_hash, branch)\n                        if covitem in not_covered:\n                            not_covered[covitem].append(i)\n                        else:\n                            not_covered[covitem] = [i]\n                current_hash.update(struct.pack(\"<Q\", pc.getTakenAddress()))  # compute current path hash along the way\n            else:\n                pass  # Ignore all other dynamic constraints in path computation\n\n        # Now filter the map according to the branch solving strategy\n        if BranchSolvingStrategy.FIRST_LAST_NOT_COVERED in self.branch_strategy:\n            if self.strategy == CoverageStrategy.PREFIXED_EDGE:\n                # Black magic\n                m = {(\"\", e): [] for h, e in not_covered.keys()}  # Map: (\"\", edge) -> []\n                for (h, e), v in not_covered.items():          # fill map with all occurences edges regardless of path\n                    m[(\"\", e)].extend(v)\n                for k in m.keys():                             # iterate the result and only keep min and max occurence\n                    idxs = m[k]\n                    if len(idxs) > 2:\n                        m[k] = [min(idxs), max(idxs)]\n                for k in not_covered.keys():                   # Push back resulting list in not_covered items\n                    not_covered[k] = m[('', k[1])]\n\n            else:  # Straightforward\n                for k in not_covered.keys():\n                    l = not_covered[k]\n                    if len(l) > 2:\n                        not_covered[k] = [l[0], l[-1]]  # Only keep first and last iteration\n        else: # ALL_NOT_COVERED\n            pass  # Keep all occurences\n        return not_covered\n\n\n    def merge(self, other: CoverageSingleRun) -> None:\n        \"\"\"\n        Merge a CoverageSingeRun instance into this instance\n\n        :param other: The CoverageSingleRun to merge into self\n        :type other: CoverageSingleRun\n        \"\"\"\n        assert self.strategy == other.strategy\n\n        # Update instruction coverage for code coverage (in all cases keep code coverage)\n        self.covered_instructions.update(other.covered_instructions)\n\n        # Remove covered items from pending ones\n        self.pending_coverage.difference_update(other.covered_items)\n\n        # Update covered items\n        self.covered_items.update(other.covered_items)\n\n        # Update non-covered ones\n        if self.strategy == CoverageStrategy.PREFIXED_EDGE:\n            # More complex as not_covered as covitem: (hash, edge) while covered has covitems: (\"\", edge)\n\n            # remove self not covered that are now covered\n            for _, edge in other.covered_items.keys():\n                if edge in self._not_covered_items_mirror:\n                    for prefix in self._not_covered_items_mirror[edge]:  # iterate all the prefixes\n                        self.not_covered_items.discard((prefix, edge))  # and discard them\n                    self._not_covered_items_mirror.pop(edge)  # finally discard the entry\n\n            # Only add other not covered if still not covered\n            for prefix, edge in other.not_covered_items:\n                if (\"\", edge) not in self.covered_items:\n                    self.not_covered_items.add((prefix, edge))\n                    if edge not in self._not_covered_items_mirror:\n                        self._not_covered_items_mirror[edge] = [prefix]\n                    else:\n                        self._not_covered_items_mirror[edge].append(prefix)\n\n        else: # Straightfoward set difference\n            self.not_covered_items.update(other.not_covered_items - self.covered_items.keys())\n\n\n    def can_improve_coverage(self, other: CoverageSingleRun) -> bool:\n        \"\"\"\n        Check if some of the non-covered are not already in the global coverage\n        Used to know if an input is relevant to keep or not\n\n        :param other: The CoverageSingleRun to check against our global coverage state\n        :return: bool\n        \"\"\"\n        return bool(self.new_items_to_cover(other))\n\n\n    def can_cover_symbolic_pointers(self, execution: 'SymbolicExecutor') -> bool:\n        \"\"\"\n        Determines if this execution has symbolic memory accesses to enumerate. If so we may want\n        to enumerate them even though \n        \"\"\"\n        path_constraints = execution.pstate.get_path_constraints()\n\n        for pc in path_constraints:\n            if not pc.isMultipleBranches():     # If there isn't a condition i.e it's a sym ptr access\n                cmt = pc.getComment()\n\n                if (cmt.startswith(\"dyn-jmp\") and BranchSolvingStrategy.COVER_SYM_DYNJUMP in self.branch_strategy) or \\\n                   (cmt.startswith(\"sym-read\") and BranchSolvingStrategy.COVER_SYM_READ in self.branch_strategy) or \\\n                   (cmt.startswith(\"sym-write\") and BranchSolvingStrategy.COVER_SYM_WRITE in self.branch_strategy):\n                    typ, offset, addr = cmt.split(\":\")\n                    typ = SymExType(typ)\n                    offset, addr = int(offset), int(addr)\n                    if addr not in self.covered_symbolic_pointers:  # if the address pointer has never been covered\n                        return True\n        return False\n\n\n    def new_items_to_cover(self, other: CoverageSingleRun) -> Set[CovItem]:\n        \"\"\"\n        Return all coverage items (addreses, edges, paths) that the given CoverageSingleRun\n        can cover if it is possible to negate their branches\n\n        :param other: The CoverageSingleRun to check with our global coverage state\n        :return: A set of CovItem\n        \"\"\"\n        assert self.strategy == other.strategy\n        # Take not covered_items (potential candidates) substract already covered items, uncoverable and pending ones.\n        # Resulting covitem are really new ones that the trace brings\n        return other.not_covered_items - self.covered_items.keys() - self.uncoverable_items.keys() - self.pending_coverage\n\n    def improve_coverage(self, other: CoverageSingleRun) -> bool:\n        \"\"\"\n        Checks if the given object do cover new covitem than the current\n        coverage. More concretely it performs the difference between the\n        two covered dicts. If ``other`` contains new items return True.\n\n        :param other: coverage on which to check coverage\n        :return: Whether the coverage covers new items\n        \"\"\"\n        return bool(other.covered_items.keys() - self.covered_items.keys())\n\n    @staticmethod\n    def from_file(file: Union[str, Path]) -> 'GlobalCoverage':\n        with open(file, \"rb\") as f:\n            obj = pickle.load(f)\n        return obj\n\n    def to_file(self, file: Union[str, Path]) -> None:\n        copy = self._current_path_hash\n        self._current_path_hash = None\n        with open(file, \"wb\") as f:\n            pickle.dump(self, f)\n        self._current_path_hash = copy\n\n\n    def post_exploration(self, workspace: 'Workspace') -> None:\n        \"\"\" Function called at the very end of the exploration.\n        It saves the coverage in the workspace.\n\n        :param workspace: Workspace in which to save coverage\n        :type workspace: Workspace\n        \"\"\"\n        # Save the coverage\n        self.to_file(workspace.get_metadata_file_path(self.COVERAGE_FILE))\n\n    def clone(self) -> 'GlobalCoverage':\n        cov2 = GlobalCoverage(self.strategy, self.branch_strategy)\n\n        # Copy items from the CoverageSingleRun\n        cov2.covered_instructions = Counter({k: v for k, v in self.covered_instructions.items()})\n        cov2.covered_items = Counter({k: v for k, v in self.covered_items.items()})\n        cov2.not_covered_items = {x for x in self.not_covered_items}\n        cov2._not_covered_items_mirror = {k: v for k, v in self._not_covered_items_mirror.items()}\n        cov2._current_path = self._current_path[:]\n        self._current_path: List[Addr] = []\n        self._current_path_hash = self._current_path_hash.copy()\n\n        # Copy items from the global coverage\n        cov2.pending_coverage = {x for x in self.pending_coverage}\n        cov2.uncoverable_items = {k: v for k, v in self.uncoverable_items.items()}\n        cov2.covered_symbolic_pointers = {x for x in self.covered_symbolic_pointers}\n        return cov2", ""]}
{"filename": "tritondse/memory.py", "chunked_list": ["from triton import TritonContext\nimport bisect\nfrom typing import Optional, Union, Generator, List\nfrom collections import namedtuple\nimport struct\nfrom contextlib import contextmanager\n\nfrom tritondse.types import Perm, Addr, ByteSize, Endian\n\nMemMap = namedtuple('Map', \"start size perm name\")", "\nMemMap = namedtuple('Map', \"start size perm name\")\n\n\nclass MapOverlapException(Exception):\n    \"\"\"\n    Exception raised when trying to map a memory area where some of\n    the addresses overlap with an already mapped area.\n    \"\"\"\n    pass", "\n\nclass MemoryAccessViolation(Exception):\n    \"\"\"\n    Exception triggered when accessing memory with\n    the wrong permissions.\n    \"\"\"\n    def __init__(self, addr: Addr, access: Perm, map_perm: Perm = None, memory_not_mapped: bool = False, perm_error: bool = False):\n        \"\"\"\n        :param addr: address where the violation occured\n        :param access: type of access performed\n        :param map_perm: permission of the memory page of `address`\n        :param memory_not_mapped: whether the address was mapped or not\n        :param perm_error: whether it is a permission error\n        \"\"\"\n        super(MemoryAccessViolation, self).__init__()\n        self.address: Addr = addr\n        \"\"\" address where the violation occurred\"\"\"\n        self._is_mem_unmapped = memory_not_mapped\n        self._is_perm_error = perm_error\n        self.access: Perm = access\n        \"\"\"Access type that was performed\"\"\"\n        self.map_perm: Optional[Perm] = map_perm\n        \"\"\"Permissions of the memory map associated to the address\"\"\"\n\n    def is_permission_error(self) -> bool:\n        \"\"\"True if the exception was caused by a permission issue\"\"\"\n        return self._is_perm_error\n\n    def is_memory_unmapped_error(self) -> bool:\n        \"\"\"\n        Return true if the exception was raised due to an access\n        to an area not mapped\n        \"\"\"\n        return self._is_mem_unmapped\n\n    def __str__(self) -> str:\n        if self.is_permission_error():\n            return f\"(addr:{self.address:#08x}, access:{str(self.access)} on map:{str(self.map_perm)})\"\n        else:\n            return f\"({str(self.access)}: {self.address:#08x} unmapped)\"\n\n    def __repr__(self):\n        return str(self)", "\n\nSTRUCT_MAP = {\n    (True, 1): 'B',\n    (False, 1): 'b',\n    (True, 2): 'H',\n    (False, 2): 'h',\n    (True, 4): 'I',\n    (False, 4): 'i',\n    (True, 8): 'Q',", "    (False, 4): 'i',\n    (True, 8): 'Q',\n    (False, 8): 'q'\n}\n\nENDIAN_MAP = {\n    Endian.LITTLE: \"<\",\n    Endian.BIG: \">\"\n}\n", "}\n\n\nclass Memory(object):\n    \"\"\"\n    Memory representation of the current :py:class:`ProcessState` object.\n    It wraps all interaction with Triton's memory context to provide high-level\n    function. It adds a segmentation and memory permission model at the top\n    of Triton. It also overrides __getitem__ and the slice mechanism to be able\n    read and write concrete memory values in a Pythonic manner.\n    \"\"\"\n\n    def __init__(self, ctx: TritonContext, endianness: Endian = Endian.LITTLE):\n        \"\"\"\n        :param ctx: TritonContext to interface with\n        \"\"\"\n        self.ctx: TritonContext = ctx\n        \"\"\"Underlying Triton context\"\"\"\n        self._linear_map_addr = []  # List of [map_start, map_end, map_start, map_end ...]\n        self._linear_map_map = []   # List of [MemMap,    None,    MemMap,    None    ...]\n        self._segment_enabled = True\n        self._endian = endianness\n        self._endian_key = ENDIAN_MAP[self._endian]\n        self._mem_cbs_enabled = True\n        # self._maps = {}  # Addr: -> Map\n\n    def set_endianess(self, en: Endian) -> None:\n        \"\"\"\n        Set the endianness of memory accesses. By default,\n        endianess is little.\n\n        :param en: Endian: Endianess to use.\n        :return: None\n        \"\"\"\n        self._endian = en\n        self._endian_key = ENDIAN_MAP[self._endian]\n\n    @property\n    def _ptr_size(self) -> int:\n        return self.ctx.getGprSize()\n\n    @property\n    def segmentation_enabled(self) -> bool:\n        \"\"\"\n        returns whether segmentation enforcing is enabled\n\n        :return: True if segmentation is enabled\n        \"\"\"\n        return self._segment_enabled\n\n    def disable_segmentation(self) -> None:\n        \"\"\"\n        Turn-off segmentation enforcing.\n        \"\"\"\n        self._segment_enabled = False\n\n    def enable_segmentation(self) -> None:\n        \"\"\"\n        Turn-off segmentation enforcing.\n        \"\"\"\n        self._segment_enabled = True\n\n    def set_segmentation(self, enabled: bool) -> None:\n        \"\"\"\n        Set the segmentation enforcing with the given boolean.\n        \"\"\"\n        self._segment_enabled = enabled\n\n    @contextmanager\n    def without_segmentation(self, disable_callbacks=False) -> Generator['Memory', None, None]:\n        \"\"\"\n        Context manager enabling manipulating temporarily the memory\n        without considering the memory permissions.\n        E.g: It enables writing data in a memory mapped in RX\n        :param disable_callbacks: Whether to disable memory callbacks that could have been set\n        :return:\n        \"\"\"\n        previous = self._segment_enabled\n        self.disable_segmentation()\n        cbs = self._mem_cbs_enabled\n        self._mem_cbs_enabled = not disable_callbacks\n        yield self\n        self._mem_cbs_enabled = cbs\n        self.set_segmentation(previous)\n\n    def callbacks_enabled(self) -> bool:\n        \"\"\"\n        Return whether memory callbacks are enabled.\n\n        :return: True if callbacks are enabled\n        \"\"\"\n        return self._mem_cbs_enabled\n\n    def get_maps(self) -> Generator[MemMap, None, None]:\n        \"\"\"\n        Iterate all the memory maps defined, including all memory\n        areas allocated on the heap.\n\n        :return: generator of all :py:class:`MemMap` objects\n        \"\"\"\n        yield from (x for x in self._linear_map_map if x)\n\n    def map(self, start, size, perm: Perm = Perm.R | Perm.W | Perm.X, name=\"\") -> MemMap:\n        \"\"\"\n        Map the given address and size in memory with the given permission.\n\n        :raise MapOverlapException: In the case the map overlap an existing mapping\n        :param start: address to map\n        :param size: size to map\n        :param perm: permission\n        :param name: name to given to the memory region\n        :return: MemMap freshly mapped\n        \"\"\"\n        def _map_idx(idx):\n            self._linear_map_addr.insert(idx, start + size - 1)  # end address is included\n            self._linear_map_addr.insert(idx, start)\n            self._linear_map_map.insert(idx, None)\n            memmap = MemMap(start, size, perm, name)\n            self._linear_map_map.insert(idx, memmap)\n            return memmap\n\n        if not self._linear_map_addr:  # Nothing mapped yet\n            return _map_idx(0)\n\n        idx = bisect.bisect_left(self._linear_map_addr, start)\n\n        if idx == len(self._linear_map_addr):  # It should be mapped at the end\n            return _map_idx(idx)\n\n        addr = self._linear_map_addr[idx]\n        if (idx % 2) == 0:  # We are on a start address\n            if start < addr and start+size <= addr:  # Can fit before\n                return _map_idx(idx)\n            else:  # there is an overlap\n                raise MapOverlapException(f\"0x{start:08x}:{size} overlap with map: 0x{addr:08x} (even)\")\n        else:  # We are on an end address\n            prev = self._linear_map_addr[idx-1]\n            raise MapOverlapException(f\"0x{start:08x}:{size} overlap with map: 0x{prev:08x} (odd)\")\n\n    def unmap(self, addr: Addr) -> None:\n        \"\"\"\n        Unmap the :py:class:`MemMap` object mapped at the address.\n        The address can be within the map and not requires pointing\n        at the head.\n\n        :param addr: address to unmap\n        :return: None\n        \"\"\"\n        def _unmap_idx(idx):\n            self._linear_map_addr.pop(idx) # Pop the start\n            self._linear_map_addr.pop(idx) # Pop the end\n            self._linear_map_map.pop(idx)  # Pop the object\n            self._linear_map_map.pop(idx)  # Pop the None padding\n\n        idx = bisect.bisect_left(self._linear_map_addr, addr)\n        try:\n            mapaddr = self._linear_map_addr[idx]\n            if (idx % 2) == 0:  # We are on a start address (meaning we should be exactly on map start other unmapped)\n                if addr == mapaddr:  # We are exactly on the map address\n                    _unmap_idx(idx)\n                else:\n                    raise MemoryAccessViolation(addr, Perm(0), memory_not_mapped=True)\n            else:  # We are on an end address\n                _unmap_idx(idx-1)\n        except IndexError:\n            raise MemoryAccessViolation(addr, Perm(0), memory_not_mapped=True)\n\n    def mprotect(self, addr: Addr, perm: Perm) -> None:\n        \"\"\"\n        Update the map at the given address with permissions provided in argument.\n\n        :param addr: address of the map of which to change permission\n        :param perm: permission to assign\n        :return: None\n        \"\"\"\n        idx = bisect.bisect_left(self._linear_map_addr, addr)\n        try:\n            if (idx % 2) == 0:  # We are on a start address (meaning we should be exactly on map start other unmapped)\n                map = self._linear_map_map[idx]\n                self._linear_map_map[idx] = MemMap(map.start, map.size, perm, map.name)  # replace map with new perms\n            else:  # We are on an end address\n                map = self._linear_map_map[idx-1]\n                self._linear_map_map[idx-1] = MemMap(map.start, map.size, perm, map.name)  # replace map with new perms\n        except IndexError:\n            raise MemoryAccessViolation(addr, Perm(0), memory_not_mapped=True)\n\n    def __setitem__(self, key: Addr, value: bytes) -> None:\n        \"\"\"\n        Assign the given value at the address given by the key.\n        The value must be bytes but can be multiple bytes.\n        Warning: You cannot use the slice API on this function.\n\n        :param key: address to write to\n        :param value: content to write\n        :raise MemoryAccessViolation: in case of invalid access\n        \"\"\"\n        if isinstance(key, slice):\n            raise TypeError(\"slice unsupported for __setitem__\")\n        else:\n            self.write(key, value)\n\n    def __getitem__(self, item: Union[Addr, slice]) -> bytes:\n        \"\"\"\n        Read the memory at the given address. If the key\n        is an integer reads a single byte. If the key is\n        a slice: read addr+size bytes in memory.\n\n        :param item: address, or address:size to read\n        :return: memory content\n        :raise MemoryAccessViolation: if the access is invalid\n        \"\"\"\n        if isinstance(item, slice):\n            return self.read(item.start, item.stop)\n        elif isinstance(item, int):\n            return self.read(item, 1)\n\n    def write(self, addr: Addr, data: bytes) -> None:\n        \"\"\"\n        Write the given `data` bytes at `addr` address.\n\n        :param addr: address where to write\n        :param data: data to write\n        :return: None\n        \"\"\"\n        if self._segment_enabled:\n            map = self._get_map(addr, len(data))\n            if map is None:\n                raise MemoryAccessViolation(addr, Perm.W, memory_not_mapped=True)\n            if Perm.W not in map.perm:\n                raise MemoryAccessViolation(addr, Perm.W, map_perm=map.perm, perm_error=True)\n        return self.ctx.setConcreteMemoryAreaValue(addr, data)\n\n    def read(self, addr: Addr, size: ByteSize) -> bytes:\n        \"\"\"\n        Read `size` bytes at `addr` address.\n\n        :param addr: address to read\n        :param size: size of content to read\n        :return: bytes read\n        \"\"\"\n        if self._segment_enabled:\n            map = self._get_map(addr, size)\n            if map is None:\n                raise MemoryAccessViolation(addr, Perm.R, memory_not_mapped=True)\n            if Perm.R not in map.perm:\n                raise MemoryAccessViolation(addr, Perm.R, map_perm=map.perm, perm_error=True)\n        return self.ctx.getConcreteMemoryAreaValue(addr, size)\n\n    def _get_map(self, ptr: Addr, size: ByteSize) -> Optional[MemMap]:\n        \"\"\"\n        Internal function returning the MemMap object associated\n        with any address. It returns None if part of the memory\n        range falls out of a memory mapping.\n        Complexity is O(log(n))\n\n        :param ptr: address in memory\n        :param size: size of the memory\n        :return: True if mapped\n        \"\"\"\n        idx = bisect.bisect_left(self._linear_map_addr, ptr)\n        try:\n            addr = self._linear_map_addr[idx]\n            if (idx % 2) == 0:  # We are on a start address (meaning we should be exactly on map start other unmapped)\n                end = self._linear_map_addr[idx+1]\n                return self._linear_map_map[idx] if (ptr == addr and ptr+size <= end+1) else None\n            else:  # We are on an end address\n                start = self._linear_map_addr[idx-1]\n                return self._linear_map_map[idx-1] if (start <= addr and ptr+size <= addr+1) else None  # fit into the map\n        except IndexError:\n            return None  # Either raised when linear_map is empty or the address is beyond everything that is mapped\n\n    def get_map(self, addr: Addr, size: ByteSize = 1) -> Optional[MemMap]:\n        \"\"\"\n        Find the MemMap associated with the given address and returns\n        it if any.\n\n        :param addr: Address of the map (or any map inside)\n        :param size: size of bytes for which we want the map\n        :return: MemMap if found\n        \"\"\"\n        return self._get_map(addr, size)\n\n    def find_map(self, name: str) -> Optional[List[MemMap]]:\n        \"\"\"\n        Find a map given its name.\n\n        :param name: Map name\n        :return: MemMap if found\n        \"\"\"\n        l = []\n        for map in (x for x in self._linear_map_map if x):\n            if map.name == name:\n                l.append(map)\n        return l\n\n    def map_from_name(self, name: str) -> MemMap:\n        \"\"\"\n        Return a map from its name. This function assumes\n        the map is present.\n\n        :raise AssertionError: If the map is not found\n        :param name: Map name\n        :return: MemMap\n        \"\"\"\n        for map in (x for x in self._linear_map_map if x):\n            if map.name == name:\n                return map\n        assert False\n\n    def is_mapped(self, ptr: Addr, size: ByteSize = 1) -> bool:\n        \"\"\"\n        The function checks whether the memory is mapped or not.\n        The implementation return False if the memory chunk overlap\n        on two memory regions.\n        Complexity is O(log(n))\n\n        :param ptr: address in memory\n        :param size: size of the memory\n        :return: True if mapped\n        \"\"\"\n        return self._get_map(ptr, size) is not None\n\n    def has_ever_been_written(self, ptr: Addr, size: ByteSize) -> bool:\n        \"\"\"\n        Returns whether the given range of addresses has previously\n        been written or not. (Do not take in account the memory mapping).\n\n        :param ptr: The pointer to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :param size: Size of the memory range to check\n        :return: True if all addresses have been defined\n        \"\"\"\n        return self.ctx.isConcreteMemoryValueDefined(ptr, size)\n\n    def read_uint(self, addr: Addr, size: ByteSize = 4):\n        \"\"\"\n        Read in the process memory a **little-endian** integer of the ``size`` at ``addr``.\n\n        :param addr: Address at which to read data\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: Number of bytes to read\n        :type size: Union[str, :py:obj:`tritondse.types.ByteSize`]\n        :return: Integer value read\n        :raise struct.error: If value can't fit in `size`\n        \"\"\"\n        data = self.read(addr, size)\n        return struct.unpack(self._endian_key+STRUCT_MAP[(True, size)], data)[0]\n\n    def read_sint(self, addr: Addr, size: ByteSize = 4):\n        \"\"\"\n        Read in the process memory a **little-endian** integer of the ``size`` at ``addr``.\n\n        :param addr: Address at which to read data\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param size: Number of bytes to read\n        :type size: Union[str, :py:obj:`tritondse.types.ByteSize`]\n        :return: Integer value read\n        :raise struct.error: If value can't fit in `size`\n        \"\"\"\n        data = self.read(addr, size)\n        return struct.unpack(self._endian_key+STRUCT_MAP[(False, size)], data)[0]\n\n    def read_ptr(self, addr: Addr) -> int:\n        \"\"\"\n        Read in the process memory a little-endian integer of size :py:attr:`tritondse.ProcessState.ptr_size`\n\n        :param addr: Address at which to read data\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :return: Integer value read\n        \"\"\"\n        return self.read_uint(addr, self._ptr_size)\n\n    def read_char(self, addr: Addr) -> int:\n        \"\"\"\n        Read a char in memory (1-byte) following endianess.\n\n        :param addr: address to read\n        :return: char value as int\n        \"\"\"\n        return self.read_sint(addr, 1)\n\n    def read_uchar(self, addr: Addr) -> int:\n        \"\"\"\n        Read an unsigned char in memory (1-byte) following endianness.\n\n        :param addr: address to read\n        :return: unsigned char value as int\n        \"\"\"\n        return self.read_uint(addr, 1)\n\n    def read_int(self, addr: Addr) -> int:\n        \"\"\"\n        Read a signed integer in memory (4-byte) following endianness.\n\n        :param addr: address to read\n        :return: signed integer value as int\n        \"\"\"\n        return self.read_sint(addr, 4)\n\n    def read_word(self, addr: Addr) -> int:\n        \"\"\"\n        Read signed word in memory (2-byte) following endianness.\n\n        :param addr: address to read\n        :return: signed word value as int\n        \"\"\"\n        return self.read_uint(addr, 2)\n\n    def read_dword(self, addr: Addr) -> int:\n        \"\"\"\n        Read signed double word in memory (4-byte) following endianness.\n\n        :param addr: address to read\n        :return: dword value as int\n        \"\"\"\n        return self.read_uint(addr, 4)\n\n    def read_qword(self, addr: Addr) -> int:\n        \"\"\"\n        Read signed qword in memory (8-byte) following endianness.\n\n        :param addr: address to read\n        :return: qword value as int\n        \"\"\"\n        return self.read_uint(addr, 8)\n\n    def read_long(self, addr: Addr) -> int:\n        \"\"\"\n        Read 'C style' long in memory (4-byte) following endianness.\n\n        :param addr: address to read\n        :return: value as int\n        \"\"\"\n        return self.read_sint(addr, 4)\n\n    def read_ulong(self, addr: Addr) -> int:\n        \"\"\"\n        Read unsigned long in memory (4-byte) following endianness.\n\n        :param addr: address to read\n        :return: unsigned long value as int\n        \"\"\"\n        return self.read_uint(addr, 4)\n\n    def read_long_long(self, addr: Addr) -> int:\n        \"\"\"\n        Read long long in memory (8-byte) following endianness.\n\n        :param addr: address to read\n        :return: long long value as int\n        \"\"\"\n        return self.read_sint(addr, 8)\n\n    def read_ulong_long(self, addr: Addr) -> int:\n        \"\"\"\n        Read unsigned long long in memory (8-byte) following endianness.\n\n        :param addr: address to read\n        :return: unsigned long long value as int\n        \"\"\"\n        return self.read_uint(addr, 8)\n\n    def read_string(self, addr: Addr) -> str:\n        \"\"\" Read a string in process memory at the given address\n\n        .. warning:: The memory read is unbounded. Thus the memory is iterated up until\n                     finding a 0x0.\n\n        :returns: the string read in memory\n        :rtype: str\n        \"\"\"\n        s = \"\"\n        index = 0\n        while True:\n            val = self.read_uint(addr+index, 1)\n            if not val:\n                return s\n            s += chr(val)\n            index += 1\n\n    def write_int(self, addr: Addr, value: int, size: ByteSize = 4):\n        \"\"\"\n        Write in the process memory the given integer value of the given size at\n        a specific address.\n\n        :param addr: Address at which to read data\n        :param value: data to write represented as an integer\n        :param size: Number of bytes to read\n        :raise struct.error: If integer value cannot fit in `size`\n        \"\"\"\n        self.write(addr, struct.pack(self._endian_key+STRUCT_MAP[(value >= 0, size)], value))\n\n    def write_ptr(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Similar to :py:meth:`write_int` but the size is automatically adjusted\n        to be ``ptr_size``.\n\n        :param addr: address where to write data\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param value: pointer value to write\n        :type value: int\n        :raise struct.error: If integer value cannot fit in a pointer size\n        \"\"\"\n        self.write_int(addr, value, self._ptr_size)\n\n    def write_char(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write the integer value as a single byte in memory.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a byte (>255)\n        \"\"\"\n        self.write_int(addr, value, 1)\n\n    def write_word(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write the word (2-byte) in memory following endianess.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a word\n        \"\"\"\n        self.write_int(addr, value, 2)\n\n    def write_dword(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write the word (4-byte) in memory following endianess.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a dword\n        \"\"\"\n        self.write_int(addr, value, 4)\n\n    def write_qword(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write the qword (8-byte) in memory following endianess.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a qword\n        \"\"\"\n        self.write_int(addr, value, 8)\n\n    def write_long(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write a \"C style\" long (4-byte) in memory following endianess.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a long\n        \"\"\"\n        return self.write_int(addr, value, 4)\n\n    def write_long_long(self, addr: Addr, value: int) -> None:\n        \"\"\"\n        Write the \"C style\" long long (8-byte) in memory following endianess.\n\n        :param addr: address to write\n        :param value: integer value\n        :raise struct.error: If integer value do not fit in a long long\n        \"\"\"\n        return self.write_int(addr, value, 8)", ""]}
{"filename": "tritondse/trace.py", "chunked_list": ["# built-in imports\nimport json\nimport os\nimport subprocess\nfrom pathlib import Path\nimport time\nfrom typing import List, Optional, Union\nfrom collections import Counter\n\n", "\n\n# local imports\nimport tritondse # NOTE We need this import so we can use it to determine the path of this file.\nfrom tritondse import Config, Program, SymbolicExecutor, CoverageStrategy, CoverageSingleRun\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"tracer\")\n\n\nclass TraceException(Exception):\n    pass", "\n\nclass TraceException(Exception):\n    pass\n\n\nclass Trace:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def run(strategy: CoverageStrategy, binary_path: str, args: List[str], output_path: str, dump_trace: bool = False, stdin_file=None) -> bool:\n        \"\"\"Run the binary passed as argument and return the coverage.\n\n        :param strategy: Coverage strategy.\n        :type strategy: :py:obj:`CoverageStrategy`.\n        :param binary_path: Path to the binary.\n        :type binary_path: :py:obj:`str`.\n        :param args: List of arguments to pass to the binary.\n        :type args: :py:obj:`List[str]`.\n        :type output_path: File where to store trace\n        :param dump_trace: Enable gather the trace\n        :param stdin_file: Path to the file that will act as stdin.\n        :type args: :py:obj:`str`.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def trace(self) -> List[int]:\n        raise NotImplementedError()\n\n    @staticmethod\n    def from_file(file: Union[str, Path]) -> 'QBDITrace':\n        raise NotImplementedError()\n\n    @property\n    def coverage(self) -> CoverageSingleRun:\n        \"\"\"\n        Coverage generated by the trace\n\n        :return: CoverageSingleRun object\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_coverage(self) -> CoverageSingleRun:\n        \"\"\"Return the execution coverage.\n\n        :return: :py:obj:`CoverageSingleRun`.\n        \"\"\"\n        return self.coverage\n\n    @property\n    def strategy(self) -> CoverageStrategy:\n        \"\"\"\n        Return the coverage strategy with which this trace\n        was generated with.\n\n        :return: :py:obj:`CoverageStrategy`\n        \"\"\"\n        return self.coverage.strategy", "\n\nclass TritonTrace(Trace):\n\n    def __init__(self):\n        super().__init__()\n\n        self._coverage = None\n\n    @staticmethod\n    def run(strategy: CoverageStrategy, binary_path: str, args: List[str], output_path: str, dump_trace: bool = False, stdin_file=None) -> bool:\n        # Override stdin with the input file.\n        if stdin_file:\n            os.dup2(os.open(stdin_file, os.O_RDONLY), 0)\n\n        config = Config(coverage_strategy=strategy)\n\n        se = SymbolicExecutor(config)\n\n        se.load(Program(binary_path))\n\n        se.run()\n\n        trace = TritonTrace()\n        trace._coverage = se.coverage\n        # FIXME: Writing the coverage to a file\n\n    @staticmethod\n    def from_file(file: Union[str, Path]) -> 'QBDITrace':\n        # FIXME: Reading coverage file from a file\n        return trace\n\n    @property\n    def coverage(self) -> CoverageSingleRun:\n        return self._coverage", "\n\nclass QBDITrace(Trace):\n\n    QBDI_SCRIPT_FILEPATH = Path(tritondse.__file__).parent / 'qbdi_trace.py'\n\n    def __init__(self):\n        super().__init__()\n        self._coverage = None\n        self._trace = None\n        self.modules = {}\n\n    @staticmethod\n    def run(strategy: CoverageStrategy, binary_path: str, args: List[str], output_path: str, dump_trace: bool = False, stdin_file=None, timeout=None, cwd=None) -> bool:\n        if not Path(binary_path).exists():\n            raise FileNotFoundError()\n\n        if stdin_file and not Path(stdin_file).exists():\n            raise FileNotFoundError()\n\n        args = [] if not args else args\n\n        cmdlne = f'timeout {timeout} python -m pyqbdipreload {QBDITrace.QBDI_SCRIPT_FILEPATH}'.split(' ') + [binary_path] + args\n        cmdlne = \" \".join(cmdlne)\n\n        logger.debug(f'Command line: {cmdlne}')\n\n        # Set environment variables.\n        environ = {\n            'PYQBDIPRELOAD_COVERAGE_STRATEGY': strategy.name,\n            'PYQBDIPRELOAD_OUTPUT_FILEPATH': output_path,\n            'PYQBDIPRELOAD_DUMP_TRACE': str(dump_trace),\n            'LD_BIND_NOW': '1',\n        }\n        environ.update(os.environ)\n\n        # Open stdin file if it is present.\n        stdin_fp = open(stdin_file, 'rb') if stdin_file else None\n\n        # Run QBDI tool.\n        process = subprocess.Popen(cmdlne,\n                                   shell=True,\n                                   stdin=stdin_fp,\n                                   stdout=subprocess.DEVNULL,\n                                   stderr=subprocess.DEVNULL,\n                                   cwd=cwd,\n                                   env=environ)\n        try:\n            stdout, stderr = process.communicate(timeout=timeout)\n            # for line in stdout.split(b\"\\n\"):\n            #     logger.debug(f\"stdout: {line}\")\n            # for line in stderr.split(b\"\\n\"):\n            #     logger.debug(f\"stdout: {line}\")\n        except subprocess.TimeoutExpired:\n            process.wait()\n            # logger.warning('QBDI tracer timeout expired!')\n            raise TraceException('QBDI tracer timeout expired')\n\n        if stdin_fp:\n            stdin_fp.close()\n\n        return Path(output_path).exists()\n\n    @staticmethod\n    def from_file(coverage_path: str) -> 'QBDITrace':\n        \"\"\"Load coverage from a file.\n\n        :param coverage_path: Path to the coverage file.\n        :type coverage_path: :py:obj:`str`.\n        \"\"\"\n        trace = QBDITrace()\n\n        logger.debug(f'Loading coverage file: {coverage_path}')\n        with open(coverage_path, 'rb') as fd:\n            data = json.load(fd)\n\n        cov = CoverageSingleRun(CoverageStrategy[data[\"coverage_strategy\"]])\n        cov.covered_instructions = Counter({int(k): v for k, v in data[\"covered_instructions\"].items()})\n\n        for (src, dst, not_taken) in data[\"covered_items\"]:\n            if not_taken is None:\n                cov.add_covered_dynamic_branch(src, dst)\n            else:\n                cov.add_covered_branch(src, dst, not_taken)\n\n        trace._coverage = cov\n        trace._trace = data['trace']\n        trace.modules = data['modules_base']\n\n        return trace\n\n    @property\n    def coverage(self) -> CoverageSingleRun:\n        \"\"\"\n        CoverageSingleRun associated with the trace.\n\n        :return: coverage object\n        \"\"\"\n        if not self._coverage:\n            logger.warning(\"Please .run() the trace before querying coverage\")\n\n        return self._coverage\n\n    @property\n    def trace(self) -> List[int]:\n        \"\"\"\n        List of addresses executed.\n\n        :return: list of addresses\n        \"\"\"\n        return self._trace", "\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) < 2:\n        print(\"Usage: trace.py program [args]\")\n        sys.exit(1)\n\n    tritondse.logging.enable()\n\n    if QBDITrace.run(CoverageStrategy.EDGE, sys.argv[1], sys.argv[2:], \"/tmp/test.cov\", dump_trace=False):\n        coverage = QBDITrace.from_file(\"/tmp/test.cov\")\n    else:\n        print(\"Something went wrong during trace generation\")", ""]}
{"filename": "tritondse/heap_allocator.py", "chunked_list": ["from tritondse.types import Addr, ByteSize, Perm\nfrom tritondse.memory import Memory\nfrom tritondse.exception import AllocatorException\nimport tritondse.logging\n\nlogger = tritondse.logging.get()\n\n\nclass HeapAllocator(object):\n    \"\"\"\n    Custom tiny heap allocator. Used by built-ins routines like malloc/free.\n    This allocation manager also provides an API enabling checking whether\n    a pointer is allocated freed etc.\n\n    .. warning:: This allocator is very simple and does not perform any\n                 coalescing of freed memory areas. Thus it may not correctly\n                 model the behavior of libc allocator.\n    \"\"\"\n\n    def __init__(self, start: Addr, end: Addr, memory: Memory):\n        \"\"\"\n        Class constructor. Takes heap bounds as parameter.\n\n        :param start Addr: Where the heap area can start\n        :type start: :py:obj:`tritondse.types.Addr`\n        :param end Addr: Where the heap area must be end\n        :type start: :py:obj:`tritondse.types.Addr`\n        :param memory: Memory: Memory object on which to perform allocations\n        \"\"\"\n        # Range of the memory mapping\n        self.start: Addr = start\n        #: Starting address of the heap\n\n        self.end: Addr = end\n        #: Ending address of the heap\n\n        self._curr_offset: Addr = self.start  #: Heap current offset address\n        self._memory = memory\n\n        # Pools memory\n        self.alloc_pool = dict() # {ptr: MemMap}\n        self.free_pool = dict() # {size: set(MemMap ...)}\n\n        # TODO: For a to-the-moon allocator, we could merge freed chunks. Like 4 chunks of 1 byte into one chunk of 4 bytes.\n        # TODO: For a to-the-moon allocator, we could split a big chunk into two chunks when asking an allocation.\n\n    def alloc(self, size: ByteSize) -> Addr:\n        \"\"\"\n        Performs an allocation of the given byte size.\n\n        :param size: Byte size to allocate\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :raise AllocatorException: if not memory is available\n        :return: The pointer address allocated\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n\n        if size <= 0:\n            logger.error(f\"Heap: invalid allocation size {size}\")\n            return 0\n\n        ptr = None\n        for sz in sorted(x for x in self.free_pool if x >= size):\n            # get the free chunk\n            ptr = self.free_pool[sz].pop().start\n\n            # If the set is empty after the pop(), remove the entry\n            if not self.free_pool[sz]:\n                del self.free_pool[sz]\n            break\n\n        if ptr is None: # We did not found reusable freed ptr\n            ptr = self._curr_offset\n            self._curr_offset += size\n\n        # Now we can allocate the chunk\n        map = self._memory.map(ptr, size, Perm.R | Perm.W, 'heap')\n        self.alloc_pool.update({ptr: map})\n\n        return ptr\n\n    def free(self, ptr: Addr) -> None:\n        \"\"\"\n        Free the given memory chunk.\n\n        :param ptr: Address to free\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :raise AllocatorException: if the pointer has already been freed or if it has never been allocated\n        \"\"\"\n        if self.is_ptr_freed(ptr):\n            raise AllocatorException('Double free or corruption!')\n\n        if not self.is_ptr_allocated(ptr):\n            raise AllocatorException(f'Invalid pointer ({hex(ptr)})')\n\n        # Add the chunk into our free_pool\n        memmap = self.alloc_pool[ptr]\n        if memmap.size in self.free_pool:\n            self.free_pool[memmap.size].add(memmap)\n        else:\n            self.free_pool[memmap.size] = {memmap}\n\n        # Remove the chunk from our alloc_pool\n        self._memory.unmap(ptr)\n        del self.alloc_pool[ptr]\n\n    def is_ptr_allocated(self, ptr: Addr) -> bool:\n        \"\"\"\n        Check whether a given address has been allocated\n\n        :param ptr: Address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :return: True if pointer points to an allocated memory region\n        \"\"\"\n        return self._memory.is_mapped(ptr, 1)\n\n    def is_ptr_freed(self, ptr: Addr) -> bool:\n        \"\"\"\n        Check whether a given pointer has recently been freed.\n\n        :param ptr: Address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :return: True if pointer has been freed, False otherwise\n        \"\"\"\n        # FIXME: This function is linear in the size of chunks. Can make it logarithmic\n        for size, chunks in self.free_pool.items():\n            for chunk in chunks:\n                if chunk.start <= ptr < chunk.start + size:\n                    return True\n        return False", "class HeapAllocator(object):\n    \"\"\"\n    Custom tiny heap allocator. Used by built-ins routines like malloc/free.\n    This allocation manager also provides an API enabling checking whether\n    a pointer is allocated freed etc.\n\n    .. warning:: This allocator is very simple and does not perform any\n                 coalescing of freed memory areas. Thus it may not correctly\n                 model the behavior of libc allocator.\n    \"\"\"\n\n    def __init__(self, start: Addr, end: Addr, memory: Memory):\n        \"\"\"\n        Class constructor. Takes heap bounds as parameter.\n\n        :param start Addr: Where the heap area can start\n        :type start: :py:obj:`tritondse.types.Addr`\n        :param end Addr: Where the heap area must be end\n        :type start: :py:obj:`tritondse.types.Addr`\n        :param memory: Memory: Memory object on which to perform allocations\n        \"\"\"\n        # Range of the memory mapping\n        self.start: Addr = start\n        #: Starting address of the heap\n\n        self.end: Addr = end\n        #: Ending address of the heap\n\n        self._curr_offset: Addr = self.start  #: Heap current offset address\n        self._memory = memory\n\n        # Pools memory\n        self.alloc_pool = dict() # {ptr: MemMap}\n        self.free_pool = dict() # {size: set(MemMap ...)}\n\n        # TODO: For a to-the-moon allocator, we could merge freed chunks. Like 4 chunks of 1 byte into one chunk of 4 bytes.\n        # TODO: For a to-the-moon allocator, we could split a big chunk into two chunks when asking an allocation.\n\n    def alloc(self, size: ByteSize) -> Addr:\n        \"\"\"\n        Performs an allocation of the given byte size.\n\n        :param size: Byte size to allocate\n        :type size: :py:obj:`tritondse.types.ByteSize`\n        :raise AllocatorException: if not memory is available\n        :return: The pointer address allocated\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n\n        if size <= 0:\n            logger.error(f\"Heap: invalid allocation size {size}\")\n            return 0\n\n        ptr = None\n        for sz in sorted(x for x in self.free_pool if x >= size):\n            # get the free chunk\n            ptr = self.free_pool[sz].pop().start\n\n            # If the set is empty after the pop(), remove the entry\n            if not self.free_pool[sz]:\n                del self.free_pool[sz]\n            break\n\n        if ptr is None: # We did not found reusable freed ptr\n            ptr = self._curr_offset\n            self._curr_offset += size\n\n        # Now we can allocate the chunk\n        map = self._memory.map(ptr, size, Perm.R | Perm.W, 'heap')\n        self.alloc_pool.update({ptr: map})\n\n        return ptr\n\n    def free(self, ptr: Addr) -> None:\n        \"\"\"\n        Free the given memory chunk.\n\n        :param ptr: Address to free\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :raise AllocatorException: if the pointer has already been freed or if it has never been allocated\n        \"\"\"\n        if self.is_ptr_freed(ptr):\n            raise AllocatorException('Double free or corruption!')\n\n        if not self.is_ptr_allocated(ptr):\n            raise AllocatorException(f'Invalid pointer ({hex(ptr)})')\n\n        # Add the chunk into our free_pool\n        memmap = self.alloc_pool[ptr]\n        if memmap.size in self.free_pool:\n            self.free_pool[memmap.size].add(memmap)\n        else:\n            self.free_pool[memmap.size] = {memmap}\n\n        # Remove the chunk from our alloc_pool\n        self._memory.unmap(ptr)\n        del self.alloc_pool[ptr]\n\n    def is_ptr_allocated(self, ptr: Addr) -> bool:\n        \"\"\"\n        Check whether a given address has been allocated\n\n        :param ptr: Address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :return: True if pointer points to an allocated memory region\n        \"\"\"\n        return self._memory.is_mapped(ptr, 1)\n\n    def is_ptr_freed(self, ptr: Addr) -> bool:\n        \"\"\"\n        Check whether a given pointer has recently been freed.\n\n        :param ptr: Address to check\n        :type ptr: :py:obj:`tritondse.types.Addr`\n        :return: True if pointer has been freed, False otherwise\n        \"\"\"\n        # FIXME: This function is linear in the size of chunks. Can make it logarithmic\n        for size, chunks in self.free_pool.items():\n            for chunk in chunks:\n                if chunk.start <= ptr < chunk.start + size:\n                    return True\n        return False", ""]}
{"filename": "tritondse/seeds_manager.py", "chunked_list": ["# built-in imports\nimport time\nimport json\nfrom typing import Generator, Optional, Type\nfrom collections import Counter\n\n# local imports\nfrom tritondse.seed              import Seed, SeedStatus\nfrom tritondse.callbacks         import CallbackManager\nfrom tritondse.coverage          import GlobalCoverage, CovItem, CoverageStrategy", "from tritondse.callbacks         import CallbackManager\nfrom tritondse.coverage          import GlobalCoverage, CovItem, CoverageStrategy\nfrom tritondse.worklist          import WorklistAddressToSet, FreshSeedPrioritizerWorklist, SeedScheduler\nfrom tritondse.workspace         import Workspace\nfrom tritondse.symbolic_executor import SymbolicExecutor\nfrom tritondse.types             import SolverStatus, SymExType\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"seedmanager\")\n", "logger = tritondse.logging.get(\"seedmanager\")\n\n\nclass SeedManager:\n    \"\"\"\n    Seed Manager.\n    This class is in charge of providing the next seed to execute by prioritizing\n    them. It also holds various sets of pending seeds, corpus, crashes etc and\n    manages them in the workspace.\n\n    It contains basically 2 types of seeds which are:\n\n    * pending seeds (kept in the seed scheduler). These are the seeds that might\n      be selected to be run\n    * seed consumed (corpus, crash, hangs) which are seeds not meant to be re-executed\n      as they cannot lead to new paths, all candidate paths are UNSAT etc.\n    \"\"\"\n    def __init__(self, coverage: GlobalCoverage, workspace: Workspace, smt_queries_limit: int, callback_manager: CallbackManager = None, seed_scheduler_class: Type[SeedScheduler] = None):\n        \"\"\"\n        :param coverage: global coverage object. The instance will be updated by the seed manager\n        :type coverage: GlobalCoverage\n        :param workspace: workspace instance object.\n        :type workspace: Workspace\n        :param smt_queries_limit: maximum number of queries for  a given execution\n        :type smt_queries_limit: int\n        :param scheduler: seed scheduler object to use as scheduling strategy\n        :type scheduler: SeedScheduler\n        \"\"\"\n        self.smt_queries_limit = smt_queries_limit\n        self.workspace = workspace\n        self.coverage = coverage\n        if seed_scheduler_class is None:\n            self.worklist = FreshSeedPrioritizerWorklist(self)\n        else:\n            self.worklist = seed_scheduler_class(self)\n        self.cbm = callback_manager\n\n        self.corpus = set()\n        self.crash = set()\n        self.hangs = set()\n\n        self.__load_seed_workspace()\n\n        self._solv_count = 0\n        self._solv_time_sum = 0\n        self._solv_status = {SolverStatus.SAT: 0, SolverStatus.UNSAT: 0, SolverStatus.UNKNOWN: 0, SolverStatus.TIMEOUT: 0}\n        self._stat_branch_reverted = Counter()\n        self._stat_branch_fail = Counter()\n        self._current_solv_time = 0\n\n    @property\n    def total_solving_time(self) -> float:\n        return self._solv_time_sum\n\n    def __load_seed_workspace(self):\n        \"\"\" Load seed from the workspace \"\"\"\n        # Load seed from the corpus\n        for seed in self.workspace.iter_corpus():\n            self.corpus.add(seed)\n        # Load hangs\n        for seed in self.workspace.iter_hangs():\n            self.hangs.add(seed)\n        # Load crashes\n        for seed in self.workspace.iter_crashes():\n            self.crash.add(seed)\n        # Load worklist\n        for seed in self.workspace.iter_worklist():\n            self.worklist.add(seed)\n\n    def is_new_seed(self, seed: Seed) -> bool:\n        \"\"\"\n        Check if a seed is a new one (not into corpus, crash and hangs)\n\n        :param seed: The seed to test\n        :type seed: Seed\n        :return: True if the seed is a new one\n\n        .. warning:: That function does not check that the seed\n                     is not in the pending seeds queue\n        \"\"\"\n        return sum(seed in x for x in [self.corpus, self.crash, self.hangs]) == 0\n\n    def add_seed_queue(self, seed: Seed) -> None:\n        \"\"\"\n        Add a seed to to appropriate internal queue depending\n        on its status. If it is new it is added in pending seed,\n        if OK, HANG or CRASH it the appropriate set.\n        **Note that the seed is not written in the workspace**\n\n        :param seed: Seed to add in an internal queue\n        :type seed: Seed\n        \"\"\"\n        # Add the seed to the appropriate list\n        if seed.status == SeedStatus.NEW:\n            self.worklist.add(seed)\n        elif seed.status == SeedStatus.OK_DONE:\n            self.corpus.add(seed)\n        elif seed.status == SeedStatus.HANG:\n            self.hangs.add(seed)\n        elif seed.status == SeedStatus.CRASH:\n            self.crash.add(seed)\n        else:\n            assert False\n\n    def post_execution(self, execution: SymbolicExecutor, seed: Seed, solve_new_path: int = True) -> float:\n        \"\"\"\n        Function called after each execution. It updates the global\n        code coverage object, and tries to generate new paths through\n        SMT in accordance with the seed scheduling strategy.\n\n        :param execution: The current execution\n        :type execution: SymbolicExecutor\n        :param seed: The seed of the execution\n        :type seed: Seed\n        :param solve_new_path: Whether or not to solve constraint to find new paths\n        :type solve_new_path: bool\n        :return: Total SMT solving time\n        \"\"\"\n\n        # Update instructions covered from the last execution into our exploration coverage\n        self.coverage.merge(execution.coverage)\n        self.worklist.update_worklist(execution.coverage)\n\n        # if the seed have target checks that we covered it\n        if seed.target:\n            color = (\"YES\", 92) if execution.coverage.is_covered(seed.target) else (\"NO\", 91)\n            logger.info(f\"Seed covered its target: \\033[{color[1]}m{color[0]}\\033[0m\")\n\n        # reset the current solving time\n        self._current_solv_time = 0\n\n        # Iterate all pending seeds to be added in the right location\n        for s in execution.pending_seeds:\n            if self.is_new_seed(s):\n                if not s.coverage_objectives:      # If they don't have objectives set the Ellipsis wildcard\n                    s.coverage_objectives.add(...)\n                self._add_seed(s)  # will add the seed in both internal queues & workspace\n            else:\n                logger.warning(f\"dropping enqueued seed: {s.hash} (already seen)\")\n\n        # Update the current seed queue\n        if seed.status == SeedStatus.NEW:\n            logger.error(f\"seed not meant to be NEW at the end of execution ({seed.hash}) (dropped)\")\n            self.drop_seed(seed)\n\n        elif seed.status in [SeedStatus.HANG, SeedStatus.CRASH]:\n            self.archive_seed(seed)\n            # NOTE: Do not perform further processing on the seed (like generating inputs from it)\n\n        elif seed.status == SeedStatus.OK_DONE:\n            if self.coverage.can_improve_coverage(execution.coverage) or self.coverage.can_cover_symbolic_pointers(execution):\n                items = self.coverage.new_items_to_cover(execution.coverage)\n                seed.coverage_objectives = items  # Set its new objectives\n\n                if self.worklist.can_solve_models() and solve_new_path:     # No fresh seeds pending thus can solve model\n                    logger.info(f'Seed {seed.hash} generate new coverage')\n                    self._generate_new_inputs(execution)\n                    self.archive_seed(seed)\n                else:\n                    logger.info(f\"Seed {seed.hash} push back in worklist (to unstack fresh)\")\n                    seed.status = SeedStatus.NEW  # Reset its status for later run\n                    self.add_seed_queue(seed)  # will be pushed back in worklist\n            else:\n                self.archive_seed(seed)\n                logger.info(f'Seed {seed.hash} archived cannot generate new coverage [{seed.status.name}]')\n\n        else:\n            assert False\n\n        logger.info(f\"Corpus:{len(self.corpus)} Crash:{len(self.crash)}\")\n        self.worklist.post_execution()\n        logger.info(f\"Coverage instruction:{self.coverage.unique_instruction_covered} covitem:{self.coverage.unique_covitem_covered}\")\n        return self._current_solv_time\n\n    def _generate_new_inputs(self, execution: SymbolicExecutor):\n        # Generate new inputs\n        for new_input in self.__iter_new_inputs(execution):\n            # Check if we already have executed this new seed\n            if self.is_new_seed(new_input):\n                self.worklist.add(new_input)\n                self.workspace.save_seed(new_input)\n                logger.info(f'New seed model {new_input.filename} dumped [{new_input.status.name}]')\n            else:\n                logger.info(f\"New seed {new_input.filename} has already been generated\")\n\n    def __iter_new_inputs(self, execution: SymbolicExecutor) -> Generator[Seed, None, None]:\n        # Get the astContext\n        actx = execution.pstate.actx\n\n        # We start with any input. T (Top)\n        path_predicate = [actx.equal(actx.bvtrue(), actx.bvtrue())]\n\n        # Define a limit of branch constraints\n        smt_queries = 0\n\n        # Solver status\n        status = None\n        path_constraints = execution.pstate.get_path_constraints()\n        total_len = len(path_constraints)\n        path_generator = self.coverage.iter_new_paths(path_constraints)\n\n        try:\n            while True:\n                # If smt_queries_limit is zero: unlimited queries\n                # If smt_queries_limit is negative: no query\n                if self.smt_queries_limit < 0:\n                    logger.info(f'The configuration is defined as: no query')\n                    break\n\n                typ, p_prefix, branch, covitem, ith = path_generator.send(status)\n\n                # Create edge in case of conditional branch, for all the other the edge shall be already set\n                edge = (branch['srcAddr'], branch['dstAddr']) if typ == SymExType.CONDITIONAL_JMP else covitem\n\n                # Add path_prefix in path predicate (regardless on whether we solve the item or not)\n                path_predicate.extend(x.getTakenPredicate() for x in p_prefix)\n\n                expr = branch['constraint'] if typ == SymExType.CONDITIONAL_JMP else branch\n\n                # Call on_branch_solving, if one replies False does not solve the branch\n                if self.cbm is not None:\n                    cb_result = all(cb(execution, execution.pstate, edge, typ, expr, path_predicate) for cb in self.cbm.get_on_solving_callback())\n                else:\n                    cb_result = True\n\n                # Skip processing the current path in case the result of the\n                # callbacks return False.\n                if not cb_result:\n                    status = None\n                    continue\n\n                # Create the constraint\n                if typ in [SymExType.DYNAMIC_JMP, SymExType.SYMBOLIC_READ, SymExType.SYMBOLIC_WRITE]:\n                    expr, (addr, tgt) = branch, covitem   # branch and covitem have a different meaning here\n                    ts = time.time()\n                    results = execution.pstate.solve_enumerate_expression(expr, path_predicate, [tgt], execution.config.smt_enumeration_limit)  # enumerate values\n\n                    # all stats updates\n                    solve_time = time.time() - ts\n                    count = len(results)\n                    status = SolverStatus.SAT if count else SolverStatus.UNSAT\n                    self._update_solve_stats(None, status, solve_time, count)\n\n                    results = [(x[0], (addr, x[1])) for x in results]   # extract results\n                    logger.info(f'pc:{ith}/{total_len} | Query n\u00b0{smt_queries}-{smt_queries+count}, enumerate:{expr} (time: {solve_time:.02f}s) values:[{count}:{self._pp_smt_status(status)}]')\n                    smt_queries += count+1  # for the unsat\n\n                elif typ == SymExType.CONDITIONAL_JMP:\n                    # if debug_pp=True solve the branch that has been taken\n                    branch_cst = actx.lnot(branch['constraint']) if execution.debug_pp else branch['constraint']\n                    constraint = actx.land(path_predicate + [branch_cst])\n\n                    # Solve the constraint\n                    ts = time.time()\n                    status, model = execution.pstate.solve(constraint, with_pp=False)  # Do not use path predicate as we are iterating it\n                    solve_time = time.time() - ts\n                    self._update_solve_stats(covitem, status, solve_time)\n                    results = [(model, covitem)]\n                    smt_queries += 1\n                    logger.info(f'pc:{ith}/{total_len} | Query n\u00b0{smt_queries}, solve:{self.coverage.pp_item(covitem)} (time: {solve_time:.02f}s) [{self._pp_smt_status(status)}]')\n                else:\n                    assert False\n\n                if status == SolverStatus.SAT:\n                    for model, covitem in results:\n                        new_seed = execution.mk_new_seed_from_model(model)\n                        # Trick to keep track of which target a seed is meant to cover\n                        new_seed.coverage_objectives.add(covitem)\n                        new_seed.meta_fname.append(self.pp_meta_filename(covitem, typ))\n                        new_seed.target = covitem if typ == SymExType.CONDITIONAL_JMP else None\n                        yield new_seed  # Yield the seed to get it added in the worklist\n                else:\n                    pass\n\n                # Check if we reached the limit of query\n                if self.smt_queries_limit and smt_queries >= self.smt_queries_limit:\n                    logger.info(f'Limit of query reached. Stop asking for models')\n                    break\n\n        except StopIteration:  # We have iterated the whole path generator\n            pass\n\n    def _update_solve_stats(self, covitem: Optional[CovItem], status: SolverStatus, solving_time: float, count=1):\n        self._solv_count += count\n        self._solv_time_sum += solving_time\n        self._current_solv_time += solving_time\n        self._solv_status[status] += count\n        logger.debug(f'Solve stats: solve_count={self._solv_count} solving_time={solving_time} solve_time_sum={self._solv_time_sum} current_solve_time={self._current_solv_time} solv_status={status} / {self._solv_status[status]}')\n\n        if covitem:\n            if status == SolverStatus.SAT:\n                self._stat_branch_reverted[covitem] += count  # Update stats\n                if covitem in self._stat_branch_fail:\n                    self._stat_branch_fail.pop(covitem)\n            elif status == SolverStatus.UNSAT:\n                self._stat_branch_fail[covitem] += count\n\n    def pick_seed(self) -> Optional[Seed]:\n        \"\"\"\n        Get the next seed to be executed by querying it\n        in the seed scheduler.\n\n        :returns: Seed to execute from the pending seeds\n        :rtype: Seed\n        \"\"\"\n        return self.worklist.pick()\n\n    def seeds_available(self) -> bool:\n        \"\"\"\n        Checks whether or not there is still pending seeds to process.\n\n        :returns: True if seeds are still pending\n        \"\"\"\n        return self.worklist.has_seed_remaining()\n\n    def add_new_seed(self, seed: Seed) -> None:\n        \"\"\"\n        Add the given seed in the manager.\n        The function uses its type to know where to add the seed.\n\n        :param seed: seed to add\n        :type seed: Seed\n        \"\"\"\n        if self.is_new_seed(seed):\n            self._add_seed(seed)\n            logger.debug(f'Seed {seed.filename} dumped [{seed.status.name}]')\n        else:\n            logger.debug(f\"seed {seed} is already known (not adding it)\")\n\n    def _add_seed(self, seed: Seed) -> None:\n        \"\"\" Add the seed in both internal queues but also workspace \"\"\"\n        self.add_seed_queue(seed)\n        self.workspace.save_seed(seed)\n\n    def drop_seed(self, seed: Seed) -> None:\n        \"\"\"\n        Drop a seed that is not of interest anymore.\n        The function thus switch its status to ``OK_DONE``\n        and move it in the corpus. *(the seed is not removed\n        from the corpus)*\n\n        :param seed: seed object to drop\n        :type seed: Seed\n        \"\"\"\n        self.archive_seed(seed)\n\n    def archive_seed(self, seed: Seed, status: SeedStatus = None) -> None:\n        \"\"\"\n        Send a seed in the corpus. As such, the seed\n        is not meant to be used anymore (for finding\n        new seeds).\n\n        :param seed: seed object\n        :type seed: Seed\n        :param status: optional status to assign the seed\n        :type status: SeedStatus\n        \"\"\"\n        if status:\n            seed.status = status\n        self.add_seed_queue(seed)  # Will put it in the corpus\n        self.workspace.update_seed_location(seed)  # Will put it in the corpus in files\n\n    def post_exploration(self) -> None:\n        \"\"\"\n        Function called at the end of exploration. It perform\n        some stats printing, but would also perform any clean\n        up tasks. *(not meant to be called by the user)*\n        \"\"\"\n        # Do things you would do at the very end of exploration\n        # (or just before it becomes idle)\n        stats = {\n            \"total_solving_time\": self._solv_time_sum,\n            \"total_solving_attempt\": self._solv_count,\n            \"branch_reverted\": {str(k): v for k, v in self._stat_branch_reverted.items()}, # convert covitem to str whatever it is\n            \"branch_not_solved\": {str(k): v for k, v in self._stat_branch_fail.items()},  # convert covitem to str whatever it is\n            \"UNSAT\": self._solv_status[SolverStatus.UNSAT],\n            \"SAT\": self._solv_status[SolverStatus.SAT],\n            \"TIMEOUT\": self._solv_status[SolverStatus.TIMEOUT]\n        }\n        self.workspace.save_metadata_file(\"solving_stats.json\", json.dumps(stats, indent=2))\n        logger.info(f\"Branches reverted: {len(self._stat_branch_reverted)}  Branches still fail: {len(self._stat_branch_fail)}\")\n        self.worklist.post_exploration(self.workspace)\n\n    @staticmethod\n    def _pp_smt_status(status: SolverStatus) -> str:\n        \"\"\" The pretty print function of the solver status \"\"\"\n        mapper = {SolverStatus.SAT: 92, SolverStatus.UNSAT: 91, SolverStatus.TIMEOUT: 93, SolverStatus.UNKNOWN: 95}\n        return f\"\\033[{mapper[status]}m{status.name}\\033[0m\"\n\n    def pp_meta_filename(self, covitem: CovItem, typ: SymExType) -> str:\n        \"\"\"pretty-print a covitem\"\"\"\n        if typ == SymExType.CONDITIONAL_JMP:\n            pp_item = self.coverage.pp_item(covitem)\n        else:\n            pp_item = f\"({covitem[0]:#08x}:@[{covitem[1]:#08x}])\"\n        map = {SymExType.CONDITIONAL_JMP: \"CC\",\n               SymExType.SYMBOLIC_READ: \"SR\",\n               SymExType.SYMBOLIC_WRITE: \"SW\",\n               SymExType.DYNAMIC_JMP: \"DYN\"}\n        return f\"{map[typ]}_{pp_item}\"", ""]}
{"filename": "tritondse/callbacks.py", "chunked_list": ["# built-in imports\nfrom __future__ import annotations\nfrom enum import Enum, auto\nfrom typing import Callable, Tuple, List, Optional, Union, Any\nimport enum_tools.documentation\n# third-party imports\nfrom triton import CALLBACK, Instruction, MemoryAccess, OPCODE\n\n# local imports\nfrom tritondse.process_state import ProcessState", "# local imports\nfrom tritondse.process_state import ProcessState\nfrom tritondse.types import Addr, Input, Register, Expression, Edge, SymExType, AstNode\nfrom tritondse.thread_context import ThreadContext\nfrom tritondse.seed import Seed\nfrom tritondse.memory import MemoryAccessViolation\nimport tritondse.logging\n\nlogger = tritondse.logging.get()  # get root tritondse logger\n", "logger = tritondse.logging.get()  # get root tritondse logger\n\n\n@enum_tools.documentation.document_enum\nclass CbPos(Enum):\n    \"\"\" Enmus representing callback position \"\"\"\n    BEFORE = auto()  # doc: Callback should be executed before the hook location interpretation\n    AFTER = auto()   # doc: Callback should be executed after the hook location interpreation\n\n", "\n\n@enum_tools.documentation.document_enum\nclass CbType(Enum):\n    \"\"\" Enmus representing all kind of callbacks \"\"\"\n    CTX_SWITCH = auto()     # doc: context-switch callback\n    MEMORY_READ = auto()    # doc: memory read callback\n    MEMORY_WRITE = auto()   # doc: memory write callback\n    POST_RTN = auto()       # doc: routine callback (after)\n    POST_ADDR = auto()      # doc: address class back (after)\n    POST_EXEC = auto()      # doc: post execution callback, after a SymbolicExecutor run\n    POST_INST = auto()      # doc: post instruction callback (all of them)\n    PRE_ADDR = auto()       # doc: pre address callback\n    PRE_EXEC = auto()       # doc: pre-execution callback (before launching a Symbolic Executor)\n    PRE_INST = auto()       # doc: pre instruction callback (all of them)\n    PRE_RTN = auto()        # doc: routine callback (before)\n    REG_READ = auto()       # doc: a register is read\n    REG_WRITE = auto()      # doc: a register is written\n    NEW_INPUT = auto()      # doc: new input has been generated\n    EXPLORE_STEP = auto()   # doc: an exploration step is reached. Called in between each SymbolicExecutor run\n    PRE_MNEM = auto()       # doc: callback on a specific mnemonic (as a string) (before)\n    POST_MNEM = auto()      # doc: callback on a specific mnemonic (as a string) (after)\n    PRE_OPCODE = auto()     # doc: callback on a specific opcode (as bytes) (before)\n    POST_OPCODE = auto()    # doc: callback on a specific opcode (as bytes) (after)\n    BRANCH_COV = auto()     # doc: a new branch is getting covered\n    SYMEX_SOLVING = auto()  # doc: An SMT formula is getting solved\n    MEM_VIOLATION = auto()  # doc: A memory violation occured", "\n\nAddrCallback            = Callable[['SymbolicExecutor', ProcessState, Addr], None]\nExplorationStepCallback = Callable[['SymbolicExplorator'], None]\nInstrCallback           = Callable[['SymbolicExecutor', ProcessState, Instruction], None]\nMemReadCallback         = Callable[['SymbolicExecutor', ProcessState, MemoryAccess], None]\nMemWriteCallback        = Callable[['SymbolicExecutor', ProcessState, MemoryAccess, int], None]\nMnemonicCallback        = Callable[['SymbolicExecutor', ProcessState, OPCODE], None]\nSymExSolvingCallback    = Callable[['SymbolicExecutor', ProcessState, Edge, SymExType, AstNode, List[AstNode]], bool]\nBranchCoveredCallback   = Callable[['SymbolicExecutor', ProcessState, Edge], bool]", "SymExSolvingCallback    = Callable[['SymbolicExecutor', ProcessState, Edge, SymExType, AstNode, List[AstNode]], bool]\nBranchCoveredCallback   = Callable[['SymbolicExecutor', ProcessState, Edge], bool]\nNewInputCallback        = Callable[['SymbolicExecutor', ProcessState, Seed], Optional[Seed]]\nOpcodeCallback          = Callable[['SymbolicExecutor', ProcessState, bytes], None]\nRegReadCallback         = Callable[['SymbolicExecutor', ProcessState, Register], None]\nRegWriteCallback        = Callable[['SymbolicExecutor', ProcessState, Register, int], None]\nRtnCallback             = Callable[['SymbolicExecutor', ProcessState, str, Addr], Optional[Union[int, Expression]]]\nSymExCallback           = Callable[['SymbolicExecutor', ProcessState], None]\nThreadCallback          = Callable[['SymbolicExecutor', ProcessState, ThreadContext], None]\nMemoryViolationCallback = Callable[['SymbolicExecutor', ProcessState, MemoryAccessViolation], None]", "ThreadCallback          = Callable[['SymbolicExecutor', ProcessState, ThreadContext], None]\nMemoryViolationCallback = Callable[['SymbolicExecutor', ProcessState, MemoryAccessViolation], None]\n\n\nclass ProbeInterface(object):\n    \"\"\" The Probe interface \"\"\"\n    def __init__(self):\n        self._cbs: List[Tuple[CbType, Callable, Optional[str]]] = []  #: list of callback infos\n\n    @property\n    def callbacks(self) -> List[Tuple[CbType, Callable, Optional[Any]]]:\n        return self._cbs\n\n    def _add_callback(self, typ: CbType, callback: Callable, arg: str = None):\n        \"\"\" Add a callback \"\"\"\n        self._cbs.append((typ, callback, arg))", "\n\nclass CallbackManager(object):\n    \"\"\"\n    Class used to aggregate all callbacks that can be plugged\n    inside a SymbolicExecutor running session. The internal\n    structure ensure that check the presence of callback can\n    be made in Log(N). All callbacks are designed to be read-only\n    \"\"\"\n\n    def __init__(self):\n        self._se = None\n\n        # SymbolicExplorator callbacks\n        self._step_cbs = []  # Callback called between each exploration steps\n\n        # SymbolicExecutor callbacks\n        self._pc_addr_cbs        = {}  # addresses reached\n        self._opcode_cbs         = {}  # opcode before and after\n        self._mnemonic_cbs       = {}  # mnemonic before and after\n        self._instr_cbs          = {CbPos.BEFORE: [], CbPos.AFTER: []}  # all instructions\n        self._pre_exec           = []  # before execution\n        self._post_exec          = []  # after execution\n        self._ctx_switch         = []  # on each thread context switch (implementing pre/post?)\n        self._new_input_cbs      = []  # each time an SMT model is get\n        self._branch_solving_cbs = []  # each time a branch is about to be solved\n        self._branch_covered_cbs = []  # each time a branch is covered\n        self._pre_rtn_cbs        = {}  # before imported routine calls ({str: [RtnCallback]})\n        self._post_rtn_cbs       = {}  # after imported routine calls ({str: [RtnCallback]})\n        self._mem_violation_cbs  = []  # called when an exception is raised\n\n        # Triton callbacks\n        self._mem_read_cbs  = []  # memory reads\n        self._mem_write_cbs = []  # memory writes\n        self._reg_read_cbs  = []  # register reads\n        self._reg_write_cbs = []  # register writes\n        self._empty         = True\n\n        # Temporary mapping of function name to register\n        self._func_to_register = {}\n\n    def is_empty(self) -> bool:\n        \"\"\"\n        Check whether a callback has alreday been registered or not\n\n        :return: True if no callback were registered\n        \"\"\"\n        return self._empty\n\n\n    def is_binded(self) -> bool:\n        \"\"\"\n        Check if the callback manager has already been binded on a given process state.\n\n        :return: True if callbacks are binded on a process state\n        \"\"\"\n        return bool(self._se) # and self._se.uid == se.uid)\n\n\n    def _trampoline_mem_read_cb(self, ctx, mem):\n        \"\"\"\n        This function is the trampoline callback on memory read from triton to tritondse\n\n        :param ctx: TritonContext\n        :param mem: MemoryAccess\n        :return: None\n        \"\"\"\n        if self._se.pstate.memory.callbacks_enabled():\n            for cb in self._mem_read_cbs:\n                cb(self._se, self._se.pstate, mem)\n\n\n    def _trampoline_mem_write_cb(self, ctx, mem, value):\n        \"\"\"\n        This function is the trampoline callback on memory write from triton to tritondse\n\n        :param ctx: TritonContext\n        :param mem: MemoryAccess\n        :param value: int\n        :return: None\n        \"\"\"\n        if self._se.pstate.memory.callbacks_enabled():\n            for cb in self._mem_write_cbs:\n                cb(self._se, self._se.pstate, mem, value)\n\n\n    def _trampoline_reg_read_cb(self, ctx, reg):\n        \"\"\"\n        This function is the trampoline callback on register read from triton to tritondse\n\n        :param ctx: TritonContext\n        :param reg: Register\n        :return: None\n        \"\"\"\n        for cb in self._reg_read_cbs:\n            cb(self._se, self._se.pstate, reg)\n\n\n    def _trampoline_reg_write_cb(self, ctx, reg, value):\n        \"\"\"\n        This function is the trampoline callback on register write from triton to tritondse\n\n        :param ctx: TritonContext\n        :param reg: Register\n        :param value: int\n        :return: None\n        \"\"\"\n        for cb in self._reg_write_cbs:\n            cb(self._se, self._se.pstate, reg, value)\n\n    def unbind(self) -> None:\n        \"\"\"\n        Unbind callbacks from the current process state as well as from\n        the Triton Context object.\n        \"\"\"\n        if self.is_binded():\n            self._se.pstate.clear_triton_callbacks()\n            self._se = None\n\n    def bind_to(self, se: 'SymbolicExecutor') -> None:\n        \"\"\"\n        Bind callbacks on the given process state. That step is required\n        to register callbacks on the Triton Context object. This is also\n        used to keep a reference on the SymbolicExecutor object;\n\n        IMPORTANT You MUST call `unbind` once you finish using the\n        SymbolicExecutor.\n\n        :param se: SymbolicExecutor on which to bind callbacks\n        :type se: SymbolicExecutor\n        \"\"\"\n        if self.is_binded() and self._se != se:\n            logger.warning(\"Callback_manager already binded (on a different executor instance)\")\n        # assert not self.is_binded()\n\n        # NOTE This creates a circular dependency between the SymbolicExecutor\n        #      received and this object, as the SymbolicExecutor keeps a\n        #      reference to it. Therefore, it is necessary to call `unbind`\n        #      once you finish using the executor.\n        self._se = se\n\n        # Register only one trampoline by kind of callback. It will be the role\n        # of the trampoline to call every registered tritondse callbacks.\n\n        if self._mem_read_cbs:\n            se.pstate.register_triton_callback(CALLBACK.GET_CONCRETE_MEMORY_VALUE, self._trampoline_mem_read_cb)\n\n        if self._mem_write_cbs:\n            se.pstate.register_triton_callback(CALLBACK.SET_CONCRETE_MEMORY_VALUE, self._trampoline_mem_write_cb)\n\n        if self._reg_read_cbs:\n            se.pstate.register_triton_callback(CALLBACK.GET_CONCRETE_REGISTER_VALUE, self._trampoline_reg_read_cb)\n\n        if self._reg_write_cbs:\n            se.pstate.register_triton_callback(CALLBACK.SET_CONCRETE_REGISTER_VALUE, self._trampoline_reg_write_cb)\n\n        # Check if there is a program on which to register functions callback\n        if self._func_to_register:\n            if se.loader:\n                for fname in list(self._func_to_register):\n                    cbs = self._func_to_register.pop(fname)\n                    addr = se.loader.find_function_addr(fname)\n                    if addr:\n                        for cb in cbs:\n                            self.register_pre_addr_callback(addr, cb)\n                    else:\n                        logger.warning(f\"can't find function '{fname}' in {se.loader}\")\n            else:\n                logger.warning(f\"function callback to resolve but no program provided\")\n\n\n    def register_addr_callback(self, pos: CbPos, addr: Addr, callback: AddrCallback) -> None:\n        \"\"\"\n        Register a callback function on a given address before or after the execution\n        of the associated instruction.\n\n        :param pos: When to trigger the callback (before or after) execution of the instruction\n        :type pos: CbPos\n        :param addr: Address where to trigger the callback\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.AddrCallback`\n        \"\"\"\n        if addr not in self._pc_addr_cbs:\n            self._pc_addr_cbs[addr] = {CbPos.BEFORE: [], CbPos.AFTER: []}\n\n        self._pc_addr_cbs[addr][pos].append(callback)\n        self._empty = False\n\n\n    def register_pre_addr_callback(self, addr: Addr, callback: AddrCallback) -> None:\n        \"\"\"\n        Register pre address callback\n\n        :param addr: Address where to trigger the callback\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.AddrCallback`\n        \"\"\"\n        self.register_addr_callback(CbPos.BEFORE, addr, callback)\n\n\n    def register_post_addr_callback(self, addr: Addr, callback: AddrCallback) -> None:\n        \"\"\"\n        Register post-address callback. Equivalent to register a pre-address on the\n        return site. *(assume the function returns)*\n\n        :param addr: Address where to trigger the callback\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.AddrCallback`\n        \"\"\"\n        self.register_addr_callback(CbPos.AFTER, addr, callback)\n\n\n    def get_address_callbacks(self, addr: Addr) -> Tuple[List[AddrCallback], List[AddrCallback]]:\n        \"\"\"\n        Get all the pre/post callbacks for a given address.\n\n        :param addr: Address where to trigger the callback\n        :type addr: :py:obj:`tritondse.types.Addr`\n        :return: tuple of lists containing callback functions for pre/post respectively\n        \"\"\"\n        cbs = self._pc_addr_cbs.get(addr, None)\n        if cbs:\n            return cbs[CbPos.BEFORE], cbs[CbPos.AFTER]\n        else:\n            return [], []\n\n\n    def register_opcode_callback(self, pos: CbPos, opcode: bytes, callback: OpcodeCallback) -> None:\n        \"\"\"\n        Register a callback function on a given opcode before or after the execution\n        of the associated instruction.\n\n        :param pos: When to trigger the callback (before or after) execution of the instruction\n        :type pos: CbPos\n        :param opcode: Opcode where to trigger the callback\n        :type opcode: :py:obj:`bytes`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.OpcodeCallback`\n        \"\"\"\n        if opcode not in self._opcode_cbs:\n            self._opcode_cbs[opcode] = {CbPos.BEFORE: [], CbPos.AFTER: []}\n\n        self._opcode_cbs[opcode][pos].append(callback)\n        self._empty = False\n\n\n    def register_pre_opcode_callback(self, opcode: bytes, callback: OpcodeCallback) -> None:\n        \"\"\"\n        Register pre-opcode callback.\n\n        :param opcode: Opcode where to trigger the callback\n        :type opcode: :py:obj:`bytes`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.OpcodeCallback`\n        \"\"\"\n        self.register_opcode_callback(CbPos.BEFORE, opcode, callback)\n\n\n    def register_post_opcode_callback(self, opcode: bytes, callback: OpcodeCallback) -> None:\n        \"\"\"\n        Register post-opcode callback.\n\n        :param opcode: Opcode where to trigger the callback\n        :type opcode: :py:obj:`bytes`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.OpcodeCallback`\n        \"\"\"\n        self.register_opcode_callback(CbPos.AFTER, opcode, callback)\n\n\n    def get_opcode_callbacks(self, opcode: bytes) -> Tuple[List[OpcodeCallback], List[OpcodeCallback]]:\n        \"\"\"\n        Get all the pre/post callbacks for a given opcode.\n\n        :param opcode: Opcode where to trigger the callback\n        :type opcode: :py:obj:`bytes`\n        :return: tuple of lists containing callback functions for pre/post respectively\n        \"\"\"\n        cbs = self._opcode_cbs.get(opcode, None)\n        if cbs:\n            return cbs[CbPos.BEFORE], cbs[CbPos.AFTER]\n        else:\n            return [], []\n\n\n    def register_mnemonic_callback(self, pos: CbPos, mnemonic: OPCODE, callback: MnemonicCallback) -> None:\n        \"\"\"\n        Register a callback function on a given mnemonic before or after the execution\n        of the associated instruction.\n\n        :param pos: When to trigger the callback (before or after) execution of the instruction\n        :type pos: CbPos\n        :param mnemonic: Mnemonic where to trigger the callback\n        :type mnemonic: :py:obj:`OPCODE`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.MnemonicCallback`\n        \"\"\"\n        if mnemonic not in self._mnemonic_cbs:\n            self._mnemonic_cbs[mnemonic] = {CbPos.BEFORE: [], CbPos.AFTER: []}\n\n        self._mnemonic_cbs[mnemonic][pos].append(callback)\n        self._empty = False\n\n\n    def register_pre_mnemonic_callback(self, mnemonic: OPCODE, callback: MnemonicCallback) -> None:\n        \"\"\"\n        Register pre-mnemonic callback.\n\n        :param mnemonic: Mnemonic where to trigger the callback\n        :type mnemonic: :py:obj:`OPCODE`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.MnemonicCallback`\n        \"\"\"\n        self.register_mnemonic_callback(CbPos.BEFORE, mnemonic, callback)\n\n\n    def register_post_mnemonic_callback(self, mnemonic: OPCODE, callback: MnemonicCallback) -> None:\n        \"\"\"\n        Register post-mnemonic callback.\n\n        :param mnemonic: Mnemonic where to trigger the callback\n        :type mnemonic: :py:obj:`OPCODE`\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.MnemonicCallback`\n        \"\"\"\n        self.register_mnemonic_callback(CbPos.AFTER, mnemonic, callback)\n\n\n    def get_mnemonic_callbacks(self, mnemonic: OPCODE) -> Tuple[List[MnemonicCallback], List[MnemonicCallback]]:\n        \"\"\"\n        Get all the pre/post callbacks for a given mnemonic.\n\n        :param mnemonic: Mnemonic where to trigger the callback\n        :type mnemonic: :py:obj:`OPCODE`\n        :return: tuple of lists containing callback functions for pre/post respectively\n        \"\"\"\n        cbs = self._mnemonic_cbs.get(mnemonic, None)\n        if cbs:\n            return cbs[CbPos.BEFORE], cbs[CbPos.AFTER]\n        else:\n            return [], []\n\n\n    def register_function_callback(self, func_name: str, callback: AddrCallback) -> None:\n        \"\"\"\n        Register a callback on the address of the given function name.\n        The function name is only resolve when the callback manager is binded\n        to a SymbolicExecutor.\n\n        :param func_name: Function name\n        :type func_name: str\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.AddrCallback`\n        \"\"\"\n        if func_name in self._func_to_register:\n            self._func_to_register[func_name].append(callback)\n        else:\n            self._func_to_register[func_name] = [callback]\n\n\n    def register_instruction_callback(self, pos: CbPos, callback: InstrCallback) -> None:\n        \"\"\"\n        Register a callback triggered on each instruction executed, before or after its\n        side effects have been applied to ProcessState.\n\n        :param pos: before, after execution of the instruction\n        :type pos: CbPos\n        :param callback: callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.InstrCallback`\n        \"\"\"\n        self._instr_cbs[pos].append(callback)\n        self._empty = False\n\n\n    def register_pre_instruction_callback(self, callback: InstrCallback) -> None:\n        \"\"\"\n        Register a pre-execution callback on all instruction executed by the engine.\n\n        :param callback: callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.InstrCallback`\n        \"\"\"\n        self.register_instruction_callback(CbPos.BEFORE, callback)\n\n\n    def register_post_instruction_callback(self, callback: InstrCallback) -> None:\n        \"\"\"\n        Register a post-execution callback on all instruction executed by the engine.\n\n        :param callback: callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.InstrCallback`\n        \"\"\"\n        self.register_instruction_callback(CbPos.AFTER, callback)\n\n\n    def get_instruction_callbacks(self) -> Tuple[List[InstrCallback], List[InstrCallback]]:\n        \"\"\"\n        Get all the pre/post callbacks for instructions.\n\n        :return: tuple of lists containing callback functions for pre/post respectively\n        \"\"\"\n        return self._instr_cbs[CbPos.BEFORE], self._instr_cbs[CbPos.AFTER]\n\n\n    def register_pre_execution_callback(self, callback: SymExCallback) -> None:\n        \"\"\"\n        Register a callback executed after program loading, registers and memory\n        initialization. Thus this callback is called just before executing the\n        first instruction.\n\n        :param callback: Callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.SymExCallback`\n        \"\"\"\n        self._pre_exec.append(callback)\n        self._empty = False\n\n\n    def register_post_execution_callback(self, callback: SymExCallback) -> None:\n        \"\"\"\n        Register a callback executed after program loading, registers and memory\n        initialization. Thus this callback is called after executing upon program\n        exit (or crash)\n\n        :param callback: Callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.SymExCallback`\n        \"\"\"\n        self._post_exec.append(callback)\n        self._empty = False\n\n\n    def register_exploration_step_callback(self, callback: ExplorationStepCallback) -> None:\n        \"\"\"\n        Register a callback executed before each exploration step. The object\n        given in parameter is the SymbolicExplorator itself.\n\n        :param callback: Callback function to trigger\n        :type callback: :py:obj:`tritondse.callbacks.ExplorationStepCallback`\n        \"\"\"\n        self._step_cbs.append(callback)\n        # self._empty = False  # Does not impact the emptiness of the callbackmanager\n\n    def get_execution_callbacks(self) -> Tuple[List[SymExCallback], List[SymExCallback]]:\n        \"\"\"\n        Get all the pre/post callbacks for the current symbolic execution.\n\n        :return: tuple of lists containing callback functions for pre/post respectively\n        \"\"\"\n        return self._pre_exec, self._post_exec\n\n\n    def register_memory_read_callback(self, callback: MemReadCallback) -> None:\n        \"\"\"\n        Register a callback that will be triggered by any read in the concrete\n        memory of the process state.\n\n        :param callback: Callback function to be called\n        :type callback: :py:obj:`tritondse.callbacks.MemCallback`\n        \"\"\"\n        self._mem_read_cbs.append(callback)\n        self._empty = False\n\n\n    def register_memory_write_callback(self, callback: MemWriteCallback) -> None:\n        \"\"\"\n        Register a callback called on each write in the concrete memory state\n        of the process.\n\n        :param callback: Callback function to be called\n        :type callback: :py:obj:`tritondse.callbacks.MemCallback`\n        \"\"\"\n        self._mem_write_cbs.append(callback)\n        self._empty = False\n\n\n    def register_register_read_callback(self, callback: RegReadCallback) -> None:\n        \"\"\"\n        Register a callback on each register read during the symbolic execution.\n\n        :param callback: Callback function to be called\n        :type callback: :py:obj:`tritondse.callbacks.RegReadCallback`\n        \"\"\"\n        self._reg_read_cbs.append(callback)\n        self._empty = False\n\n\n    def register_register_write_callback(self, callback: RegWriteCallback) -> None:\n        \"\"\"\n        Register a callback on each register write during the symbolic execution.\n\n        :param callback: Callback function to be called\n        :type callback: :py:obj:`tritondse.callbacks.RegReadCallback`\n        \"\"\"\n        self._reg_write_cbs.append(callback)\n        self._empty = False\n\n\n    def register_thread_context_switch_callback(self, callback: ThreadCallback) -> None:\n        \"\"\"\n        Register a callback triggered upon each thread context switch during the execution.\n\n        :param callback: Callback to be called\n        :type callback: :py:obj:`tritondse.callbacks.ThreadCallback`\n        \"\"\"\n        self._ctx_switch.append(callback)\n        self._empty = False\n\n\n    def get_context_switch_callback(self) -> List[ThreadCallback]:\n        \"\"\"\n        Get the list of all function callback to call when thread is being scheduled.\n\n        :return: List of callbacks defined when thread is being scheduled\n        \"\"\"\n        return self._ctx_switch\n\n\n    def register_new_input_callback(self, callback: NewInputCallback) -> None:\n        \"\"\"\n        Register a callback function called when the SMT solver find a new model namely\n        a new input. This callback is called before any treatment on the input (worklist, etc.).\n        It thus allow to post-process the input before it getting put in the queue.\n\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.NewInputCallback`\n        \"\"\"\n        self._new_input_cbs.append(callback)\n        self._empty = False\n\n\n    def get_new_input_callback(self) -> List[NewInputCallback]:\n        \"\"\"\n        Get the list of all function callback to call when an a new\n        input is generated by SMT.\n\n        :return: List of callbacks to call on input generation\n        \"\"\"\n        return self._new_input_cbs\n\n    def register_on_solving_callback(self, callback: SymExSolvingCallback) -> None:\n        \"\"\"\n        Register a callback function called when a branch is about to\n        be solved. This callback is called before the branch is solved and will\n        use the result of the callback to go ahead with the solving or skip it.\n\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.BranchSolvingCallback`\n        \"\"\"\n        self._branch_solving_cbs.append(callback)\n        self._empty = False\n\n    def get_on_solving_callback(self) -> List[SymExSolvingCallback]:\n        \"\"\"\n        Get the list of all function callbacks to call when a branch is about\n        to be solved.\n\n        :return: List of callbacks to call on branch solving\n        \"\"\"\n        return self._branch_solving_cbs\n\n    def register_on_branch_covered_callback(self, callback: BranchCoveredCallback) -> None:\n        \"\"\"\n        Register a callback function called when a branch covered. This callback\n        is called after the branch is solved.\n\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.BranchCoveredCallback`\n        \"\"\"\n        self._branch_covered_cbs.append(callback)\n        self._empty = False\n\n\n    def get_on_branch_covered_callback(self) -> List[BranchCoveredCallback]:\n        \"\"\"\n        Get the list of all function callbacks to call when a branch is about\n        to be solved.\n\n        :return: List of callbacks to call on branch covered\n        \"\"\"\n        return self._branch_covered_cbs\n\n\n    def register_memory_violation_callback(self, callback: MemoryViolationCallback) -> None:\n        \"\"\"\n        Register a callback function called when a memory violation occured during the\n        emulation.\n\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.MemoryViolationCallback`\n        \"\"\"\n        self._mem_violation_cbs.append(callback)\n        self._empty = False\n\n    def get_memory_violation_callbacks(self) -> List[MemoryViolationCallback]:\n        \"\"\"\n        Get all memory violation callbacks.\n\n        :return: list of memory violation callbacks\n        \"\"\"\n        return self._mem_violation_cbs\n\n\n    def get_exploration_step_callbacks(self) -> List[ExplorationStepCallback]:\n        \"\"\"\n        Get all the exploration step callbacks\n\n        :return: list containing callbacks\n        \"\"\"\n        return self._step_cbs\n\n    def register_pre_imported_routine_callback(self, routine_name: str, callback: RtnCallback) -> None:\n        \"\"\"\n        Register a callback before call to an imported routines\n\n        :param routine_name: the routine name\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.RtnCallback`\n        \"\"\"\n        if routine_name in self._pre_rtn_cbs:\n            self._pre_rtn_cbs[routine_name].append(callback)\n        else:\n            self._pre_rtn_cbs[routine_name] = [callback]\n        self._empty = False\n\n\n    def register_post_imported_routine_callback(self, routine_name: str, callback: RtnCallback) -> None:\n        \"\"\"\n        Register a callback, called after the call to imported routines.\n\n        :param routine_name: the routine name\n        :param callback: callback function\n        :type callback: :py:obj:`tritondse.callbacks.RtnCallback`\n        \"\"\"\n        if routine_name in self._post_rtn_cbs:\n            self._post_rtn_cbs[routine_name].append(callback)\n        else:\n            self._post_rtn_cbs[routine_name] = [callback]\n        self._empty = False\n\n\n    def get_imported_routine_callbacks(self, routine_name: str) -> Tuple[List[RtnCallback], List[RtnCallback]]:\n        \"\"\"\n        Get the list of all callbacks for an imported routine\n\n        :param routine_name: the routine name\n        :return: Tuple of list of callbacks (for pre and post)\n        \"\"\"\n        pre_ret = (self._pre_rtn_cbs[routine_name] if routine_name in self._pre_rtn_cbs else [])\n        post_ret = (self._post_rtn_cbs[routine_name] if routine_name in self._post_rtn_cbs else [])\n        return pre_ret, post_ret\n\n\n    def register_probe(self, probe: ProbeInterface) -> None:\n        \"\"\"\n        Register a probe. That function will iterate the ``cbs`` attribute\n        of the object, and will register each entries in self.\n\n        .. warning:: Does not implement all CbType\n\n        :param probe: a probe interface\n        :type probe: ProbeInterface\n        \"\"\"\n        for kind, cb, arg in probe.callbacks:\n            try:\n                mapping_with_args = {\n                    CbType.PRE_RTN: self.register_pre_imported_routine_callback,\n                    CbType.POST_RTN: self.register_post_imported_routine_callback,\n                    CbType.PRE_ADDR: self.register_pre_addr_callback,\n                    CbType.POST_ADDR: self.register_post_addr_callback,\n                    CbType.PRE_MNEM: self.register_pre_mnemonic_callback,\n                    CbType.POST_MNEM: self.register_post_mnemonic_callback,\n                    CbType.PRE_OPCODE: self.register_pre_opcode_callback,\n                    CbType.POST_OPCODE: self.register_post_opcode_callback,\n                }\n                mapping_with_args[kind](arg, cb)\n            except KeyError:\n                mapping = {\n                    CbType.CTX_SWITCH: self.register_thread_context_switch_callback,\n                    CbType.MEMORY_READ: self.register_memory_read_callback,\n                    CbType.MEMORY_WRITE: self.register_memory_write_callback,\n                    CbType.POST_EXEC: self.register_post_execution_callback,\n                    CbType.POST_INST: self.register_post_instruction_callback,\n                    CbType.PRE_EXEC: self.register_pre_execution_callback,\n                    CbType.PRE_INST: self.register_pre_instruction_callback,\n                    CbType.REG_READ: self.register_register_read_callback,\n                    CbType.REG_WRITE: self.register_register_write_callback,\n                    CbType.NEW_INPUT: self.register_new_input_callback,\n                    CbType.EXPLORE_STEP: self.register_exploration_step_callback,\n                    CbType.BRANCH_COV: self.register_on_branch_covered_callback,\n                    CbType.SYMEX_SOLVING: self.register_on_solving_callback,\n                    CbType.MEM_VIOLATION: self.register_memory_violation_callback\n                }\n                mapping[kind](cb)\n\n\n    def fork(self) -> 'CallbackManager':\n        \"\"\"\n        Fork the current CallbackManager in a new object instance\n        (that will be unbinded). That method is used by the SymbolicExplorator\n        to ensure each SymbolicExecutor running concurrently will have\n        their own instance off the CallbackManager.\n\n        :return: Fresh instance of CallbackManager\n        :rtype: CallbackManager\n        \"\"\"\n        cbs = CallbackManager()\n\n        # SymbolicExecutor callbacks\n        cbs._pc_addr_cbs        = self._pc_addr_cbs.copy()\n        cbs._opcode_cbs         = self._opcode_cbs.copy()\n        cbs._mnemonic_cbs       = self._mnemonic_cbs.copy()\n        cbs._instr_cbs          = self._instr_cbs.copy()\n        cbs._pre_exec           = self._pre_exec.copy()\n        cbs._post_exec          = self._post_exec.copy()\n        cbs._ctx_switch         = self._ctx_switch.copy()\n        cbs._new_input_cbs      = self._new_input_cbs.copy()\n        cbs._branch_solving_cbs = self._branch_solving_cbs.copy()\n        cbs._branch_covered_cbs = self._branch_covered_cbs.copy()\n        cbs._pre_rtn_cbs        = self._pre_rtn_cbs.copy()\n        cbs._post_rtn_cbs       = self._post_rtn_cbs.copy()\n        cbs._mem_violation_cbs  = self._mem_violation_cbs.copy()\n        # Triton callbacks\n        cbs._mem_read_cbs  = self._mem_read_cbs.copy()\n        cbs._mem_write_cbs = self._mem_write_cbs.copy()\n        cbs._reg_read_cbs  = self._reg_read_cbs.copy()\n        cbs._reg_write_cbs = self._reg_write_cbs.copy()\n        cbs._empty         = self._empty\n        # Copy temporary data\n        cbs._func_to_register = self._func_to_register.copy()\n\n        return cbs\n\n    def unregister_callback(self, callback: Callable) -> None:\n        \"\"\"\n        Unregister the given callback from the manager.\n\n        :param callback: callback to remove\n        :return: None\n        \"\"\"\n        for addr, itms in self._pc_addr_cbs.items():\n            for loc in CbPos:\n                if callback in itms[loc]:\n                    itms[loc].remove(callback)\n\n        for opcode, itms in self._opcode_cbs.items():\n            for loc in CbPos:\n                if callback in itms[loc]:\n                    itms[loc].remove(callback)\n\n        for mnemonic, itms in self._mnemonic_cbs.items():\n            for loc in CbPos:\n                if callback in itms[loc]:\n                    itms[loc].remove(callback)\n\n        for loc in CbPos:\n            if callback in self._instr_cbs[loc]:\n                self._instr_cbs[loc].remove(callback)\n\n        for cb_list in [self._step_cbs, self._pre_exec, self._post_exec, self._ctx_switch, self._new_input_cbs, self._branch_solving_cbs, self._branch_covered_cbs,\n                        self._mem_read_cbs, self._mem_write_cbs, self._reg_read_cbs, self._reg_write_cbs, self._mem_violation_cbs]:\n            if callback in cb_list:\n                cb_list.remove(callback)\n\n        for d in [self._pre_rtn_cbs, self._post_rtn_cbs]:\n            for cb_list in d.values():\n                if callback in cb_list:\n                    cb_list.remove(callback)\n\n    def reset(self) -> None:\n        \"\"\"\n        Reset all callbacks\n        :return:\n        \"\"\"\n        # SymbolicExplorator callbacks\n        self._step_cbs = []  # Callback called between each exploration steps\n\n        # SymbolicExecutor callbacks\n        self._pc_addr_cbs        = {}  # addresses reached\n        self._opcode_cbs         = {}  # opcode before and after\n        self._mnemonic_cbs       = {}  # mnemonic before and after\n        self._instr_cbs          = {CbPos.BEFORE: [], CbPos.AFTER: []}  # all instructions\n        self._pre_exec           = []  # before execution\n        self._post_exec          = []  # after execution\n        self._ctx_switch         = []  # on each thread context switch (implementing pre/post?)\n        self._new_input_cbs      = []  # each time an SMT model is get\n        self._branch_solving_cbs = []  # each time a covitem is about to be solved\n        self._branch_covered_cbs = []  # each time a covitem is covered\n        self._pre_rtn_cbs        = {}  # before imported routine calls ({str: [RtnCallback]})\n        self._post_rtn_cbs       = {}  # after imported routine calls ({str: [RtnCallback]})\n        self._mem_violation_cbs  = []  # reset the memory violation calls\n\n        # Triton callbacks\n        self._mem_read_cbs  = []  # memory reads\n        self._mem_write_cbs = []  # memory writes\n        self._reg_read_cbs  = []  # register reads\n        self._reg_write_cbs = []  # register writes\n        self._empty         = True", ""]}
{"filename": "tritondse/logging.py", "chunked_list": ["import logging\n\n'''\nLoggers hierarchy is the following:\n\n- tritondse.\n    - \n\n'''\n", "'''\n\n\nlogger = logging.getLogger('tritondse')\nlogger.propagate = False  # Do not propagate logs by default\ncolor_enabled = True\n_loggers = {}\n\ndef get(name: str = \"\") -> logging.Logger:\n    \"\"\"\n    Get a child logger from the tritondse one.\n    If name is empty returns the root tritondse\n    logger.\n\n    :param name: logger name\n    \"\"\"\n    log = logger.getChild(name) if name else logger\n\n    if log.name not in _loggers:\n        log.propagate = False  # first time it is retrieve disable propagation\n        _loggers[log.name] = log\n\n    return log", "def get(name: str = \"\") -> logging.Logger:\n    \"\"\"\n    Get a child logger from the tritondse one.\n    If name is empty returns the root tritondse\n    logger.\n\n    :param name: logger name\n    \"\"\"\n    log = logger.getChild(name) if name else logger\n\n    if log.name not in _loggers:\n        log.propagate = False  # first time it is retrieve disable propagation\n        _loggers[log.name] = log\n\n    return log", "\n\ndef enable(level: int = logging.INFO, name: str = \"\") -> None:\n    \"\"\"\n    Enable tritondse logging to terminal output\n\n    :param level: logging level\n    :param name: name of the logger to enable (all by default)\n    \"\"\"\n    log = get(name)\n    log.propagate = True\n    log.setLevel(level)\n\n    # Enable root logger if needed\n    if log.name != \"tritondse\":\n        logger.propagate = True\n    else:\n        for sub_logger in _loggers.values():\n            sub_logger.propagate = True", "\n\ndef enable_to_file(level: int, file: str, name: str = \"\") -> None:\n    \"\"\"\n    Enable tritondse logging to a file\n\n    :param level: logging level\n    :param file: path to log file\n    :param name: name of the logger to enable to a file\n    \"\"\"\n    log = get(name)\n    log.setLevel(level)\n    fmt = logging.Formatter(\"%(asctime)s %(threadName)s [%(levelname)s] %(message)s\")\n\n    handler = logging.FileHandler(file)\n    handler.setFormatter(fmt)\n    log.addHandler(handler)  # Add the handler to the logger"]}
{"filename": "tritondse/probes/basic_trace.py", "chunked_list": ["from tritondse import ProbeInterface, CbType, SymbolicExecutor, ProcessState, SymbolicExplorator\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"probe.basictrace\")\n\n\nclass BasicDebugTrace(ProbeInterface):\n    \"\"\"\n    Basic probe that print instruction trace\n    to logging debug.\n    \"\"\"\n    NAME = \"debugtrace-probe\"\n\n    def __init__(self):\n        super(BasicDebugTrace, self).__init__()\n        self._add_callback(CbType.PRE_INST, self.trace_debug)\n\n    def trace_debug(self, exec: SymbolicExecutor, __: ProcessState, ins: 'Instruction'):\n        logger.debug(f\"[tid:{ins.getThreadId()}] {exec.trace_offset} [0x{ins.getAddress():x}]: {ins.getDisassembly()}\")", "\n\n\nclass BasicTextTrace(ProbeInterface):\n    \"\"\"\n    Basic probe that generate a txt execution trace\n    for each run.\n    \"\"\"\n    NAME = \"txttrace-probe\"\n\n    def __init__(self):\n        super(BasicTextTrace, self).__init__()\n        self._add_callback(CbType.PRE_EXEC, self.pre_execution)\n        self._add_callback(CbType.POST_EXEC, self.post_execution)\n        self._add_callback(CbType.PRE_INST, self.trace_debug)\n\n        # File in which to write the trace\n        self._file = None\n\n    def pre_execution(self, executor: SymbolicExecutor, _: ProcessState):\n        # Triggered before each execution\n        name = f\"{executor.uid}-{executor.seed.hash}.txt\"\n        file = executor.workspace.get_metadata_file_path(f\"{self.NAME}/{name}\")\n        self._file = open(file, \"w\")\n\n    def post_execution(self, executor: SymbolicExecutor, _: ProcessState):\n        self._file.close()\n\n    def trace_debug(self, exec: SymbolicExecutor, __: ProcessState, ins: 'Instruction'):\n        \"\"\"\n        This function is mainly used for debug.\n\n        :param _: The current symbolic executor\n        :param __: The current processus state of the execution\n        :param ins: The current instruction executed\n        :return: None\n        \"\"\"\n        self._file.write(f\"[tid:{ins.getThreadId()}] {exec.trace_offset} [0x{ins.getAddress():x}]: {ins.getDisassembly()}\\n\")", ""]}
{"filename": "tritondse/probes/__init__.py", "chunked_list": [""]}
{"filename": "tritondse/loaders/quokkaprogram.py", "chunked_list": ["# Built-in imports\nfrom pathlib import Path\nfrom typing import Union, Generator, Tuple, Optional, Any, List\n\n# third-party imports\nimport quokka\nimport networkx\nimport lief\n\n# local imports", "\n# local imports\nfrom tritondse.loaders import Program, LoadableSegment\nfrom tritondse.coverage import CoverageSingleRun\nfrom tritondse.types import PathLike, Addr, Architecture, Platform, Endian\n\n\nclass QuokkaProgram(quokka.Program):\n    def __init__(self, export_file: Union[Path, str], exec_path: Union[Path, str]):\n        super(QuokkaProgram, self).__init__(export_file, exec_path)\n\n        self.program = Program(self.executable.exec_file.as_posix())\n\n    def get_call_graph(self, backedge_on_ret=False) -> networkx.DiGraph:\n        \"\"\"\n        Compute the call graph of the program.\n\n        :param backedge_on_ret: if true, add a back edge to represent the \"return\"\n        :return: call graph as a digraph\n        \"\"\"\n        g = networkx.DiGraph()\n        for fun in self.values():\n            g.add_edges_from((fun.start, x.start) for x in fun.calls)\n            if backedge_on_ret:  # Add return edge\n                g.add_edges_from((x.start, fun.start) for x in fun.calls)\n        return g\n\n    @staticmethod\n    def get_slice(graph: networkx.DiGraph, frm: Any, to: Any) -> networkx.DiGraph:\n        \"\"\"\n        Compute the slice between the two nodes on the given graph.\n        The slice is the intersection of reachable node (of ``frm``) and\n        ancestors of ``to``. The result is a subgraph of the original graph.\n\n        :param graph: Graph on which to compute the slice\n        :param frm: node identifier\n        :param to: node identifier\n        :return: sub graph\n        \"\"\"\n        succs = networkx.descendants(graph, frm)\n        preds = networkx.ancestors(graph, to)\n        return graph.subgraph(succs.intersection(preds).union({frm, to}))\n\n    def merge(self, coverage: CoverageSingleRun):\n        # TODO: To implement\n        raise NotImplementedError\n\n    def __repr__(self):\n        return f\"<{self.export_file.name}  funs:{len(self)}>\"\n\n    def get_caller_instructions(self, target: quokka.Function) -> List[int]:\n        \"\"\"Get the list of instructions calling `target`\n        \"\"\"\n\n        # Get the first instruction of the target\n        first_inst = target.get_instruction(target.start)\n        assert first_inst is not None\n\n        # Reference holder\n        ref = target.program.references\n\n        caller_instructions = []\n        for reference in ref.resolve_inst_instance(first_inst.inst_tuple, quokka.types.ReferenceType.CALL, towards=True):\n            _, block, offset = reference.source\n            inst = list(block.instructions)[offset]\n            caller_instructions.append(inst.address)\n        return caller_instructions\n\n\n    # ============== Methods for interroperability with Program object ==============\n    @property\n    def path(self) -> Path:\n        return self.program.path\n\n    @path.setter\n    def path(self, path: Path) -> None:\n        self.program.path = path\n\n    @property\n    def entry_point(self) -> Addr:\n        return self.program.entry_point\n\n    @property\n    def architecture(self) -> Architecture:\n        return self.program.architecture\n\n    @property\n    def platform(self) -> Platform:\n        return self.program.platform\n\n    @property\n    def endianness(self) -> Endian:\n        return self.program.endianness\n\n    @property\n    def format(self) -> lief.EXE_FORMATS:\n        return self.program.format\n\n    @property\n    def relocation_enum(self):\n        return self.program.relocation_enum\n\n    def memory_segments(self) -> Generator[LoadableSegment, None, None]:\n        return self.program.memory_segments()\n\n    def imported_functions_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        return self.program.imported_functions_relocations()\n\n    def imported_variable_symbols_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        return self.program.imported_variable_symbols_relocations()\n\n    def find_function_addr(self, name: str) -> Optional[Addr]:\n        return self.program.find_function_addr(name)\n\n    def find_function_from_addr(self, address: Addr) -> Optional[quokka.function.Function]:\n        for f in self.values():\n            if f.in_func(address):\n                return f", ""]}
{"filename": "tritondse/loaders/cle_loader.py", "chunked_list": ["from typing import Generator, Optional, Tuple\nfrom pathlib import Path\nimport logging\n\n# Third-party imports\nimport cle\n\n# Local imports\nfrom tritondse.loaders import Loader, LoadableSegment\nfrom tritondse.types import Addr, Architecture, PathLike, Platform, Perm, Endian", "from tritondse.loaders import Loader, LoadableSegment\nfrom tritondse.types import Addr, Architecture, PathLike, Platform, Perm, Endian\nfrom tritondse.routines import SUPPORTED_ROUTINES\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"loader\")\n\n_arch_mapper = {\n    \"ARMEL\":   Architecture.ARM32,\n    \"AARCH64\": Architecture.AARCH64,", "    \"ARMEL\":   Architecture.ARM32,\n    \"AARCH64\": Architecture.AARCH64,\n    \"AMD64\":   Architecture.X86_64,\n    \"X86\":   Architecture.X86,\n}\n\n_plfm_mapper = {\n    \"UNIX - System V\": Platform.LINUX,\n    \"windows\": Platform.WINDOWS,\n    \"macos\": Platform.MACOS", "    \"windows\": Platform.WINDOWS,\n    \"macos\": Platform.MACOS\n}\n\n\nclass CleLoader(Loader):\n    EXTERN_SYM_BASE = 0x0f001000\n    EXTERN_SYM_SIZE = 0x1000\n\n    BASE_STACK = 0xf0000000\n    END_STACK = 0x70000000  # This is inclusive\n\n    def __init__(self, path: PathLike, ld_path: Optional[PathLike] = None):\n        super(CleLoader, self).__init__(path)\n        self.path: Path = Path(path)  #: Binary file path\n        if not self.path.is_file():\n            raise FileNotFoundError(f\"file {path} not found (or not a file)\")\n\n        self._disable_vex_loggers()  # disable logs of pyvex\n\n        self.ld_path = ld_path if ld_path is not None else ()\n        self.ld = cle.Loader(str(path), ld_path=self.ld_path)\n\n    def _disable_vex_loggers(self):\n        for name, logger in logging.root.manager.loggerDict.items():\n            if \"pyvex\" in name:\n                logger.propagate = False\n\n    @property\n    def name(self) -> str:\n        \"\"\" Name of the loader\"\"\"\n        return f\"CleLoader({self.path})\"\n\n    @property\n    def architecture(self) -> Architecture:\n        \"\"\"\n        Architecture enum representing program architecture.\n\n        :rtype: Architecture\n        \"\"\"\n        return _arch_mapper[self.ld.main_object.arch.name]\n\n    @property\n    def endianness(self) -> Endian:\n        # FIXME: Depending on architecture returning good endianess\n        return Endian.LITTLE\n\n    @property\n    def entry_point(self) -> Addr:\n        \"\"\"\n        Program entrypoint address as defined in the binary headers\n\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n        return self.ld.main_object.entry\n\n    def memory_segments(self) -> Generator[LoadableSegment, None, None]:\n        \"\"\"\n        :return: Generator of tuples addrs and content\n        \"\"\"\n        for obj in self.ld.all_objects:\n            logger.debug(obj)\n            for seg in obj.segments:\n                segdata = self.ld.memory.load(seg.vaddr, seg.memsize)\n                assert len(segdata) == seg.memsize\n                perms = (Perm.R if seg.is_readable else 0) | (Perm.W if seg.is_writable else 0) | (Perm.X if seg.is_executable else 0) \n                if seg.__class__.__name__ != \"ExternSegment\":\n                    # The format string in CLE is broken if the filesize is 0. This is a workaround.\n                    logger.debug(f\"Loading segment {seg} - perms:{perms}\")\n                yield LoadableSegment(seg.vaddr, perms, content=segdata, name=f\"seg-{obj.binary_basename}\")\n        # Also return a specific map to put external symbols\n        yield LoadableSegment(self.EXTERN_SYM_BASE, self.EXTERN_SYM_SIZE, Perm.R | Perm.W, name=\"[extern]\")\n        yield LoadableSegment(self.END_STACK, self.BASE_STACK-self.END_STACK+1, Perm.R | Perm.W, name=\"[stack]\")\n\n        # FIXME. Temporary solution to prevent crashes on access to the TLB e.g fs:28\n        yield LoadableSegment(0, 0x2000, Perm.R | Perm.W, name=\"[fs]\")\n\n    # FIXME. Temporary solution to prevent crashes on access to the TLB e.g fs:28\n    @property\n    def cpustate(self):\n        # NOTE: in Triton, the segment selector is used as the segment base and not as a selector into GDT.\n        # i.e directly store the segment base into fs\n        return {\"fs\": 0x1000}\n\n    @property\n    def platform(self) -> Optional[Platform]:\n        \"\"\"\n        Platform of the binary.\n\n        :return: Platform\n        \"\"\"\n        return _plfm_mapper[self.ld.main_object.os]\n\n    def imported_functions_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported functions by the program. This function\n        is a generator of tuples associating the function and its relocation\n        address in the binary.\n\n        :return: Generator of tuples function name and relocation address\n        \"\"\"\n        # TODO I think there's a problem here. We only deal with imports from the main binary\n        # For example if a library calls a libc function, we probably need to patch the library's GOT\n        for obj in self.ld.all_objects:\n            for fun in obj.imports:\n                rtn_name = f\"rtn_{fun}\"\n                if fun in SUPPORTED_ROUTINES:\n                    reloc = obj.imports[fun]\n                    got_entry_addr = reloc.relative_addr + obj.mapped_base\n                    yield fun, got_entry_addr\n\n        # Handle indirect functions.\n        # Currently we only support indirect functions if there exists a stub for them in `routines.py`\n        # Otherwise the program will crash because CLE doesn't perform the relocation for indirect functions.\n        \n        # We could perform the relocation ourself by writing to the got slot but we need a way to figure out \n        # the correct fptr to use.\n        # In other words we should execute `resolver_fun` or parse it in some way to get the correct function ptr\n        # to write to got_slot (write with self.ld.memory.pack_word(got_slot, func_ptr))\n        for obj in self.ld.all_objects:\n            for (resolver_func, got_rva) in obj.irelatives:\n                got_slot = got_rva + obj.mapped_base\n                sym = self.ld.find_symbol(resolver_func)\n                if sym is None:\n                    continue\n                fun = sym.name\n                if fun in SUPPORTED_ROUTINES:\n                    yield fun, got_slot\n\n    def imported_variable_symbols_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported variable symbols. Yield for each of them the name and\n        the relocation address in the binary.\n\n        :return: Generator of tuples with symbol name, relocation address\n        \"\"\"\n        # TODO I think there's a problem here. We only deal with imports from the main binary\n        for s in self.ld.main_object.symbols:\n            if s.resolved and s._type == cle.SymbolType.TYPE_OBJECT:\n                logger.debug(f\"CleLoader: hooking symbol {s.name} @ {s.relative_addr:#x} {s.resolved} {s.resolvedby} {s._type}\")\n                s_addr = s.relative_addr + self.ld.main_object.mapped_base\n                yield s.name, s_addr\n\n    def find_function_addr(self, name: str) -> Optional[Addr]:\n        \"\"\"\n        Search for the function name in fonctions of the binary.\n\n        :param name: Function name\n        :type name: str\n        :return: Address of function if found\n        :rtype: Addr\n        \"\"\"\n        res = [x for x in self.ld.find_all_symbols(name) if x.is_function]\n        return res[0].rebased_addr if res else None  # if multiple elements return the first", ""]}
{"filename": "tritondse/loaders/loader.py", "chunked_list": ["from __future__ import annotations\n\n# built-in imports\nfrom pathlib import Path\nfrom typing import Optional, Generator, Tuple, Dict, List\nfrom dataclasses import dataclass\n\n# local imports\nfrom tritondse.types import Addr, Architecture, Platform, ArchMode, Perm, Endian\nfrom tritondse.arch import ARCHS", "from tritondse.types import Addr, Architecture, Platform, ArchMode, Perm, Endian\nfrom tritondse.arch import ARCHS\nimport tritondse.logging\n\nlogger = tritondse.logging.get()\n\n\n@dataclass\nclass LoadableSegment:\n    \"\"\" Represent a Segment to load in memory.\n    It can either provide a content and will thus be\n    initialized in a context or virtual.\n    \"\"\"\n    address: int\n    \"\"\" Virtual address where to load the segment \"\"\"\n    size: int = 0\n    \"\"\" Size of the segment. If content is present use len(content)\"\"\"\n    perms: Perm = Perm.R|Perm.W|Perm.X\n    \"\"\" Permissions to assign the segment \"\"\"\n    content: Optional[bytes] = None\n    \"\"\" Content of the segment \"\"\"\n    name: str = \"\"\n    \"\"\" Name to give to the segment \"\"\"", "class LoadableSegment:\n    \"\"\" Represent a Segment to load in memory.\n    It can either provide a content and will thus be\n    initialized in a context or virtual.\n    \"\"\"\n    address: int\n    \"\"\" Virtual address where to load the segment \"\"\"\n    size: int = 0\n    \"\"\" Size of the segment. If content is present use len(content)\"\"\"\n    perms: Perm = Perm.R|Perm.W|Perm.X\n    \"\"\" Permissions to assign the segment \"\"\"\n    content: Optional[bytes] = None\n    \"\"\" Content of the segment \"\"\"\n    name: str = \"\"\n    \"\"\" Name to give to the segment \"\"\"", "\n\nclass Loader(object):\n    \"\"\"\n    This class describes how to load the target program in memory.\n    \"\"\"\n    def __init__(self, path: str):\n        self.bin_path = Path(path)\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        Name of the loader and target being loaded.\n\n        :return: str of the loader name\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def entry_point(self) -> Addr:\n        \"\"\"\n        Program entrypoint address as defined in the binary headers\n\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def architecture(self) -> Architecture:\n        \"\"\"\n        Architecture enum representing program architecture.\n\n        :rtype: Architecture\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def arch_mode(self) -> Optional[ArchMode]:\n        \"\"\"\n        ArchMode enum representing the starting mode (e.g Thumb for ARM).\n        if None, the default mode of the architecture will be used.\n\n        :rtype: Optional[ArchMode]\n        \"\"\"\n        return None\n\n    @property\n    def platform(self) -> Optional[Platform]:\n        \"\"\"\n        Platform of the binary.\n\n        :return: Platform\n        \"\"\"\n        return None\n\n    @property\n    def endianness(self) -> Endian:\n        \"\"\"\n        Endianess of the loaded program\n\n        :return: Endianness\n        \"\"\"\n        raise NotImplementedError()\n\n    def memory_segments(self) -> Generator[LoadableSegment, None, None]:\n        \"\"\"\n        Iterate over all memory segments of the program as loaded in memory.\n\n        :return: Generator of tuples addrs and content\n        :raise NotImplementedError: if the binary format cannot be loaded\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def cpustate(self) -> Dict[str, int]:\n        \"\"\"\n        Provide the initial cpu state in the forma of a dictionary of\n        {\"register_name\" : register_value}\n        \"\"\"\n        return {}\n\n    def imported_functions_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported functions by the program. This function\n        is a generator of tuples associating the function and its relocation\n        address in the binary.\n\n        :return: Generator of tuples function name and relocation address\n        \"\"\"\n        yield from ()\n\n    def imported_variable_symbols_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported variable symbols. Yield for each of them the name and\n        the relocation address in the binary.\n\n        :return: Generator of tuples with symbol name, relocation address\n        \"\"\"\n        yield from ()\n\n    def find_function_addr(self, name: str) -> Optional[Addr]:\n        \"\"\"\n        Search for the function name in fonctions of the binary.\n\n        :param name: Function name\n        :type name: str\n        :return: Address of function if found\n        :rtype: Addr\n        \"\"\"\n        return None", "\n\nclass MonolithicLoader(Loader):\n    \"\"\"\n    Monolithic loader. It helps loading raw firmware at a given address\n    in DSE memory space, with the various attributes like architecture etc.\n    \"\"\"\n\n    def __init__(self,\n                 architecture: Architecture,\n                 cpustate: Dict[str, int] = None,\n                 maps: List[LoadableSegment] = None,\n                 set_thumb: bool = False,\n                 platform: Platform = None,\n                 endianess: Endian = Endian.LITTLE):\n        super(MonolithicLoader, self).__init__(\"\")\n\n        self._architecture = architecture\n        self._platform = platform if platform else None\n        self._cpustate = cpustate if cpustate else {}\n        self.maps = maps\n        self._arch_mode = ArchMode.THUMB if set_thumb else None\n        self._endian = endianess\n        if self._platform and (self._architecture, self._platform) in ARCHS:\n            self._archinfo = ARCHS[(self._architecture, self._platform)]\n        elif self._architecture in ARCHS:\n            self._archinfo = ARCHS[self._architecture]\n        else: \n            logger.error(\"Unknown architecture\")\n            assert False\n\n    @property\n    def name(self) -> str:\n        \"\"\" Name of the loader\"\"\"\n        return f\"Monolithic({self.bin_path})\"\n\n    @property\n    def architecture(self) -> Architecture:\n        \"\"\"\n        Architecture enum representing program architecture.\n\n        :rtype: Architecture\n        \"\"\"\n        return self._architecture\n\n    @property\n    def arch_mode(self) -> ArchMode:\n        \"\"\"\n        ArchMode enum representing the starting mode (e.g Thumb for ARM).\n\n        :rtype: ArchMode\n        \"\"\"\n        return self._arch_mode\n\n    @property\n    def entry_point(self) -> Addr:\n        \"\"\"\n        Program entrypoint address as defined in the binary headers\n\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n        return self.cpustate[self._archinfo.pc_reg]\n\n    def memory_segments(self) -> Generator[LoadableSegment, None, None]:\n        \"\"\"\n        In the case of a monolithic firmware, there is a single segment.\n        The generator returns a single tuple with the load address and the content.\n\n        :return: Generator of tuples addrs and content\n        \"\"\"\n        yield from self.maps\n\n    @property\n    def cpustate(self) -> Dict[str, int]:\n        \"\"\"\n        Provide the initial cpu state in the format of a dictionary of\n        {\"register_name\" : register_value}\n        \"\"\"\n        return self._cpustate\n\n    @property\n    def platform(self) -> Optional[Platform]:\n        \"\"\"\n        Platform of the binary.\n\n        :return: Platform\n        \"\"\"\n        return self._platform\n\n    @property\n    def endianness(self) -> Endian:\n        \"\"\"\n        Endianess of the monolithic loader.\n        (default is LITTLE)\n        \"\"\"\n        return self._endian", ""]}
{"filename": "tritondse/loaders/program.py", "chunked_list": ["from __future__ import annotations\n\n# built-in imports\nfrom pathlib import Path\nfrom typing import Optional, Generator, Tuple\nfrom collections import namedtuple\n\n# third party\nimport lief\n", "import lief\n\n# local imports\nfrom tritondse.types import PathLike, Addr, Architecture, Platform, ArchMode, Perm, Endian\nfrom tritondse.loaders import Loader, LoadableSegment\nimport tritondse.logging\n\nlogger = tritondse.logging.get(\"loader\")\n\n_arch_mapper = {", "\n_arch_mapper = {\n    lief.ARCHITECTURES.ARM:   Architecture.ARM32,\n    lief.ARCHITECTURES.ARM64: Architecture.AARCH64,\n    lief.ARCHITECTURES.X86:   Architecture.X86\n}\n\n_plfm_mapper = {\n    lief.EXE_FORMATS.ELF: Platform.LINUX,\n    lief.EXE_FORMATS.PE: Platform.WINDOWS,", "    lief.EXE_FORMATS.ELF: Platform.LINUX,\n    lief.EXE_FORMATS.PE: Platform.WINDOWS,\n    lief.EXE_FORMATS.MACHO: Platform.MACOS\n}\n\n\nclass Program(Loader):\n    \"\"\"\n    Representation of a program (loaded in memory). This class is wrapping\n    `LIEF <https://lief.quarkslab.com/doc/latest>`_ to represent a program\n    and to provide all the features allowing to pseudo-load one regardless\n    of its format.\n    \"\"\"\n\n    EXTERN_SYM_BASE = 0x0f001000\n    EXTERN_SYM_SIZE = 0x1000\n\n    BASE_STACK = 0xf0000000\n    END_STACK  = 0x70000000 # This is inclusive\n\n    def __init__(self, path: PathLike):\n        \"\"\"\n        :param path: Program path\n        :type path: :py:obj:`tritondse.types.PathLike`\n        :raise FileNotFoundError: if the file is not properly recognized by lief\n                                  or in the wrong architecture\n        \"\"\"\n        super(Program, self).__init__(path)\n        self.path: Path = Path(path)  #: Binary file path\n        if not self.path.is_file():\n            raise FileNotFoundError(f\"file {path} not found (or not a file)\")\n\n        self._binary = lief.parse(str(self.path))\n        if self._binary is None:  # lief has not been able to parse it\n            raise FileNotFoundError(f\"file {path} not recognised by lief\")\n\n        self._arch = self._load_arch()\n        if self._arch is None:\n            raise FileNotFoundError(f\"binary {path} architecture unsupported {self._binary.abstract.header.architecture}\")\n\n        try:\n            self._plfm = _plfm_mapper[self._binary.format]\n            # TODO: better refine for Android, iOS etc.\n        except KeyError:\n            self._plfm = None\n\n        self._funs = {f.name: f for f in self._binary.concrete.functions}\n\n    @property\n    def name(self) -> str:\n        \"\"\" Name of the loader\"\"\"\n        return f\"Program({self.path})\"\n\n    @property\n    def endianness(self) -> Endian:\n        return {lief.ENDIANNESS.LITTLE: Endian.LITTLE,\n                lief.ENDIANNESS.BIG: Endian.BIG}[self._binary.abstract.header.endianness]\n\n    @property\n    def entry_point(self) -> Addr:\n        \"\"\"\n        Program entrypoint address as defined in the binary headers\n\n        :rtype: :py:obj:`tritondse.types.Addr`\n        \"\"\"\n        return self._binary.entrypoint\n\n    @property\n    def architecture(self) -> Architecture:\n        \"\"\"\n        Architecture enum representing program architecture.\n\n        :rtype: Architecture\n        \"\"\"\n        return self._arch\n\n    @property\n    def platform(self) -> Optional[Platform]:\n        \"\"\"\n        Platform of the binary. Its solely based on the format\n        of the file ELF, PE etc..\n\n        :return: Platform\n        \"\"\"\n        return self._plfm\n\n    @property\n    def format(self) -> lief.EXE_FORMATS:\n        \"\"\"\n        Binary format. Supported formats by lief are: ELF, PE, MachO\n\n        :rtype: lief.EXE_FORMATS\n        \"\"\"\n        return self._binary.format\n\n    def _load_arch(self) -> Optional[Architecture]:\n        \"\"\"\n        Load architecture as an Architecture object.\n\n        :return: Architecture or None if unsupported\n        \"\"\"\n        arch = self._binary.abstract.header.architecture\n        if arch in _arch_mapper:\n            arch = _arch_mapper[arch]\n            if arch == Architecture.X86:\n                arch = Architecture.X86 if self._binary.abstract.header.is_32 else Architecture.X86_64\n            return arch\n        else:\n            return None\n\n    @property\n    def relocation_enum(self):\n        \"\"\"\n        LIEF relocation enum associated with the current\n        architecture of the binary.\n\n        :return: LIEF relocation enum\n        :rtype: Union[lief.ELF.RELOCATION_AARCH64,\n                      lief.ELF.RELOCATION_ARM,\n                      lief.ELF.RELOCATION_PPC64,\n                      lief.ELF.RELOCATION_PPC,\n                      lief.ELF.RELOCATION_i386,\n                      lief.ELF.RELOCATION_X86_64]\n        \"\"\"\n        rel_map = {\n            lief.ELF.ARCH.AARCH64: lief.ELF.RELOCATION_AARCH64,\n            lief.ELF.ARCH.ARM:     lief.ELF.RELOCATION_ARM,\n            lief.ELF.ARCH.PPC64:   lief.ELF.RELOCATION_PPC64,\n            lief.ELF.ARCH.PPC:     lief.ELF.RELOCATION_PPC,\n            lief.ELF.ARCH.i386:    lief.ELF.RELOCATION_i386,\n            lief.ELF.ARCH.x86_64:  lief.ELF.RELOCATION_X86_64\n        }\n        return rel_map[self._binary.concrete.header.machine_type]\n\n    def _is_glob_dat(self, rel: lief.ELF.Relocation) -> bool:\n        \"\"\" Get whether the given relocation is of type GLOB_DAT.\n        Used locally to find mandatory relocations\n        \"\"\"\n        rel_enum = self.relocation_enum\n        if hasattr(rel_enum, \"GLOB_DAT\"):\n            return rel_enum(rel.type) == getattr(rel_enum, \"GLOB_DAT\")\n        else:\n            return False  # Not GLOB_DAT relocation for this architecture\n\n    def memory_segments(self) -> Generator[LoadableSegment, None, None]:\n        \"\"\"\n        Iterate over all memory segments of the program as loaded in memory.\n\n        :return: Generator of tuples addrs and content\n        :raise NotImplementedError: if the binary format cannot be loaded\n        \"\"\"\n        if self.format == lief.EXE_FORMATS.ELF:\n            for i, seg in enumerate(self._binary.concrete.segments):\n                if seg.type == lief.ELF.SEGMENT_TYPES.LOAD:\n                    content = bytearray(seg.content)\n                    if seg.virtual_size != len(seg.content):  # pad with zeros (as it might be .bss)\n                        content += bytearray([0]) * (seg.virtual_size - seg.physical_size)\n                    yield LoadableSegment(seg.virtual_address, perms=Perm(int(seg.flags)), content=bytes(content), name=f\"seg{i}\")\n        else:\n            raise NotImplementedError(f\"memory segments not implemented for: {self.format.name}\")\n\n        # Also return a specific map to put external symbols\n        yield LoadableSegment(self.EXTERN_SYM_BASE, self.EXTERN_SYM_SIZE, Perm.R | Perm.W, name=\"[extern]\")\n        yield LoadableSegment(self.END_STACK, self.BASE_STACK-self.END_STACK, Perm.R | Perm.W, name=\"[stack]\")\n\n    def imported_functions_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported functions by the program. This function\n        is a generator of tuples associating the function and its relocation\n        address in the binary.\n\n        :return: Generator of tuples function name and relocation address\n        \"\"\"\n        if self.format == lief.EXE_FORMATS.ELF:\n            try:\n                # Iterate functions imported through PLT\n                for rel in self._binary.concrete.pltgot_relocations:\n                    yield rel.symbol.name, rel.address\n\n                # Iterate functions imported via mandatory relocation (e.g: __libc_start_main)\n                for rel in self._binary.dynamic_relocations:\n                    if self._is_glob_dat(rel) and rel.has_symbol and not rel.symbol.is_variable:\n                        yield rel.symbol.name, rel.address\n            except Exception:\n                logger.error('Something wrong with the pltgot relocations')\n\n        else:\n            raise NotImplementedError(f\"Imported functions relocations not implemented for: {self.format.name}\")\n\n    def imported_variable_symbols_relocations(self) -> Generator[Tuple[str, Addr], None, None]:\n        \"\"\"\n        Iterate over all imported variable symbols. Yield for each of them the name and\n        the relocation address in the binary.\n\n        :return: Generator of tuples with symbol name, relocation address\n        \"\"\"\n        if self.format == lief.EXE_FORMATS.ELF:\n            rel_enum = self.relocation_enum\n            # Iterate imported symbols\n            for rel in self._binary.dynamic_relocations:\n                if rel.has_symbol:\n                #if rel_enum(rel.type) == rel_enum.COPY and rel.has_symbol:\n                    if rel.symbol.is_variable:\n                        yield rel.symbol.name, rel.address\n        else:\n            raise NotImplementedError(f\"Imported symbols relocations not implemented for: {self.format.name}\")\n\n    def find_function_addr(self, name: str) -> Optional[Addr]:\n        \"\"\"\n        Search for the function name in fonctions of the binary.\n\n        :param name: Function name\n        :type name: str\n        :return: Address of function if found\n        :rtype: Addr\n        \"\"\"\n        f = self._funs.get(name)\n        return f.address if f else None\n\n    @property\n    def arch_mode(self) -> ArchMode:\n        pass  # TODO: Richard", ""]}
{"filename": "tritondse/loaders/__init__.py", "chunked_list": ["from .loader import Loader, LoadableSegment\nfrom .cle_loader import CleLoader\nfrom .program import Program\nfrom .quokkaprogram import QuokkaProgram\n"]}
