{"filename": "editing_prediction/video_generation_AugLy.py", "chunked_list": ["import argparse\nimport csv\nimport glob\nimport json\nimport os\nimport random\nimport tempfile\nfrom pathlib import Path\n\nimport decord", "\nimport decord\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\nfrom augly.video import (\n    BlendVideos,", "from augly.video import (\n    BlendVideos,\n    Brightness,\n    ColorJitter,\n    Contrast,\n    Crop,\n    Grayscale,\n    HFlip,\n    HStack,\n    InsertInBackground,", "    HStack,\n    InsertInBackground,\n    Loop,\n    Overlay,\n    OverlayDots,\n    OverlayEmoji,\n    OverlayShapes,\n    OverlayText,\n    Pad,\n    PerspectiveTransformAndShake,", "    Pad,\n    PerspectiveTransformAndShake,\n    RandomAspectRatio,\n    RandomBlur,\n    RandomEncodingQuality,\n    RandomNoise,\n    RandomPixelization,\n    RandomVideoSpeed,\n    Rotate,\n    VFlip,", "    Rotate,\n    VFlip,\n    VStack,\n)\n\n\ndef select_emoji_path():\n    emoji_dir_all = glob.glob(\"AugLy/augly/assets/twemojis/*\")\n    emoji_dir = random.choice(emoji_dir_all)\n    emoji_pic_all = glob.glob(f\"{emoji_dir}/*.png\")\n    emoji_pic = random.choice(emoji_pic_all)\n    return emoji_pic", "\n\ndef decord_clip_reader(path: Path, start: float, duration: float, count: int):\n    num_threads = 0\n    ctx = decord.cpu(0)\n\n    reader = decord.VideoReader(\n        path.as_posix(),\n        ctx=ctx,\n        num_threads=num_threads,\n    )\n\n    n_frames = len(reader)\n    timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n\n    frame_pos = np.linspace(start, start + duration, count)\n    frame_ids = np.searchsorted(timestamps[:, 0], frame_pos)\n    frame_ids = np.minimum(frame_ids, n_frames - 1)\n\n    frames = reader.get_batch(frame_ids).asnumpy()\n\n    tensor = torch.as_tensor(frames)\n    tensor = torch.permute(tensor, (0, 3, 1, 2))\n    return tensor", "\n\ndef main(args):\n\n    gt_match_csv = f\"{args.output}/cvl_train_matching_ground_truth.csv\"\n    edit_query_meta_csv = f\"{args.output}/cvl_train_query_metadata.csv\"\n    if not Path(gt_match_csv).is_file():\n        os.makedirs(f\"{args.output}/query\", exist_ok=True)\n        os.makedirs(f\"{args.output}/cvl_train_augly_metadata\", exist_ok=True)\n        with open(gt_match_csv, \"a\") as f_gt:\n            writer_gt = csv.writer(f_gt, delimiter=\",\")\n            writer_gt.writerow(\n                [\n                    \"query_id\",\n                    \"ref_id\",\n                    \"query_start\",\n                    \"query_end\",\n                    \"ref_start\",\n                    \"ref_end\",\n                ]\n            )\n        with open(edit_query_meta_csv, \"a\") as f_meta:\n            writer_meta = csv.writer(f_meta, delimiter=\",\")\n            writer_meta.writerow(\n                [\n                    \"video_id\",\n                    \"duration_sec\",\n                    \"frames_per_sec\",\n                    \"width\",\n                    \"height\",\n                    \"rn\",\n                    \"base_query_id\",\n                ]\n            )\n\n    ref_meta = pd.read_csv(args.meta_path)\n    # for i in range(0, 1):\n    for i in range(args.sample_range[0], args.sample_range[1]):\n        ref_id = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n        r_meta = ref_meta[ref_meta[\"video_id\"] == ref_id.stem]\n        # select r_start, r_end from r_meta (>5s, mean 12 s, Max: 54, Min: 2.4)\n        r_dura_org = float(r_meta[\"duration_sec\"])\n        if r_dura_org > 5:\n            r_dura_copy = random.uniform(4, min(r_dura_org - 1, 60))\n            r_start = random.uniform(0, r_dura_org - r_dura_copy)\n            r_end = r_start + r_dura_copy\n            # extract r_start, r_end from ref video -> decord_clip_reader\n            count = int(r_meta[\"frames_per_sec\"] * r_dura_copy)\n            copy_ref = decord_clip_reader(\n                path=ref_id, start=r_start, duration=r_dura_copy, count=count\n            )\n        else:\n            continue\n\n        query_id = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n        q_meta = ref_meta[ref_meta[\"video_id\"] == query_id.stem]\n        # extract 0~end from query video\n        q_dura_org = float(q_meta[\"duration_sec\"])\n        count = int(q_meta[\"frames_per_sec\"] * q_dura_org)\n        query_all = decord_clip_reader(\n            path=query_id, start=0.0, duration=q_dura_org, count=count\n        )\n        # resize to fit query video\n        if copy_ref.shape[2:] != query_all.shape[2:]:\n            transform = T.Resize((query_all.shape[2], query_all.shape[3]))\n            copy_ref = transform(copy_ref)\n        copy_ref = torch.permute(copy_ref, (0, 2, 3, 1))\n        query_all = torch.permute(query_all, (0, 2, 3, 1))\n\n        # select q_start, q_end from q_meta\n        q_insert_frame = random.choice(range(0, query_all.shape[0]))\n        # insert\n        frameNo = query_all.shape[0] + copy_ref.shape[0]\n        copy_video = torch.zeros(frameNo, query_all.shape[1], query_all.shape[2], 3)\n        copy_video[:q_insert_frame, :, :, :] = query_all[:q_insert_frame, :, :, :]\n        q_insert_end = q_insert_frame + copy_ref.shape[0]\n        copy_video[q_insert_end:, :, :, :] = query_all[q_insert_frame:, :, :, :]\n        copy_video[q_insert_frame:q_insert_end, :, :, :] = copy_ref\n\n        video_in2 = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n        emoji_path = select_emoji_path()\n        brightness_level = random.uniform(-0.6, 0.6)\n        contrast_level = random.uniform(-2.0, 2.0)\n        saturation_factor = random.uniform(0.0, 3.0)\n        rotate_degrees = random.choice([i for i in range(-30, 30, 5)])\n        # rotate_degrees = random.choice([-90, 90])\n        crop_left = random.uniform(0, 0.3)\n        crop_top = random.uniform(0, 0.3)\n        crop_right = random.uniform(0.7, 1.0)\n        crop_bottom = random.uniform(0.7, 1.0)\n        pad_w_factor = random.uniform(0, 0.25)\n        pad_h_factor = random.uniform(0, 0.25)\n        pad_color_r = random.randint(0, 255)\n        pad_color_g = random.randint(0, 255)\n        pad_color_b = random.randint(0, 255)\n        overlay_num_dots = random.randint(50, 250)\n        overlay_dot_type = random.choice([\"colored\", \"blur\"])\n        overlay_random_movement = random.choice([True, False])\n        overlay_num_shapes = random.randint(1, 3)\n        overlay_text_len = random.randint(5, 15)\n        shake_sigma = random.uniform(10, 50)\n        shake_radius = random.uniform(0, 1.0)\n        emoji_x_factor = random.uniform(0.1, 0.6)\n        emoji_y_factor = random.uniform(0.1, 0.6)\n        emoji_opacity = random.uniform(0.5, 1.0)\n        emoji_size = random.uniform(0.2, 0.6)\n        opacity = random.uniform(0.1, 0.5)\n        overlay_size = random.choice([1.0, 1.0, 1.0, 0.8, 0.9])\n        overlay_xy = [[0.2, 0], [0.3, 0], [0.4, 0], [0.5, 0]]\n        overlay_xy_i = random.randint(0, len(overlay_xy) - 1)\n        overlay_x_factor = overlay_xy[overlay_xy_i][0]\n        overlay_y_factor = overlay_xy[overlay_xy_i][1]\n        augly_method = [\n            Brightness(level=brightness_level),\n            Contrast(level=contrast_level),\n            ColorJitter(saturation_factor=2.5),\n            Grayscale(),\n            RandomBlur(min_sigma=4.0, max_sigma=12.0),\n            RandomPixelization(min_ratio=0.1, max_ratio=0.7),\n            RandomAspectRatio(min_ratio=0.5, max_ratio=2.0),\n            RandomVideoSpeed(min_factor=0.2, max_factor=5.0),\n            Rotate(degrees=rotate_degrees),\n            Crop(left=crop_left, top=crop_top, right=crop_right, bottom=crop_bottom),\n            Pad(\n                w_factor=pad_w_factor,\n                h_factor=pad_w_factor,\n                color=(pad_color_r, pad_color_g, pad_color_b),\n            ),\n            RandomNoise(min_level=15, max_level=60),\n            RandomEncodingQuality(min_quality=10, max_quality=45),\n            HFlip(),\n            VFlip(),\n            Loop(),\n            OverlayDots(\n                num_dots=overlay_num_dots,\n                dot_type=overlay_dot_type,\n                random_movement=overlay_random_movement,\n            ),\n            OverlayShapes(num_shapes=overlay_num_shapes),\n            OverlayText(text_len=overlay_text_len),\n            PerspectiveTransformAndShake(sigma=shake_sigma, shake_radius=shake_radius),\n            OverlayEmoji(\n                emoji_path=emoji_path,\n                x_factor=emoji_x_factor,\n                y_factor=emoji_y_factor,\n                opacity=emoji_opacity,\n                emoji_size=emoji_size,\n            ),\n            BlendVideos(str(video_in2), opacity=opacity, overlay_size=overlay_size),\n            HStack(str(video_in2)),\n            VStack(str(video_in2)),\n            Overlay(\n                str(video_in2), x_factor=overlay_x_factor, y_factor=overlay_y_factor\n            ),\n            InsertInBackground(str(video_in2), offset_factor=0.1),\n        ]\n        edit_method = random.choice(augly_method)\n\n        # save video\n        meta_list = []\n        out_vid_path = f\"{args.output}/query/Q4{str(i).zfill(5)}.mp4\"\n        out_meta_path = (\n            f\"{args.output}/cvl_train_augly_metadata/Q4{str(i).zfill(5)}.json\"\n        )\n        with tempfile.TemporaryDirectory() as dir:\n            copy_vid_path = f\"{dir}/copy_video.mp4\"\n            torchvision.io.write_video(\n                filename=copy_vid_path,\n                video_array=copy_video,\n                fps=float(q_meta[\"frames_per_sec\"]),\n            )\n            edit_method(copy_vid_path, out_vid_path, metadata=meta_list)\n\n        factor = 1\n        if meta_list[0][\"name\"] == \"change_video_speed\":\n            factor = meta_list[0][\"factor\"]\n        with open(gt_match_csv, \"a\") as f_gt:\n            writer_gt = csv.writer(f_gt, delimiter=\",\")\n            q_start_sec = q_insert_frame / float(q_meta[\"frames_per_sec\"]) / factor\n            q_end_sec = (\n                q_start_sec\n                + copy_ref.shape[0] / float(q_meta[\"frames_per_sec\"]) / factor\n            )\n            row = [\n                f\"Q4{str(i).zfill(5)}\",\n                ref_id.stem,\n                q_start_sec,\n                q_end_sec,\n                r_start,\n                r_end,\n            ]\n            writer_gt.writerow(row)\n\n        with open(edit_query_meta_csv, \"a\") as f_meta:\n            writer_meta = csv.writer(f_meta, delimiter=\",\")\n            duration_sec = frameNo / float(q_meta[\"frames_per_sec\"]) / factor\n            row = [\n                f\"Q4{str(i).zfill(5)}\",\n                duration_sec,\n                float(q_meta[\"frames_per_sec\"]),\n                int(q_meta[\"width\"]),\n                int(q_meta[\"height\"]),\n                i,\n                query_id.stem,\n            ]\n            writer_meta.writerow(row)\n\n        with open(out_meta_path, \"w\") as f:\n            json.dump(meta_list[0], f)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data_path\", type=str, default=\"competition_data/train/reference/\"\n    )\n    parser.add_argument(\n        \"--meta_path\",\n        type=str,\n        default=\"competition_data/train/train_reference_metadata.csv\",\n    )\n    parser.add_argument(\"--output\", type=str, default=\"gen_data/train_reference\")\n    parser.add_argument(\"--sample_range\", nargs=\"*\", type=int, required=True)\n    args = parser.parse_args()\n    main(args)", ""]}
{"filename": "editing_prediction/training/train.py", "chunked_list": ["import argparse\nimport shutil\nfrom pathlib import Path\n\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom pytorch_lightning.callbacks import ModelCheckpoint", "import torch.nn as nn\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torchvision import transforms\nfrom utils.configs import timm_model_config\nfrom utils.dataset import VscDataModule\nfrom utils.model import LitModel\n\n\ndef train(args: argparse.Namespace):\n    cudnn.benchmark = True\n    pl.seed_everything(args.seed, workers=True)\n\n    labels = (\n        pd.read_csv(args.dataset_dir_path / \"label.csv\")\n        .sort_values(\"label_idx\")\n        .drop_duplicates()\n        .sort_values(\"label_idx\")[\"label\"]\n        .tolist()\n    )\n\n    mean, std, input_size = timm_model_config(args.model_name)\n\n    train_transform = nn.Sequential(\n        transforms.ConvertImageDtype(torch.float32),\n        transforms.Normalize(mean=mean, std=std),\n        transforms.Resize(size=input_size),\n    )\n\n    valid_transform = nn.Sequential(\n        transforms.ConvertImageDtype(torch.float32),\n        transforms.Normalize(mean=mean, std=std),\n        transforms.Resize(size=input_size),\n    )\n\n    data_module = VscDataModule(\n        dataset_dir_path=args.dataset_dir_path,\n        labels=labels,\n        frames_per_video=args.frames_per_video,\n        train_transform=train_transform,\n        valid_transform=valid_transform,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        jit=False,\n    )\n\n    model = LitModel(\n        model_name=args.model_name,\n        labels=labels,\n    )\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=Path(__file__).parent / \"training_log\",\n        monitor=\"valid_loss\",\n        save_last=True,\n        save_top_k=1,\n    )\n\n    trainer = pl.Trainer(\n        strategy=args.ddp_strategy,\n        accelerator=args.accelerator,\n        devices=args.devices,\n        max_epochs=args.epochs,\n        num_sanity_val_steps=-1,\n        callbacks=[checkpoint_callback],\n    )\n    trainer.fit(model, datamodule=data_module)\n\n    shutil.copy(\n        checkpoint_callback.best_model_path,\n        \"model.ckpt\",\n    )", "def train(args: argparse.Namespace):\n    cudnn.benchmark = True\n    pl.seed_everything(args.seed, workers=True)\n\n    labels = (\n        pd.read_csv(args.dataset_dir_path / \"label.csv\")\n        .sort_values(\"label_idx\")\n        .drop_duplicates()\n        .sort_values(\"label_idx\")[\"label\"]\n        .tolist()\n    )\n\n    mean, std, input_size = timm_model_config(args.model_name)\n\n    train_transform = nn.Sequential(\n        transforms.ConvertImageDtype(torch.float32),\n        transforms.Normalize(mean=mean, std=std),\n        transforms.Resize(size=input_size),\n    )\n\n    valid_transform = nn.Sequential(\n        transforms.ConvertImageDtype(torch.float32),\n        transforms.Normalize(mean=mean, std=std),\n        transforms.Resize(size=input_size),\n    )\n\n    data_module = VscDataModule(\n        dataset_dir_path=args.dataset_dir_path,\n        labels=labels,\n        frames_per_video=args.frames_per_video,\n        train_transform=train_transform,\n        valid_transform=valid_transform,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        jit=False,\n    )\n\n    model = LitModel(\n        model_name=args.model_name,\n        labels=labels,\n    )\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=Path(__file__).parent / \"training_log\",\n        monitor=\"valid_loss\",\n        save_last=True,\n        save_top_k=1,\n    )\n\n    trainer = pl.Trainer(\n        strategy=args.ddp_strategy,\n        accelerator=args.accelerator,\n        devices=args.devices,\n        max_epochs=args.epochs,\n        num_sanity_val_steps=-1,\n        callbacks=[checkpoint_callback],\n    )\n    trainer.fit(model, datamodule=data_module)\n\n    shutil.copy(\n        checkpoint_callback.best_model_path,\n        \"model.ckpt\",\n    )", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset-dir-path\", required=True, type=Path)\n    parser.add_argument(\"--frames-per-video\", required=True, type=int)\n\n    parser.add_argument(\"--model-name\", type=str, required=True)\n    parser.add_argument(\"--epochs\", type=int, required=True)\n    parser.add_argument(\"--batch-size\", type=int, required=True)\n\n    parser.add_argument(\"--seed\", default=sum(ord(x) for x in \"vsc\"))\n\n    parser.add_argument(\"--devices\", nargs=\"+\", type=int)\n    parser.add_argument(\"--num-workers\", type=int, default=0)\n    parser.add_argument(\"--accelerator\", default=\"gpu\")\n    parser.add_argument(\"--ddp-strategy\", default=\"ddp\")\n\n    args = parser.parse_args()\n\n    train(args)", ""]}
{"filename": "editing_prediction/training/prepare.py", "chunked_list": ["import argparse\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nAUG_LABEL_MAPPING = {\n    \"rotate90\": \"rotate90_270\",", "AUG_LABEL_MAPPING = {\n    \"rotate90\": \"rotate90_270\",\n    \"rotate270\": \"rotate90_270\",\n    \"rotate180\": \"rotate180_vflip\",\n    \"vflip\": \"rotate180_vflip\",\n    \"hflip\": \"other\",\n    \"change_video_speed\": \"other\",\n}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--video-dir-path\", required=True, type=Path)\n    parser.add_argument(\"--augly-metadata-path\", required=True, type=Path)\n    args = parser.parse_args()\n\n    seed = 1234\n    dataset_dir = Path(\"./dataset\")\n    video_dir_path = args.video_dir_path\n    augly_metadata_path = args.augly_metadata_path\n\n    # Gather metadata\n    metadata = []\n    for p in sorted(augly_metadata_path.glob(\"*\")):\n        with open(p) as f:\n            m = json.load(f)\n        video_id = p.stem.split(\"_\", 1)[0]\n        row = {\n            \"video_id\": video_id,\n            \"video_path\": (video_dir_path / f\"{video_id}.mp4\").as_posix(),\n            \"aug_type\": m.get(\"sub_name\", m[\"name\"]),\n        }\n        metadata.append(row)\n    metadata = pd.DataFrame(metadata)\n\n    # Assign label id\n    aug_types = sorted(metadata[\"aug_type\"].unique())\n    aug_types = pd.DataFrame({\"aug_type\": aug_types})\n    aug_types[\"label\"] = aug_types[\"aug_type\"].replace(AUG_LABEL_MAPPING)\n\n    labels = sorted(aug_types[\"label\"].unique())\n    label_idx = pd.Series(np.arange(len(labels)), index=labels, name=\"label_idx\")\n\n    aug_label_mapping = aug_types.join(label_idx, on=\"label\", how=\"left\").set_index(\n        \"aug_type\"\n    )\n\n    print(\"Labels\")\n    print(aug_label_mapping.to_markdown())\n\n    metadata = metadata.join(aug_label_mapping, on=\"aug_type\", how=\"left\")\n\n    # Grouping\n    sorted_tuple = lambda x: tuple(sorted(x))\n    metadata = (\n        metadata.drop_duplicates()\n        .groupby([\"video_id\", \"video_path\"])\n        .agg(\n            {\n                \"aug_type\": sorted_tuple,\n                \"label\": sorted_tuple,\n                \"label_idx\": sorted_tuple,\n            }\n        )\n        .reset_index()\n    )\n\n    # Random split\n    label_counts = (\n        metadata[\"label\"]\n        .value_counts()\n        .reset_index(name=\"c\")\n        .rename(columns={\"index\": \"label\"})\n    )\n    label_counts[\"stratify_label\"] = label_counts.apply(\n        lambda x: x[\"label\"] if x[\"c\"] >= 10 else (\"minor\",), axis=1\n    )\n    stratifier = metadata.merge(label_counts, on=\"label\")[\"stratify_label\"]\n\n    train, valid = train_test_split(\n        metadata,\n        test_size=0.2,\n        stratify=stratifier,\n        random_state=seed,\n    )\n\n    print(f\"Train datset: {train.shape}\")\n    print(f\"Valid datset: {valid.shape}\")\n\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    aug_label_mapping.to_csv(dataset_dir / \"label.csv\", index=False)\n    train.to_parquet(dataset_dir / \"train.parquet\", index=False)\n    valid.to_parquet(dataset_dir / \"valid.parquet\", index=False)", "\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--video-dir-path\", required=True, type=Path)\n    parser.add_argument(\"--augly-metadata-path\", required=True, type=Path)\n    args = parser.parse_args()\n\n    seed = 1234\n    dataset_dir = Path(\"./dataset\")\n    video_dir_path = args.video_dir_path\n    augly_metadata_path = args.augly_metadata_path\n\n    # Gather metadata\n    metadata = []\n    for p in sorted(augly_metadata_path.glob(\"*\")):\n        with open(p) as f:\n            m = json.load(f)\n        video_id = p.stem.split(\"_\", 1)[0]\n        row = {\n            \"video_id\": video_id,\n            \"video_path\": (video_dir_path / f\"{video_id}.mp4\").as_posix(),\n            \"aug_type\": m.get(\"sub_name\", m[\"name\"]),\n        }\n        metadata.append(row)\n    metadata = pd.DataFrame(metadata)\n\n    # Assign label id\n    aug_types = sorted(metadata[\"aug_type\"].unique())\n    aug_types = pd.DataFrame({\"aug_type\": aug_types})\n    aug_types[\"label\"] = aug_types[\"aug_type\"].replace(AUG_LABEL_MAPPING)\n\n    labels = sorted(aug_types[\"label\"].unique())\n    label_idx = pd.Series(np.arange(len(labels)), index=labels, name=\"label_idx\")\n\n    aug_label_mapping = aug_types.join(label_idx, on=\"label\", how=\"left\").set_index(\n        \"aug_type\"\n    )\n\n    print(\"Labels\")\n    print(aug_label_mapping.to_markdown())\n\n    metadata = metadata.join(aug_label_mapping, on=\"aug_type\", how=\"left\")\n\n    # Grouping\n    sorted_tuple = lambda x: tuple(sorted(x))\n    metadata = (\n        metadata.drop_duplicates()\n        .groupby([\"video_id\", \"video_path\"])\n        .agg(\n            {\n                \"aug_type\": sorted_tuple,\n                \"label\": sorted_tuple,\n                \"label_idx\": sorted_tuple,\n            }\n        )\n        .reset_index()\n    )\n\n    # Random split\n    label_counts = (\n        metadata[\"label\"]\n        .value_counts()\n        .reset_index(name=\"c\")\n        .rename(columns={\"index\": \"label\"})\n    )\n    label_counts[\"stratify_label\"] = label_counts.apply(\n        lambda x: x[\"label\"] if x[\"c\"] >= 10 else (\"minor\",), axis=1\n    )\n    stratifier = metadata.merge(label_counts, on=\"label\")[\"stratify_label\"]\n\n    train, valid = train_test_split(\n        metadata,\n        test_size=0.2,\n        stratify=stratifier,\n        random_state=seed,\n    )\n\n    print(f\"Train datset: {train.shape}\")\n    print(f\"Valid datset: {valid.shape}\")\n\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    aug_label_mapping.to_csv(dataset_dir / \"label.csv\", index=False)\n    train.to_parquet(dataset_dir / \"train.parquet\", index=False)\n    valid.to_parquet(dataset_dir / \"valid.parquet\", index=False)", ""]}
{"filename": "editing_prediction/training/utils/model.py", "chunked_list": ["import pytorch_lightning as pl\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchmetrics\n\n\nclass LitModel(pl.LightningModule):\n    def __init__(self, model_name, labels):\n        super().__init__()\n\n        self.labels = labels\n        num_classes = len(labels)\n\n        if model_name.startswith(\"hf-hub:\"):\n            model = timm.create_model(model_name, pretrained=True)\n            model.head[-1] = nn.Linear(1024, num_classes)\n            self.model = model\n        else:\n            self.model = timm.create_model(\n                model_name, num_classes=num_classes, pretrained=True\n            )\n\n        self.criterion = nn.BCEWithLogitsLoss()\n\n        self.valid_acc = torchmetrics.MultioutputWrapper(\n            torchmetrics.Accuracy(\"binary\", average=None),\n            num_classes,\n        )\n        self.valid_auc = torchmetrics.MultioutputWrapper(\n            torchmetrics.AUROC(\"binary\", average=None),\n            num_classes,\n        )\n        self.valid_ap = torchmetrics.MultioutputWrapper(\n            torchmetrics.AveragePrecision(\"binary\", average=None),\n            num_classes,\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logit = self(x)\n        loss = self.criterion(logit, y)\n\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logit = self(x)\n        y_hat = torch.sigmoid(logit)\n\n        loss = self.criterion(logit, y)\n\n        self.valid_acc.update(y_hat, y)\n        self.valid_auc.update(y_hat, y)\n        self.valid_ap.update(y_hat, y.long())\n\n        self.log(\"valid_loss\", loss)\n\n    def validation_epoch_end(self, outputs):\n        acc = self.valid_acc.compute()\n        auc = self.valid_auc.compute()\n        ap = self.valid_ap.compute()\n\n        self.log_dict(\n            {f\"valid_{l}_acc\": x for l, x in zip(self.labels, acc)}, sync_dist=True\n        )\n        self.log_dict(\n            {f\"valid_{l}_auc\": x for l, x in zip(self.labels, auc)}, sync_dist=True\n        )\n        self.log_dict(\n            {f\"valid_{l}_ap\": x for l, x in zip(self.labels, ap)}, sync_dist=True\n        )\n\n        self.valid_acc.reset()\n        self.valid_auc.reset()\n        self.valid_ap.reset()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.0001)", "class LitModel(pl.LightningModule):\n    def __init__(self, model_name, labels):\n        super().__init__()\n\n        self.labels = labels\n        num_classes = len(labels)\n\n        if model_name.startswith(\"hf-hub:\"):\n            model = timm.create_model(model_name, pretrained=True)\n            model.head[-1] = nn.Linear(1024, num_classes)\n            self.model = model\n        else:\n            self.model = timm.create_model(\n                model_name, num_classes=num_classes, pretrained=True\n            )\n\n        self.criterion = nn.BCEWithLogitsLoss()\n\n        self.valid_acc = torchmetrics.MultioutputWrapper(\n            torchmetrics.Accuracy(\"binary\", average=None),\n            num_classes,\n        )\n        self.valid_auc = torchmetrics.MultioutputWrapper(\n            torchmetrics.AUROC(\"binary\", average=None),\n            num_classes,\n        )\n        self.valid_ap = torchmetrics.MultioutputWrapper(\n            torchmetrics.AveragePrecision(\"binary\", average=None),\n            num_classes,\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logit = self(x)\n        loss = self.criterion(logit, y)\n\n        self.log(\"train_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logit = self(x)\n        y_hat = torch.sigmoid(logit)\n\n        loss = self.criterion(logit, y)\n\n        self.valid_acc.update(y_hat, y)\n        self.valid_auc.update(y_hat, y)\n        self.valid_ap.update(y_hat, y.long())\n\n        self.log(\"valid_loss\", loss)\n\n    def validation_epoch_end(self, outputs):\n        acc = self.valid_acc.compute()\n        auc = self.valid_auc.compute()\n        ap = self.valid_ap.compute()\n\n        self.log_dict(\n            {f\"valid_{l}_acc\": x for l, x in zip(self.labels, acc)}, sync_dist=True\n        )\n        self.log_dict(\n            {f\"valid_{l}_auc\": x for l, x in zip(self.labels, auc)}, sync_dist=True\n        )\n        self.log_dict(\n            {f\"valid_{l}_ap\": x for l, x in zip(self.labels, ap)}, sync_dist=True\n        )\n\n        self.valid_acc.reset()\n        self.valid_auc.reset()\n        self.valid_ap.reset()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.0001)", ""]}
{"filename": "editing_prediction/training/utils/configs.py", "chunked_list": ["import timm\n\n\ndef timm_model_config(model_name):\n    if model_name == \"hf-hub:timm/convnext_base.clip_laion2b_augreg_ft_in1k\":\n        mean = (0.48145466, 0.4578275, 0.40821073)\n        std = (0.26862954, 0.26130258, 0.27577711)\n        input_size = (256, 256)\n    elif model_name == \"hf-hub:timm/convnext_base.clip_laiona_augreg_ft_in1k_384\":\n        mean = (0.48145466, 0.4578275, 0.40821073)\n        std = (0.26862954, 0.26130258, 0.27577711)\n        input_size = (384, 384)\n    else:\n        mean = timm.get_pretrained_cfg_value(model_name, \"mean\")\n        std = timm.get_pretrained_cfg_value(model_name, \"std\")\n        input_size = timm.get_pretrained_cfg_value(model_name, \"input_size\")[1:]\n\n    return mean, std, input_size", ""]}
{"filename": "editing_prediction/training/utils/dataset.py", "chunked_list": ["import json\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional\n\nimport decord\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom sklearn.model_selection import train_test_split", "import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        dataset: pd.DataFrame,\n        n_labels: int,\n        transform: Optional[Callable] = None,\n        tensor_format: str = \"CHW\",\n        random_relpos_offset: float = 0.0,\n    ):\n        self.dataset = dataset\n        self.n_labels = n_labels\n        self.onehot = torch.eye(n_labels, dtype=torch.float32)\n        self.transform = transform\n        self.tensor_format = tensor_format\n        self.random_relpos_offset = random_relpos_offset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, i):\n        row = self.dataset.iloc[i]\n\n        num_threads = 0\n        ctx = decord.cpu(0)\n\n        reader = decord.VideoReader(\n            row[\"video_path\"],\n            ctx=ctx,\n            num_threads=num_threads,\n        )\n        n_frames = len(reader)\n\n        relpos = row[\"relative_pos\"]\n        if self.random_relpos_offset > 0.0:\n            relpos += (np.random.random() * 2 - 1) * self.random_relpos_offset\n\n        pos = int((n_frames * relpos).round())\n\n        try:\n            frame = reader[pos].asnumpy()\n        except:\n            print(row.to_dict())\n            raise\n\n        tensor = torch.as_tensor(frame)\n\n        if self.transform:\n            tensor = tensor.permute(2, 0, 1)\n            tensor = self.transform(tensor)\n            tensor = tensor.permute(1, 2, 0)\n\n        if self.tensor_format == \"CHW\":\n            tensor = tensor.permute(2, 0, 1)\n\n        label = self.onehot[row[\"label_idx\"].tolist()].max(dim=0).values\n        return tensor, label", "\n\nclass VscDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        dataset_dir_path: Path,\n        labels: List[str],\n        frames_per_video: int,\n        batch_size: int,\n        train_transform: Optional[Callable] = None,\n        valid_transform: Optional[Callable] = None,\n        tensor_format: str = \"CHW\",\n        num_workers: int = 1,\n        jit: bool = True,\n    ):\n        super().__init__()\n        self.dataset_dir_path = dataset_dir_path\n        self.labels = labels\n        self.frames_per_video = frames_per_video\n        self.batch_size = batch_size\n        self.train_transform = train_transform\n        self.valid_transform = valid_transform\n        self.tensor_format = tensor_format\n        self.num_workers = num_workers\n        self.jit = jit\n\n    def setup(self, stage: str):\n        if stage == \"fit\":\n            train = pd.read_parquet(self.dataset_dir_path / \"train.parquet\")\n            valid = pd.read_parquet(self.dataset_dir_path / \"valid.parquet\")\n\n            t_train = self.train_transform\n            t_valid = self.valid_transform\n\n            if self.jit:\n                t_train = t_train and torch.jit.script(t_train)\n                t_valid = t_valid and torch.jit.script(t_valid)\n\n            # make dataset\n            self.train = self._make_dataset(train, len(self.labels), t_train, 0.05)\n            self.valid = self._make_dataset(valid, len(self.labels), t_valid, 0.0)\n\n    def _gather_augly_metadata(self) -> pd.DataFrame:\n        metadata = []\n        for p in sorted(self.augly_metadata_path.glob(\"*\")):\n            with open(p) as f:\n                m = json.load(f)\n            video_id = p.stem.split(\"_\", 1)[0]\n            row = {\n                \"video_id\": video_id,\n                \"video_path\": (self.video_dir_path / f\"{video_id}.mp4\").as_posix(),\n                \"aug_type\": m[\"name\"],\n            }\n            metadata.append(row)\n        return pd.DataFrame(metadata)\n\n    def _make_dataset(\n        self,\n        metadata: List[dict],\n        n_labels: int,\n        transform: Optional[Callable],\n        random_relpos_offset: float,\n    ) -> pd.DataFrame:\n        dataset = pd.DataFrame(metadata).join(\n            pd.DataFrame(\n                {\n                    \"relative_pos\": np.linspace(\n                        0.1, 0.9, self.frames_per_video, dtype=float\n                    )\n                }\n            ),\n            how=\"cross\",\n        )\n\n        return Dataset(\n            dataset, n_labels, transform, self.tensor_format, random_relpos_offset\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            drop_last=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.valid,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            drop_last=False,\n        )", ""]}
{"filename": "editing_prediction/training/utils/__init__.py", "chunked_list": [""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/main.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport subprocess\nimport time\nfrom pathlib import Path", "import time\nfrom pathlib import Path\nfrom shutil import copyfile\n\nimport numpy as np\nimport pandas as pd\n\nROOT_DIRECTORY = Path(\"/code_execution\")\nDATA_DIRECTORY = Path(\"/data\")\nQRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"", "DATA_DIRECTORY = Path(\"/data\")\nQRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"\nOUTPUT_FILE = ROOT_DIRECTORY / \"subset_query_descriptors.npz\"\nQUERY_SUBSET_FILE = DATA_DIRECTORY / \"query_subset.csv\"\n\n\ndef main():\n\n    num_videos = len(pd.read_csv(QUERY_SUBSET_FILE))\n    time_deadline_sec = num_videos * (10 + 1)  # 10 sec per video + 1 sec for overhead\n    start_time = time.time()\n\n    cmd = f\"\"\"\n    conda run --no-capture-output -n condaenv python -m src.inference \\\n        --accelerator=cuda --processes=1 --fps 2 --stride 3 \\\n        --dataset_path {str(QRY_VIDEOS_DIRECTORY)} \\\n        --output_file {str(OUTPUT_FILE)} \\\n        --read_type tensor \\\n        --video_reader DECORD\n    \"\"\"\n    subprocess.run(cmd.split())\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Elapsed time: {elapsed_time} sec. (deadline: {time_deadline_sec} sec)\")\n\n    if elapsed_time > time_deadline_sec:\n        print(\"Time limit exceeded.\")\n    else:\n        print(\"Time limit not exceeded.\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference_full.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Inference script.\n\nThis is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors", "This is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors\nin some runtime environments.\n\nWe import inference_impl, which imports libraries that may initialize cuda,\nin two circumstances: from worker processes after the main process has\nforked workers, or from the main process after worker processes have been\njoined.\n\"\"\"\n", "\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd", "\nimport pandas as pd\nimport tqdm\nfrom src.inference import Accelerator, VideoReaderType\nfrom src.inference_impl import search\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.storage import load_features, store_features\nfrom torch import multiprocessing\n\nparser = argparse.ArgumentParser()", "\nparser = argparse.ArgumentParser()\ninference_parser = parser.add_argument_group(\"Inference\")\ninference_parser.add_argument(\"--batch_size\", type=int, default=32)\ninference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\ninference_parser.add_argument(\"--distributed_size\", type=int, default=1)\ninference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(\n    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n)", "    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n)\ninference_parser.add_argument(\"--output_path\", required=True)\ninference_parser.add_argument(\"--scratch_path\", required=False)\n\ndataset_parser = parser.add_argument_group(\"Dataset\")\n# multiple dataset path\ndataset_parser.add_argument(\"--dataset_paths\", nargs=\"+\")\ndataset_parser.add_argument(\"--gt_path\")\ndataset_parser.add_argument(\"--fps\", default=1, type=float)", "dataset_parser.add_argument(\"--gt_path\")\ndataset_parser.add_argument(\"--fps\", default=1, type=float)\ndataset_parser.add_argument(\"--stride\", type=int)\ndataset_parser.add_argument(\"--len_cap\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"tensor\", type=str)\ndataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\ndataset_parser.add_argument(\n    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEGPY\"\n)\ndataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")", ")\ndataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\ndataset_parser.add_argument(\"--tta\", action=\"store_true\")\ndataset_parser.add_argument(\n    \"--mode\", default=\"whole\", choices=[\"whole\", \"eval\", \"test\", \"tune\"]\n)\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,", "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference.py\")\nlogger.setLevel(logging.INFO)\n\n\ndef main(args):\n    success = False\n    if args.processes > 1 and args.distributed_size > 1:\n        raise Exception(\n            \"Set either --processes (single-machine distributed) or \"\n            \"both --distributed_size and --distributed_rank (arbitrary \"\n            \"distributed)\"\n        )\n\n    if args.video_reader == \"DECORD\":\n        import subprocess\n\n        subprocess.run(\n            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n            check=True,\n        )\n\n    with tempfile.TemporaryDirectory() as tmp_path:\n        splits = [\n            \"_\".join(p.split(\"/\")[-2:]) for p in args.dataset_paths\n        ]  # ./vsc/eval_subset/reference -> eval_subset_reference\n        os.makedirs(args.output_path, exist_ok=True)\n        if args.scratch_path:\n            os.makedirs(args.scratch_path, exist_ok=True)\n        else:\n            args.scratch_path = tmp_path\n        if args.processes > 1:\n            processes = []\n            logger.info(f\"Spawning {args.processes} processes\")\n            accelerator = Accelerator[args.accelerator.upper()]\n            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n            multiprocessing.set_start_method(\"spawn\")\n            worker_files = []\n            try:\n                for rank in range(args.processes):\n                    output_files = [\n                        os.path.join(args.scratch_path, f\"{split}_{rank}.npz\")\n                        for split in splits\n                    ]\n                    worker_files.append(output_files)\n                    p = multiprocessing.Process(\n                        target=distributed_worker_process,\n                        args=(\n                            args,\n                            rank,\n                            args.processes,\n                            backend,\n                            output_files,\n                            args.dataset_paths,\n                            args.tta,\n                        ),\n                    )\n                    processes.append(p)\n                    p.start()\n                worker_success = []\n                for p in processes:\n                    p.join()\n                    worker_success.append(p.exitcode == os.EX_OK)\n                success = all(worker_success)\n            finally:\n                for p in processes:\n                    p.kill()\n            if success:\n\n                def merge_feature_files(filenames, output_filename: str) -> int:\n                    features = []\n                    for fn in filenames:\n                        features.extend(load_features(fn))\n                    features = sorted(features, key=lambda x: x.video_id)\n                    store_features(output_filename, features)\n                    return len(features)\n\n                output_files_each_split = [list(x) for x in zip(*worker_files)]\n                for files, split in zip(output_files_each_split, splits):\n                    output_file = os.path.join(args.output_path, f\"{split}.npz\")\n                    num_files = merge_feature_files(files, output_file)\n                    logger.info(\n                        f\"Features for {num_files} videos saved to {output_file}\"\n                    )\n\n        else:\n            output_files = [\n                os.path.join(args.output_path, f\"{split}.npz\") for split in splits\n            ]\n            worker_process(\n                args,\n                args.distributed_rank,\n                args.distributed_size,\n                output_files,\n                args.dataset_paths,\n                args.tta,\n            )\n            success = True\n\n    if success:\n        logger.info(\"Inference succeeded.\")\n    else:\n        logger.error(\"Inference FAILED!\")\n\n    if not success or args.gt_path is None:\n        return\n\n    logger.info(\"Evaluating results\")\n\n    video_features_dict = {}\n    for split in splits:\n        video_features_dict[split] = load_features(\n            os.path.join(args.output_path, f\"{split}.npz\")\n        )\n\n    evaluate(\n        queries=video_features_dict[splits[0]],\n        refs=video_features_dict[splits[1]],\n        noises=video_features_dict[splits[-1]],\n        gt_path=args.gt_path,\n        output_path=args.output_path,\n    )", "def main(args):\n    success = False\n    if args.processes > 1 and args.distributed_size > 1:\n        raise Exception(\n            \"Set either --processes (single-machine distributed) or \"\n            \"both --distributed_size and --distributed_rank (arbitrary \"\n            \"distributed)\"\n        )\n\n    if args.video_reader == \"DECORD\":\n        import subprocess\n\n        subprocess.run(\n            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n            check=True,\n        )\n\n    with tempfile.TemporaryDirectory() as tmp_path:\n        splits = [\n            \"_\".join(p.split(\"/\")[-2:]) for p in args.dataset_paths\n        ]  # ./vsc/eval_subset/reference -> eval_subset_reference\n        os.makedirs(args.output_path, exist_ok=True)\n        if args.scratch_path:\n            os.makedirs(args.scratch_path, exist_ok=True)\n        else:\n            args.scratch_path = tmp_path\n        if args.processes > 1:\n            processes = []\n            logger.info(f\"Spawning {args.processes} processes\")\n            accelerator = Accelerator[args.accelerator.upper()]\n            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n            multiprocessing.set_start_method(\"spawn\")\n            worker_files = []\n            try:\n                for rank in range(args.processes):\n                    output_files = [\n                        os.path.join(args.scratch_path, f\"{split}_{rank}.npz\")\n                        for split in splits\n                    ]\n                    worker_files.append(output_files)\n                    p = multiprocessing.Process(\n                        target=distributed_worker_process,\n                        args=(\n                            args,\n                            rank,\n                            args.processes,\n                            backend,\n                            output_files,\n                            args.dataset_paths,\n                            args.tta,\n                        ),\n                    )\n                    processes.append(p)\n                    p.start()\n                worker_success = []\n                for p in processes:\n                    p.join()\n                    worker_success.append(p.exitcode == os.EX_OK)\n                success = all(worker_success)\n            finally:\n                for p in processes:\n                    p.kill()\n            if success:\n\n                def merge_feature_files(filenames, output_filename: str) -> int:\n                    features = []\n                    for fn in filenames:\n                        features.extend(load_features(fn))\n                    features = sorted(features, key=lambda x: x.video_id)\n                    store_features(output_filename, features)\n                    return len(features)\n\n                output_files_each_split = [list(x) for x in zip(*worker_files)]\n                for files, split in zip(output_files_each_split, splits):\n                    output_file = os.path.join(args.output_path, f\"{split}.npz\")\n                    num_files = merge_feature_files(files, output_file)\n                    logger.info(\n                        f\"Features for {num_files} videos saved to {output_file}\"\n                    )\n\n        else:\n            output_files = [\n                os.path.join(args.output_path, f\"{split}.npz\") for split in splits\n            ]\n            worker_process(\n                args,\n                args.distributed_rank,\n                args.distributed_size,\n                output_files,\n                args.dataset_paths,\n                args.tta,\n            )\n            success = True\n\n    if success:\n        logger.info(\"Inference succeeded.\")\n    else:\n        logger.error(\"Inference FAILED!\")\n\n    if not success or args.gt_path is None:\n        return\n\n    logger.info(\"Evaluating results\")\n\n    video_features_dict = {}\n    for split in splits:\n        video_features_dict[split] = load_features(\n            os.path.join(args.output_path, f\"{split}.npz\")\n        )\n\n    evaluate(\n        queries=video_features_dict[splits[0]],\n        refs=video_features_dict[splits[1]],\n        noises=video_features_dict[splits[-1]],\n        gt_path=args.gt_path,\n        output_path=args.output_path,\n    )", "\n\ndef distributed_worker_process(pargs, rank, world_size, backend, *args, **kwargs):\n    from torch import distributed\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"19529\"\n    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n    worker_process(pargs, rank, world_size, *args, **kwargs)\n", "\n\ndef worker_process(\n    args,\n    rank,\n    world_size,\n    output_files: List[str],\n    dataset_paths: List[str],\n    tta: bool = False,\n):\n    from src.inference_impl import VideoDataset, get_device, run_inference\n    from src.model import create_copy_type_pred_model, create_model_in_runtime\n    from torch.utils.data import DataLoader\n\n    logger.info(f\"Starting worker {rank} of {world_size}.\")\n    device = get_device(args, rank, world_size)\n    logger.info(\"Loading model\")\n    model, transforms = create_model_in_runtime(transforms_device=device)\n    model = model.to(device).eval().half()\n\n    copytype_model = create_copy_type_pred_model(transforms_device=device)\n    copytype_model = copytype_model.to(device).eval()\n\n    logger.info(\"Setting up dataset\")\n    extensions = args.video_extensions.split(\",\")\n    video_reader = VideoReaderType[args.video_reader.upper()]\n\n    if tta:\n        if len(dataset_paths) == 3:\n            do_tta_list = [\n                True,\n                False,\n                False,\n            ]\n        elif len(dataset_paths) == 4:\n            do_tta_list = [\n                True,\n                False,\n                True,\n                False,\n            ]\n        elif len(dataset_paths) == 1:\n            do_tta_list = [\n                True,\n            ]\n        else:\n            raise ValueError(\"TTA requires 3 or 4 datasets\")\n    else:\n        do_tta_list = [False] * len(dataset_paths)\n\n    for output_filename, dataset_path, do_tta in zip(\n        output_files, dataset_paths, do_tta_list\n    ):\n        dataset = VideoDataset(\n            dataset_path,\n            fps=args.fps,\n            read_type=args.read_type,\n            batch_size=1,\n            extensions=extensions,\n            distributed_world_size=world_size,\n            distributed_rank=rank,\n            video_reader=video_reader,\n            ffmpeg_path=args.ffmpeg_path,\n            filter_by_asr=do_tta,\n        )\n        loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\n        progress = tqdm.tqdm(total=dataset.num_videos())\n        video_features = []\n        for vf in run_inference(\n            dataloader=loader,\n            model=model,\n            transforms=transforms,\n            tta=do_tta,\n            copytype_model=copytype_model,\n            batch_size=args.batch_size,\n        ):\n            video_features.append(vf)\n            progress.update()\n\n        store_features(output_filename, video_features)", "\n\ndef evaluate(queries, refs, noises, gt_path, output_path, sn_method=\"SN\"):\n    import faiss\n    from src.postproc import sliding_pca_with_ref\n    from src.score_normalization import (\n        negative_embedding_subtraction,\n        score_normalize_with_ref,\n    )\n    from src.vsc.metrics import (\n        CandidatePair,\n        Match,\n        average_precision,\n    )\n\n    stride = args.stride if args.stride is not None else args.fps\n    video_features, pca_matrix = sliding_pca_with_ref(\n        queries=queries,\n        refs=refs,\n        noises=noises,\n        stride=stride,\n    )\n    queries = video_features[\"query\"]\n    refs = video_features[\"ref\"]\n    noises = video_features[\"noise\"]\n    store_features(Path(output_path) / \"noise.npz\", noises)\n    faiss.write_VectorTransform(pca_matrix, str(Path(output_path) / \"pca_matrix.bin\"))\n\n    if sn_method == \"SN\":\n        queries, refs = score_normalize_with_ref(\n            queries=queries,\n            refs=refs,\n            score_norm_refs=noises,\n            beta=1.0,\n        )\n\n    else:\n        queries, refs = negative_embedding_subtraction(\n            queries=queries,\n            refs=refs,\n            score_norm_refs=noises,\n            pre_l2_normalize=False,\n            post_l2_normalize=False,\n            beta=0.8,\n            k=10,\n            alpha=2.0,\n        )\n\n    store_features(Path(output_path) / \"processed_query.npz\", queries)\n    store_features(Path(output_path) / \"processed_ref.npz\", refs)\n\n    candidates = search(queries, refs)\n    candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\n    gt_matches = Match.read_csv(gt_path, is_gt=True)\n    ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n    CandidatePair.write_csv(candidates, Path(output_path) / \"candidates.csv\")\n    logger.info(f\"uAP: {ap.ap:.4f}\")", "\n\ndef tune(\n    queries,\n    refs,\n    noises,\n    gt_path,\n    output_path,\n):\n    from sklearn.model_selection import ParameterGrid\n    from src.postproc import sliding_pca, sliding_pca_with_ref\n    from src.score_normalization import (\n        negative_embedding_subtraction,\n        score_normalize_with_ref,\n    )\n    from src.vsc.metrics import (\n        CandidatePair,\n        Match,\n        average_precision,\n        evaluate_matching_track,\n    )\n\n    param_grid = [\n        {\n            \"sn_method\": [\"NES\"],\n            \"beta\": [1.2, 1.5],\n            \"stride\": [3],\n            \"fps\": [2],\n            \"k\": [10, 20],\n            \"alpha\": [1.0, 2.0],\n        }\n    ]\n\n    rows = []\n\n    for params in ParameterGrid(param_grid):\n\n        if params[\"fps\"] == 1:\n            _queries = [\n                VideoFeature(\n                    video_id=_.video_id,\n                    feature=_.feature[::2],\n                    timestamps=_.timestamps[::2],\n                )\n                for _ in queries\n            ]\n            _refs = [\n                VideoFeature(\n                    video_id=_.video_id,\n                    feature=_.feature[::2],\n                    timestamps=_.timestamps[::2],\n                )\n                for _ in refs\n            ]\n            _noises = [\n                VideoFeature(\n                    video_id=_.video_id,\n                    feature=_.feature[::2],\n                    timestamps=_.timestamps[::2],\n                )\n                for _ in noises\n            ]\n        else:\n            _queries = queries\n            _refs = refs\n            _noises = noises\n\n        video_features, pca_matrix = sliding_pca_with_ref(\n            queries=_queries,\n            refs=_refs,\n            noises=_noises,\n            stride=params[\"stride\"],\n        )\n        _queries = video_features[\"query\"]\n        _refs = video_features[\"ref\"]\n        _noises = video_features[\"noise\"]\n\n        if params[\"sn_method\"] == \"SN\":\n            norm_queries, norm_refs = score_normalize_with_ref(\n                queries=_queries,\n                refs=_refs,\n                score_norm_refs=_noises,\n                beta=params[\"beta\"],\n            )\n\n        else:\n            norm_queries, norm_refs = negative_embedding_subtraction(\n                queries=_queries,\n                refs=_refs,\n                score_norm_refs=_noises,\n                pre_l2_normalize=False,\n                post_l2_normalize=False,\n                beta=params[\"beta\"],\n                k=params[\"k\"],\n                alpha=params[\"alpha\"],\n            )\n\n        candidates = search(norm_queries, norm_refs)\n        candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\n        gt_matches = Match.read_csv(gt_path, is_gt=True)\n        ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\n        rows.append(\n            {\n                \"uAP\": ap.ap,\n                **params,\n            }\n        )\n        print(rows[-1])\n\n    df = pd.DataFrame(rows)\n    print(df.to_csv())\n    df.to_csv(\"tuning_result.csv\", index=False)", "\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n\n    if args.mode == \"eval\":\n        evaluate(\n            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n            sn_method=\"SN\",\n        )\n    elif args.mode == \"test\":\n        evaluate(\n            queries=load_features(\n                os.path.join(args.output_path, f\"phase_2_uB82_query.npz\")\n            ),\n            refs=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            noises=load_features(\n                os.path.join(args.output_path, f\"train_reference.npz\")\n            ),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n            sn_method=\"SN\",\n        )\n    elif args.mode == \"tune\":\n        tune(\n            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n        )\n    else:\n        main(args)", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/postproc.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nfrom typing import Dict, List\n\nimport numpy as np", "\nimport numpy as np\nfrom numpy.lib.stride_tricks import sliding_window_view\nfrom src.vsc.index import VideoFeature\n\n\ndef _sliding_window_and_concat(\n    vfs: List[VideoFeature], stride: int = 1\n) -> Dict[str, List[VideoFeature]]:\n\n    new_vfs = []\n    kernel = [0.1, 0.2, 0.4, 0.2, 0.1]\n    num_stacks = len(kernel)\n\n    for vf in vfs:\n        n, d = vf.feature.shape\n\n        num_views = 1\n        for i in range(1, len(vf.timestamps)):\n            if vf.timestamps[i] <= vf.timestamps[i - 1]:\n                num_views += 1\n\n        if n / num_views <= stride:\n            continue\n\n        reshaped_feats = vf.feature.reshape(num_views, -1, d)\n        reshaped_ts = vf.timestamps.reshape(num_views, -1)\n\n        new_feats = []\n        new_timestamps = []\n        for i in range(num_views):\n            new_feat = reshaped_feats[i]\n            new_ts = reshaped_ts[i]\n            new_feat = np.concatenate(\n                [new_feat[: num_stacks // 2], new_feat, new_feat[-(num_stacks // 2) :]],\n                axis=0,\n            )\n            new_feat = sliding_window_view(new_feat, num_stacks, axis=0)\n            assert len(new_feat) == len(reshaped_feats[i])\n            if stride > 1:\n                new_feat = new_feat[stride // 2 :: stride]\n                new_ts = new_ts[stride // 2 :: stride]\n            weight = np.array(kernel).reshape(1, 1, -1)\n            new_feat = new_feat * weight\n            new_feat = new_feat.transpose(0, 2, 1).reshape(\n                -1, new_feat.shape[1] * num_stacks\n            )\n            new_feats.append(new_feat)\n            new_timestamps.append(new_ts)\n\n        new_feats = np.concatenate(new_feats, axis=0)\n        new_timestamps = np.concatenate(new_timestamps, axis=0)\n\n        new_vfs.append(\n            VideoFeature(\n                video_id=vf.video_id,\n                timestamps=new_timestamps,\n                feature=new_feats,\n            )\n        )\n\n    return new_vfs", "\n\ndef _fit_pca(noises, n_components=512) -> Dict[str, List[VideoFeature]]:\n    import faiss\n\n    noise_feats = np.concatenate([vf.feature for vf in noises])\n    noise_feats = noise_feats.astype(np.float32)\n    mat = faiss.PCAMatrix(noise_feats.shape[-1], n_components)\n    mat.train(noise_feats)\n    assert mat.is_trained\n    return mat", "\n\ndef _apply_pca(vfs, mat) -> Dict[str, List[VideoFeature]]:\n    new_vfs = []\n    for vf in vfs:\n        new_feat = mat.apply(vf.feature.astype(np.float32))\n        # new_feat = new_feat / np.linalg.norm(new_feat, axis=-1, keepdims=True)\n        new_vfs.append(\n            VideoFeature(\n                video_id=vf.video_id,\n                timestamps=vf.timestamps,\n                feature=new_feat,\n            )\n        )\n    return new_vfs", "\n\ndef sliding_pca(\n    queries: List[VideoFeature],\n    mat: \"faiss.PCAMatrix\",\n    stride: int = 1,\n) -> List[VideoFeature]:\n    queries = _sliding_window_and_concat(queries, stride=stride)\n    queries = _apply_pca(queries, mat)\n    return queries", "\n\ndef sliding_pca_with_ref(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    noises: List[VideoFeature],\n    stride: int = 1,\n    n_components: int = 512,\n) -> Dict[str, List[VideoFeature]]:\n\n    video_features = {\n        \"query\": _sliding_window_and_concat(queries, stride=stride),\n        \"ref\": _sliding_window_and_concat(refs, stride=stride),\n        \"noise\": _sliding_window_and_concat(noises, stride=stride),\n    }\n    mat = _fit_pca(video_features[\"noise\"], n_components=n_components)\n\n    for k, vfs in video_features.items():\n        video_features[k] = _apply_pca(vfs, mat)\n\n    return video_features, mat", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/model.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nfrom __future__ import annotations\n\nimport timm", "\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\n\nclass ISCNet(nn.Module):\n    \"\"\"\n    Feature extractor for image copy-detection task.\n\n    Args:\n        backbone (`nn.Module`):\n            Backbone module.\n        fc_dim (`int=256`):\n            Feature dimension of the fc layer.\n        p (`float=1.0`):\n            Power used in gem pooling for training.\n        eval_p (`float=1.0`):\n            Power used in gem pooling for evaluation. In practice, using a larger power\n            for evaluation than training can yield a better performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        fc_dim: int = 256,\n        p: float = 1.0,\n        eval_p: float = 1.0,\n        l2_normalize=True,\n    ):\n        super().__init__()\n\n        self.backbone = backbone\n        if hasattr(backbone, \"num_features\"):\n            self.is_cnn = False\n            in_channels = backbone.num_features\n        else:\n            self.is_cnn = True\n            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        self.p = p\n        self.eval_p = eval_p\n        self.l2_normalize = l2_normalize\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.is_cnn:\n            batch_size = x.shape[0]\n            x = self.backbone(x)[-1]\n            p = self.p if self.training else self.eval_p\n            x = gem(x, p).view(batch_size, -1)\n        else:\n            x = self.backbone(x)\n        x = self.fc(x)\n        x = self.bn(x)\n        if self.l2_normalize:\n            x = F.normalize(x)\n        return x", "class ISCNet(nn.Module):\n    \"\"\"\n    Feature extractor for image copy-detection task.\n\n    Args:\n        backbone (`nn.Module`):\n            Backbone module.\n        fc_dim (`int=256`):\n            Feature dimension of the fc layer.\n        p (`float=1.0`):\n            Power used in gem pooling for training.\n        eval_p (`float=1.0`):\n            Power used in gem pooling for evaluation. In practice, using a larger power\n            for evaluation than training can yield a better performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        fc_dim: int = 256,\n        p: float = 1.0,\n        eval_p: float = 1.0,\n        l2_normalize=True,\n    ):\n        super().__init__()\n\n        self.backbone = backbone\n        if hasattr(backbone, \"num_features\"):\n            self.is_cnn = False\n            in_channels = backbone.num_features\n        else:\n            self.is_cnn = True\n            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        self.p = p\n        self.eval_p = eval_p\n        self.l2_normalize = l2_normalize\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.is_cnn:\n            batch_size = x.shape[0]\n            x = self.backbone(x)[-1]\n            p = self.p if self.training else self.eval_p\n            x = gem(x, p).view(batch_size, -1)\n        else:\n            x = self.backbone(x)\n        x = self.fc(x)\n        x = self.bn(x)\n        if self.l2_normalize:\n            x = F.normalize(x)\n        return x", "\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\n\nclass ConstDivider(nn.Module):\n    def __init__(self, c=255.0):\n        super().__init__()\n        self.c = c\n\n    def forward(self, x):\n        return x / self.c", "\n\ndef create_model_in_runtime(transforms_device=\"cpu\"):\n    weight_path = \"./model_assets/isc_ft_v107.pth.tar\"\n    ckpt = torch.load(weight_path)\n    arch = ckpt[\"arch\"]  # tf_efficientnetv2_m_in21ft1k\n    input_size = ckpt[\"args\"].input_size\n\n    backbone = timm.create_model(arch, features_only=True)\n    model = ISCNet(\n        backbone=backbone,\n        fc_dim=256,\n        p=1.0,\n        eval_p=1.0,\n        l2_normalize=True,\n    )\n    model.to(\"cuda\").train(False)\n\n    state_dict = {}\n    for s in ckpt[\"state_dict\"]:\n        state_dict[s.replace(\"module.\", \"\")] = ckpt[\"state_dict\"][s]\n\n    model.load_state_dict(state_dict)\n\n    if transforms_device == \"cpu\":\n        preprocessor = transforms.Compose(\n            [\n                transforms.Resize((input_size, input_size)),\n                # transforms.ToTensor(),\n                ConstDivider(c=255.0),\n                transforms.Normalize(\n                    mean=backbone.default_cfg[\"mean\"],\n                    std=backbone.default_cfg[\"std\"],\n                ),\n            ]\n        )\n    else:\n        preprocessor = nn.Sequential(\n            transforms.Resize((input_size, input_size)),\n            ConstDivider(c=255.0),\n            transforms.Normalize(\n                mean=backbone.default_cfg[\"mean\"],\n                std=backbone.default_cfg[\"std\"],\n            ),\n        )\n        preprocessor = torch.jit.script(preprocessor)\n        preprocessor.to(transforms_device)\n\n    return model, preprocessor", "\n\nclass CopyTypePredModel(nn.Module):\n    def __init__(self, model_name, labels, transforms):\n        super().__init__()\n        self.model = timm.create_model(model_name, num_classes=len(labels))\n        self.transforms = transforms\n        self.labels = labels\n\n    def forward(self, x):\n        x = self.transforms(x)\n        logit = self.model(x)\n        proba = torch.sigmoid(logit)\n        return {l: proba[:, i] for i, l in enumerate(self.labels)}", "\n\ndef create_copy_type_pred_model(transforms_device=\"cpu\"):\n    weight_path = \"./model_assets/copy_type_pred__convnext_clip.ckpt\"\n    state_dict = torch.load(weight_path, map_location=\"cpu\")[\"state_dict\"]\n    arch = \"convnext_base_in22ft1k\"\n    mean = (0.48145466, 0.4578275, 0.40821073)\n    std = (0.26862954, 0.26130258, 0.27577711)\n    input_size = (256, 256)\n\n    labels = [\n        \"add_noise\",\n        \"blend_videos\",\n        \"blur\",\n        \"brightness\",\n        \"change_aspect_ratio\",\n        \"color_jitter\",\n        \"contrast\",\n        \"crop\",\n        \"encoding_quality\",\n        \"grayscale\",\n        \"hstack\",\n        \"other\",\n        \"overlay\",\n        \"overlay_dots\",\n        \"overlay_emoji\",\n        \"overlay_shapes\",\n        \"overlay_text\",\n        \"pad\",\n        \"perspective_transform_and_shake\",\n        \"pixelization\",\n        \"rotate\",\n        \"rotate180_vflip\",\n        \"rotate90_270\",\n        \"vstack\",\n        \"vstack_hstack\",\n    ]\n\n    if transforms_device == \"cpu\":\n        preprocessor = transforms.Compose(\n            [\n                transforms.ConvertImageDtype(torch.float32),\n                transforms.Normalize(mean=mean, std=std),\n                transforms.Resize(size=input_size),\n            ]\n        )\n    else:\n        preprocessor = nn.Sequential(\n            transforms.ConvertImageDtype(torch.float32),\n            transforms.Normalize(mean=mean, std=std),\n            transforms.Resize(size=input_size),\n        )\n        preprocessor = torch.jit.script(preprocessor)\n        preprocessor.to(transforms_device)\n\n    model = CopyTypePredModel(arch, labels=labels, transforms=preprocessor)\n    model.load_state_dict(state_dict)\n    model.to(\"cuda\").train(False)\n\n    return model", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Inference script.\n\nThis is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors", "This is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors\nin some runtime environments.\n\nWe import inference_impl, which imports libraries that may initialize cuda,\nin two circumstances: from worker processes after the main process has\nforked workers, or from the main process after worker processes have been\njoined.\n\"\"\"\n", "\"\"\"\n\nimport argparse\nimport enum\nimport logging\nimport os\nimport tempfile\n\nfrom torch import multiprocessing\n", "from torch import multiprocessing\n\n\nclass InferenceTransforms(enum.Enum):\n    # Aspect-ratio preserving resize to 288\n    RESIZE_288 = enum.auto()\n    # Resize the short edge to 320, then take the center crop\n    RESIZE_320_CENTER = enum.auto()\n\n\nclass Accelerator(enum.Enum):\n    CPU = enum.auto()\n    CUDA = enum.auto()", "\n\nclass Accelerator(enum.Enum):\n    CPU = enum.auto()\n    CUDA = enum.auto()\n\n\nclass VideoReaderType(enum.Enum):\n    FFMPEG = enum.auto()\n    FFMPEGPY = enum.auto()\n    DECORD = enum.auto()", "\n\nparser = argparse.ArgumentParser()\ninference_parser = parser.add_argument_group(\"Inference\")\n# inference_parser.add_argument(\"--torchscript_path\", required=True)\ninference_parser.add_argument(\"--batch_size\", type=int, default=32)\ninference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\ninference_parser.add_argument(\"--distributed_size\", type=int, default=1)\ninference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(", "inference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(\n    \"--transforms\",\n    choices=[x.name for x in InferenceTransforms],\n    default=\"RESIZE_320_CENTER\",\n)\ninference_parser.add_argument(\n    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n)\ninference_parser.add_argument(\"--output_file\", required=True)", ")\ninference_parser.add_argument(\"--output_file\", required=True)\ninference_parser.add_argument(\"--scratch_path\", required=False)\n\ndataset_parser = parser.add_argument_group(\"Dataset\")\ndataset_parser.add_argument(\"--dataset_path\", required=True)\ndataset_parser.add_argument(\"--fps\", default=1, type=float)\ndataset_parser.add_argument(\"--stride\", type=int)\ndataset_parser.add_argument(\"--len_cap\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)", "dataset_parser.add_argument(\"--len_cap\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)\ndataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\ndataset_parser.add_argument(\n    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEG\"\n)\ndataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\ndataset_parser.add_argument(\"--score_norm_features\", default=\"./noise.npz\")\ndataset_parser.add_argument(\n    \"--reference_features\", default=\"./reference_descriptor.npz\"", "dataset_parser.add_argument(\n    \"--reference_features\", default=\"./reference_descriptor.npz\"\n)\ndataset_parser.add_argument(\"--pca_matrix\", default=\"./pca_matrix.bin\")\ndataset_parser.add_argument(\"--gt_path\")\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",", "    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference.py\")\nlogger.setLevel(logging.INFO)\n\n\ndef main(args):\n    success = False\n    if args.processes > 1 and args.distributed_size > 1:\n        raise Exception(\n            \"Set either --processes (single-machine distributed) or \"\n            \"both --distributed_size and --distributed_rank (arbitrary \"\n            \"distributed)\"\n        )\n\n    if args.video_reader == \"DECORD\":\n        import subprocess\n\n        subprocess.run(\n            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n            check=True,\n        )\n\n    with tempfile.TemporaryDirectory() as tmp_path:\n        os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n        if args.scratch_path:\n            os.makedirs(args.scratch_path, exist_ok=True)\n        else:\n            args.scratch_path = tmp_path\n        if args.processes > 1:\n            processes = []\n            logger.info(f\"Spawning {args.processes} processes\")\n            accelerator = Accelerator[args.accelerator.upper()]\n            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n            multiprocessing.set_start_method(\"spawn\")\n            worker_files = []\n            try:\n                for rank in range(args.processes):\n                    worker_file = os.path.join(args.scratch_path, f\"{rank}.npz\")\n                    worker_files.append(worker_file)\n                    p = multiprocessing.Process(\n                        target=distributed_worker_process,\n                        args=(args, rank, args.processes, backend, worker_file),\n                    )\n                    processes.append(p)\n                    p.start()\n                worker_success = []\n                for p in processes:\n                    p.join()\n                    worker_success.append(p.exitcode == os.EX_OK)\n                success = all(worker_success)\n            finally:\n                for p in processes:\n                    p.kill()\n            if success:\n                from .inference_impl import merge_feature_files  # @manual\n\n                num_files = merge_feature_files(worker_files, args.output_file)\n                logger.info(\n                    f\"Features for {num_files} videos saved to {args.output_file}\"\n                )\n\n        else:\n            worker_process(\n                args, args.distributed_rank, args.distributed_size, args.output_file\n            )\n            success = True\n\n    if success:\n        logger.info(\"Inference succeeded.\")\n    else:\n        logger.error(\"Inference FAILED!\")", "\n\ndef distributed_worker_process(args, rank, world_size, backend, output_filename):\n    from torch import distributed\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"19529\"\n    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n    worker_process(args, rank, world_size, output_filename)\n", "\n\ndef worker_process(*args):\n    # Late import: initialize cuda after worker spawn.\n    from src.inference_impl import worker_process as worker_impl  # @manual\n\n    return worker_impl(*args)\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)", "\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference_impl.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport glob\nimport itertools\nimport logging\nimport os\nfrom pathlib import Path", "import os\nfrom pathlib import Path\nfrom typing import Iterable, List, Optional, Tuple\n\nimport faiss\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport tqdm", "import torch.nn as nn\nimport tqdm\nfrom src.inference import Accelerator, VideoReaderType\nfrom src.model import create_copy_type_pred_model, create_model_in_runtime\nfrom src.postproc import sliding_pca\nfrom src.score_normalization import score_normalize\nfrom src.tta import (\n    TTA4ViewsTransform,\n    TTA5ViewsTransform,\n    TTAHorizontalStackTransform,", "    TTA5ViewsTransform,\n    TTAHorizontalStackTransform,\n    TTAVerticalStackTransform,\n)\nfrom src.video_reader.ffmpeg_py_video_reader import FFMpegPyVideoReader\nfrom src.video_reader.ffmpeg_video_reader import FFMpegVideoReader\nfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.metrics import (\n    CandidatePair,", "from src.vsc.metrics import (\n    CandidatePair,\n    Match,\n    average_precision,\n    evaluate_matching_track,\n)\nfrom src.vsc.storage import load_features, store_features\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom torch.utils.data._utils.collate import default_collate\nfrom torchvision import transforms", "from torch.utils.data._utils.collate import default_collate\nfrom torchvision import transforms\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference_impl.py\")\nlogger.setLevel(logging.INFO)", "logger = logging.getLogger(\"inference_impl.py\")\nlogger.setLevel(logging.INFO)\n\n\nclass VideoDataset(IterableDataset):\n    \"\"\"Decodes video frames at a fixed FPS via ffmpeg.\"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        fps: float,\n        read_type: str = \"pil\",\n        batch_size=None,\n        img_transform=None,\n        extensions=(\"mp4\",),\n        distributed_rank=0,\n        distributed_world_size=1,\n        video_reader=VideoReaderType.FFMPEG,\n        ffmpeg_path=\"ffmpeg\",\n        filter_by_asr=False,\n    ):\n        assert distributed_rank < distributed_world_size\n        self.path = path\n        self.fps = fps\n        self.read_type = read_type\n        self.batch_size = batch_size\n        self.img_transform = img_transform\n        self.video_reader = video_reader\n        self.ffmpeg_path = ffmpeg_path\n        if len(extensions) == 1:\n            filenames = glob.glob(os.path.join(path, f\"*.{extensions[0]}\"))\n        else:\n            filenames = glob.glob(os.path.join(path, \"*.*\"))\n            filenames = (fn for fn in filenames if fn.rsplit(\".\", 1)[-1] in extensions)\n\n        filenames = [\n            name for name in filenames if Path(name).stem not in [\"R102796\", \"R133364\"]\n        ]\n        self.videos = sorted(filenames)\n\n        if not self.videos:\n            raise Exception(\"No videos found!\")\n        assert distributed_rank < distributed_world_size\n        self.rank = distributed_rank\n        self.world_size = distributed_world_size\n        self.selected_videos = [\n            (i, video)\n            for (i, video) in enumerate(self.videos)\n            if (i % self.world_size) == self.rank\n        ]\n\n    def num_videos(self) -> int:\n        return len(self.selected_videos)\n\n    def __iter__(self):\n        for i, video in self.selected_videos:\n            if self.batch_size:\n                frames = self.read_frames(i, video)\n                while True:\n                    batch = list(itertools.islice(frames, self.batch_size))\n                    if not batch:\n                        break\n                    yield default_collate(batch)\n            else:\n                yield from self.read_frames(i, video)\n\n    def read_frames(self, video_id, video):\n        video_name = os.path.basename(video)\n        name = os.path.basename(video_name).split(\".\")[0]\n        if self.video_reader == VideoReaderType.FFMPEG:\n            reader = FFMpegVideoReader(\n                video_path=video,\n                required_fps=self.fps,\n                output_type=self.read_type,\n                ffmpeg_path=self.ffmpeg_path,\n            )\n        elif self.video_reader == VideoReaderType.FFMPEGPY:\n            reader = FFMpegPyVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        elif self.video_reader == VideoReaderType.DECORD:\n            from src.video_reader.decord_video_reader import DecordVideoReader\n\n            reader = DecordVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        else:\n            raise ValueError(f\"VideoReaderType: {self.video_reader} not supported\")\n        for start_timestamp, end_timestamp, frame in reader.frames():\n            if self.img_transform:\n                frame = self.img_transform(frame)\n            record = {\n                \"name\": name,\n                \"timestamp\": (np.array(start_timestamp) + np.array(end_timestamp)) / 2,\n                \"input\": frame,\n            }\n            yield record", "\n\ndef should_use_cuda(args) -> bool:\n    accelerator = Accelerator[args.accelerator.upper()]\n    return accelerator == Accelerator.CUDA\n\n\ndef get_device(args, rank, world_size):\n    if should_use_cuda(args):\n        assert torch.cuda.is_available()\n        num_devices = torch.cuda.device_count()\n        if args.processes > num_devices:\n            raise Exception(\n                f\"Asked for {args.processes} processes and cuda, but only \"\n                f\"{num_devices} devices found\"\n            )\n        if args.processes > 1 or world_size <= num_devices:\n            device_num = rank\n        else:\n            device_num = 0\n        torch.cuda.set_device(device_num)\n        return torch.device(\"cuda\", device_num)\n    return torch.device(\"cpu\")", "\n\ndef search(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    retrieve_per_query: float = 1200.0,\n    candidates_per_query: float = 25.0,\n) -> List[CandidatePair]:\n    aggregation = MaxScoreAggregation()\n    logger.info(\"Searching\")\n    cg = CandidateGeneration(refs, aggregation)\n    num_to_retrieve = int(retrieve_per_query * len(queries))\n    candidates = cg.query(queries, global_k=num_to_retrieve)\n    num_candidates = int(candidates_per_query * len(queries))\n    candidates = candidates[:num_candidates]\n    logger.info(\"Got %d candidates\", len(candidates))\n    return candidates", "\n\ndef worker_process(args, rank, world_size, output_filename):\n    logger.info(f\"Starting worker {rank} of {world_size}.\")\n    device = get_device(args, rank, world_size)\n\n    logger.info(\"Loading model\")\n    model, transforms = create_model_in_runtime(transforms_device=device)\n    model = model.to(device).eval().half()\n\n    copytype_model = create_copy_type_pred_model(transforms_device=device)\n    copytype_model = copytype_model.to(device).eval()\n\n    logger.info(\"Setting up dataset\")\n    extensions = args.video_extensions.split(\",\")\n    video_reader = VideoReaderType[args.video_reader.upper()]\n    dataset = VideoDataset(\n        args.dataset_path,\n        fps=args.fps,\n        read_type=args.read_type,\n        # img_transform=transforms,\n        # batch_size=args.batch_size,\n        batch_size=1,\n        extensions=extensions,\n        distributed_world_size=world_size,\n        distributed_rank=rank,\n        video_reader=video_reader,\n        ffmpeg_path=args.ffmpeg_path,\n        filter_by_asr=False,\n    )\n    loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\n    progress = tqdm.tqdm(total=dataset.num_videos())\n    queries = []\n    for vf in run_inference(\n        dataloader=loader,\n        model=model,\n        transforms=transforms,\n        tta=True,\n        copytype_model=copytype_model,\n        batch_size=args.batch_size,\n    ):\n        queries.append(vf)\n        progress.update()\n\n    del loader\n    del model\n    del copytype_model\n    del dataset\n\n    alpha = 1.0\n    factors = [(1 / (vf.feature @ vf.feature.T).mean()) ** alpha for vf in queries]\n\n    if args.score_norm_features:\n        stride = args.stride if args.stride is not None else args.fps\n        print(\"stride\", stride, args.stride, args.fps)\n        pca_matrix = faiss.read_VectorTransform(args.pca_matrix)\n        queries = sliding_pca(queries=queries, mat=pca_matrix, stride=stride)\n\n        queries = score_normalize(\n            queries,\n            load_features(args.score_norm_features),\n            beta=1.0,\n        )\n\n    queries = [\n        VideoFeature(\n            video_id=vf.video_id,\n            timestamps=vf.timestamps,\n            feature=vf.feature * factor,\n        )\n        for vf, factor in zip(queries, factors)\n    ]\n\n    store_features(output_filename, queries)\n    logger.info(\n        f\"Wrote worker {rank} features for {len(queries)} videos to {output_filename}\"\n    )", "\n\ndef video_level_loader(\n    frame_level_dataloader: DataLoader,\n) -> Iterable[Tuple[str, torch.Tensor, torch.Tensor]]:\n    name, imgs, timestamps = None, [], []\n\n    for frames in frame_level_dataloader:\n        _names, _imgs, _ts = frames[\"name\"], frames[\"input\"], frames[\"timestamp\"]\n\n        assert _names[0] == _names[-1]  # single-video batches\n\n        _name = _names[0]\n        if name is not None and name != _name:\n            yield name, torch.concat(imgs), torch.concat(timestamps)\n            name, imgs, timestamps = _name, [_imgs], [_ts]\n        else:\n            name = _name\n            imgs.append(_imgs)\n            timestamps.append(_ts)\n\n    yield name, torch.concat(imgs), torch.concat(timestamps)", "\n\ndef batch_forward(\n    model: nn.Module, inputs: torch.Tensor, batch_size: int, transforms=None\n) -> torch.Tensor:\n    device = next(model.parameters()).device\n    dtype = next(model.parameters()).dtype\n\n    outputs = []\n    for i in range(0, len(inputs), batch_size):\n        x = inputs[i : i + batch_size]\n\n        if transforms is not None:\n            x = transforms(x)\n\n        if dtype == torch.float16:\n            x = x.half()\n\n        x = model(x.to(device))\n\n        if isinstance(x, dict):\n            x = {k: v.cpu() for k, v in x.items()}\n        else:\n            x = x.cpu()\n        outputs.append(x)\n\n    if isinstance(outputs[0], dict):\n        output_dict = {}\n        for k in outputs[0]:\n            output_dict[k] = torch.cat([d[k] for d in outputs], dim=0)\n        return output_dict\n    else:\n        return torch.cat(outputs, dim=0)", "\n\n@torch.no_grad()\ndef run_inference(\n    dataloader, model, transforms=None, tta=True, copytype_model=None, batch_size=32\n) -> Iterable[VideoFeature]:\n    for name, imgs, timestamps in video_level_loader(dataloader):\n        n_imgs = len(imgs)\n\n        _transforms = transforms\n\n        thresh = 0.15\n        # Extend transforms\n        if _transforms is not None and tta:\n\n            if copytype_model is not None:\n                pred = batch_forward(copytype_model, imgs, batch_size=batch_size)\n                assert len(pred[\"hstack\"]) == len(\n                    imgs\n                ), f\"{len(pred['hstack'])=} != {len(imgs)=}\"\n                assert len(pred[\"vstack\"]) == len(\n                    imgs\n                ), f\"{len(pred['vstack'])=} != {len(imgs)=}\"\n\n                vstack_hstack_prob = torch.median(pred[\"vstack_hstack\"])\n                hstack_prob = torch.median(pred[\"hstack\"])\n                vstack_prob = torch.median(pred[\"vstack\"])\n\n                if vstack_hstack_prob > thresh:\n                    _transforms = TTA4ViewsTransform(_transforms)\n                    print('vstack_hstack', vstack_hstack_prob)\n                elif hstack_prob > thresh:\n                    _transforms = TTAHorizontalStackTransform(_transforms)\n                elif vstack_prob > thresh:\n                    _transforms = TTAVerticalStackTransform(_transforms)\n            # if copytype model is not given\n            else:\n                _transforms = TTA5ViewsTransform(_transforms)\n\n        if _transforms is None:\n            _transforms = torch.nn.Identity()\n\n        feature = batch_forward(\n            model,\n            imgs,\n            batch_size=batch_size,\n            transforms=_transforms,\n        ).numpy()\n\n        num_views = len(feature) // n_imgs\n        timestamps = np.tile(timestamps.numpy(), num_views)\n\n        yield VideoFeature(\n            video_id=name,\n            timestamps=timestamps,\n            feature=feature,\n        )", "\n\ndef merge_feature_files(filenames: List[str], output_filename: str) -> int:\n    features = []\n    for fn in filenames:\n        features.extend(load_features(fn))\n    store_features(output_filename, features)\n    return len(features)\n\n\ndef validate_total_descriptors(\n    video_features: List[VideoFeature], meta: Optional[pd.DataFrame] = None\n):\n    _ids = [_.video_id for _ in video_features]\n    n_features = sum([_.feature.shape[0] for _ in video_features])\n\n    if meta is None:\n        meta_root = Path(\"./metadata_root\")\n        meta_paths = [\n            meta_root / \"train\" / \"train_query_metadata.csv\",\n            meta_root / \"train\" / \"train_reference_metadata.csv\",\n            meta_root / \"test\" / \"test_query_metadata.csv\",\n            meta_root / \"test\" / \"test_reference_metadata.csv\",\n        ]\n        meta = pd.concat([pd.read_csv(path) for path in meta_paths])\n\n    meta = meta.set_index(\"video_id\").loc[_ids]\n    total_seconds = meta.duration_sec.apply(np.ceil).sum()\n\n    logger.info(f\"Saw {n_features} vectors, max allowed is {total_seconds}\")\n    if n_features > total_seconds:\n        logger.info(\"*Warning*: Too many vectors\")", "\n\ndef validate_total_descriptors(\n    video_features: List[VideoFeature], meta: Optional[pd.DataFrame] = None\n):\n    _ids = [_.video_id for _ in video_features]\n    n_features = sum([_.feature.shape[0] for _ in video_features])\n\n    if meta is None:\n        meta_root = Path(\"./metadata_root\")\n        meta_paths = [\n            meta_root / \"train\" / \"train_query_metadata.csv\",\n            meta_root / \"train\" / \"train_reference_metadata.csv\",\n            meta_root / \"test\" / \"test_query_metadata.csv\",\n            meta_root / \"test\" / \"test_reference_metadata.csv\",\n        ]\n        meta = pd.concat([pd.read_csv(path) for path in meta_paths])\n\n    meta = meta.set_index(\"video_id\").loc[_ids]\n    total_seconds = meta.duration_sec.apply(np.ceil).sum()\n\n    logger.info(f\"Saw {n_features} vectors, max allowed is {total_seconds}\")\n    if n_features > total_seconds:\n        logger.info(\"*Warning*: Too many vectors\")", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/score_normalization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport dataclasses\nimport logging\nfrom typing import Callable, List, Tuple\n\nimport faiss  # @manual", "\nimport faiss  # @manual\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\nfrom src.vsc.index import VideoFeature\n\nlogger = logging.getLogger(\"score_normalization.py\")\nlogger.setLevel(logging.INFO)\n", "logger.setLevel(logging.INFO)\n\n\ndef transform_features(\n    features: List[VideoFeature], transform: Callable\n) -> List[VideoFeature]:\n    return [\n        dataclasses.replace(feature, feature=transform(feature.feature))\n        for feature in features\n    ]", "\n\ndef score_normalize(\n    queries: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    l2_normalize: bool = True,\n    replace_dim: bool = True,\n    beta: float = 1.0,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    \"\"\"\n    CSLS style score normalization (as used in the Image Similarity Challenge)\n    has the following form. We compute a bias term for each query:\n\n      bias(query) = - beta * sim(query, noise)\n\n    then compute score normalized similarity by incorporating this as an\n    additive term for each query:\n\n      sim_sn(query, ref) = sim(query, ref) + bias(query)\n\n    sim(query, ref) is inner product similarity (query * ref), and\n    sim(query, noise) is some function of query similarity to a noise dataset\n    (score_norm_refs here), such as the similarity to the nearest neighbor.\n\n    We encode the bias term as an extra dimension in the query descriptor,\n    and add a constant 1 dimension to reference descriptors, so that inner-\n    product similarity is the score-normalized similarity:\n\n      query' = [query bias(query)]\n      ref' = [ref 1]\n      query' * ref' = (query * ref) + (bias(query) * 1)\n          = sim(query, ref) + bias(query) = sim_sn(query, ref)\n    \"\"\"\n    if score_norm_refs is not None and replace_dim:\n        # Make space for the additional score normalization dimension.\n        # We could also use PCA dim reduction, but re-centering can be\n        # destructive.\n        logger.info(\"Replacing dimension\")\n        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n        low_var_dim = sn_features.var(axis=0).argmin()\n        queries, score_norm_refs = [\n            transform_features(\n                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n            )\n            for x in [queries, score_norm_refs]\n        ]\n    if l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, score_norm_refs]\n        ]\n    logger.info(\"Applying score normalization\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    adapted_queries = []\n    # Add the additive normalization term to the queries as an extra dimension.\n    for query in queries:\n        # KNN search is ok here (versus a threshold/radius/range search) since\n        # we're not searching the dataset we're evaluating on.\n        similarity, ids = index.search(query.feature, 1)\n        norm_term = -beta * similarity[:, :1]\n        feature = np.concatenate([query.feature, norm_term], axis=1)\n        adapted_queries.append(dataclasses.replace(query, feature=feature))\n    return adapted_queries", "\n\ndef score_normalize_with_ref(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    l2_normalize: bool = True,\n    replace_dim: bool = True,\n    beta: float = 1.0,\n    return_adapted_score_norm_refs: bool = False,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    if {f.video_id for f in refs}.intersection({f.video_id for f in score_norm_refs}):\n        raise Exception(\n            \"Normalizing on the dataset we're evaluating on is against VSC rules. \"\n            \"An independent dataset is needed.\"\n        )\n    if score_norm_refs is not None and replace_dim:\n        # Make space for the additional score normalization dimension.\n        # We could also use PCA dim reduction, but re-centering can be\n        # destructive.\n        logger.info(\"Replacing dimension\")\n        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n        low_var_dim = sn_features.var(axis=0).argmin()\n        queries, refs, score_norm_refs = [\n            transform_features(\n                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n            )\n            for x in [queries, refs, score_norm_refs]\n        ]\n    if l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, refs, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n        ]\n    logger.info(\"Applying score normalization\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    adapted_queries = []\n    # Add the additive normalization term to the queries as an extra dimension.\n    for query in queries:\n        # KNN search is ok here (versus a threshold/radius/range search) since\n        # we're not searching the dataset we're evaluating on.\n        similarity, ids = index.search(query.feature, 1)\n        norm_term = -beta * similarity[:, :1]\n        feature = np.concatenate([query.feature, norm_term], axis=1)\n        adapted_queries.append(dataclasses.replace(query, feature=feature))\n    adapted_refs = []\n    for ref in refs:\n        ones = np.ones_like(ref.feature[:, :1])\n        feature = np.concatenate([ref.feature, ones], axis=1)\n        adapted_refs.append(dataclasses.replace(ref, feature=feature))\n    output = (adapted_queries, adapted_refs)\n\n    if return_adapted_score_norm_refs:\n        adapted_score_norm_refs = []\n        for score_norm_ref in score_norm_refs:\n            ones = np.ones_like(score_norm_ref.feature[:, :1])\n            feature = np.concatenate([score_norm_ref.feature, ones], axis=1)\n            adapted_score_norm_refs.append(\n                dataclasses.replace(score_norm_ref, feature=feature)\n            )\n        output += (adapted_score_norm_refs,)\n\n    return output", "\n\ndef negative_embedding_subtraction(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    pre_l2_normalize: bool = False,\n    post_l2_normalize: bool = False,\n    beta: float = 1.0,\n    k: int = 10,\n    alpha: float = 1.0,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    # impl of https://arxiv.org/abs/2112.04323\n\n    if pre_l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, refs, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n        ]\n\n    logger.info(\"Applying negative embedding subtraction\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    negative_embeddings = np.concatenate([vf.feature for vf in score_norm_refs], axis=0)\n\n    adapted_queries = []\n    for query in queries:\n        similarity, ids = index.search(query.feature, k=k)\n        weights = similarity[..., None] ** alpha\n        topk_negative_embeddings = negative_embeddings[ids] * weights\n        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n        adapted_embedding = query.feature - subtracted_embedding\n        adapted_queries.append(dataclasses.replace(query, feature=adapted_embedding))\n\n    adapted_refs = []\n    for ref in refs:\n        similarity, ids = index.search(ref.feature, k=k)\n        weights = similarity[..., None] ** alpha\n        topk_negative_embeddings = negative_embeddings[ids] * weights\n        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n        adapted_embedding = ref.feature - subtracted_embedding\n        adapted_refs.append(dataclasses.replace(ref, feature=adapted_embedding))\n\n    if post_l2_normalize:\n        logger.info(\"L2 normalizing\")\n        adapted_queries, adapted_refs = [\n            transform_features(x, normalize) for x in [adapted_queries, adapted_refs]\n        ]\n\n    return adapted_queries, adapted_refs", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/tta.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torchvision", "import torch.nn as nn\nimport torchvision\n\n\nclass TTA30ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top = x[..., : h // 2, :]  # top\n        x_bottom = x[..., h // 2 :, :]  # bottom\n        x_left = x[..., :, : w // 2]  # left\n        x_right = x[..., :, w // 2 :]  # right\n\n        if self.base_transforms is not None:\n            x = self.base_transforms(x)\n            x_top = self.base_transforms(x_top)\n            x_bottom = self.base_transforms(x_bottom)\n            x_left = self.base_transforms(x_left)\n            x_right = self.base_transforms(x_right)\n\n        crops = [\n            x,\n            torchvision.transforms.functional.rotate(x, angle=90),\n            torchvision.transforms.functional.rotate(x, angle=180),\n            torchvision.transforms.functional.rotate(x, angle=270),\n            torchvision.transforms.functional.hflip(x),\n            torchvision.transforms.functional.vflip(x),\n            x_top,\n            torchvision.transforms.functional.rotate(x_top, angle=90),\n            torchvision.transforms.functional.rotate(x_top, angle=180),\n            torchvision.transforms.functional.rotate(x_top, angle=270),\n            torchvision.transforms.functional.hflip(x_top),\n            torchvision.transforms.functional.vflip(x_top),\n            x_bottom,\n            torchvision.transforms.functional.rotate(x_bottom, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom),\n            torchvision.transforms.functional.vflip(x_bottom),\n            x_left,\n            torchvision.transforms.functional.rotate(x_left, angle=90),\n            torchvision.transforms.functional.rotate(x_left, angle=180),\n            torchvision.transforms.functional.rotate(x_left, angle=270),\n            torchvision.transforms.functional.hflip(x_left),\n            torchvision.transforms.functional.vflip(x_left),\n            x_right,\n            torchvision.transforms.functional.rotate(x_right, angle=90),\n            torchvision.transforms.functional.rotate(x_right, angle=180),\n            torchvision.transforms.functional.rotate(x_right, angle=270),\n            torchvision.transforms.functional.hflip(x_right),\n            torchvision.transforms.functional.vflip(x_right),\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA24ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\n        if self.base_transforms is not None:\n            x_top_left = self.base_transforms(x_top_left)\n            x_top_right = self.base_transforms(x_top_right)\n            x_bottom_left = self.base_transforms(x_bottom_left)\n            x_bottom_right = self.base_transforms(x_bottom_right)\n\n        crops = [\n            x_top_left,\n            torchvision.transforms.functional.rotate(x_top_left, angle=90),\n            torchvision.transforms.functional.rotate(x_top_left, angle=180),\n            torchvision.transforms.functional.rotate(x_top_left, angle=270),\n            torchvision.transforms.functional.hflip(x_top_left),\n            torchvision.transforms.functional.vflip(x_top_left),\n            x_top_right,\n            torchvision.transforms.functional.rotate(x_top_right, angle=90),\n            torchvision.transforms.functional.rotate(x_top_right, angle=180),\n            torchvision.transforms.functional.rotate(x_top_right, angle=270),\n            torchvision.transforms.functional.hflip(x_top_right),\n            torchvision.transforms.functional.vflip(x_top_right),\n            x_bottom_left,\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom_left),\n            torchvision.transforms.functional.vflip(x_bottom_left),\n            x_bottom_right,\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom_right),\n            torchvision.transforms.functional.vflip(x_bottom_right),\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA5ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top = x[..., : h // 2, :]  # top\n        x_bottom = x[..., h // 2 :, :]  # bottom\n        x_left = x[..., :, : w // 2]  # left\n        x_right = x[..., :, w // 2 :]  # right\n\n        if self.base_transforms is not None:\n            x = self.base_transforms(x)\n            x_top = self.base_transforms(x_top)\n            x_bottom = self.base_transforms(x_bottom)\n            x_left = self.base_transforms(x_left)\n            x_right = self.base_transforms(x_right)\n\n        crops = [\n            x,\n            x_top,\n            x_bottom,\n            x_left,\n            x_right,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA4ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\n        if self.base_transforms is not None:\n            x_top_left = self.base_transforms(x_top_left)\n            x_top_right = self.base_transforms(x_top_right)\n            x_bottom_left = self.base_transforms(x_bottom_left)\n            x_bottom_right = self.base_transforms(x_bottom_right)\n\n        crops = [\n            x_top_left,\n            x_top_right,\n            x_bottom_left,\n            x_bottom_right,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTAHorizontalStackTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_left = x[..., :, : w // 2]  # left\n        x_right = x[..., :, w // 2 :]  # right\n\n        if self.base_transforms is not None:\n            x_left = self.base_transforms(x_left)\n            x_right = self.base_transforms(x_right)\n\n        crops = [\n            x_left,\n            x_right,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTAVerticalStackTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top = x[..., : h // 2, :]  # top\n        x_bottom = x[..., h // 2 :, :]  # bottom\n\n        if self.base_transforms is not None:\n            x_top = self.base_transforms(x_top)\n            x_bottom = self.base_transforms(x_bottom)\n\n        crops = [\n            x_top,\n            x_bottom,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/metrics.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport dataclasses\nimport enum\nimport itertools\nfrom collections import defaultdict", "import itertools\nfrom collections import defaultdict\nfrom math import sqrt\nfrom typing import Collection, Dict, List, NamedTuple, Optional, TextIO, Tuple, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import average_precision_score\n", "from sklearn.metrics import average_precision_score\n\n\nclass Dataset(enum.Enum):\n    QUERIES = \"Q\"\n    REFS = \"R\"\n\n\ndef format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n    if isinstance(video_id, (int, np.integer)):\n        if dataset is None:\n            raise ValueError(\n                \"Unable to convert integer video_id without a Dataset enum\"\n            )\n        return f\"{dataset.value}{video_id:06d}\"\n    assert isinstance(\n        video_id, str\n    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n    if dataset is not None:\n        assert (\n            video_id[0] == dataset.value\n        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n    return video_id", "def format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n    if isinstance(video_id, (int, np.integer)):\n        if dataset is None:\n            raise ValueError(\n                \"Unable to convert integer video_id without a Dataset enum\"\n            )\n        return f\"{dataset.value}{video_id:06d}\"\n    assert isinstance(\n        video_id, str\n    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n    if dataset is not None:\n        assert (\n            video_id[0] == dataset.value\n        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n    return video_id", "\n\n@dataclasses.dataclass\nclass CandidatePair:\n    query_id: str\n    ref_id: str\n    score: float\n\n    @classmethod\n    def to_dataframe(\n        cls,\n        candidates: Collection[\"CandidatePair\"],\n    ) -> pd.DataFrame:\n        return pd.DataFrame(\n            [\n                {\n                    \"query_id\": format_video_id(c.query_id, Dataset.QUERIES),\n                    \"ref_id\": format_video_id(c.ref_id, Dataset.REFS),\n                    \"score\": c.score,\n                }\n                for c in candidates\n            ],\n        )\n\n    @classmethod\n    def write_csv(\n        cls, candidates: Collection[\"CandidatePair\"], file: Union[str, TextIO]\n    ):\n        df = cls.to_dataframe(candidates)\n        df.to_csv(file, index=False)\n\n    @classmethod\n    def read_csv(cls, file: Union[str, TextIO]) -> List[\"CandidatePair\"]:\n        df = pd.read_csv(file)\n        pairs = []\n        for _, row in df.iterrows():\n            query_id = format_video_id(row.query_id, Dataset.QUERIES)\n            ref_id = format_video_id(row.ref_id, Dataset.REFS)\n            pairs.append(\n                CandidatePair(query_id=query_id, ref_id=ref_id, score=row.score)\n            )\n        return pairs\n\n    @classmethod\n    def from_matches(cls, matches: Collection[\"Match\"]) -> List[\"CandidatePair\"]:\n        scores = collections.defaultdict(float)\n        for match in matches:\n            key = (match.query_id, match.ref_id)\n            scores[key] = max(match.score, scores[key])\n        return [\n            CandidatePair(query_id=query_id, ref_id=ref_id, score=score)\n            for ((query_id, ref_id), score) in scores.items()\n        ]", "\n\n@dataclasses.dataclass\nclass PrecisionRecallCurve:\n    precisions: np.ndarray\n    recalls: np.ndarray\n    scores: np.ndarray\n\n    def plot(self, ax=None, **kwargs):\n        if ax is None:\n            _, ax = plt.subplots()\n            ax.set_xlabel(\"recall\")\n            ax.set_ylabel(\"precision\")\n            ax.set_xlim(0, 1.05)\n            ax.set_ylim(0, 1.05)\n        ax.plot(self.recalls, self.precisions, **kwargs)\n        return ax", "\n\n@dataclasses.dataclass\nclass AveragePrecision:\n    ap: float\n    pr_curve: PrecisionRecallCurve\n    simple_ap: Optional[float] = None\n\n\nclass Intervals:\n\n    # Non-overlapping, ordered by interval start.\n    intervals: List[Tuple[float, float]]\n\n    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n        self.intervals = intervals or []\n        self._dedup()\n\n    def add(self, interval: Tuple[float, float]):\n        \"\"\"Add an interval.\"\"\"\n        self.intervals.append(interval)\n        self._dedup()\n\n    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n        return Intervals(self.intervals + intervals.intervals)\n\n    def total_length(self) -> float:\n        length = 0.0\n        for start, end in self.intervals:\n            length += end - start\n        return length\n\n    def intersect_length(self, intervals: \"Intervals\") -> float:\n        \"\"\"Compute the total_length of the intersection of two Intervals.\n\n        This works by taking the sum of their lengths, and subtracting\n        the length of their union.\n\n        |A n B| = |A| + |B| - |A U B|\n        \"\"\"\n        union = self.union(intervals)\n        return self.total_length() + intervals.total_length() - union.total_length()\n\n    def _dedup(self):\n        if len(self.intervals) <= 1:\n            return\n        deduped = []\n        intervals = sorted(self.intervals)\n        current_start, current_end = intervals[0]\n        for start, end in intervals[1:]:\n            if start <= current_end:\n                # Overlap case\n                current_end = max(end, current_end)\n            else:\n                # Non-overlap case\n                deduped.append((current_start, current_end))\n                current_start, current_end = start, end\n        deduped.append((current_start, current_end))\n        self.intervals = deduped\n\n    def __str__(self):\n        return str(self.intervals)\n\n    __repr__ = __str__", "\nclass Intervals:\n\n    # Non-overlapping, ordered by interval start.\n    intervals: List[Tuple[float, float]]\n\n    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n        self.intervals = intervals or []\n        self._dedup()\n\n    def add(self, interval: Tuple[float, float]):\n        \"\"\"Add an interval.\"\"\"\n        self.intervals.append(interval)\n        self._dedup()\n\n    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n        return Intervals(self.intervals + intervals.intervals)\n\n    def total_length(self) -> float:\n        length = 0.0\n        for start, end in self.intervals:\n            length += end - start\n        return length\n\n    def intersect_length(self, intervals: \"Intervals\") -> float:\n        \"\"\"Compute the total_length of the intersection of two Intervals.\n\n        This works by taking the sum of their lengths, and subtracting\n        the length of their union.\n\n        |A n B| = |A| + |B| - |A U B|\n        \"\"\"\n        union = self.union(intervals)\n        return self.total_length() + intervals.total_length() - union.total_length()\n\n    def _dedup(self):\n        if len(self.intervals) <= 1:\n            return\n        deduped = []\n        intervals = sorted(self.intervals)\n        current_start, current_end = intervals[0]\n        for start, end in intervals[1:]:\n            if start <= current_end:\n                # Overlap case\n                current_end = max(end, current_end)\n            else:\n                # Non-overlap case\n                deduped.append((current_start, current_end))\n                current_start, current_end = start, end\n        deduped.append((current_start, current_end))\n        self.intervals = deduped\n\n    def __str__(self):\n        return str(self.intervals)\n\n    __repr__ = __str__", "\n\nclass Axis(enum.Enum):\n    QUERY = enum.auto()\n    REF = enum.auto()\n\n\nclass Match(NamedTuple):\n    \"\"\"A ground-truth match or predicted match.\"\"\"\n\n    query_id: str\n    ref_id: str\n    score: float\n    query_start: float\n    query_end: float\n    ref_start: float\n    ref_end: float\n\n    def pair_id(self):\n        return (self.query_id, self.ref_id)\n\n    def interval(self, axis: Axis) -> Tuple[float, float]:\n        if axis == Axis.QUERY:\n            return (self.query_start, self.query_end)\n        else:\n            return (self.ref_start, self.ref_end)\n\n    def intersection_area(self, bbox: \"Match\") -> float:\n        # Compute the intersection boarders\n        inter_q_start = max(self.query_start, bbox.query_start)\n        inter_r_start = max(self.ref_start, bbox.ref_start)\n        inter_q_end = min(self.query_end, bbox.query_end)\n        inter_r_end = min(self.ref_end, bbox.ref_end)\n\n        # Compute the area of intersection rectangle\n        return abs(\n            max((inter_q_end - inter_q_start, 0))\n            * max((inter_r_end - inter_r_start), 0)\n        )\n\n    def overlaps(self, bbox: \"Match\") -> bool:\n        return self.intersection_area(bbox) > 0.0\n\n    @classmethod\n    def write_csv(\n        cls, matches: Collection[\"Match\"], file: Union[str, TextIO], drop_dup=False\n    ):\n        df = pd.DataFrame([match._asdict() for match in matches], columns=cls._fields)\n        if drop_dup:\n            df[\"score\"] = df.groupby(\n                [\n                    \"query_id\",\n                    \"ref_id\",\n                    \"query_start\",\n                    \"query_end\",\n                    \"ref_start\",\n                    \"ref_end\",\n                ]\n            )[\"score\"].transform(\"max\")\n            df.drop_duplicates(\n                [\n                    \"query_id\",\n                    \"ref_id\",\n                    \"query_start\",\n                    \"query_end\",\n                    \"ref_start\",\n                    \"ref_end\",\n                ],\n                keep=\"first\",\n                inplace=True,\n            )\n        df = df.sort_values(by=\"score\", ascending=False)\n        df.to_csv(file, index=False)\n\n    @classmethod\n    def read_csv(\n        cls, file: Union[str, TextIO], is_gt=False, check=True\n    ) -> List[\"Match\"]:\n        df = pd.read_csv(file)\n        df[\"query_id\"] = df.query_id.map(lambda x: format_video_id(x, Dataset.QUERIES))\n        df[\"ref_id\"] = df.ref_id.map(lambda x: format_video_id(x, Dataset.REFS))\n        if is_gt:\n            df[\"score\"] = 1.0\n        if check:\n            for field in cls._fields:\n                assert not df[field].isna().any()\n        return [Match(**record) for record in df.to_dict(\"records\")]", "\n\nclass VideoPair:\n    \"\"\"A video pair item that contains information regarding the gt and pred bboxes.\n\n    Provide functionalities for the combination of new predictions with the\n    existing ones and the computation of their intersection with the gt bboxes,\n    ignoring the gt bboxes that do not overlap with any prediction.\n    \"\"\"\n\n    gts: List[Match]\n    preds: List[Match]\n\n    def __init__(\n        self,\n    ):\n        self.intersections = {axis: 0.0 for axis in Axis}\n        self.totals = {axis: 0.0 for axis in Axis}\n        self.gts = []\n        self.preds = []\n\n    def total_gt_length(self, axis: Axis) -> float:\n        return Intervals([gt.interval(axis) for gt in self.gts]).total_length()\n\n    def total_pred_length(self, axis: Axis) -> float:\n        return Intervals([pred.interval(axis) for pred in self.preds]).total_length()\n\n    def gt_overlaps(self, gt: Match) -> bool:\n        \"\"\"Checks if the provided gt bbox overlaps with at least one pred bbox.\"\"\"\n        for pred in self.preds:\n            if gt.overlaps(pred):\n                return True\n        return False\n\n    def add_gt(self, bbox: Match):\n        self.gts.append(bbox)\n\n    def add_prediction(\n        self, bbox: Match\n    ) -> Tuple[Dict[Axis, float], Dict[Axis, float]]:\n        \"\"\"Add a prediction to the corresponding list and calculates the\n        differences in the intersections with the gt and the total video\n        length covered for both query and reference axes.\n        \"\"\"\n        self.preds.append(bbox)\n\n        # A subset of GTs to consider for intersection (but not total GT length).\n        gts_to_consider = [gt for gt in self.gts if self.gt_overlaps(gt)]\n\n        intersect_deltas = {}\n        total_deltas = {}\n\n        for axis in Axis:\n            pred_ints = Intervals([pred.interval(axis) for pred in self.preds])\n            gt_ints = Intervals([gt.interval(axis) for gt in gts_to_consider])\n            # New intersection and total length on this axis\n            intersect_length = pred_ints.intersect_length(gt_ints)\n            prediction_length = pred_ints.total_length()\n            # Compute differences\n            intersect_deltas[axis] = intersect_length - self.intersections[axis]\n            total_deltas[axis] = prediction_length - self.totals[axis]\n            # Update with new values\n            self.intersections[axis] = intersect_length\n            self.totals[axis] = prediction_length\n\n        return intersect_deltas, total_deltas", "\n\ndef match_metric(\n    gts: Collection[Match],\n    predictions: Collection[Match],\n) -> AveragePrecision:\n    \"\"\"Matching track metric:\n\n    Computes the AP based on the VCSL approach for the\n    calculation of Precision and Recall.\n\n    AP = \\sum_{i=1}^N P(i) \u0394R(i)\n\n    where, P(i) = sqrt(P_q * P_r) and R(i) = sqrt(R_q * R_r)\n    calculated as in the VCSL.\n    \"\"\"  # noqa: W605\n\n    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\n    # Initialize video pairs and load their gt bboxs\n    video_pairs = defaultdict(VideoPair)\n    for gt in gts:\n        video_pairs[gt.pair_id()].add_gt(gt)\n\n    # Get the total gt length for each axis\n    gt_total_lengths = {axis: 0.0 for axis in Axis}\n    for _, v in video_pairs.items():\n        for axis in Axis:\n            gt_total_lengths[axis] += v.total_gt_length(axis)\n\n    # Loop through the predictions\n    recall = 0.0\n    metric = 0.0\n    intersections = {axis: 0.0 for axis in Axis}\n    totals = {axis: 0.0 for axis in Axis}\n    pr_recalls = []\n    pr_precisions = []\n    pr_scores = []\n\n    # Update metrics for all predictions with the same score as a group.\n    for score, prediction_group in itertools.groupby(\n        predictions, key=lambda x: x.score\n    ):\n        for prediction in prediction_group:\n            # Given new predictions, we only need the differences in the intersection with\n            # gt and total video length covered for both query and reference axes.\n            # This derives from the sum of differences for every pair id\n            intersection_deltas, total_deltas = video_pairs[\n                prediction.pair_id()\n            ].add_prediction(prediction)\n            for axis in Axis:\n                # Accumulate the differences to the corresponding values\n                intersections[axis] += intersection_deltas[axis]\n                totals[axis] += total_deltas[axis]\n\n        recalls = {}\n        precisions = {}\n        for axis in Axis:\n            recalls[axis] = intersections[axis] / gt_total_lengths[axis]\n            precisions[axis] = intersections[axis] / totals[axis]\n\n        new_recall = sqrt(recalls[Axis.QUERY] * recalls[Axis.REF])\n        precision = sqrt(precisions[Axis.QUERY] * precisions[Axis.REF])\n\n        # Compute metric\n        delta_recall = new_recall - recall\n        metric += precision * delta_recall\n        recall = new_recall\n        if delta_recall > 0:\n            pr_recalls.append(recall)\n            pr_precisions.append(precision)\n            pr_scores.append(score)\n\n    curve = PrecisionRecallCurve(\n        np.array(pr_precisions), np.array(pr_recalls), np.array(pr_scores)\n    )\n    return AveragePrecision(metric, curve)", "\n\n@dataclasses.dataclass\nclass MatchingTrackMetrics:\n    # Our main evaluation metric.\n    segment_ap: AveragePrecision\n    # This metric reflects only pairwise matching, and not localization.\n    pairwise_micro_ap: AveragePrecision\n\n\ndef evaluate_matching_track(\n    ground_truth_filename: str, predictions_filename: str\n) -> MatchingTrackMetrics:\n    \"\"\"Matching track evaluation.\n\n    Predictions are expected to be a CSV file, with a column names in the header.\n    The following columns must be present, in any order:\n        query_id: str, the ID of the query for this match\n        ref_id: str, the ID of the reference for this match\n        query_start: float, the start of the query segment in seconds\n        query_end: float, the end of the query segment in seconds\n        ref_start: float, the start of the reference segment in seconds\n        ref_end: float, the end of the reference segment in seconds\n        score: float, the score of this prediction (a higher score indicates a\n            more confident prediction)\n\n    Note that ground-truth matches are specified using the same format, but score\n    is not used.\n    \"\"\"\n    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n    predictions = Match.read_csv(predictions_filename)\n    metric = match_metric(gt, predictions)\n    # Auxiliary metric: pairwise uAP\n    gt_pairs = CandidatePair.from_matches(gt)\n    pairs = CandidatePair.from_matches(predictions)\n    pair_ap = average_precision(gt_pairs, pairs)\n    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)", "\n\ndef evaluate_matching_track(\n    ground_truth_filename: str, predictions_filename: str\n) -> MatchingTrackMetrics:\n    \"\"\"Matching track evaluation.\n\n    Predictions are expected to be a CSV file, with a column names in the header.\n    The following columns must be present, in any order:\n        query_id: str, the ID of the query for this match\n        ref_id: str, the ID of the reference for this match\n        query_start: float, the start of the query segment in seconds\n        query_end: float, the end of the query segment in seconds\n        ref_start: float, the start of the reference segment in seconds\n        ref_end: float, the end of the reference segment in seconds\n        score: float, the score of this prediction (a higher score indicates a\n            more confident prediction)\n\n    Note that ground-truth matches are specified using the same format, but score\n    is not used.\n    \"\"\"\n    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n    predictions = Match.read_csv(predictions_filename)\n    metric = match_metric(gt, predictions)\n    # Auxiliary metric: pairwise uAP\n    gt_pairs = CandidatePair.from_matches(gt)\n    pairs = CandidatePair.from_matches(predictions)\n    pair_ap = average_precision(gt_pairs, pairs)\n    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)", "\n\ndef average_precision(\n    ground_truth: Collection[CandidatePair], predictions: Collection[CandidatePair]\n) -> AveragePrecision:\n    gt_pairs = {(pair.query_id, pair.ref_id) for pair in ground_truth}\n    if len(gt_pairs) != len(ground_truth):\n        raise AssertionError(\"Duplicates detected in ground truth\")\n    predicted_pairs = {(pair.query_id, pair.ref_id) for pair in predictions}\n    if len(predicted_pairs) != len(predictions):\n        raise AssertionError(\"Duplicates detected in predictions\")\n\n    # AP calculation that aligns with DrivenData's backend implementation.\n    canonical_ap = drivendata_average_precision(\n        predicted=CandidatePair.to_dataframe(predictions),\n        ground_truth=CandidatePair.to_dataframe(ground_truth),\n    )\n\n    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n    scores = np.array([pair.score for pair in predictions])\n    correct = np.array(\n        [(pair.query_id, pair.ref_id) in gt_pairs for pair in predictions]\n    )\n    total_pairs = len(gt_pairs)\n    # precision = correct_so_far / total_pairs_so_far\n    cumulative_correct = np.cumsum(correct)\n    cumulative_predicted = np.arange(len(correct)) + 1\n    recall = cumulative_correct / total_pairs\n    precision = cumulative_correct / cumulative_predicted\n    # Simple AP computation.\n    simple_ap = np.sum(precision * correct) / total_pairs\n    # Get precision and recall where correct is true\n    indices = np.nonzero(correct)[0]\n    curve = PrecisionRecallCurve(precision[indices], recall[indices], scores[indices])\n    return AveragePrecision(ap=canonical_ap, pr_curve=curve, simple_ap=simple_ap)", "\n\ndef drivendata_average_precision(\n    predicted: pd.DataFrame,\n    ground_truth: pd.DataFrame,\n):\n    \"\"\"Canonical AP implementation used for the challenge.\"\"\"\n    SCORE_COL = \"score\"\n    QUERY_ID_COL = \"query_id\"\n    DATABASE_ID_COL = \"ref_id\"\n    actual = ground_truth[[\"query_id\", \"ref_id\"]]\n    if (\n        not np.isfinite(predicted[SCORE_COL]).all()\n        or np.isnan(predicted[SCORE_COL]).any()\n    ):\n        raise ValueError(\"Scores must be finite.\")\n\n    predicted = predicted.sort_values(SCORE_COL, ascending=False)\n\n    merged = predicted.merge(\n        right=actual.assign(actual=1.0),\n        how=\"left\",\n        on=[QUERY_ID_COL, DATABASE_ID_COL],\n    ).fillna({\"actual\": 0.0})\n\n    # We may not predict for every ground truth, so calculate unadjusted AP then adjust it\n    unadjusted_ap = (\n        average_precision_score(merged[\"actual\"].values, merged[SCORE_COL].values)\n        if merged[\"actual\"].sum()\n        else 0.0\n    )\n    # Rescale average precisions based on total ground truth positive counts\n    predicted_n_pos = int(merged[\"actual\"].sum())\n\n    # avoid rows added to validate query ids, will have blank ref_id\n    actual_n_pos = int(actual[DATABASE_ID_COL].notna().sum())\n\n    adjusted_ap = unadjusted_ap * (predicted_n_pos / actual_n_pos)\n    return adjusted_ap", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/candidates.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nimport numpy as np\nfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex", "import numpy as np\nfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex\nfrom src.vsc.metrics import CandidatePair\n\n\nclass ScoreAggregation(ABC):\n    @abstractmethod\n    def aggregate(self, match: PairMatches) -> float:\n        pass\n\n    def score(self, match: PairMatches) -> CandidatePair:\n        score = self.aggregate(match)\n        return CandidatePair(query_id=match.query_id, ref_id=match.ref_id, score=score)", "\n\nclass MaxScoreAggregation(ScoreAggregation):\n    def aggregate(self, match: PairMatches) -> float:\n        return np.max([m.score for m in match.matches])\n\n\nclass CandidateGeneration:\n    def __init__(self, references: List[VideoFeature], aggregation: ScoreAggregation):\n        self.aggregation = aggregation\n        dim = references[0].dimensions()\n        self.index = VideoIndex(dim)\n        self.index.add(references)\n\n    def query(self, queries: List[VideoFeature], global_k: int) -> List[CandidatePair]:\n        matches = self.index.search(queries, global_k=global_k)\n        candidates = [self.aggregation.score(match) for match in matches]\n        candidates = sorted(candidates, key=lambda match: match.score, reverse=True)\n        return candidates", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/storage.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Optional\n\nimport numpy as np\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.metrics import Dataset, format_video_id", "from src.vsc.index import VideoFeature\nfrom src.vsc.metrics import Dataset, format_video_id\n\n\ndef store_features(f, features: List[VideoFeature], dataset: Optional[Dataset] = None):\n    video_ids = []\n    feats = []\n    timestamps = []\n    for feature in features:\n        video_id = format_video_id(feature.video_id, dataset)\n        video_ids.append(np.full(len(feature), video_id))\n        feats.append(feature.feature)\n        timestamps.append(feature.timestamps)\n    video_ids = np.concatenate(video_ids)\n    feats = np.concatenate(feats)\n    timestamps = np.concatenate(timestamps)\n    np.savez(f, video_ids=video_ids, features=feats, timestamps=timestamps)", "\n\ndef same_value_ranges(values):\n    start = 0\n    value = values[start]\n\n    for i, v in enumerate(values):\n        if v == value:\n            continue\n        yield value, start, i\n        start = i\n        value = values[start]\n\n    yield value, start, len(values)", "\n\ndef load_features(f, dataset: Optional[Dataset] = None):\n    data = np.load(f, allow_pickle=False)\n    video_ids = data[\"video_ids\"]\n    feats = data[\"features\"]\n    timestamps = data[\"timestamps\"]\n\n    ts_dims = len(timestamps.shape)\n    if timestamps.shape[0] != feats.shape[0]:\n        raise ValueError(\n            f\"Expected the same number of timestamps as features: got \"\n            f\"{timestamps.shape[0]} timestamps for {feats.shape[0]} features\"\n        )\n    if not (ts_dims == 1 or timestamps.shape[1:] == (2,)):\n        print(f\"timestamps.shape[1:]: {timestamps.shape[1:]}\")\n        print(f\"timestamps.shape[1:] == [2]: {timestamps.shape[1:] == [2]}\")\n        raise ValueError(f\"Unexpected timestamp shape. Got {timestamps.shape}\")\n\n    results = []\n    for video_id, start, end in same_value_ranges(video_ids):\n        video_id = format_video_id(video_id, dataset)\n        results.append(\n            VideoFeature(\n                video_id=video_id,\n                timestamps=timestamps[start:end],\n                feature=feats[start:end, :],\n            )\n        )\n    return results", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/index.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Iterable, List, NamedTuple, Tuple\n", "from typing import Iterable, List, NamedTuple, Tuple\n\nimport faiss  # @manual\nimport numpy as np\nfrom faiss.contrib import exhaustive_search  # @manual\n\nSearchIndices = Tuple[int, int, float]\n\n\n@dataclass\nclass VideoMetadata:\n    video_id: str\n    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\n    def __len__(self):\n        return self.timestamps.shape[0]\n\n    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n        t = self.timestamps[idx]\n        if len(self.timestamps.shape) == 1:\n            return (t, t)\n        return (t[0], t[1])", "\n@dataclass\nclass VideoMetadata:\n    video_id: str\n    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\n    def __len__(self):\n        return self.timestamps.shape[0]\n\n    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n        t = self.timestamps[idx]\n        if len(self.timestamps.shape) == 1:\n            return (t, t)\n        return (t[0], t[1])", "\n\n@dataclass\nclass VideoFeature(VideoMetadata):\n    feature: np.ndarray\n\n    def __post_init__(self):\n        assert self.feature.shape[0] == len(\n            self.timestamps\n        ), \"Mismatched timestamps / feature size\"\n\n    def metadata(self):\n        return VideoMetadata(video_id=self.video_id, timestamps=self.timestamps)\n\n    def dimensions(self):\n        return self.feature.shape[1]", "\n\nclass PairMatch(NamedTuple):\n    query_timestamps: Tuple[float, float]\n    ref_timestamps: Tuple[float, float]\n    score: float\n\n\n@dataclass\nclass PairMatches:\n    query_id: str\n    ref_id: str\n    matches: List[PairMatch]\n\n    def records(self):\n        for match in self.matches:\n            yield {\n                \"query_id\": self.query_id,\n                \"ref_id\": self.ref_id,\n                \"query_start\": match.query_timestamps[0],\n                \"query_end\": match.query_timestamps[1],\n                \"ref_start\": match.ref_timestamps[0],\n                \"ref_end\": match.ref_timestamps[1],\n                \"score\": match.score,\n            }", "@dataclass\nclass PairMatches:\n    query_id: str\n    ref_id: str\n    matches: List[PairMatch]\n\n    def records(self):\n        for match in self.matches:\n            yield {\n                \"query_id\": self.query_id,\n                \"ref_id\": self.ref_id,\n                \"query_start\": match.query_timestamps[0],\n                \"query_end\": match.query_timestamps[1],\n                \"ref_start\": match.ref_timestamps[0],\n                \"ref_end\": match.ref_timestamps[1],\n                \"score\": match.score,\n            }", "\n\nclass VideoIndex:\n    def __init__(\n        self,\n        dim: int,\n        codec_str: str = \"Flat\",\n        metric: int = faiss.METRIC_INNER_PRODUCT,\n    ):\n        self.dim = dim\n        self.index = faiss.index_factory(self.dim, codec_str, metric)\n        self.video_clip_idx = []\n        self.video_clip_to_video_ids = []\n        self.video_metadata = {}\n\n    def add(self, db: List[VideoFeature]):\n        for vf in db:\n            self.video_clip_idx.extend(list(range(vf.feature.shape[0])))\n            self.video_clip_to_video_ids.extend(\n                [vf.video_id for _ in range(vf.feature.shape[0])]\n            )\n            self.video_metadata[vf.video_id] = vf.metadata()\n            self.index.add(vf.feature)\n\n    def search(\n        self,\n        queries: List[VideoFeature],\n        global_k: int,\n    ) -> List[PairMatches]:\n        query_ids = []\n        query_indices = []\n        for q in queries:\n            query_ids.extend([q.video_id] * len(q))\n            query_indices.extend(range(len(q)))\n        query_metadatas = {q.video_id: q.metadata() for q in queries}\n        query_features = np.concatenate([q.feature for q in queries])\n        if global_k < 0:\n            # Negative values cause us to use vanilla KNN search\n            k = -global_k\n            logging.warn(\n                \"Using local k for KNN search. Warning: this is against the \"\n                \"VSC rules, since predictions for a query-ref pair are not \"\n                \"independent of other references. KNN search is provided for \"\n                \"comparison.\"\n            )\n            search_indices = self._knn_search(query_features, k)\n        else:\n            search_indices = self._global_threshold_knn_search(query_features, global_k)\n\n        pair_nns = collections.defaultdict(list)\n\n        for i, j, score in search_indices:\n            query_id = query_ids[i]\n            query_idx = query_indices[i]\n            query_metadata = query_metadatas[query_id]\n            ref_id = self.video_clip_to_video_ids[j]\n            ref_idx = self.video_clip_idx[j]\n            ref_metadata = self.video_metadata[ref_id]\n            match = PairMatch(\n                query_timestamps=query_metadata.get_timestamps(query_idx),\n                ref_timestamps=ref_metadata.get_timestamps(ref_idx),\n                score=score,\n            )\n            pair_nns[query_id, ref_id].append(match)\n\n        return [\n            PairMatches(query_id, ref_id, matches)\n            for ((query_id, ref_id), matches) in pair_nns.items()\n        ]\n\n    def _global_threshold_knn_search(\n        self, query_features: np.ndarray, global_k: int\n    ) -> Iterable[SearchIndices]:\n        use_similarity = self.index.metric_type == faiss.METRIC_INNER_PRODUCT\n        initial_radius = -1e10 if use_similarity else 1e10\n        _, limits, similarity, indices = exhaustive_search.range_search_max_results(\n            self.index,\n            exhaustive_search.exponential_query_iterator(query_features),\n            initial_radius,\n            max_results=2 * global_k,\n            min_results=global_k,\n            ngpu=-1,  # use GPU if available\n        )\n        nq = query_features.shape[0]\n        search_indices = []\n\n        for i in range(nq):\n            for j in range(limits[i], limits[i + 1]):\n                search_indices.append((i, indices[j], similarity[j]))\n\n        search_indices.sort(key=lambda x: x[2], reverse=use_similarity)\n        if len(search_indices) > global_k:\n            search_indices = search_indices[:global_k]\n        return search_indices\n\n    def _knn_search(self, query_features: np.ndarray, k) -> Iterable[SearchIndices]:\n        index = self.index\n        if faiss.get_num_gpus() > 0:\n            logging.info(\"Moving index to GPU\")\n            index = faiss.index_cpu_to_all_gpus(self.index)\n\n        logging.info(\"Performing KNN search\")\n        similarity, ids = index.search(query_features, k)\n        for i in range(ids.shape[0]):\n            for j in range(ids.shape[1]):\n                yield (i, ids[i, j], similarity[i, j])", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/ffmpeg_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Iterable, Optional, Tuple\n", "from typing import Iterable, Optional, Tuple\n\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.io.image import ImageReadMode, read_image\n\nImageT = Image.Image\n\n\nclass FFMpegVideoReader(VideoReader):\n    def __init__(\n        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n    ):\n        self.ffmpeg_path = ffmpeg_path\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n            subprocess.check_call(\n                [\n                    self.ffmpeg_path,\n                    \"-nostdin\",\n                    \"-y\",\n                    \"-i\",\n                    self.video_path,\n                    \"-start_number\",\n                    \"0\",\n                    \"-q\",\n                    \"0\",\n                    \"-vf\",\n                    \"fps=%f\" % self.required_fps,\n                    os.path.join(dir, \"%07d.png\"),\n                ],\n                stderr=null,\n            )\n            i = 0\n            while True:\n                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n                if not os.path.exists(frame_fn):\n                    break\n                if self.output_type == \"pil\":\n                    img = default_loader(frame_fn)\n                elif self.output_type == \"tensor\":\n                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n                i += 1\n                yield ((i - 1) / self.original_fps, i / self.original_fps, img)", "\n\nclass FFMpegVideoReader(VideoReader):\n    def __init__(\n        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n    ):\n        self.ffmpeg_path = ffmpeg_path\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n            subprocess.check_call(\n                [\n                    self.ffmpeg_path,\n                    \"-nostdin\",\n                    \"-y\",\n                    \"-i\",\n                    self.video_path,\n                    \"-start_number\",\n                    \"0\",\n                    \"-q\",\n                    \"0\",\n                    \"-vf\",\n                    \"fps=%f\" % self.required_fps,\n                    os.path.join(dir, \"%07d.png\"),\n                ],\n                stderr=null,\n            )\n            i = 0\n            while True:\n                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n                if not os.path.exists(frame_fn):\n                    break\n                if self.output_type == \"pil\":\n                    img = default_loader(frame_fn)\n                elif self.output_type == \"tensor\":\n                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n                i += 1\n                yield ((i - 1) / self.original_fps, i / self.original_fps, img)", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/ffmpeg_py_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport warnings\nfrom typing import Iterable, Optional, Tuple\n\nimport ffmpeg", "\nimport ffmpeg\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\n\nImageT = Image.Image\n\n\nclass FFMpegPyVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        buffer, _ = (\n            ffmpeg.input(self.video_path)\n            .video.filter(\"fps\", self.required_fps)\n            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n\n        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n        width, height = meta[\"width\"], meta[\"height\"]\n\n        if self.output_type == \"pil\":\n            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n            for i, frame in enumerate(frames):\n                img = Image.fromarray(frame).convert(\"RGB\")\n                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\n        elif self.output_type == \"tensor\":\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", message=\"The given buffer is not writable\"\n                )\n                frames = (\n                    torch.frombuffer(buffer, dtype=torch.uint8)\n                    .reshape((-1, height, width, 3))\n                    .permute(0, 3, 1, 2)\n                )\n\n            for i, frame in enumerate(frames):\n                yield i / self.original_fps, (i + 1) / self.original_fps, frame", "\n\nclass FFMpegPyVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        buffer, _ = (\n            ffmpeg.input(self.video_path)\n            .video.filter(\"fps\", self.required_fps)\n            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n\n        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n        width, height = meta[\"width\"], meta[\"height\"]\n\n        if self.output_type == \"pil\":\n            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n            for i, frame in enumerate(frames):\n                img = Image.fromarray(frame).convert(\"RGB\")\n                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\n        elif self.output_type == \"tensor\":\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", message=\"The given buffer is not writable\"\n                )\n                frames = (\n                    torch.frombuffer(buffer, dtype=torch.uint8)\n                    .reshape((-1, height, width, 3))\n                    .permute(0, 3, 1, 2)\n                )\n\n            for i, frame in enumerate(frames):\n                yield i / self.original_fps, (i + 1) / self.original_fps, frame", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom abc import ABC, abstractmethod\nfrom typing import Iterable, Optional, Tuple, Union\n\nimport torch\nfrom PIL import Image", "import torch\nfrom PIL import Image\n\nImageT = Union[Image.Image, torch.Tensor]\n\n\nclass VideoReader(ABC):\n    def __init__(self, video_path: str, required_fps: float, output_type: str) -> None:\n        self.video_path = video_path\n        self.required_fps = required_fps\n        self.output_type = output_type\n        self.original_fps = max(1, self.fps) if self.fps else 1\n        self.video_frames = None\n\n    @property\n    @abstractmethod\n    def fps(self) -> Optional[float]:\n        pass\n\n    @abstractmethod\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        \"\"\"\n        returns a tuple of [start_time, end_time, Image]\n        \"\"\"\n        pass", ""]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/decord_video_reader.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport os\nfrom typing import Iterable, Optional, Tuple\n", "from typing import Iterable, Optional, Tuple\n\nimport decord\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\nfrom torchvision.datasets.folder import default_loader\n\nImageT = Image.Image", "\nImageT = Image.Image\n\n\nclass DecordVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return self.required_fps\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        reader = decord.VideoReader(\n            self.video_path,\n            ctx=decord.cpu(0),\n            num_threads=0,\n        )\n\n        n_frames = len(reader)\n        timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n        end_timestamps = timestamps[:, 1]\n        duration = end_timestamps[-1].tolist()\n\n        fps = min(self.required_fps, n_frames / duration)\n        count = max(1, np.round(duration * fps))\n        step_size = 1 / fps\n\n        frame_pos = (np.arange(count) + 0.5) * step_size\n\n        frame_ids = np.searchsorted(end_timestamps, frame_pos)\n        frame_ids = np.minimum(frame_ids, n_frames - 1)\n\n        frames = reader.get_batch(frame_ids).asnumpy()\n\n        for i, frame in enumerate(frames):\n            if self.output_type == \"pil\":\n                img = Image.fromarray(frame).convert(\"RGB\")\n            elif self.output_type == \"tensor\":\n                img = torch.as_tensor(frame).permute(2, 0, 1)\n            yield (\n                i / self.original_fps,\n                (i + 1) / self.original_fps,\n                img,\n            )", ""]}
{"filename": "meta-vsc-descriptor-runtime/runtime/validation.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nDescriptor Track validation script\n\"\"\"\nimport logging\nfrom argparse import ArgumentParser, Namespace\n\nimport numpy as np\nimport pandas as pd\n", "import pandas as pd\n\nparser = ArgumentParser()\nparser.add_argument(\n    \"--query_features\",\n    help=\"Path containing query features\",\n    type=str,\n    required=True,\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--ref_features\",\n    help=\"Path containing reference features\",\n    type=str,\n    required=True,\n)\nparser.add_argument(\n    \"--query_metadata\",\n    help=\"Path containing query metadata\",", "    \"--query_metadata\",\n    help=\"Path containing query metadata\",\n    type=str,\n    required=True,\n)\nparser.add_argument(\n    \"--ref_metadata\",\n    help=\"Path containing reference metadata\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n)\nparser.add_argument(\"--subset\", help=\"Path containing query subset ids\", type=str)\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",", "    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"descriptor_eval_lib.py\")\nlogger.setLevel(logging.INFO)\n\n\nclass DataValidationError(AssertionError):\n    pass\n", "\n\ndef validate_total_descriptors(dataset: str, n_features: int, total_seconds: float):\n    if n_features > total_seconds:\n        raise DataValidationError(\n            f\"Number of {dataset} video features must not exceed one feature per second. \"\n            f\"Saw {n_features} vectors, max allowed is {total_seconds}\"\n        )\n\n\ndef validate_lengths(dataset: str, features_npz):\n    n_video_ids = len(features_npz[\"video_ids\"])\n    n_timestamps = len(features_npz[\"timestamps\"])\n    n_features = len(features_npz[\"features\"])\n    if not (n_video_ids == n_timestamps == n_features):\n        raise DataValidationError(\n            f\"Arrays lengths for {dataset} do not match. \"\n            f\"video_ids: {n_video_ids}; \"\n            f\"timestamps: {n_timestamps}; \"\n            f\"features: {n_features}. \"\n        )", "\n\ndef validate_lengths(dataset: str, features_npz):\n    n_video_ids = len(features_npz[\"video_ids\"])\n    n_timestamps = len(features_npz[\"timestamps\"])\n    n_features = len(features_npz[\"features\"])\n    if not (n_video_ids == n_timestamps == n_features):\n        raise DataValidationError(\n            f\"Arrays lengths for {dataset} do not match. \"\n            f\"video_ids: {n_video_ids}; \"\n            f\"timestamps: {n_timestamps}; \"\n            f\"features: {n_features}. \"\n        )", "\n\ndef validate_descriptor_dtype(dataset: str, features_array: np.ndarray):\n    if features_array.dtype != np.float32:\n        raise DataValidationError(\n            f\"Features array for {dataset} is not float32. \"\n            f\"dtype is: {features_array.dtype}\"\n        )\n\n\ndef validate_descriptor_dim(\n    dataset: str, features_array: np.ndarray, max_dim: int = 512\n):\n    if (submitted_dim := features_array.shape[1]) > max_dim:\n        raise DataValidationError(\n            f\"Features array for {dataset} exceeds max dim of {max_dim}: \"\n            f\"submitted dim is {submitted_dim}.\"\n        )", "\n\ndef validate_descriptor_dim(\n    dataset: str, features_array: np.ndarray, max_dim: int = 512\n):\n    if (submitted_dim := features_array.shape[1]) > max_dim:\n        raise DataValidationError(\n            f\"Features array for {dataset} exceeds max dim of {max_dim}: \"\n            f\"submitted dim is {submitted_dim}.\"\n        )", "\n\ndef validate_sorted_ids(dataset: str, video_ids: np.array):\n    is_sorted = video_ids[:-1] <= video_ids[1:]\n    if not np.all(is_sorted):\n        indices = np.argwhere(~is_sorted)\n        raise DataValidationError(f\"Video ids not sorted at index {indices[0]}.\")\n\n\ndef main(args: Namespace):\n    query_features = np.load(args.query_features, allow_pickle=False)\n    ref_features = np.load(args.ref_features, allow_pickle=False)\n    query_meta = pd.read_csv(args.query_metadata)\n    ref_meta = pd.read_csv(args.ref_metadata)\n    if args.subset:\n        subset = pd.read_csv(args.subset)\n        query_meta = query_meta.set_index(\"video_id\").loc[subset.video_id]\n    query_total_seconds = query_meta.duration_sec.apply(np.ceil).sum()\n    ref_total_seconds = ref_meta.duration_sec.apply(np.ceil).sum()\n\n    validate_total_descriptors(\n        \"query\", query_features[\"features\"].shape[0], query_total_seconds\n    )\n    validate_total_descriptors(\n        \"reference\", ref_features[\"features\"].shape[0], ref_total_seconds\n    )\n\n    validate_lengths(\"query\", query_features)\n    validate_lengths(\"reference\", ref_features)\n\n    validate_sorted_ids(\"query\", query_features[\"video_ids\"])\n    validate_sorted_ids(\"reference\", ref_features[\"video_ids\"])\n\n    validate_descriptor_dtype(\"query\", query_features[\"features\"])\n    validate_descriptor_dtype(\"reference\", ref_features[\"features\"])\n\n    validate_descriptor_dim(\"query\", query_features[\"features\"], max_dim=512)\n    validate_descriptor_dim(\"reference\", ref_features[\"features\"], max_dim=512)", "\ndef main(args: Namespace):\n    query_features = np.load(args.query_features, allow_pickle=False)\n    ref_features = np.load(args.ref_features, allow_pickle=False)\n    query_meta = pd.read_csv(args.query_metadata)\n    ref_meta = pd.read_csv(args.ref_metadata)\n    if args.subset:\n        subset = pd.read_csv(args.subset)\n        query_meta = query_meta.set_index(\"video_id\").loc[subset.video_id]\n    query_total_seconds = query_meta.duration_sec.apply(np.ceil).sum()\n    ref_total_seconds = ref_meta.duration_sec.apply(np.ceil).sum()\n\n    validate_total_descriptors(\n        \"query\", query_features[\"features\"].shape[0], query_total_seconds\n    )\n    validate_total_descriptors(\n        \"reference\", ref_features[\"features\"].shape[0], ref_total_seconds\n    )\n\n    validate_lengths(\"query\", query_features)\n    validate_lengths(\"reference\", ref_features)\n\n    validate_sorted_ids(\"query\", query_features[\"video_ids\"])\n    validate_sorted_ids(\"reference\", ref_features[\"video_ids\"])\n\n    validate_descriptor_dtype(\"query\", query_features[\"features\"])\n    validate_descriptor_dtype(\"reference\", ref_features[\"features\"])\n\n    validate_descriptor_dim(\"query\", query_features[\"features\"], max_dim=512)\n    validate_descriptor_dim(\"reference\", ref_features[\"features\"], max_dim=512)", "\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/conftest.py", "chunked_list": ["def pytest_addoption(parser):\n    parser.addoption(\"--submission-path\", action=\"store\", default=\"submission.csv\")\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/tests/test_validation.py", "chunked_list": ["import numpy as np\nimport pytest\nimport validation\n\n# Run with python -m pytest tests/test_validation.py from runtime/\n\n\ndef test_dim_too_large():\n    features = np.random.randn(10, 513).astype(\"float32\")\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_descriptor_dim(\"test\", features, max_dim=512)", "\n\ndef test_bad_dtype():\n    features = np.random.randn(10, 64).astype(\"float64\")\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_descriptor_dtype(\"test\", features)\n\n\ndef test_unsorted_ids():\n    video_ids = np.array([\"Q200001\", \"Q200001\", \"Q200002\", \"Q200001\"])\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_sorted_ids(\"test\", video_ids)", "def test_unsorted_ids():\n    video_ids = np.array([\"Q200001\", \"Q200001\", \"Q200002\", \"Q200001\"])\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_sorted_ids(\"test\", video_ids)\n\n\ndef test_total_descriptors():\n    features = np.random.randn(100, 64).astype(\"float32\")\n    total_seconds = 50\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_total_descriptors(\"test\", features.shape[0], total_seconds)", "\n\ndef test_length_validation():\n    video_ids = np.array([\"Q200001\", \"Q200001\", \"Q200002\", \"Q200003\"])\n    timestamps = np.array([[0, 10], [10, 20], [0, 10]])\n    features = np.random.randn(4, 16).astype(\"float32\")\n    submission = {\n        \"video_ids\": video_ids,\n        \"timestamps\": timestamps,\n        \"features\": features,\n    }\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_lengths(\"test\", submission)\n\n    timestamps = np.array([[0, 10], [10, 20], [0, 10], [10, 20]])\n    features = np.random.randn(3, 16).astype(\"float32\")\n    submission = {\n        \"video_ids\": video_ids,\n        \"timestamps\": timestamps,\n        \"features\": features,\n    }\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_lengths(\"test\", submission)\n\n    video_ids = np.array([\"Q200001\", \"Q200001\"])\n    timestamps = np.array([[0, 10], [10, 20], [0, 10], [10, 20]])\n    submission = {\n        \"video_ids\": video_ids,\n        \"timestamps\": timestamps,\n        \"features\": features,\n    }\n    with pytest.raises(validation.DataValidationError):\n        validation.validate_lengths(\"test\", submission)", ""]}
{"filename": "meta-vsc-descriptor-runtime/runtime/tests/test_packages.py", "chunked_list": ["# adapted from pangeo https://github.com/pangeo-data/pangeo-docker-images/blob/master/tests/test_pangeo-notebook.py\nimport importlib\nimport subprocess\nimport warnings\n\nimport pytest\n\npackages = [\n    # these are problem libraries that don't always seem to import, mostly due\n    # to dependencies outside the python world", "    # these are problem libraries that don't always seem to import, mostly due\n    # to dependencies outside the python world\n    \"fastai\",\n    \"keras\",\n    \"numpy\",\n    \"pandas\",\n    \"scipy\",\n    \"sklearn\",  # scikit-learn\n    \"tensorflow\",\n    \"torch\",  # pytorch", "    \"tensorflow\",\n    \"torch\",  # pytorch\n    \"faiss\",\n    \"augly\",\n    \"PIL\",\n    \"PIL.Image\",\n]\n\n\n@pytest.mark.parametrize(\"package_name\", packages, ids=packages)\ndef test_import(package_name):\n    importlib.import_module(package_name)", "\n@pytest.mark.parametrize(\"package_name\", packages, ids=packages)\ndef test_import(package_name):\n    importlib.import_module(package_name)\n\n\ndef test_gpu_packages():\n    try:\n        subprocess.check_call([\"nvidia-smi\"])\n\n        import torch\n\n        assert torch.cuda.is_available()\n\n        import tensorflow as tf\n\n        assert tf.test.is_built_with_cuda()\n        assert tf.config.list_physical_devices(\"GPU\")\n\n        import faiss\n\n        assert faiss.get_num_gpus() > 0\n\n    except FileNotFoundError:\n        warnings.warn(\n            \"Skipping GPU import tests since nvidia-smi is not present on test machine.\"\n        )", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/main.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport subprocess\nimport time\nfrom pathlib import Path", "import time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nROOT_DIRECTORY = Path(\"/code_execution\")\nDATA_DIRECTORY = Path(\"/data\")\nQRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"\nOUTPUT_FILE = ROOT_DIRECTORY / \"subset_matches.csv\"", "QRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"\nOUTPUT_FILE = ROOT_DIRECTORY / \"subset_matches.csv\"\nQUERY_SUBSET_FILE = DATA_DIRECTORY / \"query_subset.csv\"\nQUERY_META_FILE = DATA_DIRECTORY / \"query_metadata.csv\"\nREF_META_FILE = DATA_DIRECTORY / \"reference_metadata.csv\"\n\n\ndef main():\n\n    num_videos = len(pd.read_csv(QUERY_SUBSET_FILE))\n    time_deadline_sec = num_videos * (10 + 1)  # 10 sec per video + 1 sec for overhead\n    start_time = time.time()\n\n    cmd = f\"\"\"\n    conda run --no-capture-output -n condaenv python -m src.inference \\\n    --accelerator=cuda --processes=1 --fps 2 \\\n    --dataset_path {str(QRY_VIDEOS_DIRECTORY)} \\\n    --output_file {str(OUTPUT_FILE)} \\\n    --read_type tensor \\\n    --video_reader DECORD\n    \"\"\"\n    subprocess.run(cmd.split())\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Elapsed time: {elapsed_time} sec. (deadline: {time_deadline_sec} sec)\")\n\n    if elapsed_time > time_deadline_sec:\n        print(\"Time limit exceeded.\")\n    else:\n        print(\"Time limit not exceeded.\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference_full.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Inference script.\n\nThis is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors", "This is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors\nin some runtime environments.\n\nWe import inference_impl, which imports libraries that may initialize cuda,\nin two circumstances: from worker processes after the main process has\nforked workers, or from the main process after worker processes have been\njoined.\n\"\"\"\n", "\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd", "\nimport pandas as pd\nimport tqdm\nfrom src.inference import Accelerator, VideoReaderType\nfrom src.vsc.storage import load_features, store_features\nfrom torch import multiprocessing\n\nparser = argparse.ArgumentParser()\ninference_parser = parser.add_argument_group(\"Inference\")\ninference_parser.add_argument(\"--batch_size\", type=int, default=32)", "inference_parser = parser.add_argument_group(\"Inference\")\ninference_parser.add_argument(\"--batch_size\", type=int, default=32)\ninference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\ninference_parser.add_argument(\"--distributed_size\", type=int, default=1)\ninference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(\n    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n)\ninference_parser.add_argument(\"--output_path\", nargs=\"+\")\ninference_parser.add_argument(\"--scratch_path\", required=False)", "inference_parser.add_argument(\"--output_path\", nargs=\"+\")\ninference_parser.add_argument(\"--scratch_path\", required=False)\n\ndataset_parser = parser.add_argument_group(\"Dataset\")\n# multiple dataset path\ndataset_parser.add_argument(\"--dataset_paths\", nargs=\"+\")\ndataset_parser.add_argument(\"--gt_path\")\ndataset_parser.add_argument(\"--fps\", default=1, type=float)\ndataset_parser.add_argument(\"--len_cap\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"tensor\", type=str)", "dataset_parser.add_argument(\"--len_cap\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"tensor\", type=str)\ndataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\ndataset_parser.add_argument(\n    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEGPY\"\n)\ndataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\ndataset_parser.add_argument(\"--tta\", action=\"store_true\")\ndataset_parser.add_argument(\n    \"--mode\", default=\"whole\", choices=[\"whole\", \"eval\", \"test\", \"tune\"]", "dataset_parser.add_argument(\n    \"--mode\", default=\"whole\", choices=[\"whole\", \"eval\", \"test\", \"tune\"]\n)\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference.py\")", ")\nlogger = logging.getLogger(\"inference.py\")\nlogger.setLevel(logging.INFO)\n\n\ndef main(args):\n    success = False\n    if args.processes > 1 and args.distributed_size > 1:\n        raise Exception(\n            \"Set either --processes (single-machine distributed) or \"\n            \"both --distributed_size and --distributed_rank (arbitrary \"\n            \"distributed)\"\n        )\n\n    if args.video_reader == \"DECORD\":\n        import subprocess\n\n        subprocess.run(\n            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n            check=True,\n        )\n\n    with tempfile.TemporaryDirectory() as tmp_path:\n        splits = [\n            \"_\".join(p.split(\"/\")[-2:]) for p in args.dataset_paths\n        ]  # ./vsc/eval_subset/reference -> eval_subset_reference\n        os.makedirs(args.output_path, exist_ok=True)\n        if args.scratch_path:\n            os.makedirs(args.scratch_path, exist_ok=True)\n        else:\n            args.scratch_path = tmp_path\n        if args.processes > 1:\n            processes = []\n            logger.info(f\"Spawning {args.processes} processes\")\n            accelerator = Accelerator[args.accelerator.upper()]\n            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n            multiprocessing.set_start_method(\"spawn\")\n            worker_files = []\n            try:\n                for rank in range(args.processes):\n                    output_files = [\n                        os.path.join(args.scratch_path, f\"{split}_{rank}.npz\")\n                        for split in splits\n                    ]\n                    worker_files.append(output_files)\n                    p = multiprocessing.Process(\n                        target=distributed_worker_process,\n                        args=(\n                            args,\n                            rank,\n                            args.processes,\n                            backend,\n                            output_files,\n                            args.dataset_paths,\n                            args.tta,\n                        ),\n                    )\n                    processes.append(p)\n                    p.start()\n                worker_success = []\n                for p in processes:\n                    p.join()\n                    worker_success.append(p.exitcode == os.EX_OK)\n                success = all(worker_success)\n            finally:\n                for p in processes:\n                    p.kill()\n            if success:\n\n                def merge_feature_files(filenames, output_filename: str) -> int:\n                    features = []\n                    for fn in filenames:\n                        features.extend(load_features(fn))\n                    features = sorted(features, key=lambda x: x.video_id)\n                    store_features(output_filename, features)\n                    return len(features)\n\n                output_files_each_split = [list(x) for x in zip(*worker_files)]\n                for files, split in zip(output_files_each_split, splits):\n                    output_file = os.path.join(args.output_path, f\"{split}.npz\")\n                    num_files = merge_feature_files(files, output_file)\n                    logger.info(\n                        f\"Features for {num_files} videos saved to {output_file}\"\n                    )\n\n        else:\n            output_files = [\n                os.path.join(args.output_path, f\"{split}.npz\") for split in splits\n            ]\n            worker_process(\n                args,\n                args.distributed_rank,\n                args.distributed_size,\n                output_files,\n                args.dataset_paths,\n                args.tta,\n            )\n            success = True\n\n    if success:\n        logger.info(\"Inference succeeded.\")\n    else:\n        logger.error(\"Inference FAILED!\")\n\n    if not success or args.gt_path is None:\n        return\n\n    logger.info(\"Evaluating results\")\n\n    evaluate(\n        queries=load_features(os.path.join(args.output_path, f\"{splits[0]}.npz\")),\n        refs=load_features(os.path.join(args.output_path, f\"{splits[1]}.npz\")),\n        noises=load_features(os.path.join(args.output_path, f\"{splits[-1]}.npz\")),\n        gt_path=args.gt_path,\n        output_path=args.output_path,\n    )", "\n\ndef distributed_worker_process(pargs, rank, world_size, backend, *args, **kwargs):\n    from torch import distributed\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"19529\"\n    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n    worker_process(pargs, rank, world_size, *args, **kwargs)\n", "\n\ndef worker_process(\n    args,\n    rank,\n    world_size,\n    output_files: List[str],\n    dataset_paths: List[str],\n    tta: bool = False,\n):\n    from src.inference_impl import VideoDataset, get_device, run_inference\n    from src.model import create_model_in_runtime\n    from torch.utils.data import DataLoader\n\n    logger.info(f\"Starting worker {rank} of {world_size}.\")\n    device = get_device(args, rank, world_size)\n    logger.info(\"Loading model\")\n    model, transforms = create_model_in_runtime(transforms_device=device)\n    model = model.to(device).eval()\n    logger.info(\"Setting up dataset\")\n    extensions = args.video_extensions.split(\",\")\n    video_reader = VideoReaderType[args.video_reader.upper()]\n\n    if tta:\n        if len(dataset_paths) == 3:\n            do_tta_list = [\n                True,\n                False,\n                False,\n            ]\n        elif len(dataset_paths) == 4:\n            do_tta_list = [\n                True,\n                False,\n                True,\n                False,\n            ]\n        elif len(dataset_paths) == 1:\n            do_tta_list = [\n                True,\n            ]\n        else:\n            raise ValueError(\"TTA requires 3 or 4 datasets\")\n    else:\n        do_tta_list = [False] * len(dataset_paths)\n\n    for output_filename, dataset_path, do_tta in zip(\n        output_files, dataset_paths, do_tta_list\n    ):\n        batch_size = 1 if do_tta else args.batch_size\n        dataset = VideoDataset(\n            dataset_path,\n            fps=args.fps,\n            read_type=args.read_type,\n            batch_size=batch_size,\n            extensions=extensions,\n            distributed_world_size=world_size,\n            distributed_rank=rank,\n            video_reader=video_reader,\n            ffmpeg_path=args.ffmpeg_path,\n            filter_by_asr=do_tta,\n        )\n        loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\n        progress = tqdm.tqdm(total=dataset.num_videos())\n        video_features = []\n        for vf in run_inference(loader, model, device, transforms, do_tta):\n            video_features.append(vf)\n            progress.update()\n\n        store_features(output_filename, video_features)", "\n\ndef evaluate(queries, refs, noises, gt_path, output_path, sn_method=\"SN\"):\n    import faiss\n    from src.inference_impl import match\n    from src.postproc import sliding_pca, sliding_pca_with_ref\n    from src.score_normalization import (\n        negative_embedding_subtraction,\n        score_normalize_with_ref,\n    )\n    from src.vsc.metrics import (\n        CandidatePair,\n        Match,\n        average_precision,\n        evaluate_matching_track,\n    )\n\n    stride = 2\n    video_features, pca_matrix = sliding_pca_with_ref(\n        queries=queries,\n        refs=refs,\n        noises=noises,\n        stride=stride,\n    )\n    queries = video_features[\"query\"]\n    refs = video_features[\"ref\"]\n    noises = video_features[\"noise\"]\n    store_features(Path(output_path) / \"noise.npz\", noises)\n    faiss.write_VectorTransform(pca_matrix, str(Path(output_path) / \"pca_matrix.bin\"))\n\n    if sn_method == \"SN\":\n        queries, refs = score_normalize_with_ref(\n            queries=queries,\n            refs=refs,\n            score_norm_refs=noises,\n            beta=1.2,\n        )\n\n    else:\n        queries, refs = negative_embedding_subtraction(\n            queries=queries,\n            refs=refs,\n            score_norm_refs=noises,\n            pre_l2_normalize=False,\n            post_l2_normalize=False,\n            beta=0.8,\n            k=10,\n            alpha=2.0,\n        )\n\n    store_features(Path(output_path) / \"processed_ref.npz\", refs)\n\n    candidates, matches = match(\n        queries=queries,\n        refs=refs,\n        output_file=None,\n        return_results=True,\n        similarity_bias=0.5 if sn_method == \"SN\" else 0.0,\n        model_type=\"TN\",\n        tn_max_step=5,\n        min_length=3,\n        tn_top_k=2,\n        max_path=200,\n        min_sim=0.1,\n        max_iou=1.0,\n    )\n\n    gt_matches = Match.read_csv(gt_path, is_gt=True)\n    ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\n    candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n    CandidatePair.write_csv(candidates, Path(output_path) / \"candidates.csv\")\n\n    match_file = Path(output_path) / \"matches.csv\"\n    Match.write_csv(matches, match_file, drop_dup=True)\n    match_metrics = evaluate_matching_track(gt_path, match_file)\n\n    logger.info(f\"uAP: {ap.ap:.4f}, segmentAP: {match_metrics.segment_ap.ap:.4f}\")", "\n\ndef tune(\n    queries,\n    refs,\n    noises,\n    gt_path,\n    output_path,\n):\n    from sklearn.model_selection import ParameterGrid\n    from src.inference_impl import match\n    from src.score_normalization import (\n        negative_embedding_subtraction,\n        score_normalize_with_ref,\n    )\n    from src.vsc.metrics import (\n        CandidatePair,\n        Match,\n        average_precision,\n        evaluate_matching_track,\n    )\n\n    param_grid = [\n        {\n            \"sn_method\": [\"SN\"],\n            \"beta\": [1.2],\n            # \"k\": [10],\n            # \"alpha\": [2.0],\n            \"similarity_bias\": [0.5],\n            \"retrieve_per_query\": [1200],\n            \"candidates_per_query\": [25],\n            \"tn_max_step\": [5],\n            \"min_length\": [3],\n            \"tn_top_k\": [2],\n            \"max_path\": [70, 100, 150, 200],\n            \"min_sim\": [0.1],\n            \"max_iou\": [1.0],\n        }\n    ]\n\n    rows = []\n\n    for params in ParameterGrid(param_grid):\n\n        if params[\"sn_method\"] == \"SN\":\n            norm_queries, norm_refs = score_normalize_with_ref(\n                queries=queries,\n                refs=refs,\n                score_norm_refs=noises,\n                beta=params[\"beta\"],\n            )\n\n        else:\n            norm_queries, norm_refs = negative_embedding_subtraction(\n                queries=queries,\n                refs=refs,\n                score_norm_refs=noises,\n                pre_l2_normalize=False,\n                post_l2_normalize=False,\n                beta=params[\"beta\"],\n                k=params[\"k\"],\n                alpha=params[\"alpha\"],\n            )\n\n        candidates, matches = match(\n            queries=norm_queries,\n            refs=norm_refs,\n            output_file=None,\n            return_results=True,\n            similarity_bias=params[\"similarity_bias\"],\n            model_type=\"TN\",\n            tn_max_step=params[\"tn_max_step\"],\n            min_length=params[\"min_length\"],\n            concurrency=16,\n            tn_top_k=params[\"tn_top_k\"],\n            max_path=params[\"max_path\"],\n            min_sim=params[\"min_sim\"],\n            max_iou=params[\"max_iou\"],\n            discontinue=3,\n            sum_sim=8,\n            ave_sim=0.3,\n            diagonal_thres=10,\n            iou_thresh=0.9,\n            min_bins=1,\n            max_peaks=100,\n            min_peaks=10,\n            retrieve_per_query=params[\"retrieve_per_query\"],\n            candidates_per_query=params[\"candidates_per_query\"],\n        )\n\n        gt_matches = Match.read_csv(gt_path, is_gt=True)\n        ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\n        # candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n        # CandidatePair.write_csv(candidates, Path(output_path) / 'candidates.csv')\n\n        print(f\"{len(matches)=}\")\n        matches = pd.DataFrame(matches)\n        matches = matches[\n            [\n                \"query_id\",\n                \"ref_id\",\n                \"query_start\",\n                \"query_end\",\n                \"ref_start\",\n                \"ref_end\",\n                \"score\",\n            ]\n        ]\n        matches = matches.sort_values(\"score\", ascending=False)\n\n        match_file = Path(output_path) / \"tune_matches.csv\"\n        matches.to_csv(match_file, index=False)\n        match_metrics = evaluate_matching_track(gt_path, match_file)\n\n        rows.append(\n            {\n                \"uAP\": ap.ap,\n                \"segmentAP\": match_metrics.segment_ap.ap,\n                \"len_matches\": len(matches),\n                **params,\n            }\n        )\n        print(rows[-1])\n\n    df = pd.DataFrame(rows)\n    print(df.to_csv())\n    df.to_csv(\"tuning_result.csv\", index=False)", "\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n\n    if args.mode == \"eval\":\n        evaluate(\n            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n            sn_method=\"SN\",\n        )\n    elif args.mode == \"test\":\n        evaluate(\n            queries=load_features(\n                os.path.join(args.output_path, f\"phase_2_uB82_query.npz\")\n            ),\n            refs=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            noises=load_features(\n                os.path.join(args.output_path, f\"train_reference.npz\")\n            ),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n            sn_method=\"SN\",\n        )\n    elif args.mode == \"tune\":\n        tune(\n            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n            gt_path=args.gt_path,\n            output_path=args.output_path,\n        )\n    else:\n        main(args)", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/postproc.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nfrom typing import Dict, List\n\nimport numpy as np", "\nimport numpy as np\nfrom numpy.lib.stride_tricks import sliding_window_view\nfrom src.vsc.index import VideoFeature\n\n\ndef _sliding_window_and_concat(\n    vfs: List[VideoFeature], stride: int = 1\n) -> Dict[str, List[VideoFeature]]:\n\n    new_vfs = []\n    kernel = [0.1, 0.2, 0.4, 0.2, 0.1]\n    num_stacks = len(kernel)\n\n    for vf in vfs:\n        n, d = vf.feature.shape\n\n        num_views = 1\n        for i in range(1, len(vf.timestamps)):\n            if vf.timestamps[i] <= vf.timestamps[i - 1]:\n                num_views += 1\n\n        if n / num_views <= stride:\n            continue\n\n        reshaped_feats = vf.feature.reshape(num_views, -1, d)\n        reshaped_ts = vf.timestamps.reshape(num_views, -1)\n\n        new_feats = []\n        new_timestamps = []\n        for i in range(num_views):\n            new_feat = reshaped_feats[i]\n            new_ts = reshaped_ts[i]\n            new_feat = np.concatenate(\n                [new_feat[: num_stacks // 2], new_feat, new_feat[-(num_stacks // 2) :]],\n                axis=0,\n            )\n            new_feat = sliding_window_view(new_feat, num_stacks, axis=0)\n            assert len(new_feat) == len(reshaped_feats[i])\n            if stride > 1:\n                new_feat = new_feat[stride // 2 :: stride]\n                new_ts = new_ts[stride // 2 :: stride]\n            weight = np.array(kernel).reshape(1, 1, -1)\n            new_feat = new_feat * weight\n            new_feat = new_feat.transpose(0, 2, 1).reshape(\n                -1, new_feat.shape[1] * num_stacks\n            )\n            new_feats.append(new_feat)\n            new_timestamps.append(new_ts)\n\n        new_feats = np.concatenate(new_feats, axis=0)\n        new_timestamps = np.concatenate(new_timestamps, axis=0)\n\n        new_vfs.append(\n            VideoFeature(\n                video_id=vf.video_id,\n                timestamps=new_timestamps,\n                feature=new_feats,\n            )\n        )\n\n    return new_vfs", "\n\ndef _fit_pca(noises, n_components=512) -> Dict[str, List[VideoFeature]]:\n    import faiss\n\n    noise_feats = np.concatenate([vf.feature for vf in noises])\n    noise_feats = noise_feats.astype(np.float32)\n    mat = faiss.PCAMatrix(noise_feats.shape[-1], n_components)\n    mat.train(noise_feats)\n    assert mat.is_trained\n    return mat", "\n\ndef _apply_pca(vfs, mat) -> Dict[str, List[VideoFeature]]:\n    new_vfs = []\n    for vf in vfs:\n        new_feat = mat.apply(vf.feature.astype(np.float32))\n        # new_feat = new_feat / np.linalg.norm(new_feat, axis=-1, keepdims=True)\n        new_vfs.append(\n            VideoFeature(\n                video_id=vf.video_id,\n                timestamps=vf.timestamps,\n                feature=new_feat,\n            )\n        )\n    return new_vfs", "\n\ndef sliding_pca(\n    queries: List[VideoFeature],\n    mat: \"faiss.PCAMatrix\",\n    stride: int = 1,\n) -> List[VideoFeature]:\n    queries = _sliding_window_and_concat(queries, stride=stride)\n    queries = _apply_pca(queries, mat)\n    return queries", "\n\ndef sliding_pca_with_ref(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    noises: List[VideoFeature],\n    stride: int = 1,\n    n_components: int = 512,\n) -> Dict[str, List[VideoFeature]]:\n\n    video_features = {\n        \"query\": _sliding_window_and_concat(queries, stride=stride),\n        \"ref\": _sliding_window_and_concat(refs, stride=stride),\n        \"noise\": _sliding_window_and_concat(noises, stride=stride),\n    }\n    mat = _fit_pca(video_features[\"noise\"], n_components=n_components)\n\n    for k, vfs in video_features.items():\n        video_features[k] = _apply_pca(vfs, mat)\n\n    return video_features, mat", "\n\ndef ensemble_match_results(\n    path: List[str], output_file: str = \"match_ensemble.csv\"\n) -> str:\n    import os\n\n    import pandas as pd\n\n    if path[0][-4:] == \".csv\":\n        match_result_paths = path\n    else:\n        match_result_paths = [f\"{p}/matches.csv\" for p in path]\n    results = []\n    for result_path in match_result_paths:\n        results.append(pd.read_csv(result_path))\n\n    df_all = pd.concat(results, axis=0, ignore_index=True)\n    df_ensemble = pd.DataFrame(\n        columns=[\n            \"query_id\",\n            \"ref_id\",\n            \"query_start\",\n            \"query_end\",\n            \"ref_start\",\n            \"ref_end\",\n            \"score\",\n        ]\n    )\n    query_id_all = set(df_all[\"query_id\"])\n    for query_id in query_id_all:\n        df_topk = df_all.loc[df_all[\"query_id\"] == query_id].sort_values(\n            by=\"score\", ascending=False\n        )[:400]\n        df_ensemble = pd.concat([df_ensemble, df_topk], ignore_index=True)\n    df_ensemble[\"score\"] = df_ensemble.groupby(\n        [\"query_id\", \"ref_id\", \"query_start\", \"query_end\", \"ref_start\", \"ref_end\"]\n    )[\"score\"].transform(\"max\")\n    df_ensemble.drop_duplicates(\n        [\"query_id\", \"ref_id\", \"query_start\", \"query_end\", \"ref_start\", \"ref_end\"],\n        keep=\"first\",\n        inplace=True,\n    )\n    match_file_final = f\"{output_file}\"\n    df_ensemble.to_csv(match_file_final, index=False)\n    return match_file_final", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/model.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nfrom __future__ import annotations\n\nimport timm", "\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\n\nclass ISCNet(nn.Module):\n    \"\"\"\n    Feature extractor for image copy-detection task.\n\n    Args:\n        backbone (`nn.Module`):\n            Backbone module.\n        fc_dim (`int=256`):\n            Feature dimension of the fc layer.\n        p (`float=1.0`):\n            Power used in gem pooling for training.\n        eval_p (`float=1.0`):\n            Power used in gem pooling for evaluation. In practice, using a larger power\n            for evaluation than training can yield a better performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        fc_dim: int = 256,\n        p: float = 1.0,\n        eval_p: float = 1.0,\n        l2_normalize=True,\n    ):\n        super().__init__()\n\n        self.backbone = backbone\n        if hasattr(backbone, \"num_features\"):\n            self.is_cnn = False\n            in_channels = backbone.num_features\n        else:\n            self.is_cnn = True\n            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        self.p = p\n        self.eval_p = eval_p\n        self.l2_normalize = l2_normalize\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.is_cnn:\n            batch_size = x.shape[0]\n            x = self.backbone(x)[-1]\n            p = self.p if self.training else self.eval_p\n            x = gem(x, p).view(batch_size, -1)\n        else:\n            x = self.backbone(x)\n        x = self.fc(x)\n        x = self.bn(x)\n        if self.l2_normalize:\n            x = F.normalize(x)\n        return x", "class ISCNet(nn.Module):\n    \"\"\"\n    Feature extractor for image copy-detection task.\n\n    Args:\n        backbone (`nn.Module`):\n            Backbone module.\n        fc_dim (`int=256`):\n            Feature dimension of the fc layer.\n        p (`float=1.0`):\n            Power used in gem pooling for training.\n        eval_p (`float=1.0`):\n            Power used in gem pooling for evaluation. In practice, using a larger power\n            for evaluation than training can yield a better performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        fc_dim: int = 256,\n        p: float = 1.0,\n        eval_p: float = 1.0,\n        l2_normalize=True,\n    ):\n        super().__init__()\n\n        self.backbone = backbone\n        if hasattr(backbone, \"num_features\"):\n            self.is_cnn = False\n            in_channels = backbone.num_features\n        else:\n            self.is_cnn = True\n            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        self.p = p\n        self.eval_p = eval_p\n        self.l2_normalize = l2_normalize\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.is_cnn:\n            batch_size = x.shape[0]\n            x = self.backbone(x)[-1]\n            p = self.p if self.training else self.eval_p\n            x = gem(x, p).view(batch_size, -1)\n        else:\n            x = self.backbone(x)\n        x = self.fc(x)\n        x = self.bn(x)\n        if self.l2_normalize:\n            x = F.normalize(x)\n        return x", "\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\n\nclass ConstDivider(nn.Module):\n    def __init__(self, c=255.0):\n        super().__init__()\n        self.c = c\n\n    def forward(self, x):\n        return x / self.c", "\n\ndef create_model_in_runtime(transforms_device=\"cpu\"):\n    weight_path = \"./model_assets/isc_ft_v107.pth.tar\"\n    ckpt = torch.load(weight_path)\n    arch = ckpt[\"arch\"]  # tf_efficientnetv2_m_in21ft1k\n    input_size = ckpt[\"args\"].input_size\n\n    backbone = timm.create_model(arch, features_only=True)\n    model = ISCNet(\n        backbone=backbone,\n        fc_dim=256,\n        p=1.0,\n        eval_p=1.0,\n        l2_normalize=True,\n    )\n    model.to(\"cuda\").train(False)\n\n    state_dict = {}\n    for s in ckpt[\"state_dict\"]:\n        state_dict[s.replace(\"module.\", \"\")] = ckpt[\"state_dict\"][s]\n\n    model.load_state_dict(state_dict)\n\n    if transforms_device == \"cpu\":\n        preprocessor = transforms.Compose(\n            [\n                transforms.Resize((input_size, input_size)),\n                # transforms.ToTensor(),\n                ConstDivider(c=255.0),\n                transforms.Normalize(\n                    mean=backbone.default_cfg[\"mean\"],\n                    std=backbone.default_cfg[\"std\"],\n                ),\n            ]\n        )\n    else:\n        preprocessor = nn.Sequential(\n            transforms.Resize((input_size, input_size)),\n            ConstDivider(c=255.0),\n            transforms.Normalize(\n                mean=backbone.default_cfg[\"mean\"],\n                std=backbone.default_cfg[\"std\"],\n            ),\n        )\n        preprocessor = torch.jit.script(preprocessor)\n        preprocessor.to(transforms_device)\n\n    return model, preprocessor", "\n\ndef create_model_in_runtime_2(transforms_device=\"cpu\"):\n    weight_path = \"./model_assets/disc21_ft_vit_base_r50_s16_224_in21k.pth\"\n    state_dict = torch.load(weight_path, map_location=\"cpu\")\n    arch = \"vit_base_r50_s16_224_in21k\"\n    input_size = 448\n    feature_dim = 512\n\n    try:\n        backbone = timm.create_model(arch, features_only=True, pretrained=False)\n    except:\n        backbone = timm.create_model(\n            arch, pretrained=False, num_classes=0, img_size=(input_size, input_size)\n        )\n    model = ISCNet(\n        backbone=backbone,\n        fc_dim=feature_dim,\n        p=1.0,\n        eval_p=1.0,\n        l2_normalize=True,\n    )\n\n    model.load_state_dict(state_dict)\n\n    model.to(\"cuda\").train(False)\n\n    if transforms_device == \"cpu\":\n        preprocessor = transforms.Compose(\n            [\n                transforms.Resize((input_size, input_size)),\n                # transforms.ToTensor(),\n                ConstDivider(c=255.0),\n                transforms.Normalize(\n                    mean=backbone.default_cfg[\"mean\"],\n                    std=backbone.default_cfg[\"std\"],\n                ),\n            ]\n        )\n    else:\n        preprocessor = nn.Sequential(\n            transforms.Resize((input_size, input_size)),\n            ConstDivider(c=255.0),\n            transforms.Normalize(\n                mean=backbone.default_cfg[\"mean\"],\n                std=backbone.default_cfg[\"std\"],\n            ),\n        )\n        preprocessor = torch.jit.script(preprocessor)\n        preprocessor.to(transforms_device)\n\n    return model, preprocessor", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Inference script.\n\nThis is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors", "This is split into inference and inference_impl to avoid initializing cuda\nbefore processes are created that may use cuda, which can lead to errors\nin some runtime environments.\n\nWe import inference_impl, which imports libraries that may initialize cuda,\nin two circumstances: from worker processes after the main process has\nforked workers, or from the main process after worker processes have been\njoined.\n\"\"\"\n", "\"\"\"\n\nimport argparse\nimport enum\nimport glob\nimport logging\nimport os\nimport tempfile\n\nfrom torch import multiprocessing", "\nfrom torch import multiprocessing\n\n\nclass InferenceTransforms(enum.Enum):\n    # Aspect-ratio preserving resize to 288\n    RESIZE_288 = enum.auto()\n    # Resize the short edge to 320, then take the center crop\n    RESIZE_320_CENTER = enum.auto()\n", "\n\nclass Accelerator(enum.Enum):\n    CPU = enum.auto()\n    CUDA = enum.auto()\n\n\nclass VideoReaderType(enum.Enum):\n    FFMPEG = enum.auto()\n    FFMPEGPY = enum.auto()\n    DECORD = enum.auto()", "\n\nparser = argparse.ArgumentParser()\ninference_parser = parser.add_argument_group(\"Inference\")\n# inference_parser.add_argument(\"--torchscript_path\", required=True)\ninference_parser.add_argument(\"--batch_size\", type=int, default=32)\ninference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\ninference_parser.add_argument(\"--distributed_size\", type=int, default=1)\ninference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(", "inference_parser.add_argument(\"--processes\", type=int, default=1)\ninference_parser.add_argument(\n    \"--transforms\",\n    choices=[x.name for x in InferenceTransforms],\n    default=\"RESIZE_320_CENTER\",\n)\ninference_parser.add_argument(\n    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n)\ninference_parser.add_argument(\"--output_file\", required=True)", ")\ninference_parser.add_argument(\"--output_file\", required=True)\ninference_parser.add_argument(\"--scratch_path\", required=False)\n\ndataset_parser = parser.add_argument_group(\"Dataset\")\ndataset_parser.add_argument(\"--dataset_path\", required=True)\ndataset_parser.add_argument(\"--fps\", default=1, type=float)\ndataset_parser.add_argument(\"--stride\", type=int)\ndataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)\ndataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")", "dataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)\ndataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\ndataset_parser.add_argument(\n    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEG\"\n)\ndataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\ndataset_parser.add_argument(\n    \"--score_norm_features\",\n    default=[\"./src/isc/test_noise.npz\", \"./src/vit/test_noise.npz\"],\n)", "    default=[\"./src/isc/test_noise.npz\", \"./src/vit/test_noise.npz\"],\n)\ndataset_parser.add_argument(\n    \"--reference_features\",\n    default=[\"./src/isc/test_processed_ref.npz\", \"./src/vit/test_processed_ref.npz\"],\n)\ndataset_parser.add_argument(\"--model\", default=[\"isc\", \"vit\"])\ndataset_parser.add_argument(\n    \"--pca_matrix\",\n    default=[\"./src/isc/test_pca_matrix.bin\", \"./src/vit/test_pca_matrix.bin\"],", "    \"--pca_matrix\",\n    default=[\"./src/isc/test_pca_matrix.bin\", \"./src/vit/test_pca_matrix.bin\"],\n)\ndataset_parser.add_argument(\"--gt_path\")\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",", "    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference.py\")\nlogger.setLevel(logging.INFO)\n\n\ndef main(args):\n    success = False\n    if args.processes > 1 and args.distributed_size > 1:\n        raise Exception(\n            \"Set either --processes (single-machine distributed) or \"\n            \"both --distributed_size and --distributed_rank (arbitrary \"\n            \"distributed)\"\n        )\n\n    if args.video_reader == \"DECORD\":\n        import subprocess\n\n        subprocess.run(\n            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n            check=True,\n        )\n\n    for model, ref_path, noise_path, pca_matrix in zip(\n        args.model, args.reference_features, args.score_norm_features, args.pca_matrix\n    ):\n        with tempfile.TemporaryDirectory() as tmp_path:\n            os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n            if args.scratch_path:\n                os.makedirs(args.scratch_path, exist_ok=True)\n            else:\n                args.scratch_path = tmp_path\n            if args.processes > 1:\n                processes = []\n                logger.info(f\"Spawning {args.processes} processes\")\n                accelerator = Accelerator[args.accelerator.upper()]\n                backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n                # multiprocessing.set_start_method(\"spawn\")\n                try:\n                    multiprocessing.set_start_method(\"spawn\")\n                except RuntimeError:\n                    pass\n                worker_files = []\n                try:\n                    for rank in range(args.processes):\n                        worker_file = os.path.join(args.scratch_path, f\"{rank}.npz\")\n                        worker_files.append(worker_file)\n                        p = multiprocessing.Process(\n                            target=distributed_worker_process,\n                            args=(\n                                args,\n                                rank,\n                                args.processes,\n                                backend,\n                                worker_file,\n                                model,\n                                ref_path,\n                                noise_path,\n                                pca_matrix,\n                            ),\n                        )\n                        processes.append(p)\n                        p.start()\n                    worker_success = []\n                    for p in processes:\n                        p.join()\n                        worker_success.append(p.exitcode == os.EX_OK)\n                    success = all(worker_success)\n                finally:\n                    for p in processes:\n                        p.kill()\n                if success:\n                    from .inference_impl import merge_feature_files  # @manual\n\n                    num_files = merge_feature_files(worker_files, args.output_file)\n                    logger.info(\n                        f\"Features for {num_files} videos saved to {args.output_file}\"\n                    )\n\n            else:\n                worker_process(\n                    args,\n                    args.distributed_rank,\n                    args.distributed_size,\n                    args.output_file,\n                    model,\n                    ref_path,\n                    noise_path,\n                    pca_matrix,\n                )\n                success = True\n\n    matches_list = glob.glob(f\"{os.path.dirname(args.output_file)}/output/*.csv\")\n    from src.postproc import ensemble_match_results\n\n    math_file = ensemble_match_results(matches_list, args.output_file)\n\n    if success:\n        logger.info(\"Inference succeeded.\")\n    else:\n        logger.error(\"Inference FAILED!\")", "\n\ndef distributed_worker_process(\n    args, rank, world_size, backend, output_filename, model, ref_path, noise_path, pca_matrix\n):\n    from torch import distributed\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"19529\"\n    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n    worker_process(\n        args, rank, world_size, output_filename, model, ref_path, noise_path, pca_matrix\n    )", "\n\ndef worker_process(*args):\n    # Late import: initialize cuda after worker spawn.\n    from src.inference_impl import worker_process as worker_impl  # @manual\n\n    return worker_impl(*args)\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)", "\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference_impl.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport glob\nimport itertools\nimport logging\nimport os\nfrom pathlib import Path", "import os\nfrom pathlib import Path\nfrom typing import Iterable, List, Tuple\n\nimport faiss\nimport numpy as np\nimport torch\nimport tqdm\nfrom src.inference import Accelerator, VideoReaderType\nfrom src.model import create_model_in_runtime, create_model_in_runtime_2", "from src.inference import Accelerator, VideoReaderType\nfrom src.model import create_model_in_runtime, create_model_in_runtime_2\nfrom src.postproc import sliding_pca\nfrom src.score_normalization import score_normalize\nfrom src.tta import (\n    TTA4ViewsTransform,\n    TTA5ViewsTransform,\n)\nfrom src.video_reader.ffmpeg_py_video_reader import FFMpegPyVideoReader\nfrom src.video_reader.ffmpeg_video_reader import FFMpegVideoReader", "from src.video_reader.ffmpeg_py_video_reader import FFMpegPyVideoReader\nfrom src.video_reader.ffmpeg_video_reader import FFMpegVideoReader\nfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.localization import (\n    VCSLLocalizationMatchScore,\n)\nfrom src.vsc.metrics import (\n    CandidatePair,\n    Match,", "    CandidatePair,\n    Match,\n)\nfrom src.vsc.storage import load_features, store_features\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom torch.utils.data._utils.collate import default_collate\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,", "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"inference_impl.py\")\nlogger.setLevel(logging.INFO)\n\n\nclass VideoDataset(IterableDataset):\n    \"\"\"Decodes video frames at a fixed FPS via ffmpeg.\"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        fps: float,\n        read_type: str = \"pil\",\n        batch_size=None,\n        img_transform=None,\n        extensions=(\"mp4\",),\n        distributed_rank=0,\n        distributed_world_size=1,\n        video_reader=VideoReaderType.FFMPEG,\n        ffmpeg_path=\"ffmpeg\",\n        filter_by_asr=False,\n    ):\n        assert distributed_rank < distributed_world_size\n        self.path = path\n        self.fps = fps\n        self.read_type = read_type\n        self.batch_size = batch_size\n        self.img_transform = img_transform\n        self.video_reader = video_reader\n        self.ffmpeg_path = ffmpeg_path\n        if len(extensions) == 1:\n            filenames = glob.glob(os.path.join(path, f\"*.{extensions[0]}\"))\n        else:\n            filenames = glob.glob(os.path.join(path, \"*.*\"))\n            filenames = (fn for fn in filenames if fn.rsplit(\".\", 1)[-1] in extensions)\n\n        filenames = [\n            name for name in filenames if Path(name).stem not in [\"R102796\", \"R133364\"]\n        ]\n        self.videos = sorted(filenames)\n\n        if not self.videos:\n            raise Exception(\"No videos found!\")\n        assert distributed_rank < distributed_world_size\n        self.rank = distributed_rank\n        self.world_size = distributed_world_size\n        self.selected_videos = [\n            (i, video)\n            for (i, video) in enumerate(self.videos)\n            if (i % self.world_size) == self.rank\n        ]\n\n    def num_videos(self) -> int:\n        return len(self.selected_videos)\n\n    def __iter__(self):\n        for i, video in self.selected_videos:\n            if self.batch_size:\n                frames = self.read_frames(i, video)\n                while True:\n                    batch = list(itertools.islice(frames, self.batch_size))\n                    if not batch:\n                        break\n                    yield default_collate(batch)\n            else:\n                yield from self.read_frames(i, video)\n\n    def read_frames(self, video_id, video):\n        video_name = os.path.basename(video)\n        name = os.path.basename(video_name).split(\".\")[0]\n        if self.video_reader == VideoReaderType.FFMPEG:\n            reader = FFMpegVideoReader(\n                video_path=video,\n                required_fps=self.fps,\n                output_type=self.read_type,\n                ffmpeg_path=self.ffmpeg_path,\n            )\n        elif self.video_reader == VideoReaderType.FFMPEGPY:\n            reader = FFMpegPyVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        elif self.video_reader == VideoReaderType.DECORD:\n            from src.video_reader.decord_video_reader import DecordVideoReader\n\n            reader = DecordVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        else:\n            raise ValueError(f\"VideoReaderType: {self.video_reader} not supported\")\n        for start_timestamp, end_timestamp, frame in reader.frames():\n            if self.img_transform:\n                frame = self.img_transform(frame)\n            record = {\n                \"name\": name,\n                \"timestamp\": (np.array(start_timestamp) + np.array(end_timestamp)) / 2,\n                \"input\": frame,\n            }\n            yield record", "class VideoDataset(IterableDataset):\n    \"\"\"Decodes video frames at a fixed FPS via ffmpeg.\"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        fps: float,\n        read_type: str = \"pil\",\n        batch_size=None,\n        img_transform=None,\n        extensions=(\"mp4\",),\n        distributed_rank=0,\n        distributed_world_size=1,\n        video_reader=VideoReaderType.FFMPEG,\n        ffmpeg_path=\"ffmpeg\",\n        filter_by_asr=False,\n    ):\n        assert distributed_rank < distributed_world_size\n        self.path = path\n        self.fps = fps\n        self.read_type = read_type\n        self.batch_size = batch_size\n        self.img_transform = img_transform\n        self.video_reader = video_reader\n        self.ffmpeg_path = ffmpeg_path\n        if len(extensions) == 1:\n            filenames = glob.glob(os.path.join(path, f\"*.{extensions[0]}\"))\n        else:\n            filenames = glob.glob(os.path.join(path, \"*.*\"))\n            filenames = (fn for fn in filenames if fn.rsplit(\".\", 1)[-1] in extensions)\n\n        filenames = [\n            name for name in filenames if Path(name).stem not in [\"R102796\", \"R133364\"]\n        ]\n        self.videos = sorted(filenames)\n\n        if not self.videos:\n            raise Exception(\"No videos found!\")\n        assert distributed_rank < distributed_world_size\n        self.rank = distributed_rank\n        self.world_size = distributed_world_size\n        self.selected_videos = [\n            (i, video)\n            for (i, video) in enumerate(self.videos)\n            if (i % self.world_size) == self.rank\n        ]\n\n    def num_videos(self) -> int:\n        return len(self.selected_videos)\n\n    def __iter__(self):\n        for i, video in self.selected_videos:\n            if self.batch_size:\n                frames = self.read_frames(i, video)\n                while True:\n                    batch = list(itertools.islice(frames, self.batch_size))\n                    if not batch:\n                        break\n                    yield default_collate(batch)\n            else:\n                yield from self.read_frames(i, video)\n\n    def read_frames(self, video_id, video):\n        video_name = os.path.basename(video)\n        name = os.path.basename(video_name).split(\".\")[0]\n        if self.video_reader == VideoReaderType.FFMPEG:\n            reader = FFMpegVideoReader(\n                video_path=video,\n                required_fps=self.fps,\n                output_type=self.read_type,\n                ffmpeg_path=self.ffmpeg_path,\n            )\n        elif self.video_reader == VideoReaderType.FFMPEGPY:\n            reader = FFMpegPyVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        elif self.video_reader == VideoReaderType.DECORD:\n            from src.video_reader.decord_video_reader import DecordVideoReader\n\n            reader = DecordVideoReader(\n                video_path=video, required_fps=self.fps, output_type=self.read_type\n            )\n        else:\n            raise ValueError(f\"VideoReaderType: {self.video_reader} not supported\")\n        for start_timestamp, end_timestamp, frame in reader.frames():\n            if self.img_transform:\n                frame = self.img_transform(frame)\n            record = {\n                \"name\": name,\n                \"timestamp\": (np.array(start_timestamp) + np.array(end_timestamp)) / 2,\n                \"input\": frame,\n            }\n            yield record", "\n\ndef should_use_cuda(args) -> bool:\n    accelerator = Accelerator[args.accelerator.upper()]\n    return accelerator == Accelerator.CUDA\n\n\ndef get_device(args, rank, world_size):\n    if should_use_cuda(args):\n        assert torch.cuda.is_available()\n        num_devices = torch.cuda.device_count()\n        if args.processes > num_devices:\n            raise Exception(\n                f\"Asked for {args.processes} processes and cuda, but only \"\n                f\"{num_devices} devices found\"\n            )\n        if args.processes > 1 or world_size <= num_devices:\n            device_num = rank\n        else:\n            device_num = 0\n        torch.cuda.set_device(device_num)\n        return torch.device(\"cuda\", device_num)\n    return torch.device(\"cpu\")", "\n\ndef search(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    retrieve_per_query: float = 1200.0,\n    candidates_per_query: float = 25.0,\n) -> List[CandidatePair]:\n    aggregation = MaxScoreAggregation()\n    logger.info(\"Searching\")\n    cg = CandidateGeneration(refs, aggregation)\n    num_to_retrieve = int(retrieve_per_query * len(queries))\n    candidates = cg.query(queries, global_k=num_to_retrieve)\n    num_candidates = int(candidates_per_query * len(queries))\n    candidates = candidates[:num_candidates]\n    logger.info(\"Got %d candidates\", len(candidates))\n    return candidates", "\n\ndef localize_and_verify(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    candidates: List[CandidatePair],\n    localize_per_query: float = 5.0,\n    model_type: str = \"TN\",\n    tn_max_step=5,\n    min_length=4,\n    concurrency=16,\n    similarity_bias=0.5,\n    tn_top_k=5,\n    max_path=10,\n    min_sim=0.2,\n    max_iou=0.3,\n    discontinue=3,\n    sum_sim=8,\n    ave_sim=0.3,\n    diagonal_thres=10,\n    iou_thresh=0.9,\n    min_bins=1,\n    max_peaks=100,\n    min_peaks=10,\n) -> List[Match]:\n    num_to_localize = int(len(queries) * localize_per_query)\n    candidates = candidates[:num_to_localize]\n\n    alignment = VCSLLocalizationMatchScore(\n        queries,\n        refs,\n        model_type=model_type,\n        tn_max_step=tn_max_step,\n        min_length=min_length,\n        concurrency=concurrency,\n        similarity_bias=similarity_bias,\n        tn_top_k=tn_top_k,\n        max_path=max_path,\n        min_sim=min_sim,\n        max_iou=max_iou,\n        discontinue=discontinue,\n        sum_sim=sum_sim,\n        ave_sim=ave_sim,\n        diagonal_thres=diagonal_thres,\n        iou_thresh=iou_thresh,\n        min_bins=min_bins,\n        max_peaks=max_peaks,\n        min_peaks=min_peaks,\n    )\n\n    matches = []\n    logger.info(\"Aligning %s candidate pairs\", len(candidates))\n    BATCH_SIZE = 512\n    i = 0\n    while i < len(candidates):\n        batch = candidates[i : i + BATCH_SIZE]\n        matches.extend(alignment.localize_all(batch))\n        i += len(batch)\n        logger.info(\n            \"Aligned %d pairs of %d; %d predictions so far\",\n            i,\n            len(candidates),\n            len(matches),\n        )\n    return matches", "\n\ndef match(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    output_file: str = None,\n    return_results: bool = False,\n    model_type: str = \"TN\",\n    tn_max_step=5,\n    min_length=4,\n    concurrency=16,\n    similarity_bias=0.5,\n    tn_top_k=5,\n    max_path=10,\n    min_sim=0.2,\n    max_iou=0.3,\n    discontinue=3,\n    sum_sim=8,\n    ave_sim=0.3,\n    diagonal_thres=10,\n    iou_thresh=0.9,\n    min_bins=1,\n    max_peaks=100,\n    min_peaks=10,\n    retrieve_per_query: float = 1200.0,\n    candidates_per_query: float = 25.0,\n) -> Tuple[str, str]:\n    # Search\n    candidates = search(\n        queries,\n        refs,\n        retrieve_per_query=retrieve_per_query,\n        candidates_per_query=candidates_per_query,\n    )\n\n    # Localize and verify\n    matches = localize_and_verify(\n        queries,\n        refs,\n        candidates,\n        model_type=model_type,\n        tn_max_step=tn_max_step,\n        min_length=min_length,\n        concurrency=concurrency,\n        similarity_bias=similarity_bias,\n        tn_top_k=tn_top_k,\n        max_path=max_path,\n        min_sim=min_sim,\n        max_iou=max_iou,\n        discontinue=discontinue,\n        sum_sim=sum_sim,\n        ave_sim=ave_sim,\n        diagonal_thres=diagonal_thres,\n        iou_thresh=iou_thresh,\n        min_bins=min_bins,\n        max_peaks=max_peaks,\n        min_peaks=min_peaks,\n    )\n    # matches_file = os.path.join(output_path, \"matches.csv\")\n    Match.write_csv(matches, output_file, drop_dup=True)\n\n    if return_results:\n        return candidates, matches", "\n\ndef worker_process(\n    args,\n    rank,\n    world_size,\n    output_filename,\n    model_name,\n    ref_path,\n    noise_path,\n    pca_matrix_path,\n):\n    logger.info(f\"Starting worker {rank} of {world_size}.\")\n    device = get_device(args, rank, world_size)\n\n    logger.info(\"Loading model\")\n    if model_name == \"isc\":\n        model, transforms = create_model_in_runtime(transforms_device=device)\n    elif model_name == \"vit\":\n        model, transforms = create_model_in_runtime_2(transforms_device=device)\n    else:\n        raise ValueError(f\"Model {args.model} is not supported\")\n\n    model = model.to(device).eval()\n\n    logger.info(\"Setting up dataset\")\n    extensions = args.video_extensions.split(\",\")\n    video_reader = VideoReaderType[args.video_reader.upper()]\n    dataset = VideoDataset(\n        args.dataset_path,\n        fps=args.fps,\n        read_type=args.read_type,\n        batch_size=1,\n        extensions=extensions,\n        distributed_world_size=world_size,\n        distributed_rank=rank,\n        video_reader=video_reader,\n        ffmpeg_path=args.ffmpeg_path,\n        filter_by_asr=True,\n    )\n    loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\n    progress = tqdm.tqdm(total=dataset.num_videos())\n    queries = []\n    for vf in run_inference(loader, model, device, transforms):\n        queries.append(vf)\n        progress.update()\n\n    del loader\n    del model\n    del dataset\n\n    if noise_path:\n        stride = args.stride if args.stride is not None else int(args.fps)\n        pca_matrix = faiss.read_VectorTransform(pca_matrix_path)\n        queries = sliding_pca(queries=queries, mat=pca_matrix, stride=stride)\n\n        queries = score_normalize(\n            queries,\n            load_features(noise_path),\n            beta=1.2,\n        )\n\n    os.makedirs(output_filename.replace(\"subset_matches.csv\", \"output\"), exist_ok=True)\n    # store_features(output_filename.replace('subset_matches.csv', f'output/subset_queries_{model_name}.npz'), queries)\n    refs = load_features(ref_path)\n\n    match(\n        queries=queries,\n        refs=refs,\n        output_file=output_filename.replace(\n            \"subset_matches.csv\", f\"output/matches_{model_name}.csv\"\n        ),\n        tn_max_step=5,\n        min_length=3,\n        tn_top_k=2,\n        max_path=200,\n        min_sim=0.1,\n        max_iou=1.0,\n    )", "\n\n@torch.no_grad()\ndef run_inference(\n    dataloader, model, device, transforms=None, tta=True\n) -> Iterable[VideoFeature]:\n    name = None\n    embeddings = []\n    timestamps = []\n\n    for batch in dataloader:\n        names = batch[\"name\"]\n        assert names[0] == names[-1]  # single-video batches\n        if name is not None and name != names[0]:\n            timestamps = np.concatenate(timestamps, axis=0)\n            feature = np.concatenate(embeddings, axis=0)\n            if tta:\n                timestamps = timestamps.reshape(-1, num_views).transpose(1, 0).ravel()\n                feature = (\n                    feature.reshape(-1, num_views, feature.shape[1])\n                    .transpose(1, 0, 2)\n                    .reshape(-1, feature.shape[1])\n                )\n            yield VideoFeature(\n                video_id=name,\n                timestamps=timestamps,\n                feature=feature,\n            )\n            embeddings = []\n            timestamps = []\n        name = names[0]\n        if transforms is not None:\n            if tta:\n                tta_transforms = TTA5ViewsTransform(transforms)\n                img = tta_transforms(batch[\"input\"].to(device))\n            else:\n                img = transforms(batch[\"input\"].to(device))\n            num_views = img.shape[0] // batch[\"input\"].shape[0]\n            ts = batch[\"timestamp\"].numpy()\n            ts = np.repeat(ts, num_views)\n        else:\n            img = batch[\"input\"].to(device)\n            ts = batch[\"timestamp\"].numpy()\n            num_views = 1\n\n        emb = model(img).cpu().numpy()\n        embeddings.append(emb)\n        timestamps.append(ts)\n\n    timestamps = np.concatenate(timestamps, axis=0)\n    feature = np.concatenate(embeddings, axis=0)\n    if tta:\n        timestamps = timestamps.reshape(-1, num_views).transpose(1, 0).ravel()\n        feature = (\n            feature.reshape(-1, num_views, feature.shape[1])\n            .transpose(1, 0, 2)\n            .reshape(-1, feature.shape[1])\n        )\n    yield VideoFeature(\n        video_id=name,\n        timestamps=timestamps,\n        feature=feature,\n    )", "\n\ndef merge_feature_files(filenames: List[str], output_filename: str) -> int:\n    features = []\n    for fn in filenames:\n        features.extend(load_features(fn))\n    store_features(output_filename, features)\n    return len(features)\n", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/score_normalization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport dataclasses\nimport logging\nfrom typing import Callable, List, Tuple\n\nimport faiss  # @manual", "\nimport faiss  # @manual\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\nfrom src.vsc.index import VideoFeature\n\nlogger = logging.getLogger(\"score_normalization.py\")\nlogger.setLevel(logging.INFO)\n", "logger.setLevel(logging.INFO)\n\n\ndef transform_features(\n    features: List[VideoFeature], transform: Callable\n) -> List[VideoFeature]:\n    return [\n        dataclasses.replace(feature, feature=transform(feature.feature))\n        for feature in features\n    ]", "\n\ndef score_normalize(\n    queries: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    l2_normalize: bool = True,\n    replace_dim: bool = True,\n    beta: float = 1.0,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    \"\"\"\n    CSLS style score normalization (as used in the Image Similarity Challenge)\n    has the following form. We compute a bias term for each query:\n\n      bias(query) = - beta * sim(query, noise)\n\n    then compute score normalized similarity by incorporating this as an\n    additive term for each query:\n\n      sim_sn(query, ref) = sim(query, ref) + bias(query)\n\n    sim(query, ref) is inner product similarity (query * ref), and\n    sim(query, noise) is some function of query similarity to a noise dataset\n    (score_norm_refs here), such as the similarity to the nearest neighbor.\n\n    We encode the bias term as an extra dimension in the query descriptor,\n    and add a constant 1 dimension to reference descriptors, so that inner-\n    product similarity is the score-normalized similarity:\n\n      query' = [query bias(query)]\n      ref' = [ref 1]\n      query' * ref' = (query * ref) + (bias(query) * 1)\n          = sim(query, ref) + bias(query) = sim_sn(query, ref)\n    \"\"\"\n    if score_norm_refs is not None and replace_dim:\n        # Make space for the additional score normalization dimension.\n        # We could also use PCA dim reduction, but re-centering can be\n        # destructive.\n        logger.info(\"Replacing dimension\")\n        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n        low_var_dim = sn_features.var(axis=0).argmin()\n        queries, score_norm_refs = [\n            transform_features(\n                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n            )\n            for x in [queries, score_norm_refs]\n        ]\n    if l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, score_norm_refs]\n        ]\n    logger.info(\"Applying score normalization\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    adapted_queries = []\n    # Add the additive normalization term to the queries as an extra dimension.\n    for query in queries:\n        # KNN search is ok here (versus a threshold/radius/range search) since\n        # we're not searching the dataset we're evaluating on.\n        similarity, ids = index.search(query.feature, 1)\n        norm_term = -beta * similarity[:, :1]\n        feature = np.concatenate([query.feature, norm_term], axis=1)\n        adapted_queries.append(dataclasses.replace(query, feature=feature))\n    return adapted_queries", "\n\ndef score_normalize_with_ref(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    l2_normalize: bool = True,\n    replace_dim: bool = True,\n    beta: float = 1.0,\n    return_adapted_score_norm_refs: bool = False,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    if {f.video_id for f in refs}.intersection({f.video_id for f in score_norm_refs}):\n        raise Exception(\n            \"Normalizing on the dataset we're evaluating on is against VSC rules. \"\n            \"An independent dataset is needed.\"\n        )\n    if score_norm_refs is not None and replace_dim:\n        # Make space for the additional score normalization dimension.\n        # We could also use PCA dim reduction, but re-centering can be\n        # destructive.\n        logger.info(\"Replacing dimension\")\n        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n        low_var_dim = sn_features.var(axis=0).argmin()\n        queries, refs, score_norm_refs = [\n            transform_features(\n                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n            )\n            for x in [queries, refs, score_norm_refs]\n        ]\n    if l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, refs, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n        ]\n    logger.info(\"Applying score normalization\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    adapted_queries = []\n    # Add the additive normalization term to the queries as an extra dimension.\n    for query in queries:\n        # KNN search is ok here (versus a threshold/radius/range search) since\n        # we're not searching the dataset we're evaluating on.\n        similarity, ids = index.search(query.feature, 1)\n        norm_term = -beta * similarity[:, :1]\n        feature = np.concatenate([query.feature, norm_term], axis=1)\n        adapted_queries.append(dataclasses.replace(query, feature=feature))\n    adapted_refs = []\n    for ref in refs:\n        ones = np.ones_like(ref.feature[:, :1])\n        feature = np.concatenate([ref.feature, ones], axis=1)\n        adapted_refs.append(dataclasses.replace(ref, feature=feature))\n    output = (adapted_queries, adapted_refs)\n\n    if return_adapted_score_norm_refs:\n        adapted_score_norm_refs = []\n        for score_norm_ref in score_norm_refs:\n            ones = np.ones_like(score_norm_ref.feature[:, :1])\n            feature = np.concatenate([score_norm_ref.feature, ones], axis=1)\n            adapted_score_norm_refs.append(\n                dataclasses.replace(score_norm_ref, feature=feature)\n            )\n        output += (adapted_score_norm_refs,)\n\n    return output", "\n\ndef negative_embedding_subtraction(\n    queries: List[VideoFeature],\n    refs: List[VideoFeature],\n    score_norm_refs: List[VideoFeature],\n    pre_l2_normalize: bool = False,\n    post_l2_normalize: bool = False,\n    beta: float = 1.0,\n    k: int = 10,\n    alpha: float = 1.0,\n) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n    # impl of https://arxiv.org/abs/2112.04323\n\n    if pre_l2_normalize:\n        logger.info(\"L2 normalizing\")\n        queries, refs, score_norm_refs = [\n            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n        ]\n\n    logger.info(\"Applying negative embedding subtraction\")\n    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n    if faiss.get_num_gpus() > 0:\n        index = faiss.index_cpu_to_all_gpus(index)\n\n    negative_embeddings = np.concatenate([vf.feature for vf in score_norm_refs], axis=0)\n\n    adapted_queries = []\n    for query in queries:\n        similarity, ids = index.search(query.feature, k=k)\n        weights = similarity[..., None] ** alpha\n        topk_negative_embeddings = negative_embeddings[ids] * weights\n        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n        adapted_embedding = query.feature - subtracted_embedding\n        adapted_queries.append(dataclasses.replace(query, feature=adapted_embedding))\n\n    adapted_refs = []\n    for ref in refs:\n        similarity, ids = index.search(ref.feature, k=k)\n        weights = similarity[..., None] ** alpha\n        topk_negative_embeddings = negative_embeddings[ids] * weights\n        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n        adapted_embedding = ref.feature - subtracted_embedding\n        adapted_refs.append(dataclasses.replace(ref, feature=adapted_embedding))\n\n    if post_l2_normalize:\n        logger.info(\"L2 normalizing\")\n        adapted_queries, adapted_refs = [\n            transform_features(x, normalize) for x in [adapted_queries, adapted_refs]\n        ]\n\n    return adapted_queries, adapted_refs", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/tta.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torchvision", "import torch.nn as nn\nimport torchvision\n\n\nclass TTA30ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top = x[..., : h // 2, :]  # top\n        x_bottom = x[..., h // 2 :, :]  # bottom\n        x_left = x[..., :, : w // 2]  # left\n        x_right = x[..., :, w // 2 :]  # right\n\n        if self.base_transforms is not None:\n            x = self.base_transforms(x)\n            x_top = self.base_transforms(x_top)\n            x_bottom = self.base_transforms(x_bottom)\n            x_left = self.base_transforms(x_left)\n            x_right = self.base_transforms(x_right)\n\n        crops = [\n            x,\n            torchvision.transforms.functional.rotate(x, angle=90),\n            torchvision.transforms.functional.rotate(x, angle=180),\n            torchvision.transforms.functional.rotate(x, angle=270),\n            torchvision.transforms.functional.hflip(x),\n            torchvision.transforms.functional.vflip(x),\n            x_top,\n            torchvision.transforms.functional.rotate(x_top, angle=90),\n            torchvision.transforms.functional.rotate(x_top, angle=180),\n            torchvision.transforms.functional.rotate(x_top, angle=270),\n            torchvision.transforms.functional.hflip(x_top),\n            torchvision.transforms.functional.vflip(x_top),\n            x_bottom,\n            torchvision.transforms.functional.rotate(x_bottom, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom),\n            torchvision.transforms.functional.vflip(x_bottom),\n            x_left,\n            torchvision.transforms.functional.rotate(x_left, angle=90),\n            torchvision.transforms.functional.rotate(x_left, angle=180),\n            torchvision.transforms.functional.rotate(x_left, angle=270),\n            torchvision.transforms.functional.hflip(x_left),\n            torchvision.transforms.functional.vflip(x_left),\n            x_right,\n            torchvision.transforms.functional.rotate(x_right, angle=90),\n            torchvision.transforms.functional.rotate(x_right, angle=180),\n            torchvision.transforms.functional.rotate(x_right, angle=270),\n            torchvision.transforms.functional.hflip(x_right),\n            torchvision.transforms.functional.vflip(x_right),\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA24ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\n        if self.base_transforms is not None:\n            x_top_left = self.base_transforms(x_top_left)\n            x_top_right = self.base_transforms(x_top_right)\n            x_bottom_left = self.base_transforms(x_bottom_left)\n            x_bottom_right = self.base_transforms(x_bottom_right)\n\n        crops = [\n            x_top_left,\n            torchvision.transforms.functional.rotate(x_top_left, angle=90),\n            torchvision.transforms.functional.rotate(x_top_left, angle=180),\n            torchvision.transforms.functional.rotate(x_top_left, angle=270),\n            torchvision.transforms.functional.hflip(x_top_left),\n            torchvision.transforms.functional.vflip(x_top_left),\n            x_top_right,\n            torchvision.transforms.functional.rotate(x_top_right, angle=90),\n            torchvision.transforms.functional.rotate(x_top_right, angle=180),\n            torchvision.transforms.functional.rotate(x_top_right, angle=270),\n            torchvision.transforms.functional.hflip(x_top_right),\n            torchvision.transforms.functional.vflip(x_top_right),\n            x_bottom_left,\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom_left, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom_left),\n            torchvision.transforms.functional.vflip(x_bottom_left),\n            x_bottom_right,\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=90),\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=180),\n            torchvision.transforms.functional.rotate(x_bottom_right, angle=270),\n            torchvision.transforms.functional.hflip(x_bottom_right),\n            torchvision.transforms.functional.vflip(x_bottom_right),\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA5ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top = x[..., : h // 2, :]  # top\n        x_bottom = x[..., h // 2 :, :]  # bottom\n        x_left = x[..., :, : w // 2]  # left\n        x_right = x[..., :, w // 2 :]  # right\n\n        if self.base_transforms is not None:\n            x = self.base_transforms(x)\n            x_top = self.base_transforms(x_top)\n            x_bottom = self.base_transforms(x_bottom)\n            x_left = self.base_transforms(x_left)\n            x_right = self.base_transforms(x_right)\n\n        crops = [\n            x,\n            x_top,\n            x_bottom,\n            x_left,\n            x_right,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", "\n\nclass TTA4ViewsTransform(nn.Module):\n    def __init__(self, base_transforms=None):\n        super().__init__()\n        self.base_transforms = base_transforms\n\n    def forward(self, x) -> torch.Tensor:\n        *_, h, w = x.shape\n\n        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\n        if self.base_transforms is not None:\n            x_top_left = self.base_transforms(x_top_left)\n            x_top_right = self.base_transforms(x_top_right)\n            x_bottom_left = self.base_transforms(x_bottom_left)\n            x_bottom_right = self.base_transforms(x_bottom_right)\n\n        crops = [\n            x_top_left,\n            x_top_right,\n            x_bottom_left,\n            x_bottom_right,\n        ]\n        crops = torch.cat(crops, dim=0)\n\n        return crops", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/vta.py", "chunked_list": ["import io\nimport itertools\nimport json\nimport time\nfrom functools import partial\nfrom multiprocessing import Pool\nfrom typing import Any, Dict, List, Tuple\n\nimport networkx as nx\nimport numpy as np", "import networkx as nx\nimport numpy as np\nimport torch\nfrom loguru import logger\nfrom networkx.algorithms.dag import dag_longest_path\nfrom numba import njit, prange\nfrom tslearn.metrics import dtw_path_from_metric\n\n\ndef chamfer_sim_cpu(q: np.ndarray, r: np.ndarray):\n    sim = np.tensordot(q, r.T, axes=1)\n    chamfer_sim_1 = np.squeeze(\n        np.mean(np.max(sim, axis=1, keepdims=True), axis=2, keepdims=True)\n    )\n    chamfer_sim_2 = np.squeeze(\n        np.mean(np.max(sim, axis=2, keepdims=True), axis=1, keepdims=True)\n    )\n    return (chamfer_sim_1 + chamfer_sim_2) / 2", "\ndef chamfer_sim_cpu(q: np.ndarray, r: np.ndarray):\n    sim = np.tensordot(q, r.T, axes=1)\n    chamfer_sim_1 = np.squeeze(\n        np.mean(np.max(sim, axis=1, keepdims=True), axis=2, keepdims=True)\n    )\n    chamfer_sim_2 = np.squeeze(\n        np.mean(np.max(sim, axis=2, keepdims=True), axis=1, keepdims=True)\n    )\n    return (chamfer_sim_1 + chamfer_sim_2) / 2", "\n\ndef chamfer_sim_gpu(q: np.ndarray, r: np.ndarray):\n    sim = torch.tensordot(q, r.T, 1)\n    chamfer_sim_1 = torch.squeeze(torch.mean(torch.amax(sim, 1, True), 2, True))\n    chamfer_sim_2 = torch.squeeze(torch.mean(torch.amax(sim, 2, True), 1, True))\n    return ((chamfer_sim_1 + chamfer_sim_2) / 2).cpu().numpy()\n\n\ndef sim_norm(sim: np.ndarray, lower_bound=0, upper_bound=0.3):\n    return np.clip(sim, lower_bound, upper_bound) / (upper_bound - lower_bound)", "\ndef sim_norm(sim: np.ndarray, lower_bound=0, upper_bound=0.3):\n    return np.clip(sim, lower_bound, upper_bound) / (upper_bound - lower_bound)\n\n\ndef sim_map_cpu(\n    qid, rid, q: np.ndarray, r: np.ndarray, normalize_input=False, similarity_type=\"cos\"\n):\n    if normalize_input:\n        q = q / np.linalg.norm(q, axis=1, keepdims=True)\n        r = r / np.linalg.norm(r, axis=1, keepdims=True)\n    if similarity_type == \"cos\":\n        return qid, rid, np.dot(q, r.T)\n    elif similarity_type == \"chamfer\":\n        return qid, rid, sim_norm(chamfer_sim_cpu(q, r))\n    else:\n        raise ValueError(f\"Unknown method {similarity_type}\")", "\n\ndef sim_map_gpu(\n    qid,\n    rid,\n    q: np.ndarray,\n    r: np.ndarray,\n    normalize_input=False,\n    similarity_type=\"cos\",\n    device=0,\n):\n    with torch.cuda.device(device):\n        q = torch.from_numpy(q).cuda()\n        r = torch.from_numpy(r).cuda()\n\n    if normalize_input:\n        q = torch.nn.functional.normalize(q, dim=1, p=2)\n        r = torch.nn.functional.normalize(r, dim=1, p=2)\n\n    if similarity_type == \"cos\":\n        return qid, rid, torch.matmul(q, r.T).cpu().numpy()\n    elif similarity_type == \"chamfer\":\n        return qid, rid, sim_norm(chamfer_sim_gpu(q, r))\n    else:\n        raise ValueError(f\"Unknown method {similarity_type}\")", "\n\nclass VideoSimMapModel(object):\n    def __init__(self, concurrency=4):\n        self.concurrency = concurrency\n        self.pool = Pool(self.concurrency)\n\n    def forward(\n        self,\n        data: List[Tuple[str, str, np.array, np.array]],\n        use_cuda=True,\n        normalize_input=False,\n        similarity_type=\"cos\",\n        device=0,\n    ) -> List[Any]:\n        if use_cuda:\n            func = partial(\n                sim_map_gpu,\n                normalize_input=normalize_input,\n                similarity_type=similarity_type,\n                device=device,\n            )\n        else:\n            func = partial(\n                sim_map_cpu,\n                normalize_input=normalize_input,\n                similarity_type=similarity_type,\n            )\n\n        return self.pool.starmap(func, data)", "\n\ndef iou(bbox, gt):\n    \"\"\"\n    :param bbox: (n, 4)\n    :param gt: (m, 4)\n    :return: (n, m)\n    \"\"\"\n    if len(bbox) == 0 or len(gt) == 0:\n        return np.array(0)\n    lt = np.maximum(bbox[:, None, :2], gt[:, :2])  # left_top (x, y)\n    rb = np.minimum(bbox[:, None, 2:], gt[:, 2:])  # right_bottom (x, y)\n    wh = np.maximum(rb - lt + 1, 0)  # inter_area (w, h)\n    inter_areas = wh[:, :, 0] * wh[:, :, 1]  # shape: (n, m)\n    box_areas = (bbox[:, 2] - bbox[:, 0] + 1) * (bbox[:, 3] - bbox[:, 1] + 1)\n    gt_areas = (gt[:, 2] - gt[:, 0] + 1) * (gt[:, 3] - gt[:, 1] + 1)\n    IoU = inter_areas / (box_areas[:, None] + gt_areas - inter_areas)\n    return np.array(IoU)", "\n\ndef zero_runs(a):\n    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n    absdiff = np.abs(np.diff(iszero))\n    # Runs start and end where absdiff is 1.\n    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n    return ranges\n", "\n\ndef cut_path(path: np.ndarray, diagonal_thres):\n    # range [start, end)\n    vertical_ranges = zero_runs(np.diff(path[:, 0]))\n    vertical_ranges[:, 1] += 1\n    horizontal_ranges = zero_runs(np.diff(path[:, 1]))\n    horizontal_ranges[:, 1] += 1\n\n    vertical_ranges = vertical_ranges[\n        np.diff(vertical_ranges, axis=-1).squeeze(axis=-1) > diagonal_thres\n    ]\n    horizontal_ranges = horizontal_ranges[\n        np.diff(horizontal_ranges, axis=-1).squeeze(axis=-1) > diagonal_thres\n    ]\n    discard_ranges = np.concatenate([vertical_ranges, horizontal_ranges], axis=0)\n    discard_ranges = discard_ranges[discard_ranges[:, 0].argsort()]\n\n    endpoints = discard_ranges.ravel()\n    if len(endpoints) == 0:\n        keep_ranges = np.array([[0, len(path)]], dtype=np.int32)\n    else:\n        endpoints = (\n            endpoints[1:] if endpoints[0] == 0 else np.concatenate([[0], endpoints])\n        )\n        endpoints = (\n            endpoints[:-1]\n            if endpoints[-1] == len(path)\n            else np.concatenate([endpoints, [len(path)]])\n        )\n\n        keep_ranges = endpoints.reshape(-1, 2)\n    return keep_ranges", "\n\ndef dtw(sim_matrix: np.ndarray, discontinue=3, min_sim=0.2, min_length=5, max_iou=0.3):\n    # ATTENTION: sim_matrix represents similarity, we need a distance matrix\n    path, sim_score = dtw_path_from_metric(1 - sim_matrix, metric=\"precomputed\")\n    path = np.array(path)\n\n    # remove horizontal and vertical paths\n    keep_ranges = cut_path(path, diagonal_thres=discontinue)\n    keep_ranges = keep_ranges[\n        np.diff(keep_ranges, axis=-1).squeeze(axis=-1) > min_length\n    ]\n\n    result_list = []\n    for s, e in keep_ranges:\n        assert s < e, f\"{s} < {e}, {sim_matrix.shape}\"\n        sub_path = path[s:e]\n        mean_sim = np.mean(sim_matrix[sub_path[:, 0], sub_path[:, 1]])\n\n        if (\n            mean_sim > min_sim\n            and (sub_path[-1][0] - sub_path[0][0]) > min_length\n            and (sub_path[-1][1] - sub_path[0][1]) > min_length\n        ):\n            result_list.append(\n                [\n                    int(sub_path[0][0]),\n                    int(sub_path[0][1]),\n                    int(sub_path[-1][0]),\n                    int(sub_path[-1][1]),\n                ]\n            )\n\n    return result_list", "\n\ndef find_path(dp_mat, back_trace_mat):\n    max_i, max_j = np.unravel_index(np.argmax(dp_mat), dp_mat.shape)\n    path = [(max_i, max_j)]\n\n    while back_trace_mat[max_i, max_j] != -1:\n        if back_trace_mat[max_i, max_j] == 0:\n            max_i, max_j = max_i - 1, max_j - 1\n        elif back_trace_mat[max_i, max_j] == 1:\n            max_i, max_j = max_i - 1, max_j\n        else:\n            max_i, max_j = max_i, max_j - 1\n\n        if dp_mat[max_i, max_j] == np.NINF:\n            break\n\n        path.append((max_i, max_j))\n\n    path = np.array(path, dtype=np.int32)[::-1, :]\n    return path", "\n\n@njit()\ndef njit_dp_matrix(sim_mat: np.ndarray, discontinue=3, min_sim=0):\n    dp_mat = sim_mat.copy()\n\n    M, N = sim_mat.shape[:2]\n\n    accu_unmatch_mat = np.zeros(dp_mat.shape, dtype=np.int32)\n    back_trace_mat = -np.ones(dp_mat.shape, dtype=np.int8)\n\n    for i in prange(1, M):\n        for j in prange(1, N):\n            cand_locs = [(i - 1, j - 1), (i - 1, j), (i, j - 1)]\n            # (i-1, j-1) top-left\n            top_left = dp_mat[i - 1, j - 1]\n            # (i-1, j) top\n            top = dp_mat[i - 1, j]\n            # (i, j-1) left\n            left = dp_mat[i, j - 1]\n\n            values = np.array(\n                [\n                    top_left + sim_mat[i, j],\n                    top + 0.5 * sim_mat[i, j],\n                    left + 0.5 * sim_mat[i, j],\n                ]\n            )\n            max_ind = np.argmax(values)\n            max_value = values[max_ind]\n            prev_loc = cand_locs[max_ind]\n\n            # sim value is too small\n            unmatch = sim_mat[i, j] < min_sim\n            if unmatch:\n                accu_unmatch_mat[i, j] = accu_unmatch_mat[prev_loc] + 1\n\n            if accu_unmatch_mat[i, j] <= discontinue:\n                back_trace_mat[i, j] = max_ind\n                dp_mat[i, j] = max_value\n    return dp_mat, accu_unmatch_mat, back_trace_mat", "\n\ndef dp(\n    sim_matrix: np.ndarray,\n    discontinue=3,\n    min_sim=1,\n    ave_sim=1.3,\n    min_length=5,\n    diagonal_thres=30,\n):\n    # implemented mPDP from Pattern-Based Near-Duplicate Video Retrieval and Localization on Web-Scale Videos\n    # rescale to make cosine-similarity scores non-negative\n    sim_matrix += 1\n\n    dp_mat, accu_unmatch_mat, back_trace_mat = njit_dp_matrix(\n        sim_matrix, discontinue=discontinue, min_sim=min_sim\n    )\n\n    result_list = []\n    cnt = 100\n    while cnt > 0:\n        path = find_path(dp_mat, back_trace_mat)\n\n        if dp_mat[path[-1][0], path[-1][1]] == np.NINF:\n            break\n\n        r1, c1 = int(path[0][0]), int(path[0][1])\n        r2, c2 = int(path[-1][0]), int(path[-1][1])\n        dp_mat[r1 : r2 + 1, c1 : c2 + 1] = np.NINF\n\n        keep_ranges = cut_path(path, diagonal_thres=diagonal_thres)\n        keep_ranges = keep_ranges[\n            np.diff(keep_ranges, axis=-1).squeeze(axis=-1) > min_length\n        ]\n        for s, e in keep_ranges:\n            sub_path = path[s:e]\n            mean_sim = np.mean(sim_matrix[sub_path[:, 0], sub_path[:, 1]])\n\n            if (\n                mean_sim > ave_sim\n                and (sub_path[-1][0] - sub_path[0][0]) > min_length\n                and (sub_path[-1][1] - sub_path[0][1]) > min_length\n            ):\n                result_list.append(\n                    [\n                        int(sub_path[0][0]),\n                        int(sub_path[0][1]),\n                        int(sub_path[-1][0]),\n                        int(sub_path[-1][1]),\n                    ]\n                )\n\n        cnt -= 1\n\n    return result_list", "\n\ndef tn(\n    views: List,\n    sims: np.ndarray,\n    tn_max_step: int = 10,\n    tn_top_k: int = 5,\n    max_path: int = 10,\n    min_sim: float = 0.2,\n    min_length: int = 4,\n    max_iou: float = 0.3,\n) -> List[List[int]]:\n    \"\"\"\n    TN method for video temporal alignment.\n    Reimplemented paper:\n    {Tan H K, Ngo C W, Hong R, et al. Scalable detection of partial near-duplicate videos by visual-temporal consistency\n     [C]//Proceedings of the 17th ACM international conference on Multimedia. 2009: 145-154.}\n    Parameters\n    ----------\n    sims: input similarity map computed from a copied video pair.\n    tn_max_step: max step range in TN.\n    tn_top_k: Top k frame similarity selection in TN.\n    max_path: max loop for multiply segments detection.\n    min_sim: min average similarity score for each aligned segment.\n    min_length: min segment length.\n    max_iou: max iou for filtering overlap segments (bbox).\n\n    Returns\n    -------\n    list of temporal aligned copied segments, [query_min, ref_min, query_max, ref_max] for each segment\n\n    \"\"\"\n    q_view, r_view = views\n    q_frame = sims.shape[0] // q_view\n    r_frame = sims.shape[1] // r_view\n    infringe_box_list = []\n    infringe_box_score_list = []\n    path = 0\n    node_pair2id = {}\n    node_pair2id[(-1, -1)] = 0\n\n    node_id2pair = {}\n    node_id2pair[0] = (-1, -1)  # source\n\n    node_num = 1\n\n    DG = nx.DiGraph()\n    DG.add_node(0)\n\n    # get top-k values and indices, shape (Q_LEN, top_k)\n    top = min(tn_top_k, sims.shape[1])\n    topk_indices = np.argsort(-sims)[:, :top]\n    topk_sims = np.take_along_axis(sims, topk_indices, axis=-1)\n\n    # add nodes\n    for qf_idx in range(sims.shape[0]):\n        for k in range(top):\n            rf_idx = topk_indices[qf_idx][k]\n\n            node_id2pair[node_num] = (qf_idx, rf_idx)\n            node_pair2id[(qf_idx, rf_idx)] = node_num\n\n            DG.add_node(node_num)\n            node_num += 1\n    # create graph by adding edges\n    for q_i in range(sims.shape[0]):\n        r_i = topk_indices[q_i]\n\n        intermediate_rs = np.empty((0,), dtype=np.int32)\n        # implements Constraints C1 by limiting range end\n        qj_range = [\n            i\n            for i in range(\n                q_i + 1, min((q_i // q_frame + 1) * q_frame, q_i + tn_max_step)\n            )\n        ]\n        for q_j in qj_range:\n            r_j = topk_indices[q_j]  # shape (top_k, )\n            r_diff = r_j[:, None] - r_i  # dst - src, shape (top_k, top_k)\n\n            # Constraints C2\n            C2 = (r_diff > 0) & (r_diff < tn_max_step) & (r_j % r_frame > r_i % r_frame)\n\n            # Constraints C4\n            s_j = topk_sims[q_j]  # shape (top_k, )\n            s_j = np.repeat(\n                s_j.reshape(-1, 1), r_diff.shape[1], axis=1\n            )  # shape (top_k, top_k)\n            C4 = s_j >= min_sim\n\n            # val_rows, val_cols = np.where(C2 & C3 & C4)\n            val_rows, val_cols = np.where(C2 & C4)\n            val_sims = s_j[val_rows, val_cols]\n            # update intermediate_rs\n            valid_r_j = r_j[val_rows]\n            intermediate_rs = np.unique(np.concatenate([intermediate_rs, valid_r_j]))\n\n            edges = [\n                (\n                    node_pair2id[(q_i, r_i[c])],\n                    node_pair2id[(q_j, r_j[r])],\n                    dict(weight=s),\n                )\n                for c, r, s in zip(val_cols, val_rows, val_sims)\n            ]\n\n            DG.add_edges_from(edges)\n\n    # logger.info(\"Graph N {} E {} for sim {}x{}\", DG.number_of_nodes(), DG.number_of_edges(), sims.shape[0],\n    #             sims.shape[1])\n    # link sink node\n    for i in range(0, node_num - 1):\n        j = node_num - 1\n\n        pair_i = node_id2pair[i]\n        pair_j = node_id2pair[j]\n\n        if i == 0:\n            DG.add_edge(i, j, weight=0)\n        else:\n            if (\n                pair_j[0] % q_frame > pair_i[0] % q_frame\n                and pair_j[1] % r_frame > pair_i[1] % r_frame\n                and pair_j[0] - pair_i[0] <= tn_max_step\n                and pair_j[1] - pair_i[1] <= tn_max_step\n            ):\n                DG.add_edge(i, j, weight=0)\n\n    while True:\n        if path > max_path:\n            break\n        longest_path = dag_longest_path(DG)\n        for i in range(1, len(longest_path)):\n            DG.add_edge(longest_path[i - 1], longest_path[i], weight=0.0)\n        if 0 in longest_path:\n            longest_path.remove(0)  # remove source node\n        if node_num - 1 in longest_path:\n            longest_path.remove(node_num - 1)  # remove sink node\n        path_query = [node_id2pair[node_id][0] for node_id in longest_path]\n        path_refer = [node_id2pair[node_id][1] for node_id in longest_path]\n\n        if len(path_query) == 0:\n            break\n        score = 0.0\n        for (qf_idx, rf_idx) in zip(path_query, path_refer):\n            score += sims[qf_idx][rf_idx]\n        if score > 0:\n            query_min, query_max = min(path_query), max(path_query)\n            refer_min, refer_max = min(path_refer), max(path_refer)\n        else:\n            query_min, query_max = 0, 0\n            refer_min, refer_max = 0, 0\n        ave_length = (refer_max - refer_min + query_max - query_min) / 2\n        ious = iou(\n            np.expand_dims(\n                np.array([query_min, refer_min, query_max, refer_max]), axis=0\n            ),\n            np.array(infringe_box_list),\n        )\n        if (\n            ave_length != 0\n            and score / ave_length > min_sim\n            and min(refer_max - refer_min, query_max - query_min) > min_length\n            and ious.max() < max_iou\n        ):\n            infringe_box_list.append(\n                [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n            )\n            infringe_box_score_list.append(\n                [\n                    int(query_min),\n                    int(refer_min),\n                    int(query_max),\n                    int(refer_max),\n                    score / ave_length,\n                ]\n            )\n        path += 1\n    return infringe_box_score_list", "\n\ndef hv(sims: np.ndarray, iou_thresh=0.9, min_sim=0.2, max_peaks=100):\n    infringe_box_list = (\n        []\n    )  ## box_type = [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n\n    ## step1: remove all pairs lower than min_sim\n    sims[sims < min_sim] = 0.0\n\n    ## step2: calculate the time_bins histogram\n    query_inds, refer_inds = np.where(sims >= min_sim)\n    sigma_inds = np.unique(refer_inds - query_inds)\n    sigma_hists = dict()\n    for s_i in range(sigma_inds.shape[0]):\n        sigma = sigma_inds[s_i]\n        if sigma not in sigma_hists:\n            sigma_hists[sigma] = dict()\n            sigma_hists[sigma][\"score\"] = 0.0\n            sigma_hists[sigma][\"matches\"] = list()\n        start_idx = -sigma if sigma < 0 else 0\n        end_idx = sims.shape[1] - sigma  # if sigma>0 else sims.shape[1] - sigma\n        end_idx = min(max(end_idx, 0), sims.shape[0])\n        query_idx = range(start_idx, end_idx)\n        refer_idx = range(start_idx + sigma, end_idx + sigma)\n        sub_sims = sims[query_idx, refer_idx]\n        sigma_hists[sigma][\"score\"] = float(np.sum(sub_sims))\n        sigma_hists[sigma][\"matches\"] = [\n            [query_idx[x], refer_idx[x], sub_sims[x]] for x in range(len(query_idx))\n        ]\n\n    ## step3: refine the final matches\n    sorted_sigma_hists = sorted(\n        sigma_hists.items(), key=lambda x: x[1][\"score\"], reverse=True\n    )\n    del sigma_hists\n    sorted_sigma_hists = sorted_sigma_hists[:max_peaks]\n    \"\"\"\n    final_hists = dict()\n    for sigma, sum in sorted_sigma_hists:\n        matches = sum['matches']\n        final_hists[sigma] = dict()\n        final_hists[sigma]['score'] = 0.\n        final_hists[sigma]['matches'] = list()\n        for match in matches:\n            query_id, refer_id, score = match\n            if abs(query_id - refer_id - sigma) < min_peaks and match not in final_hists[sigma]['matches']:\n                 final_hists[sigma]['score'] += score\n                 final_hists[sigma]['matches'].append(match)\n    sorted_final_hists = sorted(final_hists.items(), key=lambda x: x[1]['score'], reverse=True)\n    \"\"\"\n    ## step4: output the final infringe_box_list\n    for sigma, sum in sorted_sigma_hists:\n        if sum[\"score\"] <= 0.0:\n            continue\n        matches = sum[\"matches\"]\n        query_ids = [x[0] for x in matches]\n        refer_ids = [x[1] for x in matches]\n        query_min = min(query_ids)\n        query_max = max(query_ids)\n        refer_min = min(refer_ids)\n        refer_max = max(refer_ids)\n        cur_box = [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n        ## === add nms\n        ious = iou(\n            np.expand_dims(cur_box, axis=0),\n            np.array(infringe_box_list, dtype=np.float32),\n        )\n        if np.any(ious > iou_thresh):\n            continue\n        infringe_box_list.append(cur_box)\n    return infringe_box_list", "\n\ndef func_wrapper_with_exception(rid, views, item, func):\n    try:\n        return rid, func(views, item)\n    except Exception as e:\n        logger.exception(\"Fail to run with rid {}\", rid)\n        raise RuntimeError(f\"Fail to run with data with {rid}\") from e\n\n\nclass BaseVtaModel(object):\n    def __init__(self, concurrency, func_to_run):\n        self.pool = Pool(concurrency)\n        self.func_to_run = func_to_run\n\n    def forward(self, data: List[Tuple[str, str, np.array, np.array]]) -> List[Any]:\n        sim_func = partial(sim_map_cpu, normalize_input=False)\n        sim_list = self.pool.map(sim_func, data)\n        sim_list = [(f\"{q}-{r}\", v) for q, r, v in sim_list]\n        return self.forward_sim(sim_list)\n\n    def forward_sim(self, data: List[Tuple[str, List, np.array]]) -> List[Any]:\n        algo = partial(func_wrapper_with_exception, func=self.func_to_run)\n        results = self.pool.starmap(algo, data)\n        return results", "\n\nclass BaseVtaModel(object):\n    def __init__(self, concurrency, func_to_run):\n        self.pool = Pool(concurrency)\n        self.func_to_run = func_to_run\n\n    def forward(self, data: List[Tuple[str, str, np.array, np.array]]) -> List[Any]:\n        sim_func = partial(sim_map_cpu, normalize_input=False)\n        sim_list = self.pool.map(sim_func, data)\n        sim_list = [(f\"{q}-{r}\", v) for q, r, v in sim_list]\n        return self.forward_sim(sim_list)\n\n    def forward_sim(self, data: List[Tuple[str, List, np.array]]) -> List[Any]:\n        algo = partial(func_wrapper_with_exception, func=self.func_to_run)\n        results = self.pool.starmap(algo, data)\n        return results", "\n\nclass DtwModel(BaseVtaModel):\n    def __init__(\n        self,\n        concurrency=4,\n        version=\"v1\",\n        discontinue=3,\n        min_sim=0.2,\n        min_length=5,\n        max_iou=0.3,\n        **kwargs,\n    ):\n        self.min_length = min_length\n        self.min_sim = min_sim\n        self.max_iou = max_iou\n        self.discontinue = discontinue\n        self.version = version\n        func = partial(\n            dtw,\n            discontinue=self.discontinue,\n            min_sim=self.min_sim,\n            min_length=self.min_length,\n            max_iou=self.max_iou,\n        )\n\n        super(DtwModel, self).__init__(concurrency=concurrency, func_to_run=func)", "\n\nclass DpVtaModel(BaseVtaModel):\n    def __init__(\n        self,\n        concurrency=4,\n        version=\"v1\",\n        discontinue=3,\n        min_sim=0.0,\n        min_length=5,\n        max_iou=0.3,\n        sum_sim=8,\n        ave_sim=0.3,\n        diagonal_thres=10,\n        **kwargs,\n    ):\n        self.min_sim = min_sim\n        self.min_length = min_length\n        self.max_iou = max_iou\n        self.sum_sim = sum_sim\n        self.ave_sim = ave_sim\n        self.discontinue = discontinue\n        self.diagonal_thres = diagonal_thres\n        self.version = version\n\n        func = partial(\n            dp,\n            discontinue=self.discontinue,\n            min_sim=self.min_sim,\n            min_length=self.min_length,\n            ave_sim=self.ave_sim,\n            diagonal_thres=self.diagonal_thres,\n        )\n\n        super(DpVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)", "\n\nclass TnVtaModel(BaseVtaModel):\n    def __init__(\n        self,\n        concurrency=4,\n        version=\"v1\",\n        tn_max_step=10,\n        tn_top_k=5,\n        max_path=10,\n        min_sim=0.2,\n        min_length=5,\n        max_iou=0.3,\n        **kwargs,\n    ):\n        self.tn_max_step = tn_max_step\n        self.tn_top_k = tn_top_k\n        self.max_path = max_path\n\n        self.min_sim = min_sim\n        self.min_length = min_length\n        self.max_iou = max_iou\n\n        self.version = version\n\n        func = partial(\n            tn,\n            tn_max_step=self.tn_max_step,\n            tn_top_k=self.tn_top_k,\n            max_path=self.max_path,\n            min_sim=self.min_sim,\n            min_length=self.min_length,\n            max_iou=self.max_iou,\n        )\n        super(TnVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)", "\n\nclass HvVtaModel(BaseVtaModel):\n    def __init__(\n        self,\n        concurrency=4,\n        version=\"v1\",\n        iou_thresh=0.9,\n        min_sim=0.0,\n        min_bins=1,\n        max_peaks=100,\n        min_peaks=10,\n        **kwargs,\n    ):\n        self.min_sim = min_sim\n        self.min_bins = min_bins\n        self.max_peaks = max_peaks\n        self.min_peaks = min_peaks\n        self.iou_thresh = iou_thresh\n        self.version = version\n\n        func = partial(\n            hv,\n            iou_thresh=self.iou_thresh,\n            min_sim=self.min_sim,\n            max_peaks=self.max_peaks,\n        )\n\n        super(HvVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)", "\n\ndef build_vta_model(method=\"DTW\", concurrency=4, **config) -> BaseVtaModel:\n    if method == \"DTW\":\n        return DtwModel(concurrency=concurrency, version=\"v1\", **config)\n    elif method == \"TN\":\n        return TnVtaModel(concurrency=concurrency, version=\"v1\", **config)\n    elif method == \"DP\":\n        return DpVtaModel(concurrency=concurrency, version=\"v1\", **config)\n    elif method == \"HV\":\n        return HvVtaModel(concurrency=concurrency, version=\"v1\", **config)\n    else:\n        raise ValueError(f\"Unknown method {method}\")", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/metrics.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport dataclasses\nimport enum\nimport itertools\nfrom collections import defaultdict", "import itertools\nfrom collections import defaultdict\nfrom math import sqrt\nfrom typing import Collection, Dict, List, NamedTuple, Optional, TextIO, Tuple, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import average_precision_score\n", "from sklearn.metrics import average_precision_score\n\n\nclass Dataset(enum.Enum):\n    QUERIES = \"Q\"\n    REFS = \"R\"\n\n\ndef format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n    if isinstance(video_id, (int, np.integer)):\n        if dataset is None:\n            raise ValueError(\n                \"Unable to convert integer video_id without a Dataset enum\"\n            )\n        return f\"{dataset.value}{video_id:06d}\"\n    assert isinstance(\n        video_id, str\n    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n    if dataset is not None:\n        assert (\n            video_id[0] == dataset.value\n        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n    return video_id", "def format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n    if isinstance(video_id, (int, np.integer)):\n        if dataset is None:\n            raise ValueError(\n                \"Unable to convert integer video_id without a Dataset enum\"\n            )\n        return f\"{dataset.value}{video_id:06d}\"\n    assert isinstance(\n        video_id, str\n    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n    if dataset is not None:\n        assert (\n            video_id[0] == dataset.value\n        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n    return video_id", "\n\n@dataclasses.dataclass\nclass CandidatePair:\n    query_id: str\n    ref_id: str\n    score: float\n\n    @classmethod\n    def to_dataframe(\n        cls,\n        candidates: Collection[\"CandidatePair\"],\n    ) -> pd.DataFrame:\n        return pd.DataFrame(\n            [\n                {\n                    \"query_id\": format_video_id(c.query_id, Dataset.QUERIES),\n                    \"ref_id\": format_video_id(c.ref_id, Dataset.REFS),\n                    \"score\": c.score,\n                }\n                for c in candidates\n            ],\n        )\n\n    @classmethod\n    def write_csv(\n        cls, candidates: Collection[\"CandidatePair\"], file: Union[str, TextIO]\n    ):\n        df = cls.to_dataframe(candidates)\n        df.to_csv(file, index=False)\n\n    @classmethod\n    def read_csv(cls, file: Union[str, TextIO]) -> List[\"CandidatePair\"]:\n        df = pd.read_csv(file)\n        pairs = []\n        for _, row in df.iterrows():\n            query_id = format_video_id(row.query_id, Dataset.QUERIES)\n            ref_id = format_video_id(row.ref_id, Dataset.REFS)\n            pairs.append(\n                CandidatePair(query_id=query_id, ref_id=ref_id, score=row.score)\n            )\n        return pairs\n\n    @classmethod\n    def from_matches(cls, matches: Collection[\"Match\"]) -> List[\"CandidatePair\"]:\n        scores = collections.defaultdict(float)\n        for match in matches:\n            key = (match.query_id, match.ref_id)\n            scores[key] = max(match.score, scores[key])\n        return [\n            CandidatePair(query_id=query_id, ref_id=ref_id, score=score)\n            for ((query_id, ref_id), score) in scores.items()\n        ]", "\n\n@dataclasses.dataclass\nclass PrecisionRecallCurve:\n    precisions: np.ndarray\n    recalls: np.ndarray\n    scores: np.ndarray\n\n    def plot(self, ax=None, **kwargs):\n        if ax is None:\n            _, ax = plt.subplots()\n            ax.set_xlabel(\"recall\")\n            ax.set_ylabel(\"precision\")\n            ax.set_xlim(0, 1.05)\n            ax.set_ylim(0, 1.05)\n        ax.plot(self.recalls, self.precisions, **kwargs)\n        return ax", "\n\n@dataclasses.dataclass\nclass AveragePrecision:\n    ap: float\n    pr_curve: PrecisionRecallCurve\n    simple_ap: Optional[float] = None\n\n\nclass Intervals:\n\n    # Non-overlapping, ordered by interval start.\n    intervals: List[Tuple[float, float]]\n\n    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n        self.intervals = intervals or []\n        self._dedup()\n\n    def add(self, interval: Tuple[float, float]):\n        \"\"\"Add an interval.\"\"\"\n        self.intervals.append(interval)\n        self._dedup()\n\n    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n        return Intervals(self.intervals + intervals.intervals)\n\n    def total_length(self) -> float:\n        length = 0.0\n        for start, end in self.intervals:\n            length += end - start\n        return length\n\n    def intersect_length(self, intervals: \"Intervals\") -> float:\n        \"\"\"Compute the total_length of the intersection of two Intervals.\n\n        This works by taking the sum of their lengths, and subtracting\n        the length of their union.\n\n        |A n B| = |A| + |B| - |A U B|\n        \"\"\"\n        union = self.union(intervals)\n        return self.total_length() + intervals.total_length() - union.total_length()\n\n    def _dedup(self):\n        if len(self.intervals) <= 1:\n            return\n        deduped = []\n        intervals = sorted(self.intervals)\n        current_start, current_end = intervals[0]\n        for start, end in intervals[1:]:\n            if start <= current_end:\n                # Overlap case\n                current_end = max(end, current_end)\n            else:\n                # Non-overlap case\n                deduped.append((current_start, current_end))\n                current_start, current_end = start, end\n        deduped.append((current_start, current_end))\n        self.intervals = deduped\n\n    def __str__(self):\n        return str(self.intervals)\n\n    __repr__ = __str__", "\nclass Intervals:\n\n    # Non-overlapping, ordered by interval start.\n    intervals: List[Tuple[float, float]]\n\n    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n        self.intervals = intervals or []\n        self._dedup()\n\n    def add(self, interval: Tuple[float, float]):\n        \"\"\"Add an interval.\"\"\"\n        self.intervals.append(interval)\n        self._dedup()\n\n    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n        return Intervals(self.intervals + intervals.intervals)\n\n    def total_length(self) -> float:\n        length = 0.0\n        for start, end in self.intervals:\n            length += end - start\n        return length\n\n    def intersect_length(self, intervals: \"Intervals\") -> float:\n        \"\"\"Compute the total_length of the intersection of two Intervals.\n\n        This works by taking the sum of their lengths, and subtracting\n        the length of their union.\n\n        |A n B| = |A| + |B| - |A U B|\n        \"\"\"\n        union = self.union(intervals)\n        return self.total_length() + intervals.total_length() - union.total_length()\n\n    def _dedup(self):\n        if len(self.intervals) <= 1:\n            return\n        deduped = []\n        intervals = sorted(self.intervals)\n        current_start, current_end = intervals[0]\n        for start, end in intervals[1:]:\n            if start <= current_end:\n                # Overlap case\n                current_end = max(end, current_end)\n            else:\n                # Non-overlap case\n                deduped.append((current_start, current_end))\n                current_start, current_end = start, end\n        deduped.append((current_start, current_end))\n        self.intervals = deduped\n\n    def __str__(self):\n        return str(self.intervals)\n\n    __repr__ = __str__", "\n\nclass Axis(enum.Enum):\n    QUERY = enum.auto()\n    REF = enum.auto()\n\n\nclass Match(NamedTuple):\n    \"\"\"A ground-truth match or predicted match.\"\"\"\n\n    query_id: str\n    ref_id: str\n    query_start: float\n    query_end: float\n    ref_start: float\n    ref_end: float\n    score: float\n\n    def pair_id(self):\n        return (self.query_id, self.ref_id)\n\n    def interval(self, axis: Axis) -> Tuple[float, float]:\n        if axis == Axis.QUERY:\n            return (self.query_start, self.query_end)\n        else:\n            return (self.ref_start, self.ref_end)\n\n    def intersection_area(self, bbox: \"Match\") -> float:\n        # Compute the intersection boarders\n        inter_q_start = max(self.query_start, bbox.query_start)\n        inter_r_start = max(self.ref_start, bbox.ref_start)\n        inter_q_end = min(self.query_end, bbox.query_end)\n        inter_r_end = min(self.ref_end, bbox.ref_end)\n\n        # Compute the area of intersection rectangle\n        return abs(\n            max((inter_q_end - inter_q_start, 0))\n            * max((inter_r_end - inter_r_start), 0)\n        )\n\n    def overlaps(self, bbox: \"Match\") -> bool:\n        return self.intersection_area(bbox) > 0.0\n\n    @classmethod\n    def write_csv(\n        cls, matches: Collection[\"Match\"], file: Union[str, TextIO], drop_dup=False\n    ):\n        df = pd.DataFrame([match._asdict() for match in matches], columns=cls._fields)\n        if drop_dup:\n            df[\"score\"] = df.groupby(\n                [\n                    \"query_id\",\n                    \"ref_id\",\n                    \"query_start\",\n                    \"query_end\",\n                    \"ref_start\",\n                    \"ref_end\",\n                ]\n            )[\"score\"].transform(\"max\")\n            df.drop_duplicates(\n                [\n                    \"query_id\",\n                    \"ref_id\",\n                    \"query_start\",\n                    \"query_end\",\n                    \"ref_start\",\n                    \"ref_end\",\n                ],\n                keep=\"first\",\n                inplace=True,\n            )\n        df = df.sort_values(by=\"score\", ascending=False)\n        df.to_csv(file, index=False)\n\n    @classmethod\n    def read_csv(\n        cls, file: Union[str, TextIO], is_gt=False, check=True\n    ) -> List[\"Match\"]:\n        df = pd.read_csv(file)\n        df[\"query_id\"] = df.query_id.map(lambda x: format_video_id(x, Dataset.QUERIES))\n        df[\"ref_id\"] = df.ref_id.map(lambda x: format_video_id(x, Dataset.REFS))\n        if is_gt:\n            df[\"score\"] = 1.0\n        if check:\n            for field in cls._fields:\n                assert not df[field].isna().any()\n        return [Match(**record) for record in df.to_dict(\"records\")]", "\n\nclass VideoPair:\n    \"\"\"A video pair item that contains information regarding the gt and pred bboxes.\n\n    Provide functionalities for the combination of new predictions with the\n    existing ones and the computation of their intersection with the gt bboxes,\n    ignoring the gt bboxes that do not overlap with any prediction.\n    \"\"\"\n\n    gts: List[Match]\n    preds: List[Match]\n\n    def __init__(\n        self,\n    ):\n        self.intersections = {axis: 0.0 for axis in Axis}\n        self.totals = {axis: 0.0 for axis in Axis}\n        self.gts = []\n        self.preds = []\n\n    def total_gt_length(self, axis: Axis) -> float:\n        return Intervals([gt.interval(axis) for gt in self.gts]).total_length()\n\n    def total_pred_length(self, axis: Axis) -> float:\n        return Intervals([pred.interval(axis) for pred in self.preds]).total_length()\n\n    def gt_overlaps(self, gt: Match) -> bool:\n        \"\"\"Checks if the provided gt bbox overlaps with at least one pred bbox.\"\"\"\n        for pred in self.preds:\n            if gt.overlaps(pred):\n                return True\n        return False\n\n    def add_gt(self, bbox: Match):\n        self.gts.append(bbox)\n\n    def add_prediction(\n        self, bbox: Match\n    ) -> Tuple[Dict[Axis, float], Dict[Axis, float]]:\n        \"\"\"Add a prediction to the corresponding list and calculates the\n        differences in the intersections with the gt and the total video\n        length covered for both query and reference axes.\n        \"\"\"\n        self.preds.append(bbox)\n\n        # A subset of GTs to consider for intersection (but not total GT length).\n        gts_to_consider = [gt for gt in self.gts if self.gt_overlaps(gt)]\n\n        intersect_deltas = {}\n        total_deltas = {}\n\n        for axis in Axis:\n            pred_ints = Intervals([pred.interval(axis) for pred in self.preds])\n            gt_ints = Intervals([gt.interval(axis) for gt in gts_to_consider])\n            # New intersection and total length on this axis\n            intersect_length = pred_ints.intersect_length(gt_ints)\n            prediction_length = pred_ints.total_length()\n            # Compute differences\n            intersect_deltas[axis] = intersect_length - self.intersections[axis]\n            total_deltas[axis] = prediction_length - self.totals[axis]\n            # Update with new values\n            self.intersections[axis] = intersect_length\n            self.totals[axis] = prediction_length\n\n        return intersect_deltas, total_deltas", "\n\ndef match_metric(\n    gts: Collection[Match],\n    predictions: Collection[Match],\n) -> AveragePrecision:\n    \"\"\"Matching track metric:\n\n    Computes the AP based on the VCSL approach for the\n    calculation of Precision and Recall.\n\n    AP = \\sum_{i=1}^N P(i) \u0394R(i)\n\n    where, P(i) = sqrt(P_q * P_r) and R(i) = sqrt(R_q * R_r)\n    calculated as in the VCSL.\n    \"\"\"  # noqa: W605\n\n    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\n    # Initialize video pairs and load their gt bboxs\n    video_pairs = defaultdict(VideoPair)\n    for gt in gts:\n        video_pairs[gt.pair_id()].add_gt(gt)\n\n    # Get the total gt length for each axis\n    gt_total_lengths = {axis: 0.0 for axis in Axis}\n    for _, v in video_pairs.items():\n        for axis in Axis:\n            gt_total_lengths[axis] += v.total_gt_length(axis)\n\n    # Loop through the predictions\n    recall = 0.0\n    metric = 0.0\n    intersections = {axis: 0.0 for axis in Axis}\n    totals = {axis: 0.0 for axis in Axis}\n    pr_recalls = []\n    pr_precisions = []\n    pr_scores = []\n\n    # Update metrics for all predictions with the same score as a group.\n    for score, prediction_group in itertools.groupby(\n        predictions, key=lambda x: x.score\n    ):\n        for prediction in prediction_group:\n            # Given new predictions, we only need the differences in the intersection with\n            # gt and total video length covered for both query and reference axes.\n            # This derives from the sum of differences for every pair id\n            intersection_deltas, total_deltas = video_pairs[\n                prediction.pair_id()\n            ].add_prediction(prediction)\n            for axis in Axis:\n                # Accumulate the differences to the corresponding values\n                intersections[axis] += intersection_deltas[axis]\n                totals[axis] += total_deltas[axis]\n\n        recalls = {}\n        precisions = {}\n        for axis in Axis:\n            recalls[axis] = intersections[axis] / gt_total_lengths[axis]\n            precisions[axis] = intersections[axis] / totals[axis]\n\n        new_recall = sqrt(recalls[Axis.QUERY] * recalls[Axis.REF])\n        precision = sqrt(precisions[Axis.QUERY] * precisions[Axis.REF])\n\n        # Compute metric\n        delta_recall = new_recall - recall\n        metric += precision * delta_recall\n        recall = new_recall\n        if delta_recall > 0:\n            pr_recalls.append(recall)\n            pr_precisions.append(precision)\n            pr_scores.append(score)\n\n    curve = PrecisionRecallCurve(\n        np.array(pr_precisions), np.array(pr_recalls), np.array(pr_scores)\n    )\n    return AveragePrecision(metric, curve)", "\n\n@dataclasses.dataclass\nclass MatchingTrackMetrics:\n    # Our main evaluation metric.\n    segment_ap: AveragePrecision\n    # This metric reflects only pairwise matching, and not localization.\n    pairwise_micro_ap: AveragePrecision\n\n\ndef evaluate_matching_track(\n    ground_truth_filename: str, predictions_filename: str\n) -> MatchingTrackMetrics:\n    \"\"\"Matching track evaluation.\n\n    Predictions are expected to be a CSV file, with a column names in the header.\n    The following columns must be present, in any order:\n        query_id: str, the ID of the query for this match\n        ref_id: str, the ID of the reference for this match\n        query_start: float, the start of the query segment in seconds\n        query_end: float, the end of the query segment in seconds\n        ref_start: float, the start of the reference segment in seconds\n        ref_end: float, the end of the reference segment in seconds\n        score: float, the score of this prediction (a higher score indicates a\n            more confident prediction)\n\n    Note that ground-truth matches are specified using the same format, but score\n    is not used.\n    \"\"\"\n    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n    predictions = Match.read_csv(predictions_filename)\n    metric = match_metric(gt, predictions)\n    # Auxiliary metric: pairwise uAP\n    gt_pairs = CandidatePair.from_matches(gt)\n    pairs = CandidatePair.from_matches(predictions)\n    pair_ap = average_precision(gt_pairs, pairs)\n    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)", "\n\ndef evaluate_matching_track(\n    ground_truth_filename: str, predictions_filename: str\n) -> MatchingTrackMetrics:\n    \"\"\"Matching track evaluation.\n\n    Predictions are expected to be a CSV file, with a column names in the header.\n    The following columns must be present, in any order:\n        query_id: str, the ID of the query for this match\n        ref_id: str, the ID of the reference for this match\n        query_start: float, the start of the query segment in seconds\n        query_end: float, the end of the query segment in seconds\n        ref_start: float, the start of the reference segment in seconds\n        ref_end: float, the end of the reference segment in seconds\n        score: float, the score of this prediction (a higher score indicates a\n            more confident prediction)\n\n    Note that ground-truth matches are specified using the same format, but score\n    is not used.\n    \"\"\"\n    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n    predictions = Match.read_csv(predictions_filename)\n    metric = match_metric(gt, predictions)\n    # Auxiliary metric: pairwise uAP\n    gt_pairs = CandidatePair.from_matches(gt)\n    pairs = CandidatePair.from_matches(predictions)\n    pair_ap = average_precision(gt_pairs, pairs)\n    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)", "\n\ndef average_precision(\n    ground_truth: Collection[CandidatePair], predictions: Collection[CandidatePair]\n) -> AveragePrecision:\n    gt_pairs = {(pair.query_id, pair.ref_id) for pair in ground_truth}\n    if len(gt_pairs) != len(ground_truth):\n        raise AssertionError(\"Duplicates detected in ground truth\")\n    predicted_pairs = {(pair.query_id, pair.ref_id) for pair in predictions}\n    if len(predicted_pairs) != len(predictions):\n        raise AssertionError(\"Duplicates detected in predictions\")\n\n    # AP calculation that aligns with DrivenData's backend implementation.\n    canonical_ap = drivendata_average_precision(\n        predicted=CandidatePair.to_dataframe(predictions),\n        ground_truth=CandidatePair.to_dataframe(ground_truth),\n    )\n\n    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n    scores = np.array([pair.score for pair in predictions])\n    correct = np.array(\n        [(pair.query_id, pair.ref_id) in gt_pairs for pair in predictions]\n    )\n    total_pairs = len(gt_pairs)\n    # precision = correct_so_far / total_pairs_so_far\n    cumulative_correct = np.cumsum(correct)\n    cumulative_predicted = np.arange(len(correct)) + 1\n    recall = cumulative_correct / total_pairs\n    precision = cumulative_correct / cumulative_predicted\n    # Simple AP computation.\n    simple_ap = np.sum(precision * correct) / total_pairs\n    # Get precision and recall where correct is true\n    indices = np.nonzero(correct)[0]\n    curve = PrecisionRecallCurve(precision[indices], recall[indices], scores[indices])\n    return AveragePrecision(ap=canonical_ap, pr_curve=curve, simple_ap=simple_ap)", "\n\ndef drivendata_average_precision(\n    predicted: pd.DataFrame,\n    ground_truth: pd.DataFrame,\n):\n    \"\"\"Canonical AP implementation used for the challenge.\"\"\"\n    SCORE_COL = \"score\"\n    QUERY_ID_COL = \"query_id\"\n    DATABASE_ID_COL = \"ref_id\"\n    actual = ground_truth[[\"query_id\", \"ref_id\"]]\n    if (\n        not np.isfinite(predicted[SCORE_COL]).all()\n        or np.isnan(predicted[SCORE_COL]).any()\n    ):\n        raise ValueError(\"Scores must be finite.\")\n\n    predicted = predicted.sort_values(SCORE_COL, ascending=False)\n\n    merged = predicted.merge(\n        right=actual.assign(actual=1.0),\n        how=\"left\",\n        on=[QUERY_ID_COL, DATABASE_ID_COL],\n    ).fillna({\"actual\": 0.0})\n\n    # We may not predict for every ground truth, so calculate unadjusted AP then adjust it\n    unadjusted_ap = (\n        average_precision_score(merged[\"actual\"].values, merged[SCORE_COL].values)\n        if merged[\"actual\"].sum()\n        else 0.0\n    )\n    # Rescale average precisions based on total ground truth positive counts\n    predicted_n_pos = int(merged[\"actual\"].sum())\n\n    # avoid rows added to validate query ids, will have blank ref_id\n    actual_n_pos = int(actual[DATABASE_ID_COL].notna().sum())\n\n    adjusted_ap = unadjusted_ap * (predicted_n_pos / actual_n_pos)\n    return adjusted_ap", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/candidates.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.neration\n\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nimport numpy as np\nfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex", "import numpy as np\nfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex\nfrom src.vsc.metrics import CandidatePair\n\n\nclass ScoreAggregation(ABC):\n    @abstractmethod\n    def aggregate(self, match: PairMatches) -> float:\n        pass\n\n    def score(self, match: PairMatches) -> CandidatePair:\n        score = self.aggregate(match)\n        return CandidatePair(query_id=match.query_id, ref_id=match.ref_id, score=score)", "\n\nclass MaxScoreAggregation(ScoreAggregation):\n    def aggregate(self, match: PairMatches) -> float:\n        return np.max([m.score for m in match.matches])\n\n\nclass CandidateGeneration:\n    def __init__(self, references: List[VideoFeature], aggregation: ScoreAggregation):\n        self.aggregation = aggregation\n        dim = references[0].dimensions()\n        self.index = VideoIndex(dim)\n        self.index.add(references)\n\n    def query(self, queries: List[VideoFeature], global_k: int) -> List[CandidatePair]:\n        matches = self.index.search(queries, global_k=global_k)\n        candidates = [self.aggregation.score(match) for match in matches]\n        candidates = sorted(candidates, key=lambda match: match.score, reverse=True)\n        return candidates", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/storage.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Optional\n\nimport numpy as np\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.metrics import Dataset, format_video_id", "from src.vsc.index import VideoFeature\nfrom src.vsc.metrics import Dataset, format_video_id\n\n\ndef store_features(f, features: List[VideoFeature], dataset: Optional[Dataset] = None):\n    video_ids = []\n    feats = []\n    timestamps = []\n    for feature in features:\n        video_id = format_video_id(feature.video_id, dataset)\n        video_ids.append(np.full(len(feature), video_id))\n        feats.append(feature.feature)\n        timestamps.append(feature.timestamps)\n    video_ids = np.concatenate(video_ids)\n    feats = np.concatenate(feats)\n    timestamps = np.concatenate(timestamps)\n    np.savez(f, video_ids=video_ids, features=feats, timestamps=timestamps)", "\n\ndef same_value_ranges(values):\n    start = 0\n    value = values[start]\n\n    for i, v in enumerate(values):\n        if v == value:\n            continue\n        yield value, start, i\n        start = i\n        value = values[start]\n\n    yield value, start, len(values)", "\n\ndef load_features(f, dataset: Optional[Dataset] = None):\n    data = np.load(f, allow_pickle=False)\n    video_ids = data[\"video_ids\"]\n    feats = data[\"features\"]\n    timestamps = data[\"timestamps\"]\n\n    ts_dims = len(timestamps.shape)\n    if timestamps.shape[0] != feats.shape[0]:\n        raise ValueError(\n            f\"Expected the same number of timestamps as features: got \"\n            f\"{timestamps.shape[0]} timestamps for {feats.shape[0]} features\"\n        )\n    if not (ts_dims == 1 or timestamps.shape[1:] == (2,)):\n        print(f\"timestamps.shape[1:]: {timestamps.shape[1:]}\")\n        print(f\"timestamps.shape[1:] == [2]: {timestamps.shape[1:] == [2]}\")\n        raise ValueError(f\"Unexpected timestamp shape. Got {timestamps.shape}\")\n\n    results = []\n    for video_id, start, end in same_value_ranges(video_ids):\n        video_id = format_video_id(video_id, dataset)\n        results.append(\n            VideoFeature(\n                video_id=video_id,\n                timestamps=timestamps[start:end],\n                feature=feats[start:end, :],\n            )\n        )\n    return results", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/index.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Iterable, List, NamedTuple, Tuple\n", "from typing import Iterable, List, NamedTuple, Tuple\n\nimport faiss  # @manual\nimport numpy as np\nfrom faiss.contrib import exhaustive_search  # @manual\n\nSearchIndices = Tuple[int, int, float]\n\n\n@dataclass\nclass VideoMetadata:\n    video_id: str\n    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\n    def __len__(self):\n        return self.timestamps.shape[0]\n\n    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n        t = self.timestamps[idx]\n        if len(self.timestamps.shape) == 1:\n            return (t, t)\n        return (t[0], t[1])", "\n@dataclass\nclass VideoMetadata:\n    video_id: str\n    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\n    def __len__(self):\n        return self.timestamps.shape[0]\n\n    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n        t = self.timestamps[idx]\n        if len(self.timestamps.shape) == 1:\n            return (t, t)\n        return (t[0], t[1])", "\n\n@dataclass\nclass VideoFeature(VideoMetadata):\n    feature: np.ndarray\n\n    def __post_init__(self):\n        assert self.feature.shape[0] == len(\n            self.timestamps\n        ), \"Mismatched timestamps / feature size\"\n\n    def metadata(self):\n        return VideoMetadata(video_id=self.video_id, timestamps=self.timestamps)\n\n    def dimensions(self):\n        return self.feature.shape[1]", "\n\nclass PairMatch(NamedTuple):\n    query_timestamps: Tuple[float, float]\n    ref_timestamps: Tuple[float, float]\n    score: float\n\n\n@dataclass\nclass PairMatches:\n    query_id: str\n    ref_id: str\n    matches: List[PairMatch]\n\n    def records(self):\n        for match in self.matches:\n            yield {\n                \"query_id\": self.query_id,\n                \"ref_id\": self.ref_id,\n                \"query_start\": match.query_timestamps[0],\n                \"query_end\": match.query_timestamps[1],\n                \"ref_start\": match.ref_timestamps[0],\n                \"ref_end\": match.ref_timestamps[1],\n                \"score\": match.score,\n            }", "@dataclass\nclass PairMatches:\n    query_id: str\n    ref_id: str\n    matches: List[PairMatch]\n\n    def records(self):\n        for match in self.matches:\n            yield {\n                \"query_id\": self.query_id,\n                \"ref_id\": self.ref_id,\n                \"query_start\": match.query_timestamps[0],\n                \"query_end\": match.query_timestamps[1],\n                \"ref_start\": match.ref_timestamps[0],\n                \"ref_end\": match.ref_timestamps[1],\n                \"score\": match.score,\n            }", "\n\nclass VideoIndex:\n    def __init__(\n        self,\n        dim: int,\n        codec_str: str = \"Flat\",\n        metric: int = faiss.METRIC_INNER_PRODUCT,\n    ):\n        self.dim = dim\n        self.index = faiss.index_factory(self.dim, codec_str, metric)\n        self.video_clip_idx = []\n        self.video_clip_to_video_ids = []\n        self.video_metadata = {}\n\n    def add(self, db: List[VideoFeature]):\n        for vf in db:\n            self.video_clip_idx.extend(list(range(vf.feature.shape[0])))\n            self.video_clip_to_video_ids.extend(\n                [vf.video_id for _ in range(vf.feature.shape[0])]\n            )\n            self.video_metadata[vf.video_id] = vf.metadata()\n            self.index.add(vf.feature)\n\n    def search(\n        self,\n        queries: List[VideoFeature],\n        global_k: int,\n    ) -> List[PairMatches]:\n        query_ids = []\n        query_indices = []\n        for q in queries:\n            query_ids.extend([q.video_id] * len(q))\n            query_indices.extend(range(len(q)))\n        query_metadatas = {q.video_id: q.metadata() for q in queries}\n        query_features = np.concatenate([q.feature for q in queries])\n        if global_k < 0:\n            # Negative values cause us to use vanilla KNN search\n            k = -global_k\n            logging.warn(\n                \"Using local k for KNN search. Warning: this is against the \"\n                \"VSC rules, since predictions for a query-ref pair are not \"\n                \"independent of other references. KNN search is provided for \"\n                \"comparison.\"\n            )\n            search_indices = self._knn_search(query_features, k)\n        else:\n            search_indices = self._global_threshold_knn_search(query_features, global_k)\n\n        pair_nns = collections.defaultdict(list)\n\n        for i, j, score in search_indices:\n            query_id = query_ids[i]\n            query_idx = query_indices[i]\n            query_metadata = query_metadatas[query_id]\n            ref_id = self.video_clip_to_video_ids[j]\n            ref_idx = self.video_clip_idx[j]\n            ref_metadata = self.video_metadata[ref_id]\n            match = PairMatch(\n                query_timestamps=query_metadata.get_timestamps(query_idx),\n                ref_timestamps=ref_metadata.get_timestamps(ref_idx),\n                score=score,\n            )\n            pair_nns[query_id, ref_id].append(match)\n\n        return [\n            PairMatches(query_id, ref_id, matches)\n            for ((query_id, ref_id), matches) in pair_nns.items()\n        ]\n\n    def _global_threshold_knn_search(\n        self, query_features: np.ndarray, global_k: int\n    ) -> Iterable[SearchIndices]:\n        use_similarity = self.index.metric_type == faiss.METRIC_INNER_PRODUCT\n        initial_radius = -1e10 if use_similarity else 1e10\n        _, limits, similarity, indices = exhaustive_search.range_search_max_results(\n            self.index,\n            exhaustive_search.exponential_query_iterator(query_features),\n            initial_radius,\n            max_results=2 * global_k,\n            min_results=global_k,\n            ngpu=-1,  # use GPU if available\n        )\n        nq = query_features.shape[0]\n        search_indices = []\n\n        for i in range(nq):\n            for j in range(limits[i], limits[i + 1]):\n                search_indices.append((i, indices[j], similarity[j]))\n\n        search_indices.sort(key=lambda x: x[2], reverse=use_similarity)\n        if len(search_indices) > global_k:\n            search_indices = search_indices[:global_k]\n        return search_indices\n\n    def _knn_search(self, query_features: np.ndarray, k) -> Iterable[SearchIndices]:\n        index = self.index\n        if faiss.get_num_gpus() > 0:\n            logging.info(\"Moving index to GPU\")\n            index = faiss.index_cpu_to_all_gpus(self.index)\n\n        logging.info(\"Performing KNN search\")\n        similarity, ids = index.search(query_features, k)\n        for i in range(ids.shape[0]):\n            for j in range(ids.shape[1]):\n                yield (i, ids[i, j], similarity[i, j])", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/localization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport abc\nfrom typing import List\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom src.vsc.index import VideoFeature\nfrom src.vsc.metrics import CandidatePair, Match\n\n\nclass Localization(abc.ABC):\n    @abc.abstractmethod\n    def localize(self, candidate: CandidatePair) -> List[Match]:\n        pass\n\n    def localize_all(self, candidates: List[CandidatePair]) -> List[Match]:\n        matches = []\n        for candidate in candidates:\n            matches.extend(self.localize(candidate))\n        return matches", "\n\nclass LocalizationWithMetadata(Localization):\n    def __init__(self, queries: List[VideoFeature], refs: List[VideoFeature]):\n        self.queries = {m.video_id: m for m in queries}\n        self.refs = {m.video_id: m for m in refs}\n\n    def similarity(self, candidate: CandidatePair):\n        a = self.queries[candidate.query_id].feature\n        b = self.refs[candidate.ref_id].feature\n        return np.matmul(a, b.T)\n\n    def count_views(self, candidate: CandidatePair):\n        a = self.queries[candidate.query_id].timestamps\n        q_views = np.count_nonzero(a == a[0])\n        b = self.refs[candidate.ref_id].timestamps\n        r_views = np.count_nonzero(b == b[0])\n        return [q_views, r_views]", "\n\nclass VCSLLocalization(LocalizationWithMetadata):\n    def __init__(self, queries, refs, model_type, similarity_bias=0.0, **kwargs):\n        super().__init__(queries, refs)\n\n        # Late import: allow OSS use without VCSL installed\n        from src.vsc.vta import build_vta_model  # @manual\n\n        self.model = build_vta_model(model_type, **kwargs)\n        self.similarity_bias = similarity_bias\n\n    def similarity(self, candidate: CandidatePair):\n        \"\"\"Add an optional similarity bias.\n\n        Some localization methods do not tolerate negative values well.\n        \"\"\"\n        return super().similarity(candidate) + self.similarity_bias\n\n    def localize_all(self, candidates: List[CandidatePair]) -> List[Match]:\n        # sims = [(f\"{c.query_id}-{c.ref_id}\", self.similarity(c)) for c in candidates]\n        sims = [\n            (f\"{c.query_id}-{c.ref_id}\", self.count_views(c), self.similarity(c))\n            for c in candidates\n        ]\n        results = self.model.forward_sim(sims)\n        assert len(results) == len(candidates)\n        matches = []\n        for (candidate, (key, _, sim), result) in zip(candidates, sims, results):\n            query: VideoFeature = self.queries[candidate.query_id]\n            ref: VideoFeature = self.refs[candidate.ref_id]\n            assert key == result[0]\n            for box in result[1]:\n                (x1, y1, x2, y2, score) = box\n                match = Match(\n                    query_id=candidate.query_id,\n                    ref_id=candidate.ref_id,\n                    query_start=query.get_timestamps(x1)[0],\n                    query_end=query.get_timestamps(x2)[1],\n                    ref_start=ref.get_timestamps(y1)[0],\n                    ref_end=ref.get_timestamps(y2)[1],\n                    score=0.0,\n                )\n                # score = self.score(candidate, match, box, sim)\n                match = match._replace(score=score)\n                matches.append(match)\n        return matches\n\n    def localize(self, candidate: CandidatePair) -> List[Match]:\n        return self.localize_all([candidate])\n\n    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n        return 1.0", "\n\nclass VCSLLocalizationMaxSim(VCSLLocalization):\n    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n        x1, y1, x2, y2, _score = box\n        return similarity[x1:x2, y1:y2].max() - self.similarity_bias\n\n\nclass VCSLLocalizationMatchScore(VCSLLocalization):\n    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n        x1, y1, x2, y2, _score = box\n        return _score", "class VCSLLocalizationMatchScore(VCSLLocalization):\n    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n        x1, y1, x2, y2, _score = box\n        return _score\n\n\nclass VCSLLocalizationCandidateScore(VCSLLocalization):\n    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n        return candidate.score\n", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/ffmpeg_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Iterable, Optional, Tuple\n", "from typing import Iterable, Optional, Tuple\n\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.io.image import ImageReadMode, read_image\n\nImageT = Image.Image\n\n\nclass FFMpegVideoReader(VideoReader):\n    def __init__(\n        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n    ):\n        self.ffmpeg_path = ffmpeg_path\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n            subprocess.check_call(\n                [\n                    self.ffmpeg_path,\n                    \"-nostdin\",\n                    \"-y\",\n                    \"-i\",\n                    self.video_path,\n                    \"-start_number\",\n                    \"0\",\n                    \"-q\",\n                    \"0\",\n                    \"-vf\",\n                    \"fps=%f\" % self.required_fps,\n                    os.path.join(dir, \"%07d.png\"),\n                ],\n                stderr=null,\n            )\n            i = 0\n            while True:\n                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n                if not os.path.exists(frame_fn):\n                    break\n                if self.output_type == \"pil\":\n                    img = default_loader(frame_fn)\n                elif self.output_type == \"tensor\":\n                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n                i += 1\n                yield ((i - 1) / self.original_fps, i / self.original_fps, img)", "\n\nclass FFMpegVideoReader(VideoReader):\n    def __init__(\n        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n    ):\n        self.ffmpeg_path = ffmpeg_path\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n            subprocess.check_call(\n                [\n                    self.ffmpeg_path,\n                    \"-nostdin\",\n                    \"-y\",\n                    \"-i\",\n                    self.video_path,\n                    \"-start_number\",\n                    \"0\",\n                    \"-q\",\n                    \"0\",\n                    \"-vf\",\n                    \"fps=%f\" % self.required_fps,\n                    os.path.join(dir, \"%07d.png\"),\n                ],\n                stderr=null,\n            )\n            i = 0\n            while True:\n                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n                if not os.path.exists(frame_fn):\n                    break\n                if self.output_type == \"pil\":\n                    img = default_loader(frame_fn)\n                elif self.output_type == \"tensor\":\n                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n                i += 1\n                yield ((i - 1) / self.original_fps, i / self.original_fps, img)", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/ffmpeg_py_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport warnings\nfrom typing import Iterable, Optional, Tuple\n\nimport ffmpeg", "\nimport ffmpeg\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\n\nImageT = Image.Image\n\n\nclass FFMpegPyVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        buffer, _ = (\n            ffmpeg.input(self.video_path)\n            .video.filter(\"fps\", self.required_fps)\n            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n\n        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n        width, height = meta[\"width\"], meta[\"height\"]\n\n        if self.output_type == \"pil\":\n            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n            for i, frame in enumerate(frames):\n                img = Image.fromarray(frame).convert(\"RGB\")\n                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\n        elif self.output_type == \"tensor\":\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", message=\"The given buffer is not writable\"\n                )\n                frames = (\n                    torch.frombuffer(buffer, dtype=torch.uint8)\n                    .reshape((-1, height, width, 3))\n                    .permute(0, 3, 1, 2)\n                )\n\n            for i, frame in enumerate(frames):\n                yield i / self.original_fps, (i + 1) / self.original_fps, frame", "\n\nclass FFMpegPyVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return None\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        buffer, _ = (\n            ffmpeg.input(self.video_path)\n            .video.filter(\"fps\", self.required_fps)\n            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n\n        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n        width, height = meta[\"width\"], meta[\"height\"]\n\n        if self.output_type == \"pil\":\n            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n            for i, frame in enumerate(frames):\n                img = Image.fromarray(frame).convert(\"RGB\")\n                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\n        elif self.output_type == \"tensor\":\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", message=\"The given buffer is not writable\"\n                )\n                frames = (\n                    torch.frombuffer(buffer, dtype=torch.uint8)\n                    .reshape((-1, height, width, 3))\n                    .permute(0, 3, 1, 2)\n                )\n\n            for i, frame in enumerate(frames):\n                yield i / self.original_fps, (i + 1) / self.original_fps, frame", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom abc import ABC, abstractmethod\nfrom typing import Iterable, Optional, Tuple, Union\n\nimport torch\nfrom PIL import Image", "import torch\nfrom PIL import Image\n\nImageT = Union[Image.Image, torch.Tensor]\n\n\nclass VideoReader(ABC):\n    def __init__(self, video_path: str, required_fps: float, output_type: str) -> None:\n        self.video_path = video_path\n        self.required_fps = required_fps\n        self.output_type = output_type\n        self.original_fps = max(1, self.fps) if self.fps else 1\n        self.video_frames = None\n\n    @property\n    @abstractmethod\n    def fps(self) -> Optional[float]:\n        pass\n\n    @abstractmethod\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        \"\"\"\n        returns a tuple of [start_time, end_time, Image]\n        \"\"\"\n        pass", ""]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/decord_video_reader.py", "chunked_list": ["\"\"\"\nCopyright 2023 LINE Corporation\n\nLINE Corporation licenses this file to you under the Apache License,\nversion 2.0 (the \"License\"); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at:\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software", "\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations\nunder the License.\n\"\"\"\nimport os\nfrom typing import Iterable, Optional, Tuple\n", "from typing import Iterable, Optional, Tuple\n\nimport decord\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom src.video_reader.video_reader import VideoReader\nfrom torchvision.datasets.folder import default_loader\n\nImageT = Image.Image", "\nImageT = Image.Image\n\n\nclass DecordVideoReader(VideoReader):\n    def __init__(self, video_path: str, required_fps: float, output_type: str):\n        super().__init__(video_path, required_fps, output_type)\n\n    @property\n    def fps(self) -> Optional[float]:\n        return self.required_fps\n\n    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n        reader = decord.VideoReader(\n            self.video_path,\n            ctx=decord.cpu(0),\n            num_threads=0,\n        )\n\n        n_frames = len(reader)\n        timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n        end_timestamps = timestamps[:, 1]\n        duration = end_timestamps[-1].tolist()\n\n        fps = min(self.required_fps, n_frames / duration)\n        count = max(1, np.round(duration * fps))\n        step_size = 1 / fps\n\n        frame_pos = (np.arange(count) + 0.5) * step_size\n\n        frame_ids = np.searchsorted(end_timestamps, frame_pos)\n        frame_ids = np.minimum(frame_ids, n_frames - 1)\n\n        frames = reader.get_batch(frame_ids).asnumpy()\n\n        for i, frame in enumerate(frames):\n            if self.output_type == \"pil\":\n                img = Image.fromarray(frame).convert(\"RGB\")\n            elif self.output_type == \"tensor\":\n                img = torch.as_tensor(frame).permute(2, 0, 1)\n            yield (\n                i / self.original_fps,\n                (i + 1) / self.original_fps,\n                img,\n            )", ""]}
{"filename": "meta-vsc-matching-runtime/runtime/validation.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nMatching Track validation script\n\"\"\"\nfrom argparse import ArgumentParser, Namespace\nfrom typing import List\n\nimport pandas as pd\n\nparser = ArgumentParser()", "\nparser = ArgumentParser()\nparser.add_argument(\n    \"--path\",\n    help=\"Path to match csv\",\n    type=str,\n    required=True,\n)\n\n\nclass DataValidationError(AssertionError):\n    pass", "\n\nclass DataValidationError(AssertionError):\n    pass\n\n\ndef validate_column_order(columns: List[str]):\n    expected_cols = [\n        \"query_id\",\n        \"ref_id\",\n        \"query_start\",\n        \"query_end\",\n        \"ref_start\",\n        \"ref_end\",\n        \"score\",\n    ]\n\n    for i, (expected, found) in enumerate(zip(expected_cols, columns)):\n        if expected != found:\n            raise DataValidationError(\n                f\"Columns in incorrect order. Expected {expected} in position {i}, but got {found}.\"\n            )", "\n\ndef validate_timestamps(df: pd.DataFrame):\n    for col in [\"query_start\", \"query_end\", \"ref_start\", \"ref_end\"]:\n        if not (df[col] >= 0).all():\n            raise DataValidationError(\n                f\"Found negative timestamps in {col}: \"\n                f\"all timestamps should be greater than or equal to zero.\"\n            )\n    if not (df[\"query_start\"] <= df[\"query_end\"]).all():\n        raise DataValidationError(\n            f\"Found query start timestamps greater than query end timestamps: \"\n            f\"all end timestamps should be greater than start timestamps.\"\n        )\n    if not (df[\"ref_start\"] <= df[\"ref_end\"]).all():\n        raise DataValidationError(\n            f\"Found query start timestamps greater than query end timestamps: \"\n            f\"all end timestamps should be greater than start timestamps.\"\n        )", "\n\ndef main(args: Namespace):\n    matches_df = pd.read_csv(args.path)\n    validate_column_order(matches_df.columns)\n    validate_timestamps(matches_df)\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)", "if __name__ == \"__main__\":\n    args = parser.parse_args()\n    main(args)\n"]}
{"filename": "meta-vsc-matching-runtime/runtime/conftest.py", "chunked_list": ["def pytest_addoption(parser):\n    parser.addoption(\"--submission-path\", action=\"store\", default=\"submission.csv\")\n"]}
{"filename": "meta-vsc-matching-runtime/runtime/tests/test_packages.py", "chunked_list": ["# adapted from pangeo https://github.com/pangeo-data/pangeo-docker-images/blob/master/tests/test_pangeo-notebook.py\nimport importlib\nimport subprocess\nimport warnings\n\nimport pytest\n\npackages = [\n    # these are problem libraries that don't always seem to import, mostly due\n    # to dependencies outside the python world", "    # these are problem libraries that don't always seem to import, mostly due\n    # to dependencies outside the python world\n    \"fastai\",\n    \"keras\",\n    \"numpy\",\n    \"pandas\",\n    \"scipy\",\n    \"sklearn\",  # scikit-learn\n    \"tensorflow\",\n    \"torch\",  # pytorch", "    \"tensorflow\",\n    \"torch\",  # pytorch\n    \"faiss\",\n]\n\n\n@pytest.mark.parametrize(\"package_name\", packages, ids=packages)\ndef test_import(package_name):\n    importlib.import_module(package_name)\n", "\n\ndef test_gpu_packages():\n    try:\n        subprocess.check_call([\"nvidia-smi\"])\n\n        import torch\n\n        assert torch.cuda.is_available()\n\n        import tensorflow as tf\n\n        assert tf.test.is_built_with_cuda()\n        assert tf.config.list_physical_devices(\"GPU\")\n\n    except FileNotFoundError:\n        warnings.warn(\n            \"Skipping GPU import tests since nvidia-smi is not present on test machine.\"\n        )", ""]}
