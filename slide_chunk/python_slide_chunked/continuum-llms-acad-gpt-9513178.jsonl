{"filename": "acad_bot.py", "chunked_list": ["\"\"\"\nadd deps\nTODO: pip install discord requests validators bs4 elasticsearch==7.0.0\n\n# TODO:\n- add timestamps for:\n    1. when bookmark webpage itself was actually created (optional)\n    2. when bookmark was last updated\n- annotated file `upload` command\n", "- annotated file `upload` command\n\nCollaboration:\n- add clear roadmap for how to add new commands based on other modalities\n- target for mid may to release to public\n\"\"\"\n\n\nfrom typing import List, Optional\n", "from typing import List, Optional\n\nimport discord\nimport validators\nfrom discord import app_commands\n\nfrom acad_bot_utils import get_url_type, process_url\nfrom acad_gpt.datastore import ElasticSearchDataStore, ElasticSearchStoreConfig, RedisDataStore, RedisDataStoreConfig\nfrom acad_gpt.environment import (\n    DISCORD_BOOKMARK_BOT_GUILD_ID,", "from acad_gpt.environment import (\n    DISCORD_BOOKMARK_BOT_GUILD_ID,\n    DISCORD_BOOKMARK_BOT_TOKEN,\n    ES_INDEX,\n    ES_PASSWORD,\n    ES_PORT,\n    ES_URL,\n    ES_USERNAME,\n    OPENAI_API_KEY,\n    REDIS_HOST,", "    OPENAI_API_KEY,\n    REDIS_HOST,\n    REDIS_PASSWORD,\n    REDIS_PORT,\n)\nfrom acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\nfrom acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient\nfrom acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\nfrom acad_gpt.memory.manager import MemoryManager\n", "from acad_gpt.memory.manager import MemoryManager\n\n# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\nembed_client = EmbeddingClient(config=embedding_config)\n\n# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\nredis_datastore_config = RedisDataStoreConfig(\n    host=REDIS_HOST,\n    port=REDIS_PORT,", "    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD,\n)\nredis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n# Instantiate an ElasticSearchDataStore object with the ElasticSearchStoreConfig object\nes_datastore_config = ElasticSearchStoreConfig(\n    host=ES_URL,\n    port=ES_PORT,", "    host=ES_URL,\n    port=ES_PORT,\n    username=ES_USERNAME,\n    password=ES_PASSWORD,\n    index_name=ES_INDEX,\n)\nes_datastore = ElasticSearchDataStore(config=es_datastore_config)\n\n# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\nmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)", "# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\nmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\nchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nclient = discord.Client(intents=intents)", "intents.members = True\nclient = discord.Client(intents=intents)\ntree = app_commands.CommandTree(client)\n\n\n@client.event\nasync def on_ready():\n    await tree.sync(guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID))\n    print(\"Logged in as {0.user}\".format(client))\n", "    print(\"Logged in as {0.user}\".format(client))\n\n\n@client.event\nasync def on_message(message):\n    if message.author == client.user:\n        return\n    print(\"{0.author}: {0.content}\".format(message))\n\n    if validators.url(message.content):\n        type, url = get_url_type(message.content)\n        document = process_url(url, type, embed_client)\n        redis_datastore.index_documents(documents=[document])\n        es_datastore.index_documents(documents=[document])\n        await message.channel.send(f\"`{type}` Bookmark saved!\")\n\n    else:\n        await message.channel.send(\"Something went wrong!\")", "\n    if validators.url(message.content):\n        type, url = get_url_type(message.content)\n        document = process_url(url, type, embed_client)\n        redis_datastore.index_documents(documents=[document])\n        es_datastore.index_documents(documents=[document])\n        await message.channel.send(f\"`{type}` Bookmark saved!\")\n\n    else:\n        await message.channel.send(\"Something went wrong!\")", "\n\n@tree.command(\n    name=\"search\", description=\"To search indexed content\", guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID)\n)\nasync def search(\n    interaction, query: str, status: Optional[str] = None, topk: Optional[int] = 5, ask_gpt: Optional[bool] = False\n):\n    if ask_gpt:\n        await interaction.response.defer()\n        chat_result = (\n            f\"`{client.user}`: {query}\\n`ChatGPT`: {chat_gpt_client.converse(query, topk=topk).chat_gpt_answer}\"\n        )\n        await interaction.followup.send(chat_result)\n    else:\n        topk_hits = es_datastore.search_documents(query, status=status, topk=topk)\n        if len(topk_hits) > 0:\n            response = f\"Here are the top {len(topk_hits)} results for your query: `{query}`\\n\"\n            response += \"\\n\".join([f\"{idx+1}. {hit['title']}\\n<{hit['url']}>\" for idx, hit in enumerate(topk_hits)])\n            await interaction.response.send_message(response)\n        else:\n            await interaction.response.send_message(f\"Sorry, no results found for your query: {query}\")", "    if ask_gpt:\n        await interaction.response.defer()\n        chat_result = (\n            f\"`{client.user}`: {query}\\n`ChatGPT`: {chat_gpt_client.converse(query, topk=topk).chat_gpt_answer}\"\n        )\n        await interaction.followup.send(chat_result)\n    else:\n        topk_hits = es_datastore.search_documents(query, status=status, topk=topk)\n        if len(topk_hits) > 0:\n            response = f\"Here are the top {len(topk_hits)} results for your query: `{query}`\\n\"\n            response += \"\\n\".join([f\"{idx+1}. {hit['title']}\\n<{hit['url']}>\" for idx, hit in enumerate(topk_hits)])\n            await interaction.response.send_message(response)\n        else:\n            await interaction.response.send_message(f\"Sorry, no results found for your query: {query}\")", "\n\n@tree.command(\n    name=\"update\", description=\"To update indexed content\", guild=discord.Object(id=DISCORD_BOOKMARK_BOT_GUILD_ID)\n)\nasync def update(interaction, url: str, status: str):\n    if validators.url(url):\n        type, url = get_url_type(url)\n        document = process_url(url, type, embed_client, status=status)\n        redis_datastore.index_documents(documents=[document])\n        document.pop(\"embedding\")\n        es_datastore.index_documents(documents=[document])\n        await interaction.response.send_message(f\"Bookmark for <{url}> is set to {status} status!\")\n    else:\n        await interaction.response.send_message(\"Something went wrong!\")", "\n\n@update.autocomplete(\"status\")\n@search.autocomplete(\"status\")\nasync def status_autocomplete(\n    interaction: discord.Interaction,\n    current: str,\n) -> List[app_commands.Choice[str]]:\n    statuses = [\"todo\", \"done\"]\n    return [app_commands.Choice(name=status, value=status) for status in statuses if current.lower() in status.lower()]", "    statuses = [\"todo\", \"done\"]\n    return [app_commands.Choice(name=status, value=status) for status in statuses if current.lower() in status.lower()]\n\n\nclient.run(DISCORD_BOOKMARK_BOT_TOKEN)\n"]}
{"filename": "rest_api.py", "chunked_list": ["# import enum\n# import os\n# import shutil\n# import uuid\n# from pathlib import Path\n# from typing import Any, Dict, List, Union\n\n# import numpy as np\n# from fastapi import FastAPI, File, UploadFile\n# from fastapi.middleware.cors import CORSMiddleware", "# from fastapi import FastAPI, File, UploadFile\n# from fastapi.middleware.cors import CORSMiddleware\n# from pydantic import BaseModel\n\n# from acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n# from acad_gpt.docstore.hf_file_system_storage import HfFSDocStore, HfFSDocStoreConfig\n# from acad_gpt.environment import (\n#     FILE_UPLOAD_PATH,\n#     HF_ENDPOINT,\n#     HF_REPO,", "#     HF_ENDPOINT,\n#     HF_REPO,\n#     HF_TOKEN,\n#     OPENAI_API_KEY,\n#     REDIS_HOST,\n#     REDIS_PASSWORD,\n#     REDIS_PORT,\n# )\n# from acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\n# from acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient", "# from acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\n# from acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient\n# from acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\n# from acad_gpt.memory.manager import MemoryManager\n# from acad_gpt.parsers import ParserConfig, PDFParser\n\n\n# class DocumentType(str, enum.Enum):\n#     Table = \"Table\"\n#     Figure = \"Figure\"", "#     Table = \"Table\"\n#     Figure = \"Figure\"\n#     Highlight = \"Highlight\"\n#     Section = \"Section\"\n\n\n# class SearchPayload(BaseModel):\n#     query: str\n#     k: int = 5\n#     document_type: DocumentType = DocumentType.Section", "#     k: int = 5\n#     document_type: DocumentType = DocumentType.Section\n\n\n# class Response(BaseModel):\n#     file_id: str\n#     message: str\n\n\n# origins = [", "\n# origins = [\n#     \"http://localhost:3000\",\n# ]\n\n# app = FastAPI()\n# app.add_middleware(\n#     CORSMiddleware,\n#     allow_origins=origins,\n#     allow_credentials=True,", "#     allow_origins=origins,\n#     allow_credentials=True,\n#     allow_methods=[\"*\"],\n#     allow_headers=[\"*\"],\n# )\n\n# # Instantiate document store for storing PDFs on cloud\n# hf_docstore_config = HfFSDocStoreConfig(repo=HF_REPO, token=HF_TOKEN, endpoint=HF_ENDPOINT)\n# hf_docstore = HfFSDocStore(config=hf_docstore_config)\n", "# hf_docstore = HfFSDocStore(config=hf_docstore_config)\n\n# # Instantiate an EmbeddingConfig object with the OpenAI API key\n# embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n# # Instantiate an EmbeddingClient object with the EmbeddingConfig object\n# embed_client = EmbeddingClient(config=embedding_config)\n\n# # Instantiate a RedisDataStoreConfig object with the Redis connection details\n# redis_datastore_config = RedisDataStoreConfig(", "# # Instantiate a RedisDataStoreConfig object with the Redis connection details\n# redis_datastore_config = RedisDataStoreConfig(\n#     host=REDIS_HOST,\n#     port=REDIS_PORT,\n#     password=REDIS_PASSWORD,\n# )\n\n# # Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n# redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n", "# redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n# # Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\n# memory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\n# # Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\n# chat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n# # Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\n# chat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n", "# chat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\n\n# @app.post(\"/file-upload\")\n# def upload_file(\n#     files: Union[UploadFile, List[UploadFile]] = File(...),\n# ):\n#     response = []\n#     for file in files:\n#         try:", "#     for file in files:\n#         try:\n#             file_name = f\"{uuid.uuid4().hex}_{file.filename}\"\n#             file_path = Path(FILE_UPLOAD_PATH) / file_name\n\n#             isExist = os.path.exists(FILE_UPLOAD_PATH)\n#             if not isExist:\n#                 # create the metadata directory because it does not exist\n#                 os.makedirs(FILE_UPLOAD_PATH)\n", "#                 os.makedirs(FILE_UPLOAD_PATH)\n\n#             with file_path.open(\"wb\") as buffer:\n#                 shutil.copyfileobj(file.file, buffer)\n#                 hf_docstore.upload_from_filename(file_path=f\"{FILE_UPLOAD_PATH}/{file_name}\", file_name=file_name)\n#                 parser = PDFParser()\n#                 parser_config = ParserConfig(file_path_or_url=str(file_path), file_type=\"PDF\", extract_figures=False)\n#                 results = parser.parse(config=parser_config)\n#                 documents = parser.pdf_to_documents(\n#                     pdf_contents=results, embed_client=embed_client, file_name=file_name.replace(\".pdf\", \"\")", "#                 documents = parser.pdf_to_documents(\n#                     pdf_contents=results, embed_client=embed_client, file_name=file_name.replace(\".pdf\", \"\")\n#                 )\n#                 redis_datastore.index_documents(documents)\n#                 response.append(\n#                     Response(\n#                         file_id=file_name,\n#                         message=f\"File `{file.filename}` has been indexed with {len(documents)} passages.\",\n#                     )\n#                 )", "#                     )\n#                 )\n#         except Exception as e:\n#             response.append(\n#                 Response(\n#                     file_id=file_name,\n#                     message=f\"Something went wrong!\\n Detail: {e}\",\n#                 )\n#             )\n#         finally:", "#             )\n#         finally:\n#             file.file.close()\n\n#     return response\n\n\n# @app.post(\"/search/\")\n# async def search(search_payload: SearchPayload) -> List[Any]:\n#     query_vector = embed_client.embed_queries(queries=[search_payload.query])[0].astype(np.float32).tobytes()", "# async def search(search_payload: SearchPayload) -> List[Any]:\n#     query_vector = embed_client.embed_queries(queries=[search_payload.query])[0].astype(np.float32).tobytes()\n#     search_result = redis_datastore.search_documents(query_vector=query_vector, topk=search_payload.k)\n#     return search_result\n\n\n# @app.post(\"/chat/\")\n# async def chat(search_payload: SearchPayload) -> Dict:\n#     chat_result = chat_gpt_client.converse(message=search_payload.query).dict()\n#     return chat_result", "#     chat_result = chat_gpt_client.converse(message=search_payload.query).dict()\n#     return chat_result\n"]}
{"filename": "acad_bot_utils.py", "chunked_list": ["import os\n\nimport requests\n\nfrom acad_gpt.environment import FILE_UPLOAD_PATH\nfrom acad_gpt.parsers import DocumentType\nfrom acad_gpt.parsers.base_parser import DocumentStatus\nfrom acad_gpt.parsers.config import ParserConfig\nfrom acad_gpt.parsers.pdf_parser import PDFParser\nfrom acad_gpt.parsers.webpage_parser import WebPageParser", "from acad_gpt.parsers.pdf_parser import PDFParser\nfrom acad_gpt.parsers.webpage_parser import WebPageParser\n\n\ndef get_url_type(url):\n    if url.split(\".\")[-1].strip() == \"pdf\":\n        return DocumentType.pdf, url\n    else:\n        return DocumentType.webpage, url\n", "\n\ndef download(url: str):\n    if not os.path.exists(FILE_UPLOAD_PATH):\n        os.makedirs(FILE_UPLOAD_PATH)  # create folder if it does not exist\n\n    filename = url.split(\"/\")[-1].replace(\" \", \"_\")  # be careful with file names\n    file_path = os.path.join(FILE_UPLOAD_PATH, filename)\n\n    r = requests.get(url, stream=True)\n    is_saved = False\n    if r.ok:\n        print(\"saving to\", os.path.abspath(file_path))\n        with open(file_path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=1024 * 8):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n                    os.fsync(f.fileno())\n            is_saved = True\n    else:  # HTTP status code 4XX/5XX\n        print(\"Download failed: status code {}\\n{}\".format(r.status_code, r.text))\n    return is_saved, file_path", "\n\ndef process_url(\n    url,\n    type,\n    embed_client,\n    status=DocumentStatus.todo,\n):\n    document = None\n    if type in [DocumentType.pdf, DocumentType.paper]:\n        is_saved, file_path = download(url)\n        if is_saved:\n            parser = PDFParser()\n            parser_config = ParserConfig(file_path=file_path, file_url=url)\n            results = parser.parse(config=parser_config)\n            document = parser.to_documents(\n                pdf_contents=results,\n                embed_client=embed_client,\n                type=type,\n                status=status,\n            )\n    else:\n        parser = WebPageParser()\n        parser_config = ParserConfig(file_path=url, file_url=url)\n        results = parser.parse(config=parser_config)\n        document = parser.to_documents(\n            web_contents=results,\n            embed_client=embed_client,\n            type=type,\n            status=status,\n        )\n    return document", ""]}
{"filename": "tests/test_memory_manager.py", "chunked_list": ["from acad_gpt.datastore.config import RedisDataStoreConfig\nfrom acad_gpt.datastore.redis import RedisDataStore\nfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\nfrom acad_gpt.memory.manager import MemoryManager\nfrom acad_gpt.memory.memory import Memory\n\n\nclass TestMemoryManager:\n    def setup(self):\n        # create a redis datastore\n        redis_datastore_config = RedisDataStoreConfig(\n            host=REDIS_HOST,\n            port=REDIS_PORT,\n            password=REDIS_PASSWORD,\n        )\n        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\n        # create an openai embedding client\n        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n        self.embedding_client = EmbeddingClient(embedding_client_config)\n\n    def test_conversation_insertion_and_deletion(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # assert that the memory manager is initially empty\n        assert len(memory_manager.conversations) == 0\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # remove the conversation from the memory manager\n        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager is empty\n        assert len(memory_manager.conversations) == 0\n\n    def test_adding_messages_to_conversation(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # add a message to the conversation\n        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\n        # get messages for that conversation\n        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\n        # assert that the message was added\n        assert len(messages) == 1\n\n        # assert that the message is correct\n        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n        assert messages[0].conversation_id == \"1\"", "\nclass TestMemoryManager:\n    def setup(self):\n        # create a redis datastore\n        redis_datastore_config = RedisDataStoreConfig(\n            host=REDIS_HOST,\n            port=REDIS_PORT,\n            password=REDIS_PASSWORD,\n        )\n        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\n        # create an openai embedding client\n        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n        self.embedding_client = EmbeddingClient(embedding_client_config)\n\n    def test_conversation_insertion_and_deletion(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # assert that the memory manager is initially empty\n        assert len(memory_manager.conversations) == 0\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # remove the conversation from the memory manager\n        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager is empty\n        assert len(memory_manager.conversations) == 0\n\n    def test_adding_messages_to_conversation(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # add a message to the conversation\n        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\n        # get messages for that conversation\n        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\n        # assert that the message was added\n        assert len(messages) == 1\n\n        # assert that the message is correct\n        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n        assert messages[0].conversation_id == \"1\"", ""]}
{"filename": "tests/test_redis_datastore.py", "chunked_list": ["import numpy as np\n\nfrom acad_gpt.datastore.redis import RedisDataStore\nfrom acad_gpt.environment import OPENAI_API_KEY\nfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nSAMPLE_QUERIES = [\"Where is Berlin?\"]\nSAMPLE_DOCUMENTS = [\n    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},", "SAMPLE_DOCUMENTS = [\n    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},\n    {\"text\": \"Vienna is in Austria.\", \"conversation_id\": \"1\"},\n    {\"text\": \"Salzburg is in Austria.\", \"conversation_id\": \"2\"},\n]\n\n\ndef test_redis_datastore(redis_datastore: RedisDataStore):\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    openai_embedding_client = EmbeddingClient(config=embedding_config)\n    assert (\n        redis_datastore.redis_connection.ping()\n    ), \"Redis connection failed,\\\n          double check your connection parameters\"\n\n    document_embeddings: np.ndarray = openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS)\n    for idx, embedding in enumerate(document_embeddings):\n        SAMPLE_DOCUMENTS[idx][\"embedding\"] = embedding.astype(np.float32).tobytes()\n    redis_datastore.index_documents(documents=SAMPLE_DOCUMENTS)\n\n    query_embeddings: np.ndarray = openai_embedding_client.embed_queries(SAMPLE_QUERIES)\n    query_vector = query_embeddings[0].astype(np.float32).tobytes()\n    search_results = redis_datastore.search_documents(query_vector=query_vector, conversation_id=\"1\", topk=1)\n    assert len(search_results), \"No documents returned, expected 1 document.\"\n\n    assert search_results[0].text == \"Berlin is located in Germany.\", \"Incorrect document returned as search result.\"\n\n    redis_datastore.delete_documents(conversation_id=\"1\")\n    assert redis_datastore.get_all_conversation_ids() == [\n        \"2\"\n    ], \"Document deletion failed, inconsistent documents in redis index\"", ""]}
{"filename": "tests/test_docstore.py", "chunked_list": ["import pytest\n\nfrom acad_gpt.docstore.base import DocStore\nfrom acad_gpt.docstore.in_memory_storage import InMemoryStorage, InMemoryStorageConfig\n\n\nclass TestDocStore:\n    @pytest.fixture\n    def storage(self) -> DocStore:\n        config = InMemoryStorageConfig()\n        return InMemoryStorage(config)\n\n    def test_upload_from_filename_and_exists(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        assert not storage.exists(\"test.txt\")\n\n        storage.upload_from_filename(str(file_path), \"test.txt\")\n\n        assert storage.exists(\"test.txt\")\n\n    def test_upload_from_file_and_exists(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        assert not storage.exists(\"test.txt\")\n\n        with open(str(file_path), \"rb\") as f:\n            storage.upload_from_file(f, \"test.txt\")\n\n        assert storage.exists(\"test.txt\")\n\n    def test_download_to_filename(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        storage.upload_from_filename(str(file_path), \"test.txt\")\n        other_file_path = tmp_path / \"other_test.txt\"\n        assert not other_file_path.exists(), \"other_test.txt should not exist\"\n        storage.download_to_filename(\"test.txt\", str(other_file_path))\n\n        assert other_file_path.read_text() == \"test\"\n\n    def test_download_to_file(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        storage.upload_from_filename(str(file_path), \"test.txt\")\n        other_file_path = tmp_path / \"other_test.txt\"\n        assert not other_file_path.exists(), \"other_test.txt should not exist\"\n        with open(str(other_file_path), \"wb\") as f:\n            storage.download_to_file(\"test.txt\", f)\n\n        assert other_file_path.read_text() == \"test\"\n\n    def test_list(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        storage.upload_from_filename(str(file_path), \"test.txt\")\n        storage.upload_from_filename(str(file_path), \"test2.txt\")\n        assert storage.list() == [\"test.txt\", \"test2.txt\"]\n\n    def test_delete(self, tmp_path, storage: DocStore):\n        file_path = tmp_path / \"test.txt\"\n        file_path.write_text(\"test\")\n\n        storage.upload_from_filename(str(file_path), \"test.txt\")\n        assert storage.exists(\"test.txt\")\n        storage.delete(\"test.txt\")\n        assert not storage.exists(\"test.txt\")", ""]}
{"filename": "tests/test_llm_embedding_client.py", "chunked_list": ["from acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nSAMPLE_QUERIES = [\"Where is Berlin?\"]\nSAMPLE_DOCUMENTS = [{\"text\": \"Berlin is located in Germany.\"}]\n\nEXPECTED_EMBEDDING_DIMENSIONS = (1, 1024)\n\n\ndef test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n    assert (\n        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated query embedding is of inconsistent dimension\"\n\n    assert (\n        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated document(s) embedding is of inconsistent dimension\"", "def test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n    assert (\n        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated query embedding is of inconsistent dimension\"\n\n    assert (\n        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated document(s) embedding is of inconsistent dimension\"\n", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\n\nfrom acad_gpt.datastore.config import RedisDataStoreConfig\nfrom acad_gpt.datastore.redis import RedisDataStore\nfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\n\n@pytest.fixture(scope=\"session\")\ndef openai_embedding_client():\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    return EmbeddingClient(config=embedding_config)", "\n@pytest.fixture(scope=\"session\")\ndef openai_embedding_client():\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    return EmbeddingClient(config=embedding_config)\n\n\n@pytest.fixture(scope=\"session\")\ndef redis_datastore():\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n    return redis_datastore", "def redis_datastore():\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n    return redis_datastore\n", ""]}
{"filename": "examples/simple_usage.py", "chunked_list": ["#!/bin/env python3\n\"\"\"\nThis script describes a simple usage of the library.\nYou can see a breakdown of the individual steps in the README.md file.\n\"\"\"\nfrom acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n\n## set the following ENVIRONMENT Variables before running this script\n# Import necessary modules\nfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT", "# Import necessary modules\nfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom acad_gpt.llm_client import ChatGPTClient, ChatGPTConfig, EmbeddingClient, EmbeddingConfig\nfrom acad_gpt.memory import MemoryManager\n\n# Instantiate an EmbeddingConfig object with the OpenAI API key\nembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembed_client = EmbeddingClient(config=embedding_config)", "# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembed_client = EmbeddingClient(config=embedding_config)\n\n# Instantiate a RedisDataStoreConfig object with the Redis connection details\nredis_datastore_config = RedisDataStoreConfig(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD,\n)\n", ")\n\n# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\nredis_datastore = RedisDataStore(config=redis_datastore_config)\n\n# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\nmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\n# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)", "# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\n# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\nchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\n# Initialize conversation_id to None\nconversation_id = None\n\n# Start the chatbot loop", "\n# Start the chatbot loop\nwhile True:\n    # Prompt the user for input\n    user_message = input(\"\\n Please enter your message: \")\n\n    # Use the ChatGPTClient object to generate a response\n    response = chat_gpt_client.converse(message=user_message, conversation_id=conversation_id)\n\n    # Update the conversation_id with the conversation_id from the response", "\n    # Update the conversation_id with the conversation_id from the response\n    conversation_id = response.conversation_id\n\n    # Print the response generated by the chatbot\n    print(response.chat_gpt_answer)\n"]}
{"filename": "examples/paper_highlights/paper_highlights_bullets.py", "chunked_list": ["import numpy as np\n\nfrom acad_gpt.datastore import RedisDataStore, RedisDataStoreConfig\n\n## set the following ENVIRONMENT Variables before running this script\n# Import necessary modules\nfrom acad_gpt.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom acad_gpt.llm_client import EmbeddingClient, EmbeddingConfig\nfrom acad_gpt.parsers import ParserConfig, PDFParser\n\nif __name__ == \"__main__\":\n    # Instantiate an EmbeddingConfig object with the OpenAI API key\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n    # Instantiate an EmbeddingClient object with the EmbeddingConfig object\n    embed_client = EmbeddingClient(config=embedding_config)\n\n    # Instantiate a RedisDataStoreConfig object with the Redis connection details\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n\n    # Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n    redis_datastore = RedisDataStore(config=redis_datastore_config)\n\n    parser = PDFParser()\n    parser_config = ParserConfig(\n        file_path_or_url=\"examples/paper_highlights/pdf/paper.pdf\", file_type=\"PDF\", extract_figures=True\n    )\n    results = parser.parse(config=parser_config)\n    documents = parser.pdf_to_documents(pdf_contents=results, embed_client=embed_client, file_name=\"paper.pdf\")\n    redis_datastore.index_documents(documents)\n\n    while True:\n        query = input(\"Enter your query: \")\n        query_vector = embed_client.embed_queries(queries=[query])[0].astype(np.float32).tobytes()\n        print(redis_datastore.search_documents(query_vector=query_vector))", "from acad_gpt.parsers import ParserConfig, PDFParser\n\nif __name__ == \"__main__\":\n    # Instantiate an EmbeddingConfig object with the OpenAI API key\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n    # Instantiate an EmbeddingClient object with the EmbeddingConfig object\n    embed_client = EmbeddingClient(config=embedding_config)\n\n    # Instantiate a RedisDataStoreConfig object with the Redis connection details\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n\n    # Instantiate a RedisDataStore object with the RedisDataStoreConfig object\n    redis_datastore = RedisDataStore(config=redis_datastore_config)\n\n    parser = PDFParser()\n    parser_config = ParserConfig(\n        file_path_or_url=\"examples/paper_highlights/pdf/paper.pdf\", file_type=\"PDF\", extract_figures=True\n    )\n    results = parser.parse(config=parser_config)\n    documents = parser.pdf_to_documents(pdf_contents=results, embed_client=embed_client, file_name=\"paper.pdf\")\n    redis_datastore.index_documents(documents)\n\n    while True:\n        query = input(\"Enter your query: \")\n        query_vector = embed_client.embed_queries(queries=[query])[0].astype(np.float32).tobytes()\n        print(redis_datastore.search_documents(query_vector=query_vector))", ""]}
{"filename": "acad_gpt/environment.py", "chunked_list": ["import os\n\nimport dotenv\n\n# Load environment variables from .env file\n_TESTING = os.getenv(\"CHATGPT_MEMORY_TESTING\", False)\nif _TESTING:\n    # for testing we use the .env.example file instead\n    dotenv.load_dotenv(dotenv.find_dotenv(\".env.example\"))\nelse:\n    dotenv.load_dotenv()", "\n# Any remote API (OpenAI, Cohere etc.)\nOPENAI_TIMEOUT = float(os.getenv(\"REMOTE_API_TIMEOUT_SEC\", 30))\nOPENAI_BACKOFF = float(os.getenv(\"REMOTE_API_BACKOFF_SEC\", 10))\nOPENAI_MAX_RETRIES = int(os.getenv(\"REMOTE_API_MAX_RETRIES\", 5))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Cloud data store (Redis, Pinecone etc.)\nREDIS_HOST = os.getenv(\"REDIS_HOST\")\nREDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))", "REDIS_HOST = os.getenv(\"REDIS_HOST\")\nREDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\nREDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n\n# configure discord bot\nDISCORD_BOOKMARK_BOT_TOKEN = os.getenv(\"DISCORD_BOOKMARK_TOKEN\")\nDISCORD_BOOKMARK_BOT_GUILD_ID = int(os.getenv(\"DISCORD_BOOKMARK_BOT_GUILD_ID\"))\n\n# configure elastic search\nES_URL = os.getenv(\"ES_URL\")", "# configure elastic search\nES_URL = os.getenv(\"ES_URL\")\nES_USERNAME = os.getenv(\"ES_USERNAME\")\nES_PASSWORD = os.getenv(\"ES_PASSWORD\")\nES_INDEX = os.getenv(\"ES_INDEX\", \"discord-bookmark\")\nES_PORT = int(os.getenv(\"ES_PORT\", 443))\n\n\n# HF Token\n# HF_TOKEN = os.getenv(\"HF_TOKEN\")", "# HF Token\n# HF_TOKEN = os.getenv(\"HF_TOKEN\")\n# assert (\n#     os.getenv(\"HF_REPO\", None) is not None\n# ), \"\"\"\n# Environment variable `HF_REPO` should be set prior to running acad-gpt.\n# You'd need to create a new model on the huggingface hub using the url:\n# `https://huggingface.co/new`\n\n# then set `HF_REPO` variables value to: HF_REPO={hf_username}/{hf_model_name}", "\n# then set `HF_REPO` variables value to: HF_REPO={hf_username}/{hf_model_name}\n# \"\"\"\n# HF_REPO = os.getenv(\"HF_REPO\")\n# HF_ENDPOINT = os.getenv(\"HF_ENDPOINT\", \"https://huggingface.co\")\n\n# API Config\nDEFAULT_PATH = str(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"examples/paper_highlights/pdf\")))\nFILE_UPLOAD_PATH = os.getenv(\"FILE_UPLOAD_PATH\", DEFAULT_PATH)\n", "FILE_UPLOAD_PATH = os.getenv(\"FILE_UPLOAD_PATH\", DEFAULT_PATH)\n"]}
{"filename": "acad_gpt/errors.py", "chunked_list": ["\"\"\"Custom Errors for ChatGptMemory\"\"\"\n\nfrom typing import Optional\n\n\nclass ChatGPTMemoryError(Exception):\n    \"\"\"\n    Any error generated by ChatGptMemory.\n\n    This error wraps its source transparently in such a way that its attributes\n    can be accessed directly: for example, if the original error has a `message`\n    attribute.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: Optional[str] = None,\n    ):\n        super().__init__()\n        if message:\n            self.message = message\n\n    def __getattr__(self, attr):\n        # If self.__cause__ is None, it will raise the expected AttributeError\n        getattr(self.__cause__, attr)\n\n    def __repr__(self):\n        return str(self)", "\n\nclass OpenAIError(ChatGPTMemoryError):\n    \"\"\"Exception for issues that occur in the OpenAI APIs\"\"\"\n\n    def __init__(\n        self,\n        message: Optional[str] = None,\n        status_code: Optional[int] = None,\n    ):\n        super().__init__(message=message)\n        self.status_code = status_code", "\n\nclass OpenAIRateLimitError(OpenAIError):\n    \"\"\"\n    Rate limit error for OpenAI API (status code 429), See below:\n    https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors\n    https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n    \"\"\"\n\n    def __init__(self, message: Optional[str] = None):\n        super().__init__(message=message, status_code=429)\n\n    def __repr__(self):\n        return f\"message= {self.message}, status_code={self.status_code}\"", ""]}
{"filename": "acad_gpt/__init__.py", "chunked_list": [""]}
{"filename": "acad_gpt/constants.py", "chunked_list": ["# LLM Config related\n\"\"\"\nif OpenAI embedding model type is \"*-001\" the set max sequence length to `2046`,\notherwise for type \"*-002\" set `8191`\n\"\"\"\nMAX_ALLOWED_SEQ_LEN_001 = 2046\nMAX_ALLOWED_SEQ_LEN_002 = 8191\n"]}
{"filename": "acad_gpt/utils/reflection.py", "chunked_list": ["import inspect\nimport logging\nimport time\nfrom random import random\nfrom typing import Any, Callable, Dict, Tuple\n\nfrom acad_gpt.errors import OpenAIRateLimitError\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\ndef args_to_kwargs(args: Tuple, func: Callable) -> Dict[str, Any]:\n    sig = inspect.signature(func)\n    arg_names = list(sig.parameters.keys())\n    # skip self and cls args for instance and class methods\n    if any(arg_names) and arg_names[0] in [\"self\", \"cls\"]:\n        arg_names = arg_names[1 : 1 + len(args)]\n    args_as_kwargs = {arg_name: arg for arg, arg_name in zip(args, arg_names)}\n    return args_as_kwargs", "\n\ndef retry_with_exponential_backoff(\n    backoff_in_seconds: float = 1,\n    max_retries: int = 10,\n    errors: tuple = (OpenAIRateLimitError,),\n):\n    \"\"\"\n    Decorator to retry a function with exponential backoff.\n    :param backoff_in_seconds: The initial backoff in seconds.\n    :param max_retries: The maximum number of retries.\n    :param errors: The errors to catch retry on.\n    \"\"\"\n\n    def decorator(function):\n        def wrapper(*args, **kwargs):\n            # Initialize variables\n            num_retries = 0\n\n            # Loop until a successful response or max_retries is hit or an\n            # exception is raised\n            while True:\n                try:\n                    return function(*args, **kwargs)\n\n                # Retry on specified errors\n                except errors as e:\n                    # Check if max retries has been reached\n                    if num_retries > max_retries:\n                        raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n\n                    # Increment the delay\n                    sleep_time = backoff_in_seconds * 2**num_retries + random()\n\n                    # Sleep for the delay\n                    logger.warning(\n                        \"%s - %s, retry %s in %s seconds...\",\n                        e.__class__.__name__,\n                        e,\n                        function.__name__,\n                        \"{0:.2f}\".format(sleep_time),\n                    )\n                    time.sleep(sleep_time)\n\n                    # Increment retries\n                    num_retries += 1\n\n        return wrapper\n\n    return decorator", ""]}
{"filename": "acad_gpt/utils/pdf_column_classifier.py", "chunked_list": ["\"\"\"\nThis module contains a function for classifying a PDF as either single-column or two-column.\n\nThe classification is based on simple heuristics, and is not guaranteed to be accurate.\n\nIt is done as follows:\n1. Convert the PDF to a list of images.\n2. For each image, calculate the percentage of whitish pixels in the middle of the image.\n3. If the percentage of whitish pixels in the middle of the image is greater than `VOTE_THRESHOLD`,\n    then the image is two-column.", "3. If the percentage of whitish pixels in the middle of the image is greater than `VOTE_THRESHOLD`,\n    then the image is two-column.\n4. If the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n\"\"\"\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport numpy as np\nimport pdf2image\nfrom PIL import Image, ImageFilter", "import pdf2image\nfrom PIL import Image, ImageFilter\n\n__all__ = [\"classify_pdf_from_path\", \"classify_pdf_from_bytes\"]\n\n\nDEFAULT_DPI = 50\nDEFAULT_MIDDLE_COLUMN_WIDTH = 2\n\nEROSION_KERNEL_SIZE = 5", "\nEROSION_KERNEL_SIZE = 5\nVOTE_THRESHOLD = 0.7\n\nPDF_CONVERSION_OPTIONS = dict(\n    grayscale=True,\n    hide_annotations=True,\n    paths_only=True,\n)\n", ")\n\n\ndef classify_pdf_from_path(pdf_path: Union[str, Path], dpi: int = DEFAULT_DPI) -> bool:\n    \"\"\"\n    Classify a PDF as either single-column or two-column.\n\n    Args:\n        pdf_path: The PDF to classify.\n        dpi: The DPI to use when converting the PDF to images. Defaults to DEFAULT_DPI (50).\n\n    Returns:\n        True if the PDF is two-column, False otherwise.\n    \"\"\"\n    # first, convert the PDF to a list of images.\n    imgs = pdf2image.convert_from_path(pdf_path, dpi=dpi, **PDF_CONVERSION_OPTIONS)\n    return _classify_imgs(imgs)", "\n\ndef classify_pdf_from_bytes(pdf_bytes: bytes, dpi: int = DEFAULT_DPI) -> bool:\n    \"\"\"\n    Classify a PDF as either single-column or two-column.\n\n    Args:\n        pdf_bytes: The PDF as bytes to classify.\n        dpi: The DPI to use when converting the PDF to images. Defaults to DEFAULT_DPI (50).\n\n    Returns:\n        True if the PDF is two-column, False otherwise.\n    \"\"\"\n    # first, convert the PDF to a list of images.\n    imgs = pdf2image.convert_from_bytes(pdf_bytes, dpi=dpi, **PDF_CONVERSION_OPTIONS)\n    return _classify_imgs(imgs)", "\n\ndef _classify_imgs(imgs: List[Image]) -> bool:\n    \"\"\"\n    Classify a list of images as either single-column or two-column.\n\n    Args:\n        imgs: The images to classify.\n\n    Returns:\n        True if the images are two-column, False otherwise.\n    \"\"\"\n    # classify each image in the PDF as either single-column or two-column.\n    votes = []\n    for img in imgs:\n        # TODO: should we consider image pdfs? (we're not considering doing OCR anyways)\n        # apply erosion to make the text more prominent\n        if EROSION_KERNEL_SIZE > 1:\n            img = img.filter(ImageFilter.MinFilter(EROSION_KERNEL_SIZE))\n\n        # calculate the percentage of whitish pixels in the middle of the image\n        percentage_whitish_in_middle = _calc_percentage_whitish_in_middle(img)\n\n        # if the percentage of whitish pixels in the middle of the\n        # image is greater than VOTE_THRESHOLD, then the image is two-column.\n        vote = percentage_whitish_in_middle > VOTE_THRESHOLD\n\n        # add the vote to list\n        votes.append(vote)\n\n    # if the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n    return sum(votes) > len(votes) // 2", "\n\ndef _calc_percentage_whitish_in_middle(img: Image, middle_column_width: int = DEFAULT_MIDDLE_COLUMN_WIDTH) -> float:\n    \"\"\"\n    Calculate the percentage of whitish pixels in the middle of the image.\n\n    Args:\n        img: The image to calculate the percentage of whitish pixels in the middle of.\n        middle_column_width: The width of the middle column to calculate the percentage of whitish pixels in.\n            Defaults to DEFAULT_MIDDLE_COLUMN_WIDTH (2).\n\n    Returns:\n        The percentage of whitish pixels in the middle of the image.\n    \"\"\"\n    img = img.convert(\"L\")\n    middle_column = img.crop(\n        (img.width // 2 - middle_column_width // 2, 0, img.width // 2 + middle_column_width // 2, img.height)\n    )\n    middle_column = middle_column.point(lambda x: 0 if x < 150 else 255, mode=\"1\")\n    return np.array(middle_column).mean()", ""]}
{"filename": "acad_gpt/utils/openai_utils.py", "chunked_list": ["\"\"\"Utils for using OpenAI API\"\"\"\nimport json\nimport logging\nfrom typing import Any, Dict, Tuple, Union\n\nimport requests\nfrom transformers import GPT2TokenizerFast\n\nfrom acad_gpt.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\nfrom acad_gpt.errors import OpenAIError, OpenAIRateLimitError", "from acad_gpt.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\nfrom acad_gpt.errors import OpenAIError, OpenAIRateLimitError\nfrom acad_gpt.utils.reflection import retry_with_exponential_backoff\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_openai_tokenizer(tokenizer_name: str, use_tiktoken: bool) -> Any:\n    \"\"\"\n    Load either the tokenizer from tiktoken (if the library is available) or\n    fallback to the GPT2TokenizerFast from the transformers library.\n\n    Args:\n        tokenizer_name (str): The name of the tokenizer to load.\n        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\n    Raises:\n        ImportError: When `tiktoken` package is missing.\n        To use tiktoken tokenizer install it as follows:\n        `pip install tiktoken`\n\n    Returns:\n        tokenizer: Tokenizer of either GPT2 kind or tiktoken based.\n    \"\"\"\n    tokenizer = None\n    if use_tiktoken:\n        try:\n            import tiktoken  # pylint: disable=import-error\n\n            logger.debug(\"Using tiktoken %s tokenizer\", tokenizer_name)\n            tokenizer = tiktoken.get_encoding(tokenizer_name)\n        except ImportError:\n            raise ImportError(\n                \"The `tiktoken` package not found.\",\n                \"To install it use the following:\",\n                \"`pip install tiktoken`\",\n            )\n    else:\n        logger.warning(\n            \"OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and \"\n            \"AARCH64. Falling back to GPT2TokenizerFast.\"\n        )\n\n        logger.debug(\"Using GPT2TokenizerFast tokenizer\")\n        tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n    return tokenizer", "\n\ndef count_openai_tokens(text: str, tokenizer: Any, use_tiktoken: bool) -> int:\n    \"\"\"\n    Count the number of tokens in `text` based on the provided OpenAI `tokenizer`.\n\n    Args:\n        text (str):  A string to be tokenized.\n        tokenizer (Any): An OpenAI tokenizer.\n        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\n    Returns:\n        int: Number of tokens in the text.\n    \"\"\"\n\n    if use_tiktoken:\n        return len(tokenizer.encode(text))\n    else:\n        return len(tokenizer.tokenize(text))", "\n\n@retry_with_exponential_backoff(\n    backoff_in_seconds=OPENAI_BACKOFF,\n    max_retries=OPENAI_MAX_RETRIES,\n    errors=(OpenAIRateLimitError, OpenAIError),\n)\ndef openai_request(\n    url: str,\n    headers: Dict,\n    payload: Dict,\n    timeout: Union[float, Tuple[float, float]] = OPENAI_TIMEOUT,\n) -> Dict:\n    \"\"\"\n    Make a request to the OpenAI API given a `url`, `headers`, `payload`, and\n    `timeout`.\n\n    Args:\n        url (str): The URL of the OpenAI API.\n        headers (Dict): Dictionary of HTTP Headers to send with the :class:`Request`.\n        payload (Dict): The payload to send with the request.\n        timeout (Union[float, Tuple[float, float]], optional): The timeout length of the request. The default is 30s.\n        Defaults to OPENAI_TIMEOUT.\n\n    Raises:\n        openai_error: If the request fails.\n\n    Returns:\n        Dict: OpenAI Embedding API response.\n    \"\"\"\n\n    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), timeout=timeout)\n    res = json.loads(response.text)\n\n    # if request is unsucessful and `status_code = 429` then,\n    # raise rate limiting error else the OpenAIError\n    if response.status_code != 200:\n        openai_error: OpenAIError\n        if response.status_code == 429:\n            openai_error = OpenAIRateLimitError(f\"API rate limit exceeded: {response.text}\")\n        else:\n            openai_error = OpenAIError(\n                f\"OpenAI returned an error.\\n\"\n                f\"Status code: {response.status_code}\\n\"\n                f\"Response body: {response.text}\",\n                status_code=response.status_code,\n            )\n        raise openai_error\n\n    return res", "\n\ndef get_prompt(message: str, history: str) -> str:\n    \"\"\"\n    Generates the prompt based on the current history and message.\n\n    Args:\n        message (str): Current message from user.\n        history (str): Retrieved history for the current message.\n        History follows the following format for example:\n        ```\n        Human: hello\n        Assistant: hello, how are you?\n        Human: good, you?\n        Assistant: I am doing good as well. How may I help you?\n        ```\n    Returns:\n        prompt: Curated prompt for the ChatGPT API based on current params.\n    \"\"\"\n    prompt = f\"\"\"Assistant is a large language model trained by OpenAI.\n\n    Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\n    Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\n    Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\n    {history}\n    Human: {message}\n    Assistant:\"\"\"\n\n    return prompt", ""]}
{"filename": "acad_gpt/datastore/config.py", "chunked_list": ["from enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass RedisIndexType(str, Enum):\n    hnsw = \"HNSW\"\n    flat = \"FLAT\"\n\n\nclass DataStoreConfig(BaseModel):\n    host: str\n    port: int\n    password: str", "\n\nclass DataStoreConfig(BaseModel):\n    host: str\n    port: int\n    password: str\n\n\nclass RedisDataStoreConfig(DataStoreConfig):\n    index_type: str = RedisIndexType.hnsw\n    vector_field_name: str = \"embedding\"\n    vector_dimensions: int = 1024\n    distance_metric: str = \"L2\"\n    number_of_vectors: int = 686\n    M: int = 40\n    EF: int = 200", "class RedisDataStoreConfig(DataStoreConfig):\n    index_type: str = RedisIndexType.hnsw\n    vector_field_name: str = \"embedding\"\n    vector_dimensions: int = 1024\n    distance_metric: str = \"L2\"\n    number_of_vectors: int = 686\n    M: int = 40\n    EF: int = 200\n\n\nclass ElasticSearchStoreConfig(DataStoreConfig):\n    username: str\n    index_name: str", "\n\nclass ElasticSearchStoreConfig(DataStoreConfig):\n    username: str\n    index_name: str\n"]}
{"filename": "acad_gpt/datastore/elasticsearch.py", "chunked_list": ["import hashlib\nimport logging\nfrom typing import Any, Dict, List\n\nfrom elasticsearch import Elasticsearch, RequestsHttpConnection\n\nfrom acad_gpt.datastore.config import ElasticSearchStoreConfig\nfrom acad_gpt.datastore.datastore import DataStore\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass ElasticSearchDataStore(DataStore):\n    def __init__(self, config: ElasticSearchStoreConfig, **kwargs):\n        super().__init__(config=config)\n        self.config = config\n\n        self.connect()\n        self.create_index()\n\n    def connect(self):\n        \"\"\"\n        Connect to the ES server.\n        \"\"\"\n        self.es_connection = Elasticsearch(\n            self.config.host,\n            port=self.config.port,\n            connection_class=RequestsHttpConnection,\n            http_auth=(self.config.username, self.config.password),\n            use_ssl=True,\n            verify_certs=False,\n        )\n\n    def create_index(self):\n        \"\"\"\n        Creates a ES index.\n        \"\"\"\n        try:\n            ES_INDEX_MAPPING = {\n                \"properties\": {\n                    \"title\": {\"type\": \"text\"},\n                    \"text\": {\"type\": \"text\"},\n                    \"url\": {\"type\": \"text\"},\n                    \"status\": {\"type\": \"keyword\"},\n                    \"type\": {\"type\": \"keyword\"},\n                }\n            }\n            if not self.es_connection.indices.exists(self.config.index_name):\n                self.es_client.indices.create(index=self.config.index_name, body={\"mappings\": ES_INDEX_MAPPING})\n            logger.info(\"Created a new ES index\")\n        except Exception as es_error:\n            logger.info(f\"Working with existing ES index.\\nDetails: {es_error}\")\n\n    def index_documents(self, documents: List[Dict]):\n        \"\"\"\n        Indexes the set of documents.\n\n        Args:\n            documents (List[Dict]): List of documents to be indexed.\n        \"\"\"\n        for document in documents:\n            try:\n                assert \"text\" in document and \"url\" in document, \"Document must include the fields `text` and `url`\"\n                sha = hashlib.sha256()\n                sha.update(document.get(\"url\").encode())\n\n                self.es_connection.update(\n                    index=self.config.index_name, id=sha.hexdigest(), body={\"doc\": document, \"doc_as_upsert\": True}\n                )\n            except Exception as es_error:\n                logger.info(f\"Error in indexing document.\\nDetails: {es_error}\")\n\n    def search_documents(self, query: str, topk: int = 5, **kwargs) -> List[Any]:\n        \"\"\"\n        Searches the redis index using the query vector.\n\n        Args:\n            query_vector (np.ndarray): Embedded query vector.\n            topk (int, optional): Number of results. Defaults to 5.\n\n        Returns:\n            List[Any]: Search result documents.\n        \"\"\"\n        filter = [{\"match\": {\"text\": query}}]\n        status = kwargs.get(\"status\")\n        if status:\n            filter.append({\"match\": {\"status\": status}})\n        es_query = {\n            \"query\": {\"bool\": {\"must\": filter}},\n            \"size\": topk,\n        }\n        topk_hits = []\n        es_response = self.es_connection.search(index=self.config.index_name, body=es_query)\n        for inter_res in es_response[\"hits\"][\"hits\"]:\n            topk_hits.append(inter_res[\"_source\"])\n        return topk_hits\n\n    def delete_document(self, url: str):\n        \"\"\"\n        Deletes all documents for a given conversation id.\n\n        Args:\n            document_id (str): Id of the conversation to be deleted.\n        \"\"\"\n        sha = hashlib.sha256()\n        sha.update(url.encode())\n        try:\n            self.es_connection.delete(index=self.es_connection, id=sha.hexdigest())\n        except Exception as es_error:\n            logger.info(f\"Error in deleting document.\\nDetails: {es_error}\")", ""]}
{"filename": "acad_gpt/datastore/__init__.py", "chunked_list": ["from acad_gpt.datastore.config import (  # noqa: F401\n    DataStoreConfig,\n    ElasticSearchStoreConfig,\n    RedisDataStoreConfig,\n    RedisIndexType,\n)\nfrom acad_gpt.datastore.elasticsearch import ElasticSearchDataStore  # noqa: F401\nfrom acad_gpt.datastore.redis import RedisDataStore  # noqa: F401\n", ""]}
{"filename": "acad_gpt/datastore/redis.py", "chunked_list": ["import hashlib\nimport logging\nfrom typing import Any, Dict, List\n\nimport redis\nfrom redis.commands.search.field import TagField, TextField, VectorField\nfrom redis.commands.search.query import Query\n\nfrom acad_gpt.datastore.config import RedisDataStoreConfig\nfrom acad_gpt.datastore.datastore import DataStore", "from acad_gpt.datastore.config import RedisDataStoreConfig\nfrom acad_gpt.datastore.datastore import DataStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass RedisDataStore(DataStore):\n    def __init__(self, config: RedisDataStoreConfig, do_flush_data: bool = False, **kwargs):\n        super().__init__(config=config)\n        self.config = config\n        self.do_flush_data = do_flush_data\n\n        self.connect()\n        self.create_index()\n\n    def connect(self):\n        \"\"\"\n        Connect to the Redis server.\n        \"\"\"\n        connection_pool = redis.ConnectionPool(\n            host=self.config.host, port=self.config.port, password=self.config.password\n        )\n        self.redis_connection = redis.Redis(connection_pool=connection_pool)\n\n        # flush data only once after establishing connection\n        if self.do_flush_data:\n            self.flush_all_documents()\n            self.do_flush_data = False\n\n    def flush_all_documents(self):\n        \"\"\"\n        Removes all documents from the redis index.\n        \"\"\"\n        self.redis_connection.flushall()\n\n    def create_index(self):\n        \"\"\"\n        Creates a Redis index with a dense vector field.\n        \"\"\"\n        try:\n            self.redis_connection.ft().create_index(\n                [\n                    VectorField(\n                        self.config.vector_field_name,\n                        self.config.index_type,\n                        {\n                            \"TYPE\": \"FLOAT32\",\n                            \"DIM\": self.config.vector_dimensions,\n                            \"DISTANCE_METRIC\": self.config.distance_metric,\n                            \"INITIAL_CAP\": self.config.number_of_vectors,\n                            \"M\": self.config.M,\n                            \"EF_CONSTRUCTION\": self.config.EF,\n                        },\n                    ),\n                    TextField(\"text\"),  # contains the original message\n                    TextField(\"title\"),\n                    TagField(\"type\"),\n                    TagField(\"status\"),\n                    TextField(\"url\"),\n                ]\n            )\n            logger.info(\"Created a new Redis index for storing chat history\")\n        except redis.exceptions.ResponseError as redis_error:\n            logger.info(f\"Working with existing Redis index.\\nDetails: {redis_error}\")\n\n    def index_documents(self, documents: List[Dict]):\n        \"\"\"\n        Indexes the set of documents.\n\n        Args:\n            documents (List[Dict]): List of documents to be indexed.\n        \"\"\"\n        redis_pipeline = self.redis_connection.pipeline(transaction=False)\n        for document in documents:\n            assert \"text\" in document and \"url\" in document, \"Document must include the fields `text`, and `url`\"\n            sha = hashlib.sha256()\n            sha.update(document.get(\"url\").encode())\n            redis_pipeline.hset(sha.hexdigest(), mapping=document)\n        redis_pipeline.execute()\n\n    def search_documents(self, query: bytes, topk: int = 5, **kwargs) -> List[Any]:\n        \"\"\"\n        Searches the redis index using the query vector.\n\n        Args:\n            query_vector (np.ndarray): Embedded query vector.\n            topk (int, optional): Number of results. Defaults to 5.\n\n        Returns:\n            List[Any]: Search result documents.\n        \"\"\"\n        status = kwargs.get(\"status\", None)\n        type = kwargs.get(\"type\", None)\n        tag = \"(\"\n        if status:\n            tag += f\"@status:{{{status}}}\"\n        if type:\n            tag += f\"@type:{{{type}}}\"\n        tag += \")\"\n        # if no tags are selected\n        if len(tag) < 3:\n            tag = \"*\"\n        query_obj = (\n            Query(\n                f\"\"\"{tag}=>[KNN {topk} \\\n                    @{self.config.vector_field_name} $vec_param AS vector_score]\"\"\"\n            )\n            .sort_by(\"vector_score\")\n            .paging(0, topk)\n            .return_fields(\n                \"vector_score\",\n                \"text\",\n                \"title\",\n                \"type\",\n                \"status\",\n                \"url\",\n            )\n            .dialect(2)\n        )\n        params_dict = {\"vec_param\": query}\n        try:\n            result_documents = self.redis_connection.ft().search(query_obj, query_params=params_dict).docs\n        except redis.exceptions.ResponseError as redis_error:\n            logger.info(f\"Details: {redis_error}\")\n        return result_documents\n\n    def get_all_document_ids(self) -> List[str]:\n        \"\"\"\n        Returns document titles of all documents.\n\n        Returns:\n            List[str]: List of document titles stored in redis.\n        \"\"\"\n        query = Query(\"*\").return_fields(\"title\")\n        result_documents = self.redis_connection.ft().search(query).docs\n\n        document_ids: List[str] = []\n        document_ids = list(set([getattr(result_document, \"document_id\") for result_document in result_documents]))\n\n        return document_ids\n\n    def delete_documents(self, document_id: str):\n        \"\"\"\n        Deletes all documents for a given conversation id.\n\n        Args:\n            document_id (str): Id of the conversation to be deleted.\n        \"\"\"\n        query = (\n            Query(f\"\"\"(@document_id:{{{document_id}}})\"\"\")\n            .return_fields(\n                \"id\",\n            )\n            .dialect(2)\n        )\n        for document in self.redis_connection.ft().search(query).docs:\n            document_id = getattr(document, \"id\")\n            deletion_status = self.redis_connection.ft().delete_document(document_id, delete_actual_document=True)\n\n            assert deletion_status, f\"Deletion of the document with id {document_id} failed!\"", ""]}
{"filename": "acad_gpt/datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List\n\nfrom acad_gpt.datastore.config import DataStoreConfig\n\n\nclass DataStore(ABC):\n    \"\"\"\n    Abstract class for datastores.\n    \"\"\"\n\n    def __init__(self, config: DataStoreConfig):\n        self.config = config\n\n    @abstractmethod\n    def connect(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_index(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def index_documents(self, documents: List[Dict]):\n        raise NotImplementedError\n\n    @abstractmethod\n    def search_documents(self, query: Any, topk: int, **kwargs) -> List[Any]:\n        raise NotImplementedError", ""]}
{"filename": "acad_gpt/parsers/config.py", "chunked_list": ["from pydantic import BaseModel\n\n\nclass ParserConfig(BaseModel):\n    file_path: str\n    file_url: str\n\n\nclass PDF2ImageConversionOptions(BaseModel):\n    grayscale: bool = True\n    hide_annotations: bool = True\n    paths_only: bool = True\n    last_page: int = 5", "class PDF2ImageConversionOptions(BaseModel):\n    grayscale: bool = True\n    hide_annotations: bool = True\n    paths_only: bool = True\n    last_page: int = 5\n\n\nclass PDFColumnClassifierConfig(BaseModel):\n    dpi: int = 50\n    middle_column_width: int = 2\n    erosion_kernel_size: int = 5\n    vote_threshold: float = 0.7\n\n    pdf_conversion_options: PDF2ImageConversionOptions = PDF2ImageConversionOptions()", ""]}
{"filename": "acad_gpt/parsers/__init__.py", "chunked_list": ["from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentType  # noqa: 401\nfrom acad_gpt.parsers.config import ParserConfig  # noqa: 401\nfrom acad_gpt.parsers.pdf_parser import PDFParser  # noqa: 401\nfrom acad_gpt.parsers.webpage_parser import WebPageParser  # noqa: 401\n"]}
{"filename": "acad_gpt/parsers/pdf_parser_deprecated.py", "chunked_list": ["# \"\"\"\n# pdfannots = \"^0.4\"\n# PyMuPDF = \"^1.18.13\"\n# pillow = \"^9.5.0\"\n# pdf2image = \"^1.16.3\"\n# \"\"\"\n\n# import json\n# import logging\n# import os", "# import logging\n# import os\n# import shutil\n# from pathlib import Path\n# from typing import Any, Dict, List, Union\n# from uuid import uuid4\n\n# import numpy as np\n# import pdf2image\n# import pdfannots", "# import pdf2image\n# import pdfannots\n# import pdfminer\n# import scipdf\n# from PIL import Image, ImageFilter\n\n# from acad_gpt.environment import FILE_UPLOAD_PATH\n# from acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n# from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentType\n# from acad_gpt.parsers.config import ParserConfig, PDFColumnClassifierConfig", "# from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentType\n# from acad_gpt.parsers.config import ParserConfig, PDFColumnClassifierConfig\n\n# logger = logging.getLogger(__name__)\n\n\n# class PDFParser(BaseParser):\n#     def parse(self, config: Union[ParserConfig, List[ParserConfig]]) -> Dict:\n#         if not isinstance(config, List):\n#             config = [config]", "#         if not isinstance(config, List):\n#             config = [config]\n\n#         parsed_content: Dict[str, Any] = {}\n#         for config_item in config:\n#             file_path = getattr(config_item, \"file_path_or_url\")\n#             extract_highlights = getattr(config_item, \"extract_highlights\")\n#             pdf_metadata, metadata_path = PDFParser.get_pdf_metadata(\n#                 file_path=file_path, extract_figures=config_item.extract_figures\n#             )", "#                 file_path=file_path, extract_figures=config_item.extract_figures\n#             )\n#             pdf_metadata[\"file_name\"] = os.path.basename(file_path)\n#             pdf_metadata[\"url\"] = file_path\n#             parsed_content = {\n#                 \"metadata\": pdf_metadata,\n#                 \"metadata_path\": metadata_path if config_item.extract_figures else None,\n#             }\n#             if extract_highlights:\n#                 pdf_layout = 2 if PDFParser.classify_pdf_from_path(pdf_path=file_path) else 1", "#             if extract_highlights:\n#                 pdf_layout = 2 if PDFParser.classify_pdf_from_path(pdf_path=file_path) else 1\n#                 parsed_content[\"highlights\"] = PDFParser.get_pdf_highlights(file_path=file_path, pdf_layout=pdf_layout)\n\n#         return parsed_content\n\n#     @staticmethod\n#     def get_pdf_highlights(file_path: str, pdf_layout: int) -> List[Dict]:\n#         highlights: List[Dict] = []\n#         path = Path(file_path)", "#         highlights: List[Dict] = []\n#         path = Path(file_path)\n#         laparams = pdfminer.layout.LAParams()\n#         with path.open(\"rb\") as f:\n#             doc = pdfannots.process_file(f, columns_per_page=pdf_layout, laparams=laparams)\n#         for page in doc.pages:\n#             for annotation in page.annots:\n#                 try:\n#                     highlights.append(\n#                         {", "#                     highlights.append(\n#                         {\n#                             \"text\": annotation.gettext(remove_hyphens=True),\n#                             \"page\": annotation.pos.page.pageno + 1,\n#                             \"regionBoundary\": [annotation.pos.x, annotation.pos.y],\n#                         }\n#                     )\n#                 except Exception:\n#                     continue\n#         return highlights", "#                     continue\n#         return highlights\n\n#     @staticmethod\n#     def get_pdf_metadata(file_path: str, extract_figures: bool = False, figures_directory: str = FILE_UPLOAD_PATH):\n#         pdf_metadata = None\n#         try:\n#             pdf_metadata = scipdf.parse_pdf_to_dict(file_path)\n#             if extract_figures:\n#                 isExist = os.path.exists(figures_directory)", "#             if extract_figures:\n#                 isExist = os.path.exists(figures_directory)\n#                 if not isExist:\n#                     # create the metadata directory because it does not exist\n#                     os.makedirs(figures_directory)\n\n#                 # folder should contain only PDF files\n#                 scipdf.parse_figures(file_path, output_folder=figures_directory)\n#                 # TODO: clean up extracted images after storing them in object storage\n#         except Exception as error:", "#                 # TODO: clean up extracted images after storing them in object storage\n#         except Exception as error:\n#             logger.error(error)\n#         return pdf_metadata, figures_directory\n\n#     def pdf_to_documents(\n#         self,\n#         pdf_contents: Dict,\n#         embed_client: EmbeddingClient,\n#         file_name: str,", "#         embed_client: EmbeddingClient,\n#         file_name: str,\n#         clean_up: bool = True,\n#         extract_highlights: bool = False,\n#     ) -> List[Dict]:\n#         metadata_path = pdf_contents.pop(\"metadata_path\")\n#         documents = []\n#         metadata = dict(pdf_contents.pop(\"metadata\")) if pdf_contents.get(\"metadata\") else None\n#         document_id = metadata.get(\"file_name\") if metadata else None\n#         url = metadata.get(\"file_name\", \"\")", "#         document_id = metadata.get(\"file_name\") if metadata else None\n#         url = metadata.get(\"file_name\", \"\")\n#         title = metadata.pop(\"title\", \"\")\n#         if metadata and metadata_path:\n#             abstract = metadata.pop(\"abstract\")\n#             sections = \"\\n\".join([section.get(\"text\", \"\") for section in metadata.pop(\"sections\", [])])\n#             embedding = (\n#                 embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  Abstract: \\n  {abstract}\"}])[0]\n#                 .astype(np.float32)\n#                 .tobytes()", "#                 .astype(np.float32)\n#                 .tobytes()\n#             )\n#             abstract_doc = Document(\n#                 text=f\"{abstract} {sections}\",\n#                 title=title,\n#                 type=DocumentType.paper,\n#                 url=url,\n#                 embedding=embedding,\n#             ).dict()", "#                 embedding=embedding,\n#             ).dict()\n\n#             documents = [abstract_doc]\n#         if \"highlights\" in pdf_contents:\n#             for section in sections:\n#                 section_name = section.get(\"heading\", \"\")\n#                 text = section.get(\"text\", \"\")\n#                 embedding = (\n#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n {section_name}: \\n  {text}\"}])[0]", "#                 embedding = (\n#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n {section_name}: \\n  {text}\"}])[0]\n#                     .astype(np.float32)\n#                     .tobytes()\n#                 )\n#                 document = Document(\n#                     document_id=document_id,\n#                     section=section_name,\n#                     text=text,\n#                     title=title,", "#                     text=text,\n#                     title=title,\n#                     type=\"Section\",\n#                     page=0,\n#                     embedding=embedding,\n#                     regionBoundary=\"\",\n#                 )\n#                 documents.append(document.dict())\n\n#             with open(f\"{metadata_path}/data/{file_name}.json\") as user_file:", "\n#             with open(f\"{metadata_path}/data/{file_name}.json\") as user_file:\n#                 file_contents = user_file.read()\n\n#             for doc in json.loads(file_contents):\n#                 text = doc.get(\"caption\", \"\")\n#                 embedding = (\n#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n {text}\"}])[0].astype(np.float32).tobytes()\n#                 )\n#                 document = Document(", "#                 )\n#                 document = Document(\n#                     document_id=uuid4().hex,\n#                     section=\"\",\n#                     text=text,\n#                     title=title,\n#                     type=doc.get(\"figType\", \"\"),\n#                     page=int(doc.get(\"page\", \"\")),\n#                     embedding=embedding,\n#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),", "#                     embedding=embedding,\n#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),\n#                 )\n#                 documents.append(document.dict())\n#             for doc in pdf_contents[\"highlights\"]:\n#                 text = doc.get(\"text\", \"\")\n#                 embedding = (\n#                     embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  {text}\"}])[0].astype(np.float32).tobytes()\n#                 )\n#                 document = Document(", "#                 )\n#                 document = Document(\n#                     document_id=uuid4().hex,\n#                     section=\"\",\n#                     text=text,\n#                     title=title,\n#                     type=\"Highlight\",\n#                     page=int(doc.get(\"page\", -1)),\n#                     embedding=embedding,\n#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),", "#                     embedding=embedding,\n#                     regionBoundary=str(doc.get(\"regionBoundary\", \"\")),\n#                 )\n#                 documents.append(document.dict())\n#         if clean_up and os.path.exists(f\"{FILE_UPLOAD_PATH}\"):\n#             shutil.rmtree(FILE_UPLOAD_PATH)\n#         return documents\n\n#     @staticmethod\n#     def classify_pdf_from_path(", "#     @staticmethod\n#     def classify_pdf_from_path(\n#         pdf_path: Union[str, Path], config: PDFColumnClassifierConfig = PDFColumnClassifierConfig()\n#     ) -> bool:\n#         \"\"\"\n#         Classify a PDF as either single-column or two-column.\n\n#         Args:\n#             pdf_path: The PDF to classify.\n#             config: The configuration to use when classifying the PDF. Uses the default configuration if not specified.", "#             pdf_path: The PDF to classify.\n#             config: The configuration to use when classifying the PDF. Uses the default configuration if not specified.\n\n#         Returns:\n#             True if the PDF is two-column, False otherwise.\n#         \"\"\"\n#         # first, convert the PDF to a list of images.\n#         imgs = pdf2image.convert_from_path(pdf_path, dpi=config.dpi, **config.pdf_conversion_options.dict())\n#         return PDFParser._classify_imgs(imgs, config)\n", "#         return PDFParser._classify_imgs(imgs, config)\n\n#     @staticmethod\n#     def classify_pdf_from_bytes(\n#         pdf_bytes: bytes, config: PDFColumnClassifierConfig = PDFColumnClassifierConfig()\n#     ) -> bool:\n#         \"\"\"\n#         Classify a PDF as either single-column or two-column.\n\n#         Args:", "\n#         Args:\n#             pdf_bytes: The PDF as bytes to classify.\n#             config: The configuration to use when classifying the PDF. Uses the default configuration if not specified.\n\n#         Returns:\n#             True if the PDF is two-column, False otherwise.\n#         \"\"\"\n#         # first, convert the PDF to a list of images.\n#         imgs = pdf2image.convert_from_bytes(pdf_bytes, dpi=config.dpi, **config.pdf_conversion_options.dict())", "#         # first, convert the PDF to a list of images.\n#         imgs = pdf2image.convert_from_bytes(pdf_bytes, dpi=config.dpi, **config.pdf_conversion_options.dict())\n#         return PDFParser._classify_imgs(imgs, config)\n\n#     @staticmethod\n#     def _classify_imgs(imgs: List[Any], config: PDFColumnClassifierConfig) -> bool:\n#         \"\"\"\n#         Classify a list of images as either single-column or two-column.\n\n#         Args:", "\n#         Args:\n#             imgs: The images to classify.\n#             config: The configuration to use when classifying the images.\n\n#         Returns:\n#             True if the images are two-column, False otherwise.\n#         \"\"\"\n#         # classify each image in the PDF as either single-column or two-column.\n#         votes = []", "#         # classify each image in the PDF as either single-column or two-column.\n#         votes = []\n#         for img in imgs:\n#             # apply erosion to make the text more prominent\n#             if config.erosion_kernel_size > 1:\n#                 img = img.filter(ImageFilter.MinFilter(config.erosion_kernel_size))\n\n#             # calculate the percentage of whitish pixels in the middle of the image\n#             percentage_whitish_in_middle = PDFParser._calc_percentage_whitish_in_middle(img, config.middle_column_width)\n", "#             percentage_whitish_in_middle = PDFParser._calc_percentage_whitish_in_middle(img, config.middle_column_width)\n\n#             # if the percentage of whitish pixels in the middle of the\n#             # image is greater than VOTE_THRESHOLD, then the image is two-column.\n#             vote = percentage_whitish_in_middle > config.vote_threshold\n\n#             # add the vote to list\n#             votes.append(vote)\n\n#         # if the majority of the images in the PDF are classified as two-column, then the PDF is two-column.", "\n#         # if the majority of the images in the PDF are classified as two-column, then the PDF is two-column.\n#         return sum(votes) > len(votes) // 2\n\n#     @staticmethod\n#     def _calc_percentage_whitish_in_middle(img: Image, middle_column_width: int) -> float:\n#         \"\"\"\n#         Calculate the percentage of whitish pixels in the middle of the image.\n\n#         Args:", "\n#         Args:\n#             img: The image to calculate the percentage of whitish pixels in the middle of.\n#             middle_column_width: The width of the middle column to calculate the percentage of whitish pixels in.\n\n#         Returns:\n#             The percentage of whitish pixels in the middle of the image.\n#         \"\"\"\n#         img = img.convert(\"L\")\n#         middle_column = img.crop(", "#         img = img.convert(\"L\")\n#         middle_column = img.crop(\n#             (img.width // 2 - middle_column_width // 2, 0, img.width // 2 + middle_column_width // 2, img.height)\n#         )\n#         middle_column = middle_column.point(lambda x: 0 if x < 150 else 255, mode=\"1\")\n#         return np.array(middle_column).mean()\n"]}
{"filename": "acad_gpt/parsers/base_parser.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom acad_gpt.parsers.config import ParserConfig\n\n\nclass DocumentType(str, Enum):\n    paper = \"paper\"\n    code = \"code\"\n    webpage = \"webpage\"\n    pdf = \"pdf\"", "\nclass DocumentType(str, Enum):\n    paper = \"paper\"\n    code = \"code\"\n    webpage = \"webpage\"\n    pdf = \"pdf\"\n\n\nclass DocumentStatus(str, Enum):\n    todo = \"todo\"\n    done = \"done\"", "class DocumentStatus(str, Enum):\n    todo = \"todo\"\n    done = \"done\"\n\n\nclass Document(BaseModel):\n    text: str\n    title: str\n    type: str\n    url: str\n    embedding: Any\n    status: str = DocumentStatus.todo", "\n\nclass BaseParser(ABC):\n    \"\"\"\n    Abstract class for datastores.\n    \"\"\"\n\n    @abstractmethod\n    def parse(self, config: ParserConfig):\n        raise NotImplementedError", ""]}
{"filename": "acad_gpt/parsers/pdf_parser.py", "chunked_list": ["import logging\nimport os\nimport shutil\nfrom typing import Any, Dict, List, Union\n\nimport numpy as np\nimport scipdf\n\nfrom acad_gpt.environment import FILE_UPLOAD_PATH\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient", "from acad_gpt.environment import FILE_UPLOAD_PATH\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\nfrom acad_gpt.parsers.base_parser import BaseParser, Document, DocumentStatus, DocumentType\nfrom acad_gpt.parsers.config import ParserConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass PDFParser(BaseParser):\n    def parse(self, config: Union[ParserConfig, List[ParserConfig]]) -> Dict:\n        if not isinstance(config, List):\n            config = [config]\n\n        parsed_content: Dict[str, Any] = {}\n        for config_item in config:\n            file_path = getattr(config_item, \"file_path\")\n\n            pdf_metadata = PDFParser.get_pdf_metadata(file_path=file_path)\n            pdf_metadata[\"file_name\"] = os.path.basename(file_path)\n            pdf_metadata[\"url\"] = getattr(config_item, \"file_url\")\n\n            parsed_content = {\n                \"metadata\": pdf_metadata,\n            }\n\n        return parsed_content\n\n    @staticmethod\n    def get_pdf_metadata(file_path: str, figures_directory: str = FILE_UPLOAD_PATH):\n        pdf_metadata = None\n\n        try:\n            pdf_metadata = scipdf.parse_pdf_to_dict(file_path)\n        except Exception as error:\n            logger.error(error)\n\n        return pdf_metadata\n\n    def to_documents(\n        self,\n        pdf_contents: Dict,\n        embed_client: EmbeddingClient,\n        type: str = DocumentType.pdf,\n        status: str = DocumentStatus.todo,\n        **kwargs,\n    ) -> Dict:\n        document = {}\n        clean_up = bool(kwargs.get(\"clean_up\", True))\n\n        metadata = dict(pdf_contents.pop(\"metadata\")) if pdf_contents.get(\"metadata\") else None\n        url = metadata.get(\"url\", \"\")\n        title = metadata.pop(\"title\", \"\")\n\n        if metadata:\n            abstract = metadata.pop(\"abstract\")\n            sections = \"\\n\".join([section.get(\"text\", \"\") for section in metadata.pop(\"sections\", [])])\n            embedding = (\n                embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  {abstract} \\n  {sections}\"}])[0]\n                .astype(np.float32)\n                .tobytes()\n            )\n            document = Document(\n                text=abstract,\n                title=title,\n                type=type,\n                url=url,\n                status=status,\n                embedding=embedding,\n            ).dict()\n\n        if clean_up and os.path.exists(f\"{FILE_UPLOAD_PATH}\"):\n            shutil.rmtree(FILE_UPLOAD_PATH)\n\n        return document", "class PDFParser(BaseParser):\n    def parse(self, config: Union[ParserConfig, List[ParserConfig]]) -> Dict:\n        if not isinstance(config, List):\n            config = [config]\n\n        parsed_content: Dict[str, Any] = {}\n        for config_item in config:\n            file_path = getattr(config_item, \"file_path\")\n\n            pdf_metadata = PDFParser.get_pdf_metadata(file_path=file_path)\n            pdf_metadata[\"file_name\"] = os.path.basename(file_path)\n            pdf_metadata[\"url\"] = getattr(config_item, \"file_url\")\n\n            parsed_content = {\n                \"metadata\": pdf_metadata,\n            }\n\n        return parsed_content\n\n    @staticmethod\n    def get_pdf_metadata(file_path: str, figures_directory: str = FILE_UPLOAD_PATH):\n        pdf_metadata = None\n\n        try:\n            pdf_metadata = scipdf.parse_pdf_to_dict(file_path)\n        except Exception as error:\n            logger.error(error)\n\n        return pdf_metadata\n\n    def to_documents(\n        self,\n        pdf_contents: Dict,\n        embed_client: EmbeddingClient,\n        type: str = DocumentType.pdf,\n        status: str = DocumentStatus.todo,\n        **kwargs,\n    ) -> Dict:\n        document = {}\n        clean_up = bool(kwargs.get(\"clean_up\", True))\n\n        metadata = dict(pdf_contents.pop(\"metadata\")) if pdf_contents.get(\"metadata\") else None\n        url = metadata.get(\"url\", \"\")\n        title = metadata.pop(\"title\", \"\")\n\n        if metadata:\n            abstract = metadata.pop(\"abstract\")\n            sections = \"\\n\".join([section.get(\"text\", \"\") for section in metadata.pop(\"sections\", [])])\n            embedding = (\n                embed_client.embed_documents(docs=[{\"text\": f\"{title} \\n  {abstract} \\n  {sections}\"}])[0]\n                .astype(np.float32)\n                .tobytes()\n            )\n            document = Document(\n                text=abstract,\n                title=title,\n                type=type,\n                url=url,\n                status=status,\n                embedding=embedding,\n            ).dict()\n\n        if clean_up and os.path.exists(f\"{FILE_UPLOAD_PATH}\"):\n            shutil.rmtree(FILE_UPLOAD_PATH)\n\n        return document", ""]}
{"filename": "acad_gpt/parsers/webpage_parser.py", "chunked_list": ["import logging\nfrom typing import Dict\n\nimport numpy as np\nimport requests\nfrom bs4 import BeautifulSoup, Comment\n\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\nfrom acad_gpt.parsers.base_parser import BaseParser, Document, DocumentStatus, DocumentType\nfrom acad_gpt.parsers.config import ParserConfig", "from acad_gpt.parsers.base_parser import BaseParser, Document, DocumentStatus, DocumentType\nfrom acad_gpt.parsers.config import ParserConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass WebPageParser(BaseParser):\n    def parse(self, config: ParserConfig) -> Dict:\n        self.config = config\n\n        # parse the webpage\n        title = self.get_title()\n        text = self.get_text()\n\n        return {\"title\": title, \"text\": text, \"url\": self.config.file_url}\n\n    def get_title(self):\n        reqs = requests.get(self.config.file_url)\n        soup = BeautifulSoup(reqs.text, \"html.parser\")\n\n        for title in soup.find_all(\"title\"):\n            if title.get_text():\n                return title.get_text()\n            else:\n                return \"\"\n\n    def get_text(self):\n        reqs = requests.get(self.config.file_url)\n        soup = BeautifulSoup(reqs.text, \"html.parser\")\n        texts = soup.findAll(text=True)\n        visible_texts = filter(WebPageParser.tag_visible, texts)\n        return \" \".join(t.strip() for t in visible_texts)\n\n    @staticmethod\n    def tag_visible(element):\n        if element.parent.name in [\"style\", \"script\", \"head\", \"title\", \"meta\", \"[document]\"]:\n            return False\n        if isinstance(element, Comment):\n            return False\n        return True\n\n    def to_documents(\n        self,\n        web_contents: Dict,\n        embed_client: EmbeddingClient,\n        type: str = DocumentType.webpage,\n        status: str = DocumentStatus.todo,\n        **kwargs,\n    ) -> Dict:\n        document = {}\n\n        url = web_contents.get(\"url\", \"\")\n        title = web_contents.pop(\"title\", \"\")\n        text = web_contents.pop(\"text\", \"\")[:4500]\n        embedding = embed_client.embed_documents(docs=[{\"text\": text}])[0].astype(np.float32).tobytes()\n        document = Document(\n            text=text,\n            title=title,\n            url=url,\n            type=type,\n            embedding=embedding,\n            status=status,\n        ).dict()\n\n        return document", ""]}
{"filename": "acad_gpt/llm_client/config.py", "chunked_list": ["from pydantic import BaseModel\n\n\nclass LLMClientConfig(BaseModel):\n    api_key: str\n    time_out: float = 30\n"]}
{"filename": "acad_gpt/llm_client/__init__.py", "chunked_list": ["from acad_gpt.llm_client.openai.conversation.chatgpt_client import ChatGPTClient, ChatGPTConfig  # noqa: F401\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient  # noqa: F401\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingConfig  # noqa: F401\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingModels  # noqa: F401\n"]}
{"filename": "acad_gpt/llm_client/llm_client.py", "chunked_list": ["from abc import ABC\n\nfrom acad_gpt.llm_client.config import LLMClientConfig\n\n\nclass LLMClient(ABC):\n    \"\"\"\n    Wrapper for the HTTP APIs for LLMs acting as data container for API configurations.\n    \"\"\"\n\n    def __init__(self, config: LLMClientConfig):\n        self._api_key = config.api_key\n        self._time_out = config.time_out\n\n    @property\n    def api_key(self):\n        return self._api_key\n\n    @property\n    def time_out(self):\n        return self._time_out", ""]}
{"filename": "acad_gpt/llm_client/openai/__init__.py", "chunked_list": [""]}
{"filename": "acad_gpt/llm_client/openai/embedding/embedding_client.py", "chunked_list": ["import logging\nfrom typing import Any, Dict, List, Union\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom acad_gpt.constants import MAX_ALLOWED_SEQ_LEN_001, MAX_ALLOWED_SEQ_LEN_002\nfrom acad_gpt.llm_client.llm_client import LLMClient\nfrom acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\nfrom acad_gpt.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request", "from acad_gpt.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\nfrom acad_gpt.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingClient(LLMClient):\n    def __init__(self, config: EmbeddingConfig):\n        super().__init__(config=config)\n\n        self.openai_embedding_config = config\n        model_class: str = EmbeddingModels(self.openai_embedding_config.model).name\n\n        tokenizer = self._setup_encoding_models(\n            model_class,\n            self.openai_embedding_config.model,\n            self.openai_embedding_config.max_seq_len,\n        )\n        self._tokenizer = load_openai_tokenizer(\n            tokenizer_name=tokenizer,\n            use_tiktoken=self.openai_embedding_config.use_tiktoken,\n        )\n\n    def _setup_encoding_models(self, model_class: str, model_name: str, max_seq_len: int):\n        \"\"\"\n        Setup the encoding models for the retriever.\n\n        Raises:\n            ImportError: When `tiktoken` package is missing.\n            To use tiktoken tokenizer install it as follows:\n            `pip install tiktoken`\n        \"\"\"\n\n        tokenizer_name = \"gpt2\"\n        # new generation of embedding models (December 2022), specify the full name\n        if model_name.endswith(\"-002\"):\n            self.query_encoder_model = model_name\n            self.doc_encoder_model = model_name\n            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_002, max_seq_len)\n            if self.openai_embedding_config.use_tiktoken:\n                try:\n                    from tiktoken.model import MODEL_TO_ENCODING\n\n                    tokenizer_name = MODEL_TO_ENCODING.get(model_name, \"cl100k_base\")\n                except ImportError:\n                    raise ImportError(\n                        \"The `tiktoken` package not found.\",\n                        \"To install it use the following:\",\n                        \"`pip install tiktoken`\",\n                    )\n        else:\n            self.query_encoder_model = f\"text-search-{model_class}-query-001\"\n            self.doc_encoder_model = f\"text-search-{model_class}-doc-001\"\n            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_001, max_seq_len)\n\n        return tokenizer_name\n\n    def _ensure_text_limit(self, text: str) -> str:\n        \"\"\"\n         Ensure that length of the text is within the maximum length of the model.\n        OpenAI v1 embedding models have a limit of 2046 tokens, and v2 models have\n        a limit of 8191 tokens.\n\n        Args:\n            text (str):  Text to be checked if it exceeds the max token limit\n\n        Returns:\n            text (str): Trimmed text if exceeds the max token limit\n        \"\"\"\n        n_tokens = count_openai_tokens(text, self._tokenizer, self.openai_embedding_config.use_tiktoken)\n        if n_tokens <= self.max_seq_len:\n            return text\n\n        logger.warning(\n            \"The prompt has been truncated from %s tokens to %s tokens to fit\" \"within the max token limit.\",\n            \"Reduce the length of the prompt to prevent it from being cut off.\",\n            n_tokens,\n            self.max_seq_len,\n        )\n\n        if self.openai_embedding_config.use_tiktoken:\n            tokenized_payload = self._tokenizer.encode(text)\n            decoded_string = self._tokenizer.decode(tokenized_payload[: self.max_seq_len])\n        else:\n            tokenized_payload = self._tokenizer.tokenize(text)\n            decoded_string = self._tokenizer.convert_tokens_to_string(tokenized_payload[: self.max_seq_len])\n\n        return decoded_string\n\n    def embed(self, model: str, text: List[str]) -> np.ndarray:\n        \"\"\"\n        Embeds the batch of texts using the specified LLM.\n\n        Args:\n            model (str): LLM model name for embeddings.\n            text (List[str]): List of documents to be embedded.\n\n        Raises:\n            ValueError: When the OpenAI API key is missing.\n\n        Returns:\n            np.ndarray: embeddings for the input documents.\n        \"\"\"\n        if self.api_key is None:\n            raise ValueError(\n                \"OpenAI API key is not set. You can set it via the \" \"`api_key` parameter of the `LLMClient`.\"\n            )\n\n        generated_embeddings: List[Any] = []\n\n        headers: Dict[str, str] = {\"Content-Type\": \"application/json\"}\n        payload: Dict[str, Union[List[str], str]] = {\"model\": model, \"input\": text}\n        headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        res = openai_request(\n            url=self.openai_embedding_config.url,\n            headers=headers,\n            payload=payload,\n            timeout=self.time_out,\n        )\n\n        unordered_embeddings = [(ans[\"index\"], ans[\"embedding\"]) for ans in res[\"data\"]]\n        ordered_embeddings = sorted(unordered_embeddings, key=lambda x: x[0])\n\n        generated_embeddings = [emb[1] for emb in ordered_embeddings]\n\n        return np.array(generated_embeddings)\n\n    def embed_batch(self, model: str, text: List[str]) -> np.ndarray:\n        all_embeddings = []\n        for i in tqdm(\n            range(0, len(text), self.openai_embedding_config.batch_size),\n            disable=not self.openai_embedding_config.progress_bar,\n            desc=\"Calculating embeddings\",\n        ):\n            batch = text[i : i + self.openai_embedding_config.batch_size]\n            batch_limited = [self._ensure_text_limit(content) for content in batch]\n            generated_embeddings = self.embed(model, batch_limited)\n            all_embeddings.append(generated_embeddings)\n\n        return np.concatenate(all_embeddings)\n\n    def embed_queries(self, queries: List[str]) -> np.ndarray:\n        return self.embed_batch(self.query_encoder_model, queries)\n\n    def embed_documents(self, docs: List[Dict]) -> np.ndarray:\n        return self.embed_batch(self.doc_encoder_model, [d[\"text\"] for d in docs])", ""]}
{"filename": "acad_gpt/llm_client/openai/embedding/config.py", "chunked_list": ["from enum import Enum\n\nfrom acad_gpt.llm_client.config import LLMClientConfig\n\n\nclass EmbeddingModels(Enum):\n    ada = \"*-ada-*-001\"\n    babbage = \"*-babbage-*-001\"\n    curie = \"*-curie-*-001\"\n    davinci = \"*-davinci-*-001\"", "\n\nclass EmbeddingConfig(LLMClientConfig):\n    url: str = \"https://api.openai.com/v1/embeddings\"\n    batch_size: int = 64\n    progress_bar: bool = False\n    model: str = EmbeddingModels.ada.value\n    max_seq_len: int = 8191\n    use_tiktoken: bool = False\n", ""]}
{"filename": "acad_gpt/llm_client/openai/embedding/__init__.py", "chunked_list": [""]}
{"filename": "acad_gpt/llm_client/openai/conversation/config.py", "chunked_list": ["from acad_gpt.llm_client.config import LLMClientConfig\n\n\nclass ChatGPTConfig(LLMClientConfig):\n    temperature: float = 0\n    model_name: str = \"gpt-3.5-turbo\"\n    max_retries: int = 6\n    max_tokens: int = 256\n    verbose: bool = False\n", ""]}
{"filename": "acad_gpt/llm_client/openai/conversation/__init__.py", "chunked_list": [""]}
{"filename": "acad_gpt/llm_client/openai/conversation/chatgpt_client.py", "chunked_list": ["import logging\n\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nfrom pydantic import BaseModel\n\nfrom acad_gpt.llm_client.llm_client import LLMClient\nfrom acad_gpt.llm_client.openai.conversation.config import ChatGPTConfig\nfrom acad_gpt.memory.manager import MemoryManager\nfrom acad_gpt.utils.openai_utils import get_prompt\n", "from acad_gpt.utils.openai_utils import get_prompt\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatGPTResponse(BaseModel):\n    message: str\n    chat_gpt_answer: str\n\n\nclass ChatGPTClient(LLMClient):\n    \"\"\"\n    ChatGPT client allows to interact with the ChatGPT model alonside having infinite contextual and adaptive memory.\n\n    \"\"\"\n\n    def __init__(self, config: ChatGPTConfig, memory_manager: MemoryManager):\n        super().__init__(config=config)\n        prompt = PromptTemplate(input_variables=[\"prompt\"], template=\"{prompt}\")\n        self.chatgpt_chain = LLMChain(\n            llm=OpenAI(\n                temperature=config.temperature,\n                openai_api_key=self.api_key,\n                model_name=config.model_name,\n                max_retries=config.max_retries,\n                max_tokens=config.max_tokens,\n            ),\n            prompt=prompt,\n            verbose=config.verbose,\n        )\n        self.memory_manager = memory_manager\n\n    def converse(self, message: str, topk: int = 5, **kwargs) -> ChatGPTResponse:\n        \"\"\"\n        Allows user to chat with user by leveraging the infinite contextual memory for fetching and\n        adding historical messages to the prompt to the ChatGPT model.\n\n        Args:\n            message (str): Message by the human user.\n\n        Returns:\n            ChatGPTResponse: Response includes answer from th ChatGPT, conversation_id, and human message.\n        \"\"\"\n\n        history = \"\"\n        try:\n            past_messages = self.memory_manager.get_messages(query=message, topk=topk, kwargs=kwargs)\n            history = \"\\n\".join([past_message.text for past_message in past_messages if getattr(past_message, \"text\")])\n        except ValueError as history_not_found_error:\n            logger.warning(f\"Details: {history_not_found_error}\")\n        prompt = get_prompt(message=message, history=history)\n        chat_gpt_answer = self.chatgpt_chain.predict(prompt=prompt)\n\n        return ChatGPTResponse(message=message, chat_gpt_answer=chat_gpt_answer)", "\n\nclass ChatGPTClient(LLMClient):\n    \"\"\"\n    ChatGPT client allows to interact with the ChatGPT model alonside having infinite contextual and adaptive memory.\n\n    \"\"\"\n\n    def __init__(self, config: ChatGPTConfig, memory_manager: MemoryManager):\n        super().__init__(config=config)\n        prompt = PromptTemplate(input_variables=[\"prompt\"], template=\"{prompt}\")\n        self.chatgpt_chain = LLMChain(\n            llm=OpenAI(\n                temperature=config.temperature,\n                openai_api_key=self.api_key,\n                model_name=config.model_name,\n                max_retries=config.max_retries,\n                max_tokens=config.max_tokens,\n            ),\n            prompt=prompt,\n            verbose=config.verbose,\n        )\n        self.memory_manager = memory_manager\n\n    def converse(self, message: str, topk: int = 5, **kwargs) -> ChatGPTResponse:\n        \"\"\"\n        Allows user to chat with user by leveraging the infinite contextual memory for fetching and\n        adding historical messages to the prompt to the ChatGPT model.\n\n        Args:\n            message (str): Message by the human user.\n\n        Returns:\n            ChatGPTResponse: Response includes answer from th ChatGPT, conversation_id, and human message.\n        \"\"\"\n\n        history = \"\"\n        try:\n            past_messages = self.memory_manager.get_messages(query=message, topk=topk, kwargs=kwargs)\n            history = \"\\n\".join([past_message.text for past_message in past_messages if getattr(past_message, \"text\")])\n        except ValueError as history_not_found_error:\n            logger.warning(f\"Details: {history_not_found_error}\")\n        prompt = get_prompt(message=message, history=history)\n        chat_gpt_answer = self.chatgpt_chain.predict(prompt=prompt)\n\n        return ChatGPTResponse(message=message, chat_gpt_answer=chat_gpt_answer)", ""]}
{"filename": "acad_gpt/docstore/base.py", "chunked_list": ["import abc\nfrom typing import List\n\n\nclass DocStore(abc.ABC):\n    @abc.abstractmethod\n    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n        \"\"\"\n        Uploads a file to the docstore from a local file path.\n\n        Args:\n            file_path: The path to the file to upload.\n            file_name: The name to give the file in the docstore.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def download_to_filename(self, file_name: str, file_path: str) -> None:\n        \"\"\"\n        Downloads a file from the docstore to a local file path.\n\n        Args:\n            file_name: The name of the file in the docstore.\n            file_path: The path to save the file to.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def upload_from_file(self, file, file_name: str) -> None:\n        \"\"\"\n        Uploads a file to the docstore from a file object.\n\n        Args:\n            file: The file object to upload.\n            file_name: The name to give the file in the docstore.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def download_to_file(self, file_name: str, file) -> None:\n        \"\"\"\n        Downloads a file from the docstore to a byte array.\n\n        Args:\n            file_name: The name of the file in the docstore.\n            file: The file object to save the file to.\n\n        Returns:\n            The bytes of the file.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def delete(self, file_name: str) -> None:\n        \"\"\"\n        Deletes a file from the docstore.\n\n        Args:\n            file_name: The name of the file in the docstore.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def list(self) -> List[str]:\n        \"\"\"\n        Lists all files in the docstore.\n\n        Returns:\n            A list of file names.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def exists(self, file_name: str) -> bool:\n        \"\"\"\n        Checks if a file exists in the docstore.\n\n        Args:\n            file_name: The name of the file in the docstore.\n\n        Returns:\n            True if the file exists, False otherwise.\n        \"\"\"\n        raise NotImplementedError", ""]}
{"filename": "acad_gpt/docstore/hf_file_system_storage.py", "chunked_list": ["\"\"\"\nBased on HuggingFace's File System API.\n\nSee https://huggingface.co/docs/huggingface_hub/v0.14.1/en/package_reference/hf_file_system#huggingface_hub.HfFileSystem\n\"\"\"\nimport logging\nimport os\nfrom typing import List, Optional\n\nfrom huggingface_hub import HfFileSystem, login", "\nfrom huggingface_hub import HfFileSystem, login\nfrom pydantic import BaseSettings, Field\n\nfrom acad_gpt.docstore.base import DocStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass HfFSDocStoreConfig(BaseSettings):\n    repo: str = Field(..., env=\"HF_FS_REPO\")\n    \"\"\" The repository to use as a file system docstore. \"\"\"\n    endpoint: Optional[str] = Field(None, env=\"HF_ENDPOINT\")\n    \"\"\" The endpoint to use. If None, the default endpoint is used. \"\"\"\n    token: Optional[str] = Field(None, env=\"HF_TOKEN\")\n    \"\"\" The token to use. If None, the default stored token is used. \"\"\"", "\nclass HfFSDocStoreConfig(BaseSettings):\n    repo: str = Field(..., env=\"HF_FS_REPO\")\n    \"\"\" The repository to use as a file system docstore. \"\"\"\n    endpoint: Optional[str] = Field(None, env=\"HF_ENDPOINT\")\n    \"\"\" The endpoint to use. If None, the default endpoint is used. \"\"\"\n    token: Optional[str] = Field(None, env=\"HF_TOKEN\")\n    \"\"\" The token to use. If None, the default stored token is used. \"\"\"\n\n\nclass HfFSDocStore(DocStore):\n    \"\"\"\n    A docstore that uses the HuggingFace File System API as the backend.\n\n    Args:\n        config (HfFSDocStoreConfig): The configuration to use.\n    \"\"\"\n\n    def __init__(self, config: HfFSDocStoreConfig):\n        self.config = config\n        \"\"\" The config to use.\"\"\"\n        # fail early incase no token is set\n        try:\n            login(self.config.token)\n        except ValueError as hf_value_error:\n            logger.error(hf_value_error)\n\n        self._hf_fs = HfFileSystem(**self.config.dict())\n\n    @property\n    def fs(self) -> HfFileSystem:\n        return self._hf_fs\n\n    def path_in_repo(self, file_name: str) -> str:\n        return os.path.join(self.config.repo, file_name)\n\n    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n        self.fs.put(lpath=file_path, rpath=self.path_in_repo(file_name))\n\n    def download_to_filename(self, file_name: str, file_path: str) -> None:\n        self.fs.get(rpath=self.path_in_repo(file_name), lpath=file_path)\n\n    def upload_from_file(self, file, file_name: str) -> None:\n        with self.fs.open(self.path_in_repo(file_name), \"wb\") as f:\n            f.write(file.read())\n\n    def download_to_file(self, file_name: str, file) -> None:\n        with self.fs.open(self.path_in_repo(file_name), \"rb\") as f:\n            file.write(f.read())\n\n    def delete(self, file_name: str) -> None:\n        self.fs.delete(self.path_in_repo(file_name))\n\n    def list(self) -> List[str]:\n        # 1. get files\n        files = self.fs.ls(self.config.repo, detail=False)\n\n        # 2. remove repo prefix for consistency with other API\n        files = [file_name[len(self.config.repo) + 1 :] for file_name in files]\n\n        return files\n\n    def exists(self, file_name: str) -> bool:\n        return self.fs.exists(self.path_in_repo(file_name))", "\n\nclass HfFSDocStore(DocStore):\n    \"\"\"\n    A docstore that uses the HuggingFace File System API as the backend.\n\n    Args:\n        config (HfFSDocStoreConfig): The configuration to use.\n    \"\"\"\n\n    def __init__(self, config: HfFSDocStoreConfig):\n        self.config = config\n        \"\"\" The config to use.\"\"\"\n        # fail early incase no token is set\n        try:\n            login(self.config.token)\n        except ValueError as hf_value_error:\n            logger.error(hf_value_error)\n\n        self._hf_fs = HfFileSystem(**self.config.dict())\n\n    @property\n    def fs(self) -> HfFileSystem:\n        return self._hf_fs\n\n    def path_in_repo(self, file_name: str) -> str:\n        return os.path.join(self.config.repo, file_name)\n\n    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n        self.fs.put(lpath=file_path, rpath=self.path_in_repo(file_name))\n\n    def download_to_filename(self, file_name: str, file_path: str) -> None:\n        self.fs.get(rpath=self.path_in_repo(file_name), lpath=file_path)\n\n    def upload_from_file(self, file, file_name: str) -> None:\n        with self.fs.open(self.path_in_repo(file_name), \"wb\") as f:\n            f.write(file.read())\n\n    def download_to_file(self, file_name: str, file) -> None:\n        with self.fs.open(self.path_in_repo(file_name), \"rb\") as f:\n            file.write(f.read())\n\n    def delete(self, file_name: str) -> None:\n        self.fs.delete(self.path_in_repo(file_name))\n\n    def list(self) -> List[str]:\n        # 1. get files\n        files = self.fs.ls(self.config.repo, detail=False)\n\n        # 2. remove repo prefix for consistency with other API\n        files = [file_name[len(self.config.repo) + 1 :] for file_name in files]\n\n        return files\n\n    def exists(self, file_name: str) -> bool:\n        return self.fs.exists(self.path_in_repo(file_name))", ""]}
{"filename": "acad_gpt/docstore/__init__.py", "chunked_list": [""]}
{"filename": "acad_gpt/docstore/in_memory_storage.py", "chunked_list": ["from typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom acad_gpt.docstore.base import DocStore\n\n\nclass InMemoryStorageConfig(BaseModel):\n    mutable: bool = True\n    \"\"\" Whether the storage is mutable. If mutable, the storage can be modified. If not, the storage is read-only. \"\"\"\n    max_size: Optional[int] = None\n    \"\"\" The maximum size of the storage. If None, the storage has no maximum size. \"\"\"", "\n\nclass InMemoryStorage(DocStore):\n    \"\"\"\n    A docstore that uses an in-memory dictionary as the backend.\n\n    Note:\n        Primarily intended for testing purposes.\n\n    Args:\n        config (InMemoryStorageConfig): The configuration to use.\n        storage (Optional[dict]): The storage to use. If None, an empty storage is initialized.\n    \"\"\"\n\n    def __init__(self, config: InMemoryStorageConfig, storage: Optional[dict] = None):\n        self.config = config\n        self.mutable = config.mutable\n        self.max_size = config.max_size\n        if storage is None:\n            storage = dict()\n        self.storage = storage\n        \"\"\" The in-memory storage. \"\"\"\n\n    def upload_from_filename(self, file_path: str, file_name: str) -> None:\n        if not self.mutable:\n            raise ValueError(\"The storage is not mutable.\")\n        if self.max_size is not None and len(self.storage) >= self.max_size:\n            raise ValueError(\"The storage is full.\")\n        with open(file_path, \"rb\") as f:\n            self.storage[file_name] = f.read()\n\n    def download_to_filename(self, file_name: str, file_path: str) -> None:\n        with open(file_path, \"wb\") as f:\n            f.write(self.storage[file_name])\n\n    def upload_from_file(self, file, file_name: str) -> None:\n        if not self.mutable:\n            raise ValueError(\"The storage is not mutable.\")\n        if self.max_size is not None and len(self.storage) >= self.max_size:\n            raise ValueError(\"The storage is full.\")\n        self.storage[file_name] = file.read()\n\n    def download_to_file(self, file_name: str, file) -> None:\n        file.write(self.storage[file_name])\n\n    def exists(self, file_name: str) -> bool:\n        return file_name in self.storage\n\n    def delete(self, file_name: str) -> None:\n        if not self.mutable:\n            raise ValueError(\"The storage is not mutable.\")\n        del self.storage[file_name]\n\n    def list(self) -> List[str]:\n        return list(self.storage.keys())", ""]}
{"filename": "acad_gpt/memory/manager.py", "chunked_list": ["from typing import Any, Dict, List\n\nimport numpy as np\n\nfrom acad_gpt.datastore.redis import RedisDataStore\nfrom acad_gpt.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nfrom .memory import Memory\n\n\nclass MemoryManager:\n    \"\"\"\n    Manages the memory of conversations.\n\n    Attributes:\n        datastore (DataStore): Datastore to use for storing and retrieving memories.\n        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n    \"\"\"\n\n    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n        \"\"\"\n        Initializes the memory manager.\n\n        Args:\n            datastore (DataStore): Datastore to be used. Assumed to be connected.\n            embed_client (EmbeddingClient): Embedding client to be used.\n            topk (int): Number of past message to be retrieved as context for current message.\n        \"\"\"\n        self.datastore = datastore\n        self.embed_client = embed_client\n        self.topk = topk\n        self.conversations: List[Memory] = [\n            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_document_ids()\n        ]\n\n    def __del__(self) -> None:\n        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n        self.clear()\n\n    def add_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Adds a conversation to the memory manager to be stored and manage.\n\n        Args:\n            conversation (Memory): Conversation to be added.\n        \"\"\"\n        if conversation not in self.conversations:\n            self.conversations.append(conversation)\n\n    def remove_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Removes a conversation from the memory manager.\n\n        Args:\n            conversation (Memory): Conversation to be removed containing `conversation_id`.\n        \"\"\"\n        if conversation not in self.conversations:\n            return\n\n        conversation_idx = self.conversations.index(conversation)\n        if conversation_idx >= 0:\n            del self.conversations[conversation_idx]\n            self.datastore.delete_documents(document_id=conversation.conversation_id)\n\n    def clear(self) -> None:\n        \"\"\"\n        Clears the memory manager.\n        \"\"\"\n        self.datastore.flush_all_documents()\n        self.conversations = []\n\n    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n        \"\"\"\n        Adds a message to a conversation.\n\n        Args:\n            conversation_id (str): ID of the conversation to add the message to.\n            human (str): User message.\n            assistant (str): Assistant message.\n        \"\"\"\n        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n        self.datastore.index_documents(documents=[document])\n\n        # optionally check if it is a new conversation\n        self.add_conversation(Memory(conversation_id=conversation_id))\n\n    def get_messages(self, query: str, topk: int = 5, **kwargs) -> List[Any]:\n        \"\"\"\n        Gets the messages of a conversation using the query message.\n\n        Args:\n            query (str): Current user message you want to pull history for to use in the prompt.\n            topk (int): Number of messages to be returned. Defaults to 5.\n\n        Returns:\n            List[Any]: List of messages of the conversation.\n        \"\"\"\n        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n        messages = self.datastore.search_documents(query=query_vector, topk=topk, kwargs=kwargs)\n        return messages", "\n\nclass MemoryManager:\n    \"\"\"\n    Manages the memory of conversations.\n\n    Attributes:\n        datastore (DataStore): Datastore to use for storing and retrieving memories.\n        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n    \"\"\"\n\n    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n        \"\"\"\n        Initializes the memory manager.\n\n        Args:\n            datastore (DataStore): Datastore to be used. Assumed to be connected.\n            embed_client (EmbeddingClient): Embedding client to be used.\n            topk (int): Number of past message to be retrieved as context for current message.\n        \"\"\"\n        self.datastore = datastore\n        self.embed_client = embed_client\n        self.topk = topk\n        self.conversations: List[Memory] = [\n            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_document_ids()\n        ]\n\n    def __del__(self) -> None:\n        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n        self.clear()\n\n    def add_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Adds a conversation to the memory manager to be stored and manage.\n\n        Args:\n            conversation (Memory): Conversation to be added.\n        \"\"\"\n        if conversation not in self.conversations:\n            self.conversations.append(conversation)\n\n    def remove_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Removes a conversation from the memory manager.\n\n        Args:\n            conversation (Memory): Conversation to be removed containing `conversation_id`.\n        \"\"\"\n        if conversation not in self.conversations:\n            return\n\n        conversation_idx = self.conversations.index(conversation)\n        if conversation_idx >= 0:\n            del self.conversations[conversation_idx]\n            self.datastore.delete_documents(document_id=conversation.conversation_id)\n\n    def clear(self) -> None:\n        \"\"\"\n        Clears the memory manager.\n        \"\"\"\n        self.datastore.flush_all_documents()\n        self.conversations = []\n\n    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n        \"\"\"\n        Adds a message to a conversation.\n\n        Args:\n            conversation_id (str): ID of the conversation to add the message to.\n            human (str): User message.\n            assistant (str): Assistant message.\n        \"\"\"\n        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n        self.datastore.index_documents(documents=[document])\n\n        # optionally check if it is a new conversation\n        self.add_conversation(Memory(conversation_id=conversation_id))\n\n    def get_messages(self, query: str, topk: int = 5, **kwargs) -> List[Any]:\n        \"\"\"\n        Gets the messages of a conversation using the query message.\n\n        Args:\n            query (str): Current user message you want to pull history for to use in the prompt.\n            topk (int): Number of messages to be returned. Defaults to 5.\n\n        Returns:\n            List[Any]: List of messages of the conversation.\n        \"\"\"\n        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n        messages = self.datastore.search_documents(query=query_vector, topk=topk, kwargs=kwargs)\n        return messages", ""]}
{"filename": "acad_gpt/memory/__init__.py", "chunked_list": ["from acad_gpt.memory.manager import MemoryManager  # noqa: F401\nfrom acad_gpt.memory.memory import Memory  # noqa: F401\n"]}
{"filename": "acad_gpt/memory/memory.py", "chunked_list": ["\"\"\"\nContains a memory dataclass.\n\"\"\"\nfrom pydantic import BaseModel\n\n\nclass Memory(BaseModel):\n    \"\"\"\n    A memory dataclass.\n    \"\"\"\n\n    conversation_id: str\n    \"\"\"ID of the conversation.\"\"\"", ""]}
