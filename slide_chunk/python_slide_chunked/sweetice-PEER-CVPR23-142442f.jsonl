{"filename": "bullet_mujoco/peer.py", "chunked_list": ["import copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Implementation of PEER, based on (TD3)\n# TD3 Paper: https://arxiv.org/abs/1802.09477", "# Implementation of PEER, based on (TD3)\n# TD3 Paper: https://arxiv.org/abs/1802.09477\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = F.relu(self.l1(state))\n        a = F.relu(self.l2(a))\n        return self.max_action * torch.tanh(self.l3(a))", "\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        # Q1 architecture\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, 1)\n\n        # Q2 architecture\n        self.l4 = nn.Linear(state_dim + action_dim, 256)\n        self.l5 = nn.Linear(256, 256)\n        self.l6 = nn.Linear(256, 1)\n\n    def forward(self, state, action, feature=False):\n        sa = torch.cat([state, action], 1)\n\n        q1 = F.relu(self.l1(sa))\n        f1 = F.relu(self.l2(q1))\n        q1 = self.l3(f1)\n\n        q2 = F.relu(self.l4(sa))\n        f2 = F.relu(self.l5(q2))\n        q2 = self.l6(f2)\n        if feature:\n            return q1, f1, q2, f2\n        else:\n            return q1, q2\n\n\n    def Q1(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q1 = F.relu(self.l1(sa))\n        q1 = F.relu(self.l2(q1))\n        q1 = self.l3(q1)\n        return q1\n\n    def feature(self, state, action):\n        sa = torch.cat([state, action], 1)\n        q1 = F.relu(self.l1(sa))\n        q1 = F.relu(self.l2(q1))\n\n        return q1", "\n\nclass PEER(object):\n    def __init__(\n            self,\n            state_dim,\n            action_dim,\n            max_action,\n            discount=0.99,\n            tau=0.005,\n            policy_noise=0.2,\n            noise_clip=0.5,\n            policy_freq=2,\n\n            feature_coef = 1e-3\n    ):\n        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n        self.actor_target = copy.deepcopy(self.actor)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n\n        self.critic = Critic(state_dim, action_dim).to(device)\n        self.critic_target = copy.deepcopy(self.critic)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n\n        self.max_action = max_action\n        self.discount = discount\n        self.tau = tau\n        self.policy_noise = policy_noise\n        self.noise_clip = noise_clip\n        self.policy_freq = policy_freq\n        self.total_it = 0\n        self.beta = feature_coef # beta in paper\n\n    def select_action(self, state):\n        with torch.no_grad():\n            state = torch.from_numpy(state).reshape(1, -1).to(device)\n            return self.actor(state).cpu().numpy().flatten()\n\n    def train(self, replay_buffer, batch_size=100):\n        self.total_it += 1\n\n        # Sample replay buffer\n        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n\n        with torch.no_grad():\n            # Select action according to policy and add clipped noise\n            noise = (\n                    torch.randn_like(action) * self.policy_noise\n            ).clamp(-self.noise_clip, self.noise_clip)\n\n            next_action = (\n                    self.actor_target(next_state) + noise\n            ).clamp(-self.max_action, self.max_action)\n\n            # Compute the target Q value\n            target_Q1, target_feature1, target_Q2, target_feature2 = self.critic_target(next_state, next_action, feature=True)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = reward + not_done * self.discount * target_Q\n\n        # Get current Q estimates\n        current_Q1, current_feature1, current_Q2, current_feature2 = self.critic(state, action, feature=True)\n\n        # Compute critic loss\n        critic_loss1, critic_loss2 = F.mse_loss(current_Q1, target_Q), F.mse_loss(current_Q2, target_Q)\n        peer_loss1 = torch.einsum('ij,ij->i', [current_feature1, target_feature1]).mean()\n        peer_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\n        total_peer_loss = (peer_loss1 + peer_loss2)* self.beta\n        critic_loss = critic_loss1 + critic_loss2\n        Q_function_loss = critic_loss + total_peer_loss\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        Q_function_loss.backward()\n        self.critic_optimizer.step()\n\n        # Delayed policy updates\n        if self.total_it % self.policy_freq == 0:\n\n            # Compute actor loss\n            actor_loss = - self.critic.Q1(state, self.actor(state)).mean()\n\n            # Optimize the actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Update the frozen target models\n            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)", ""]}
{"filename": "bullet_mujoco/main.py", "chunked_list": ["import numpy as np\nimport torch\nimport gym\nimport argparse\nimport os\nimport mujoco\n\nimport utils\nimport time\nimport random", "import time\nimport random\nimport pybullet_envs\n\n\nif __name__ == \"__main__\":\n\n    begin_time = time.asctime(time.localtime(time.time()))\n    start = time.time()\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--policy\", default=\"PEER\")\n    parser.add_argument(\"--env\", default='HopperBulletEnv-v0')\n    parser.add_argument(\"--seed\", default=0, type=int)\n    parser.add_argument(\"--start_timesteps\", default=25e3, type=int)  #\n    # default 25e3, for test 1e3\n    parser.add_argument(\"--eval_freq\", default=5e3, type=int)\n    parser.add_argument(\"--max_timesteps\", default=1e6, type=int)\n    parser.add_argument(\"--expl_noise\", default=0.1)\n    parser.add_argument(\"--batch_size\", default=256, type=int)\n    parser.add_argument(\"--discount\", default=0.99)\n    parser.add_argument(\"--tau\", default=0.005)\n    parser.add_argument(\"--policy_noise\", default=0.2)\n    parser.add_argument(\"--noise_clip\", default=0.5)\n    parser.add_argument(\"--policy_freq\", default=2, type=int)\n    parser.add_argument(\"--save_model\", default=True)\n    parser.add_argument(\"--load_model\", default=\"\")\n    parser.add_argument(\"--eval_state_value\", default=True)\n    parser.add_argument(\"--gpu_idx\", default=0, type=int)\n    parser.add_argument(\"--gpu_num\", default=0, type=int)\n    parser.add_argument(\"--feature_coef\", default=5e-4, type=float) # PRO coef\n\n    args = parser.parse_args()\n\n    num_thread = args.gpu_num\n    gpu_idx = args.gpu_idx\n    file_name = f\"{args.policy}_{args.env}_{args.seed}\"\n    print(\"---------------------------------------\")\n    print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n    print(\"---------------------------------------\")\n\n    env = gym.make(args.env)\n\n    # Set seeds\n    random.seed(args.seed)\n    env.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.seed(args.seed)\n\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    max_action = float(env.action_space.high[0])\n\n    kwargs = {\n        \"state_dim\": state_dim,\n        \"action_dim\": action_dim,\n        \"max_action\": max_action,\n        \"discount\": args.discount,\n        \"tau\": args.tau,\n    }\n\n    if args.policy == \"METD3\":\n        # Target policy smoothing is scaled wrt the action\n        import metd3\n        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n        kwargs[\"policy_freq\"] = args.policy_freq\n        policy = metd3.METD3(**kwargs)\n\n    elif args.policy == \"PEER\":\n        # Target policy smoothing is scaled wrt the action\n        import peer\n        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n        kwargs[\"policy_freq\"] = args.policy_freq\n        kwargs[\"feature_coef\"] = args.feature_coef\n        policy = peer.PEER(**kwargs)\n\n    else:\n        raise NotImplementedError(\"No policy named\", args.policy)\n\n\n    replay_buffer = utils.ReplayBufferMuJoCo(state_dim, action_dim)\n\n\n    state, done = env.reset(), False\n    episode_reward = 0\n    episode_timesteps = 0\n    episode_num = 0\n\n    for t in range(int(args.max_timesteps)):\n\n        episode_timesteps += 1\n\n        # Select action randomly or according to policy\n        if t < args.start_timesteps:\n            action = env.action_space.sample()\n        else:\n            action = (\n                    policy.select_action(np.array(state, dtype='float32'))\n                    + np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n            ).clip(-max_action, max_action)\n\n        # Perform action\n        next_state, reward, done, _ = env.step(action)\n        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n\n        # Store data in replay buffer\n        replay_buffer.add(state, action, next_state, reward, done_bool)\n\n        state = next_state\n        episode_reward += reward\n\n        # Train agent after collecting sufficient data\n        if t >= args.start_timesteps:\n            policy.train(replay_buffer, args.batch_size)\n\n        if done:\n            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n            print(\n                f\"Algo: {args.policy} Env: {args.env} Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n\n            # Reset environment\n            state, done = env.reset(), False\n            episode_reward = 0\n            episode_timesteps = 0\n            episode_num += 1"]}
{"filename": "bullet_mujoco/utils.py", "chunked_list": ["import torch\nimport numpy as np\nimport torch.nn as nn\nimport gym\nimport os\nfrom collections import deque\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport time\nfrom skimage.util.shape import view_as_windows", "import time\nfrom skimage.util.shape import view_as_windows\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "\n\ndef soft_update_params(net, target_net, tau):\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(\n            tau * param.data + (1 - tau) * target_param.data\n        )\n\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)", "\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef module_hash(module):\n    result = 0\n    for tensor in module.state_dict().values():\n        result += tensor.sum().item()\n    return result", "\ndef module_hash(module):\n    result = 0\n    for tensor in module.state_dict().values():\n        result += tensor.sum().item()\n    return result\n\n\ndef make_dir(dir_path):\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path", "def make_dir(dir_path):\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n\n\ndef preprocess_obs(obs, bits=5):\n    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n    bins = 2**bits\n    assert obs.dtype == torch.float32\n    if bits < 8:\n        obs = torch.floor(obs / 2**(8 - bits))\n    obs = obs / bins\n    obs = obs + torch.rand_like(obs) / bins\n    obs = obs - 0.5\n    return obs", "def preprocess_obs(obs, bits=5):\n    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n    bins = 2**bits\n    assert obs.dtype == torch.float32\n    if bits < 8:\n        obs = torch.floor(obs / 2**(8 - bits))\n    obs = obs / bins\n    obs = obs + torch.rand_like(obs) / bins\n    obs = obs - 0.5\n    return obs", "\n\nclass ReplayBuffer(Dataset):\n    \"\"\"Buffer to store environment transitions.\"\"\"\n    def __init__(self, obs_shape, action_shape, capacity, batch_size, device,image_size=84,transform=None):\n        self.capacity = capacity\n        self.batch_size = batch_size\n        self.device = device\n        self.image_size = image_size\n        self.transform = transform\n        # the proprioceptive obs is stored as float32, pixels obs as uint8\n        obs_dtype = np.float32 if len(obs_shape) == 1 else np.uint8\n        \n        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.last_save = 0\n        self.full = False\n\n\n    def add(self, obs, action, reward, next_obs, done):\n       \n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample_proprio(self):\n        \n        idxs = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=self.batch_size\n        )\n        \n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        next_obses = torch.as_tensor(\n            next_obses, device=self.device\n        ).float()\n        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n        return obses, actions, rewards, next_obses, not_dones\n\n    def sample_cpc(self):\n\n        start = time.time()\n        idxs = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=self.batch_size\n        )\n      \n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n        pos = obses.copy()\n\n        obses = random_crop(obses, self.image_size)\n        next_obses = random_crop(next_obses, self.image_size)\n        pos = random_crop(pos, self.image_size)\n    \n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(\n            next_obses, device=self.device\n        ).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\n        pos = torch.as_tensor(pos, device=self.device).float()\n        cpc_kwargs = dict(obs_anchor=obses, obs_pos=pos,\n                          time_anchor=None, time_pos=None)\n\n        return obses, actions, rewards, next_obses, not_dones, cpc_kwargs\n\n    def save(self, save_dir):\n        if self.idx == self.last_save:\n            return\n        path = os.path.join(save_dir, '%d_%d.pt' % (self.last_save, self.idx))\n        payload = [\n            self.obses[self.last_save:self.idx],\n            self.next_obses[self.last_save:self.idx],\n            self.actions[self.last_save:self.idx],\n            self.rewards[self.last_save:self.idx],\n            self.not_dones[self.last_save:self.idx]\n        ]\n        self.last_save = self.idx\n        torch.save(payload, path)\n\n    def load(self, save_dir):\n        chunks = os.listdir(save_dir)\n        chucks = sorted(chunks, key=lambda x: int(x.split('_')[0]))\n        for chunk in chucks:\n            start, end = [int(x) for x in chunk.split('.')[0].split('_')]\n            path = os.path.join(save_dir, chunk)\n            payload = torch.load(path)\n            assert self.idx == start\n            self.obses[start:end] = payload[0]\n            self.next_obses[start:end] = payload[1]\n            self.actions[start:end] = payload[2]\n            self.rewards[start:end] = payload[3]\n            self.not_dones[start:end] = payload[4]\n            self.idx = end\n\n    def __getitem__(self, idx):\n        idx = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=1\n        )\n        idx = idx[0]\n        obs = self.obses[idx]\n        action = self.actions[idx]\n        reward = self.rewards[idx]\n        next_obs = self.next_obses[idx]\n        not_done = self.not_dones[idx]\n\n        if self.transform:\n            obs = self.transform(obs)\n            next_obs = self.transform(next_obs)\n\n        return obs, action, reward, next_obs, not_done\n\n    def __len__(self):\n        return self.capacity ", "\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self._k = k\n        self._frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=1,\n            shape=((shp[0] * k,) + shp[1:]),\n            dtype=env.observation_space.dtype\n        )\n        self._max_episode_steps = env._max_episode_steps\n\n    def reset(self):\n        obs = self.env.reset()\n        for _ in range(self._k):\n            self._frames.append(obs)\n        return self._get_obs()\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self._frames.append(obs)\n        return self._get_obs(), reward, done, info\n\n    def _get_obs(self):\n        assert len(self._frames) == self._k\n        return np.concatenate(list(self._frames), axis=0)", "\n\ndef random_crop(imgs, output_size):\n    \"\"\"\n    Vectorized way to do random crop using sliding windows\n    and picking out random ones\n\n    args:\n        imgs, batch images with shape (B,C,H,W)\n    \"\"\"\n    # batch size\n    n = imgs.shape[0]\n    img_size = imgs.shape[-1]\n    crop_max = img_size - output_size\n    imgs = np.transpose(imgs, (0, 2, 3, 1))\n    w1 = np.random.randint(0, crop_max, n)\n    h1 = np.random.randint(0, crop_max, n)\n    # creates all sliding windows combinations of size (output_size)\n    windows = view_as_windows(\n        imgs, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n    # selects a random window for each batch element\n    cropped_imgs = windows[np.arange(n), w1, h1]\n    return cropped_imgs", "\ndef center_crop_image(image, output_size):\n    h, w = image.shape[1:]\n    new_h, new_w = output_size, output_size\n\n    top = (h - new_h)//2\n    left = (w - new_w)//2\n\n    image = image[:, top:top + new_h, left:left + new_w]\n    return image", "\nclass ReplayBufferMuJoCo(object):\n    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n        self.max_size = max_size\n        self.ptr = 0\n        self.size = 0\n\n        self.state = np.zeros((max_size, state_dim), dtype='float32')\n        self.action = np.zeros((max_size, action_dim), dtype='float32')\n        self.next_state = np.zeros((max_size, state_dim), dtype='float32')\n        self.reward = np.zeros((max_size, 1), dtype='float32')\n        self.not_done = np.zeros((max_size, 1), dtype='float32')\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, state, action, next_state, reward, done):\n        self.state[self.ptr] = state\n        self.action[self.ptr] = action\n        self.next_state[self.ptr] = next_state\n        self.reward[self.ptr] = reward\n        self.not_done[self.ptr] = 1. - done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, self.size, size=batch_size)\n\n        return (\n            torch.from_numpy(self.state[ind]).to(self.device),\n            torch.from_numpy(self.action[ind]).to(self.device),\n            torch.from_numpy(self.next_state[ind]).to(self.device),\n            torch.from_numpy(self.reward[ind]).to(self.device),\n            torch.from_numpy(self.not_done[ind]).to(self.device)\n        )", "\n"]}
{"filename": "dmc/PEER-DrQ/peer.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport utils\nimport os\n\nclass Encoder(nn.Module):\n    \"\"\"Convolutional encoder for image-based observations.\"\"\"\n    def __init__(self, obs_shape, feature_dim):\n        super().__init__()\n\n        assert len(obs_shape) == 3\n        self.num_layers = 4\n        self.num_filters = 32\n        self.output_dim = 35\n        self.output_logits = False\n        self.feature_dim = feature_dim\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(obs_shape[0], self.num_filters, 3, stride=2),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1)\n        ])\n\n        self.head = nn.Sequential(\n            nn.Linear(self.num_filters * 35 * 35, self.feature_dim),\n            nn.LayerNorm(self.feature_dim))\n\n        self.outputs = dict()\n\n    def forward_conv(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        conv = torch.relu(self.convs[0](obs))\n        self.outputs['conv1'] = conv\n\n        for i in range(1, self.num_layers):\n            conv = torch.relu(self.convs[i](conv))\n            self.outputs['conv%s' % (i + 1)] = conv\n\n        h = conv.view(conv.size(0), -1)\n        return h\n\n    def forward(self, obs, detach=False):\n        h = self.forward_conv(obs)\n\n        if detach:\n            h = h.detach()\n\n        out = self.head(h)\n        if not self.output_logits:\n            out = torch.tanh(out)\n\n        self.outputs['out'] = out\n\n        return out\n\n    def copy_conv_weights_from(self, source):\n        \"\"\"Tie convolutional layers\"\"\"\n        for i in range(self.num_layers):\n            utils.tie_weights(src=source.convs[i], trg=self.convs[i])", "class Encoder(nn.Module):\n    \"\"\"Convolutional encoder for image-based observations.\"\"\"\n    def __init__(self, obs_shape, feature_dim):\n        super().__init__()\n\n        assert len(obs_shape) == 3\n        self.num_layers = 4\n        self.num_filters = 32\n        self.output_dim = 35\n        self.output_logits = False\n        self.feature_dim = feature_dim\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(obs_shape[0], self.num_filters, 3, stride=2),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1)\n        ])\n\n        self.head = nn.Sequential(\n            nn.Linear(self.num_filters * 35 * 35, self.feature_dim),\n            nn.LayerNorm(self.feature_dim))\n\n        self.outputs = dict()\n\n    def forward_conv(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        conv = torch.relu(self.convs[0](obs))\n        self.outputs['conv1'] = conv\n\n        for i in range(1, self.num_layers):\n            conv = torch.relu(self.convs[i](conv))\n            self.outputs['conv%s' % (i + 1)] = conv\n\n        h = conv.view(conv.size(0), -1)\n        return h\n\n    def forward(self, obs, detach=False):\n        h = self.forward_conv(obs)\n\n        if detach:\n            h = h.detach()\n\n        out = self.head(h)\n        if not self.output_logits:\n            out = torch.tanh(out)\n\n        self.outputs['out'] = out\n\n        return out\n\n    def copy_conv_weights_from(self, source):\n        \"\"\"Tie convolutional layers\"\"\"\n        for i in range(self.num_layers):\n            utils.tie_weights(src=source.convs[i], trg=self.convs[i])", "\n\nclass Actor(nn.Module):\n    \"\"\"torch.distributions implementation of an diagonal Gaussian policy.\"\"\"\n    def __init__(self, action_shape, hidden_dim, hidden_depth,\n                 log_std_bounds, obs_shape, feature_dim=50):\n        super().__init__()\n\n        self.encoder = Encoder(obs_shape=obs_shape, feature_dim=feature_dim)\n\n        self.log_std_bounds = log_std_bounds\n        self.trunk = utils.mlp(self.encoder.feature_dim, hidden_dim,\n                               2 * action_shape[0], hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, detach_encoder=False):\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\n        # constrain log_std inside [log_std_min, log_std_max]\n        log_std = torch.tanh(log_std)\n        log_std_min, log_std_max = self.log_std_bounds\n        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std +\n                                                                     1)\n        std = log_std.exp()\n\n        self.outputs['mu'] = mu\n        self.outputs['std'] = std\n\n        dist = utils.SquashedNormal(mu, std)\n        return dist", "\n\n\nclass Critic(nn.Module):\n    \"\"\"Critic network, employes double Q-learning.\"\"\"\n    def __init__(self, action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim=50):\n        super().__init__()\n\n        # self.encoder = hydra.utils.instantiate(encoder_cfg)\n        self.encoder = Encoder(obs_shape=obs_shape, feature_dim=feature_dim)\n\n        self.repr1 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n                            hidden_dim, hidden_dim, hidden_depth-1)\n        self.repr2 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n                            hidden_dim, hidden_dim, hidden_depth-1)\n\n        self.Q1=torch.nn.Linear(hidden_dim, 1)\n        self.Q2=torch.nn.Linear(hidden_dim, 1)\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, action, detach_encoder=False):\n        assert obs.size(0) == action.size(0)\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        obs_action = torch.cat([obs, action], dim=-1)\n        repr1, repr2 = self.repr1(obs_action), self.repr2(obs_action) # output_dim=32\n        q1 = self.Q1(repr1)  # input_dim=1024\n        q2 = self.Q2(repr2)\n\n        self.outputs['q1'] = q1\n        self.outputs['q2'] = q2\n\n        return repr1, repr2, q1, q2", "\n\nclass PEER(object):\n    \"\"\"Data regularized Q: actor-critic method for learning from pixels.\"\"\"\n    def __init__(self, action_shape, action_range, device,\n                discount,\n                 init_temperature, lr, actor_update_frequency, critic_tau,\n                 critic_target_update_frequency, batch_size, env_name, seed,\n\n                 # critic setting\n                 obs_shape,\n                 feature_dim,\n                 hidden_dim=1024,\n                 hidden_depth=2,\n\n                 # actor setting\n                 log_std_bounds=[-10, 2],\n\n\n                 # PEER\n                 beta = None\n                 ):\n        assert beta is not None\n        self.action_range = action_range\n        self.device = \"cuda\"\n        self.discount = discount\n        self.critic_tau = critic_tau\n        self.actor_update_frequency = actor_update_frequency\n        self.critic_target_update_frequency = critic_target_update_frequency\n        self.batch_size = batch_size\n\n        self.actor = Actor(action_shape, hidden_dim, hidden_depth,\n                 log_std_bounds, obs_shape, feature_dim=50).to(self.device)\n\n        # self.critic = hydra.utils.instantiate(critic_cfg).to(self.device)\n        self.critic = Critic(action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim).to(self.device)\n\n        self.critic_target = Critic(action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        # tie conv layers between actor and critic\n        self.actor.encoder.copy_conv_weights_from(self.critic.encoder)\n\n        self.log_alpha = torch.tensor(np.log(init_temperature)).to(device)\n        self.log_alpha.requires_grad = True\n        # set target entropy to -|A|\n        self.target_entropy = -action_shape[0]\n\n        # optimizers\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=lr)\n        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)\n\n        # PEER loss\n        self.beta = beta\n\n        # self.count = 0\n        self.train()\n        self.critic_target.train()\n\n\n    def train(self, training=True):\n        self.training = training\n        self.actor.train(training)\n        self.critic.train(training)\n\n    @property\n    def alpha(self):\n        return self.log_alpha.exp()\n\n    def act(self, obs, sample=False):\n        with torch.no_grad():\n            obs = torch.FloatTensor(obs).to(self.device)\n            obs = obs.unsqueeze(0)\n            dist = self.actor(obs)\n            action = dist.sample() if sample else dist.mean\n            action = action.clamp(*self.action_range)\n            assert action.ndim == 2 and action.shape[0] == 1\n            return utils.to_np(action[0])\n\n    def update_critic(self, obs, obs_aug, action, reward, next_obs,\n                      next_obs_aug, not_done):\n        with torch.no_grad():\n            dist = self.actor(next_obs)\n            next_action = dist.rsample()\n            log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n            target_repr1, target_repr2, target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n            target_V = torch.min(target_Q1,\n                                 target_Q2) - self.alpha.detach() * log_prob\n            target_Q = reward + (not_done * self.discount * target_V)\n\n            dist_aug = self.actor(next_obs_aug)\n            next_action_aug = dist_aug.rsample()\n            log_prob_aug = dist_aug.log_prob(next_action_aug).sum(-1,\n                                                                  keepdim=True)\n            target_repr1_aug, target_repr2_aug, target_Q1, target_Q2 = self.critic_target(next_obs_aug,\n                                                      next_action_aug)\n            # We should get the target repr\n            target_V = torch.min(\n                target_Q1, target_Q2) - self.alpha.detach() * log_prob_aug\n            target_Q_aug = reward + (not_done * self.discount * target_V)\n\n            target_Q = (target_Q + target_Q_aug) / 2\n\n        # get current Q estimates\n        current_repr1, current_repr2, current_Q1, current_Q2 = self.critic(obs, action)\n\n        PEER_loss1 = torch.einsum('ij,ij->i', [current_repr1, target_repr1])\n        PEER_loss2 = torch.einsum('ij,ij->i', [current_repr2, target_repr2])\n        critic_loss1 = F.mse_loss(current_Q1, target_Q)\n        critic_loss2 = F.mse_loss(current_Q2, target_Q)\n        critic_loss_original = critic_loss1 + critic_loss2\n        PEER_loss1_beta = self.beta * PEER_loss1.mean()\n        PEER_loss2_beta = self.beta * PEER_loss2.mean()\n        critic_loss_before_aug = critic_loss_original + PEER_loss1_beta + PEER_loss2_beta\n        # critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n        #     current_Q2, target_Q) + self.beta * PEER_loss1.mean() + self.beta * PEER_loss2.mean()\n\n        repr_aug1, repr_aug2, Q1_aug, Q2_aug = self.critic(obs_aug, action)\n        PEER_aug1 = torch.einsum('ij,ij->i', [repr_aug1, target_repr1_aug])\n        PEER_aug2 = torch.einsum('ij,ij->i', [repr_aug2, target_repr2_aug])\n        PEER_aug1_beta = PEER_aug1.mean() * self.beta\n        PEER_aug2_beta = PEER_aug2.mean() * self.beta\n        aug_loss_without_peer = F.mse_loss(Q1_aug, target_Q) + F.mse_loss(\n            Q2_aug, target_Q)\n        critic_aug_loss = aug_loss_without_peer + PEER_aug1_beta + PEER_aug2_beta\n        total_critic_loss = critic_loss_before_aug + critic_aug_loss\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        total_critic_loss.backward()\n        self.critic_optimizer.step()\n\n\n    def update_actor_and_alpha(self, obs):\n        # detach conv filters, so we don't update them with the actor loss\n        dist = self.actor(obs, detach_encoder=True)\n        action = dist.rsample()\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n        # detach conv filters, so we don't update them with the actor loss\n        _, _, actor_Q1, actor_Q2 = self.critic(obs, action, detach_encoder=True)\n\n        actor_Q = torch.min(actor_Q1, actor_Q2)\n\n        actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n\n        # optimize the actor\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        self.log_alpha_optimizer.zero_grad()\n        alpha_loss = (self.alpha *\n                      (-log_prob - self.target_entropy).detach()).mean()\n\n        alpha_loss.backward()\n        self.log_alpha_optimizer.step()\n\n\n    def update(self, replay_buffer, step):\n        obs, action, reward, next_obs, not_done, obs_aug, next_obs_aug = replay_buffer.sample(\n            self.batch_size)\n\n        self.update_critic(obs, obs_aug, action, reward, next_obs,\n                           next_obs_aug, not_done)\n\n        if step % self.actor_update_frequency == 0:\n            self.update_actor_and_alpha(obs)\n\n        if step % self.critic_target_update_frequency == 0:\n            utils.soft_update_params(self.critic, self.critic_target,\n                                     self.critic_tau)", ""]}
{"filename": "dmc/PEER-DrQ/main.py", "chunked_list": ["import os\nimport time\nimport numpy as np\n\nimport dmc2gym\nfrom peer import PEER\nimport torch\nimport utils\nfrom replay_buffer import ReplayBuffer\nfrom utils import CFG", "from replay_buffer import ReplayBuffer\nfrom utils import CFG\nimport argparse\nfrom tqdm import trange\n\ntorch.backends.cudnn.benchmark = True\ntorch.set_num_threads(8)\n\ndef make_env(cfg, seed=None):\n    \"\"\"Helper function to create dm_control environment\"\"\"\n    if cfg.env == 'ball_in_cup_catch':\n        domain_name = 'ball_in_cup'\n        task_name = 'catch'\n    elif cfg.env == 'point_mass_easy':\n        domain_name = 'point_mass'\n        task_name = 'easy'\n    else:\n        domain_name = cfg.env.split('_')[0]\n        task_name = '_'.join(cfg.env.split('_')[1:])\n\n    # per dreamer: https://github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/wrappers.py#L26\n    camera_id = 2 if domain_name == 'quadruped' else 0\n    seed = cfg.seed if seed is None else seed\n    env = dmc2gym.make(domain_name=domain_name,\n                       task_name=task_name,\n                       seed=seed,\n                       visualize_reward=False,\n                       from_pixels=True,\n                       height=cfg.image_size,\n                       width=cfg.image_size,\n                       frame_skip=cfg.action_repeat,\n                       camera_id=camera_id)\n\n    env = utils.FrameStack(env, k=cfg.frame_stack)\n\n    env.seed(cfg.seed)\n    assert env.action_space.low.min() >= -1\n    assert env.action_space.high.max() <= 1\n\n    return env", "def make_env(cfg, seed=None):\n    \"\"\"Helper function to create dm_control environment\"\"\"\n    if cfg.env == 'ball_in_cup_catch':\n        domain_name = 'ball_in_cup'\n        task_name = 'catch'\n    elif cfg.env == 'point_mass_easy':\n        domain_name = 'point_mass'\n        task_name = 'easy'\n    else:\n        domain_name = cfg.env.split('_')[0]\n        task_name = '_'.join(cfg.env.split('_')[1:])\n\n    # per dreamer: https://github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/wrappers.py#L26\n    camera_id = 2 if domain_name == 'quadruped' else 0\n    seed = cfg.seed if seed is None else seed\n    env = dmc2gym.make(domain_name=domain_name,\n                       task_name=task_name,\n                       seed=seed,\n                       visualize_reward=False,\n                       from_pixels=True,\n                       height=cfg.image_size,\n                       width=cfg.image_size,\n                       frame_skip=cfg.action_repeat,\n                       camera_id=camera_id)\n\n    env = utils.FrameStack(env, k=cfg.frame_stack)\n\n    env.seed(cfg.seed)\n    assert env.action_space.low.min() >= -1\n    assert env.action_space.high.max() <= 1\n\n    return env", "\n\nclass Workspace(object):\n    def __init__(self, cfg):\n        self.work_dir = os.getcwd()\n        print(f'workspace: {self.work_dir}')\n\n        self.cfg = cfg\n        utils.set_seed_everywhere(cfg.seed)\n        self.device = torch.device(cfg.device)\n        self.env = make_env(cfg)\n\n        if self.cfg.algo == \"PEER\":\n            print(\"Agent: \", self.cfg.algo)\n            self.agent = PEER(\n                action_shape = self.env.action_space.shape,\n                action_range = [\n                float(self.env.action_space.low.min()),\n                float(self.env.action_space.high.max())\n            ],\n                device=\"cuda\",\n                discount=self.cfg.discount,\n                init_temperature = self.cfg.init_temperature,\n                lr=self.cfg.lr,\n                actor_update_frequency=self.cfg.actor_update_frequency,\n                critic_tau=self.cfg.critic_tau,\n                critic_target_update_frequency=self.cfg.critic_target_update_frequency,\n                batch_size=self.cfg.batch_size,\n                env_name=self.cfg.env_name,\n                seed=self.cfg.seed,\n                obs_shape=self.env.observation_space.shape,\n                feature_dim=self.cfg.feature_dim,\n                hidden_dim=self.cfg.hidden_dim,\n                hidden_depth=self.cfg.hidden_depth,\n\n                # actor setting\n                log_std_bounds=[-10, 2],\n                beta=args.beta\n            )\n        else:\n            raise  NotImplementedError(\"Unknown Agent\")\n\n        self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n                                          self.env.action_space.shape,\n                                          cfg.replay_buffer_capacity,\n                                          self.cfg.image_pad, self.device)\n        self.step = 0\n\n\n    def run(self):\n        episode, episode_reward, episode_step, done = 0, 0, 1, True\n        start_time = time.time()\n        for place_holder in trange(self.cfg.num_train_steps):\n            if done:\n                print(\n                    f\"Algorithm: {self.cfg.algo}, Env: {self.cfg.env_name}, Seed: {self.cfg.seed}, Num Step: {self.step}, Episode Reward: {episode_reward}\")\n                # evaluate agent periodically\n\n                obs = self.env.reset()\n                done = False\n                episode_reward = 0\n                episode_step = 0\n                episode += 1\n\n            # sample action for data collection\n            if self.step < self.cfg.num_seed_steps:\n                action = self.env.action_space.sample()\n            else:\n                with utils.eval_mode(self.agent):\n                    action = self.agent.act(obs, sample=True)\n\n            # run training update\n            if self.step >= self.cfg.num_seed_steps:\n                for _ in range(self.cfg.num_train_iters):\n                    self.agent.update(self.replay_buffer, self.step)\n\n\n            next_obs, reward, done, info = self.env.step(action)\n\n            # allow infinite bootstrap\n            done = float(done)\n            done_no_max = 0 if episode_step + 1 == self.env._max_episode_steps else done\n            episode_reward += reward\n\n            self.replay_buffer.add(obs, action, reward, next_obs, done,\n                                   done_no_max)\n\n            obs = next_obs\n            episode_step += 1\n            self.step += 1", "\n\ndef main(cfg):\n    from main import Workspace as W\n    workspace = W(cfg)\n    workspace.run()\n\nparser = argparse.ArgumentParser(\"DRQ-PEER\")\nparser.add_argument('--domain_name', default=\"ball_in_cup\")\nparser.add_argument('--task_name', default=\"catch\")", "parser.add_argument('--domain_name', default=\"ball_in_cup\")\nparser.add_argument('--task_name', default=\"catch\")\nparser.add_argument('--seed', default=1, type=int)\nparser.add_argument('--gpu_idx', default=0, type=int)\nparser.add_argument('--algo', default=\"PEER\", type=str)\nparser.add_argument('--beta', default=5e-3, type=float)\n# parser.add_argument('--debug', action=, type=bool)\nargs = parser.parse_args()\nargs.debug = False\n", "args.debug = False\n\n\ncfg=CFG(domain=args.domain_name,\n        task=args.task_name,\n        seed=args.seed,\n        debug=args.debug,\n        algo=args.algo,\n        beta=args.beta\n        )", "        beta=args.beta\n        )\n\nmain(cfg)"]}
{"filename": "dmc/PEER-DrQ/utils.py", "chunked_list": ["import math\nimport os\nimport random\nfrom collections import deque\n\nimport numpy as np\nimport scipy.linalg as sp_la\n\nimport gym\nimport torch", "import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skimage.util.shape import view_as_windows\nfrom torch import distributions as pyd\nimport pickle\nimport pandas as pd\n\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "\n\ndef soft_update_params(net, target_net, tau):\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(tau * param.data +\n                                (1 - tau) * target_param.data)\n\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)", "def set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef make_dir(*path_parts):\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path", "def make_dir(*path_parts):\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n\n\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias", "\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias\n\n\ndef weight_init(m):\n    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal_(m.weight.data)\n        if hasattr(m.bias, 'data'):\n            m.bias.data.fill_(0.0)\n    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.orthogonal_(m.weight.data, gain)\n        if hasattr(m.bias, 'data'):\n            m.bias.data.fill_(0.0)", "\n\ndef mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n    if hidden_depth == 0:\n        mods = [nn.Linear(input_dim, output_dim)]\n    else:\n        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n        for i in range(hidden_depth - 1):\n            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n        mods.append(nn.Linear(hidden_dim, output_dim))\n    if output_mod is not None:\n        mods.append(output_mod)\n    trunk = nn.Sequential(*mods)\n    return trunk", "\n\ndef to_np(t):\n    if t is None:\n        return None\n    elif t.nelement() == 0:\n        return np.array([])\n    else:\n        return t.cpu().detach().numpy()\n", "\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self._k = k\n        self._frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=1,\n            shape=((shp[0] * k,) + shp[1:]),\n            dtype=env.observation_space.dtype)\n        self._max_episode_steps = env._max_episode_steps\n\n    def reset(self):\n        obs = self.env.reset()\n        for _ in range(self._k):\n            self._frames.append(obs)\n        return self._get_obs()\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self._frames.append(obs)\n        return self._get_obs(), reward, done, info\n\n    def _get_obs(self):\n        assert len(self._frames) == self._k\n        return np.concatenate(list(self._frames), axis=0)", "\n\nclass TanhTransform(pyd.transforms.Transform):\n    domain = pyd.constraints.real\n    codomain = pyd.constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    def __init__(self, cache_size=1):\n        super().__init__(cache_size=cache_size)\n\n    @staticmethod\n    def atanh(x):\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        return x.tanh()\n\n    def _inverse(self, y):\n        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n        # one should use `cache_size=1` instead\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2. * (math.log(2.) - x - F.softplus(-2. * x))", "\n\nclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n    def __init__(self, loc, scale):\n        self.loc = loc\n        self.scale = scale\n\n        self.base_dist = pyd.Normal(loc, scale)\n        transforms = [TanhTransform()]\n        super().__init__(self.base_dist, transforms)\n\n    @property\n    def mean(self):\n        mu = self.loc\n        for tr in self.transforms:\n            mu = tr(mu)\n        return mu", "\nclass CFG():\n    env = \"cartpole_swingup\"\n    # IMPORTANT= if action_repeat is used the effective number of env steps needs to be\n    # multiplied by action_repeat in the result graphs.\n    # This is a common practice for a fair comparison.\n    # See the 2nd paragraph in Appendix C of SLAC= https=//arxiv.org/pdf/1907.00953.pdf\n    # See Dreamer TF2's implementation= https=//github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/dreamer.py#L340\n    action_repeat = 4\n    # train\n    num_train_steps = 110000\n    num_train_iters = 1\n    num_seed_steps = 1000 #\n    replay_buffer_capacity = 100000 # \u221a\n    seed = 1\n    # eval\n    eval_frequency = 10000\n    num_eval_episodes = 10\n    # misc\n    log_frequency_step = 10000\n    log_save_tb = False\n    save_video = False\n    device = \"cuda\"\n    # observation\n    image_size = 84\n    image_pad = 4\n    frame_stack = 3\n    # global params\n    lr = 1e-3 # \u221a\n    # IMPORTANT= please use a batch size of 512 to reproduce the results in the paper. Hovewer with a smaller batch size it still works well.\n    batch_size = 512\n\n    obs_shape = 128\n    action_shape = 128\n    action_range = 128\n    encoder_cfg = None\n    critic_cfg = None\n    actor_cfg = None\n    discount = 0.99 # \u221a\n    init_temperature = 0.1 # \u221a\n    actor_update_frequency = 2 # \u221a\n    critic_tau = 0.01 # \u221a\n    critic_target_update_frequency = 2 # \u221a\n\n    hidden_dim = 1024 # \u221a\n    hidden_depth = 2 # \u221a\n\n    log_std_bounds = [-10, 2] #\n    feature_dim = 50 # \u221a\n    def __init__(self, domain=None, task=None, seed=None, debug=False, algo=None, beta=None):\n        self.domain = domain\n        self.task = task\n        self.env_name = f\"{self.domain}-{self.task}\"\n        self.seed = seed\n        self.algo = algo\n        self.beta = beta\n\n        if debug:\n            self.num_seed_steps = 2\n            self.batch_size = 2\n            self.num_eval_episodes = 2\n            self.eval_frequency = 2000\n            self.logger_interval = 500"]}
{"filename": "dmc/PEER-DrQ/replay_buffer.py", "chunked_list": ["import numpy as np\n\nimport kornia\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport utils\n\n\nclass ReplayBuffer(object):\n    \"\"\"Buffer to store environment transitions.\"\"\"\n    def __init__(self, obs_shape, action_shape, capacity, image_pad, device):\n        self.capacity = capacity\n        self.device = device\n\n        self.aug_trans = nn.Sequential(\n            nn.ReplicationPad2d(image_pad),\n            kornia.augmentation.RandomCrop((obs_shape[-1], obs_shape[-1])))\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones_no_max = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done, done_no_max):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n        np.copyto(self.not_dones_no_max[self.idx], not done_no_max)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample(self, batch_size):\n        idxs = np.random.randint(0,\n                                 self.capacity if self.full else self.idx,\n                                 size=batch_size)\n\n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n        obses_aug = obses.copy()\n        next_obses_aug = next_obses.copy()\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n        obses_aug = torch.as_tensor(obses_aug, device=self.device).float()\n        next_obses_aug = torch.as_tensor(next_obses_aug,\n                                         device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        not_dones_no_max = torch.as_tensor(self.not_dones_no_max[idxs],\n                                           device=self.device)\n\n        obses = self.aug_trans(obses)\n        next_obses = self.aug_trans(next_obses)\n\n        obses_aug = self.aug_trans(obses_aug)\n        next_obses_aug = self.aug_trans(next_obses_aug)\n\n        return obses, actions, rewards, next_obses, not_dones_no_max, obses_aug, next_obses_aug", "\nclass ReplayBuffer(object):\n    \"\"\"Buffer to store environment transitions.\"\"\"\n    def __init__(self, obs_shape, action_shape, capacity, image_pad, device):\n        self.capacity = capacity\n        self.device = device\n\n        self.aug_trans = nn.Sequential(\n            nn.ReplicationPad2d(image_pad),\n            kornia.augmentation.RandomCrop((obs_shape[-1], obs_shape[-1])))\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones_no_max = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done, done_no_max):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n        np.copyto(self.not_dones_no_max[self.idx], not done_no_max)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample(self, batch_size):\n        idxs = np.random.randint(0,\n                                 self.capacity if self.full else self.idx,\n                                 size=batch_size)\n\n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n        obses_aug = obses.copy()\n        next_obses_aug = next_obses.copy()\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n        obses_aug = torch.as_tensor(obses_aug, device=self.device).float()\n        next_obses_aug = torch.as_tensor(next_obses_aug,\n                                         device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        not_dones_no_max = torch.as_tensor(self.not_dones_no_max[idxs],\n                                           device=self.device)\n\n        obses = self.aug_trans(obses)\n        next_obses = self.aug_trans(next_obses)\n\n        obses_aug = self.aug_trans(obses_aug)\n        next_obses_aug = self.aug_trans(next_obses_aug)\n\n        return obses, actions, rewards, next_obses, not_dones_no_max, obses_aug, next_obses_aug", ""]}
{"filename": "dmc/PEER-CURL/peer.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nimport math\nimport os\n\nimport utils\nfrom encoder import make_encoder", "import utils\nfrom encoder import make_encoder\n\n\n# Implementation of peer, based on (CURL)\n# CURL code: https://github.com/MishaLaskin/curl\n\ndef gaussian_logprob(noise, log_std):\n    \"\"\"Compute Gaussian log probability.\"\"\"\n    residual = (-0.5 * noise.pow(2) - log_std).sum(-1, keepdim=True)\n    return residual - 0.5 * np.log(2 * np.pi) * noise.size(-1)", "\n\ndef squash(mu, pi, log_pi):\n    \"\"\"Apply squashing function.\n    See appendix C from https://arxiv.org/pdf/1812.05905.pdf.\n    \"\"\"\n    mu = torch.tanh(mu)\n    if pi is not None:\n        pi = torch.tanh(pi)\n    if log_pi is not None:\n        log_pi -= torch.log(F.relu(1 - pi.pow(2)) + 1e-6).sum(-1, keepdim=True)\n    return mu, pi, log_pi", "\n\ndef weight_init(m):\n    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal_(m.weight.data)\n        m.bias.data.fill_(0.0)\n    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        # delta-orthogonal init from https://arxiv.org/pdf/1806.05393.pdf\n        assert m.weight.size(2) == m.weight.size(3)\n        m.weight.data.fill_(0.0)\n        m.bias.data.fill_(0.0)\n        mid = m.weight.size(2) // 2\n        gain = nn.init.calculate_gain('relu')\n        nn.init.orthogonal_(m.weight.data[:, :, mid, mid], gain)", "\n\nclass Actor(nn.Module):\n    \"\"\"MLP actor network.\"\"\"\n\n    def __init__(\n            self, obs_shape, action_shape, hidden_dim, encoder_type,\n            encoder_feature_dim, log_std_min, log_std_max, num_layers, num_filters\n    ):\n        super().__init__()\n\n        self.encoder = make_encoder(\n            encoder_type, obs_shape, encoder_feature_dim, num_layers,\n            num_filters, output_logits=True\n        )\n\n        self.log_std_min = log_std_min\n        self.log_std_max = log_std_max\n\n        self.trunk = nn.Sequential(\n            nn.Linear(self.encoder.feature_dim, hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, 2 * action_shape[0])\n        )\n\n        self.outputs = dict()\n        self.apply(weight_init)\n\n    def forward(\n            self, obs, compute_pi=True, compute_log_pi=True, detach_encoder=False\n    ):\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\n        # constrain log_std inside [log_std_min, log_std_max]\n        log_std = torch.tanh(log_std)\n        log_std = self.log_std_min + 0.5 * (\n                self.log_std_max - self.log_std_min\n        ) * (log_std + 1)\n\n        self.outputs['mu'] = mu\n        self.outputs['std'] = log_std.exp()\n\n        if compute_pi:\n            std = log_std.exp()\n            noise = torch.randn_like(mu)\n            pi = mu + noise * std\n        else:\n            pi = None\n            entropy = None\n\n        if compute_log_pi:\n            log_pi = gaussian_logprob(noise, log_std)\n        else:\n            log_pi = None\n\n        mu, pi, log_pi = squash(mu, pi, log_pi)\n\n        return mu, pi, log_pi, log_std", "\n\nclass QFunction(nn.Module):\n    \"\"\"MLP for q-function.\"\"\"\n\n    def __init__(self, obs_dim, action_dim, hidden_dim):\n        super().__init__()\n\n        self.trunk = nn.Sequential(\n            nn.Linear(obs_dim + action_dim, hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n        )\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, obs, action):\n        assert obs.size(0) == action.size(0)\n\n        obs_action = torch.cat([obs, action], dim=1)\n        feature = self.trunk(obs_action)\n\n        Q = self.fc(feature)\n\n        return Q, feature", "\n\nclass Critic(nn.Module):\n    \"\"\"Critic network, employes two q-functions.\"\"\"\n\n    def __init__(\n            self, obs_shape, action_shape, hidden_dim, encoder_type,\n            encoder_feature_dim, num_layers, num_filters\n    ):\n        super().__init__()\n\n        self.encoder = make_encoder(\n            encoder_type, obs_shape, encoder_feature_dim, num_layers,\n            num_filters, output_logits=True\n        )\n\n        self.Q1 = QFunction(\n            self.encoder.feature_dim, action_shape[0], hidden_dim\n        )\n        self.Q2 = QFunction(\n            self.encoder.feature_dim, action_shape[0], hidden_dim\n        )\n\n        self.outputs = dict()\n        self.apply(weight_init)\n\n    def forward(self, obs, action, detach_encoder=False, feature=False):\n        # detach_encoder allows to stop gradient propogation to encoder\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        q1, f1 = self.Q1(obs, action)\n        q2, f2 = self.Q2(obs, action)\n\n        self.outputs['q1'] = q1\n        self.outputs['q2'] = q2\n\n        return q1, f1, q2, f2\n\n    def feature(self, obs, action, detach_encoder=False):\n        obs = self.encoder(obs, detach=detach_encoder)\n        q1, f1 = self.Q1(obs, action)\n        return f1", "\n\n\nclass PEER(nn.Module):\n\n    def __init__(self, obs_shape, z_dim, batch_size, critic, critic_target, output_type=\"continuous\"):\n        super(PEER, self).__init__()\n        self.batch_size = batch_size\n\n        self.encoder = critic.encoder\n\n        self.encoder_target = critic_target.encoder\n\n        self.W = nn.Parameter(torch.rand(z_dim, z_dim))\n        self.output_type = output_type\n\n    def encode(self, x, detach=False, ema=False):\n        \"\"\"\n        Encoder: z_t = e(x_t)\n        :param x: x_t, x y coordinates\n        :return: z_t, value in r2\n        \"\"\"\n        if ema:\n            with torch.no_grad():\n                z_out = self.encoder_target(x)\n        else:\n            z_out = self.encoder(x)\n\n        if detach:\n            z_out = z_out.detach()\n        return z_out\n\n    def compute_logits(self, z_a, z_pos):\n        Wz = torch.matmul(self.W, z_pos.T)  # (z_dim,B)\n        logits = torch.matmul(z_a, Wz)  # (B,B)\n        logits = logits - torch.max(logits, 1)[0][:, None]\n        return logits", "\n\nclass PEERAgent(object):\n\n    def __init__(\n            self,\n            obs_shape,\n            action_shape,\n            device,\n            hidden_dim=256,\n            discount=0.99,\n            init_temperature=0.01,\n            alpha_lr=1e-3,\n            alpha_beta=0.9,\n            actor_lr=1e-3,\n            actor_beta=0.9,\n            actor_log_std_min=-10,\n            actor_log_std_max=2,\n            actor_update_freq=2,\n            critic_lr=1e-3,\n            critic_beta=0.9,\n            critic_tau=0.005,\n            critic_target_update_freq=2,\n            encoder_type='pixel',\n            encoder_feature_dim=50,\n            encoder_lr=1e-3,\n            encoder_tau=0.005,\n            num_layers=4,\n            num_filters=32,\n            cpc_update_freq=1,\n            log_interval=100,\n            detach_encoder=False,\n            curl_latent_dim=128,\n            env_name=None,\n            seed=None\n    ):\n        self.device = device\n        self.discount = discount\n        self.critic_tau = critic_tau\n        self.encoder_tau = encoder_tau\n        self.actor_update_freq = actor_update_freq\n        self.critic_target_update_freq = critic_target_update_freq\n        self.cpc_update_freq = cpc_update_freq\n        self.log_interval = log_interval\n        self.image_size = obs_shape[-1]\n        self.curl_latent_dim = curl_latent_dim\n        self.detach_encoder = detach_encoder\n        self.encoder_type = encoder_type\n\n        self.feature_coef = 5e-4 # beta in paper\n\n        assert env_name is not None\n        assert seed is not None\n\n        self.actor = Actor(\n            obs_shape, action_shape, hidden_dim, encoder_type,\n            encoder_feature_dim, actor_log_std_min, actor_log_std_max,\n            num_layers, num_filters\n        ).to(device)\n\n        self.critic = Critic(\n            obs_shape, action_shape, hidden_dim, encoder_type,\n            encoder_feature_dim, num_layers, num_filters\n        ).to(device)\n\n        self.critic_target = Critic(\n            obs_shape, action_shape, hidden_dim, encoder_type,\n            encoder_feature_dim, num_layers, num_filters\n        ).to(device)\n\n\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        self.actor.encoder.copy_conv_weights_from(self.critic.encoder)\n\n        self.log_alpha = torch.tensor(np.log(init_temperature)).to(device)\n        self.log_alpha.requires_grad = True\n\n        self.target_entropy = -np.prod(action_shape)\n\n        # optimizers\n        self.actor_optimizer = torch.optim.Adam(\n            self.actor.parameters(), lr=actor_lr, betas=(actor_beta, 0.999)\n        )\n\n        self.critic_optimizer = torch.optim.Adam(\n            self.critic.parameters(), lr=critic_lr, betas=(critic_beta, 0.999)\n        )\n\n        self.log_alpha_optimizer = torch.optim.Adam(\n            [self.log_alpha], lr=alpha_lr, betas=(alpha_beta, 0.999)\n        )\n\n        if self.encoder_type == 'pixel':\n\n            self.PEER = PEER(obs_shape, encoder_feature_dim,\n                             self.curl_latent_dim, self.critic, self.critic_target, output_type='continuous').to(\n                self.device)\n\n            # optimizer for critic encoder for reconstruction loss\n            self.encoder_optimizer = torch.optim.Adam(\n                self.critic.encoder.parameters(), lr=encoder_lr\n            )\n\n            self.cpc_optimizer = torch.optim.Adam(\n                self.PEER.parameters(), lr=encoder_lr\n            )\n        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\n        self.train()\n        self.critic_target.train()\n\n    def train(self, training=True):\n        self.training = training\n        self.actor.train(training)\n        self.critic.train(training)\n        if self.encoder_type == 'pixel':\n            self.PEER.train(training)\n\n    @property\n    def alpha(self):\n        return self.log_alpha.exp()\n\n    def select_action(self, obs):\n        with torch.no_grad():\n            obs = torch.FloatTensor(obs).to(self.device)\n            obs = obs.unsqueeze(0)\n            mu, _, _, _ = self.actor(\n                obs, compute_pi=False, compute_log_pi=False\n            )\n            return mu.cpu().data.numpy().flatten()\n\n    def sample_action(self, obs):\n        if obs.shape[-1] != self.image_size:\n            obs = utils.center_crop_image(obs, self.image_size)\n\n        with torch.no_grad():\n            obs = torch.FloatTensor(obs).to(self.device)\n            obs = obs.unsqueeze(0)\n            mu, pi, _, _ = self.actor(obs, compute_log_pi=False)\n            return pi.cpu().data.numpy().flatten()\n\n    def update_critic(self, obs, action, reward, next_obs, not_done):\n        with torch.no_grad():\n            _, policy_action, log_pi, _ = self.actor(next_obs)\n            target_Q1, target_feature1, target_Q2, target_feature2 = self.critic_target(next_obs, policy_action)\n            target_V = torch.min(target_Q1,\n                                 target_Q2) - self.alpha.detach() * log_pi\n            target_Q = reward + (not_done * self.discount * target_V)\n\n        # get current Q estimates\n        current_Q1, current_feature1, current_Q2, current_feature2 = self.critic(\n            obs, action, detach_encoder=self.detach_encoder)\n        critic_loss = F.mse_loss(current_Q1,\n                                 target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        feature_loss1 = torch.einsum('ij,ij->i', [current_feature1, target_feature1]).mean()\n        feature_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\n        total_feature_loss = (feature_loss1 + feature_loss2) * self.feature_coef\n        critic_loss = total_feature_loss + critic_loss\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n    def update_actor_and_alpha(self, obs):\n        # detach encoder, so we don't update it with the actor loss\n        _, pi, log_pi, log_std = self.actor(obs, detach_encoder=True)\n        actor_Q1,_, actor_Q2,_ = self.critic(obs, pi, detach_encoder=True)\n\n        actor_Q = torch.min(actor_Q1, actor_Q2)\n        actor_loss = (self.alpha.detach() * log_pi - actor_Q).mean()\n\n        # optimize the actor\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        self.log_alpha_optimizer.zero_grad()\n        alpha_loss = (self.alpha *\n                      (-log_pi - self.target_entropy).detach()).mean()\n\n\n        alpha_loss.backward()\n        self.log_alpha_optimizer.step()\n\n    def update_cpc(self, obs_anchor, obs_pos):\n\n        z_a = self.PEER.encode(obs_anchor)\n        z_pos = self.PEER.encode(obs_pos, ema=True)\n\n        logits = self.PEER.compute_logits(z_a, z_pos)\n        labels = torch.arange(logits.shape[0]).long().to(self.device)\n        loss = self.cross_entropy_loss(logits, labels)\n\n        self.encoder_optimizer.zero_grad()\n        self.cpc_optimizer.zero_grad()\n        loss.backward()\n\n        self.encoder_optimizer.step()\n        self.cpc_optimizer.step()\n\n\n    def update(self, replay_buffer, MyLogger, step):\n        if self.encoder_type == 'pixel':\n            obs, action, reward, next_obs, not_done, cpc_kwargs = replay_buffer.sample_cpc()\n        else:\n            obs, action, reward, next_obs, not_done = replay_buffer.sample_proprio()\n\n        self.update_critic(obs, action, reward, next_obs, not_done, MyLogger, step)\n\n        if step % self.actor_update_freq == 0:\n\n            self.update_actor_and_alpha(obs, MyLogger, step)\n\n        if step % self.critic_target_update_freq == 0:\n            utils.soft_update_params(\n                self.critic.Q1, self.critic_target.Q1, self.critic_tau\n            )\n            utils.soft_update_params(\n                self.critic.Q2, self.critic_target.Q2, self.critic_tau\n            )\n            utils.soft_update_params(\n                self.critic.encoder, self.critic_target.encoder,\n                self.encoder_tau\n            )\n\n        if step % self.cpc_update_freq == 0 and self.encoder_type == 'pixel':\n            obs_anchor, obs_pos = cpc_kwargs[\"obs_anchor\"], cpc_kwargs[\"obs_pos\"]\n            self.update_cpc(obs_anchor, obs_pos, cpc_kwargs, MyLogger, step)\n\n    def save(self, model_dir, step):\n        torch.save(\n            self.actor.state_dict(), '%s/actor_%s.pt' % (model_dir, step)\n        )\n        torch.save(\n            self.critic.state_dict(), '%s/critic_%s.pt' % (model_dir, step)\n        )\n\n    def save_curl(self, model_dir, step):\n        torch.save(\n            self.PEER.state_dict(), '%s/curl_%s.pt' % (model_dir, step)\n        )\n\n    def load(self, model_dir, step):\n        self.actor.load_state_dict(\n            torch.load('%s/actor_%s.pt' % (model_dir, step))\n        )\n        self.critic.load_state_dict(\n            torch.load('%s/critic_%s.pt' % (model_dir, step))\n        )"]}
{"filename": "dmc/PEER-CURL/main.py", "chunked_list": ["import mujoco\nimport dm_control\nfrom tqdm import trange\nimport numpy as np\nimport torch\nimport argparse\n\nimport dmc2gym\nimport utils\n", "import utils\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    # environment\n    parser.add_argument('--domain_name', default=\"ball_in_cup\")\n    parser.add_argument('--task_name', default=\"catch\")\n    parser.add_argument('--pre_transform_image_size', default=100, type=int)\n\n    parser.add_argument('--image_size', default=84, type=int)\n    parser.add_argument('--action_repeat', default=1, type=int)\n    parser.add_argument('--frame_stack', default=3, type=int)\n    # replay buffer\n    parser.add_argument('--replay_buffer_capacity', default=100000, type=int)\n    # train\n    parser.add_argument('--agent', default='curl_sac', type=str)\n    parser.add_argument('--init_steps', default=1000, type=int) \n    parser.add_argument('--num_train_steps', default=500000, type=int)\n    parser.add_argument('--batch_size', default=512, type=int)\n    parser.add_argument('--hidden_dim', default=1024, type=int)\n    # eval\n    parser.add_argument('--eval_freq', default=5000, type=int)\n    parser.add_argument('--num_eval_episodes', default=10, type=int)\n    # critic\n    parser.add_argument('--critic_lr', default=1e-3, type=float)\n    parser.add_argument('--critic_beta', default=0.9, type=float)\n    parser.add_argument('--critic_tau', default=0.01, type=float) \n    parser.add_argument('--critic_target_update_freq', default=2, type=int) \n    # actor\n    parser.add_argument('--actor_lr', default=1e-3, type=float)\n    parser.add_argument('--actor_beta', default=0.9, type=float)\n    parser.add_argument('--actor_log_std_min', default=-10, type=float)\n    parser.add_argument('--actor_log_std_max', default=2, type=float)\n    parser.add_argument('--actor_update_freq', default=2, type=int)\n    # encoder\n    parser.add_argument('--encoder_type', default='pixel', type=str)\n    parser.add_argument('--encoder_feature_dim', default=50, type=int)\n    parser.add_argument('--encoder_lr', default=1e-3, type=float)\n    parser.add_argument('--encoder_tau', default=0.05, type=float)\n    parser.add_argument('--num_layers', default=4, type=int)\n    parser.add_argument('--num_filters', default=32, type=int)\n    parser.add_argument('--curl_latent_dim', default=128, type=int)\n    # sac\n    parser.add_argument('--discount', default=0.99, type=float)\n    parser.add_argument('--init_temperature', default=0.1, type=float)\n    parser.add_argument('--alpha_lr', default=1e-4, type=float)\n    parser.add_argument('--alpha_beta', default=0.5, type=float)\n    # misc\n    parser.add_argument('--seed', default=1, type=int)\n    parser.add_argument('--work_dir', default='./curl_exp', type=str)\n    parser.add_argument('--save_tb', default=False, action='store_true')\n    parser.add_argument('--save_buffer', default=False, action='store_true')\n    parser.add_argument('--save_video', default=False, action='store_true')\n    parser.add_argument('--save_model', default=False, action='store_true')\n    parser.add_argument('--detach_encoder', default=False, action='store_true')\n\n    parser.add_argument('--log_interval', default=100, type=int)\n    args = parser.parse_args()\n\n    # setting hyper-parameter\n    # action repeat\n    if args.domain_name in [\"finger\", \"walker\"]:\n        args.action_repeat = 2\n    elif args.domain_name == \"cartpole\":\n        args.action_repeat = 8\n    else:\n        args.action_repeat = 4\n\n    # learning rate\n    if args.domain_name in [\"cheetah\"]:\n        args.actor_lr = 2e-4\n        args.critic_lr = 2e-4\n        args.encoder_lr = 2e-4\n    else:\n        args.actor_lr = 1e-3\n        args.critic_lr = 1e-3\n        args.encoder_lr = 1e-3\n\n    return args", "\n\n\ndef make_agent(obs_shape, action_shape, args, device):\n    import peer\n    return peer.PEERAgent(\n        obs_shape=obs_shape,\n        action_shape=action_shape,\n        device=device,\n        hidden_dim=args.hidden_dim,\n        discount=args.discount,\n        init_temperature=args.init_temperature,\n        alpha_lr=args.alpha_lr,\n        alpha_beta=args.alpha_beta,\n        actor_lr=args.actor_lr,\n        actor_beta=args.actor_beta,\n        actor_log_std_min=args.actor_log_std_min,\n        actor_log_std_max=args.actor_log_std_max,\n        actor_update_freq=args.actor_update_freq,\n        critic_lr=args.critic_lr,\n        critic_beta=args.critic_beta,\n        critic_tau=args.critic_tau,\n        critic_target_update_freq=args.critic_target_update_freq,\n        encoder_type=args.encoder_type,\n        encoder_feature_dim=args.encoder_feature_dim,\n        encoder_lr=args.encoder_lr,\n        encoder_tau=args.encoder_tau,\n        num_layers=args.num_layers,\n        num_filters=args.num_filters,\n        log_interval=args.log_interval,\n        detach_encoder=args.detach_encoder,\n        curl_latent_dim=args.curl_latent_dim,\n        env_name=f\"{args.domain_name}-{args.task_name}\",\n        seed=args.seed\n    )", "\n\ndef main():\n    args = parse_args()\n    if args.seed == -1:\n        args.__dict__[\"seed\"] = np.random.randint(1, 1000000)\n    utils.set_seed_everywhere(args.seed)\n    env = dmc2gym.make(\n        domain_name=args.domain_name,\n        task_name=args.task_name,\n        seed=args.seed,\n        visualize_reward=False,\n        from_pixels=(args.encoder_type == 'pixel'),\n        height=args.pre_transform_image_size,\n        width=args.pre_transform_image_size,\n        frame_skip=args.action_repeat\n    )\n\n    env.seed(args.seed)\n\n    # stack several consecutive frames together\n    if args.encoder_type == 'pixel':\n        env = utils.FrameStack(env, k=args.frame_stack)\n\n    device = torch.device('cuda')\n    action_shape = env.action_space.shape\n\n    if args.encoder_type == 'pixel':\n        obs_shape = (3 * args.frame_stack, args.image_size, args.image_size)\n        pre_aug_obs_shape = (3 * args.frame_stack, args.pre_transform_image_size, args.pre_transform_image_size)\n\n    else:\n        obs_shape = env.observation_space.shape\n        pre_aug_obs_shape = obs_shape\n\n    replay_buffer = utils.ReplayBuffer(\n        obs_shape=pre_aug_obs_shape,\n        action_shape=action_shape,\n        capacity=args.replay_buffer_capacity,\n        batch_size=args.batch_size,\n        device=device,\n        image_size=args.image_size,\n    )\n\n    agent = make_agent(\n        obs_shape=obs_shape,\n        action_shape=action_shape,\n        args=args,\n        device=device\n    )\n\n\n    episode, episode_reward, done = 0, 0, True\n    episode_step = 0\n\n    for step in trange(args.num_train_steps):\n\n        if done:\n            obs, done = env.reset(), False\n            print(f\"Domain_name: {args.domain_name}, Task_name: {args.task_name}, Seed: {args.seed}, Num_steps: {step} Episode_length: {episode_step}, Episode_reward: {episode_reward}\")\n            # done = False\n            episode_reward = 0\n            episode_step = 0\n            episode += 1\n        # sample action for data collection\n        if step < args.init_steps:\n            action = env.action_space.sample()\n        else:\n            with utils.eval_mode(agent):\n                action = agent.sample_action(obs)\n\n        # run training update\n        if step >= args.init_steps:\n            num_updates = 1\n            for _ in range(num_updates):\n                agent.update(replay_buffer, None, step)\n\n        next_obs, reward, done, _ = env.step(action)\n\n        # allow infinit bootstrap\n        done_bool = 0 if episode_step + 1 == env._max_episode_steps else float(\n            done\n        )\n        episode_reward += reward\n        replay_buffer.add(obs, action, reward, next_obs, done_bool)\n        obs = next_obs\n        episode_step += 1", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "dmc/PEER-CURL/utils.py", "chunked_list": ["import torch\nimport numpy as np\nimport torch.nn as nn\nimport gym\nimport os\nfrom collections import deque\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport time\nfrom skimage.util.shape import view_as_windows", "import time\nfrom skimage.util.shape import view_as_windows\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "\n\ndef soft_update_params(net, target_net, tau):\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(\n            tau * param.data + (1 - tau) * target_param.data\n        )\n\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)", "\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef module_hash(module):\n    result = 0\n    for tensor in module.state_dict().values():\n        result += tensor.sum().item()\n    return result", "\ndef module_hash(module):\n    result = 0\n    for tensor in module.state_dict().values():\n        result += tensor.sum().item()\n    return result\n\n\ndef make_dir(dir_path):\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path", "def make_dir(dir_path):\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n\n\ndef preprocess_obs(obs, bits=5):\n    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n    bins = 2**bits\n    assert obs.dtype == torch.float32\n    if bits < 8:\n        obs = torch.floor(obs / 2**(8 - bits))\n    obs = obs / bins\n    obs = obs + torch.rand_like(obs) / bins\n    obs = obs - 0.5\n    return obs", "def preprocess_obs(obs, bits=5):\n    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n    bins = 2**bits\n    assert obs.dtype == torch.float32\n    if bits < 8:\n        obs = torch.floor(obs / 2**(8 - bits))\n    obs = obs / bins\n    obs = obs + torch.rand_like(obs) / bins\n    obs = obs - 0.5\n    return obs", "\n\nclass ReplayBuffer(Dataset):\n    \"\"\"Buffer to store environment transitions.\"\"\"\n    def __init__(self, obs_shape, action_shape, capacity, batch_size, device,image_size=84,transform=None):\n        self.capacity = capacity\n        self.batch_size = batch_size\n        self.device = device\n        self.image_size = image_size\n        self.transform = transform\n        # the proprioceptive obs is stored as float32, pixels obs as uint8\n        obs_dtype = np.float32 if len(obs_shape) == 1 else np.uint8\n        \n        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.last_save = 0\n        self.full = False\n\n\n    def add(self, obs, action, reward, next_obs, done):\n       \n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample_proprio(self):\n        \n        idxs = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=self.batch_size\n        )\n        \n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        next_obses = torch.as_tensor(\n            next_obses, device=self.device\n        ).float()\n        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n        return obses, actions, rewards, next_obses, not_dones\n\n    def sample_cpc(self):\n\n        start = time.time()\n        idxs = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=self.batch_size\n        )\n      \n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n        pos = obses.copy()\n\n        obses = random_crop(obses, self.image_size)\n        next_obses = random_crop(next_obses, self.image_size)\n        pos = random_crop(pos, self.image_size)\n    \n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(\n            next_obses, device=self.device\n        ).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\n        pos = torch.as_tensor(pos, device=self.device).float()\n        cpc_kwargs = dict(obs_anchor=obses, obs_pos=pos,\n                          time_anchor=None, time_pos=None)\n\n        return obses, actions, rewards, next_obses, not_dones, cpc_kwargs\n\n    def save(self, save_dir):\n        if self.idx == self.last_save:\n            return\n        path = os.path.join(save_dir, '%d_%d.pt' % (self.last_save, self.idx))\n        payload = [\n            self.obses[self.last_save:self.idx],\n            self.next_obses[self.last_save:self.idx],\n            self.actions[self.last_save:self.idx],\n            self.rewards[self.last_save:self.idx],\n            self.not_dones[self.last_save:self.idx]\n        ]\n        self.last_save = self.idx\n        torch.save(payload, path)\n\n    def load(self, save_dir):\n        chunks = os.listdir(save_dir)\n        chucks = sorted(chunks, key=lambda x: int(x.split('_')[0]))\n        for chunk in chucks:\n            start, end = [int(x) for x in chunk.split('.')[0].split('_')]\n            path = os.path.join(save_dir, chunk)\n            payload = torch.load(path)\n            assert self.idx == start\n            self.obses[start:end] = payload[0]\n            self.next_obses[start:end] = payload[1]\n            self.actions[start:end] = payload[2]\n            self.rewards[start:end] = payload[3]\n            self.not_dones[start:end] = payload[4]\n            self.idx = end\n\n    def __getitem__(self, idx):\n        idx = np.random.randint(\n            0, self.capacity if self.full else self.idx, size=1\n        )\n        idx = idx[0]\n        obs = self.obses[idx]\n        action = self.actions[idx]\n        reward = self.rewards[idx]\n        next_obs = self.next_obses[idx]\n        not_done = self.not_dones[idx]\n\n        if self.transform:\n            obs = self.transform(obs)\n            next_obs = self.transform(next_obs)\n\n        return obs, action, reward, next_obs, not_done\n\n    def __len__(self):\n        return self.capacity ", "\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self._k = k\n        self._frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=1,\n            shape=((shp[0] * k,) + shp[1:]),\n            dtype=env.observation_space.dtype\n        )\n        self._max_episode_steps = env._max_episode_steps\n\n    def reset(self):\n        obs = self.env.reset()\n        for _ in range(self._k):\n            self._frames.append(obs)\n        return self._get_obs()\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self._frames.append(obs)\n        return self._get_obs(), reward, done, info\n\n    def _get_obs(self):\n        assert len(self._frames) == self._k\n        return np.concatenate(list(self._frames), axis=0)", "\n\ndef random_crop(imgs, output_size):\n    \"\"\"\n    Vectorized way to do random crop using sliding windows\n    and picking out random ones\n\n    args:\n        imgs, batch images with shape (B,C,H,W)\n    \"\"\"\n    # batch size\n    n = imgs.shape[0]\n    img_size = imgs.shape[-1]\n    crop_max = img_size - output_size\n    imgs = np.transpose(imgs, (0, 2, 3, 1))\n    w1 = np.random.randint(0, crop_max, n)\n    h1 = np.random.randint(0, crop_max, n)\n    # creates all sliding windows combinations of size (output_size)\n    windows = view_as_windows(\n        imgs, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n    # selects a random window for each batch element\n    cropped_imgs = windows[np.arange(n), w1, h1]\n    return cropped_imgs", "\ndef center_crop_image(image, output_size):\n    h, w = image.shape[1:]\n    new_h, new_w = output_size, output_size\n\n    top = (h - new_h)//2\n    left = (w - new_w)//2\n\n    image = image[:, top:top + new_h, left:left + new_w]\n    return image", "\nclass ReplayBufferMuJoCo(object):\n    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n        self.max_size = max_size\n        self.ptr = 0\n        self.size = 0\n\n        self.state = np.zeros((max_size, state_dim), dtype='float32')\n        self.action = np.zeros((max_size, action_dim), dtype='float32')\n        self.next_state = np.zeros((max_size, state_dim), dtype='float32')\n        self.reward = np.zeros((max_size, 1), dtype='float32')\n        self.not_done = np.zeros((max_size, 1), dtype='float32')\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, state, action, next_state, reward, done):\n        self.state[self.ptr] = state\n        self.action[self.ptr] = action\n        self.next_state[self.ptr] = next_state\n        self.reward[self.ptr] = reward\n        self.not_done[self.ptr] = 1. - done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, self.size, size=batch_size)\n\n        return (\n            torch.from_numpy(self.state[ind]).to(self.device),\n            torch.from_numpy(self.action[ind]).to(self.device),\n            torch.from_numpy(self.next_state[ind]).to(self.device),\n            torch.from_numpy(self.reward[ind]).to(self.device),\n            torch.from_numpy(self.not_done[ind]).to(self.device)\n        )", "\n"]}
{"filename": "dmc/PEER-CURL/encoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias\n\n", "\n\n# for 84 x 84 inputs\nOUT_DIM = {2: 39, 4: 35, 6: 31}\n# for 64 x 64 inputs\nOUT_DIM_64 = {2: 29, 4: 25, 6: 21}\n\n\nclass PixelEncoder(nn.Module):\n    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n    def __init__(self, obs_shape, feature_dim, num_layers=2, num_filters=32,output_logits=False):\n        super().__init__()\n\n        assert len(obs_shape) == 3\n        self.obs_shape = obs_shape\n        self.feature_dim = feature_dim\n        self.num_layers = num_layers\n\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(obs_shape[0], num_filters, 3, stride=2)]\n        )\n        for i in range(num_layers - 1):\n            self.convs.append(nn.Conv2d(num_filters, num_filters, 3, stride=1))\n\n        out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers] \n        self.fc = nn.Linear(num_filters * out_dim * out_dim, self.feature_dim)\n        self.ln = nn.LayerNorm(self.feature_dim)\n\n        self.outputs = dict()\n        self.output_logits = output_logits\n\n    def reparameterize(self, mu, logstd):\n        std = torch.exp(logstd)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward_conv(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        conv = torch.relu(self.convs[0](obs))\n        self.outputs['conv1'] = conv\n\n        for i in range(1, self.num_layers):\n            conv = torch.relu(self.convs[i](conv))\n            self.outputs['conv%s' % (i + 1)] = conv\n\n        h = conv.reshape(conv.size(0), -1)\n        return h\n\n    def forward(self, obs, detach=False):\n        h = self.forward_conv(obs)\n\n        if detach:\n            h = h.detach()\n\n        h_fc = self.fc(h)\n        self.outputs['fc'] = h_fc\n\n        h_norm = self.ln(h_fc)\n        self.outputs['ln'] = h_norm\n\n        if self.output_logits:\n            out = h_norm\n        else:\n            out = torch.tanh(h_norm)\n            self.outputs['tanh'] = out\n\n        return out\n\n    def copy_conv_weights_from(self, source):\n        \"\"\"Tie convolutional layers\"\"\"\n        # only tie conv layers\n        for i in range(self.num_layers):\n            tie_weights(src=source.convs[i], trg=self.convs[i])\n\n    def log(self, L, step, log_freq):\n        if step % log_freq != 0:\n            return\n\n        for k, v in self.outputs.items():\n            L.log_histogram('train_encoder/%s_hist' % k, v, step)\n            if len(v.shape) > 2:\n                L.log_image('train_encoder/%s_img' % k, v[0], step)\n\n        for i in range(self.num_layers):\n            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)\n        L.log_param('train_encoder/fc', self.fc, step)\n        L.log_param('train_encoder/ln', self.ln, step)", "class PixelEncoder(nn.Module):\n    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n    def __init__(self, obs_shape, feature_dim, num_layers=2, num_filters=32,output_logits=False):\n        super().__init__()\n\n        assert len(obs_shape) == 3\n        self.obs_shape = obs_shape\n        self.feature_dim = feature_dim\n        self.num_layers = num_layers\n\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(obs_shape[0], num_filters, 3, stride=2)]\n        )\n        for i in range(num_layers - 1):\n            self.convs.append(nn.Conv2d(num_filters, num_filters, 3, stride=1))\n\n        out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers] \n        self.fc = nn.Linear(num_filters * out_dim * out_dim, self.feature_dim)\n        self.ln = nn.LayerNorm(self.feature_dim)\n\n        self.outputs = dict()\n        self.output_logits = output_logits\n\n    def reparameterize(self, mu, logstd):\n        std = torch.exp(logstd)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward_conv(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        conv = torch.relu(self.convs[0](obs))\n        self.outputs['conv1'] = conv\n\n        for i in range(1, self.num_layers):\n            conv = torch.relu(self.convs[i](conv))\n            self.outputs['conv%s' % (i + 1)] = conv\n\n        h = conv.reshape(conv.size(0), -1)\n        return h\n\n    def forward(self, obs, detach=False):\n        h = self.forward_conv(obs)\n\n        if detach:\n            h = h.detach()\n\n        h_fc = self.fc(h)\n        self.outputs['fc'] = h_fc\n\n        h_norm = self.ln(h_fc)\n        self.outputs['ln'] = h_norm\n\n        if self.output_logits:\n            out = h_norm\n        else:\n            out = torch.tanh(h_norm)\n            self.outputs['tanh'] = out\n\n        return out\n\n    def copy_conv_weights_from(self, source):\n        \"\"\"Tie convolutional layers\"\"\"\n        # only tie conv layers\n        for i in range(self.num_layers):\n            tie_weights(src=source.convs[i], trg=self.convs[i])\n\n    def log(self, L, step, log_freq):\n        if step % log_freq != 0:\n            return\n\n        for k, v in self.outputs.items():\n            L.log_histogram('train_encoder/%s_hist' % k, v, step)\n            if len(v.shape) > 2:\n                L.log_image('train_encoder/%s_img' % k, v[0], step)\n\n        for i in range(self.num_layers):\n            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)\n        L.log_param('train_encoder/fc', self.fc, step)\n        L.log_param('train_encoder/ln', self.ln, step)", "\n\nclass IdentityEncoder(nn.Module):\n    def __init__(self, obs_shape, feature_dim, num_layers, num_filters,*args):\n        super().__init__()\n\n        assert len(obs_shape) == 1\n        self.feature_dim = obs_shape[0]\n\n    def forward(self, obs, detach=False):\n        return obs\n\n    def copy_conv_weights_from(self, source):\n        pass\n\n    def log(self, L, step, log_freq):\n        pass", "\n\n_AVAILABLE_ENCODERS = {'pixel': PixelEncoder, 'identity': IdentityEncoder}\n\n\ndef make_encoder(\n    encoder_type, obs_shape, feature_dim, num_layers, num_filters, output_logits=False\n):\n    assert encoder_type in _AVAILABLE_ENCODERS\n    return _AVAILABLE_ENCODERS[encoder_type](\n        obs_shape, feature_dim, num_layers, num_filters, output_logits\n    )", ""]}
{"filename": "atari/PEER-DrQ/peer.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nimport math\n\nimport utils\nimport hydra\nimport kornia", "import hydra\nimport kornia\nimport os\n\nfrom replay_buffer import PrioritizedReplayBuffer\n\n\n# from\n# https://github.com/mlperf/inference/blob/master/others/edge/object_detection/ssd_mobilenet/pytorch/utils.py#L40\nclass Conv2d_tf(nn.Conv2d):\n    \"\"\"\n    Conv2d with the padding behavior from TF\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Conv2d_tf, self).__init__(*args, **kwargs)\n        self.padding = kwargs.get(\"padding\", \"SAME\")\n\n    def _compute_padding(self, input, dim):\n        input_size = input.size(dim + 2)\n        filter_size = self.weight.size(dim + 2)\n        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n        total_padding = max(0, (out_size - 1) * self.stride[dim] +\n                            effective_filter_size - input_size)\n        additional_padding = int(total_padding % 2 != 0)\n\n        return additional_padding, total_padding\n\n    def forward(self, input):\n        if self.padding == \"VALID\":\n            return F.conv2d(\n                input,\n                self.weight,\n                self.bias,\n                self.stride,\n                padding=0,\n                dilation=self.dilation,\n                groups=self.groups,\n            )\n        rows_odd, padding_rows = self._compute_padding(input, dim=0)\n        cols_odd, padding_cols = self._compute_padding(input, dim=1)\n        if rows_odd or cols_odd:\n            input = F.pad(input, [0, cols_odd, 0, rows_odd])\n\n        return F.conv2d(\n            input,\n            self.weight,\n            self.bias,\n            self.stride,\n            padding=(padding_rows // 2, padding_cols // 2),\n            dilation=self.dilation,\n            groups=self.groups,\n        )", "# https://github.com/mlperf/inference/blob/master/others/edge/object_detection/ssd_mobilenet/pytorch/utils.py#L40\nclass Conv2d_tf(nn.Conv2d):\n    \"\"\"\n    Conv2d with the padding behavior from TF\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Conv2d_tf, self).__init__(*args, **kwargs)\n        self.padding = kwargs.get(\"padding\", \"SAME\")\n\n    def _compute_padding(self, input, dim):\n        input_size = input.size(dim + 2)\n        filter_size = self.weight.size(dim + 2)\n        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n        total_padding = max(0, (out_size - 1) * self.stride[dim] +\n                            effective_filter_size - input_size)\n        additional_padding = int(total_padding % 2 != 0)\n\n        return additional_padding, total_padding\n\n    def forward(self, input):\n        if self.padding == \"VALID\":\n            return F.conv2d(\n                input,\n                self.weight,\n                self.bias,\n                self.stride,\n                padding=0,\n                dilation=self.dilation,\n                groups=self.groups,\n            )\n        rows_odd, padding_rows = self._compute_padding(input, dim=0)\n        cols_odd, padding_cols = self._compute_padding(input, dim=1)\n        if rows_odd or cols_odd:\n            input = F.pad(input, [0, cols_odd, 0, rows_odd])\n\n        return F.conv2d(\n            input,\n            self.weight,\n            self.bias,\n            self.stride,\n            padding=(padding_rows // 2, padding_cols // 2),\n            dilation=self.dilation,\n            groups=self.groups,\n        )", "\n\nclass SEEncoder(nn.Module):\n    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n    def __init__(self, obs_shape):\n        super().__init__()\n\n        self.feature_dim = 64 * 4 * 4\n\n        self.conv1 = Conv2d_tf(obs_shape[0], 32, 5, stride=5, padding='SAME')\n        self.conv2 = Conv2d_tf(32, 64, 5, stride=5, padding='SAME')\n\n        self.outputs = dict()\n\n    def forward(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        h = torch.relu(self.conv1(obs))\n        self.outputs['conv1'] = h\n\n        h = torch.relu(self.conv2(h))\n        self.outputs['conv2'] = h\n\n        out = h.view(h.size(0), -1)\n        self.outputs['out'] = out\n\n        assert out.shape[1] == self.feature_dim\n        return out", "\n\n\nclass Encoder(nn.Module):\n    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n    def __init__(self, obs_shape):\n        super().__init__()\n\n        self.feature_dim = 64 * 11 * 11\n\n        self.conv1 = Conv2d_tf(obs_shape[0], 32, 8, stride=4, padding='valid')\n        self.conv2 = Conv2d_tf(32, 64, 4, stride=2, padding='valid')\n        self.conv3 = Conv2d_tf(64, 64, 3, stride=1, padding='valid')\n\n        self.outputs = dict()\n\n    def forward(self, obs):\n        obs = obs / 255.\n        self.outputs['obs'] = obs\n\n        h = torch.relu(self.conv1(obs))\n        self.outputs['conv1'] = h\n\n        h = torch.relu(self.conv2(h))\n        self.outputs['conv2'] = h\n\n        h = torch.relu(self.conv3(h))\n        self.outputs['conv3'] = h\n\n        out = h.view(h.size(0), -1)\n        self.outputs['out'] = out\n\n        assert out.shape[1] == self.feature_dim\n\n        return out", "\n\nclass Intensity(nn.Module):\n    def __init__(self, scale=0.1):\n        super().__init__()\n        self.scale = scale\n\n    def forward(self, x):\n        noise = 1.0 + (self.scale * torch.randn(\n            (x.size(0), 1, 1, 1), device=x.device).clamp_(-2.0, 2.0))\n        return x * noise", "\n\nclass Critic(nn.Module):\n    \"\"\"Critic network, employes double Q-learning.\"\"\"\n    def __init__(self, obs_shape, num_actions, hidden_dim, hidden_depth,\n                 dueling, aug_type, image_pad, intensity_scale):\n        super().__init__()\n\n        AUGMENTATIONS = {\n            'intensity':\n            Intensity(scale=intensity_scale),\n            'reflect_crop':\n            nn.Sequential(nn.ReplicationPad2d(image_pad),\n                          kornia.augmentation.RandomCrop((84, 84))),\n            'crop_intensity':\n            nn.Sequential(nn.ReplicationPad2d(image_pad),\n                          kornia.augmentation.RandomCrop((84, 84)),\n                          Intensity(scale=intensity_scale)),\n            'zero_crop':\n            nn.Sequential(nn.ZeroPad2d(image_pad),\n                          kornia.augmentation.RandomCrop((84, 84))),\n            'rotate':\n            kornia.augmentation.RandomRotation(degrees=5.0),\n            'h_flip':\n            kornia.augmentation.RandomHorizontalFlip(p=0.5),\n            'v_flip':\n            kornia.augmentation.RandomVerticalFlip(p=0.5),\n            'none':\n            nn.Identity(),\n            'all':\n            nn.Sequential(nn.ReplicationPad2d(image_pad),\n                          kornia.augmentation.RandomCrop((84, 84)),\n                          kornia.augmentation.RandomHorizontalFlip(p=0.5),\n                          kornia.augmentation.RandomVerticalFlip(p=0.5),\n                          kornia.augmentation.RandomRotation(degrees=5.0))\n        }\n\n        assert aug_type in AUGMENTATIONS.keys()\n\n        self.aug_trans = AUGMENTATIONS.get(aug_type)\n\n        # self.encoder = hydra.utils.instantiate(encoder_cfg)\n        self.encoder = Encoder(obs_shape)\n\n        self.dueling = dueling\n        self.num_actions = num_actions\n\n        if dueling:\n            self.V = utils.mlp(self.encoder.feature_dim, hidden_dim, 1,\n                               hidden_depth)\n            self.A = utils.mlp(self.encoder.feature_dim, hidden_dim,\n                               num_actions, hidden_depth)\n        else:\n            self.Q = utils.mlp(self.encoder.feature_dim, hidden_dim,\n                               num_actions, hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, use_aug=False):\n        if use_aug:\n            obs = self.aug_trans(obs)\n\n        obs = self.encoder(obs)\n\n        if self.dueling:\n            v = self.V(obs)\n            a = self.A(obs)\n            q = v + a - a.mean(1, keepdim=True)\n        else:\n            q = self.Q(obs)\n\n        self.outputs['q'] = q\n        # return representation\n        return obs, q", "\n\nclass PEER(object):\n    \"\"\"Data regularized Q-learning: Deep Q-learning.\"\"\"\n    def __init__(self, obs_shape, num_actions, device,\n                 # encoder_cfg, critic_cfg,\n                 discount, lr, beta_1, beta_2, weight_decay, adam_eps,\n                 max_grad_norm, critic_tau, critic_target_update_frequency,\n                 batch_size, multistep_return, eval_eps, double_q,\n                 prioritized_replay_beta0, prioritized_replay_beta_steps,\n\n                 # New add for logging\n                 env_name, seed,\n                 # encoder\n                 # obs_shape\n                 # decoder\n                 hidden_dim, hidden_depth,\n                 dueling, aug_type, image_pad, intensity_scale\n                 ):\n\n        self.device = device\n        self.discount = discount\n        self.critic_tau = critic_tau\n        self.num_actions = num_actions\n        self.critic_target_update_frequency = critic_target_update_frequency\n        self.batch_size = batch_size\n        self.eval_eps = eval_eps\n        self.max_grad_norm = max_grad_norm\n        self.multistep_return = multistep_return\n        self.double_q = double_q\n        assert prioritized_replay_beta0 <= 1.0\n        self.prioritized_replay_beta0 = prioritized_replay_beta0\n        self.prioritized_replay_beta_steps = prioritized_replay_beta_steps\n        self.eps = 0\n\n        self.critic = Critic(obs_shape, num_actions, hidden_dim, hidden_depth,\n                 dueling, aug_type, image_pad, intensity_scale).to(self.device)\n        self.critic_target = Critic(obs_shape, num_actions, hidden_dim, hidden_depth,\n                 dueling, aug_type, image_pad, intensity_scale).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=lr,\n                                                 betas=(beta_1, beta_2),\n                                                 weight_decay=weight_decay,\n                                                 eps=adam_eps)\n        # PEER setting\n        self.beta = 5e-4\n        self.train()\n        self.critic_target.train()\n\n    def train(self, training=True):\n        self.training = training\n        self.critic.train(training)\n\n    def act(self, obs):\n        with torch.no_grad():\n            obs = torch.FloatTensor(obs).to(self.device)\n            obs = obs.unsqueeze(0).contiguous()\n            _, q = self.critic(obs)\n            action = q.max(dim=1)[1].item()\n        return action\n\n    def update_critic(self, obs, action, reward, next_obs, not_done, weights,\n                      logger, step):\n        with torch.no_grad():\n            discount = self.discount**self.multistep_return\n            if self.double_q:\n                next_repr, next_Q_target, = self.critic_target(next_obs, use_aug=True)\n                _, next_Q = self.critic(next_obs, use_aug=True)\n                next_action = next_Q.max(dim=1)[1].unsqueeze(1)\n                next_Q = next_Q_target.gather(1, next_action)\n                target_Q = reward + (not_done * discount * next_Q)\n            else:\n                next_repr, next_Q = self.critic_target(next_obs, use_aug=True)\n                next_Q = next_Q.max(dim=1)[0].unsqueeze(1)\n                target_Q = reward + (not_done * discount * next_Q)\n\n        # get current Q estimates\n        current_repr, current_Q = self.critic(obs, use_aug=True)\n        current_Q = current_Q.gather(1, action)\n\n        td_errors = current_Q - target_Q\n        peer_loss = torch.einsum('ij,ij->i', [current_repr, next_repr]).mean() * self.beta\n        critic_losses = F.smooth_l1_loss(current_Q, target_Q, reduction='none')  + peer_loss\n        if weights is not None:\n            critic_losses *= weights\n\n        critic_loss = critic_losses.mean()\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        if self.max_grad_norm > 0.0:\n            nn.utils.clip_grad_norm_(self.critic.parameters(),\n                                     self.max_grad_norm)\n        self.critic_optimizer.step()\n\n\n        return td_errors.squeeze(dim=1).detach().cpu().numpy()\n\n    def update(self, replay_buffer, logger, step):\n\n        prioritized_replay = type(replay_buffer) == PrioritizedReplayBuffer\n\n        if prioritized_replay:\n            fraction = min(step / self.prioritized_replay_beta_steps, 1.0)\n            beta = self.prioritized_replay_beta0 + fraction * (\n                1.0 - self.prioritized_replay_beta0)\n            obs, action, reward, next_obs, not_done, weights, idxs = replay_buffer.sample_multistep(\n                self.batch_size, beta, self.discount, self.multistep_return)\n        else:\n            obs, action, reward, next_obs, not_done = replay_buffer.sample_multistep(\n                self.batch_size, self.discount, self.multistep_return)\n            weights = None\n\n\n        td_errors = self.update_critic(obs, action, reward, next_obs, not_done,\n                                       weights, logger, step)\n\n        if prioritized_replay:\n            prios = np.abs(td_errors) + 1e-6\n            replay_buffer.update_priorities(idxs, prios)\n\n        if step % self.critic_target_update_frequency == 0:\n            utils.soft_update_params(self.critic, self.critic_target,\n                                     self.critic_tau)", ""]}
{"filename": "atari/PEER-DrQ/main.py", "chunked_list": ["import os\nimport time\nimport numpy as np\nimport atari\nimport torch\nimport utils\nimport argparse\nfrom replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\nfrom tqdm import trange\nfrom peer import PEER", "from tqdm import trange\nfrom peer import PEER\n\n\ntorch.backends.cudnn.benchmark = True\ntorch.set_num_threads(4)\n\nclass Workspace(object):\n    def __init__(self, cfg):\n        self.work_dir = os.getcwd()\n        print(f'workspace: {self.work_dir}')\n\n        self.cfg = cfg\n\n        utils.set_seed_everywhere(cfg.seed)\n        self.device = torch.device(cfg.device)\n        self.env = atari.make_env(cfg.env, cfg.seed, cfg.terminal_on_life_loss)\n        self.eval_env = atari.make_env(cfg.env, cfg.seed + 1,\n                                       cfg.terminal_on_life_loss)\n\n\n        self.agent = PEER(\n            obs_shape=self.env.observation_space.shape,\n            num_actions=self.env.action_space.n,\n            device=cfg.device,\n            # encoder_cfg,\n            # critic_cfg,\n            discount=cfg.discount,\n            lr=cfg.lr,\n            beta_1=cfg.beta_1,\n            beta_2=cfg.beta_2,\n            weight_decay=cfg.weight_decay,\n            adam_eps=cfg.adam_eps,\n            max_grad_norm=cfg.max_grad_norm,\n            critic_tau=cfg.critic_tau,\n            critic_target_update_frequency=cfg.critic_target_update_frequency,\n            batch_size=cfg.batch_size,\n            multistep_return=cfg.multistep_return,\n            eval_eps=cfg.eval_eps,\n            double_q=cfg.double_q,\n            prioritized_replay_beta0=cfg.prioritized_replay_beta0,\n            prioritized_replay_beta_steps=cfg.prioritized_replay_beta_steps,\n            env_name=cfg.env,\n            seed=cfg.seed,\n            hidden_dim=cfg.hidden_dim, hidden_depth=cfg.hidden_depth,\n            dueling=cfg.dueling, aug_type=cfg.aug_type,\n            image_pad=cfg.image_pad, intensity_scale=cfg.intensity_scale,\n        )\n        if cfg.prioritized_replay:\n            self.replay_buffer = PrioritizedReplayBuffer(\n                self.env.observation_space.shape, cfg.replay_buffer_capacity,\n                cfg.prioritized_replay_alpha, self.device)\n        else:\n            self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n                                              cfg.replay_buffer_capacity,\n                                              self.device)\n\n        self.step = 0\n\n    def run(self):\n        episode, episode_reward, episode_step, done = 0, 0, 1, True\n\n        for placeholder in trange(self.cfg.num_train_steps):\n        # while self.step < self.cfg.num_train_steps:\n            if done:\n                print(\n                    f\"Algorithm: DrQ PEER,  Env: {self.cfg.env}, Num Step: {self.step}, Episode Return: {episode_reward}\")\n\n                obs = self.env.reset()\n                done = False\n                episode_reward = 0\n                episode_step = 0\n                episode += 1\n\n\n            steps_left = self.cfg.num_exploration_steps + self.cfg.start_training_steps - self.step\n            bonus = (1.0 - self.cfg.min_eps\n                     ) * steps_left / self.cfg.num_exploration_steps\n            bonus = np.clip(bonus, 0., 1. - self.cfg.min_eps)\n            self.agent.eps = self.cfg.min_eps + bonus\n\n\n            # sample action for data collection\n            if np.random.rand() < self.agent.eps:\n                action = self.env.action_space.sample()\n            else:\n                with utils.eval_mode(self.agent):\n                    action = self.agent.act(obs)\n\n            # run training update\n            if self.step >= self.cfg.start_training_steps:\n                for _ in range(self.cfg.num_train_iters):\n                    self.agent.update(self.replay_buffer, None,\n                                      self.step)\n                    # print(\"Train \" * 30)\n\n            next_obs, reward, terminal, info = self.env.step(action)\n\n            time_limit = 'TimeLimit.truncated' in info\n            done = info['game_over'] or time_limit\n\n            terminal = float(terminal)\n            terminal_no_max = 0 if time_limit else terminal\n\n            episode_reward += reward\n\n            self.replay_buffer.add(obs, action, reward, next_obs,\n                                   terminal_no_max)\n\n            obs = next_obs\n            episode_step += 1\n            self.step += 1", "\n\ndef main(cfg):\n    # from train import Workspace as W\n    workspace = Workspace(cfg)\n    workspace.run()\n\n\nparser = argparse.ArgumentParser(\"DrQ-Atari-100k\")\nparser.add_argument(\"--env\", default=\"Breakout\", type=str)", "parser = argparse.ArgumentParser(\"DrQ-Atari-100k\")\nparser.add_argument(\"--env\", default=\"Breakout\", type=str)\nparser.add_argument('--seed', default=1, type=int)\nparser.add_argument('--gpu_idx', default=0, type=int)\n# parser.add_argument('--debug', action=\"store_true\", type=bool)\nargs = parser.parse_args()\n\nargs.debug = False\nif args.debug == True:\n    print(\"-\" * 30)\n    print(\"Mode: Debug\")\n    print(\"-\"*30)\n    print()\n    print()\n    time.sleep(3)", "if args.debug == True:\n    print(\"-\" * 30)\n    print(\"Mode: Debug\")\n    print(\"-\"*30)\n    print()\n    print()\n    time.sleep(3)\n\ncfg=utils.CFG(env=args.env,\n        seed=args.seed,", "cfg=utils.CFG(env=args.env,\n        seed=args.seed,\n        debug=args.debug\n        )\n\nmain(cfg)\n"]}
{"filename": "atari/PEER-DrQ/segment_tree.py", "chunked_list": ["import operator\n\n\nclass SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        \"\"\"Build a Segment Tree data structure.\n\n        https://en.wikipedia.org/wiki/Segment_tree\n\n        Can be used as regular array, but with two\n        important differences:\n\n            a) setting item's value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient ( O(log segment size) )\n               `reduce` operation which reduces `operation` over\n               a contiguous subsequence of items in the array.\n\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must form a mathematical group together with the set of\n            possible values for array elements (i.e. be associative)\n        neutral_element: obj\n            neutral element for the operation above. eg. float('-inf')\n            for max and 0 for sum.\n        \"\"\"\n        assert capacity > 0 and capacity & (\n            capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1,\n                                           node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1,\n                                        node_end))\n\n    def reduce(self, start=0, end=None):\n        \"\"\"Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        \"\"\"\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(self._value[2 * idx],\n                                               self._value[2 * idx + 1])\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]", "\n\nclass SumSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(capacity=capacity,\n                                             operation=operator.add,\n                                             neutral_element=0.0)\n\n    def sum(self, start=0, end=None):\n        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n        return super(SumSegmentTree, self).reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        \"\"\"Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        \"\"\"\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity", "\n\nclass MinSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(capacity=capacity,\n                                             operation=min,\n                                             neutral_element=float('inf'))\n\n    def min(self, start=0, end=None):\n        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n\n        return super(MinSegmentTree, self).reduce(start, end)", ""]}
{"filename": "atari/PEER-DrQ/utils.py", "chunked_list": ["import math\nimport os\nimport random\nfrom collections import deque\n\nimport numpy as np\n\nimport gym\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import distributions as pyd\nimport pickle\nimport pandas as pd\n\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "class eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False", "\n\ndef soft_update_params(net, target_net, tau):\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(tau * param.data +\n                                (1 - tau) * target_param.data)\n\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)", "def set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef make_dir(*path_parts):\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path", "def make_dir(*path_parts):\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n\n\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias", "\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias\n\n\ndef weight_init(m):\n    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal_(m.weight.data)\n        if hasattr(m.bias, 'data'):\n            m.bias.data.fill_(0.0)\n    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.orthogonal_(m.weight.data, gain)\n        if hasattr(m.bias, 'data'):\n            m.bias.data.fill_(0.0)", "\n\ndef mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n    if hidden_depth == 0:\n        mods = [nn.Linear(input_dim, output_dim)]\n    else:\n        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n        for i in range(hidden_depth - 1):\n            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n        mods.append(nn.Linear(hidden_dim, output_dim))\n    if output_mod is not None:\n        mods.append(output_mod)\n    trunk = nn.Sequential(*mods)\n    return trunk", "\n\ndef to_np(t):\n    if t is None:\n        return None\n    elif t.nelement() == 0:\n        return np.array([])\n    else:\n        return t.cpu().detach().numpy()\n", "\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self._k = k\n        self._frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=1,\n            shape=((shp[0] * k,) + shp[1:]),\n            dtype=env.observation_space.dtype)\n        self._max_episode_steps = env._max_episode_steps\n\n    def reset(self):\n        obs = self.env.reset()\n        for _ in range(self._k):\n            self._frames.append(obs)\n        return self._get_obs()\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self._frames.append(obs)\n        return self._get_obs(), reward, done, info\n\n    def _get_obs(self):\n        assert len(self._frames) == self._k\n        return np.concatenate(list(self._frames), axis=0)", "\n\nclass TanhTransform(pyd.transforms.Transform):\n    domain = pyd.constraints.real\n    codomain = pyd.constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    def __init__(self, cache_size=1):\n        super().__init__(cache_size=cache_size)\n\n    @staticmethod\n    def atanh(x):\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        return x.tanh()\n\n    def _inverse(self, y):\n        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n        # one should use `cache_size=1` instead\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2. * (math.log(2.) - x - F.softplus(-2. * x))", "\n\nclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n    def __init__(self, loc, scale):\n        self.loc = loc\n        self.scale = scale\n\n        self.base_dist = pyd.Normal(loc, scale)\n        transforms = [TanhTransform()]\n        super().__init__(self.base_dist, transforms)\n\n    @property\n    def mean(self):\n        mu = self.loc\n        for tr in self.transforms:\n            mu = tr(mu)\n        return mu", "\n\n\nclass CFG():\n    env = \"Breakout\"\n    debug=False\n    terminal_on_life_loss = True\n    # train\n    num_train_steps = 100000\n    num_train_iters = 1\n    num_exploration_steps = 5000\n    start_training_steps = 1600 #\n    min_eps = 0.1\n    prioritized_replay = False\n    prioritized_replay_alpha = 0.6\n    seed = 1\n    discount=0.99\n    eval_frequency = 100000\n    num_eval_steps = 100000\n    log_frequency_step = 10000\n    log_save_tb = False\n    save_video = False\n    save_train_video = False\n    device = \"cuda\"\n    # observation\n    image_pad = 4\n    intensity_scale = 0.1\n    aug_type = \"all\"\n    # global params\n    lr = 0.0001\n    beta_1 = 0.9\n    beta_2 = 0.999\n    weight_decay = 0.0\n    adam_eps = 0.00015\n    max_grad_norm = 10.0\n    hidden_depth = 1\n    batch_size = 32\n\n    # agent\n    obs_shape= 100 # to be specified later\n    num_actions= 100\n    critic_tau= 1.0\n    critic_target_update_frequency= 1\n    multistep_return= 10\n    eval_eps= 0.05\n    double_q = True\n    prioritized_replay_beta0= 0.4\n\n    # critic\n    hidden_dim= 512\n    dueling= True\n\n    def __init__(self, env, seed, debug):\n        self.replay_buffer_capacity = self.num_train_steps\n        self.prioritized_replay_beta_steps = self.num_train_steps\n        self.env = env\n        self.seed = int(seed)\n        self.debug = debug", "\n"]}
{"filename": "atari/PEER-DrQ/replay_buffer.py", "chunked_list": ["import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport utils\n\nfrom segment_tree import SumSegmentTree, MinSegmentTree\n\n\nclass ReplayBuffer(object):\n    def __init__(self, obs_shape, capacity, device):\n        self.capacity = capacity\n        self.device = device\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, 1), dtype=np.int64)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def fetch(self, idxs, discount, n):\n        assert idxs.max() + n <= len(self)\n\n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs + n - 1]\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = np.zeros((idxs.shape[0], 1), dtype=np.float32)\n        not_dones = np.ones((idxs.shape[0], 1), dtype=np.float32)\n        for i in range(n):\n            rewards += (discount**n) * not_dones * np.sign(\n                self.rewards[idxs + i])\n            not_dones = np.minimum(not_dones, self.not_dones[idxs + i])\n\n        rewards = torch.as_tensor(rewards, device=self.device)\n        not_dones = torch.as_tensor(not_dones, device=self.device)\n\n        return obses, actions, rewards, next_obses, not_dones\n\n    def sample_idxs(self, batch_size, n):\n        last_idx = (self.capacity if self.full else self.idx) - (n - 1)\n        idxs = np.random.randint(0, last_idx, size=batch_size)\n\n        return idxs\n\n    def sample_multistep(self, batch_size, discount, n):\n        assert n <= self.idx or self.full\n        idxs = self.sample_idxs(batch_size, n)\n\n        return self.fetch(idxs, discount, n)", "\n\nclass ReplayBuffer(object):\n    def __init__(self, obs_shape, capacity, device):\n        self.capacity = capacity\n        self.device = device\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, 1), dtype=np.int64)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def fetch(self, idxs, discount, n):\n        assert idxs.max() + n <= len(self)\n\n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs + n - 1]\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = np.zeros((idxs.shape[0], 1), dtype=np.float32)\n        not_dones = np.ones((idxs.shape[0], 1), dtype=np.float32)\n        for i in range(n):\n            rewards += (discount**n) * not_dones * np.sign(\n                self.rewards[idxs + i])\n            not_dones = np.minimum(not_dones, self.not_dones[idxs + i])\n\n        rewards = torch.as_tensor(rewards, device=self.device)\n        not_dones = torch.as_tensor(not_dones, device=self.device)\n\n        return obses, actions, rewards, next_obses, not_dones\n\n    def sample_idxs(self, batch_size, n):\n        last_idx = (self.capacity if self.full else self.idx) - (n - 1)\n        idxs = np.random.randint(0, last_idx, size=batch_size)\n\n        return idxs\n\n    def sample_multistep(self, batch_size, discount, n):\n        assert n <= self.idx or self.full\n        idxs = self.sample_idxs(batch_size, n)\n\n        return self.fetch(idxs, discount, n)", "\n\nclass PrioritizedReplayBuffer(ReplayBuffer):\n    def __init__(self, obs_shape, capacity, alpha, device):\n        super().__init__(obs_shape, capacity, device)\n\n        assert alpha >= 0\n        self.alpha = alpha\n\n        tree_capacity = 1\n        while tree_capacity < capacity:\n            tree_capacity *= 2\n\n        self.sum_tree = SumSegmentTree(tree_capacity)\n        self.min_tree = MinSegmentTree(tree_capacity)\n        self.max_priority = 1.0\n\n    def add(self, obs, action, reward, next_obs, done):\n        super().add(obs, action, reward, next_obs, done)\n        self.sum_tree[self.idx] = self.max_priority**self.alpha\n        self.min_tree[self.idx] = self.max_priority**self.alpha\n\n    def sample_idxs(self, batch_size, n):\n        idxs = []\n        p_total = self.sum_tree.sum(0, len(self) - n - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            while True:\n                mass = np.random.rand() * every_range_len + i * every_range_len\n                idx = self.sum_tree.find_prefixsum_idx(mass)\n                if idx + n <= len(self):\n                    idxs.append(idx)\n                    break\n        return np.array(idxs)\n\n    def sample_multistep(self, batch_size, beta, discount, n):\n        assert n <= self.idx or self.full\n        assert beta > 0\n\n        idxs = self.sample_idxs(batch_size, n)\n\n        weights = []\n        p_min = self.min_tree.min() / self.sum_tree.sum()\n        max_weight = (p_min * len(self))**(-beta)\n\n        for idx in idxs:\n            p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n            weight = (p_sample * len(self))**(-beta)\n            weights.append(weight / max_weight)\n        weights = torch.as_tensor(np.array(weights),\n                                  device=self.device).unsqueeze(dim=1)\n\n        sample = self.fetch(idxs, discount, n)\n\n        return tuple(list(sample) + [weights, idxs])\n\n    def update_priorities(self, idxs, prios):\n        assert idxs.shape[0] == prios.shape[0]\n\n        for idx, prio in zip(idxs, prios):\n            assert prio > 0\n            assert 0 <= idx < len(self)\n\n            self.sum_tree[idx] = prio**self.alpha\n            self.min_tree[idx] = prio**self.alpha\n\n            self.max_priority = max(self.max_priority, prio)", ""]}
{"filename": "atari/PEER-DrQ/atari.py", "chunked_list": ["# coding=utf-8\n# Copyright 2019 The SEED Authors\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A class implementing minimal Atari 2600 preprocessing.\n\nAdapted from Dopamine.\n\"\"\"\n", "\"\"\"\n\nfrom gym.spaces.box import Box\nimport numpy as np\nimport gym\nfrom collections import deque\n\nimport cv2\n\n\nclass AtariPreprocessing(object):\n    \"\"\"A class implementing image preprocessing for Atari 2600 agents.\n\n    Specifically, this provides the following subset from the JAIR paper\n    (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\n    * Frame skipping (defaults to 4).\n    * Terminal signal when a life is lost (off by default).\n    * Grayscale and max-pooling of the last two frames.\n    * Downsample the screen to a square image (defaults to 84x84).\n\n    More generally, this class follows the preprocessing guidelines set down in\n    Machado et al. (2018), \"Revisiting the Arcade Learning Environment:\n    Evaluation Protocols and Open Problems for General Agents\".\n\n    It also provides random starting no-ops, which are used in the Rainbow, Apex\n    and R2D2 papers.\n    \"\"\"\n    def __init__(self,\n                 environment,\n                 frame_skip=4,\n                 terminal_on_life_loss=False,\n                 screen_size=84,\n                 max_random_noops=0):\n        \"\"\"Constructor for an Atari 2600 preprocessor.\n\n        Args:\n          environment: Gym environment whose observations are preprocessed.\n          frame_skip: int, the frequency at which the agent experiences the game.\n          terminal_on_life_loss: bool, If True, the step() method returns\n            is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n          screen_size: int, size of a resized Atari 2600 frame.\n          max_random_noops: int, maximum number of no-ops to apply at the beginning\n            of each episode to reduce determinism. These no-ops are applied at a\n            low-level, before frame skipping.\n\n        Raises:\n          ValueError: if frame_skip or screen_size are not strictly positive.\n        \"\"\"\n        if frame_skip <= 0:\n            raise ValueError(\n                'Frame skip should be strictly positive, got {}'.format(\n                    frame_skip))\n        if screen_size <= 0:\n            raise ValueError(\n                'Target screen size should be strictly positive, got {}'.\n                format(screen_size))\n\n        self.environment = environment\n        self.terminal_on_life_loss = terminal_on_life_loss\n        self.frame_skip = frame_skip\n        self.screen_size = screen_size\n        self.max_random_noops = max_random_noops\n\n        obs_dims = self.environment.observation_space\n        # Stores temporary observations used for pooling over two successive\n        # frames.\n        self.screen_buffer = [\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n        ]\n\n        self.game_over = False\n        self.lives = 0  # Will need to be set by reset().\n\n    @property\n    def observation_space(self):\n        # Return the observation space adjusted to match the shape of the processed\n        # observations.\n        return Box(low=0,\n                   high=255,\n                   shape=(self.screen_size, self.screen_size, 1),\n                   dtype=np.uint8)\n\n    @property\n    def action_space(self):\n        return self.environment.action_space\n\n    @property\n    def reward_range(self):\n        return self.environment.reward_range\n\n    @property\n    def metadata(self):\n        return self.environment.metadata\n\n    def close(self):\n        return self.environment.close()\n\n    def apply_random_noops(self):\n        \"\"\"Steps self.environment with random no-ops.\"\"\"\n        if self.max_random_noops <= 0:\n            return\n        # Other no-ops implementations actually always do at least 1 no-op. We\n        # follow them.\n        no_ops = self.environment.np_random.randint(1,\n                                                    self.max_random_noops + 1)\n        for _ in range(no_ops):\n            _, _, game_over, _ = self.environment.step(0)\n            if game_over:\n                self.environment.reset()\n\n    def reset(self):\n        \"\"\"Resets the environment.\n\n        Returns:\n          observation: numpy array, the initial observation emitted by the\n            environment.\n        \"\"\"\n        self.environment.reset()\n        self.apply_random_noops()\n\n        self.lives = self.environment.ale.lives()\n        self._fetch_grayscale_observation(self.screen_buffer[0])\n        self.screen_buffer[1].fill(0)\n        return self._pool_and_resize()\n\n    def render(self, mode):\n        \"\"\"Renders the current screen, before preprocessing.\n\n        This calls the Gym API's render() method.\n\n        Args:\n          mode: Mode argument for the environment's render() method.\n            Valid values (str) are:\n              'rgb_array': returns the raw ALE image.\n              'human': renders to display via the Gym renderer.\n\n        Returns:\n          if mode='rgb_array': numpy array, the most recent screen.\n          if mode='human': bool, whether the rendering was successful.\n        \"\"\"\n        return self.environment.render(mode)\n\n    def step(self, action):\n        \"\"\"Applies the given action in the environment.\n\n        Remarks:\n\n          * If a terminal state (from life loss or episode end) is reached, this may\n            execute fewer than self.frame_skip steps in the environment.\n          * Furthermore, in this case the returned observation may not contain valid\n            image data and should be ignored.\n\n        Args:\n          action: The action to be executed.\n\n        Returns:\n          observation: numpy array, the observation following the action.\n          reward: float, the reward following the action.\n          is_terminal: bool, whether the environment has reached a terminal state.\n            This is true when a life is lost and terminal_on_life_loss, or when the\n            episode is over.\n          info: Gym API's info data structure.\n        \"\"\"\n        accumulated_reward = 0.\n\n        for time_step in range(self.frame_skip):\n            # We bypass the Gym observation altogether and directly fetch the\n            # grayscale image from the ALE. This is a little faster.\n            _, reward, game_over, info = self.environment.step(action)\n            accumulated_reward += reward\n            info['game_over'] = game_over\n\n            if self.terminal_on_life_loss:\n                new_lives = self.environment.ale.lives()\n                is_terminal = game_over or new_lives < self.lives\n                self.lives = new_lives\n            else:\n                is_terminal = game_over\n\n            if is_terminal:\n                break\n            # We max-pool over the last two frames, in grayscale.\n            elif time_step >= self.frame_skip - 2:\n                t = time_step - (self.frame_skip - 2)\n                self._fetch_grayscale_observation(self.screen_buffer[t])\n\n        # Pool the last two observations.\n        observation = self._pool_and_resize()\n\n        self.game_over = game_over\n        return observation, accumulated_reward, is_terminal, info\n\n    def _fetch_grayscale_observation(self, output):\n        \"\"\"Returns the current observation in grayscale.\n\n        The returned observation is stored in 'output'.\n\n        Args:\n          output: numpy array, screen buffer to hold the returned observation.\n\n        Returns:\n          observation: numpy array, the current observation in grayscale.\n        \"\"\"\n        self.environment.ale.getScreenGrayscale(output)\n        return output\n\n    def _pool_and_resize(self):\n        \"\"\"Transforms two frames into a Nature DQN observation.\n\n        For efficiency, the transformation is done in-place in self.screen_buffer.\n\n        Returns:\n          transformed_screen: numpy array, pooled, resized screen.\n        \"\"\"\n        # Pool if there are enough screens to do so.\n        if self.frame_skip > 1:\n            np.maximum(self.screen_buffer[0],\n                       self.screen_buffer[1],\n                       out=self.screen_buffer[0])\n\n        transformed_image = cv2.resize(self.screen_buffer[0],\n                                       (self.screen_size, self.screen_size),\n                                       interpolation=cv2.INTER_LINEAR)\n        int_image = np.asarray(transformed_image, dtype=np.uint8)\n        return np.expand_dims(int_image, axis=2)", "\n\nclass AtariPreprocessing(object):\n    \"\"\"A class implementing image preprocessing for Atari 2600 agents.\n\n    Specifically, this provides the following subset from the JAIR paper\n    (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\n    * Frame skipping (defaults to 4).\n    * Terminal signal when a life is lost (off by default).\n    * Grayscale and max-pooling of the last two frames.\n    * Downsample the screen to a square image (defaults to 84x84).\n\n    More generally, this class follows the preprocessing guidelines set down in\n    Machado et al. (2018), \"Revisiting the Arcade Learning Environment:\n    Evaluation Protocols and Open Problems for General Agents\".\n\n    It also provides random starting no-ops, which are used in the Rainbow, Apex\n    and R2D2 papers.\n    \"\"\"\n    def __init__(self,\n                 environment,\n                 frame_skip=4,\n                 terminal_on_life_loss=False,\n                 screen_size=84,\n                 max_random_noops=0):\n        \"\"\"Constructor for an Atari 2600 preprocessor.\n\n        Args:\n          environment: Gym environment whose observations are preprocessed.\n          frame_skip: int, the frequency at which the agent experiences the game.\n          terminal_on_life_loss: bool, If True, the step() method returns\n            is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n          screen_size: int, size of a resized Atari 2600 frame.\n          max_random_noops: int, maximum number of no-ops to apply at the beginning\n            of each episode to reduce determinism. These no-ops are applied at a\n            low-level, before frame skipping.\n\n        Raises:\n          ValueError: if frame_skip or screen_size are not strictly positive.\n        \"\"\"\n        if frame_skip <= 0:\n            raise ValueError(\n                'Frame skip should be strictly positive, got {}'.format(\n                    frame_skip))\n        if screen_size <= 0:\n            raise ValueError(\n                'Target screen size should be strictly positive, got {}'.\n                format(screen_size))\n\n        self.environment = environment\n        self.terminal_on_life_loss = terminal_on_life_loss\n        self.frame_skip = frame_skip\n        self.screen_size = screen_size\n        self.max_random_noops = max_random_noops\n\n        obs_dims = self.environment.observation_space\n        # Stores temporary observations used for pooling over two successive\n        # frames.\n        self.screen_buffer = [\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n        ]\n\n        self.game_over = False\n        self.lives = 0  # Will need to be set by reset().\n\n    @property\n    def observation_space(self):\n        # Return the observation space adjusted to match the shape of the processed\n        # observations.\n        return Box(low=0,\n                   high=255,\n                   shape=(self.screen_size, self.screen_size, 1),\n                   dtype=np.uint8)\n\n    @property\n    def action_space(self):\n        return self.environment.action_space\n\n    @property\n    def reward_range(self):\n        return self.environment.reward_range\n\n    @property\n    def metadata(self):\n        return self.environment.metadata\n\n    def close(self):\n        return self.environment.close()\n\n    def apply_random_noops(self):\n        \"\"\"Steps self.environment with random no-ops.\"\"\"\n        if self.max_random_noops <= 0:\n            return\n        # Other no-ops implementations actually always do at least 1 no-op. We\n        # follow them.\n        no_ops = self.environment.np_random.randint(1,\n                                                    self.max_random_noops + 1)\n        for _ in range(no_ops):\n            _, _, game_over, _ = self.environment.step(0)\n            if game_over:\n                self.environment.reset()\n\n    def reset(self):\n        \"\"\"Resets the environment.\n\n        Returns:\n          observation: numpy array, the initial observation emitted by the\n            environment.\n        \"\"\"\n        self.environment.reset()\n        self.apply_random_noops()\n\n        self.lives = self.environment.ale.lives()\n        self._fetch_grayscale_observation(self.screen_buffer[0])\n        self.screen_buffer[1].fill(0)\n        return self._pool_and_resize()\n\n    def render(self, mode):\n        \"\"\"Renders the current screen, before preprocessing.\n\n        This calls the Gym API's render() method.\n\n        Args:\n          mode: Mode argument for the environment's render() method.\n            Valid values (str) are:\n              'rgb_array': returns the raw ALE image.\n              'human': renders to display via the Gym renderer.\n\n        Returns:\n          if mode='rgb_array': numpy array, the most recent screen.\n          if mode='human': bool, whether the rendering was successful.\n        \"\"\"\n        return self.environment.render(mode)\n\n    def step(self, action):\n        \"\"\"Applies the given action in the environment.\n\n        Remarks:\n\n          * If a terminal state (from life loss or episode end) is reached, this may\n            execute fewer than self.frame_skip steps in the environment.\n          * Furthermore, in this case the returned observation may not contain valid\n            image data and should be ignored.\n\n        Args:\n          action: The action to be executed.\n\n        Returns:\n          observation: numpy array, the observation following the action.\n          reward: float, the reward following the action.\n          is_terminal: bool, whether the environment has reached a terminal state.\n            This is true when a life is lost and terminal_on_life_loss, or when the\n            episode is over.\n          info: Gym API's info data structure.\n        \"\"\"\n        accumulated_reward = 0.\n\n        for time_step in range(self.frame_skip):\n            # We bypass the Gym observation altogether and directly fetch the\n            # grayscale image from the ALE. This is a little faster.\n            _, reward, game_over, info = self.environment.step(action)\n            accumulated_reward += reward\n            info['game_over'] = game_over\n\n            if self.terminal_on_life_loss:\n                new_lives = self.environment.ale.lives()\n                is_terminal = game_over or new_lives < self.lives\n                self.lives = new_lives\n            else:\n                is_terminal = game_over\n\n            if is_terminal:\n                break\n            # We max-pool over the last two frames, in grayscale.\n            elif time_step >= self.frame_skip - 2:\n                t = time_step - (self.frame_skip - 2)\n                self._fetch_grayscale_observation(self.screen_buffer[t])\n\n        # Pool the last two observations.\n        observation = self._pool_and_resize()\n\n        self.game_over = game_over\n        return observation, accumulated_reward, is_terminal, info\n\n    def _fetch_grayscale_observation(self, output):\n        \"\"\"Returns the current observation in grayscale.\n\n        The returned observation is stored in 'output'.\n\n        Args:\n          output: numpy array, screen buffer to hold the returned observation.\n\n        Returns:\n          observation: numpy array, the current observation in grayscale.\n        \"\"\"\n        self.environment.ale.getScreenGrayscale(output)\n        return output\n\n    def _pool_and_resize(self):\n        \"\"\"Transforms two frames into a Nature DQN observation.\n\n        For efficiency, the transformation is done in-place in self.screen_buffer.\n\n        Returns:\n          transformed_screen: numpy array, pooled, resized screen.\n        \"\"\"\n        # Pool if there are enough screens to do so.\n        if self.frame_skip > 1:\n            np.maximum(self.screen_buffer[0],\n                       self.screen_buffer[1],\n                       out=self.screen_buffer[0])\n\n        transformed_image = cv2.resize(self.screen_buffer[0],\n                                       (self.screen_size, self.screen_size),\n                                       interpolation=cv2.INTER_LINEAR)\n        int_image = np.asarray(transformed_image, dtype=np.uint8)\n        return np.expand_dims(int_image, axis=2)", "\n\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        super().__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info['TimeLimit.truncated'] = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)", "\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        super().__init__(env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(shp[:-1] + (shp[-1] * k,)),\n            dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return np.concatenate(self.frames, axis=-1)", "\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        old_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n            dtype=np.uint8,\n        )\n\n    def observation(self, observation):\n        return np.transpose(observation, axes=(2, 0, 1))", "\n\ndef make_env(env, seed, terminal_on_life_loss):\n    # Also try with sticky actions, instead of random noops\n    env_id = f'{env}NoFrameskip-v4'\n    env = gym.make(env_id)\n    env.seed(seed)\n\n    # terminal_on_life_loss: see section 4.1 in https://arxiv.org/pdf/1812.06110.pdf\n\n    # use env.env to remove TimeLimit\n    env = AtariPreprocessing(env.env,\n                             frame_skip=4,\n                             max_random_noops=0,\n                             terminal_on_life_loss=terminal_on_life_loss)\n    env = TimeLimit(env, max_episode_steps=27000)\n    env = FrameStack(env, k=4)\n    env = ImageToPyTorch(env)\n\n    return env", ""]}
{"filename": "atari/PEER-CURL/model.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2017 Kai Arulkumaran\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# ==============================================================================\nfrom __future__ import division", "# ==============================================================================\nfrom __future__ import division\n\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n", "\n\n# Factorised NoisyLinear layer with bias\nclass NoisyLinear(nn.Module):\n    def __init__(self, in_features, out_features, std_init=0.5):\n        super(NoisyLinear, self).__init__()\n        self.module_name = 'noisy_linear'\n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = std_init\n        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n        self.bias_mu = nn.Parameter(torch.empty(out_features))\n        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n        self.register_buffer('bias_epsilon', torch.empty(out_features))\n        self.reset_parameters()\n        self.reset_noise()\n\n    def reset_parameters(self):\n        mu_range = 1 / math.sqrt(self.in_features)\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n\n    def _scale_noise(self, size):\n        x = torch.randn(size)\n        return x.sign().mul_(x.abs().sqrt_())\n\n    def reset_noise(self):\n        epsilon_in = self._scale_noise(self.in_features)\n        epsilon_out = self._scale_noise(self.out_features)\n        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n        self.bias_epsilon.copy_(epsilon_out)\n\n    def forward(self, input):\n        if self.training:\n            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon,\n                            self.bias_mu + self.bias_sigma * self.bias_epsilon)\n        else:\n            return F.linear(input, self.weight_mu, self.bias_mu)", "\n\nclass DQN(nn.Module):\n    def __init__(self, args, action_space):\n        super(DQN, self).__init__()\n        self.atoms = args.atoms\n        self.action_space = action_space\n\n        if args.architecture == 'canonical':\n            self.convs = nn.Sequential(nn.Conv2d(args.history_length, 32, 8, stride=4, padding=0), nn.ReLU(),\n                                       nn.Conv2d(32, 64, 4, stride=2, padding=0), nn.ReLU(),\n                                       nn.Conv2d(64, 64, 3, stride=1, padding=0), nn.ReLU())\n            self.conv_output_size = 3136\n        elif args.architecture == 'data-efficient':\n            self.convs = nn.Sequential(nn.Conv2d(args.history_length, 32, 5, stride=5, padding=0), nn.ReLU(),\n                                       nn.Conv2d(32, 64, 5, stride=5, padding=0), nn.ReLU())\n            self.conv_output_size = 576\n        self.fc_h_v = NoisyLinear(self.conv_output_size, args.hidden_size, std_init=args.noisy_std)\n        self.fc_h_a = NoisyLinear(self.conv_output_size, args.hidden_size, std_init=args.noisy_std)\n        self.fc_z_v = NoisyLinear(args.hidden_size, self.atoms, std_init=args.noisy_std)\n        self.fc_z_a = NoisyLinear(args.hidden_size, action_space * self.atoms, std_init=args.noisy_std)\n\n        self.W_h = nn.Parameter(torch.rand(self.conv_output_size, args.hidden_size))\n        self.W_c = nn.Parameter(torch.rand(args.hidden_size, 128))\n        self.b_h = nn.Parameter(torch.zeros(args.hidden_size))\n        self.b_c = nn.Parameter(torch.zeros(128))\n        self.W = nn.Parameter(torch.rand(128, 128))\n\n        # self.layernorm1 = nn.LayerNorm(256).to(\"cuda\")\n        # self.layernorm2 = nn.LayerNorm(128).to(\"cuda\")\n\n    def forward(self, x, log=False, representation=False):\n        x = self.convs(x)\n        x = x.view(-1, self.conv_output_size) # X is representation\n        v = self.fc_z_v(F.relu(self.fc_h_v(x)))  # Value stream\n        a = self.fc_z_a(F.relu(self.fc_h_a(x)))  # Advantage stream\n        h = torch.matmul(x, self.W_h) + self.b_h  # Contrastive head\n        # h = self.layernorm1(h)\n        h = nn.LayerNorm(h.shape[1], device=\"cuda\")(h)\n        h = F.relu(h)\n        h = torch.matmul(h, self.W_c) + self.b_c  # Contrastive head\n        # h = self.layernorm2(h)\n        h = nn.LayerNorm(128, device=\"cuda\")(h)\n        v, a = v.view(-1, 1, self.atoms), a.view(-1, self.action_space, self.atoms)\n        q = v + a - a.mean(1, keepdim=True)  # Combine streams\n        if log:  # Use log softmax for numerical stability\n            q = F.log_softmax(q, dim=2)  # Log probabilities with action over second dimension\n        else:\n            q = F.softmax(q, dim=2)  # Probabilities with action over second dimension\n        if representation:\n            return q, h, x\n        else:\n            return q, h\n\n    def reset_noise(self):\n        for name, module in self.named_children():\n            if 'fc' in name:\n                module.reset_noise()", ""]}
{"filename": "atari/PEER-CURL/main.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2017 Kai Arulkumaran\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# ==============================================================================\nfrom __future__ import division", "# ==============================================================================\nfrom __future__ import division\n\nimport argparse\nimport bz2\nimport os\nimport pickle\nfrom datetime import datetime\n\nimport atari_py", "\nimport atari_py\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\nfrom agent import Agent\nfrom env import Env\nfrom memory import ReplayMemory\nfrom utils import set_seed_everywhere", "from memory import ReplayMemory\nfrom utils import set_seed_everywhere\n\n\n# Note that hyperparameters may originally be reported in ATARI game frames instead of agent steps\nparser = argparse.ArgumentParser(description='PEER')\nparser.add_argument('--id', type=str, default='default', help='Experiment ID')\nparser.add_argument('--seed', type=int, default=0, help='Random seed')\nparser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\nparser.add_argument('--game', type=str, default='ms_pacman', choices=atari_py.list_games(), help='ATARI game')", "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\nparser.add_argument('--game', type=str, default='ms_pacman', choices=atari_py.list_games(), help='ATARI game')\nparser.add_argument('--T-max', type=int, default=int(1e5), metavar='STEPS',\n                    help='Number of training steps (4x number of frames)')\nparser.add_argument('--max-episode-length', type=int, default=int(108e3), metavar='LENGTH',\n                    help='Max episode length in game frames (0 to disable)')\nparser.add_argument('--history-length', type=int, default=4, metavar='T', help='Number of consecutive states processed')\nparser.add_argument('--architecture', type=str, default='data-efficient', choices=['canonical', 'data-efficient'],\n                    metavar='ARCH', help='Network architecture')\nparser.add_argument('--hidden-size', type=int, default=256, metavar='SIZE', help='Network hidden size')", "                    metavar='ARCH', help='Network architecture')\nparser.add_argument('--hidden-size', type=int, default=256, metavar='SIZE', help='Network hidden size')\nparser.add_argument('--noisy-std', type=float, default=0.1, metavar='\u03c3',\n                    help='Initial standard deviation of noisy linear layers')\nparser.add_argument('--atoms', type=int, default=51, metavar='C', help='Discretised size of value distribution')\nparser.add_argument('--V-min', type=float, default=-10, metavar='V', help='Minimum of value distribution support')\nparser.add_argument('--V-max', type=float, default=10, metavar='V', help='Maximum of value distribution support')\nparser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model (state dict)')\nparser.add_argument('--memory-capacity', type=int, default=int(1e5), metavar='CAPACITY',\n                    help='Experience replay memory capacity')", "parser.add_argument('--memory-capacity', type=int, default=int(1e5), metavar='CAPACITY',\n                    help='Experience replay memory capacity')\nparser.add_argument('--replay-frequency', type=int, default=1, metavar='k', help='Frequency of sampling from memory')\nparser.add_argument('--priority-exponent', type=float, default=0.5, metavar='\u03c9',\n                    help='Prioritised experience replay exponent (originally denoted \u03b1)')\nparser.add_argument('--priority-weight', type=float, default=0.4, metavar='\u03b2',\n                    help='Initial prioritised experience replay importance sampling weight')\nparser.add_argument('--multi-step', type=int, default=20, metavar='n', help='Number of steps for multi-step return')\nparser.add_argument('--discount', type=float, default=0.99, metavar='\u03b3', help='Discount factor')\nparser.add_argument('--target-update', type=int, default=int(2e3), metavar='\u03c4',", "parser.add_argument('--discount', type=float, default=0.99, metavar='\u03b3', help='Discount factor')\nparser.add_argument('--target-update', type=int, default=int(2e3), metavar='\u03c4',\n                    help='Number of steps after which to update target network')\nparser.add_argument('--reward-clip', type=int, default=1, metavar='VALUE', help='Reward clipping (0 to disable)')\nparser.add_argument('--learning-rate', type=float, default=0.0001, metavar='\u03b7', help='Learning rate')\nparser.add_argument('--adam-eps', type=float, default=1.5e-5, metavar='\u03b5', help='Adam epsilon')\nparser.add_argument('--batch-size', type=int, default=32, metavar='SIZE', help='Batch size') # May be we\nparser.add_argument('--norm-clip', type=float, default=10, metavar='NORM', help='Max L2 norm for gradient clipping')\nparser.add_argument('--learn-start', type=int, default=int(1600), metavar='STEPS',\n                    help='Number of steps before starting training')", "parser.add_argument('--learn-start', type=int, default=int(1600), metavar='STEPS',\n                    help='Number of steps before starting training')\nparser.add_argument('--evaluate', action='store_true', help='Evaluate only')\nparser.add_argument('--evaluation-interval', type=int, default=10000, metavar='STEPS',\n                    help='Number of training steps between evaluations')\nparser.add_argument('--evaluation-episodes', type=int, default=10, metavar='N',\n                    help='Number of evaluation episodes to average over')\n\nparser.add_argument('--evaluation-size', type=int, default=500, metavar='N',\n                    help='Number of transitions to use for validating Q')", "parser.add_argument('--evaluation-size', type=int, default=500, metavar='N',\n                    help='Number of transitions to use for validating Q')\nparser.add_argument('--render', action='store_true', help='Display screen (testing only)')\nparser.add_argument('--enable-cudnn', default=True, help='Enable cuDNN (faster but nondeterministic)')\nparser.add_argument('--checkpoint-interval', default=0,\n                    help='How often to checkpoint the model, defaults to 0 (never checkpoint)')\nparser.add_argument('--memory', help='Path to save/load the memory from')\nparser.add_argument('--disable-bzip-memory', action='store_true',\n                    help='Don\\'t zip the memory file. Not recommended (zipping is a bit slower and much, much smaller)')\nparser.add_argument(\"--peer_coef\", type=float, default=5e-4)", "                    help='Don\\'t zip the memory file. Not recommended (zipping is a bit slower and much, much smaller)')\nparser.add_argument(\"--peer_coef\", type=float, default=5e-4)\n# Setup\nargs = parser.parse_args()\nxid = 'peer-' + args.game + '-' + str(args.seed)\nargs.id = xid\n\nprint(' ' * 26 + 'Options')\nfor k, v in vars(args).items():\n    print(' ' * 26 + k + ': ' + str(v))", "for k, v in vars(args).items():\n    print(' ' * 26 + k + ': ' + str(v))\n\nset_seed_everywhere(args.seed)\nargs.device = torch.device('cuda')\n# torch.cuda.manual_seed(np.random.randint(1, 10000))\ntorch.backends.cudnn.enabled = True\n\n\ndef load_memory(memory_path, disable_bzip):\n    if disable_bzip:\n        with open(memory_path, 'rb') as pickle_file:\n            return pickle.load(pickle_file)\n    else:\n        with bz2.open(memory_path, 'rb') as zipped_pickle_file:\n            return pickle.load(zipped_pickle_file)", "\ndef load_memory(memory_path, disable_bzip):\n    if disable_bzip:\n        with open(memory_path, 'rb') as pickle_file:\n            return pickle.load(pickle_file)\n    else:\n        with bz2.open(memory_path, 'rb') as zipped_pickle_file:\n            return pickle.load(zipped_pickle_file)\n\n\ndef save_memory(memory, memory_path, disable_bzip):\n    if disable_bzip:\n        with open(memory_path, 'wb') as pickle_file:\n            pickle.dump(memory, pickle_file)\n    else:\n        with bz2.open(memory_path, 'wb') as zipped_pickle_file:\n            pickle.dump(memory, zipped_pickle_file)", "\n\ndef save_memory(memory, memory_path, disable_bzip):\n    if disable_bzip:\n        with open(memory_path, 'wb') as pickle_file:\n            pickle.dump(memory, pickle_file)\n    else:\n        with bz2.open(memory_path, 'wb') as zipped_pickle_file:\n            pickle.dump(memory, zipped_pickle_file)\n", "\n\n# Environment\nenv = Env(args)\nenv.train()\naction_space = env.action_space()\n\n# PEER Agent\npeer = Agent(args, env)\n", "peer = Agent(args, env)\n\n\nmem = ReplayMemory(args, args.memory_capacity)\n\npriority_weight_increase = (1 - args.priority_weight) / (args.T_max - args.learn_start)\n\n# Construct validation memory\nval_mem = ReplayMemory(args, args.evaluation_size)\nT, done = 0, True", "val_mem = ReplayMemory(args, args.evaluation_size)\nT, done = 0, True\nwhile T < args.evaluation_size:\n    if done:\n        state, done = env.reset(), False\n\n    next_state, _, done = env.step(np.random.randint(0, action_space))\n    val_mem.append(state, None, None, done)\n    state = next_state\n    T += 1", "    state = next_state\n    T += 1\n\nelse:\n\n    # Training loop\n    peer.train()\n    # Initial testing\n    T, done = 0, True\n    for T in trange(1, args.T_max + 1):\n        if done:\n            state, done = env.reset(), False\n\n        if T % args.replay_frequency == 0:\n            peer.reset_noise()  # Draw a new set of noisy weights\n\n        action = peer.act(state)  # Choose an action greedily (with noisy weights)\n        next_state, reward, done = env.step(action)  # Step\n        if args.reward_clip > 0:\n            reward = max(min(reward, args.reward_clip), -args.reward_clip)  # Clip rewards\n        mem.append(state, action, reward, done)  # Append transition to memory\n\n        # Train and test\n        if T >= args.learn_start:\n            mem.priority_weight = min(mem.priority_weight + priority_weight_increase,\n                                      1)  # Anneal importance sampling weight \u03b2 to 1\n\n            if T % args.replay_frequency == 0:\n                # for _ in range(4):\n                peer.learn(mem)  # Train with n-step distributional double-Q learning\n                peer.update_momentum_net()  # MoCo momentum upate\n\n            if T % args.evaluation_interval == 0:\n                peer.eval()  # Set DQN (online network) to evaluation mode\n                peer.train()  # Set DQN (online network) back to training mode\n\n                # If memory path provided, save it\n                if args.memory is not None:\n                    save_memory(mem, args.memory, args.disable_bzip_memory)\n\n            # Update target network\n            if T % args.target_update == 0:\n                peer.update_target_net()\n        state = next_state", "    T, done = 0, True\n    for T in trange(1, args.T_max + 1):\n        if done:\n            state, done = env.reset(), False\n\n        if T % args.replay_frequency == 0:\n            peer.reset_noise()  # Draw a new set of noisy weights\n\n        action = peer.act(state)  # Choose an action greedily (with noisy weights)\n        next_state, reward, done = env.step(action)  # Step\n        if args.reward_clip > 0:\n            reward = max(min(reward, args.reward_clip), -args.reward_clip)  # Clip rewards\n        mem.append(state, action, reward, done)  # Append transition to memory\n\n        # Train and test\n        if T >= args.learn_start:\n            mem.priority_weight = min(mem.priority_weight + priority_weight_increase,\n                                      1)  # Anneal importance sampling weight \u03b2 to 1\n\n            if T % args.replay_frequency == 0:\n                # for _ in range(4):\n                peer.learn(mem)  # Train with n-step distributional double-Q learning\n                peer.update_momentum_net()  # MoCo momentum upate\n\n            if T % args.evaluation_interval == 0:\n                peer.eval()  # Set DQN (online network) to evaluation mode\n                peer.train()  # Set DQN (online network) back to training mode\n\n                # If memory path provided, save it\n                if args.memory is not None:\n                    save_memory(mem, args.memory, args.disable_bzip_memory)\n\n            # Update target network\n            if T % args.target_update == 0:\n                peer.update_target_net()\n        state = next_state", "\nenv.close()\n"]}
{"filename": "atari/PEER-CURL/agent.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2017 Kai Arulkumaran\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# ==============================================================================\n", "# ==============================================================================\n\n# PEER agent\nfrom __future__ import division\nimport os\nimport kornia.augmentation as aug\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import optim", "import torch.nn as nn\nfrom torch import optim\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom model import DQN\n\nrandom_shift = nn.Sequential(aug.RandomCrop((80, 80)), nn.ReplicationPad2d(4), aug.RandomCrop((84, 84)))\naug = random_shift\n\n\nclass Agent():\n    def __init__(self, args, env):\n\n        self.args = args\n        self.action_space = env.action_space()\n        self.atoms = args.atoms\n        self.Vmin = args.V_min\n        self.Vmax = args.V_max\n        self.support = torch.linspace(args.V_min, args.V_max, self.atoms).to(device=args.device)  # Support (range) of z\n        self.delta_z = (args.V_max - args.V_min) / (self.atoms - 1)\n        self.batch_size = args.batch_size\n        self.n = args.multi_step\n        self.discount = args.discount\n        self.norm_clip = args.norm_clip\n        self.coeff = 0.01 if args.game in ['pong', 'boxing', 'private_eye', 'freeway'] else 1.\n        self.peer_coef = args.peer_coef\n        self.online_net = DQN(args, self.action_space).to(device=args.device)\n        self.momentum_net = DQN(args, self.action_space).to(device=args.device)\n\n\n        self.online_net.train()\n        self.initialize_momentum_net()\n        self.momentum_net.train()\n\n        self.target_net = DQN(args, self.action_space).to(device=args.device)\n        self.update_target_net()\n        self.target_net.train()\n        for param in self.target_net.parameters():\n            param.requires_grad = False\n\n        for param in self.momentum_net.parameters():\n            param.requires_grad = False\n        self.optimiser = optim.Adam(self.online_net.parameters(), lr=args.learning_rate, eps=args.adam_eps)\n\n    # Resets noisy weights in all linear layers (of online net only)\n    def reset_noise(self):\n        self.online_net.reset_noise()\n\n    # Acts based on single state (no batch)\n    def act(self, state):\n        with torch.no_grad():\n            a, _ = self.online_net(state.unsqueeze(0))\n            return (a * self.support).sum(2).argmax(1).item()\n\n    # Acts with an \u03b5-greedy policy (used for evaluation only)\n    def act_e_greedy(self, state, epsilon=0.001):  # High \u03b5 can reduce evaluation scores drastically\n        return np.random.randint(0, self.action_space) if np.random.random() < epsilon else self.act(state)\n\n    def learn(self, mem):\n        # Sample transitions\n        idxs, states, actions, returns, next_states, nonterminals, weights = mem.sample(self.batch_size)\n        aug_states_1 = aug(states).to(device=self.args.device)\n        aug_states_2 = aug(states).to(device=self.args.device)\n        # Calculate current state probabilities (online network noise already sampled)\n        log_ps, _, current_representation = self.online_net(states, log=True, representation=True)  # Log probabilities log p(s_t, \u00b7; \u03b8online)\n        _, z_anch = self.online_net(aug_states_1, log=True)\n        _, z_target = self.momentum_net(aug_states_2, log=True)\n        z_proj = torch.matmul(self.online_net.W, z_target.T)\n        logits = torch.matmul(z_anch, z_proj)\n        logits = (logits - torch.max(logits, 1)[0][:, None])\n        logits = logits * 0.1\n        labels = torch.arange(logits.shape[0]).long().to(device=self.args.device)\n        moco_loss = (nn.CrossEntropyLoss()(logits, labels)).to(device=self.args.device)\n\n        log_ps_a = log_ps[range(self.batch_size), actions]  # log p(s_t, a_t; \u03b8online)\n\n        with torch.no_grad():\n            # Calculate nth next state probabilities\n            pns, _ = self.online_net(next_states)  # Probabilities p(s_t+n, \u00b7; \u03b8online)\n            dns = self.support.expand_as(pns) * pns  # Distribution d_t+n = (z, p(s_t+n, \u00b7; \u03b8online))\n            argmax_indices_ns = dns.sum(2).argmax(\n                1)  # Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; \u03b8online))]\n            self.target_net.reset_noise()  # Sample new target net noise\n            pns, _, target_representation = self.target_net(next_states, representation=True)  # Probabilities p(s_t+n, \u00b7; \u03b8target)\n            pns_a = pns[range(\n                self.batch_size), argmax_indices_ns]  # Double-Q probabilities p(s_t+n, argmax_a[(z, p(s_t+n, a; \u03b8online))]; \u03b8target)\n\n            # Compute Tz (Bellman operator T applied to z)\n            Tz = returns.unsqueeze(1) + nonterminals * (self.discount ** self.n) * self.support.unsqueeze(\n                0)  # Tz = R^n + (\u03b3^n)z (accounting for terminal states)\n            Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)  # Clamp between supported values\n            # Compute L2 projection of Tz onto fixed support z\n            b = (Tz - self.Vmin) / self.delta_z  # b = (Tz - Vmin) / \u0394z\n            l, u = b.floor().to(torch.int64), b.ceil().to(torch.int64)\n            # Fix disappearing probability mass when l = b = u (b is int)\n            l[(u > 0) * (l == u)] -= 1\n            u[(l < (self.atoms - 1)) * (l == u)] += 1\n\n            # Distribute probability of Tz\n            m = states.new_zeros(self.batch_size, self.atoms)\n            offset = torch.linspace(0, ((self.batch_size - 1) * self.atoms), self.batch_size).unsqueeze(1).expand(\n                self.batch_size, self.atoms).to(actions)\n            m.view(-1).index_add_(0, (l + offset).view(-1),\n                                  (pns_a * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n            m.view(-1).index_add_(0, (u + offset).view(-1),\n                                  (pns_a * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n\n        peer_loss = torch.einsum('ij,ij->i', [current_representation, target_representation]).mean()\n        # feature_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\n        loss = -torch.sum(m * log_ps_a, 1)  # Cross-entropy loss (minimises DKL(m||p(s_t, a_t)))\n        loss = loss + (moco_loss * self.coeff) + self.peer_coef * peer_loss\n        self.online_net.zero_grad()\n        curl_loss = (weights * loss).mean()\n        curl_loss.mean().backward()  # Backpropagate importance-weighted minibatch loss\n        clip_grad_norm_(self.online_net.parameters(), self.norm_clip)  # Clip gradients by L2 norm\n        self.optimiser.step()\n\n        mem.update_priorities(idxs, loss.detach().cpu().numpy())  # Update priorities of sampled transitions\n\n    def update_target_net(self):\n        self.target_net.load_state_dict(self.online_net.state_dict())\n\n    def initialize_momentum_net(self):\n        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n            param_k.data.copy_(param_q.data)  # update\n            param_k.requires_grad = False  # not update by gradient\n\n    # Code for this function from https://github.com/facebookresearch/moco\n    @torch.no_grad()\n    def update_momentum_net(self, momentum=0.999):\n        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n            param_k.data.copy_(momentum * param_k.data + (1. - momentum) * param_q.data)  # update\n\n    # Save model parameters on current device (don't move model between devices)\n    def save(self, path, name='model.pth'):\n        torch.save(self.online_net.state_dict(), os.path.join(path, name))\n\n    # Evaluates Q-value based on single state (no batch)\n    def evaluate_q(self, state):\n        with torch.no_grad():\n            a, _ = self.online_net(state.unsqueeze(0))\n            return (a * self.support).sum(2).max(1)[0].item()\n\n    def train(self):\n        self.online_net.train()\n\n    def eval(self):\n        self.online_net.eval()", "\n\nclass Agent():\n    def __init__(self, args, env):\n\n        self.args = args\n        self.action_space = env.action_space()\n        self.atoms = args.atoms\n        self.Vmin = args.V_min\n        self.Vmax = args.V_max\n        self.support = torch.linspace(args.V_min, args.V_max, self.atoms).to(device=args.device)  # Support (range) of z\n        self.delta_z = (args.V_max - args.V_min) / (self.atoms - 1)\n        self.batch_size = args.batch_size\n        self.n = args.multi_step\n        self.discount = args.discount\n        self.norm_clip = args.norm_clip\n        self.coeff = 0.01 if args.game in ['pong', 'boxing', 'private_eye', 'freeway'] else 1.\n        self.peer_coef = args.peer_coef\n        self.online_net = DQN(args, self.action_space).to(device=args.device)\n        self.momentum_net = DQN(args, self.action_space).to(device=args.device)\n\n\n        self.online_net.train()\n        self.initialize_momentum_net()\n        self.momentum_net.train()\n\n        self.target_net = DQN(args, self.action_space).to(device=args.device)\n        self.update_target_net()\n        self.target_net.train()\n        for param in self.target_net.parameters():\n            param.requires_grad = False\n\n        for param in self.momentum_net.parameters():\n            param.requires_grad = False\n        self.optimiser = optim.Adam(self.online_net.parameters(), lr=args.learning_rate, eps=args.adam_eps)\n\n    # Resets noisy weights in all linear layers (of online net only)\n    def reset_noise(self):\n        self.online_net.reset_noise()\n\n    # Acts based on single state (no batch)\n    def act(self, state):\n        with torch.no_grad():\n            a, _ = self.online_net(state.unsqueeze(0))\n            return (a * self.support).sum(2).argmax(1).item()\n\n    # Acts with an \u03b5-greedy policy (used for evaluation only)\n    def act_e_greedy(self, state, epsilon=0.001):  # High \u03b5 can reduce evaluation scores drastically\n        return np.random.randint(0, self.action_space) if np.random.random() < epsilon else self.act(state)\n\n    def learn(self, mem):\n        # Sample transitions\n        idxs, states, actions, returns, next_states, nonterminals, weights = mem.sample(self.batch_size)\n        aug_states_1 = aug(states).to(device=self.args.device)\n        aug_states_2 = aug(states).to(device=self.args.device)\n        # Calculate current state probabilities (online network noise already sampled)\n        log_ps, _, current_representation = self.online_net(states, log=True, representation=True)  # Log probabilities log p(s_t, \u00b7; \u03b8online)\n        _, z_anch = self.online_net(aug_states_1, log=True)\n        _, z_target = self.momentum_net(aug_states_2, log=True)\n        z_proj = torch.matmul(self.online_net.W, z_target.T)\n        logits = torch.matmul(z_anch, z_proj)\n        logits = (logits - torch.max(logits, 1)[0][:, None])\n        logits = logits * 0.1\n        labels = torch.arange(logits.shape[0]).long().to(device=self.args.device)\n        moco_loss = (nn.CrossEntropyLoss()(logits, labels)).to(device=self.args.device)\n\n        log_ps_a = log_ps[range(self.batch_size), actions]  # log p(s_t, a_t; \u03b8online)\n\n        with torch.no_grad():\n            # Calculate nth next state probabilities\n            pns, _ = self.online_net(next_states)  # Probabilities p(s_t+n, \u00b7; \u03b8online)\n            dns = self.support.expand_as(pns) * pns  # Distribution d_t+n = (z, p(s_t+n, \u00b7; \u03b8online))\n            argmax_indices_ns = dns.sum(2).argmax(\n                1)  # Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; \u03b8online))]\n            self.target_net.reset_noise()  # Sample new target net noise\n            pns, _, target_representation = self.target_net(next_states, representation=True)  # Probabilities p(s_t+n, \u00b7; \u03b8target)\n            pns_a = pns[range(\n                self.batch_size), argmax_indices_ns]  # Double-Q probabilities p(s_t+n, argmax_a[(z, p(s_t+n, a; \u03b8online))]; \u03b8target)\n\n            # Compute Tz (Bellman operator T applied to z)\n            Tz = returns.unsqueeze(1) + nonterminals * (self.discount ** self.n) * self.support.unsqueeze(\n                0)  # Tz = R^n + (\u03b3^n)z (accounting for terminal states)\n            Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)  # Clamp between supported values\n            # Compute L2 projection of Tz onto fixed support z\n            b = (Tz - self.Vmin) / self.delta_z  # b = (Tz - Vmin) / \u0394z\n            l, u = b.floor().to(torch.int64), b.ceil().to(torch.int64)\n            # Fix disappearing probability mass when l = b = u (b is int)\n            l[(u > 0) * (l == u)] -= 1\n            u[(l < (self.atoms - 1)) * (l == u)] += 1\n\n            # Distribute probability of Tz\n            m = states.new_zeros(self.batch_size, self.atoms)\n            offset = torch.linspace(0, ((self.batch_size - 1) * self.atoms), self.batch_size).unsqueeze(1).expand(\n                self.batch_size, self.atoms).to(actions)\n            m.view(-1).index_add_(0, (l + offset).view(-1),\n                                  (pns_a * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n            m.view(-1).index_add_(0, (u + offset).view(-1),\n                                  (pns_a * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n\n        peer_loss = torch.einsum('ij,ij->i', [current_representation, target_representation]).mean()\n        # feature_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\n        loss = -torch.sum(m * log_ps_a, 1)  # Cross-entropy loss (minimises DKL(m||p(s_t, a_t)))\n        loss = loss + (moco_loss * self.coeff) + self.peer_coef * peer_loss\n        self.online_net.zero_grad()\n        curl_loss = (weights * loss).mean()\n        curl_loss.mean().backward()  # Backpropagate importance-weighted minibatch loss\n        clip_grad_norm_(self.online_net.parameters(), self.norm_clip)  # Clip gradients by L2 norm\n        self.optimiser.step()\n\n        mem.update_priorities(idxs, loss.detach().cpu().numpy())  # Update priorities of sampled transitions\n\n    def update_target_net(self):\n        self.target_net.load_state_dict(self.online_net.state_dict())\n\n    def initialize_momentum_net(self):\n        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n            param_k.data.copy_(param_q.data)  # update\n            param_k.requires_grad = False  # not update by gradient\n\n    # Code for this function from https://github.com/facebookresearch/moco\n    @torch.no_grad()\n    def update_momentum_net(self, momentum=0.999):\n        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n            param_k.data.copy_(momentum * param_k.data + (1. - momentum) * param_q.data)  # update\n\n    # Save model parameters on current device (don't move model between devices)\n    def save(self, path, name='model.pth'):\n        torch.save(self.online_net.state_dict(), os.path.join(path, name))\n\n    # Evaluates Q-value based on single state (no batch)\n    def evaluate_q(self, state):\n        with torch.no_grad():\n            a, _ = self.online_net(state.unsqueeze(0))\n            return (a * self.support).sum(2).max(1)[0].item()\n\n    def train(self):\n        self.online_net.train()\n\n    def eval(self):\n        self.online_net.eval()", ""]}
{"filename": "atari/PEER-CURL/utils.py", "chunked_list": ["import numpy as np\nimport torch\nimport random\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)", ""]}
{"filename": "atari/PEER-CURL/env.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2017 Kai Arulkumaran\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# ==============================================================================\nimport random", "# ==============================================================================\nimport random\nfrom collections import deque\n\nimport atari_py\nimport cv2\nimport torch\n\n\nclass Env():\n    def __init__(self, args):\n        # self.device = \"cpu\"\n        self.device = args.device\n        self.ale = atari_py.ALEInterface()\n        self.ale.setInt('random_seed', args.seed)\n        self.ale.setInt('max_num_frames_per_episode', args.max_episode_length)\n        self.ale.setFloat('repeat_action_probability', 0)  # Disable sticky actions\n        self.ale.setInt('frame_skip', 0)\n        self.ale.setBool('color_averaging', False)\n        self.ale.loadROM(atari_py.get_game_path(args.game))  # ROM loading must be done after setting options\n        actions = self.ale.getMinimalActionSet()\n        self.actions = dict([i, e] for i, e in zip(range(len(actions)), actions))\n        self.lives = 0  # Life counter (used in DeepMind training)\n        self.life_termination = False  # Used to check if resetting only from loss of life\n        self.window = args.history_length  # Number of frames to concatenate\n        self.state_buffer = deque([], maxlen=args.history_length)\n        self.training = True  # Consistent with model training mode\n\n    def _get_state(self):\n        state = cv2.resize(self.ale.getScreenGrayscale(), (84, 84), interpolation=cv2.INTER_LINEAR)\n        return torch.tensor(state, dtype=torch.float32, device=self.device).div_(255)\n\n    def _reset_buffer(self):\n        for _ in range(self.window):\n            self.state_buffer.append(torch.zeros(84, 84, device=self.device))\n\n    def reset(self):\n        if self.life_termination:\n            self.life_termination = False  # Reset flag\n            self.ale.act(0)  # Use a no-op after loss of life\n        else:\n            # Reset internals\n            self._reset_buffer()\n            self.ale.reset_game()\n            # Perform up to 30 random no-ops before starting\n            for _ in range(random.randrange(30)):\n                self.ale.act(0)  # Assumes raw action 0 is always no-op\n                if self.ale.game_over():\n                    self.ale.reset_game()\n        # Process and return \"initial\" state\n        observation = self._get_state()\n        self.state_buffer.append(observation)\n        self.lives = self.ale.lives()\n        return torch.stack(list(self.state_buffer), 0)\n\n    def step(self, action):\n        # Repeat action 4 times, max pool over last 2 frames\n        frame_buffer = torch.zeros(2, 84, 84, device=self.device)\n        reward, done = 0, False\n        for t in range(4):\n            reward += self.ale.act(self.actions.get(action))\n            if t == 2:\n                frame_buffer[0] = self._get_state()\n            elif t == 3:\n                frame_buffer[1] = self._get_state()\n            done = self.ale.game_over()\n            if done:\n                break\n        observation = frame_buffer.max(0)[0]\n        self.state_buffer.append(observation)\n        # Detect loss of life as terminal in training mode\n        if self.training:\n            lives = self.ale.lives()\n            if lives < self.lives and lives > 0:  # Lives > 0 for Q*bert\n                self.life_termination = not done  # Only set flag when not truly done\n                done = True\n            self.lives = lives\n        # Return state, reward, done\n        return torch.stack(list(self.state_buffer), 0), reward, done\n\n    # Uses loss of life as terminal signal\n    def train(self):\n        self.training = True\n\n    # Uses standard terminal signal\n    def eval(self):\n        self.training = False\n\n    def action_space(self):\n        return len(self.actions)\n\n    def render(self):\n        cv2.imshow('screen', self.ale.getScreenRGB()[:, :, ::-1])\n        cv2.waitKey(1)\n\n    def close(self):\n        cv2.destroyAllWindows()", "\nclass Env():\n    def __init__(self, args):\n        # self.device = \"cpu\"\n        self.device = args.device\n        self.ale = atari_py.ALEInterface()\n        self.ale.setInt('random_seed', args.seed)\n        self.ale.setInt('max_num_frames_per_episode', args.max_episode_length)\n        self.ale.setFloat('repeat_action_probability', 0)  # Disable sticky actions\n        self.ale.setInt('frame_skip', 0)\n        self.ale.setBool('color_averaging', False)\n        self.ale.loadROM(atari_py.get_game_path(args.game))  # ROM loading must be done after setting options\n        actions = self.ale.getMinimalActionSet()\n        self.actions = dict([i, e] for i, e in zip(range(len(actions)), actions))\n        self.lives = 0  # Life counter (used in DeepMind training)\n        self.life_termination = False  # Used to check if resetting only from loss of life\n        self.window = args.history_length  # Number of frames to concatenate\n        self.state_buffer = deque([], maxlen=args.history_length)\n        self.training = True  # Consistent with model training mode\n\n    def _get_state(self):\n        state = cv2.resize(self.ale.getScreenGrayscale(), (84, 84), interpolation=cv2.INTER_LINEAR)\n        return torch.tensor(state, dtype=torch.float32, device=self.device).div_(255)\n\n    def _reset_buffer(self):\n        for _ in range(self.window):\n            self.state_buffer.append(torch.zeros(84, 84, device=self.device))\n\n    def reset(self):\n        if self.life_termination:\n            self.life_termination = False  # Reset flag\n            self.ale.act(0)  # Use a no-op after loss of life\n        else:\n            # Reset internals\n            self._reset_buffer()\n            self.ale.reset_game()\n            # Perform up to 30 random no-ops before starting\n            for _ in range(random.randrange(30)):\n                self.ale.act(0)  # Assumes raw action 0 is always no-op\n                if self.ale.game_over():\n                    self.ale.reset_game()\n        # Process and return \"initial\" state\n        observation = self._get_state()\n        self.state_buffer.append(observation)\n        self.lives = self.ale.lives()\n        return torch.stack(list(self.state_buffer), 0)\n\n    def step(self, action):\n        # Repeat action 4 times, max pool over last 2 frames\n        frame_buffer = torch.zeros(2, 84, 84, device=self.device)\n        reward, done = 0, False\n        for t in range(4):\n            reward += self.ale.act(self.actions.get(action))\n            if t == 2:\n                frame_buffer[0] = self._get_state()\n            elif t == 3:\n                frame_buffer[1] = self._get_state()\n            done = self.ale.game_over()\n            if done:\n                break\n        observation = frame_buffer.max(0)[0]\n        self.state_buffer.append(observation)\n        # Detect loss of life as terminal in training mode\n        if self.training:\n            lives = self.ale.lives()\n            if lives < self.lives and lives > 0:  # Lives > 0 for Q*bert\n                self.life_termination = not done  # Only set flag when not truly done\n                done = True\n            self.lives = lives\n        # Return state, reward, done\n        return torch.stack(list(self.state_buffer), 0), reward, done\n\n    # Uses loss of life as terminal signal\n    def train(self):\n        self.training = True\n\n    # Uses standard terminal signal\n    def eval(self):\n        self.training = False\n\n    def action_space(self):\n        return len(self.actions)\n\n    def render(self):\n        cv2.imshow('screen', self.ale.getScreenRGB()[:, :, ::-1])\n        cv2.waitKey(1)\n\n    def close(self):\n        cv2.destroyAllWindows()", ""]}
{"filename": "atari/PEER-CURL/memory.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# MIT License\n#\n# Copyright (c) 2017 Kai Arulkumaran\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n# ==============================================================================\nfrom __future__ import division", "# ==============================================================================\nfrom __future__ import division\n\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\n\nTransition = namedtuple('Transition', ('timestep', 'state', 'action', 'reward', 'nonterminal'))\nblank_trans = Transition(0, torch.zeros(84, 84, dtype=torch.uint8), None, 0, False)", "Transition = namedtuple('Transition', ('timestep', 'state', 'action', 'reward', 'nonterminal'))\nblank_trans = Transition(0, torch.zeros(84, 84, dtype=torch.uint8), None, 0, False)\n\n\n# Segment tree data structure where parent node values are sum/max of children node values\nclass SegmentTree():\n    def __init__(self, size):\n        self.index = 0\n        self.size = size\n        self.full = False  # Used to track actual capacity\n        self.sum_tree = np.zeros((2 * size - 1,),\n                                 dtype=np.float32)  # Initialise fixed size tree with all (priority) zeros\n        self.data = np.array([None] * size)  # Wrap-around cyclic buffer\n        self.max = 1  # Initial max value to return (1 = 1^\u03c9)\n\n    # Propagates value up tree given a tree index\n    def _propagate(self, index, value):\n        parent = (index - 1) // 2\n        left, right = 2 * parent + 1, 2 * parent + 2\n        self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n        if parent != 0:\n            self._propagate(parent, value)\n\n    # Updates value given a tree index\n    def update(self, index, value):\n        self.sum_tree[index] = value  # Set new value\n        self._propagate(index, value)  # Propagate value\n        self.max = max(value, self.max)\n\n    def append(self, data, value):\n        self.data[self.index] = data  # Store data in underlying data structure\n        self.update(self.index + self.size - 1, value)  # Update tree\n        self.index = (self.index + 1) % self.size  # Update index\n        self.full = self.full or self.index == 0  # Save when capacity reached\n        self.max = max(value, self.max)\n\n    # Searches for the location of a value in sum tree\n    def _retrieve(self, index, value):\n        left, right = 2 * index + 1, 2 * index + 2\n        if left >= len(self.sum_tree):\n            return index\n        elif value <= self.sum_tree[left]:\n            return self._retrieve(left, value)\n        else:\n            return self._retrieve(right, value - self.sum_tree[left])\n\n    # Searches for a value in sum tree and returns value, data index and tree index\n    def find(self, value):\n        index = self._retrieve(0, value)  # Search for index of item from root\n        data_index = index - self.size + 1\n        return (self.sum_tree[index], data_index, index)  # Return value, data index, tree index\n\n    # Returns data given a data index\n    def get(self, data_index):\n        return self.data[data_index % self.size]\n\n    def total(self):\n        return self.sum_tree[0]", "\n\nclass ReplayMemory():\n    def __init__(self, args, capacity):\n        self.device = args.device\n        self.capacity = capacity\n        self.history = args.history_length\n        self.discount = args.discount\n        self.n = args.multi_step\n        self.priority_weight = args.priority_weight  # Initial importance sampling weight \u03b2, annealed to 1 over course of training\n        self.priority_exponent = args.priority_exponent\n        self.t = 0  # Internal episode timestep counter\n        self.transitions = SegmentTree(\n            capacity)  # Store transitions in a wrap-around cyclic buffer within a sum tree for querying priorities\n\n    # Adds state and action at time t, reward and terminal at time t + 1\n    def append(self, state, action, reward, terminal):\n        state = state[-1].mul(255).to(dtype=torch.uint8,\n                                      device=torch.device('cpu'))  # Only store last frame and discretise to save memory\n        self.transitions.append(Transition(self.t, state, action, reward, not terminal),\n                                self.transitions.max)  # Store new transition with maximum priority\n        self.t = 0 if terminal else self.t + 1  # Start new episodes with t = 0\n\n    # Returns a transition with blank states where appropriate\n    def _get_transition(self, idx):\n        transition = np.array([None] * (self.history + self.n))\n        transition[self.history - 1] = self.transitions.get(idx)\n        for t in range(self.history - 2, -1, -1):  # e.g. 2 1 0\n            if transition[t + 1].timestep == 0:\n                transition[t] = blank_trans  # If future frame has timestep 0\n            else:\n                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n        for t in range(self.history, self.history + self.n):  # e.g. 4 5 6\n            if transition[t - 1].nonterminal:\n                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n            else:\n                transition[t] = blank_trans  # If prev (next) frame is terminal\n        return transition\n\n    # Returns a valid sample from a segment\n    def _get_sample_from_segment(self, segment, i):\n        valid = False\n        while not valid:\n            sample = np.random.uniform(i * segment,\n                                       (i + 1) * segment)  # Uniformly sample an element from within a segment\n            prob, idx, tree_idx = self.transitions.find(\n                sample)  # Retrieve sample from tree with un-normalised probability\n            # Resample if transition straddled current index or probablity 0\n            if (self.transitions.index - idx) % self.capacity > self.n and (\n                    idx - self.transitions.index) % self.capacity >= self.history and prob != 0:\n                valid = True  # Note that conditions are valid but extra conservative around buffer index 0\n\n        # Retrieve all required transition data (from t - h to t + n)\n        transition = self._get_transition(idx)\n        # Create un-discretised state and nth next state\n        state = torch.stack([trans.state for trans in transition[:self.history]]).to(device=self.device).to(\n            dtype=torch.float32).div_(255)\n        next_state = torch.stack([trans.state for trans in transition[self.n:self.n + self.history]]).to(\n            device=self.device).to(dtype=torch.float32).div_(255)\n        # Discrete action to be used as index\n        action = torch.tensor([transition[self.history - 1].action], dtype=torch.int64, device=self.device)\n        # Calculate truncated n-step discounted return R^n = \u03a3_k=0->n-1 (\u03b3^k)R_t+k+1 (note that invalid nth next states have reward 0)\n        R = torch.tensor([sum(self.discount ** n * transition[self.history + n - 1].reward for n in range(self.n))],\n                         dtype=torch.float32, device=self.device)\n        # Mask for non-terminal nth next states\n        nonterminal = torch.tensor([transition[self.history + self.n - 1].nonterminal], dtype=torch.float32,\n                                   device=self.device)\n\n        return prob, idx, tree_idx, state, action, R, next_state, nonterminal\n\n    def sample(self, batch_size):\n        p_total = self.transitions.total()  # Retrieve sum of all priorities (used to create a normalised probability distribution)\n        segment = p_total / batch_size  # Batch size number of segments, based on sum over all probabilities\n        batch = [self._get_sample_from_segment(segment, i) for i in range(batch_size)]  # Get batch of valid samples\n        probs, idxs, tree_idxs, states, actions, returns, next_states, nonterminals = zip(*batch)\n        states, next_states, = torch.stack(states), torch.stack(next_states)\n        actions, returns, nonterminals = torch.cat(actions), torch.cat(returns), torch.stack(nonterminals)\n        probs = np.array(probs, dtype=np.float32) / p_total  # Calculate normalised probabilities\n        capacity = self.capacity if self.transitions.full else self.transitions.index\n        weights = (capacity * probs) ** -self.priority_weight  # Compute importance-sampling weights w\n        weights = torch.tensor(weights / weights.max(), dtype=torch.float32,\n                               device=self.device)  # Normalise by max importance-sampling weight from batch\n        return tree_idxs, states, actions, returns, next_states, nonterminals, weights\n\n    def update_priorities(self, idxs, priorities):\n        priorities = np.power(priorities, self.priority_exponent)\n        [self.transitions.update(idx, priority) for idx, priority in zip(idxs, priorities)]\n\n    # Set up internal state for iterator\n    def __iter__(self):\n        self.current_idx = 0\n        return self\n\n    # Return valid states for validation\n    def __next__(self):\n        if self.current_idx == self.capacity:\n            raise StopIteration\n        # Create stack of states\n        state_stack = [None] * self.history\n        state_stack[-1] = self.transitions.data[self.current_idx].state\n        prev_timestep = self.transitions.data[self.current_idx].timestep\n        for t in reversed(range(self.history - 1)):\n            if prev_timestep == 0:\n                state_stack[t] = blank_trans.state  # If future frame has timestep 0\n            else:\n                state_stack[t] = self.transitions.data[self.current_idx + t - self.history + 1].state\n                prev_timestep -= 1\n        state = torch.stack(state_stack, 0).to(dtype=torch.float32, device=self.device).div_(\n            255)  # Agent will turn into batch\n        self.current_idx += 1\n        return state\n\n    next = __next__  # Alias __next__ for Python 2 compatibility", ""]}
