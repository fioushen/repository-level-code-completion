{"filename": "python/setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n    name=\"prophecy-spark-ai\",\n    version=\"0.1.8\",\n    url=\"https://github.com/prophecy-io/spark-ai\",\n    packages=find_packages(exclude=[\"tests\", \"tests.*\"]),\n    package_dir={'spark_ai': 'spark_ai'},\n    description=\"High-performance AI/ML library for Spark to build and deploy your LLM applications in production.\",\n    long_description_content_type=\"text/markdown\",", "    description=\"High-performance AI/ML library for Spark to build and deploy your LLM applications in production.\",\n    long_description_content_type=\"text/markdown\",\n    long_description=open(\"../README.md\").read(),\n    install_requires=[\n        \"slack-sdk>=3.21.3\",\n        \"openai[datalib]>=0.27.8\",\n        \"pinecone-client>=2.2.2\",\n        \"python-dotenv>=1.0.0\",\n        \"requests>=2.31.0\",\n        \"beautifulsoup4>=4.12.2\",", "        \"requests>=2.31.0\",\n        \"beautifulsoup4>=4.12.2\",\n        \"unstructured>=0.7.4\",\n        \"numpy>=1.24.3\"\n    ],\n    keywords=[\"python\", \"prophecy\"],\n    classifiers=[],\n    zip_safe=False,\n)\n", ")\n"]}
{"filename": "python/__init__.py", "chunked_list": [""]}
{"filename": "python/tests/test_spark.py", "chunked_list": ["from pyspark.sql.functions import array, struct, lit, col\nfrom pyspark.sql.types import StructField, StringType, ArrayType, StructType, IntegerType, FloatType, Row\n\nfrom spark_ai.dbs.pinecone import UpsertResponse, QueryResponse\nfrom spark_ai.spark import SparkUtils\nfrom tests import BaseUnitTest\n\n\nclass TestSparkUtils(BaseUnitTest):\n\n    def test_join(self):\n\n        msgs = [\n            ('1', 'Hey everyone', None),\n            ('2', 'Hey Matt', '1'),\n            ('3', 'Hey sir', '1'),\n            ('4', 'Another one', None),\n        ]\n\n        r = Row('ts', 'msg', 'thread_ts')\n\n        df_msgs = self.spark.createDataFrame([r(msg[0], msg[1], msg[2]) for msg in msgs])\n        df_msgs.show()\n\n        df_msgs = df_msgs.alias(\"m\")\n        df_submsgs = df_msgs.alias(\"sm\")\n        df_msgs.join(df_submsgs, (col(\"m.ts\") == col(\"sm.thread_ts\")), \"left_outer\").filter(col(\"m.thread_ts\").isNull()).show()\n\n\n    def test_default_missing_columns(self):\n        df_original = self.spark.range(1).select(\n            lit(1).alias('a'),\n            struct(\n                lit('c').alias('c'),\n                array(\n                    struct(lit('e').alias('e'), lit(None).cast(StringType()).alias('f')),\n                    struct(lit('e').alias('e'), lit('f').alias('f'))\n                ).alias('d')\n            ).alias('b')\n        )\n\n        df_original.show(truncate=False)\n\n        expected_schema = StructType([\n            StructField('a', IntegerType(), False),\n            StructField('b', StructType([\n                StructField('c', StringType(), False),\n                StructField('d', ArrayType(StructType([\n                    StructField('e', StringType(), False),\n                    StructField('f', StringType(), True),\n                    StructField('g', StringType(), True)\n                ]), containsNull=False), False),\n                StructField('h', StringType(), True),\n            ]), False)\n        ])\n\n        df_result = SparkUtils.default_missing_columns(df_original, expected_schema)\n        self.assertEquals(expected_schema, df_result.schema)\n\n    def test_dataclass_to_spark(self):\n        upsert_response = SparkUtils.dataclass_to_spark(UpsertResponse)\n        self.assertEquals(\n            upsert_response,\n            StructType([\n                StructField('count', IntegerType(), False),\n                StructField('error', StringType(), True)\n            ])\n        )\n\n        query_response = SparkUtils.dataclass_to_spark(QueryResponse)\n        self.assertEquals(\n            query_response,\n            StructType([\n                StructField('matches', ArrayType(StructType([\n                    StructField('id', StringType(), False),\n                    StructField('score', FloatType(), False),\n                ]), False), False),\n                StructField('error', StringType(), True)\n            ])\n        )", "class TestSparkUtils(BaseUnitTest):\n\n    def test_join(self):\n\n        msgs = [\n            ('1', 'Hey everyone', None),\n            ('2', 'Hey Matt', '1'),\n            ('3', 'Hey sir', '1'),\n            ('4', 'Another one', None),\n        ]\n\n        r = Row('ts', 'msg', 'thread_ts')\n\n        df_msgs = self.spark.createDataFrame([r(msg[0], msg[1], msg[2]) for msg in msgs])\n        df_msgs.show()\n\n        df_msgs = df_msgs.alias(\"m\")\n        df_submsgs = df_msgs.alias(\"sm\")\n        df_msgs.join(df_submsgs, (col(\"m.ts\") == col(\"sm.thread_ts\")), \"left_outer\").filter(col(\"m.thread_ts\").isNull()).show()\n\n\n    def test_default_missing_columns(self):\n        df_original = self.spark.range(1).select(\n            lit(1).alias('a'),\n            struct(\n                lit('c').alias('c'),\n                array(\n                    struct(lit('e').alias('e'), lit(None).cast(StringType()).alias('f')),\n                    struct(lit('e').alias('e'), lit('f').alias('f'))\n                ).alias('d')\n            ).alias('b')\n        )\n\n        df_original.show(truncate=False)\n\n        expected_schema = StructType([\n            StructField('a', IntegerType(), False),\n            StructField('b', StructType([\n                StructField('c', StringType(), False),\n                StructField('d', ArrayType(StructType([\n                    StructField('e', StringType(), False),\n                    StructField('f', StringType(), True),\n                    StructField('g', StringType(), True)\n                ]), containsNull=False), False),\n                StructField('h', StringType(), True),\n            ]), False)\n        ])\n\n        df_result = SparkUtils.default_missing_columns(df_original, expected_schema)\n        self.assertEquals(expected_schema, df_result.schema)\n\n    def test_dataclass_to_spark(self):\n        upsert_response = SparkUtils.dataclass_to_spark(UpsertResponse)\n        self.assertEquals(\n            upsert_response,\n            StructType([\n                StructField('count', IntegerType(), False),\n                StructField('error', StringType(), True)\n            ])\n        )\n\n        query_response = SparkUtils.dataclass_to_spark(QueryResponse)\n        self.assertEquals(\n            query_response,\n            StructType([\n                StructField('matches', ArrayType(StructType([\n                    StructField('id', StringType(), False),\n                    StructField('score', FloatType(), False),\n                ]), False), False),\n                StructField('error', StringType(), True)\n            ])\n        )", ""]}
{"filename": "python/tests/__init__.py", "chunked_list": ["import os\nimport unittest\n\nfrom pyspark.sql import SparkSession, DataFrame\nfrom dotenv import load_dotenv\n\nclass BaseUnitTest(unittest.TestCase):\n    debug = False\n\n    def setUp(self):\n        load_dotenv()\n        self.debug = os.getenv('DEBUG', 'False').lower() == 'true'\n\n        self.spark = SparkSession.builder.master('local[1]').getOrCreate()\n\n    def tearDown(self):\n        self.spark.stop()\n\n    def df_debug(self, df: DataFrame):\n        if self.debug:\n            df.printSchema()\n            df.show(truncate=False)", ""]}
{"filename": "python/tests/dbs/__init__.py", "chunked_list": [""]}
{"filename": "python/tests/dbs/test_pinecone.py", "chunked_list": ["import os\nfrom random import random\nfrom typing import List\n\nfrom pyspark import Row\nfrom pyspark.sql.functions import expr, col\n\nfrom spark_ai.dbs.pinecone import PineconeDB, IdVector\nfrom tests import BaseUnitTest\n", "from tests import BaseUnitTest\n\n\nclass TestPineconeDB(BaseUnitTest):\n    pinecone_api_key = ''\n    pinecone_environment = ''\n    index_name = 'dev-unit-testing'\n\n    def setUp(self):\n        super().setUp()\n\n        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')\n        self.pinecone_environment = os.getenv('PINECONE_ENVIRONMENT')\n\n        PineconeDB(self.pinecone_api_key, self.pinecone_environment).register_udfs(self.spark)\n\n    def test_upsert(self):\n        data = [\n            [IdVector('1', self._random_vector()), IdVector('2', self._random_vector())],\n            [IdVector('3', self._random_vector())],\n        ]\n\n        r = Row('id_vectors')\n        df_data = self.spark.createDataFrame([r(id_vectors) for id_vectors in data])\n\n        df_upserted = df_data \\\n            .withColumn('upserted', expr(f'pinecone_upsert(\"{self.index_name}\", id_vectors)')) \\\n            .select(col('*'), col('upserted.*')) \\\n            .select(col('id_vectors'), col('count'), col('error'))\n\n        self.df_debug(df_upserted)\n        self.assertEqual(df_upserted.filter('error is null').count(), 2)\n        self.assertEqual(df_upserted.agg(expr('sum(count) as total_count')).collect(), [Row(total_count=3)])\n\n    def test_query(self):\n        vector = self._random_vector()\n        data = [[IdVector('5', vector)]]\n\n        r = Row('id_vectors')\n        df_data = self.spark.createDataFrame([r(id_vectors) for id_vectors in data])\n\n        df_data \\\n            .withColumn('upserted', expr(f'pinecone_upsert(\"{self.index_name}\", id_vectors)')) \\\n            .count()\n\n        df_result = df_data \\\n            .withColumn('vector', expr('id_vectors[0].vector')) \\\n            .withColumn('result', expr(f'pinecone_query(\"{self.index_name}\", vector, 1)')) \\\n            .select(col('*'), col('result.*'))\n\n        self.df_debug(df_result)\n        self.assertEquals(df_result.filter('cast(matches[0].id as int) > 0').count(), 1)\n        self.assertEquals(df_result.filter('error is not null').count(), 0)\n\n    @staticmethod\n    def _random_vector() -> List[float]:\n        return [random() for _ in range(0, 1536)]", ""]}
{"filename": "python/tests/webapps/__init__.py", "chunked_list": [""]}
{"filename": "python/tests/webapps/test_web.py", "chunked_list": ["from pyspark.sql.functions import lit, expr\n\nfrom spark_ai.webapps import WebUtils\nfrom tests import BaseUnitTest\n\n\nclass TestWeb(BaseUnitTest):\n\n    def _init_web(self) -> WebUtils:\n        utils = WebUtils()\n        utils.register_udfs(spark=self.spark)\n\n        return utils\n\n    def test_scrape(self):\n        self._init_web()\n\n        df_url = self.spark.range(1).select(lit(\"https://docs.prophecy.io/sitemap.xml\").alias(\"url\"))\n        df_results = df_url.withColumn(\"content\", expr(\"cast(web_scrape(url) as string)\"))\n\n        self.assertTrue(df_results.collect()[0].content.startswith(\"<?xml version=\"))", ""]}
{"filename": "python/tests/webapps/test_slack.py", "chunked_list": ["import os\nimport unittest\n\nfrom pyspark.sql import SparkSession\n\nfrom spark_ai.webapps.slack import SlackUtilities\nfrom tests import BaseUnitTest\n\n\nclass TestSlackUtilities(BaseUnitTest):\n    _token = None\n    _path_tmp = '/tmp/slack_data/'\n    _limit = 100\n\n    def _init_slack(self, limit: int = _limit) -> SlackUtilities:\n        return SlackUtilities(\n            token=self._token,\n            spark=self.spark,\n            path_tmp=self._path_tmp,\n            limit=limit\n        )\n\n    def setUp(self):\n        super().setUp()\n\n        self.spark = SparkSession.builder.master('local[1]').getOrCreate()\n        self._token = os.getenv('SLACK_TOKEN')\n\n    def test_read_channels(self):\n        slack = self._init_slack()\n        df_channels = slack.read_channels()\n\n        self.assertIn('id', df_channels.columns)\n        self.assertIn('name', df_channels.columns)\n\n        self.assertTrue(df_channels.count() >= self._limit)\n\n    def test_join_channels(self):\n        slack = self._init_slack(limit=1)\n\n        df_channels = slack.read_channels()\n        df_results = slack.join_channels(df_channels)\n\n        self.assertIn('id', df_results.columns)\n        self.assertIn('result', df_results.columns)\n\n        self.assertTrue(df_results.count() >= 1)\n\n    def test_read_conversations(self):\n        slack = self._init_slack()\n\n        df_conversations = slack.read_conversations(df_channels=slack.read_channels())\n        self.assertIn('text', df_conversations.columns)\n        self.assertIn('ts', df_conversations.columns)\n        self.assertIn('channel_id', df_conversations.columns)\n\n        self.assertTrue(df_conversations.count() >= self._limit)\n\n    def test_find_max_ts_per_channel(self):\n        slack = self._init_slack()\n\n        df_channels = slack.read_channels()\n        df_conversations = slack.read_conversations(df_channels=df_channels)\n        max_ts_per_channel = slack.find_max_ts_per_channel(df_conversations)\n\n        self.assertTrue(len(max_ts_per_channel) >= 1)\n\n    def test_write_messages(self):\n        slack = self._init_slack()", "\nclass TestSlackUtilities(BaseUnitTest):\n    _token = None\n    _path_tmp = '/tmp/slack_data/'\n    _limit = 100\n\n    def _init_slack(self, limit: int = _limit) -> SlackUtilities:\n        return SlackUtilities(\n            token=self._token,\n            spark=self.spark,\n            path_tmp=self._path_tmp,\n            limit=limit\n        )\n\n    def setUp(self):\n        super().setUp()\n\n        self.spark = SparkSession.builder.master('local[1]').getOrCreate()\n        self._token = os.getenv('SLACK_TOKEN')\n\n    def test_read_channels(self):\n        slack = self._init_slack()\n        df_channels = slack.read_channels()\n\n        self.assertIn('id', df_channels.columns)\n        self.assertIn('name', df_channels.columns)\n\n        self.assertTrue(df_channels.count() >= self._limit)\n\n    def test_join_channels(self):\n        slack = self._init_slack(limit=1)\n\n        df_channels = slack.read_channels()\n        df_results = slack.join_channels(df_channels)\n\n        self.assertIn('id', df_results.columns)\n        self.assertIn('result', df_results.columns)\n\n        self.assertTrue(df_results.count() >= 1)\n\n    def test_read_conversations(self):\n        slack = self._init_slack()\n\n        df_conversations = slack.read_conversations(df_channels=slack.read_channels())\n        self.assertIn('text', df_conversations.columns)\n        self.assertIn('ts', df_conversations.columns)\n        self.assertIn('channel_id', df_conversations.columns)\n\n        self.assertTrue(df_conversations.count() >= self._limit)\n\n    def test_find_max_ts_per_channel(self):\n        slack = self._init_slack()\n\n        df_channels = slack.read_channels()\n        df_conversations = slack.read_conversations(df_channels=df_channels)\n        max_ts_per_channel = slack.find_max_ts_per_channel(df_conversations)\n\n        self.assertTrue(len(max_ts_per_channel) >= 1)\n\n    def test_write_messages(self):\n        slack = self._init_slack()", "\n\n\nif __name__ == '__main__':\n    unittest.main()\n"]}
{"filename": "python/tests/llms/test_openai.py", "chunked_list": ["import os\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import expr, col, row_number, ceil, collect_list, struct\nfrom pyspark.sql.types import Row\n\nfrom spark_ai.llms.openai import OpenAiLLM\nfrom tests import BaseUnitTest\n\n\nclass TestOpenAiLLM(BaseUnitTest):\n    api_key = ''\n\n    def setUp(self):\n        super().setUp()\n\n        self.api_key = os.getenv('OPENAI_API_KEY')\n        OpenAiLLM(api_key=self.api_key).register_udfs(spark=self.spark)\n\n    def test_grouped_embed_texts(self):\n        data = [\n            'Hello, my dog is cute',\n            'Hello, my cat is cute',\n            'Hello world',\n            'Hello Poland'\n        ]\n\n        r = Row('text')\n        df_data = self.spark.createDataFrame([r(text) for text in data])\n\n        df_embedded = df_data \\\n            .withColumn(\"_row_num\",\n                        row_number().over(Window.partitionBy().orderBy(col(\"text\")))) \\\n            .withColumn(\"_group_num\", ceil(col(\"_row_num\") / 20)) \\\n            .withColumn(\"_data\", struct(col(\"*\"))) \\\n            .groupBy(col(\"_group_num\")) \\\n            .agg(collect_list(col(\"_data\")).alias(\"_data\"), collect_list(col(\"text\")).alias(\"_texts\")) \\\n            .withColumn(\"_embedded\", expr(\"openai_embed_texts(_texts)\")) \\\n            .select(col(\"_texts\").alias(\"_texts\"), col(\"_embedded.embeddings\").alias(\"_embeddings\"),\n                    col(\"_embedded.error\").alias(\"openai_error\"), col(\"_data\")) \\\n            .select(expr(\"explode_outer(arrays_zip(_embeddings, _data))\").alias(\"_content\"), col(\"openai_error\")) \\\n            .select(col(\"_content._embeddings\").alias(\"openai_embedding\"), col(\"openai_error\"), col(\"_content._data.*\"))\n\n        self.df_debug(df_embedded)\n\n        self.assertEqual(df_embedded.filter('openai_error is null').count(), 4)\n        self.assertEqual(df_embedded.filter('size(openai_embedding) = 1536').count(), 4)\n\n    def test_embed_texts(self):\n        data = [\n            ['Hello, my dog is cute', 'Hello, my cat is cute'],\n            ['Hello world', 'Hello Poland']\n        ]\n\n        r = Row('texts')\n        df_data = self.spark.createDataFrame([r(text) for text in data])\n\n        df_embedded = df_data \\\n            .withColumn('embedded', expr('openai_embed_texts(texts)')) \\\n            .select(col('texts').alias('texts'), col('embedded.embeddings').alias('embeddings'),\n                    col('embedded.error').alias('error')) \\\n            .select(expr('explode_outer(arrays_zip(texts, embeddings))').alias('content'), col('error')) \\\n            .select(col('content.texts').alias('text'), col('content.embeddings').alias('embedding'), col('error'))\n\n        self.assertEqual(df_embedded.filter('error is null').count(), 4)\n        self.assertEqual(df_embedded.filter('size(embedding) = 1536').count(), 4)\n\n    def test_answer_questions(self):\n        data = [\n            ['Barack Obama is the president of the United States.', 'Who is the president of the United States?',\n             'Barack Obama'],\n            ['Prophecy is a Low-code Data Engineering Platform.', 'What is Prophecy?', 'low-code'],\n        ]\n\n        r = Row('context', 'query', 'expected_answer')\n        df_data = self.spark.createDataFrame([r(context, query, answer) for context, query, answer in data])\n\n        df_answered = df_data \\\n            .withColumn('generated_answer', expr('openai_answer_question(context, query)')) \\\n            .select(col('*'), col('generated_answer.*')) \\\n            .withColumn('best_generated_answer', expr('choices[0]')) \\\n            .withColumn('answer_comparison', expr('contains(lower(best_generated_answer), lower(expected_answer))'))\n\n        if self.debug:\n            df_answered.show(truncate=False)\n\n        self.assertEqual(df_answered.filter('answer_comparison = true').count(), 2)", "\n\nclass TestOpenAiLLM(BaseUnitTest):\n    api_key = ''\n\n    def setUp(self):\n        super().setUp()\n\n        self.api_key = os.getenv('OPENAI_API_KEY')\n        OpenAiLLM(api_key=self.api_key).register_udfs(spark=self.spark)\n\n    def test_grouped_embed_texts(self):\n        data = [\n            'Hello, my dog is cute',\n            'Hello, my cat is cute',\n            'Hello world',\n            'Hello Poland'\n        ]\n\n        r = Row('text')\n        df_data = self.spark.createDataFrame([r(text) for text in data])\n\n        df_embedded = df_data \\\n            .withColumn(\"_row_num\",\n                        row_number().over(Window.partitionBy().orderBy(col(\"text\")))) \\\n            .withColumn(\"_group_num\", ceil(col(\"_row_num\") / 20)) \\\n            .withColumn(\"_data\", struct(col(\"*\"))) \\\n            .groupBy(col(\"_group_num\")) \\\n            .agg(collect_list(col(\"_data\")).alias(\"_data\"), collect_list(col(\"text\")).alias(\"_texts\")) \\\n            .withColumn(\"_embedded\", expr(\"openai_embed_texts(_texts)\")) \\\n            .select(col(\"_texts\").alias(\"_texts\"), col(\"_embedded.embeddings\").alias(\"_embeddings\"),\n                    col(\"_embedded.error\").alias(\"openai_error\"), col(\"_data\")) \\\n            .select(expr(\"explode_outer(arrays_zip(_embeddings, _data))\").alias(\"_content\"), col(\"openai_error\")) \\\n            .select(col(\"_content._embeddings\").alias(\"openai_embedding\"), col(\"openai_error\"), col(\"_content._data.*\"))\n\n        self.df_debug(df_embedded)\n\n        self.assertEqual(df_embedded.filter('openai_error is null').count(), 4)\n        self.assertEqual(df_embedded.filter('size(openai_embedding) = 1536').count(), 4)\n\n    def test_embed_texts(self):\n        data = [\n            ['Hello, my dog is cute', 'Hello, my cat is cute'],\n            ['Hello world', 'Hello Poland']\n        ]\n\n        r = Row('texts')\n        df_data = self.spark.createDataFrame([r(text) for text in data])\n\n        df_embedded = df_data \\\n            .withColumn('embedded', expr('openai_embed_texts(texts)')) \\\n            .select(col('texts').alias('texts'), col('embedded.embeddings').alias('embeddings'),\n                    col('embedded.error').alias('error')) \\\n            .select(expr('explode_outer(arrays_zip(texts, embeddings))').alias('content'), col('error')) \\\n            .select(col('content.texts').alias('text'), col('content.embeddings').alias('embedding'), col('error'))\n\n        self.assertEqual(df_embedded.filter('error is null').count(), 4)\n        self.assertEqual(df_embedded.filter('size(embedding) = 1536').count(), 4)\n\n    def test_answer_questions(self):\n        data = [\n            ['Barack Obama is the president of the United States.', 'Who is the president of the United States?',\n             'Barack Obama'],\n            ['Prophecy is a Low-code Data Engineering Platform.', 'What is Prophecy?', 'low-code'],\n        ]\n\n        r = Row('context', 'query', 'expected_answer')\n        df_data = self.spark.createDataFrame([r(context, query, answer) for context, query, answer in data])\n\n        df_answered = df_data \\\n            .withColumn('generated_answer', expr('openai_answer_question(context, query)')) \\\n            .select(col('*'), col('generated_answer.*')) \\\n            .withColumn('best_generated_answer', expr('choices[0]')) \\\n            .withColumn('answer_comparison', expr('contains(lower(best_generated_answer), lower(expected_answer))'))\n\n        if self.debug:\n            df_answered.show(truncate=False)\n\n        self.assertEqual(df_answered.filter('answer_comparison = true').count(), 2)", ""]}
{"filename": "python/tests/llms/__init__.py", "chunked_list": [""]}
{"filename": "python/spark_ai/__init__.py", "chunked_list": [""]}
{"filename": "python/spark_ai/spark.py", "chunked_list": ["import dataclasses\nimport typing\nfrom typing import List, Union\n\nfrom pyspark.sql import DataFrame, Column\nfrom pyspark.sql.functions import lit, struct, transform\nfrom pyspark.sql.types import StructType, ArrayType, MapType, DataType, IntegerType, FloatType, BooleanType, StringType, \\\n    StructField\n\n\nclass SparkUtils:\n\n    @staticmethod\n    def _get_column(df: Union[DataFrame, Column], name: str, current_type: DataType, expected_type: DataType) -> Column:\n        def with_alias(column: Column) -> Column:\n            if len(name) > 0:\n                return column.alias(name)\n            else:\n                return column\n\n        df_element = df[name] if len(name) > 0 else df\n\n        if isinstance(current_type, StructType) and isinstance(expected_type, StructType):\n            nested_columns = SparkUtils._get_columns(df_element, current_type, expected_type)\n            return with_alias(struct(*nested_columns))\n        elif isinstance(current_type, ArrayType) and isinstance(expected_type, ArrayType):\n            current_element_type = current_type.elementType\n            expected_element_type = expected_type.elementType\n\n            def array_element(row: Column):\n                return SparkUtils._get_column(row, '', current_element_type, expected_element_type)\n\n            return with_alias(transform(df_element, array_element))\n        elif isinstance(current_type, MapType) and isinstance(expected_type, MapType):\n            raise Exception(\"unsupported type map\")\n        else:\n            return with_alias(df_element)\n\n    @staticmethod\n    def _get_columns(df: Union[DataFrame, Column], current_schema: StructType, expected_schema: StructType) -> List[\n        Column]:\n        columns = []\n        for field in expected_schema.fields:\n            if field.name in current_schema.names:\n                current_column_type = current_schema[field.name].dataType\n                columns.append(SparkUtils._get_column(df, field.name, current_column_type, field.dataType))\n            else:\n                columns.append(lit(None).cast(field.dataType).alias(field.name))\n\n        return columns\n\n    @staticmethod\n    def default_missing_columns(df: DataFrame, expected_schema: StructType) -> DataFrame:\n        columns = SparkUtils._get_columns(df, df.schema, expected_schema)\n        return df.select(*columns)\n\n    @staticmethod\n    def dataclass_to_spark(tpe) -> StructType:\n        if not dataclasses.is_dataclass(tpe):\n            raise ValueError(f\"Provided type is not a dataclass: {tpe}\")\n\n        return SparkUtils.type_to_spark(tpe)[0]\n\n    @staticmethod\n    def type_to_spark(tpe) -> (DataType, bool):\n        type_mapping = {\n            int: IntegerType(),\n            float: FloatType(),\n            bool: BooleanType(),\n            str: StringType()\n        }\n\n        optional = False\n        if typing.get_origin(tpe) is Union and type(None) in typing.get_args(tpe):\n            tpe = list(filter(lambda t: not isinstance(t, type(None)), typing.get_args(tpe)))[0]\n            optional = True\n\n        if tpe in type_mapping:\n            return type_mapping[tpe], optional\n        elif typing.get_origin(tpe) is list:\n            sub_type = typing.get_args(tpe)[0]\n            (sub_spark_type, sub_optional) = SparkUtils.type_to_spark(sub_type)\n            return ArrayType(sub_spark_type, containsNull=sub_optional), optional\n        elif dataclasses.is_dataclass(tpe):\n            fields = []\n            for field in dataclasses.fields(tpe):\n                (field_type, field_optional) = SparkUtils.type_to_spark(field.type)\n                fields.append(StructField(field.name, field_type, nullable=field_optional))\n\n            return StructType(fields), optional\n        else:\n            raise ValueError(f\"Unsupported type: {tpe}\")", "\n\nclass SparkUtils:\n\n    @staticmethod\n    def _get_column(df: Union[DataFrame, Column], name: str, current_type: DataType, expected_type: DataType) -> Column:\n        def with_alias(column: Column) -> Column:\n            if len(name) > 0:\n                return column.alias(name)\n            else:\n                return column\n\n        df_element = df[name] if len(name) > 0 else df\n\n        if isinstance(current_type, StructType) and isinstance(expected_type, StructType):\n            nested_columns = SparkUtils._get_columns(df_element, current_type, expected_type)\n            return with_alias(struct(*nested_columns))\n        elif isinstance(current_type, ArrayType) and isinstance(expected_type, ArrayType):\n            current_element_type = current_type.elementType\n            expected_element_type = expected_type.elementType\n\n            def array_element(row: Column):\n                return SparkUtils._get_column(row, '', current_element_type, expected_element_type)\n\n            return with_alias(transform(df_element, array_element))\n        elif isinstance(current_type, MapType) and isinstance(expected_type, MapType):\n            raise Exception(\"unsupported type map\")\n        else:\n            return with_alias(df_element)\n\n    @staticmethod\n    def _get_columns(df: Union[DataFrame, Column], current_schema: StructType, expected_schema: StructType) -> List[\n        Column]:\n        columns = []\n        for field in expected_schema.fields:\n            if field.name in current_schema.names:\n                current_column_type = current_schema[field.name].dataType\n                columns.append(SparkUtils._get_column(df, field.name, current_column_type, field.dataType))\n            else:\n                columns.append(lit(None).cast(field.dataType).alias(field.name))\n\n        return columns\n\n    @staticmethod\n    def default_missing_columns(df: DataFrame, expected_schema: StructType) -> DataFrame:\n        columns = SparkUtils._get_columns(df, df.schema, expected_schema)\n        return df.select(*columns)\n\n    @staticmethod\n    def dataclass_to_spark(tpe) -> StructType:\n        if not dataclasses.is_dataclass(tpe):\n            raise ValueError(f\"Provided type is not a dataclass: {tpe}\")\n\n        return SparkUtils.type_to_spark(tpe)[0]\n\n    @staticmethod\n    def type_to_spark(tpe) -> (DataType, bool):\n        type_mapping = {\n            int: IntegerType(),\n            float: FloatType(),\n            bool: BooleanType(),\n            str: StringType()\n        }\n\n        optional = False\n        if typing.get_origin(tpe) is Union and type(None) in typing.get_args(tpe):\n            tpe = list(filter(lambda t: not isinstance(t, type(None)), typing.get_args(tpe)))[0]\n            optional = True\n\n        if tpe in type_mapping:\n            return type_mapping[tpe], optional\n        elif typing.get_origin(tpe) is list:\n            sub_type = typing.get_args(tpe)[0]\n            (sub_spark_type, sub_optional) = SparkUtils.type_to_spark(sub_type)\n            return ArrayType(sub_spark_type, containsNull=sub_optional), optional\n        elif dataclasses.is_dataclass(tpe):\n            fields = []\n            for field in dataclasses.fields(tpe):\n                (field_type, field_optional) = SparkUtils.type_to_spark(field.type)\n                fields.append(StructField(field.name, field_type, nullable=field_optional))\n\n            return StructType(fields), optional\n        else:\n            raise ValueError(f\"Unsupported type: {tpe}\")", ""]}
{"filename": "python/spark_ai/dbs/__init__.py", "chunked_list": [""]}
{"filename": "python/spark_ai/dbs/pinecone.py", "chunked_list": ["from dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport pinecone\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\n\nfrom spark_ai.spark import SparkUtils\n\n", "\n\n@dataclass\nclass IdVector:\n    id: str\n    vector: List[float]\n\n\n@dataclass\nclass UpsertResponse:\n    count: int = 0\n    error: Optional[str] = None", "@dataclass\nclass UpsertResponse:\n    count: int = 0\n    error: Optional[str] = None\n\n\n@dataclass\nclass QueryMatch:\n    id: str\n    score: float", "\n\n@dataclass\nclass QueryResponse:\n    matches: List[QueryMatch] = field(default_factory=lambda: [])\n    error: Optional[str] = None\n\n\nclass PineconeDB:\n\n    def __init__(self, api_key: str, environment: str):\n        self.api_key = api_key\n        self.environment = environment\n\n    def register_pinecone(self):\n        pinecone.init(api_key=self.api_key, environment=self.environment)\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('pinecone_upsert', self.upsert, returnType=self.upsert_type())\n        spark.udf.register('pinecone_query', self.query, returnType=self.query_type())\n\n    def upsert(self, index_name: str, id_vectors: List[IdVector]) -> UpsertResponse:\n        self.register_pinecone()\n\n        try:\n            index = pinecone.Index(index_name)\n            response = index.upsert([pinecone.Vector(id_vector.id, id_vector.vector) for id_vector in id_vectors])\n            return UpsertResponse(count=response['upserted_count'])\n        except Exception as error:\n            return UpsertResponse(error=str(error))\n\n    @staticmethod\n    def upsert_type() -> StructType:\n        return SparkUtils.dataclass_to_spark(UpsertResponse)\n\n    def query(self, index_name: str, vector: List[float], top_k: int = 3) -> QueryResponse:\n        self.register_pinecone()\n\n        try:\n            index = pinecone.Index(index_name)\n            response = index.query(vector=vector, top_k=top_k, include_values=False)\n            matches = [QueryMatch(id=match['id'], score=match['score']) for match in response.to_dict()['matches']]\n            return QueryResponse(matches=matches)\n        except Exception as error:\n            return QueryResponse(error=str(error))\n\n    @staticmethod\n    def query_type() -> StructType:\n        return SparkUtils.dataclass_to_spark(QueryResponse)", "class PineconeDB:\n\n    def __init__(self, api_key: str, environment: str):\n        self.api_key = api_key\n        self.environment = environment\n\n    def register_pinecone(self):\n        pinecone.init(api_key=self.api_key, environment=self.environment)\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('pinecone_upsert', self.upsert, returnType=self.upsert_type())\n        spark.udf.register('pinecone_query', self.query, returnType=self.query_type())\n\n    def upsert(self, index_name: str, id_vectors: List[IdVector]) -> UpsertResponse:\n        self.register_pinecone()\n\n        try:\n            index = pinecone.Index(index_name)\n            response = index.upsert([pinecone.Vector(id_vector.id, id_vector.vector) for id_vector in id_vectors])\n            return UpsertResponse(count=response['upserted_count'])\n        except Exception as error:\n            return UpsertResponse(error=str(error))\n\n    @staticmethod\n    def upsert_type() -> StructType:\n        return SparkUtils.dataclass_to_spark(UpsertResponse)\n\n    def query(self, index_name: str, vector: List[float], top_k: int = 3) -> QueryResponse:\n        self.register_pinecone()\n\n        try:\n            index = pinecone.Index(index_name)\n            response = index.query(vector=vector, top_k=top_k, include_values=False)\n            matches = [QueryMatch(id=match['id'], score=match['score']) for match in response.to_dict()['matches']]\n            return QueryResponse(matches=matches)\n        except Exception as error:\n            return QueryResponse(error=str(error))\n\n    @staticmethod\n    def query_type() -> StructType:\n        return SparkUtils.dataclass_to_spark(QueryResponse)", ""]}
{"filename": "python/spark_ai/webapps/slack.py", "chunked_list": ["import json\nimport math\nimport os\nimport time\nfrom typing import Dict\n\nfrom pyspark.sql import DataFrame, SparkSession\nfrom pyspark.sql.functions import udf, col, max as sql_max\nfrom pyspark.sql.types import *\nfrom slack_sdk import WebClient", "from pyspark.sql.types import *\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclass SlackUtilities:\n\n    def __init__(self, token: str, spark: SparkSession, path_tmp: str = 'dbfs:/tmp/slack_data/', limit: int = -1):\n\n        self.token = token\n        self.client = WebClient(token=token)\n\n        self.spark = spark\n\n        self.path_tmp = path_tmp\n        self.limit = limit\n\n    def read_channels(self) -> DataFrame:\n        os.makedirs(self.path_tmp, exist_ok=True)\n        path_channels = os.path.join(self.path_tmp, 'channels.json')\n\n        cursor = None\n        channels = []\n\n        while True:\n            result = self.client.conversations_list(cursor=cursor,\n                                                    limit=min(self.limit if self.limit != -1 else math.inf, 100))\n            cursor = result.data.get('response_metadata', {}).get('next_cursor')\n\n            for channel in result['channels']:\n                channels.append(channel)\n\n            if not cursor:\n                break\n\n            if self.limit != -1 and len(channels) > self.limit:\n                break\n\n        df_channels_text = self.spark.createDataFrame([Row(json.dumps(channel)) for channel in channels])\n        df_channels_text.write.mode('overwrite').text(path_channels)\n\n        return self.spark.read.json(path_channels)\n\n    def join_channels(self, df_channels: DataFrame) -> DataFrame:\n        client = self.client\n\n        @udf(returnType=StructType([\n            StructField(\"ok\", BooleanType()),\n            StructField(\"channel\", StructType([\n                StructField(\"id\", StringType())\n            ])),\n            StructField(\"warning\", StringType()),\n            StructField(\"error\", StringType())\n        ]))\n        def udf_join_channels(channel_id):\n            time.sleep(1)\n\n            try:\n                return client.conversations_join(channel=channel_id).data\n            except SlackApiError as error:\n                return error.response\n\n        return df_channels \\\n            .repartition(2) \\\n            .withColumn('result', udf_join_channels(col('id')))\n\n    @staticmethod\n    def find_max_ts_per_channel(df_conversations: DataFrame) -> Dict[str, float]:\n        results = df_conversations \\\n            .groupBy(col('channel_id')) \\\n            .agg(sql_max(col('ts')).alias('max_ts')) \\\n            .select('channel_id', 'max_ts') \\\n            .collect()\n\n        max_ts_per_channel = {}\n        for row in results:\n            max_ts_per_channel[row['channel_id']] = row['max_ts']\n\n        return max_ts_per_channel\n\n    def read_conversations(self, df_channels: DataFrame, max_ts_per_channel=None) -> DataFrame:\n        # ------------------------------------------------\n        # Fetches the already last timestamps for channels\n        # ------------------------------------------------\n\n        if max_ts_per_channel is None:\n            max_ts_per_channel = {}\n\n        # ------------------------------------------------\n        # Fetches the latest conversations\n        # ------------------------------------------------\n\n        full_conversation_history_size = 0\n        conversation_history_batch = []\n        batch_idx = 0\n\n        path_conversations = os.path.join(self.path_tmp, f'conversations/')\n\n        if self.limit == -1:\n            channels = df_channels.collect()\n        else:\n            channels = df_channels.limit(self.limit).collect()\n\n        for idx, channel in enumerate(channels):\n            channel_id = channel['id']\n\n            if channel['is_member'] is not True:\n                continue\n\n            # Call the conversations.history method using the WebClient\n            # conversations.history returns the first 100 messages by default\n            # These results are paginated, see: https://api.slack.com/methods/conversations.history$pagination\n\n            cursor = None\n            while True:\n                result = self._get_conversations_history(channel_id, cursor=cursor,\n                                                         limit=min(self.limit if self.limit != -1 else math.inf,\n                                                                   10 * 1000),\n                                                         oldest=max_ts_per_channel.get(channel_id, \"0\"))\n                cursor = result.data.get('response_metadata', {}).get('next_cursor')\n\n                for message in result[\"messages\"]:\n                    message['channel_id'] = channel_id\n                    conversation_history_batch.append(message)\n\n                    if 'thread_ts' in message and message['thread_ts'] is not None:\n                        for thread_message in self._get_all_conversations_replies(channel_id, message['thread_ts']):\n                            conversation_history_batch.append(thread_message)\n\n                    if len(conversation_history_batch) > (self.limit if self.limit != -1 else math.inf):\n                        break\n\n                print(\n                    f\"Progress: {idx} out of {len(channels)} channels and {full_conversation_history_size + len(conversation_history_batch)} messages total\")\n\n                if len(conversation_history_batch) > min(self.limit if self.limit != -1 else math.inf, 10 * 1000):\n                    df_batch = self.spark.createDataFrame(\n                        [Row(json.dumps(message)) for message in conversation_history_batch])\n                    mode = 'overwrite' if batch_idx == 0 else 'append'\n                    df_batch.write.mode(mode).text(path_conversations)\n\n                    full_conversation_history_size += len(conversation_history_batch)\n\n                    conversation_history_batch = []\n                    batch_idx += 1\n\n                if not cursor:\n                    break\n\n                if full_conversation_history_size > self.limit:\n                    break\n\n            if full_conversation_history_size > self.limit:\n                break\n\n        if len(conversation_history_batch) > 0:\n            df_batch = self.spark.createDataFrame([Row(json.dumps(message)) for message in conversation_history_batch])\n            mode = 'overwrite' if batch_idx == 0 else 'append'\n            df_batch.write.mode(mode).text(path_conversations)\n\n        return self.spark.read.json(path_conversations)\n\n    def _list_to_df(self, elements):\n        R = Row('data')\n        rows = [R(json.dumps(element)) for element in elements]\n        return self.spark.createDataFrame(rows)\n\n    def _any_to_df(self, obj):\n        R = Row('data')\n        rows = [R(json.dumps(obj))]\n        return self.spark.createDataFrame(rows)\n\n    def _get_conversations_history(self, channel_id, cursor=None, limit=500, max_retries=5, oldest=\"0\"):\n        retries = 0\n        while retries <= max_retries:\n            try:\n                response = self.client.conversations_history(channel=channel_id, cursor=cursor, limit=limit,\n                                                             oldest=oldest)\n                return response\n            except SlackApiError as e:\n                if 'error' in e.response and e.response[\"error\"] == \"ratelimited\":\n                    retries += 1\n                    time.sleep(5)  # wait for 5 seconds before retrying\n                else:\n                    raise e  # re-raise the exception if it's not a rate limit error\n        raise Exception(\"Reached maximum number of retries\")\n\n    def _get_conversations_replies(self, channel_id, thread_ts, cursor=None, limit=500, max_retries=5):\n        retries = 0\n        while retries <= max_retries:\n            try:\n                response = self.client.conversations_replies(channel=channel_id, ts=thread_ts, cursor=cursor,\n                                                             limit=limit)\n                return response\n            except SlackApiError as e:\n                if 'error' in e.response and e.response[\"error\"] == \"ratelimited\":\n                    retries += 1\n                    time.sleep(5)  # wait for 5 seconds before retrying\n                else:\n                    raise e  # re-raise the exception if it's not a rate limit error\n        raise Exception(\"Reached maximum number of retries\")\n\n    def _get_all_conversations_replies(self, channel_id, thread_ts):\n        messages = []\n        cursor = None\n        while True:\n            result = self._get_conversations_replies(channel_id, thread_ts, cursor=cursor)\n            for message in result['messages']:\n                message['channel_id'] = channel_id\n                messages.append(message)\n\n            if not cursor:\n                break\n\n        return messages\n\n    def write_messages(self, df: DataFrame):\n        if not df.isStreaming:\n            raise TypeError(\"Slack messages write is for streaming pipelines only\")\n\n        df.writeStream.outputMode(\"update\").foreachBatch(self._write_batch).start()\n\n    def _write_batch(self, df_batch: DataFrame, epoch_id: int):\n        responses = df_batch.collect()\n        for response in responses:\n            client = WebClient(token=self.token)\n\n            try:\n                client.chat_postMessage(\n                    channel=response[\"channel\"],\n                    text=response[\"answer\"],\n                    thread_ts=response[\"ts\"]\n                )\n            except SlackApiError as e:\n                assert e.response[\"ok\"] is False\n                assert e.response[\"error\"]\n                print(f\"Got an error: {e.response}\")", ""]}
{"filename": "python/spark_ai/webapps/__init__.py", "chunked_list": ["import requests\nfrom bs4 import BeautifulSoup\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, BinaryType\n\n\nclass WebUtils:\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('web_scrape', self.scrape, returnType=BinaryType())\n        spark.udf.register('web_scrape_text', self.scrape_text, returnType=StringType())\n\n    @staticmethod\n    def scrape(url: str):\n        response = requests.get(url)\n\n        return response.content\n\n    @staticmethod\n    def scrape_text(url: str):\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        text = soup.get_text(' ')\n\n        return text", ""]}
{"filename": "python/spark_ai/llms/openai.py", "chunked_list": ["from typing import List\n\nimport openai\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, ArrayType, FloatType, StringType\n\n\nclass OpenAiLLM:\n    default_embedding_model = 'text-embedding-ada-002'\n    default_chat_model = 'gpt-3.5-turbo'\n    default_prompt_qa = \"\"\"Answer the question based on the context below.\n\nContext:\n```\n{context}\n```\n\nQuestion: \n```\n{query}\n```\n\nAnswer:\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def register_openai(self):\n        openai.api_key = self.api_key\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('openai_embed_texts', self.embed_texts, returnType=self.embed_texts_type())\n\n        spark.udf.register('openai_chat_complete', self.chat_complete, returnType=self.chat_complete_type())\n        spark.udf.register('openai_answer_question', self.answer_question, returnType=self.chat_complete_type())\n\n    def embed_texts(self, texts: List[str], model: str = default_embedding_model):\n        self.register_openai()\n\n        try:\n            response = openai.Embedding.create(\n                model=model,\n                input=texts\n            )\n\n            return {'embeddings': [embedding['embedding'] for embedding in response['data']], 'error': None}\n        except Exception as error:\n            return {'embeddings': None, 'error': str(error)}\n\n    @staticmethod\n    def embed_texts_type():\n        return StructType([\n            StructField('embeddings', ArrayType(ArrayType(FloatType()))),\n            StructField('error', StringType())\n        ])\n\n    def chat_complete(self, prompt: str, model: str = default_chat_model):\n        self.register_openai()\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=[self.chat_as_user(prompt)]\n            )\n\n            return {'choices': [choice['message']['content'] for choice in response['choices']], 'error': None}\n        except Exception as error:\n            return {'choices': None, 'error': str(error)}\n\n    @staticmethod\n    def chat_complete_type() -> StructType:\n        return StructType([\n            StructField('choices', ArrayType(StringType())),\n            StructField('error', StringType())\n        ])\n\n    @staticmethod\n    def chat_as_user(query: str):\n        return {'role': 'user', 'content': query}\n\n    @staticmethod\n    def chat_as_assistant(query: str):\n        return {'role': 'user', 'content': query}\n\n    def answer_question(self, context: str, query: str, template: str = default_prompt_qa,\n                        model: str = default_chat_model):\n        return self.chat_complete(template.format(context=context, query=query), model=model)", ""]}
{"filename": "python/spark_ai/llms/__init__.py", "chunked_list": [""]}
{"filename": "python/spark_ai/files/text.py", "chunked_list": ["from typing import List\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import ArrayType, StringType\n\n\nclass FileTextUtils:\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('text_split_into_chunks', self.split_into_chunks, returnType=ArrayType(StringType()))\n\n    @staticmethod\n    def split_into_chunks(text: str, chunk_size: int = 100) -> List[str]:\n        lst = text.split(' ')\n        return [' '.join(lst[i:i + chunk_size]) for i in range(0, len(lst), chunk_size)]", ""]}
{"filename": "python/spark_ai/files/pdf.py", "chunked_list": ["from io import BytesIO\nfrom typing import List\n\nfrom unstructured.partition.pdf import partition_pdf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\n\n\nclass FilePDFUtils:\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('pdf_parse', self.parse, returnType=self.parse_type())\n\n    @staticmethod\n    def parse(binary: bytes) -> List[dict]:\n        elements = partition_pdf(file=BytesIO(binary))\n        return [element.to_dict() for element in elements]\n\n    @staticmethod\n    def parse_type() -> DataType:\n        return ArrayType(StructType([\n            StructField(\"element_id\", StringType()),\n            StructField(\"coordinates\", ArrayType(FloatType())),\n            StructField(\"text\", StringType()),\n            StructField(\"type\", StringType()),\n            StructField(\"metadata\", StructType([\n                StructField(\"filename\", StringType()),\n                StructField(\"filetype\", StringType()),\n                StructField(\"page_number\", IntegerType())\n            ]))\n        ]))", "class FilePDFUtils:\n\n    def register_udfs(self, spark: SparkSession):\n        spark.udf.register('pdf_parse', self.parse, returnType=self.parse_type())\n\n    @staticmethod\n    def parse(binary: bytes) -> List[dict]:\n        elements = partition_pdf(file=BytesIO(binary))\n        return [element.to_dict() for element in elements]\n\n    @staticmethod\n    def parse_type() -> DataType:\n        return ArrayType(StructType([\n            StructField(\"element_id\", StringType()),\n            StructField(\"coordinates\", ArrayType(FloatType())),\n            StructField(\"text\", StringType()),\n            StructField(\"type\", StringType()),\n            StructField(\"metadata\", StructType([\n                StructField(\"filename\", StringType()),\n                StructField(\"filetype\", StringType()),\n                StructField(\"page_number\", IntegerType())\n            ]))\n        ]))", ""]}
{"filename": "python/spark_ai/files/__init__.py", "chunked_list": [""]}
