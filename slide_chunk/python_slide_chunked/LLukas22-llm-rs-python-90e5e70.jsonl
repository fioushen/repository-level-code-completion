{"filename": "build_scripts/pyproject_patcher.py", "chunked_list": ["import toml\nimport os\nimport sys\n\nif __name__ == \"__main__\":\n\n    file = \"pyproject.toml\"\n    if len(sys.argv) > 1:\n        file = sys.argv[1]\n        \n    name = os.environ.get(\"PYPROJECT_NAME\",\"llm-rs\")\n\n    try:\n        data = toml.load(open(file,\"r\",encoding=\"utf-8\"))\n        data[\"project\"][\"name\"] = name\n        toml.dump(data,open(\"pyproject.toml.new\",\"w\",encoding=\"utf-8\"))\n        #Only replace if no error occured\n        os.remove(\"pyproject.toml\")\n        os.rename(\"pyproject.toml.new\",\"pyproject.toml\")\n        print(f\"Changed projectname of file {file} to {name}!\")\n    except Exception as e:\n        print(f\"Could not change projectname of file {file} to {name}!\")\n        print(e)", "\n\n"]}
{"filename": "build_scripts/repair_windows_wheels.py", "chunked_list": ["from delvewheel._wheel_repair import WheelRepair\nimport sys\nimport pathlib\n\nif __name__ == \"__main__\":\n\n    args = sys.argv\n    wheel_dir = args[1] if len(args) > 1 else \".\"\n    wheel_dir = pathlib.Path(wheel_dir).absolute()\n    print(f\"Repairing wheels in {wheel_dir}\")\n\n    wheels = list(wheel_dir.glob(\"*.whl\"))\n    print(f\"Found {len(wheels)} wheels\")\n    \n    target_dir = pathlib.Path(\"./wheelhouse\").absolute()\n    for wheel in wheels:\n        print(f\"Repairing {wheel}\")\n        wr = WheelRepair(whl_path=wheel,\n                         extract_dir=None,\n                         add_dlls=None,\n                         no_dlls=None,\n                         ignore_in_wheel=True, \n                         verbose=0,\n                         test=[])\n        wr.show()\n        wr.repair(target=target_dir,\n                  no_mangles=set(),\n                  no_mangle_all=False,\n                  strip=False,\n                  lib_sdir=\".libs\",\n                  no_diagnostic=False)", "                    "]}
{"filename": "llm_rs/auto.py", "chunked_list": ["from .config import QuantizationType,ContainerType,SessionConfig\nimport os\nimport pathlib\nfrom .models import Mpt,GptNeoX,GptJ,Gpt2,Bloom,Llama\nfrom .base_model import Model\nimport logging\nfrom typing import Optional, List, Union,Type,Dict, Callable\nimport os\nfrom enum import Enum, auto\nfrom dataclasses import dataclass", "from enum import Enum, auto\nfrom dataclasses import dataclass\nimport json\nfrom blake3 import blake3\nfrom huggingface_hub import snapshot_download\nfrom huggingface_hub.utils import validate_repo_id, HFValidationError\nfrom huggingface_hub import cached_assets_path\n\nclass QuantizationVersions(Enum):\n    Not_Quantized=auto()\n    V1=auto()\n    V2=auto()", "class QuantizationVersions(Enum):\n    Not_Quantized=auto()\n    V1=auto()\n    V2=auto()\n\nclass KnownModels(Enum):\n    GptNeoX = auto()\n    Mpt = auto()\n    GptJ = auto()\n    Gpt2 = auto()\n    Bloom = auto()\n    Llama = auto()", "\n\n_QUANTIZATION_TYPE_MAP = {\n    \"Q4_0\": QuantizationType.Q4_0,\n    \"Q4_1\": QuantizationType.Q4_1,\n    \"Q5_0\": QuantizationType.Q5_0,\n    \"Q5_1\": QuantizationType.Q5_1,\n    \"Q8_0\": QuantizationType.Q8_0,\n    \"F16\": QuantizationType.F16\n}", "    \"F16\": QuantizationType.F16\n}\n\n_CONTAINER_TYPE_MAP = {\n    \"GGML\": ContainerType.GGML,\n    \"GGJT\": ContainerType.GGJT\n}\n\n_KNOWN_MODELS_MAP = {\n    KnownModels.GptNeoX: GptNeoX,", "_KNOWN_MODELS_MAP = {\n    KnownModels.GptNeoX: GptNeoX,\n    KnownModels.Mpt: Mpt,\n    KnownModels.GptJ: GptJ,\n    KnownModels.Gpt2: Gpt2,\n    KnownModels.Bloom: Bloom,\n    KnownModels.Llama: Llama\n}\n\n", "\n\n_STRING_TO_KNOWN_MODEL_MAP = {\n    \"gpt2\": KnownModels.Gpt2,\n    \"starcoder\": KnownModels.Gpt2,\n    \"gpt_neox\": KnownModels.GptNeoX,\n    \"dolly-v2\": KnownModels.GptNeoX,\n    \"llama\": KnownModels.Llama,\n    \"mpt\": KnownModels.Mpt,\n    \"gptj\": KnownModels.GptJ,", "    \"mpt\": KnownModels.Mpt,\n    \"gptj\": KnownModels.GptJ,\n    \"bloom\": KnownModels.Bloom,\n}\n\nCURRENT_QUANTIZATION_VERSION = QuantizationVersions.V2\n\nclass PathType(Enum):\n    DIR = auto()\n    FILE = auto()\n    REPO = auto()\n    UNKNOWN = auto() ", "\ndef _get_path_type(path: Union[str,os.PathLike]) -> PathType:\n    p = pathlib.Path(path)\n    if p.is_file():\n        return PathType.FILE\n    elif p.is_dir():\n        return PathType.DIR\n    try:\n        validate_repo_id(str(path))\n        return PathType.REPO\n    except HFValidationError:\n        pass\n    return PathType.UNKNOWN", "\n\n@dataclass()\nclass ModelMetadata():\n    \"\"\"\n    A dataclass to store metadata about a model.\n    \"\"\"\n    model: KnownModels\n    quantization: QuantizationType\n    container: ContainerType\n    quantization_version: QuantizationVersions=QuantizationVersions.Not_Quantized\n    converter:str=\"llm-rs\"\n    hash:Optional[str]=None\n    base_model:Optional[str]=None\n\n    def add_hash(self,model_path:Union[str,os.PathLike]):\n        h  = blake3(max_threads=blake3.AUTO)\n        b  = bytearray(128_000_000)\n        mv = memoryview(b)\n        with open(model_path, 'rb', buffering=0) as f:\n            for n in iter(lambda : f.readinto(mv), 0):\n                h.update(mv[:n])\n        self.hash=h.hexdigest()\n\n    def serialize(self):\n        return {\n            \"model\": self.model.name,\n            \"quantization\": repr(self.quantization).split(\".\")[-1],\n            \"quantization_version\": self.quantization_version.name,\n            \"container\": repr(self.container).split(\".\")[-1],\n            \"converter\": self.converter,\n            \"hash\": self.hash,\n            \"base_model\": self.base_model\n        }\n    \n    @staticmethod\n    def deserialize(metadata_dict:Dict[str,str])->\"ModelMetadata\":\n        return ModelMetadata(\n            model = KnownModels[metadata_dict[\"model\"]],\n            quantization = _QUANTIZATION_TYPE_MAP[metadata_dict[\"quantization\"]],\n            quantization_version= QuantizationVersions[metadata_dict[\"quantization_version\"]],\n            container = _CONTAINER_TYPE_MAP[metadata_dict[\"container\"]],\n            converter = metadata_dict[\"converter\"],\n            hash = metadata_dict.get(\"hash\"),\n            base_model = metadata_dict.get(\"base_model\")\n        )", "\n\n@dataclass\nclass AutoConfig():\n    repo_type:Optional[str] = None\n    model_type: Optional[str] = None\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_path_or_repo_id: Union[str, os.PathLike],\n        **kwargs,\n    ) -> 'AutoConfig':\n        path_type = _get_path_type(model_path_or_repo_id)\n        path = pathlib.Path(model_path_or_repo_id)\n        if path_type == PathType.UNKNOWN:\n            raise ValueError(\n                f\"Model path '{model_path_or_repo_id}' doesn't exist.\")\n        elif path_type == PathType.FILE:\n            path = path.resolve().parent\n        \n        auto_config = AutoConfig()\n        if path_type == PathType.DIR:\n            cls._update_from_dir(str(path), auto_config)\n        elif path_type == PathType.REPO:\n            cls._update_from_repo(str(model_path_or_repo_id), auto_config)\n\n        return auto_config\n\n    @classmethod\n    def _update_from_repo(\n        cls,\n        repo_id: str,\n        auto_config: 'AutoConfig',\n    ) -> None:\n        path = snapshot_download(repo_id=repo_id, allow_patterns='config.json')\n        cls._update_from_dir(path, auto_config)\n\n    @classmethod\n    def _update_from_dir(cls, path: str, auto_config: 'AutoConfig') -> None:\n        resolved_path = (pathlib.Path(path) / 'config.json').resolve()\n        if resolved_path.is_file():\n            cls._update_from_file(str(resolved_path), auto_config)\n        else:\n            raise ValueError(f\"Config path '{resolved_path}' doesn't exist.\")\n\n    @classmethod\n    def _update_from_file(cls, path: str, auto_config: 'AutoConfig') -> None:\n        with open(path) as f:\n            config = json.load(f)\n        auto_config.model_type = config.get('model_type')\n        if 'repo_type' in config:\n            auto_config.repo_type = config.get('repo_type')\n        elif len(config) == 1:\n            auto_config.repo_type = \"GGML\"\n        else:\n            auto_config.repo_type = \"HuggingFace\"", "            \n        \n\nclass AutoModel():\n    \"\"\"\n    Utility to load models, without having to specify the model type.\n    \"\"\"\n\n    @classmethod\n    def has_metadata_file(cls,model_file:Union[str,os.PathLike])->bool:\n        path = pathlib.Path(model_file)\n        metadata_file = path.with_suffix(\".meta\")\n        return metadata_file.exists()\n\n    @classmethod\n    def load_metadata(cls,model_file:Union[str,os.PathLike])->ModelMetadata:\n        path = pathlib.Path(model_file)\n        if not path.is_file():\n            raise ValueError(f\"Model file '{model_file}' is not a file!\")\n        if not path.exists():\n            raise ValueError(f\"Model file '{model_file}' does not exist!\")\n\n        metadata_file = path.with_suffix(\".meta\")\n        if not metadata_file.exists():\n            raise ValueError(f\"Model file '{model_file}' does not have a metadata file '{metadata_file}'! If you want to autoload this model, please specify the model type.\")\n        \n        metadata = ModelMetadata.deserialize(json.loads(metadata_file.read_text()))\n        return metadata\n    \n    @classmethod\n    def _infer_model_type(cls,model_file:Union[str,os.PathLike],known_model:Optional[KnownModels]=None,config:Optional[AutoConfig]=None)->Type[Model]:\n        model_to_lookup = None\n        if known_model:\n            model_to_lookup = known_model\n        elif cls.has_metadata_file(model_file):\n            metadata = cls.load_metadata(model_file)\n            model_to_lookup = metadata.model\n        elif config and config.model_type:\n            if config.model_type.lower() in _STRING_TO_KNOWN_MODEL_MAP:\n                model_to_lookup = _STRING_TO_KNOWN_MODEL_MAP[config.model_type.lower()]\n            else:\n                raise ValueError(f\"Unknown model type '{config.model_type}' in config file '{model_file}'! Please specify the model type via `known_model`.\")\n        else:\n            raise ValueError(f\"Model file '{model_file}' does not have a metadata or config file and no model type was specified! Please specify the model type via `known_model`.\")\n            \n\n        if model_to_lookup in _KNOWN_MODELS_MAP:\n            return _KNOWN_MODELS_MAP[model_to_lookup]\n        else:\n            raise ValueError(f\"Unknown model type '{model_to_lookup}'\")\n            \n        \n    @classmethod\n    def from_file(cls, \n                  path:Union[str,os.PathLike],\n                  config:Optional[AutoConfig],\n                  model_type: Optional[KnownModels] = None,\n                  session_config:SessionConfig=SessionConfig(),\n                  tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n                  lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n                  verbose:bool=False,\n                  use_hf_tokenizer:bool=True)->Model:\n        \n        tokenizer = tokenizer_path_or_repo_id\n        if use_hf_tokenizer and tokenizer is None:\n            try:\n                metadata = cls.load_metadata(path)\n                tokenizer = metadata.base_model\n            except Exception as e:\n                logging.warning(f\"Could not load metadata for model '{path}'!\")\n\n            if tokenizer is None or tokenizer == \"\":\n                logging.warning(f\"Model file '{path}' does not have a base_model specified in its metadata file but wants to use a huggingface-tokenizer! Please expilicitly specify a tokenizer via `tokenizer_path_or_repo_id` if you intend to use a huggingface-tokenizer.\")\n\n        model = cls._infer_model_type(path,model_type,config)\n        return model(path,session_config,tokenizer_path_or_repo_id,lora_paths,verbose)\n    \n    @classmethod\n    def from_pretrained(cls,\n        model_path_or_repo_id: Union[str,os.PathLike],\n        model_file: Optional[str] = None,\n        model_type: Optional[KnownModels] = None,\n        session_config:SessionConfig=SessionConfig(),\n        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n        verbose:bool=False,\n        use_hf_tokenizer:bool=True,\n        default_quantization:QuantizationType=QuantizationType.Q4_0,\n        default_container:ContainerType=ContainerType.GGJT)->Model:\n\n        try: \n            config = AutoConfig.from_pretrained(\n                model_path_or_repo_id,\n            )\n        except ValueError:\n            logging.warning(\"Could not find config.json in repo, assuming GGML model...\")\n            config = AutoConfig(repo_type=\"GGML\")\n\n        if model_file:\n            config.repo_type = \"GGML\"\n        \n        path_type = _get_path_type(model_path_or_repo_id)\n\n        if path_type == PathType.UNKNOWN:\n            raise ValueError(f\"Unknown path type for '{model_path_or_repo_id}'\")\n        elif path_type == PathType.FILE:\n            return cls.from_file(model_path_or_repo_id,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n        else:\n            if path_type == PathType.REPO:\n                if config.repo_type != \"GGML\":\n                    logging.warning(\"Found normal HuggingFace model, starting conversion...\")\n                    return cls.from_transformer(model_path_or_repo_id, session_config, tokenizer_path_or_repo_id, lora_paths, verbose, use_hf_tokenizer,default_quantization, default_container)\n            \n                resolved_path = cls._find_model_path_from_repo(str(model_path_or_repo_id),model_file)\n                return cls.from_file(resolved_path,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n            \n            elif path_type == PathType.DIR:\n                resolved_path = cls._find_model_path_from_dir(str(model_path_or_repo_id),model_file)\n                return cls.from_file(resolved_path,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n            \n            else:\n                raise ValueError(f\"Unknown path type '{path_type}'\")\n\n\n    @classmethod\n    def _find_model_path_from_dir(\n        cls,\n        directory: str,\n        filename: Optional[str] = None,\n    ) -> str:\n        path = pathlib.Path(directory).resolve()\n        if filename:\n            file = (path / filename)\n            if not file.is_file():\n                raise ValueError(\n                    f\"Model file '{filename}' not found in '{path}'\")\n            return str(file)\n\n        files = [\n            f for f in path.iterdir()\n            if f.is_file() and f.name.endswith('.bin')\n        ]\n\n        if len(files) == 0:\n            raise ValueError(f\"No model files found in '{path}'\")\n        elif len(files) > 1:\n            raise ValueError(\n                f\"Multiple model files found in '{path}'! Please specify one of the following model files to load: '{','.join([f.name for f in files])}'\"\n            )\n\n        return str(files[0].resolve())\n    \n    @classmethod\n    def _find_model_path_from_repo(\n        cls,\n        repo_id: str,\n        filename: Optional[str] = None,\n    ) -> str:\n        cache_directory = cached_assets_path(\"llm-rs\",namespace=repo_id)\n        #we dont want to download the whole repo, just the metadata\n        directory = snapshot_download(repo_id=repo_id,\n                                allow_patterns='*.meta',\n                                local_dir=cache_directory)\n        path = pathlib.Path(directory).resolve()\n        files = [\n            f for f in path.iterdir()\n            if f.is_file() and f.name.endswith('.meta')\n        ]\n\n        if len(files) == 0 and filename is None:\n            raise ValueError(f\"No model files found in '{repo_id}'! Either specify the model file via the `model_file` parameter or make sure the repository contains a *.meta files for each model.\")\n        if len(files) > 1 and filename is None:\n            raise ValueError(\n                f\"Multiple model files found in '{repo_id}'! Please specify one of the following model files to load: '{','.join([f.name.replace('.meta','.bin') for f in files])}' \"\n            )\n        #if we have a filename, we can just download that file\n        filename = filename if filename else files[0].name.replace('.meta','.bin')\n        model_directory = snapshot_download(repo_id=repo_id,\n                                allow_patterns=filename,\n                                local_dir=cache_directory)\n        \n        return cls._find_model_path_from_dir(model_directory, filename=filename)\n    \n    @classmethod\n    def from_transformer(cls,\n        model_path_or_repo_id: Union[str,os.PathLike],\n        session_config:SessionConfig=SessionConfig(),\n        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n        verbose:bool=False,\n        use_hf_tokenizer:bool=True,\n        default_quantization:QuantizationType=QuantizationType.Q4_0,\n        default_container:ContainerType=ContainerType.GGJT):\n        \n        try:\n            from .convert import AutoConverter\n            from .convert.auto_converter import get_name_from_config\n            from transformers import AutoConfig\n        except ImportError:\n            raise ImportError(\"Model conversion needs additional dependencies. Please install 'llm-rs[convert]' to use this functionality!\")\n        \n\n        config = AutoConfig.from_pretrained(model_path_or_repo_id,trust_remote_code=True)\n        model_name = get_name_from_config(config)\n        export_path = cached_assets_path(\"llm-rs\",namespace=model_name)\n        converted_model = AutoConverter.convert(model_path_or_repo_id,export_path)\n        if default_quantization != QuantizationType.F16:\n            converted_model = AutoQuantizer.quantize(converted_model,quantization=default_quantization,container=default_container)\n        return cls.from_file(converted_model,None,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)", "    \n# Hack to make the quantization type enum hashable\n_APPENDIX_MAP = {\n    QuantizationType.Q4_0.__repr__(): \"q4_0\",\n    QuantizationType.Q4_1.__repr__(): \"q4_1\",\n    QuantizationType.Q5_0.__repr__(): \"q5_0\",\n    QuantizationType.Q5_1.__repr__(): \"q5_1\",\n    QuantizationType.Q8_0.__repr__(): \"q8_0\",\n}\n\nclass AutoQuantizer():\n    \"\"\"\n    Utility to quantize models, without having to specify the model type.\n    \"\"\"\n    @staticmethod\n    def quantize(\n        model_file:Union[str,os.PathLike],\n        target_path:Optional[Union[str,os.PathLike]]=None,\n        quantization:QuantizationType=QuantizationType.Q4_0,\n        container:ContainerType=ContainerType.GGJT,\n        callback:Optional[Callable[[str],None]]=None\n        )->Union[str,os.PathLike]:\n        metadata=AutoModel.load_metadata(model_file)\n        if metadata.quantization != QuantizationType.F16:\n            raise ValueError(f\"Model '{model_file}' is already quantized to '{metadata.quantization}'\")\n\n        model_type = AutoModel._infer_model_type(model_file)\n\n        if target_path is None:\n            target_path = os.path.dirname(model_file)\n            \n        def build_target_name()->str:\n            output_path = pathlib.Path(target_path)\n            if output_path.is_file():\n                return str(output_path)\n            else:\n                output_path.mkdir(parents=True,exist_ok=True)\n                model_path = pathlib.Path(model_file)\n                appendix = \"\"\n\n                if quantization.__repr__() in _APPENDIX_MAP:\n                    appendix += f\"-{_APPENDIX_MAP[quantization.__repr__()]}\"\n\n                if container == ContainerType.GGJT:\n                    appendix += \"-ggjt\"\n            \n                filename = model_path.stem.replace(\"-f16\",\"\") + appendix + model_path.suffix\n                return str(output_path / filename)\n\n        target_file = build_target_name()\n        if pathlib.Path(target_file).exists():\n            logging.warning(f\"Target file '{target_file}' already exists, skipping quantization\")\n            return target_file\n        \n        logging.info(f\"Quantizing model '{model_file}' to '{target_file}'\")\n        model_type.quantize(str(model_file),target_file,quantization,container,callback=callback)\n\n        metadata_file = pathlib.Path(target_file).with_suffix(\".meta\")\n        quantized_metadata = ModelMetadata(model=metadata.model,quantization=quantization,container=container,quantization_version=CURRENT_QUANTIZATION_VERSION,base_model=metadata.base_model)\n        quantized_metadata.add_hash(target_file)\n        logging.info(f\"Writing metadata file '{metadata_file}'\")\n        metadata_file.write_text(json.dumps(quantized_metadata.serialize(),indent=4))\n        logging.info(f\"Finished quantizing model '{model_file}' to '{target_file}'\")\n        return target_file", "}\n\nclass AutoQuantizer():\n    \"\"\"\n    Utility to quantize models, without having to specify the model type.\n    \"\"\"\n    @staticmethod\n    def quantize(\n        model_file:Union[str,os.PathLike],\n        target_path:Optional[Union[str,os.PathLike]]=None,\n        quantization:QuantizationType=QuantizationType.Q4_0,\n        container:ContainerType=ContainerType.GGJT,\n        callback:Optional[Callable[[str],None]]=None\n        )->Union[str,os.PathLike]:\n        metadata=AutoModel.load_metadata(model_file)\n        if metadata.quantization != QuantizationType.F16:\n            raise ValueError(f\"Model '{model_file}' is already quantized to '{metadata.quantization}'\")\n\n        model_type = AutoModel._infer_model_type(model_file)\n\n        if target_path is None:\n            target_path = os.path.dirname(model_file)\n            \n        def build_target_name()->str:\n            output_path = pathlib.Path(target_path)\n            if output_path.is_file():\n                return str(output_path)\n            else:\n                output_path.mkdir(parents=True,exist_ok=True)\n                model_path = pathlib.Path(model_file)\n                appendix = \"\"\n\n                if quantization.__repr__() in _APPENDIX_MAP:\n                    appendix += f\"-{_APPENDIX_MAP[quantization.__repr__()]}\"\n\n                if container == ContainerType.GGJT:\n                    appendix += \"-ggjt\"\n            \n                filename = model_path.stem.replace(\"-f16\",\"\") + appendix + model_path.suffix\n                return str(output_path / filename)\n\n        target_file = build_target_name()\n        if pathlib.Path(target_file).exists():\n            logging.warning(f\"Target file '{target_file}' already exists, skipping quantization\")\n            return target_file\n        \n        logging.info(f\"Quantizing model '{model_file}' to '{target_file}'\")\n        model_type.quantize(str(model_file),target_file,quantization,container,callback=callback)\n\n        metadata_file = pathlib.Path(target_file).with_suffix(\".meta\")\n        quantized_metadata = ModelMetadata(model=metadata.model,quantization=quantization,container=container,quantization_version=CURRENT_QUANTIZATION_VERSION,base_model=metadata.base_model)\n        quantized_metadata.add_hash(target_file)\n        logging.info(f\"Writing metadata file '{metadata_file}'\")\n        metadata_file.write_text(json.dumps(quantized_metadata.serialize(),indent=4))\n        logging.info(f\"Finished quantizing model '{model_file}' to '{target_file}'\")\n        return target_file", "\n\n"]}
{"filename": "llm_rs/__init__.py", "chunked_list": ["try:\n    from .llm_rs import *\nexcept ImportError as e:\n    print(\"DLLs were not boundled with this package. Trying to locate them...\")\n    import os\n    import platform\n\n    #Try to locate CUDA_PATH environment variable\n    cuda_path = os.environ.get(\"CUDA_PATH\",None)\n    if cuda_path:\n        print(f\"Found CUDA_PATH environment variable: {cuda_path}\")\n        if platform.system() == \"Windows\":\n            cuda_path = os.path.join(cuda_path,\"bin\")\n        else:\n            cuda_path = os.path.join(cuda_path,\"lib64\")\n\n        print(f\"Adding {cuda_path} to DLL search path...\")\n        os.add_dll_directory(cuda_path)\n\n    try:\n        from .llm_rs import *\n    except ImportError as inner_e:\n        raise  ImportError(\"Could not locate DLLs. Please check the documentation for more information.\")", "   \n\n\nfrom .config import GenerationConfig, SessionConfig, Precision, ContainerType, QuantizationType\nfrom .models import Llama, GptJ, Gpt2, Bloom, GptNeoX, Mpt\nfrom .auto import AutoModel, ModelMetadata, KnownModels, AutoQuantizer\n\n__doc__ = llm_rs.__doc__\nif hasattr(llm_rs, \"__all__\"):\n    __all__ = llm_rs.__all__", "if hasattr(llm_rs, \"__all__\"):\n    __all__ = llm_rs.__all__"]}
{"filename": "llm_rs/base_model.py", "chunked_list": ["from typing import Optional, Callable, List, Union, Generator\nfrom abc import ABC\nimport os\n\nfrom .config import GenerationConfig, SessionConfig, ContainerType, QuantizationType\nfrom .results import GenerationResult\n\n\n\n#Theoretically this is incorrect as the 'model' doesnt actually exist, but it is a good enough approximation for now. \nclass Model(ABC):\n    \"\"\"\n    Wrapper around a llm model.\n    \"\"\"\n    config:SessionConfig\n    \n    @property\n    def path(self)->str: ...\n    \n    @property\n    def verbose(self)->bool: ...\n\n    @property\n    def lora_paths(self)->Optional[List[str]]: ...\n\n    def  __init__(self,\n                  path:Union[str,os.PathLike],\n                  session_config:SessionConfig=SessionConfig(),\n                  tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None,\n                  lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n                  verbose:bool=False) -> None: ...\n    \n    def generate(self,prompt:str,\n                 generation_config:Optional[GenerationConfig]=None,\n                 callback:Optional[Callable[[str],Optional[bool]]]=None) -> GenerationResult: \n        \"\"\"\n        Generates text from a prompt.\n        \"\"\" \n        ...\n\n    def generate(self,prompt:str) -> List[float]: \n        \"\"\"\n        Embed a given prompt into vector representation.\n        \"\"\" \n        ...\n\n    def stream(self,prompt:str,\n                 generation_config:Optional[GenerationConfig]=None,\n                 ) -> Generator[str,None,None]: \n        \"\"\"\n        Streams text from a prompt.\n        \"\"\" \n        ...\n    \n    def tokenize(self,text:str) -> List[int]:\n        \"\"\"\n        Tokenizes a string into a list of tokens.\n        \"\"\"\n        ...\n    \n    def decode(self,tokens:List[int]) -> str:\n        \"\"\"\n        Decodes a list of tokens into a string.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def quantize(source:str,destination:str,quantization:QuantizationType=QuantizationType.Q4_0,container:ContainerType=ContainerType.GGJT,callback:Optional[Callable[[str],None]]=None)->None:\n        \"\"\"\n        Quantizes the model.\n        \"\"\"\n        ...", "\n#Theoretically this is incorrect as the 'model' doesnt actually exist, but it is a good enough approximation for now. \nclass Model(ABC):\n    \"\"\"\n    Wrapper around a llm model.\n    \"\"\"\n    config:SessionConfig\n    \n    @property\n    def path(self)->str: ...\n    \n    @property\n    def verbose(self)->bool: ...\n\n    @property\n    def lora_paths(self)->Optional[List[str]]: ...\n\n    def  __init__(self,\n                  path:Union[str,os.PathLike],\n                  session_config:SessionConfig=SessionConfig(),\n                  tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None,\n                  lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n                  verbose:bool=False) -> None: ...\n    \n    def generate(self,prompt:str,\n                 generation_config:Optional[GenerationConfig]=None,\n                 callback:Optional[Callable[[str],Optional[bool]]]=None) -> GenerationResult: \n        \"\"\"\n        Generates text from a prompt.\n        \"\"\" \n        ...\n\n    def generate(self,prompt:str) -> List[float]: \n        \"\"\"\n        Embed a given prompt into vector representation.\n        \"\"\" \n        ...\n\n    def stream(self,prompt:str,\n                 generation_config:Optional[GenerationConfig]=None,\n                 ) -> Generator[str,None,None]: \n        \"\"\"\n        Streams text from a prompt.\n        \"\"\" \n        ...\n    \n    def tokenize(self,text:str) -> List[int]:\n        \"\"\"\n        Tokenizes a string into a list of tokens.\n        \"\"\"\n        ...\n    \n    def decode(self,tokens:List[int]) -> str:\n        \"\"\"\n        Decodes a list of tokens into a string.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def quantize(source:str,destination:str,quantization:QuantizationType=QuantizationType.Q4_0,container:ContainerType=ContainerType.GGJT,callback:Optional[Callable[[str],None]]=None)->None:\n        \"\"\"\n        Quantizes the model.\n        \"\"\"\n        ..."]}
{"filename": "llm_rs/repository.py", "chunked_list": ["from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete\nfrom huggingface_hub import create_repo,metadata_update,cached_assets_path\nfrom typing import List, Optional, Union\nimport json\nimport os\nimport pathlib\nimport logging \n\nclass Repository():\n    def __init__(self,name:str,user_or_organization:Optional[str]=None,token:Optional[str]=None,private:bool=False) -> None:\n        \n        self.name = f\"{user_or_organization}/{name}\" if user_or_organization else name\n        self.api = HfApi(token=token)\n\n        try:\n            self.url = str(create_repo(self.name,token=token,private=private))\n            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n            # Uplaod the config file\n            config_path = cached_assets_path(\"llm-rs\",namespace=self.name) / \"config.json\"\n            with open(config_path,\"w\") as f:\n                f.write(json.dumps({\"repo_type\":\"GGML\"}))\n            self.api.create_commit(\n                self.name,\n                operations=[CommitOperationAdd(path_in_repo=\"config.json\", path_or_fileobj=config_path)],\n                commit_message=\"Auto initialized repo via llm-rs\",)\n            \n            metadata={}\n            metadata[\"pipeline_tag\"]= \"text-generation\"\n            metadata[\"tags\"] = [\"llm-rs\",\"ggml\"]\n            metadata_update(self.name, metadata, token=token)\n            \n        except Exception as e:\n            self.url = create_repo(self.name,token=token,private=private,exist_ok=True)\n            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n\n    def upload(self,model_file:Union[str,os.PathLike],delete_old:bool=True):\n        # search for the metadata file\n        path = pathlib.Path(model_file)\n        metadata_path = path.with_suffix(\".meta\")\n\n        if not path.exists():\n            raise FileNotFoundError(f\"Could not find model file {model_file}\")\n        \n        if delete_old:\n            to_delete = [path.name]\n            if metadata_path.exists():\n                to_delete.append(metadata_path.name)\n\n            for n in to_delete:\n                try:\n                    self.api.create_commit(\n                        self.name,\n                        operations=[CommitOperationDelete(path_in_repo=n,is_folder=False)],\n                        commit_message=f\"Delete old file: '{n}'\",\n                        )\n                except Exception as e:\n                    logging.error(f\"Could not delete old file {n} from repo {self.name}\")\n            \n\n        upload_operations = [CommitOperationAdd(path_in_repo=path.name, path_or_fileobj=path)] \n        if metadata_path.exists():\n            upload_operations.append(CommitOperationAdd(path_in_repo=metadata_path.name, path_or_fileobj=metadata_path))\n        \n        self.api.create_commit(\n            self.name,\n            upload_operations,\n            commit_message=f\"Upload new model file: '{path.name}'\",)", "class Repository():\n    def __init__(self,name:str,user_or_organization:Optional[str]=None,token:Optional[str]=None,private:bool=False) -> None:\n        \n        self.name = f\"{user_or_organization}/{name}\" if user_or_organization else name\n        self.api = HfApi(token=token)\n\n        try:\n            self.url = str(create_repo(self.name,token=token,private=private))\n            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n            # Uplaod the config file\n            config_path = cached_assets_path(\"llm-rs\",namespace=self.name) / \"config.json\"\n            with open(config_path,\"w\") as f:\n                f.write(json.dumps({\"repo_type\":\"GGML\"}))\n            self.api.create_commit(\n                self.name,\n                operations=[CommitOperationAdd(path_in_repo=\"config.json\", path_or_fileobj=config_path)],\n                commit_message=\"Auto initialized repo via llm-rs\",)\n            \n            metadata={}\n            metadata[\"pipeline_tag\"]= \"text-generation\"\n            metadata[\"tags\"] = [\"llm-rs\",\"ggml\"]\n            metadata_update(self.name, metadata, token=token)\n            \n        except Exception as e:\n            self.url = create_repo(self.name,token=token,private=private,exist_ok=True)\n            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n\n    def upload(self,model_file:Union[str,os.PathLike],delete_old:bool=True):\n        # search for the metadata file\n        path = pathlib.Path(model_file)\n        metadata_path = path.with_suffix(\".meta\")\n\n        if not path.exists():\n            raise FileNotFoundError(f\"Could not find model file {model_file}\")\n        \n        if delete_old:\n            to_delete = [path.name]\n            if metadata_path.exists():\n                to_delete.append(metadata_path.name)\n\n            for n in to_delete:\n                try:\n                    self.api.create_commit(\n                        self.name,\n                        operations=[CommitOperationDelete(path_in_repo=n,is_folder=False)],\n                        commit_message=f\"Delete old file: '{n}'\",\n                        )\n                except Exception as e:\n                    logging.error(f\"Could not delete old file {n} from repo {self.name}\")\n            \n\n        upload_operations = [CommitOperationAdd(path_in_repo=path.name, path_or_fileobj=path)] \n        if metadata_path.exists():\n            upload_operations.append(CommitOperationAdd(path_in_repo=metadata_path.name, path_or_fileobj=metadata_path))\n        \n        self.api.create_commit(\n            self.name,\n            upload_operations,\n            commit_message=f\"Upload new model file: '{path.name}'\",)", "\n"]}
{"filename": "llm_rs/langchain/langchain.py", "chunked_list": ["try:\n    from langchain.llms.base import LLM\n    from langchain.embeddings.base import Embeddings\n    from langchain.callbacks.manager import CallbackManagerForLLMRun\nexcept ImportError:\n    raise ImportError(\n        'To use the llm_rs.langchain module, please install llm-rs with the additional \"langchain\" dependencies via: pip install llm-rs[langchain]')\n\nfrom typing import Any, Dict, Optional, Sequence, Union, List\nimport os", "from typing import Any, Dict, Optional, Sequence, Union, List\nimport os\n\nfrom pydantic import root_validator\n\nfrom ..auto import AutoModel, KnownModels\nfrom ..config import GenerationConfig, SessionConfig\nfrom ..base_model import Model\n\nclass RustformersLLM(LLM,Embeddings):\n    \"\"\"\n    Langchain-Wrapper around a Rustformers model.\n    \"\"\"\n\n    model: Optional[Model] = None #: :meta private:\n\n    model_path_or_repo_id: Union[str,os.PathLike]\n    \"\"\"The path to the model file or directory or the name of a Hugging Face Hub\n    model repo.\"\"\"\n\n    model_type: Optional[KnownModels] = None\n    \"\"\"The model type.\"\"\"\n\n    model_file: Optional[str] = None\n    \"\"\"The name of the model file in repo or directory.\"\"\"\n\n    session_config:SessionConfig=SessionConfig()\n    \"\"\"Session config for the model.\"\"\"\n\n    generation_config:GenerationConfig=GenerationConfig()\n    \"\"\"Generation config for the model.\"\"\"\n\n    tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None\n    \"\"\"The path to the tokenizer file or directory or the name of a Hugging Face Hub repo containing the tokenizer.\"\"\"\n\n    use_hf_tokenizer:bool=True\n    \"\"\"Whether to use the Hugging Face tokenizer or the integrated GGML tokenizer.\"\"\"\n\n    lora_paths:Optional[List[Union[str,os.PathLike]]]=None\n    \"\"\"Paths to the lora files.\"\"\"\n\n    verbose:bool=False\n    \"\"\"Whether to print the loading process.\"\"\"\n\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            'model_path_or_repo_id': self.model_path_or_repo_id,\n            'model_type': self.model_type,\n            'model_file': self.model_file,\n            'session_config': self.session_config,\n            'generation_config': self.generation_config,\n            'tokenizer_path_or_repo_id': self.tokenizer_path_or_repo_id,\n            'lora_paths': self.lora_paths,\n            'use_hf_tokenizer': self.use_hf_tokenizer,\n            'verbose': self.verbose\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return 'rustformers'\n\n    @root_validator()\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate and load model from a local file or remote repo.\"\"\"\n        values['model'] = AutoModel.from_pretrained(\n            model_path_or_repo_id= values['model_path_or_repo_id'],\n            model_type=values['model_type'],\n            model_file=values['model_file'],\n            session_config=values['session_config'],\n            tokenizer_path_or_repo_id= values['tokenizer_path_or_repo_id'],\n            lora_paths=values['lora_paths'],\n            use_hf_tokenizer=values['use_hf_tokenizer'],\n            verbose=values['verbose']\n        )\n        return values\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[Sequence[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n    ) -> str:\n        \"\"\"Generate text from a prompt.\n\n        Args:\n            prompt: The prompt to generate text from.\n            stop: A list of sequences to stop generation when encountered.\n\n        Returns:\n            The generated text.\n        \"\"\"\n        text = []\n\n        generation_config = self.generation_config\n        if stop:\n            generation_config.stop_words = list(stop)\n        for chunk in self.model.stream(prompt, generation_config=generation_config):\n            text.append(chunk)\n            if run_manager:\n                run_manager.on_llm_new_token(chunk, verbose=self.verbose)\n        return ''.join(text)\n    \n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed search docs.\"\"\"\n        embeddings = []\n        for text in texts:\n            embeddings.append(self.embed_query(text))\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed query text.\"\"\"\n        return self.model.embed(text)", "\nclass RustformersLLM(LLM,Embeddings):\n    \"\"\"\n    Langchain-Wrapper around a Rustformers model.\n    \"\"\"\n\n    model: Optional[Model] = None #: :meta private:\n\n    model_path_or_repo_id: Union[str,os.PathLike]\n    \"\"\"The path to the model file or directory or the name of a Hugging Face Hub\n    model repo.\"\"\"\n\n    model_type: Optional[KnownModels] = None\n    \"\"\"The model type.\"\"\"\n\n    model_file: Optional[str] = None\n    \"\"\"The name of the model file in repo or directory.\"\"\"\n\n    session_config:SessionConfig=SessionConfig()\n    \"\"\"Session config for the model.\"\"\"\n\n    generation_config:GenerationConfig=GenerationConfig()\n    \"\"\"Generation config for the model.\"\"\"\n\n    tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None\n    \"\"\"The path to the tokenizer file or directory or the name of a Hugging Face Hub repo containing the tokenizer.\"\"\"\n\n    use_hf_tokenizer:bool=True\n    \"\"\"Whether to use the Hugging Face tokenizer or the integrated GGML tokenizer.\"\"\"\n\n    lora_paths:Optional[List[Union[str,os.PathLike]]]=None\n    \"\"\"Paths to the lora files.\"\"\"\n\n    verbose:bool=False\n    \"\"\"Whether to print the loading process.\"\"\"\n\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            'model_path_or_repo_id': self.model_path_or_repo_id,\n            'model_type': self.model_type,\n            'model_file': self.model_file,\n            'session_config': self.session_config,\n            'generation_config': self.generation_config,\n            'tokenizer_path_or_repo_id': self.tokenizer_path_or_repo_id,\n            'lora_paths': self.lora_paths,\n            'use_hf_tokenizer': self.use_hf_tokenizer,\n            'verbose': self.verbose\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return 'rustformers'\n\n    @root_validator()\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate and load model from a local file or remote repo.\"\"\"\n        values['model'] = AutoModel.from_pretrained(\n            model_path_or_repo_id= values['model_path_or_repo_id'],\n            model_type=values['model_type'],\n            model_file=values['model_file'],\n            session_config=values['session_config'],\n            tokenizer_path_or_repo_id= values['tokenizer_path_or_repo_id'],\n            lora_paths=values['lora_paths'],\n            use_hf_tokenizer=values['use_hf_tokenizer'],\n            verbose=values['verbose']\n        )\n        return values\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[Sequence[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n    ) -> str:\n        \"\"\"Generate text from a prompt.\n\n        Args:\n            prompt: The prompt to generate text from.\n            stop: A list of sequences to stop generation when encountered.\n\n        Returns:\n            The generated text.\n        \"\"\"\n        text = []\n\n        generation_config = self.generation_config\n        if stop:\n            generation_config.stop_words = list(stop)\n        for chunk in self.model.stream(prompt, generation_config=generation_config):\n            text.append(chunk)\n            if run_manager:\n                run_manager.on_llm_new_token(chunk, verbose=self.verbose)\n        return ''.join(text)\n    \n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed search docs.\"\"\"\n        embeddings = []\n        for text in texts:\n            embeddings.append(self.embed_query(text))\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed query text.\"\"\"\n        return self.model.embed(text)"]}
{"filename": "llm_rs/langchain/__init__.py", "chunked_list": ["from .langchain import RustformersLLM"]}
{"filename": "llm_rs/convert/auto_converter.py", "chunked_list": ["from transformers import AutoConfig\nimport os\nfrom typing import Union\nfrom .models import MPTConverter,GptNeoXConverter,GptJConverter,Gpt2Converter,BloomConverter,LlamaConverter\nimport logging \nimport pathlib\n\n\n_ARCHITECTURE_CONVERTER_MAP = {\n    \"MPTForCausalLM\": MPTConverter,", "_ARCHITECTURE_CONVERTER_MAP = {\n    \"MPTForCausalLM\": MPTConverter,\n    \"GPTNeoXForCausalLM\": GptNeoXConverter,\n    \"GPTJForCausalLM\": GptJConverter,\n    \"GPT2LMHeadModel\": Gpt2Converter,\n    \"BloomForCausalLM\":BloomConverter,\n    \"LLaMAForCausalLM\":LlamaConverter,\n    \"LlamaForCausalLM\":LlamaConverter #Open-LLaMA uses this as architecture name\n}\n\nclass AutoConverter():\n\n    @staticmethod\n    def convert(pretrained_model_name_or_path:Union[str,os.PathLike],output_path:Union[str,os.PathLike])->str:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n        architecture = config.architectures[0]\n        model_name = get_name_from_config(config)\n\n        adapter=None\n        if architecture not in _ARCHITECTURE_CONVERTER_MAP:\n            raise ValueError(f\"Unsupported architecture '{architecture}' for '{model_name}'\")\n        adapter = _ARCHITECTURE_CONVERTER_MAP[architecture]\n\n        output_file = build_path(output_path,model_name)\n        if os.path.exists(output_file):\n            logging.warning(f\"Skipping {model_name} via {adapter.__name__} because '{output_file}' already exists!\")\n            return output_file\n        \n        logging.info(f\"Converting {model_name} via {adapter.__name__} to '{output_file}'\")\n        adapter(pretrained_model_name_or_path).convert(output_file)\n        return output_file", "}\n\nclass AutoConverter():\n\n    @staticmethod\n    def convert(pretrained_model_name_or_path:Union[str,os.PathLike],output_path:Union[str,os.PathLike])->str:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n        architecture = config.architectures[0]\n        model_name = get_name_from_config(config)\n\n        adapter=None\n        if architecture not in _ARCHITECTURE_CONVERTER_MAP:\n            raise ValueError(f\"Unsupported architecture '{architecture}' for '{model_name}'\")\n        adapter = _ARCHITECTURE_CONVERTER_MAP[architecture]\n\n        output_file = build_path(output_path,model_name)\n        if os.path.exists(output_file):\n            logging.warning(f\"Skipping {model_name} via {adapter.__name__} because '{output_file}' already exists!\")\n            return output_file\n        \n        logging.info(f\"Converting {model_name} via {adapter.__name__} to '{output_file}'\")\n        adapter(pretrained_model_name_or_path).convert(output_file)\n        return output_file", "\n\ndef get_name_from_config(config:AutoConfig)->str:\n    raw_name:str = \"\"\n    if hasattr(config,\"name_or_path\"):\n        raw_name = config.name_or_path\n    elif hasattr(config,\"model_type\"):\n        raw_name = config.model_type\n    else:\n        raw_name = config.architectures[0]\n    return raw_name.split(\"/\")[-1]", "\n\ndef build_path(output_path:Union[str,os.PathLike],model_name:str)->str:\n    output_path = pathlib.Path(output_path)\n    #User specified a file, so just use that\n    if output_path.is_file():\n        return output_path\n\n    #User specified a directory => auto generate a file name\n    output_path.mkdir(parents=True,exist_ok=True)\n    return os.path.join(output_path,f\"{model_name}-f16.bin\")", "    \n    "]}
{"filename": "llm_rs/convert/__init__.py", "chunked_list": ["from .auto_converter import AutoConverter\nfrom .models.mpt import MPTConverter"]}
{"filename": "llm_rs/convert/models/gpt2.py", "chunked_list": ["import numpy as np\nfrom ._base import BaseAdapter\nfrom typing import Union,Tuple,Optional,BinaryIO \nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,GPT2LMHeadModel\nimport os\nimport torch\nimport struct\nimport numpy as np\nfrom ...auto import KnownModels\nimport re", "from ...auto import KnownModels\nimport re\n\n#based on https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-h5-to-ggml.py\nclass Gpt2Converter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Gpt2\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = GPT2LMHeadModel.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_positions\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_embd\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n\n\n    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") \n    \n    def _rename_weights(self, name: str) -> str:\n        new_name=name\n        # rename headers to keep compatibility\n        # cerebras renames, see https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-cerebras-to-ggml.py\n        if name == \"transformer.ln_f.weight\":\n            new_name = \"model/ln_f/g\"\n        elif name == \"transformer.ln_f.bias\":\n            new_name = \"model/ln_f/b\"\n        elif name == \"transformer.wte.weight\":\n            new_name = \"model/wte\"\n        elif name == \"transformer.wpe.weight\":\n            new_name = \"model/wpe\"\n        elif name == \"lm_head.weight\":\n            new_name = \"model/lm_head\"\n        elif re.match(r\"transformer.h\\.\\d+\\.ln_1\\.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/ln_1/g\"\n        elif re.match(r\"transformer.h\\.\\d+\\.ln_1\\.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/ln_1/b\"\n        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_attn\\.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/attn/c_attn/w\"\n        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_attn\\.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/attn/c_attn/b\"\n        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_proj\\.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/attn/c_proj/w\"\n        elif re.match(r\"transformer.h.\\d+.attn.c_proj.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/attn/c_proj/b\"\n        elif re.match(r\"transformer.h.\\d+.ln_2.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/ln_2/g\"\n        elif re.match(r\"transformer.h.\\d+.ln_2.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/ln_2/b\"\n        elif re.match(r\"transformer.h.\\d+.mlp.c_fc.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/mlp/c_fc/w\"\n        elif re.match(r\"transformer.h.\\d+.mlp.c_fc.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/mlp/c_fc/b\"\n        elif re.match(r\"transformer.h.\\d+.mlp.c_proj.weight\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/mlp/c_proj/w\"\n        elif re.match(r\"transformer.h.\\d+.mlp.c_proj.bias\", name):\n            i = re.findall(\"\\d+\", name)[0]\n            new_name = f\"model/h{i}/mlp/c_proj/b\"\n\n        return new_name\n    \n    def _write_vocabulary(self, out_file: BinaryIO):\n        # write the vocabulary size\n        out_file.write(struct.pack(\"i\", self.config.vocab_size))\n        return super()._write_vocabulary(out_file)\n\n    def _transform_weights(self, name: str, weight: torch.Tensor) -> torch.Tensor:\n        # for efficiency - transpose these matrices:\n        if name.endswith(\"/mlp/c_proj/w\") or name.endswith(\"/mlp/c_fc/w\") or name.endswith(\"/attn/c_proj/w\") or name.endswith(\"/attn/c_attn/w\"):\n            #see this issue: https://github.com/pytorch/pytorch/issues/50275\n            return weight.transpose(0,1)\n        return weight\n    \n    def _filter_f16_weights(self, name: str, data: np.ndarray) -> bool:\n        n_dims = len(data.shape)\n        return (name == \"model/wte\" or name == \"model/lm_head\" or name[-2:] == \"/g\" or name[-2:] == \"/w\") and n_dims == 2", "\n    \n\n        "]}
{"filename": "llm_rs/convert/models/gptj.py", "chunked_list": ["import numpy as np\nfrom ._base import BaseAdapter\nfrom typing import Union,Tuple,Optional,BinaryIO \nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,GPTJForCausalLM\nimport os\nimport torch\nimport struct\nimport numpy as np\nfrom ...auto import KnownModels\n", "from ...auto import KnownModels\n\n#based on https://github.com/ggerganov/ggml/blob/master/examples/gpt-j/convert-h5-to-ggml.py\nclass GptJConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.GptJ\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = GPTJForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_positions\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_embd\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"rotary_dim\"]))\n\n    def _write_vocabulary(self, out_file: BinaryIO):\n        # write the vocabulary size\n        out_file.write(struct.pack(\"i\", self.config.vocab_size))\n        return super()._write_vocabulary(out_file)\n    \n\n    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") "]}
{"filename": "llm_rs/convert/models/bloom.py", "chunked_list": ["import numpy as np\nfrom ._base import BaseAdapter\nfrom typing import Union,Tuple,Optional,BinaryIO \nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,BloomForCausalLM\nimport os\nimport torch\nimport struct\nimport numpy as np\nfrom ...auto import KnownModels\n", "from ...auto import KnownModels\n\n#based on https://github.com/NouamaneTazi/bloomz.cpp/blob/main/convert-hf-to-ggml.py\n\nconv_map = {\n    'word_embeddings'       : 'tok_embeddings',\n    \"word_embeddings_layernorm\": 'norm',\n        'input_layernorm'        : 'attention_norm',\n        'self_attention.query_key_value': 'attention.query_key_value',\n        'self_attention.dense':          'attention.wo',", "        'self_attention.query_key_value': 'attention.query_key_value',\n        'self_attention.dense':          'attention.wo',\n        'post_attention_layernorm': 'ffn_norm',\n        'mlp.dense_h_to_4h'           : 'feed_forward.w1',\n        'mlp.dense_4h_to_h'           : 'feed_forward.w2',\n        'ln_f'                        : 'output_norm',\n        'lm_head' : 'output',\n        }\n\nclass BloomConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Bloom\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = BloomForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        hyperparameters[\"multiple_of\"] = 1\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"multiple_of\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n\n    def _rename_weights(self, name: str) -> str:\n        #Arrrrrg i hate this, but upstream is inconsistent with naming\n        nn = name\n        if name != \"lm_head.weight\":\n            nn = nn.split(\".\")[1:]\n        else:\n            nn = nn.split(\".\")\n\n        if nn[0] == \"h\":\n            nn[0] = \"layers\"\n            mapped = conv_map[\".\".join(nn[2:-1])]\n            name = \".\".join(nn[:2] + [mapped] + nn[-1:])\n        else:\n            mapped = conv_map[\".\".join(nn[:-1])]\n            name = \".\".join([mapped] + nn[-1:])\n        return name\n\n    def _transform_weights(self, name: str, weight: torch.Tensor) ->  torch.Tensor:\n        if \"query_key_value\" in name:\n            q, k, v = weight.reshape(self.config.n_head, 3, -1).unbind(1)\n            return torch.cat([q, k, v], dim=0).reshape_as(weight)\n        return weight\n    \n    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") ", "\nclass BloomConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Bloom\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = BloomForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        hyperparameters[\"multiple_of\"] = 1\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"multiple_of\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n\n    def _rename_weights(self, name: str) -> str:\n        #Arrrrrg i hate this, but upstream is inconsistent with naming\n        nn = name\n        if name != \"lm_head.weight\":\n            nn = nn.split(\".\")[1:]\n        else:\n            nn = nn.split(\".\")\n\n        if nn[0] == \"h\":\n            nn[0] = \"layers\"\n            mapped = conv_map[\".\".join(nn[2:-1])]\n            name = \".\".join(nn[:2] + [mapped] + nn[-1:])\n        else:\n            mapped = conv_map[\".\".join(nn[:-1])]\n            name = \".\".join([mapped] + nn[-1:])\n        return name\n\n    def _transform_weights(self, name: str, weight: torch.Tensor) ->  torch.Tensor:\n        if \"query_key_value\" in name:\n            q, k, v = weight.reshape(self.config.n_head, 3, -1).unbind(1)\n            return torch.cat([q, k, v], dim=0).reshape_as(weight)\n        return weight\n    \n    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") "]}
{"filename": "llm_rs/convert/models/llama.py", "chunked_list": ["from ._base import BaseAdapter,GGJT_MAGIC\nfrom typing import Union,Tuple,Optional,BinaryIO,List,Iterable\nfrom transformers import LlamaConfig,LlamaForCausalLM,LlamaTokenizer,AutoConfig,AutoTokenizer,AutoModelForCausalLM\nimport os\nimport torch\nimport struct\nfrom ...auto import KnownModels\nimport re\nimport numpy as np\nimport logging", "import numpy as np\nimport logging\n\n#based on https://github.com/ggerganov/llama.cpp/blob/master/convert.py\nclass LlamaConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Llama\n    file_magic=GGJT_MAGIC\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoConfig,AutoTokenizer,AutoModelForCausalLM]:\n        config = LlamaConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = LlamaForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        \n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name)\n\n        def make_tensors_list() -> List[str]:\n            ret = [\n                'tok_embeddings.weight',\n                'norm.weight',\n                'output.weight',\n            ]\n            for i in range(80):  # maximum number of layer\n                ret += [\n                    f'layers.{i}.attention.wq.weight',\n                    f'layers.{i}.attention.wk.weight',\n                    f'layers.{i}.attention.wv.weight',\n                    f'layers.{i}.attention.wo.weight',\n                    f'layers.{i}.attention_norm.weight',\n                    f'layers.{i}.feed_forward.w1.weight',\n                    f'layers.{i}.feed_forward.w2.weight',\n                    f'layers.{i}.feed_forward.w3.weight',\n                    f'layers.{i}.ffn_norm.weight',\n                ]\n            return ret\n\n        self.TENSORS_LIST = make_tensors_list()\n        self.TENSORS_SET = set(self.TENSORS_LIST)\n\n        self.mapping = {\n            \"model.layers.{i}.self_attn.q_proj.weight\": \"layers.{i}.attention.wq.weight\",\n            \"model.layers.{i}.self_attn.k_proj.weight\": \"layers.{i}.attention.wk.weight\",\n            \"model.layers.{i}.self_attn.v_proj.weight\": \"layers.{i}.attention.wv.weight\",\n            \"model.layers.{i}.self_attn.o_proj.weight\": \"layers.{i}.attention.wo.weight\",\n            \"model.layers.{i}.mlp.gate_proj.weight\": \"layers.{i}.feed_forward.w1.weight\",\n            \"model.layers.{i}.mlp.down_proj.weight\": \"layers.{i}.feed_forward.w2.weight\",\n            \"model.layers.{i}.mlp.up_proj.weight\": \"layers.{i}.feed_forward.w3.weight\",\n            \"model.layers.{i}.input_layernorm.weight\": \"layers.{i}.attention_norm.weight\",\n            \"model.layers.{i}.post_attention_layernorm.weight\": \"layers.{i}.ffn_norm.weight\",\n        }\n\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n        out_file.write(struct.pack(\"i\", 256))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"num_attention_heads\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"num_hidden_layers\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]//hyperparameters[\"num_attention_heads\"])) # rot (obsolete)\n\n    def _rename_weights(self, name: str) -> str:\n        if name == \"model.embed_tokens.weight\":\n            return \"tok_embeddings.weight\"\n        elif name == \"model.norm.weight\":\n            return \"norm.weight\"\n        elif name == \"lm_head.weight\":\n            return \"output.weight\"\n        \n        for old, new in self.mapping.items():\n            pattern = old.replace(\"{i}\", \"(\\d+)\")\n            match = re.fullmatch(pattern, name)\n            if match:\n                return new.replace(\"{i}\", match.group(1))\n            \n        return name\n    \n    def _transform_weights(self, name: str, weight: torch.Tensor) -> torch.Tensor:\n        to_permut = [r\"layers.(\\d+).attention.wq.weight\",r\"layers.(\\d+).attention.wk.weight\"]\n        for pattern in to_permut:\n            match = re.fullmatch(pattern, name)\n            if match:\n                n_heads = self.config.num_attention_heads\n                numpy_weights:np.ndarray = weight.numpy()\n                reshaped = (numpy_weights.reshape(n_heads, 2, numpy_weights.shape[0] // n_heads // 2, *numpy_weights.shape[1:])\n                   .swapaxes(1, 2)\n                   .reshape(numpy_weights.shape))\n                logging.info(f\"Permuting {name} from {weight.shape} to {reshaped.shape}\")\n                return torch.from_numpy(reshaped)\n        return weight\n    \n    def _filter_weights_after_rename(self, name: str, weight: torch.Tensor) -> bool:\n            return name not in self.TENSORS_SET\n    \n\n    def _write_vocabulary(self,out_file:BinaryIO):\n        sentence_piece_tokenizer = self.tokenizer.sp_model\n\n        logging.info(f\"Processing vocabulary with size {self.config.vocab_size}\")\n        def sentencepiece_tokens() -> Iterable[Tuple[bytes, float]]:\n            tokenizer = sentence_piece_tokenizer\n            for i in range(tokenizer.vocab_size()):\n                text: bytes\n                if tokenizer.is_unknown(i):\n                    text = \" \\u2047 \".encode(\"utf-8\")\n                elif tokenizer.is_control(i):\n                    text = b\"\"\n                elif tokenizer.is_byte(i):\n                    piece = tokenizer.id_to_piece(i)\n                    if len(piece) != 6:\n                        raise Exception(f\"Invalid token: {piece}\")\n                    byte_value = int(piece[3:-1], 16)\n                    text = struct.pack(\"B\", byte_value)\n                else:\n                    text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n                score: float = tokenizer.get_score(i)\n                yield text, score\n\n        for text, score in sentencepiece_tokens():\n            out_file.write(struct.pack(\"i\", len(text)))\n            out_file.write(text)\n            out_file.write(struct.pack(\"f\", score))", "\n    \n\n"]}
{"filename": "llm_rs/convert/models/__init__.py", "chunked_list": ["from .bloom import BloomConverter\nfrom .gpt2 import Gpt2Converter\nfrom .gptj import GptJConverter\nfrom .gptneox import GptNeoXConverter\nfrom .llama import LlamaConverter\nfrom .mpt import MPTConverter"]}
{"filename": "llm_rs/convert/models/gptneox.py", "chunked_list": ["import numpy as np\nfrom ._base import BaseAdapter\nfrom typing import Union,Tuple,Optional,BinaryIO \nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\nimport os\nimport torch\nimport struct\nimport numpy as np\nfrom ...auto import KnownModels\n", "from ...auto import KnownModels\n\n#based on https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox\nclass GptNeoXConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.GptNeoX\n\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            )\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"max_position_embeddings\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"num_attention_heads\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"num_hidden_layers\"]))\n        out_file.write(struct.pack(\"i\", int(hyperparameters[\"rotary_pct\"]*(hyperparameters[\"hidden_size\"]//hyperparameters[\"num_attention_heads\"]))))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"use_parallel_residual\"] if \"use_parallel_residual\" in hyperparameters else True))\n\n\n    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n        return name.endswith(\".attention.masked_bias\") or     \\\n       name.endswith(\".attention.bias\") or \\\n       name.endswith(\".attention.rotary_emb.inv_freq\")"]}
{"filename": "llm_rs/convert/models/mpt.py", "chunked_list": ["from ._base import BaseAdapter\nfrom typing import Union,Tuple,Optional,BinaryIO \nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\nimport os\nimport torch\nimport struct\nfrom ...auto import KnownModels\n\n#based on https://github.com/ggerganov/ggml/blob/master/examples/mpt/convert-h5-to-ggml.py\nclass MPTConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Mpt\n    \n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            )\n        \n        #MPT uses the `EleutherAI/gpt-neox-20b` tokenizer => use it if no tokenizer is specified\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else \"EleutherAI/gpt-neox-20b\" \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"d_model\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"max_seq_len\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_heads\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layers\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"alibi_bias_max\"]))\n        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"clip_qkv\"] or 0.0))", "#based on https://github.com/ggerganov/ggml/blob/master/examples/mpt/convert-h5-to-ggml.py\nclass MPTConverter(BaseAdapter):\n    model_type:KnownModels=KnownModels.Mpt\n    \n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path,\n            torch_dtype=torch.float16,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            )\n        \n        #MPT uses the `EleutherAI/gpt-neox-20b` tokenizer => use it if no tokenizer is specified\n        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else \"EleutherAI/gpt-neox-20b\" \n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        return config,tokenizer,model\n    \n    def _write_hyperparameters(self,out_file:BinaryIO):\n        hyperparameters = self.config.to_dict()\n        out_file.write(struct.pack(\"i\", hyperparameters[\"d_model\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"max_seq_len\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_heads\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layers\"]))\n        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"alibi_bias_max\"]))\n        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"clip_qkv\"] or 0.0))"]}
{"filename": "llm_rs/convert/models/_base.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Union,Tuple,Any,Optional,BinaryIO\nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\nimport os\nimport struct\nfrom enum import Enum\nimport logging\nimport numpy as np\nfrom ...config import QuantizationType,ContainerType\nfrom ...auto import ModelMetadata, KnownModels, QuantizationVersions", "from ...config import QuantizationType,ContainerType\nfrom ...auto import ModelMetadata, KnownModels, QuantizationVersions\nimport pathlib\nimport json\nimport torch\n\nGGML_MAGIC = 0x67676D6C\nGGMF_MAGIC = 0x67676D66\nGGJT_MAGIC = 0x67676a74\n\nclass FileTypes(Enum):\n    FP32 = 0\n    FP16 = 1", "GGJT_MAGIC = 0x67676a74\n\nclass FileTypes(Enum):\n    FP32 = 0\n    FP16 = 1\n\nclass BaseAdapter(ABC):\n    model_type:KnownModels=None\n    file_magic:int=GGML_MAGIC\n    version:int=1\n\n    def  __init__(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None) -> None:\n        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n        self.config,self.tokenizer,self.model= self.load(pretrained_model_name_or_path,pretrained_tokenizer_name_or_path)\n\n    @abstractmethod\n    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoConfig,AutoTokenizer,AutoModelForCausalLM]:\n        ...\n\n    @abstractmethod\n    def _write_hyperparameters(self,out_file:BinaryIO):\n        ...\n\n    def _write_vocabulary(self,out_file:BinaryIO):\n        # TODO: temporary hack to not deal with implementing the tokenizer\n        logging.info(f\"Processing vocabulary with size {self.config.vocab_size}\")\n        dot_token = self.tokenizer.encode(\".\")[0]\n        for i in range(self.config.vocab_size):\n            text = self.tokenizer.decode([dot_token, i]).encode(\"utf-8\")\n            # remove the first byte (it's always '.')\n            text = text[1:]\n            out_file.write(struct.pack(\"i\", len(text)))\n            out_file.write(text)\n\n\n    def _filter_weights(self,name:str,weight:torch.Tensor)->bool:\n        \"\"\"Filter weights that should be skipped\"\"\"\n        return False\n    \n    def _filter_weights_after_rename(self,name:str,weight:torch.Tensor)->bool:\n        \"\"\"Filter weights that should be skipped\"\"\"\n        return False\n    \n    def _filter_f16_weights(self,name:str,data:np.ndarray)->bool:\n        \"\"\"Filter weights that should be stored as fp16\"\"\"\n        n_dims = len(data.shape)\n        return name.endswith(\".weight\") and n_dims == 2\n    \n    def _rename_weights(self,name:str)->str:\n        \"\"\"Rename weights that should be renamed\"\"\"\n        return name\n    \n    def _transform_weights(self,name:str,weight:torch.Tensor)->torch.Tensor:\n        \"\"\"Transform weights that should be transformed\"\"\"\n        return weight\n    \n    def _write_weights(self,out_file:BinaryIO):\n        weights = self.model.state_dict()\n        for name, weight in weights.items():\n            \n\n            if self._filter_weights(name,weight):\n                logging.info(f\"Skipping layer '{name}'\")\n                continue\n            \n            name = self._rename_weights(name)\n            weight = self._transform_weights(name,weight)\n            if self._filter_weights_after_rename(name,weight):\n                logging.info(f\"Skipping layer '{name}'\")\n                continue\n\n            data = weight.squeeze().numpy()\n            n_dims = len(data.shape);    \n            type = FileTypes.FP32\n\n            if self._filter_f16_weights(name,data):\n                data = data.astype(np.float16)\n                type = FileTypes.FP16\n            else:\n                data = data.astype(np.float32)\n\n            encoded_name = name.encode(\"utf-8\")\n            out_file.write(struct.pack(\"iii\", n_dims, len(encoded_name), type.value))\n\n            if self.file_magic == GGJT_MAGIC or self.file_magic == GGMF_MAGIC:\n                out_file.write(struct.pack(\"i\" * len(data.shape), *data.shape[::-1]))\n            else:\n                for i in range(n_dims):\n                    out_file.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n            out_file.write(encoded_name)\n\n\n            if self.file_magic == GGJT_MAGIC:\n                # pad to 32 bytes\n                out_file.seek((out_file.tell() + 31) & -32)\n\n            # data\n            data.tofile(out_file)\n            logging.info(f\"Converted layer '{name}' with shape {data.shape}\")\n\n\n    def write_magic(self,out_file:BinaryIO):\n        out_file.write(struct.pack(\"i\", self.file_magic))\n        if self.file_magic == GGJT_MAGIC or self.file_magic == GGMF_MAGIC:\n            out_file.write(struct.pack(\"i\", self.version))\n\n    def write_file_type(self,out_file:BinaryIO,file_type:FileTypes=FileTypes.FP16):\n        out_file.write(struct.pack(\"i\", file_type.value))\n\n\n\n    def convert(self,output_file:Union[str,os.PathLike])->None:\n\n        with open(output_file, \"wb\") as out_file:\n            # write magic\n            self.write_magic(out_file)\n            logging.info(f\"Processing hyperparameters ...\")\n            self._write_hyperparameters(out_file)\n            self.write_file_type(out_file)\n\n            self._write_vocabulary(out_file)\n            self._write_weights(out_file)\n            logging.info(f\"Done converting model to GGML format. Saved to '{output_file}'\")\n\n        #Create the *.meta file needed for automatic loading\n        metadata_file = pathlib.Path(output_file).with_suffix(\".meta\")\n        metadata = ModelMetadata(\n            model = self.model_type,\n            quantization = QuantizationType.F16,\n            container = ContainerType.GGML,\n            quantization_version=QuantizationVersions.Not_Quantized,\n            base_model=str(self.pretrained_model_name_or_path),\n            )\n        metadata.add_hash(output_file)\n        metadata_file.write_text(json.dumps(metadata.serialize(),indent=4))\n        \n        logging.info(f\"Created metadata file at '{output_file}'\")", "\n"]}
{"filename": "llm_rs/haystack/__init__.py", "chunked_list": ["from .haystack import RustformersInvocationLayer"]}
{"filename": "llm_rs/haystack/haystack.py", "chunked_list": ["try:\n    from haystack.nodes import PromptModelInvocationLayer\n    from haystack.nodes.prompt.invocation_layer import DefaultTokenStreamingHandler\nexcept ImportError:\n     raise ImportError(\n        'To use the llm_rs.haystack module, please install llm-rs with the additional \"haystack\" dependencies e.g. via: pip install llm-rs[haystack]')\n\nimport os\nfrom typing import Dict, List, Union, Type, Optional\nfrom ..auto import AutoModel,KnownModels", "from typing import Dict, List, Union, Type, Optional\nfrom ..auto import AutoModel,KnownModels\nfrom ..config import QuantizationType,ContainerType,SessionConfig,GenerationConfig\nfrom ..base_model import Model\nimport logging \n\nlogger = logging.getLogger(__name__)\n\nclass RustformersInvocationLayer(PromptModelInvocationLayer):\n    def __init__(self, model_name_or_path: Union[str,os.PathLike], \n        max_length: int = 512,\n        model_file: Optional[str] = None,\n        model_type: Optional[KnownModels] = None,\n        session_config:SessionConfig=SessionConfig(),\n        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n        verbose:bool=False,\n        use_hf_tokenizer:bool=True,\n        default_quantization:QuantizationType=QuantizationType.Q4_0,\n        default_container:ContainerType=ContainerType.GGJT,\n        **kwargs):\n\n        \"\"\"\n        Creates a new RustformersInvocationLayer instance.\n\n        :param model_name_or_path: The name or path of the underlying model.\n        :param kwargs: See `AutoModel.from_pretrained`.\n        \"\"\"\n        if model_name_or_path is None or len(model_name_or_path) == 0:\n            raise ValueError(\"model_name_or_path cannot be None or empty string\")\n\n        self.model_name_or_path = model_name_or_path\n        self.max_length = max_length \n        self.model:Model = AutoModel.from_pretrained(model_path_or_repo_id = model_name_or_path,\n            model_file=model_file,\n            model_type=model_type,\n            session_config=session_config,\n            tokenizer_path_or_repo_id=tokenizer_path_or_repo_id,\n            lora_paths=lora_paths,\n            verbose=verbose,\n            use_hf_tokenizer=use_hf_tokenizer,\n            default_quantization=default_quantization,\n            default_container=default_container)\n        \n\n    def _ensure_token_limit(self, prompt: Union[str, List[Dict[str, str]]]) -> Union[str, List[Dict[str, str]]]:\n        \"\"\"Ensure that length of the prompt and answer is within the maximum token length of the PromptModel.\n\n        :param prompt: Prompt text to be sent to the generative model.\n        \"\"\"\n        if not isinstance(prompt, str):\n            raise ValueError(f\"Prompt must be of type str but got {type(prompt)}\")\n        \n        context_length = self.model.config.context_length\n        tokenized_prompt = self.model.tokenize(prompt)\n        if len(tokenized_prompt) + self.max_length > context_length:\n            logger.warning(\n            \"The prompt has been truncated from %s tokens to %s tokens so that the prompt length and \"\n            \"answer length (%s tokens) fit within the max token limit (%s tokens). \"\n            \"Shorten the prompt to prevent it from being cut off\",\n            len(tokenized_prompt),\n            max(0, context_length -  self.max_length),\n            self.max_length,\n            context_length,\n            )\n            return self.model.decode(tokenized_prompt[:max(0, context_length -  self.max_length)])\n\n        return prompt\n\n    def invoke(self, *args, **kwargs):\n        \"\"\"\n        It takes a prompt and returns a list of generated text using the underlying model.\n        :return: A list of generated text.\n        \"\"\"\n\n        prompt = kwargs.pop(\"prompt\",None)\n\n        if not prompt:\n            raise ValueError(\"`prompt` cannot be None\")\n    \n        generation_config = kwargs.pop(\"generation_config\", GenerationConfig())\n        generation_config.max_new_tokens = self.max_length\n\n        stream = kwargs.pop(\"stream\", False)\n        stream_handler = kwargs.get(\"stream_handler\", DefaultTokenStreamingHandler())\n\n        generated_text = []\n        for token in self.model.stream(prompt=prompt,generation_config=generation_config):\n            generated_text.append(token)\n            if stream:\n                stream_handler(token)\n\n        return generated_text\n\n    def supports(cls, model_name_or_path: str, **kwargs) -> bool:\n        \"\"\"\n        Checks if the given model is supported by this invocation layer.\n\n        :param model_name_or_path: The name or path of the model.\n        :param kwargs: Additional keyword arguments passed to the underlying model which might be used to determine\n        if the model is supported.\n        :return: True if this invocation layer supports the model, False otherwise.\n        \"\"\"\n        #I guess there is not much to validate here \u00af\\_(\u30c4)_/\u00af\n        return model_name_or_path is not None and len(model_name_or_path) > 0", "class RustformersInvocationLayer(PromptModelInvocationLayer):\n    def __init__(self, model_name_or_path: Union[str,os.PathLike], \n        max_length: int = 512,\n        model_file: Optional[str] = None,\n        model_type: Optional[KnownModels] = None,\n        session_config:SessionConfig=SessionConfig(),\n        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n        verbose:bool=False,\n        use_hf_tokenizer:bool=True,\n        default_quantization:QuantizationType=QuantizationType.Q4_0,\n        default_container:ContainerType=ContainerType.GGJT,\n        **kwargs):\n\n        \"\"\"\n        Creates a new RustformersInvocationLayer instance.\n\n        :param model_name_or_path: The name or path of the underlying model.\n        :param kwargs: See `AutoModel.from_pretrained`.\n        \"\"\"\n        if model_name_or_path is None or len(model_name_or_path) == 0:\n            raise ValueError(\"model_name_or_path cannot be None or empty string\")\n\n        self.model_name_or_path = model_name_or_path\n        self.max_length = max_length \n        self.model:Model = AutoModel.from_pretrained(model_path_or_repo_id = model_name_or_path,\n            model_file=model_file,\n            model_type=model_type,\n            session_config=session_config,\n            tokenizer_path_or_repo_id=tokenizer_path_or_repo_id,\n            lora_paths=lora_paths,\n            verbose=verbose,\n            use_hf_tokenizer=use_hf_tokenizer,\n            default_quantization=default_quantization,\n            default_container=default_container)\n        \n\n    def _ensure_token_limit(self, prompt: Union[str, List[Dict[str, str]]]) -> Union[str, List[Dict[str, str]]]:\n        \"\"\"Ensure that length of the prompt and answer is within the maximum token length of the PromptModel.\n\n        :param prompt: Prompt text to be sent to the generative model.\n        \"\"\"\n        if not isinstance(prompt, str):\n            raise ValueError(f\"Prompt must be of type str but got {type(prompt)}\")\n        \n        context_length = self.model.config.context_length\n        tokenized_prompt = self.model.tokenize(prompt)\n        if len(tokenized_prompt) + self.max_length > context_length:\n            logger.warning(\n            \"The prompt has been truncated from %s tokens to %s tokens so that the prompt length and \"\n            \"answer length (%s tokens) fit within the max token limit (%s tokens). \"\n            \"Shorten the prompt to prevent it from being cut off\",\n            len(tokenized_prompt),\n            max(0, context_length -  self.max_length),\n            self.max_length,\n            context_length,\n            )\n            return self.model.decode(tokenized_prompt[:max(0, context_length -  self.max_length)])\n\n        return prompt\n\n    def invoke(self, *args, **kwargs):\n        \"\"\"\n        It takes a prompt and returns a list of generated text using the underlying model.\n        :return: A list of generated text.\n        \"\"\"\n\n        prompt = kwargs.pop(\"prompt\",None)\n\n        if not prompt:\n            raise ValueError(\"`prompt` cannot be None\")\n    \n        generation_config = kwargs.pop(\"generation_config\", GenerationConfig())\n        generation_config.max_new_tokens = self.max_length\n\n        stream = kwargs.pop(\"stream\", False)\n        stream_handler = kwargs.get(\"stream_handler\", DefaultTokenStreamingHandler())\n\n        generated_text = []\n        for token in self.model.stream(prompt=prompt,generation_config=generation_config):\n            generated_text.append(token)\n            if stream:\n                stream_handler(token)\n\n        return generated_text\n\n    def supports(cls, model_name_or_path: str, **kwargs) -> bool:\n        \"\"\"\n        Checks if the given model is supported by this invocation layer.\n\n        :param model_name_or_path: The name or path of the model.\n        :param kwargs: Additional keyword arguments passed to the underlying model which might be used to determine\n        if the model is supported.\n        :return: True if this invocation layer supports the model, False otherwise.\n        \"\"\"\n        #I guess there is not much to validate here \u00af\\_(\u30c4)_/\u00af\n        return model_name_or_path is not None and len(model_name_or_path) > 0"]}
{"filename": "examples/haystack_example.py", "chunked_list": ["from haystack.nodes import PromptModel\nfrom llm_rs.haystack import RustformersInvocationLayer\nfrom llm_rs import KnownModels,SessionConfig\n\n\n#Enable `use_gpu` to use GPU acceleration\nsession_config = SessionConfig(use_gpu=False)\n    \nmodel = PromptModel(\"TheBloke/Llama-2-7B-Chat-GGML\",\n                    max_length=4096,", "model = PromptModel(\"TheBloke/Llama-2-7B-Chat-GGML\",\n                    max_length=4096,\n                    invocation_layer_class=RustformersInvocationLayer,\n                    model_kwargs={\n                        \"model_file\":\"llama-2-7b-chat.ggmlv3.q4_K_S.bin\",\n                        \"session_config\":session_config,\n                        \"verbose\":True,\n                        })\n\nprompt= \"\"\"", "\nprompt= \"\"\"\nSystem: You are a helpful, respectful and honest assistant.\nUser: Tell me a Story about a Lama riding the crab named Ferris in about 1000 words.\nAssistant:\n\"\"\"\nmodel.invoke(prompt=prompt,stream=True)"]}
{"filename": "examples/langchain_example.py", "chunked_list": ["from llm_rs.langchain import RustformersLLM\nfrom llm_rs import KnownModels, SessionConfig\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ntemplate=\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{instruction}\n### Response:", "{instruction}\n### Response:\nAnswer:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"instruction\"],template=template,)\n\nllm = RustformersLLM(model_path_or_repo_id=\"TheBloke/Nous-Hermes-13B-GGML\",\n                     model_file=\"nous-hermes-13b.ggmlv3.q4_K_S.bin\",\n                     verbose=True,\n                     model_type=KnownModels.Llama,", "                     verbose=True,\n                     model_type=KnownModels.Llama,\n                     session_config=SessionConfig(use_gpu=True),\n                     callbacks=[StreamingStdOutCallbackHandler()]\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nchain.run(\"Write me some Cypher Querry language examples for Neo4j. Try to use the example movie dataset. Give me 5 examples of how to create nodes and relationships and how to query them.\")", "chain.run(\"Write me some Cypher Querry language examples for Neo4j. Try to use the example movie dataset. Give me 5 examples of how to create nodes and relationships and how to query them.\")"]}
