{"filename": "train.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Train diffusion-based generative model using the techniques described in the\npaper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "paper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport os\nimport re\nimport json\nimport click\nimport torch\nimport dnnlib\nfrom torch_utils import distributed as dist\nfrom training import training_loop", "from torch_utils import distributed as dist\nfrom training import training_loop\n\nimport warnings\nwarnings.filterwarnings('ignore', 'Grad strides do not match bucket view strides') # False warning printed by PyTorch 1.12.\n\n#----------------------------------------------------------------------------\n# Parse a comma separated list of numbers or ranges and return a list of ints.\n# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges", "# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges", "\n#----------------------------------------------------------------------------\n\n@click.command()\n\n#Patch options\n@click.option('--real_p',        help='Full size image ratio', metavar='INT',                       type=click.FloatRange(min=0, max=1), default=0.5, show_default=True)\n@click.option('--train_on_latents',      help='Training on latent embeddings', metavar='BOOL',      type=bool, default=False, show_default=True)\n@click.option('--progressive',      help='Training on latent embeddings', metavar='BOOL',           type=bool, default=False, show_default=True)\n", "@click.option('--progressive',      help='Training on latent embeddings', metavar='BOOL',           type=bool, default=False, show_default=True)\n\n# Main options.\n@click.option('--outdir',        help='Where to save the results', metavar='DIR',                   type=str, required=True)\n@click.option('--data',          help='Path to the dataset', metavar='ZIP|DIR',                     type=str, required=True)\n@click.option('--cond',          help='Train class-conditional model', metavar='BOOL',              type=bool, default=False, show_default=True)\n@click.option('--arch',          help='Network architecture', metavar='ddpmpp|ncsnpp|adm',          type=click.Choice(['ddpmpp', 'ncsnpp', 'adm']), default='ddpmpp', show_default=True)\n@click.option('--precond',       help='Preconditioning & loss function', metavar='vp|ve|edm',       type=click.Choice(['vp', 've', 'edm', 'pedm']), default='pedm', show_default=True)\n\n# Hyperparameters.", "\n# Hyperparameters.\n@click.option('--duration',      help='Training duration', metavar='MIMG',                          type=click.FloatRange(min=0, min_open=True), default=200, show_default=True)\n@click.option('--batch',         help='Total batch size', metavar='INT',                            type=click.IntRange(min=1), default=512, show_default=True)\n@click.option('--batch-gpu',     help='Limit batch size per GPU', metavar='INT',                    type=click.IntRange(min=1))\n@click.option('--cbase',         help='Channel multiplier  [default: varies]', metavar='INT',       type=int)\n@click.option('--cres',          help='Channels per resolution  [default: varies]', metavar='LIST', type=parse_int_list)\n@click.option('--lr',            help='Learning rate', metavar='FLOAT',                             type=click.FloatRange(min=0, min_open=True), default=10e-4, show_default=True)\n@click.option('--ema',           help='EMA half-life', metavar='MIMG',                              type=click.FloatRange(min=0), default=0.5, show_default=True)\n@click.option('--dropout',       help='Dropout probability', metavar='FLOAT',                       type=click.FloatRange(min=0, max=1), default=0.13, show_default=True)", "@click.option('--ema',           help='EMA half-life', metavar='MIMG',                              type=click.FloatRange(min=0), default=0.5, show_default=True)\n@click.option('--dropout',       help='Dropout probability', metavar='FLOAT',                       type=click.FloatRange(min=0, max=1), default=0.13, show_default=True)\n@click.option('--augment',       help='Augment probability', metavar='FLOAT',                       type=click.FloatRange(min=0, max=1), default=0.12, show_default=True)\n@click.option('--xflip',         help='Enable dataset x-flips', metavar='BOOL',                     type=bool, default=False, show_default=True)\n@click.option('--implicit_mlp',  help='encoding coordbefore sending to the conv', metavar='BOOL',   type=bool, default=False, show_default=True)\n\n# Performance-related.\n@click.option('--fp16',          help='Enable mixed-precision training', metavar='BOOL',            type=bool, default=False, show_default=True)\n@click.option('--ls',            help='Loss scaling', metavar='FLOAT',                              type=click.FloatRange(min=0, min_open=True), default=1, show_default=True)\n@click.option('--bench',         help='Enable cuDNN benchmarking', metavar='BOOL',                  type=bool, default=True, show_default=True)", "@click.option('--ls',            help='Loss scaling', metavar='FLOAT',                              type=click.FloatRange(min=0, min_open=True), default=1, show_default=True)\n@click.option('--bench',         help='Enable cuDNN benchmarking', metavar='BOOL',                  type=bool, default=True, show_default=True)\n@click.option('--cache',         help='Cache dataset in CPU memory', metavar='BOOL',                type=bool, default=True, show_default=True)\n@click.option('--workers',       help='DataLoader worker processes', metavar='INT',                 type=click.IntRange(min=1), default=1, show_default=True)\n\n# I/O-related.\n@click.option('--desc',          help='String to include in result dir name', metavar='STR',        type=str)\n@click.option('--nosubdir',      help='Do not create a subdirectory for results',                   is_flag=True)\n@click.option('--tick',          help='How often to print progress', metavar='KIMG',                type=click.IntRange(min=1), default=50, show_default=True)\n@click.option('--snap',          help='How often to save snapshots', metavar='TICKS',               type=click.IntRange(min=1), default=50, show_default=True)", "@click.option('--tick',          help='How often to print progress', metavar='KIMG',                type=click.IntRange(min=1), default=50, show_default=True)\n@click.option('--snap',          help='How often to save snapshots', metavar='TICKS',               type=click.IntRange(min=1), default=50, show_default=True)\n@click.option('--dump',          help='How often to dump state', metavar='TICKS',                   type=click.IntRange(min=1), default=500, show_default=True)\n@click.option('--seed',          help='Random seed  [default: random]', metavar='INT',              type=int)\n@click.option('--transfer',      help='Transfer learning from network pickle', metavar='PKL|URL',   type=str)\n@click.option('--resume',        help='Resume from previous training state', metavar='PT',          type=str)\n@click.option('-n', '--dry-run', help='Print training options and exit',                            is_flag=True)\n\ndef main(**kwargs):\n    \"\"\"Train diffusion-based generative model using the techniques described in the\n    paper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\n    Examples:\n\n    \\b\n    # Train DDPM++ model for class-conditional CIFAR-10 using 8 GPUs\n    torchrun --standalone --nproc_per_node=8 train.py --outdir=training-runs \\\\\n        --data=datasets/cifar10-32x32.zip --cond=1 --arch=ddpmpp\n    \"\"\"\n    opts = dnnlib.EasyDict(kwargs)\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    # Initialize config dict.\n    c = dnnlib.EasyDict()\n    c.dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=opts.data, use_labels=opts.cond, xflip=opts.xflip, cache=opts.cache)\n    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, num_workers=opts.workers, prefetch_factor=2)\n    c.network_kwargs = dnnlib.EasyDict()\n    c.loss_kwargs = dnnlib.EasyDict()\n    c.optimizer_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=opts.lr, betas=[0.9,0.999], eps=1e-8)\n    c.real_p = opts.real_p\n    c.train_on_latents = opts.train_on_latents\n    c.progressive = opts.progressive\n\n    # Validate dataset options.\n    try:\n        dataset_obj = dnnlib.util.construct_class_by_name(**c.dataset_kwargs)\n        dataset_name = dataset_obj.name\n        c.dataset_kwargs.resolution = dataset_obj.resolution # be explicit about dataset resolution\n        c.dataset_kwargs.max_size = len(dataset_obj) # be explicit about dataset size\n        if opts.cond and not dataset_obj.has_labels:\n            raise click.ClickException('--cond=True requires labels specified in dataset.json')\n        del dataset_obj # conserve memory\n    except IOError as err:\n        raise click.ClickException(f'--data: {err}')\n\n    # Network architecture.\n    if opts.arch == 'ddpmpp':\n        c.network_kwargs.update(model_type='SongUNet', embedding_type='positional', encoder_type='standard', decoder_type='standard')\n        c.network_kwargs.update(channel_mult_noise=1, resample_filter=[1,1], model_channels=128, channel_mult=[2,2,2])\n    elif opts.arch == 'ncsnpp':\n        c.network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n        c.network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n    else:\n        assert opts.arch == 'adm'\n        c.network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n\n    # Preconditioning & loss function.\n    if opts.precond == 'vp':\n        c.network_kwargs.class_name = 'training.networks.VPPrecond'\n        c.loss_kwargs.class_name = 'training.loss.VPLoss'\n    elif opts.precond == 've':\n        c.network_kwargs.class_name = 'training.networks.VEPrecond'\n        c.loss_kwargs.class_name = 'training.loss.VELoss'\n    elif opts.precond == 'pedm':\n        c.network_kwargs.class_name = 'training.networks.Patch_EDMPrecond'\n        c.loss_kwargs.class_name = 'training.patch_loss.Patch_EDMLoss'\n    else:\n        assert opts.precond == 'edm'\n        c.network_kwargs.class_name = 'training.networks.EDMPrecond'\n        c.loss_kwargs.class_name = 'training.loss.EDMLoss'\n\n    # Network options.\n    if opts.cbase is not None:\n        c.network_kwargs.model_channels = opts.cbase\n    if opts.cres is not None:\n        c.network_kwargs.channel_mult = opts.cres\n    if opts.augment:\n        c.augment_kwargs = dnnlib.EasyDict(class_name='training.augment.AugmentPipe', p=opts.augment)\n        c.augment_kwargs.update(xflip=1e8, yflip=1, scale=1, rotate_frac=1, aniso=1, translate_frac=1)\n        c.network_kwargs.augment_dim = 9\n        # c.augment_kwargs.update(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1)\n        # c.network_kwargs.augment_dim = 6\n    if opts.implicit_mlp:\n        c.network_kwargs.implicit_mlp = True\n    c.network_kwargs.update(dropout=opts.dropout, use_fp16=opts.fp16)\n\n    # Training options.\n    c.total_kimg = max(int(opts.duration * 1000), 1)\n    c.ema_halflife_kimg = int(opts.ema * 1000)\n    c.update(batch_size=opts.batch, batch_gpu=opts.batch_gpu)\n    c.update(loss_scaling=opts.ls, cudnn_benchmark=opts.bench)\n    c.update(kimg_per_tick=opts.tick, snapshot_ticks=opts.snap, state_dump_ticks=opts.dump)\n\n    # Random seed.\n    if opts.seed is not None:\n        c.seed = opts.seed\n    else:\n        seed = torch.randint(1 << 31, size=[], device=torch.device('cuda'))\n        torch.distributed.broadcast(seed, src=0)\n        c.seed = int(seed)\n\n    # Transfer learning and resume.\n    if opts.transfer is not None:\n        if opts.resume is not None:\n            raise click.ClickException('--transfer and --resume cannot be specified at the same time')\n        c.resume_pkl = opts.transfer\n        c.ema_rampup_ratio = None\n    elif opts.resume is not None:\n        match = re.fullmatch(r'training-state-(\\d+).pt', os.path.basename(opts.resume))\n        if not match or not os.path.isfile(opts.resume):\n            raise click.ClickException('--resume must point to training-state-*.pt from a previous training run')\n        c.resume_pkl = os.path.join(os.path.dirname(opts.resume), f'network-snapshot-{match.group(1)}.pkl')\n        c.resume_kimg = int(match.group(1))\n        c.resume_state_dump = opts.resume\n\n    # Description string.\n    cond_str = 'cond' if c.dataset_kwargs.use_labels else 'uncond'\n    dtype_str = 'fp16' if c.network_kwargs.use_fp16 else 'fp32'\n    desc = f'{dataset_name:s}-{cond_str:s}-{opts.arch:s}-{opts.precond:s}-gpus{dist.get_world_size():d}-batch{c.batch_size:d}-{dtype_str:s}'\n    if opts.desc is not None:\n        desc += f'-{opts.desc}'\n\n    # Pick output directory.\n    if dist.get_rank() != 0:\n        c.run_dir = None\n    elif opts.nosubdir:\n        c.run_dir = opts.outdir\n    else:\n        prev_run_dirs = []\n        if os.path.isdir(opts.outdir):\n            prev_run_dirs = [x for x in os.listdir(opts.outdir) if os.path.isdir(os.path.join(opts.outdir, x))]\n        prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n        prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n        cur_run_id = max(prev_run_ids, default=-1) + 1\n        c.run_dir = os.path.join(opts.outdir, f'{cur_run_id:05d}-{desc}')\n        assert not os.path.exists(c.run_dir)\n\n    # Print options.\n    dist.print0()\n    dist.print0('Training options:')\n    dist.print0(json.dumps(c, indent=2))\n    dist.print0()\n    dist.print0(f'Output directory:        {c.run_dir}')\n    dist.print0(f'Dataset path:            {c.dataset_kwargs.path}')\n    dist.print0(f'Class-conditional:       {c.dataset_kwargs.use_labels}')\n    dist.print0(f'Network architecture:    {opts.arch}')\n    dist.print0(f'Preconditioning & loss:  {opts.precond}')\n    dist.print0(f'Number of GPUs:          {dist.get_world_size()}')\n    dist.print0(f'Batch size:              {c.batch_size}')\n    dist.print0(f'Mixed-precision:         {c.network_kwargs.use_fp16}')\n    dist.print0()\n\n    # Dry run?\n    if opts.dry_run:\n        dist.print0('Dry run; exiting.')\n        return\n\n    # Create output directory.\n    dist.print0('Creating output directory...')\n    if dist.get_rank() == 0:\n        os.makedirs(c.run_dir, exist_ok=True)\n        with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n            json.dump(c, f, indent=2)\n        dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    # Train.\n    training_loop.training_loop(**c)", "def main(**kwargs):\n    \"\"\"Train diffusion-based generative model using the techniques described in the\n    paper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\n    Examples:\n\n    \\b\n    # Train DDPM++ model for class-conditional CIFAR-10 using 8 GPUs\n    torchrun --standalone --nproc_per_node=8 train.py --outdir=training-runs \\\\\n        --data=datasets/cifar10-32x32.zip --cond=1 --arch=ddpmpp\n    \"\"\"\n    opts = dnnlib.EasyDict(kwargs)\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    # Initialize config dict.\n    c = dnnlib.EasyDict()\n    c.dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=opts.data, use_labels=opts.cond, xflip=opts.xflip, cache=opts.cache)\n    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, num_workers=opts.workers, prefetch_factor=2)\n    c.network_kwargs = dnnlib.EasyDict()\n    c.loss_kwargs = dnnlib.EasyDict()\n    c.optimizer_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=opts.lr, betas=[0.9,0.999], eps=1e-8)\n    c.real_p = opts.real_p\n    c.train_on_latents = opts.train_on_latents\n    c.progressive = opts.progressive\n\n    # Validate dataset options.\n    try:\n        dataset_obj = dnnlib.util.construct_class_by_name(**c.dataset_kwargs)\n        dataset_name = dataset_obj.name\n        c.dataset_kwargs.resolution = dataset_obj.resolution # be explicit about dataset resolution\n        c.dataset_kwargs.max_size = len(dataset_obj) # be explicit about dataset size\n        if opts.cond and not dataset_obj.has_labels:\n            raise click.ClickException('--cond=True requires labels specified in dataset.json')\n        del dataset_obj # conserve memory\n    except IOError as err:\n        raise click.ClickException(f'--data: {err}')\n\n    # Network architecture.\n    if opts.arch == 'ddpmpp':\n        c.network_kwargs.update(model_type='SongUNet', embedding_type='positional', encoder_type='standard', decoder_type='standard')\n        c.network_kwargs.update(channel_mult_noise=1, resample_filter=[1,1], model_channels=128, channel_mult=[2,2,2])\n    elif opts.arch == 'ncsnpp':\n        c.network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n        c.network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n    else:\n        assert opts.arch == 'adm'\n        c.network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n\n    # Preconditioning & loss function.\n    if opts.precond == 'vp':\n        c.network_kwargs.class_name = 'training.networks.VPPrecond'\n        c.loss_kwargs.class_name = 'training.loss.VPLoss'\n    elif opts.precond == 've':\n        c.network_kwargs.class_name = 'training.networks.VEPrecond'\n        c.loss_kwargs.class_name = 'training.loss.VELoss'\n    elif opts.precond == 'pedm':\n        c.network_kwargs.class_name = 'training.networks.Patch_EDMPrecond'\n        c.loss_kwargs.class_name = 'training.patch_loss.Patch_EDMLoss'\n    else:\n        assert opts.precond == 'edm'\n        c.network_kwargs.class_name = 'training.networks.EDMPrecond'\n        c.loss_kwargs.class_name = 'training.loss.EDMLoss'\n\n    # Network options.\n    if opts.cbase is not None:\n        c.network_kwargs.model_channels = opts.cbase\n    if opts.cres is not None:\n        c.network_kwargs.channel_mult = opts.cres\n    if opts.augment:\n        c.augment_kwargs = dnnlib.EasyDict(class_name='training.augment.AugmentPipe', p=opts.augment)\n        c.augment_kwargs.update(xflip=1e8, yflip=1, scale=1, rotate_frac=1, aniso=1, translate_frac=1)\n        c.network_kwargs.augment_dim = 9\n        # c.augment_kwargs.update(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1)\n        # c.network_kwargs.augment_dim = 6\n    if opts.implicit_mlp:\n        c.network_kwargs.implicit_mlp = True\n    c.network_kwargs.update(dropout=opts.dropout, use_fp16=opts.fp16)\n\n    # Training options.\n    c.total_kimg = max(int(opts.duration * 1000), 1)\n    c.ema_halflife_kimg = int(opts.ema * 1000)\n    c.update(batch_size=opts.batch, batch_gpu=opts.batch_gpu)\n    c.update(loss_scaling=opts.ls, cudnn_benchmark=opts.bench)\n    c.update(kimg_per_tick=opts.tick, snapshot_ticks=opts.snap, state_dump_ticks=opts.dump)\n\n    # Random seed.\n    if opts.seed is not None:\n        c.seed = opts.seed\n    else:\n        seed = torch.randint(1 << 31, size=[], device=torch.device('cuda'))\n        torch.distributed.broadcast(seed, src=0)\n        c.seed = int(seed)\n\n    # Transfer learning and resume.\n    if opts.transfer is not None:\n        if opts.resume is not None:\n            raise click.ClickException('--transfer and --resume cannot be specified at the same time')\n        c.resume_pkl = opts.transfer\n        c.ema_rampup_ratio = None\n    elif opts.resume is not None:\n        match = re.fullmatch(r'training-state-(\\d+).pt', os.path.basename(opts.resume))\n        if not match or not os.path.isfile(opts.resume):\n            raise click.ClickException('--resume must point to training-state-*.pt from a previous training run')\n        c.resume_pkl = os.path.join(os.path.dirname(opts.resume), f'network-snapshot-{match.group(1)}.pkl')\n        c.resume_kimg = int(match.group(1))\n        c.resume_state_dump = opts.resume\n\n    # Description string.\n    cond_str = 'cond' if c.dataset_kwargs.use_labels else 'uncond'\n    dtype_str = 'fp16' if c.network_kwargs.use_fp16 else 'fp32'\n    desc = f'{dataset_name:s}-{cond_str:s}-{opts.arch:s}-{opts.precond:s}-gpus{dist.get_world_size():d}-batch{c.batch_size:d}-{dtype_str:s}'\n    if opts.desc is not None:\n        desc += f'-{opts.desc}'\n\n    # Pick output directory.\n    if dist.get_rank() != 0:\n        c.run_dir = None\n    elif opts.nosubdir:\n        c.run_dir = opts.outdir\n    else:\n        prev_run_dirs = []\n        if os.path.isdir(opts.outdir):\n            prev_run_dirs = [x for x in os.listdir(opts.outdir) if os.path.isdir(os.path.join(opts.outdir, x))]\n        prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n        prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n        cur_run_id = max(prev_run_ids, default=-1) + 1\n        c.run_dir = os.path.join(opts.outdir, f'{cur_run_id:05d}-{desc}')\n        assert not os.path.exists(c.run_dir)\n\n    # Print options.\n    dist.print0()\n    dist.print0('Training options:')\n    dist.print0(json.dumps(c, indent=2))\n    dist.print0()\n    dist.print0(f'Output directory:        {c.run_dir}')\n    dist.print0(f'Dataset path:            {c.dataset_kwargs.path}')\n    dist.print0(f'Class-conditional:       {c.dataset_kwargs.use_labels}')\n    dist.print0(f'Network architecture:    {opts.arch}')\n    dist.print0(f'Preconditioning & loss:  {opts.precond}')\n    dist.print0(f'Number of GPUs:          {dist.get_world_size()}')\n    dist.print0(f'Batch size:              {c.batch_size}')\n    dist.print0(f'Mixed-precision:         {c.network_kwargs.use_fp16}')\n    dist.print0()\n\n    # Dry run?\n    if opts.dry_run:\n        dist.print0('Dry run; exiting.')\n        return\n\n    # Create output directory.\n    dist.print0('Creating output directory...')\n    if dist.get_rank() == 0:\n        os.makedirs(c.run_dir, exist_ok=True)\n        with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n            json.dump(c, f, indent=2)\n        dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    # Train.\n    training_loop.training_loop(**c)", "\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "fid.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Script for calculating Frechet Inception Distance (FID).\"\"\"\n\nimport os", "\nimport os\nimport click\nimport tqdm\nimport pickle\nimport numpy as np\nimport scipy.linalg\nimport torch\nimport dnnlib\nfrom torch_utils import distributed as dist", "import dnnlib\nfrom torch_utils import distributed as dist\nfrom training import dataset\n\n#----------------------------------------------------------------------------\n\ndef calculate_inception_stats(\n    image_path, num_expected=None, seed=0, max_batch_size=64,\n    num_workers=3, prefetch_factor=2, device=torch.device('cuda'),\n):\n    # Rank 0 goes first.\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()\n\n    # Load Inception-v3 model.\n    # This is a direct PyTorch translation of http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n    dist.print0('Loading Inception-v3 model...')\n    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n    detector_kwargs = dict(return_features=True)\n    feature_dim = 2048\n    with dnnlib.util.open_url(detector_url, verbose=(dist.get_rank() == 0)) as f:\n        detector_net = pickle.load(f).to(device)\n\n    # List images.\n    dist.print0(f'Loading images from \"{image_path}\"...')\n    dataset_obj = dataset.ImageFolderDataset(path=image_path, max_size=num_expected, random_seed=seed)\n    if num_expected is not None and len(dataset_obj) < num_expected:\n        raise click.ClickException(f'Found {len(dataset_obj)} images, but expected at least {num_expected}')\n    if len(dataset_obj) < 2:\n        raise click.ClickException(f'Found {len(dataset_obj)} images, but need at least 2 to compute statistics')\n\n    # Other ranks follow.\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n\n    # Divide images into batches.\n    num_batches = ((len(dataset_obj) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.arange(len(dataset_obj)).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n    data_loader = torch.utils.data.DataLoader(dataset_obj, batch_sampler=rank_batches, num_workers=num_workers, prefetch_factor=prefetch_factor)\n\n    # Accumulate statistics.\n    dist.print0(f'Calculating statistics for {len(dataset_obj)} images...')\n    mu = torch.zeros([feature_dim], dtype=torch.float64, device=device)\n    sigma = torch.zeros([feature_dim, feature_dim], dtype=torch.float64, device=device)\n    for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        if images.shape[0] == 0:\n            continue\n        if images.shape[1] == 1:\n            images = images.repeat([1, 3, 1, 1])\n        features = detector_net(images.to(device), **detector_kwargs).to(torch.float64)\n        mu += features.sum(0)\n        sigma += features.T @ features\n\n    # Calculate grand totals.\n    torch.distributed.all_reduce(mu)\n    torch.distributed.all_reduce(sigma)\n    mu /= len(dataset_obj)\n    sigma -= mu.ger(mu) * len(dataset_obj)\n    sigma /= len(dataset_obj) - 1\n    return mu.cpu().numpy(), sigma.cpu().numpy()", "\n#----------------------------------------------------------------------------\n\ndef calculate_fid_from_inception_stats(mu, sigma, mu_ref, sigma_ref):\n    m = np.square(mu - mu_ref).sum()\n    s, _ = scipy.linalg.sqrtm(np.dot(sigma, sigma_ref), disp=False)\n    fid = m + np.trace(sigma + sigma_ref - s * 2)\n    return float(np.real(fid))\n\n#----------------------------------------------------------------------------", "\n#----------------------------------------------------------------------------\n\n@click.group()\ndef main():\n    \"\"\"Calculate Frechet Inception Distance (FID).\n\n    Examples:\n\n    \\b\n    # Generate 50000 images and save them as fid-tmp/*/*.png\n    torchrun --standalone --nproc_per_node=1 generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\n    \\b\n    # Calculate FID\n    torchrun --standalone --nproc_per_node=1 fid.py calc --images=fid-tmp \\\\\n        --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\n\n    \\b\n    # Compute dataset reference statistics\n    python fid.py ref --data=datasets/my-dataset.zip --dest=fid-refs/my-dataset.npz\n    \"\"\"", "\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.option('--images', 'image_path', help='Path to the images', metavar='PATH|ZIP',              type=str, required=True)\n@click.option('--ref', 'ref_path',      help='Dataset reference statistics ', metavar='NPZ|URL',    type=str, required=True)\n@click.option('--num', 'num_expected',  help='Number of images to use', metavar='INT',              type=click.IntRange(min=2), default=50000, show_default=True)\n@click.option('--seed',                 help='Random seed for selecting the images', metavar='INT', type=int, default=0, show_default=True)\n@click.option('--batch',                help='Maximum batch size', metavar='INT',                   type=click.IntRange(min=1), default=64, show_default=True)\n\ndef calc(image_path, ref_path, num_expected, seed, batch):\n    \"\"\"Calculate FID for a given set of images.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    dist.print0(f'Loading dataset reference statistics from \"{ref_path}\"...')\n    ref = None\n    if dist.get_rank() == 0:\n        with dnnlib.util.open_url(ref_path) as f:\n            ref = dict(np.load(f))\n\n    mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n    dist.print0('Calculating FID...')\n    if dist.get_rank() == 0:\n        fid = calculate_fid_from_inception_stats(mu, sigma, ref['mu'], ref['sigma'])\n        print(f'{fid:g}')\n    torch.distributed.barrier()", "@click.option('--batch',                help='Maximum batch size', metavar='INT',                   type=click.IntRange(min=1), default=64, show_default=True)\n\ndef calc(image_path, ref_path, num_expected, seed, batch):\n    \"\"\"Calculate FID for a given set of images.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    dist.print0(f'Loading dataset reference statistics from \"{ref_path}\"...')\n    ref = None\n    if dist.get_rank() == 0:\n        with dnnlib.util.open_url(ref_path) as f:\n            ref = dict(np.load(f))\n\n    mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n    dist.print0('Calculating FID...')\n    if dist.get_rank() == 0:\n        fid = calculate_fid_from_inception_stats(mu, sigma, ref['mu'], ref['sigma'])\n        print(f'{fid:g}')\n    torch.distributed.barrier()", "\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.option('--data', 'dataset_path', help='Path to the dataset', metavar='PATH|ZIP', type=str, required=True)\n@click.option('--dest', 'dest_path',    help='Destination .npz file', metavar='NPZ',    type=str, required=True)\n@click.option('--batch',                help='Maximum batch size', metavar='INT',       type=click.IntRange(min=1), default=64, show_default=True)\n\ndef ref(dataset_path, dest_path, batch):\n    \"\"\"Calculate dataset reference statistics needed by 'calc'.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    mu, sigma = calculate_inception_stats(image_path=dataset_path, max_batch_size=batch)\n    dist.print0(f'Saving dataset reference statistics to \"{dest_path}\"...')\n    if dist.get_rank() == 0:\n        if os.path.dirname(dest_path):\n            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n        np.savez(dest_path, mu=mu, sigma=sigma)\n\n    torch.distributed.barrier()\n    dist.print0('Done.')", "def ref(dataset_path, dest_path, batch):\n    \"\"\"Calculate dataset reference statistics needed by 'calc'.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    dist.init()\n\n    mu, sigma = calculate_inception_stats(image_path=dataset_path, max_batch_size=batch)\n    dist.print0(f'Saving dataset reference statistics to \"{dest_path}\"...')\n    if dist.get_rank() == 0:\n        if os.path.dirname(dest_path):\n            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n        np.savez(dest_path, mu=mu, sigma=sigma)\n\n    torch.distributed.barrier()\n    dist.print0('Done.')", "\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "example.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Minimal standalone example to reproduce the main results from the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport tqdm\nimport pickle\nimport numpy as np\nimport torch\nimport PIL.Image\nimport dnnlib\n\n#----------------------------------------------------------------------------", "\n#----------------------------------------------------------------------------\n\ndef generate_image_grid(\n    network_pkl, dest_path,\n    seed=0, gridw=8, gridh=8, device=torch.device('cuda'),\n    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n):\n    batch_size = gridw * gridh\n    torch.manual_seed(seed)\n\n    # Load network.\n    print(f'Loading network from \"{network_pkl}\"...')\n    with dnnlib.util.open_url(network_pkl) as f:\n        net = pickle.load(f)['ema'].to(device)\n\n    # Pick latents and labels.\n    print(f'Generating {batch_size} images...')\n    latents = torch.randn([batch_size, net.img_channels, net.img_resolution, net.img_resolution], device=device)\n    class_labels = None\n    if net.label_dim:\n        class_labels = torch.eye(net.label_dim, device=device)[torch.randint(net.label_dim, size=[batch_size], device=device)]\n\n    # Adjust noise levels based on what's supported by the network.\n    sigma_min = max(sigma_min, net.sigma_min)\n    sigma_max = min(sigma_max, net.sigma_max)\n\n    # Time step discretization.\n    step_indices = torch.arange(num_steps, dtype=torch.float64, device=device)\n    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n\n    # Main sampling loop.\n    x_next = latents.to(torch.float64) * t_steps[0]\n    for i, (t_cur, t_next) in tqdm.tqdm(list(enumerate(zip(t_steps[:-1], t_steps[1:]))), unit='step'): # 0, ..., N-1\n        x_cur = x_next\n\n        # Increase noise temporarily.\n        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * torch.randn_like(x_cur)\n\n        # Euler step.\n        denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n        d_cur = (x_hat - denoised) / t_hat\n        x_next = x_hat + (t_next - t_hat) * d_cur\n\n        # Apply 2nd order correction.\n        if i < num_steps - 1:\n            denoised = net(x_next, t_next, class_labels).to(torch.float64)\n            d_prime = (x_next - denoised) / t_next\n            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n\n    # Save image grid.\n    print(f'Saving image grid to \"{dest_path}\"...')\n    image = (x_next * 127.5 + 128).clip(0, 255).to(torch.uint8)\n    image = image.reshape(gridh, gridw, *image.shape[1:]).permute(0, 3, 1, 4, 2)\n    image = image.reshape(gridh * net.img_resolution, gridw * net.img_resolution, net.img_channels)\n    image = image.cpu().numpy()\n    PIL.Image.fromarray(image, 'RGB').save(dest_path)\n    print('Done.')", "\n#----------------------------------------------------------------------------\n\ndef main():\n    model_root = 'https://nvlabs-fi-cdn.nvidia.com/edm/pretrained'\n    generate_image_grid(f'{model_root}/edm-cifar10-32x32-cond-vp.pkl',   'cifar10-32x32.png',  num_steps=18) # FID = 1.79, NFE = 35\n    generate_image_grid(f'{model_root}/edm-ffhq-64x64-uncond-vp.pkl',    'ffhq-64x64.png',     num_steps=40) # FID = 1.97, NFE = 79\n    generate_image_grid(f'{model_root}/edm-afhqv2-64x64-uncond-vp.pkl',  'afhqv2-64x64.png',   num_steps=40) # FID = 1.96, NFE = 79\n    generate_image_grid(f'{model_root}/edm-imagenet-64x64-cond-adm.pkl', 'imagenet-64x64.png', num_steps=256, S_churn=40, S_min=0.05, S_max=50, S_noise=1.003) # FID = 1.36, NFE = 511\n", "\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "dataset_tool.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Tool for creating ZIP/PNG based datasets.\"\"\"\n\nimport functools", "\nimport functools\nimport gzip\nimport io\nimport json\nimport os\nimport pickle\nimport re\nimport sys\nimport tarfile", "import sys\nimport tarfile\nimport zipfile\nfrom pathlib import Path\nfrom typing import Callable, Optional, Tuple, Union\nimport click\nimport numpy as np\nimport PIL.Image\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\n#----------------------------------------------------------------------------\n# Parse a 'M,N' or 'MxN' integer tuple.\n# Example: '4x2' returns (4,2)\n\ndef parse_tuple(s: str) -> Tuple[int, int]:\n    m = re.match(r'^(\\d+)[x,](\\d+)$', s)\n    if m:\n        return int(m.group(1)), int(m.group(2))\n    raise click.ClickException(f'cannot parse tuple {s}')", "\n#----------------------------------------------------------------------------\n\ndef maybe_min(a: int, b: Optional[int]) -> int:\n    if b is not None:\n        return min(a, b)\n    return a\n\n#----------------------------------------------------------------------------\n\ndef file_ext(name: Union[str, Path]) -> str:\n    return str(name).split('.')[-1]", "#----------------------------------------------------------------------------\n\ndef file_ext(name: Union[str, Path]) -> str:\n    return str(name).split('.')[-1]\n\n#----------------------------------------------------------------------------\n\ndef is_image_ext(fname: Union[str, Path]) -> bool:\n    ext = file_ext(fname).lower()\n    return f'.{ext}' in PIL.Image.EXTENSION", "\n#----------------------------------------------------------------------------\n\ndef open_image_folder(source_dir, *, max_images: Optional[int]):\n    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]\n    arch_fnames = {fname: os.path.relpath(fname, source_dir).replace('\\\\', '/') for fname in input_images}\n    max_idx = maybe_min(len(input_images), max_images)\n\n    # Load labels.\n    labels = dict()\n    meta_fname = os.path.join(source_dir, 'dataset.json')\n    if os.path.isfile(meta_fname):\n        with open(meta_fname, 'r') as file:\n            data = json.load(file)['labels']\n            if data is not None:\n                labels = {x[0]: x[1] for x in data}\n\n    # No labels available => determine from top-level directory names.\n    if len(labels) == 0:\n        toplevel_names = {arch_fname: arch_fname.split('/')[0] if '/' in arch_fname else '' for arch_fname in arch_fnames.values()}\n        toplevel_indices = {toplevel_name: idx for idx, toplevel_name in enumerate(sorted(set(toplevel_names.values())))}\n        if len(toplevel_indices) > 1:\n            labels = {arch_fname: toplevel_indices[toplevel_name] for arch_fname, toplevel_name in toplevel_names.items()}\n\n    def iterate_images():\n        for idx, fname in enumerate(input_images):\n            img = np.array(PIL.Image.open(fname))\n            yield dict(img=img, label=labels.get(arch_fnames.get(fname)))\n            if idx >= max_idx - 1:\n                break\n    return max_idx, iterate_images()", "\n#----------------------------------------------------------------------------\n\ndef open_image_zip(source, *, max_images: Optional[int]):\n    with zipfile.ZipFile(source, mode='r') as z:\n        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]\n        max_idx = maybe_min(len(input_images), max_images)\n\n        # Load labels.\n        labels = dict()\n        if 'dataset.json' in z.namelist():\n            with z.open('dataset.json', 'r') as file:\n                data = json.load(file)['labels']\n                if data is not None:\n                    labels = {x[0]: x[1] for x in data}\n\n    def iterate_images():\n        with zipfile.ZipFile(source, mode='r') as z:\n            for idx, fname in enumerate(input_images):\n                with z.open(fname, 'r') as file:\n                    img = np.array(PIL.Image.open(file))\n                yield dict(img=img, label=labels.get(fname))\n                if idx >= max_idx - 1:\n                    break\n    return max_idx, iterate_images()", "\n#----------------------------------------------------------------------------\n\ndef open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):\n    import cv2  # pyright: ignore [reportMissingImports] # pip install opencv-python\n    import lmdb  # pyright: ignore [reportMissingImports] # pip install lmdb\n\n    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n        max_idx = maybe_min(txn.stat()['entries'], max_images)\n\n    def iterate_images():\n        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n            for idx, (_key, value) in enumerate(txn.cursor()):\n                try:\n                    try:\n                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)\n                        if img is None:\n                            raise IOError('cv2.imdecode failed')\n                        img = img[:, :, ::-1] # BGR => RGB\n                    except IOError:\n                        img = np.array(PIL.Image.open(io.BytesIO(value)))\n                    yield dict(img=img, label=None)\n                    if idx >= max_idx - 1:\n                        break\n                except:\n                    print(sys.exc_info()[1])\n\n    return max_idx, iterate_images()", "\n#----------------------------------------------------------------------------\n\ndef open_cifar10(tarball: str, *, max_images: Optional[int]):\n    images = []\n    labels = []\n\n    with tarfile.open(tarball, 'r:gz') as tar:\n        for batch in range(1, 6):\n            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')\n            with tar.extractfile(member) as file:\n                data = pickle.load(file, encoding='latin1')\n            images.append(data['data'].reshape(-1, 3, 32, 32))\n            labels.append(data['labels'])\n\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC\n    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx - 1:\n                break\n\n    return max_idx, iterate_images()", "\n#----------------------------------------------------------------------------\n\ndef open_mnist(images_gz: str, *, max_images: Optional[int]):\n    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')\n    assert labels_gz != images_gz\n    images = []\n    labels = []\n\n    with gzip.open(images_gz, 'rb') as f:\n        images = np.frombuffer(f.read(), np.uint8, offset=16)\n    with gzip.open(labels_gz, 'rb') as f:\n        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n\n    images = images.reshape(-1, 28, 28)\n    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)\n    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (60000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx - 1:\n                break\n\n    return max_idx, iterate_images()", "\n#----------------------------------------------------------------------------\n\ndef make_transform(\n    transform: Optional[str],\n    output_width: Optional[int],\n    output_height: Optional[int]\n) -> Callable[[np.ndarray], Optional[np.ndarray]]:\n    def scale(width, height, img):\n        w = img.shape[1]\n        h = img.shape[0]\n        if width == w and height == h:\n            return img\n        img = PIL.Image.fromarray(img)\n        ww = width if width is not None else w\n        hh = height if height is not None else h\n        img = img.resize((ww, hh), PIL.Image.Resampling.LANCZOS)\n        return np.array(img)\n\n    def center_crop(width, height, img):\n        crop = np.min(img.shape[:2])\n        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]\n        if img.ndim == 2:\n            img = img[:, :, np.newaxis].repeat(3, axis=2)\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), PIL.Image.Resampling.LANCZOS)\n        return np.array(img)\n\n    def center_crop_wide(width, height, img):\n        ch = int(np.round(width * img.shape[0] / img.shape[1]))\n        if img.shape[1] < width or ch < height:\n            return None\n\n        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]\n        if img.ndim == 2:\n            img = img[:, :, np.newaxis].repeat(3, axis=2)\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), PIL.Image.Resampling.LANCZOS)\n        img = np.array(img)\n\n        canvas = np.zeros([width, width, 3], dtype=np.uint8)\n        canvas[(width - height) // 2 : (width + height) // 2, :] = img\n        return canvas\n\n    if transform is None:\n        return functools.partial(scale, output_width, output_height)\n    if transform == 'center-crop':\n        if output_width is None or output_height is None:\n            raise click.ClickException('must specify --resolution=WxH when using ' + transform + 'transform')\n        return functools.partial(center_crop, output_width, output_height)\n    if transform == 'center-crop-wide':\n        if output_width is None or output_height is None:\n            raise click.ClickException('must specify --resolution=WxH when using ' + transform + ' transform')\n        return functools.partial(center_crop_wide, output_width, output_height)\n    assert False, 'unknown transform'", "\n#----------------------------------------------------------------------------\n\ndef open_dataset(source, *, max_images: Optional[int]):\n    if os.path.isdir(source):\n        if source.rstrip('/').endswith('_lmdb'):\n            return open_lmdb(source, max_images=max_images)\n        else:\n            return open_image_folder(source, max_images=max_images)\n    elif os.path.isfile(source):\n        if os.path.basename(source) == 'cifar-10-python.tar.gz':\n            return open_cifar10(source, max_images=max_images)\n        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':\n            return open_mnist(source, max_images=max_images)\n        elif file_ext(source) == 'zip':\n            return open_image_zip(source, max_images=max_images)\n        else:\n            assert False, 'unknown archive type'\n    else:\n        raise click.ClickException(f'Missing input file or directory: {source}')", "\n#----------------------------------------------------------------------------\n\ndef open_dest(dest: str) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:\n    dest_ext = file_ext(dest)\n\n    if dest_ext == 'zip':\n        if os.path.dirname(dest) != '':\n            os.makedirs(os.path.dirname(dest), exist_ok=True)\n        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)\n        def zip_write_bytes(fname: str, data: Union[bytes, str]):\n            zf.writestr(fname, data)\n        return '', zip_write_bytes, zf.close\n    else:\n        # If the output folder already exists, check that is is\n        # empty.\n        #\n        # Note: creating the output directory is not strictly\n        # necessary as folder_write_bytes() also mkdirs, but it's better\n        # to give an error message earlier in case the dest folder\n        # somehow cannot be created.\n        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:\n            raise click.ClickException('--dest folder must be empty')\n        os.makedirs(dest, exist_ok=True)\n\n        def folder_write_bytes(fname: str, data: Union[bytes, str]):\n            os.makedirs(os.path.dirname(fname), exist_ok=True)\n            with open(fname, 'wb') as fout:\n                if isinstance(data, str):\n                    data = data.encode('utf8')\n                fout.write(data)\n        return dest, folder_write_bytes, lambda: None", "\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--source',     help='Input directory or archive name', metavar='PATH',   type=str, required=True)\n@click.option('--dest',       help='Output directory or archive name', metavar='PATH',  type=str, required=True)\n@click.option('--max-images', help='Maximum number of images to output', metavar='INT', type=int)\n@click.option('--transform',  help='Input crop/resize mode', metavar='MODE',            type=click.Choice(['center-crop', 'center-crop-wide']))\n@click.option('--resolution', help='Output resolution (e.g., 512x512)', metavar='WxH',  type=parse_tuple)\n\ndef main(\n    source: str,\n    dest: str,\n    max_images: Optional[int],\n    transform: Optional[str],\n    resolution: Optional[Tuple[int, int]]\n):\n    \"\"\"Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.\n\n    The input dataset format is guessed from the --source argument:\n\n    \\b\n    --source *_lmdb/                    Load LSUN dataset\n    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n    --source train-images-idx3-ubyte.gz Load MNIST dataset\n    --source path/                      Recursively load all images from path/\n    --source dataset.zip                Recursively load all images from dataset.zip\n\n    Specifying the output format and path:\n\n    \\b\n    --dest /path/to/dir                 Save output files under /path/to/dir\n    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n\n    The output dataset format can be either an image folder or an uncompressed zip archive.\n    Zip archives makes it easier to move datasets around file servers and clusters, and may\n    offer better training performance on network file systems.\n\n    Images within the dataset archive will be stored as uncompressed PNG.\n    Uncompresed PNGs can be efficiently decoded in the training loop.\n\n    Class labels are stored in a file called 'dataset.json' that is stored at the\n    dataset root folder.  This file has the following structure:\n\n    \\b\n    {\n        \"labels\": [\n            [\"00000/img00000000.png\",6],\n            [\"00000/img00000001.png\",9],\n            ... repeated for every image in the datase\n            [\"00049/img00049999.png\",1]\n        ]\n    }\n\n    If the 'dataset.json' file cannot be found, class labels are determined from\n    top-level directory names.\n\n    Image scale/crop and resolution requirements:\n\n    Output images must be square-shaped and they must all have the same power-of-two\n    dimensions.\n\n    To scale arbitrary input image size to a specific width and height, use the\n    --resolution option.  Output resolution will be either the original\n    input resolution (if resolution was not specified) or the one specified with\n    --resolution option.\n\n    Use the --transform=center-crop or --transform=center-crop-wide options to apply a\n    center crop transform on the input image.  These options should be used with the\n    --resolution option.  For example:\n\n    \\b\n    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\\\n        --transform=center-crop-wide --resolution=512x384\n    \"\"\"\n\n    PIL.Image.init()\n\n    if dest == '':\n        raise click.ClickException('--dest output filename or directory must not be an empty string')\n\n    num_files, input_iter = open_dataset(source, max_images=max_images)\n    archive_root_dir, save_bytes, close_dest = open_dest(dest)\n\n    if resolution is None: resolution = (None, None)\n    transform_image = make_transform(transform, *resolution)\n\n    dataset_attrs = None\n\n    labels = []\n    for idx, image in tqdm(enumerate(input_iter), total=num_files):\n        idx_str = f'{idx:08d}'\n        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'\n\n        # Apply crop and resize.\n        img = transform_image(image['img'])\n        if img is None:\n            continue\n\n        # Error check to require uniform image attributes across\n        # the whole dataset.\n        channels = img.shape[2] if img.ndim == 3 else 1\n        cur_image_attrs = {'width': img.shape[1], 'height': img.shape[0], 'channels': channels}\n        if dataset_attrs is None:\n            dataset_attrs = cur_image_attrs\n            width = dataset_attrs['width']\n            height = dataset_attrs['height']\n            if width != height:\n                raise click.ClickException(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')\n            if dataset_attrs['channels'] not in [1, 3]:\n                raise click.ClickException('Input images must be stored as RGB or grayscale')\n            if width != 2 ** int(np.floor(np.log2(width))):\n                raise click.ClickException('Image width/height after scale and crop are required to be power-of-two')\n        elif dataset_attrs != cur_image_attrs:\n            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]\n            raise click.ClickException(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\\n' + '\\n'.join(err))\n\n        # Save the image as an uncompressed PNG.\n        img = PIL.Image.fromarray(img, {1: 'L', 3: 'RGB'}[channels])\n        image_bits = io.BytesIO()\n        img.save(image_bits, format='png', compress_level=0, optimize=False)\n        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())\n        labels.append([archive_fname, image['label']] if image['label'] is not None else None)\n\n    metadata = {'labels': labels if all(x is not None for x in labels) else None}\n    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))\n    close_dest()", "@click.option('--resolution', help='Output resolution (e.g., 512x512)', metavar='WxH',  type=parse_tuple)\n\ndef main(\n    source: str,\n    dest: str,\n    max_images: Optional[int],\n    transform: Optional[str],\n    resolution: Optional[Tuple[int, int]]\n):\n    \"\"\"Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.\n\n    The input dataset format is guessed from the --source argument:\n\n    \\b\n    --source *_lmdb/                    Load LSUN dataset\n    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n    --source train-images-idx3-ubyte.gz Load MNIST dataset\n    --source path/                      Recursively load all images from path/\n    --source dataset.zip                Recursively load all images from dataset.zip\n\n    Specifying the output format and path:\n\n    \\b\n    --dest /path/to/dir                 Save output files under /path/to/dir\n    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n\n    The output dataset format can be either an image folder or an uncompressed zip archive.\n    Zip archives makes it easier to move datasets around file servers and clusters, and may\n    offer better training performance on network file systems.\n\n    Images within the dataset archive will be stored as uncompressed PNG.\n    Uncompresed PNGs can be efficiently decoded in the training loop.\n\n    Class labels are stored in a file called 'dataset.json' that is stored at the\n    dataset root folder.  This file has the following structure:\n\n    \\b\n    {\n        \"labels\": [\n            [\"00000/img00000000.png\",6],\n            [\"00000/img00000001.png\",9],\n            ... repeated for every image in the datase\n            [\"00049/img00049999.png\",1]\n        ]\n    }\n\n    If the 'dataset.json' file cannot be found, class labels are determined from\n    top-level directory names.\n\n    Image scale/crop and resolution requirements:\n\n    Output images must be square-shaped and they must all have the same power-of-two\n    dimensions.\n\n    To scale arbitrary input image size to a specific width and height, use the\n    --resolution option.  Output resolution will be either the original\n    input resolution (if resolution was not specified) or the one specified with\n    --resolution option.\n\n    Use the --transform=center-crop or --transform=center-crop-wide options to apply a\n    center crop transform on the input image.  These options should be used with the\n    --resolution option.  For example:\n\n    \\b\n    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\\\n        --transform=center-crop-wide --resolution=512x384\n    \"\"\"\n\n    PIL.Image.init()\n\n    if dest == '':\n        raise click.ClickException('--dest output filename or directory must not be an empty string')\n\n    num_files, input_iter = open_dataset(source, max_images=max_images)\n    archive_root_dir, save_bytes, close_dest = open_dest(dest)\n\n    if resolution is None: resolution = (None, None)\n    transform_image = make_transform(transform, *resolution)\n\n    dataset_attrs = None\n\n    labels = []\n    for idx, image in tqdm(enumerate(input_iter), total=num_files):\n        idx_str = f'{idx:08d}'\n        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'\n\n        # Apply crop and resize.\n        img = transform_image(image['img'])\n        if img is None:\n            continue\n\n        # Error check to require uniform image attributes across\n        # the whole dataset.\n        channels = img.shape[2] if img.ndim == 3 else 1\n        cur_image_attrs = {'width': img.shape[1], 'height': img.shape[0], 'channels': channels}\n        if dataset_attrs is None:\n            dataset_attrs = cur_image_attrs\n            width = dataset_attrs['width']\n            height = dataset_attrs['height']\n            if width != height:\n                raise click.ClickException(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')\n            if dataset_attrs['channels'] not in [1, 3]:\n                raise click.ClickException('Input images must be stored as RGB or grayscale')\n            if width != 2 ** int(np.floor(np.log2(width))):\n                raise click.ClickException('Image width/height after scale and crop are required to be power-of-two')\n        elif dataset_attrs != cur_image_attrs:\n            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]\n            raise click.ClickException(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\\n' + '\\n'.join(err))\n\n        # Save the image as an uncompressed PNG.\n        img = PIL.Image.fromarray(img, {1: 'L', 3: 'RGB'}[channels])\n        image_bits = io.BytesIO()\n        img.save(image_bits, format='png', compress_level=0, optimize=False)\n        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())\n        labels.append([archive_fname, image['label']] if image['label'] is not None else None)\n\n    metadata = {'labels': labels if all(x is not None for x in labels) else None}\n    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))\n    close_dest()", "\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "generate.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Generate random images using the techniques described in the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport os\nimport re\nimport click\nimport tqdm\nimport pickle\nimport numpy as np\nimport torch\nimport PIL.Image", "import torch\nimport PIL.Image\nimport dnnlib\nfrom torch_utils import distributed as dist\nfrom training.pos_embedding import Pos_Embedding\n\nfrom diffusers import AutoencoderKL\n\n\ndef random_patch(images, patch_size, resolution):\n    device = images.device\n\n    pos_shape = (images.shape[0], 1, patch_size, patch_size)\n    x_pos = torch.ones(pos_shape)\n    y_pos = torch.ones(pos_shape)\n    x_start = np.random.randint(resolution - patch_size)\n    y_start = np.random.randint(resolution - patch_size)\n\n    x_pos = x_pos * x_start + torch.arange(patch_size).view(1, -1)\n    y_pos = y_pos * y_start + torch.arange(patch_size).view(-1, 1)\n\n    x_pos = (x_pos / resolution - 0.5) * 2.\n    y_pos = (y_pos / resolution - 0.5) * 2.\n\n    # Add x and y additional position channels\n    images_patch = images[:, :, x_start:x_start + patch_size, y_start:y_start + patch_size]\n    images_pos = torch.cat([x_pos.to(device), y_pos.to(device)], dim=1)\n\n    return images_patch, images_pos", "\ndef random_patch(images, patch_size, resolution):\n    device = images.device\n\n    pos_shape = (images.shape[0], 1, patch_size, patch_size)\n    x_pos = torch.ones(pos_shape)\n    y_pos = torch.ones(pos_shape)\n    x_start = np.random.randint(resolution - patch_size)\n    y_start = np.random.randint(resolution - patch_size)\n\n    x_pos = x_pos * x_start + torch.arange(patch_size).view(1, -1)\n    y_pos = y_pos * y_start + torch.arange(patch_size).view(-1, 1)\n\n    x_pos = (x_pos / resolution - 0.5) * 2.\n    y_pos = (y_pos / resolution - 0.5) * 2.\n\n    # Add x and y additional position channels\n    images_patch = images[:, :, x_start:x_start + patch_size, y_start:y_start + patch_size]\n    images_pos = torch.cat([x_pos.to(device), y_pos.to(device)], dim=1)\n\n    return images_patch, images_pos", "\n#----------------------------------------------------------------------------\n# Proposed EDM sampler (Algorithm 2).\n\ndef edm_sampler(\n    net, latents, latents_pos, mask_pos, class_labels=None, randn_like=torch.randn_like,\n    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n):\n    # img_channel = latents.shape[1]\n    # Adjust noise levels based on what's supported by the network.\n    sigma_min = max(sigma_min, net.sigma_min)\n    sigma_max = min(sigma_max, net.sigma_max)\n\n    # Time step discretization.\n    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n\n    # Main sampling loop.\n    x_next = latents.to(torch.float64) * t_steps[0]\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n        x_cur = x_next\n\n        # Increase noise temporarily.\n        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n\n        # Euler step.\n        if mask_pos:\n            denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n        else:\n            # x_hat_w_pos = torch.cat([x_hat, latents_pos], dim=1)\n            denoised = net(x_hat, t_hat, latents_pos, class_labels).to(torch.float64)\n            # denoised = denoised[:, :img_channel]\n\n        d_cur = (x_hat - denoised) / t_hat\n        x_next = x_hat + (t_next - t_hat) * d_cur\n\n        # Apply 2nd order correction.\n        if i < num_steps - 1:\n            if mask_pos:\n                denoised = net(x_next, t_next, class_labels).to(torch.float64)\n            else:\n                # x_next_w_pos = torch.cat([x_next, latents_pos], dim=1)\n                denoised = net(x_next, t_next, latents_pos, class_labels).to(torch.float64)\n                # denoised = denoised[:, :img_channel]\n\n            d_prime = (x_next - denoised) / t_next\n            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n\n    return x_next", "\n#----------------------------------------------------------------------------\n# Generalized ablation sampler, representing the superset of all sampling\n# methods discussed in the paper.\n\ndef ablation_sampler(\n    net, latents, class_labels=None, randn_like=torch.randn_like,\n    num_steps=18, sigma_min=None, sigma_max=None, rho=7,\n    solver='heun', discretization='edm', schedule='linear', scaling='none',\n    epsilon_s=1e-3, C_1=0.001, C_2=0.008, M=1000, alpha=1,\n    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n):\n    assert solver in ['euler', 'heun']\n    assert discretization in ['vp', 've', 'iddpm', 'edm']\n    assert schedule in ['vp', 've', 'linear']\n    assert scaling in ['vp', 'none']\n\n    # Helper functions for VP & VE noise level schedules.\n    vp_sigma = lambda beta_d, beta_min: lambda t: (np.e ** (0.5 * beta_d * (t ** 2) + beta_min * t) - 1) ** 0.5\n    vp_sigma_deriv = lambda beta_d, beta_min: lambda t: 0.5 * (beta_min + beta_d * t) * (sigma(t) + 1 / sigma(t))\n    vp_sigma_inv = lambda beta_d, beta_min: lambda sigma: ((beta_min ** 2 + 2 * beta_d * (sigma ** 2 + 1).log()).sqrt() - beta_min) / beta_d\n    ve_sigma = lambda t: t.sqrt()\n    ve_sigma_deriv = lambda t: 0.5 / t.sqrt()\n    ve_sigma_inv = lambda sigma: sigma ** 2\n\n    # Select default noise level range based on the specified time step discretization.\n    if sigma_min is None:\n        vp_def = vp_sigma(beta_d=19.1, beta_min=0.1)(t=epsilon_s)\n        sigma_min = {'vp': vp_def, 've': 0.02, 'iddpm': 0.002, 'edm': 0.002}[discretization]\n    if sigma_max is None:\n        vp_def = vp_sigma(beta_d=19.1, beta_min=0.1)(t=1)\n        sigma_max = {'vp': vp_def, 've': 100, 'iddpm': 81, 'edm': 80}[discretization]\n\n    # Adjust noise levels based on what's supported by the network.\n    sigma_min = max(sigma_min, net.sigma_min)\n    sigma_max = min(sigma_max, net.sigma_max)\n\n    # Compute corresponding betas for VP.\n    vp_beta_d = 2 * (np.log(sigma_min ** 2 + 1) / epsilon_s - np.log(sigma_max ** 2 + 1)) / (epsilon_s - 1)\n    vp_beta_min = np.log(sigma_max ** 2 + 1) - 0.5 * vp_beta_d\n\n    # Define time steps in terms of noise level.\n    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n    if discretization == 'vp':\n        orig_t_steps = 1 + step_indices / (num_steps - 1) * (epsilon_s - 1)\n        sigma_steps = vp_sigma(vp_beta_d, vp_beta_min)(orig_t_steps)\n    elif discretization == 've':\n        orig_t_steps = (sigma_max ** 2) * ((sigma_min ** 2 / sigma_max ** 2) ** (step_indices / (num_steps - 1)))\n        sigma_steps = ve_sigma(orig_t_steps)\n    elif discretization == 'iddpm':\n        u = torch.zeros(M + 1, dtype=torch.float64, device=latents.device)\n        alpha_bar = lambda j: (0.5 * np.pi * j / M / (C_2 + 1)).sin() ** 2\n        for j in torch.arange(M, 0, -1, device=latents.device): # M, ..., 1\n            u[j - 1] = ((u[j] ** 2 + 1) / (alpha_bar(j - 1) / alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n        u_filtered = u[torch.logical_and(u >= sigma_min, u <= sigma_max)]\n        sigma_steps = u_filtered[((len(u_filtered) - 1) / (num_steps - 1) * step_indices).round().to(torch.int64)]\n    else:\n        assert discretization == 'edm'\n        sigma_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n\n    # Define noise level schedule.\n    if schedule == 'vp':\n        sigma = vp_sigma(vp_beta_d, vp_beta_min)\n        sigma_deriv = vp_sigma_deriv(vp_beta_d, vp_beta_min)\n        sigma_inv = vp_sigma_inv(vp_beta_d, vp_beta_min)\n    elif schedule == 've':\n        sigma = ve_sigma\n        sigma_deriv = ve_sigma_deriv\n        sigma_inv = ve_sigma_inv\n    else:\n        assert schedule == 'linear'\n        sigma = lambda t: t\n        sigma_deriv = lambda t: 1\n        sigma_inv = lambda sigma: sigma\n\n    # Define scaling schedule.\n    if scaling == 'vp':\n        s = lambda t: 1 / (1 + sigma(t) ** 2).sqrt()\n        s_deriv = lambda t: -sigma(t) * sigma_deriv(t) * (s(t) ** 3)\n    else:\n        assert scaling == 'none'\n        s = lambda t: 1\n        s_deriv = lambda t: 0\n\n    # Compute final time steps based on the corresponding noise levels.\n    t_steps = sigma_inv(net.round_sigma(sigma_steps))\n    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n\n    # Main sampling loop.\n    t_next = t_steps[0]\n    x_next = latents.to(torch.float64) * (sigma(t_next) * s(t_next))\n    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n        x_cur = x_next\n\n        # Increase noise temporarily.\n        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= sigma(t_cur) <= S_max else 0\n        t_hat = sigma_inv(net.round_sigma(sigma(t_cur) + gamma * sigma(t_cur)))\n        x_hat = s(t_hat) / s(t_cur) * x_cur + (sigma(t_hat) ** 2 - sigma(t_cur) ** 2).clip(min=0).sqrt() * s(t_hat) * S_noise * randn_like(x_cur)\n\n        # Euler step.\n        h = t_next - t_hat\n        denoised = net(x_hat / s(t_hat), sigma(t_hat), class_labels).to(torch.float64)\n        d_cur = (sigma_deriv(t_hat) / sigma(t_hat) + s_deriv(t_hat) / s(t_hat)) * x_hat - sigma_deriv(t_hat) * s(t_hat) / sigma(t_hat) * denoised\n        x_prime = x_hat + alpha * h * d_cur\n        t_prime = t_hat + alpha * h\n\n        # Apply 2nd order correction.\n        if solver == 'euler' or i == num_steps - 1:\n            x_next = x_hat + h * d_cur\n        else:\n            assert solver == 'heun'\n            denoised = net(x_prime / s(t_prime), sigma(t_prime), class_labels).to(torch.float64)\n            d_prime = (sigma_deriv(t_prime) / sigma(t_prime) + s_deriv(t_prime) / s(t_prime)) * x_prime - sigma_deriv(t_prime) * s(t_prime) / sigma(t_prime) * denoised\n            x_next = x_hat + h * ((1 - 1 / (2 * alpha)) * d_cur + 1 / (2 * alpha) * d_prime)\n\n    return x_next", "\n#----------------------------------------------------------------------------\n# Wrapper for torch.Generator that allows specifying a different random seed\n# for each sample in a minibatch.\n\nclass StackedRandomGenerator:\n    def __init__(self, device, seeds):\n        super().__init__()\n        self.generators = [torch.Generator(device).manual_seed(int(seed) % (1 << 32)) for seed in seeds]\n\n    def randn(self, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randn(size[1:], generator=gen, **kwargs) for gen in self.generators])\n\n    def randn_like(self, input):\n        return self.randn(input.shape, dtype=input.dtype, layout=input.layout, device=input.device)\n\n    def randint(self, *args, size, **kwargs):\n        assert size[0] == len(self.generators)\n        return torch.stack([torch.randint(*args, size=size[1:], generator=gen, **kwargs) for gen in self.generators])", "\n#----------------------------------------------------------------------------\n# Parse a comma separated list of numbers or ranges and return a list of ints.\n# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\ndef parse_int_list(s):\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges", "\n#----------------------------------------------------------------------------\n\ndef set_requires_grad(model, value):\n    for param in model.parameters():\n        param.requires_grad = value\n\n#----------------------------------------------------------------------------\n\n", "\n\n@click.command()\n@click.option('--network', 'network_pkl',  help='Network pickle filename', metavar='PATH|URL',                      type=str, required=True)\n@click.option('--resolution',              help='Sample resolution', metavar='INT',                                 type=int, default=64)\n@click.option('--embed_fq',                help='Positional embedding frequency', metavar='INT',                    type=int, default=0)\n@click.option('--mask_pos',                help='Mask out pos channels', metavar='BOOL',                            type=bool, default=False, show_default=True)\n@click.option('--on_latents',              help='Generate with latent vae', metavar='BOOL',                            type=bool, default=False, show_default=True)\n@click.option('--outdir',                  help='Where to save the output images', metavar='DIR',                   type=str, required=True)\n", "@click.option('--outdir',                  help='Where to save the output images', metavar='DIR',                   type=str, required=True)\n\n# patch options\n@click.option('--x_start',                 help='Sample resolution', metavar='INT',                                 type=int, default=0)\n@click.option('--y_start',                 help='Sample resolution', metavar='INT',                                 type=int, default=0)\n@click.option('--image_size',                help='Sample resolution', metavar='INT',                                 type=int, default=None)\n\n@click.option('--seeds',                   help='Random seeds (e.g. 1,2,5-10)', metavar='LIST',                     type=parse_int_list, default='0-63', show_default=True)\n@click.option('--subdirs',                 help='Create subdirectory for every 1000 seeds',                         is_flag=True)\n@click.option('--class', 'class_idx',      help='Class label  [default: random]', metavar='INT',                    type=click.IntRange(min=0), default=None)", "@click.option('--subdirs',                 help='Create subdirectory for every 1000 seeds',                         is_flag=True)\n@click.option('--class', 'class_idx',      help='Class label  [default: random]', metavar='INT',                    type=click.IntRange(min=0), default=None)\n@click.option('--batch', 'max_batch_size', help='Maximum batch size', metavar='INT',                                type=click.IntRange(min=1), default=64, show_default=True)\n\n@click.option('--steps', 'num_steps',      help='Number of sampling steps', metavar='INT',                          type=click.IntRange(min=1), default=18, show_default=True)\n@click.option('--sigma_min',               help='Lowest noise level  [default: varies]', metavar='FLOAT',           type=click.FloatRange(min=0, min_open=True))\n@click.option('--sigma_max',               help='Highest noise level  [default: varies]', metavar='FLOAT',          type=click.FloatRange(min=0, min_open=True))\n@click.option('--rho',                     help='Time step exponent', metavar='FLOAT',                              type=click.FloatRange(min=0, min_open=True), default=7, show_default=True)\n@click.option('--S_churn', 'S_churn',      help='Stochasticity strength', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)\n@click.option('--S_min', 'S_min',          help='Stoch. min noise level', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)", "@click.option('--S_churn', 'S_churn',      help='Stochasticity strength', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)\n@click.option('--S_min', 'S_min',          help='Stoch. min noise level', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)\n@click.option('--S_max', 'S_max',          help='Stoch. max noise level', metavar='FLOAT',                          type=click.FloatRange(min=0), default='inf', show_default=True)\n@click.option('--S_noise', 'S_noise',      help='Stoch. noise inflation', metavar='FLOAT',                          type=float, default=1, show_default=True)\n\n@click.option('--solver',                  help='Ablate ODE solver', metavar='euler|heun',                          type=click.Choice(['euler', 'heun']))\n@click.option('--disc', 'discretization',  help='Ablate time step discretization {t_i}', metavar='vp|ve|iddpm|edm', type=click.Choice(['vp', 've', 'iddpm', 'edm']))\n@click.option('--schedule',                help='Ablate noise schedule sigma(t)', metavar='vp|ve|linear',           type=click.Choice(['vp', 've', 'linear']))\n@click.option('--scaling',                 help='Ablate signal scaling s(t)', metavar='vp|none',                    type=click.Choice(['vp', 'none']))\n\ndef main(network_pkl, resolution, on_latents, embed_fq, mask_pos, x_start, y_start, image_size, outdir, subdirs, seeds, class_idx, max_batch_size, device=torch.device('cuda'), **sampler_kwargs):\n    \"\"\"Generate random images using the techniques described in the paper\n    \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\n    Examples:\n\n    \\b\n    # Generate 64 images and save them as out/*.png\n    python generate.py --outdir=out --seeds=0-63 --batch=64 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\n    \\b\n    # Generate 1024 images using 2 GPUs\n    torchrun --standalone --nproc_per_node=2 generate.py --outdir=out --seeds=0-999 --batch=64 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n    \"\"\"\n    dist.init()\n    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\n    # Rank 0 goes first.\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()\n\n    # Load network.\n    dist.print0(f'Loading network from \"{network_pkl}\"...')\n    with dnnlib.util.open_url(network_pkl, verbose=(dist.get_rank() == 0)) as f:\n        net = pickle.load(f)['ema'].to(device)\n\n    # Other ranks follow.\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n\n    if on_latents:\n        # img_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n        img_vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n        img_vae.eval()\n        set_requires_grad(img_vae, False)\n        latent_scale_factor = 0.18215\n\n    # Loop over batches.\n    dist.print0(f'Generating {len(seeds)} images to \"{outdir}\"...')\n    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        batch_size = len(batch_seeds)\n        if batch_size == 0:\n            continue\n\n        # Pick latents and labels.\n        rnd = StackedRandomGenerator(device, batch_seeds)\n\n        image_channel = 3\n        if image_size is None: image_size = resolution\n        if on_latents:\n            x_start, image_size, resolution, image_channel = 0, 32, 32, 4\n\n        x_pos = torch.arange(x_start, x_start+image_size).view(1, -1).repeat(image_size, 1)\n        y_pos = torch.arange(y_start, y_start+image_size).view(-1, 1).repeat(1, image_size)\n        x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n        y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n        latents_pos = torch.stack([x_pos, y_pos], dim=0).to(device)\n        latents_pos = latents_pos.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n        if mask_pos: latents_pos = torch.zeros_like(latents_pos)\n        if embed_fq > 0:\n            pos_embed = Pos_Embedding(num_freqs=embed_fq)\n            latents_pos = pos_embed(latents_pos)\n\n        latents = rnd.randn([batch_size, image_channel, image_size, image_size], device=device)\n        # rnd = StackedRandomGenerator(device, batch_seeds)\n        # latents = rnd.randn([batch_size, 3, 64, 64], device=device)\n        # latents, latents_pos = random_patch(latents, 16, 64)\n\n        class_labels = None\n        if net.label_dim:\n            class_labels = torch.eye(net.label_dim, device=device)[rnd.randint(net.label_dim, size=[batch_size], device=device)]\n        if class_idx is not None:\n            class_labels[:, :] = 0\n            class_labels[:, class_idx] = 1\n\n        # Generate images.\n        sampler_kwargs = {key: value for key, value in sampler_kwargs.items() if value is not None}\n        have_ablation_kwargs = any(x in sampler_kwargs for x in ['solver', 'discretization', 'schedule', 'scaling'])\n        sampler_fn = ablation_sampler if have_ablation_kwargs else edm_sampler\n        images = sampler_fn(net, latents, latents_pos, mask_pos, class_labels, randn_like=rnd.randn_like, **sampler_kwargs)\n\n        if on_latents:\n            images = 1 / 0.18215 * images\n            images = img_vae.decode(images.float()).sample\n\n        # Save images.\n        images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n        for seed, image_np in zip(batch_seeds, images_np):\n            image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if subdirs else outdir\n            os.makedirs(image_dir, exist_ok=True)\n            image_path = os.path.join(image_dir, f'{seed:06d}.png')\n            if image_np.shape[2] == 1:\n                PIL.Image.fromarray(image_np[:, :, 0], 'L').save(image_path)\n            else:\n                PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n\n    # Done.\n    torch.distributed.barrier()\n    dist.print0('Done.')", "@click.option('--scaling',                 help='Ablate signal scaling s(t)', metavar='vp|none',                    type=click.Choice(['vp', 'none']))\n\ndef main(network_pkl, resolution, on_latents, embed_fq, mask_pos, x_start, y_start, image_size, outdir, subdirs, seeds, class_idx, max_batch_size, device=torch.device('cuda'), **sampler_kwargs):\n    \"\"\"Generate random images using the techniques described in the paper\n    \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\n    Examples:\n\n    \\b\n    # Generate 64 images and save them as out/*.png\n    python generate.py --outdir=out --seeds=0-63 --batch=64 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\n    \\b\n    # Generate 1024 images using 2 GPUs\n    torchrun --standalone --nproc_per_node=2 generate.py --outdir=out --seeds=0-999 --batch=64 \\\\\n        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n    \"\"\"\n    dist.init()\n    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\n    # Rank 0 goes first.\n    if dist.get_rank() != 0:\n        torch.distributed.barrier()\n\n    # Load network.\n    dist.print0(f'Loading network from \"{network_pkl}\"...')\n    with dnnlib.util.open_url(network_pkl, verbose=(dist.get_rank() == 0)) as f:\n        net = pickle.load(f)['ema'].to(device)\n\n    # Other ranks follow.\n    if dist.get_rank() == 0:\n        torch.distributed.barrier()\n\n    if on_latents:\n        # img_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n        img_vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n        img_vae.eval()\n        set_requires_grad(img_vae, False)\n        latent_scale_factor = 0.18215\n\n    # Loop over batches.\n    dist.print0(f'Generating {len(seeds)} images to \"{outdir}\"...')\n    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n        torch.distributed.barrier()\n        batch_size = len(batch_seeds)\n        if batch_size == 0:\n            continue\n\n        # Pick latents and labels.\n        rnd = StackedRandomGenerator(device, batch_seeds)\n\n        image_channel = 3\n        if image_size is None: image_size = resolution\n        if on_latents:\n            x_start, image_size, resolution, image_channel = 0, 32, 32, 4\n\n        x_pos = torch.arange(x_start, x_start+image_size).view(1, -1).repeat(image_size, 1)\n        y_pos = torch.arange(y_start, y_start+image_size).view(-1, 1).repeat(1, image_size)\n        x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n        y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n        latents_pos = torch.stack([x_pos, y_pos], dim=0).to(device)\n        latents_pos = latents_pos.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n        if mask_pos: latents_pos = torch.zeros_like(latents_pos)\n        if embed_fq > 0:\n            pos_embed = Pos_Embedding(num_freqs=embed_fq)\n            latents_pos = pos_embed(latents_pos)\n\n        latents = rnd.randn([batch_size, image_channel, image_size, image_size], device=device)\n        # rnd = StackedRandomGenerator(device, batch_seeds)\n        # latents = rnd.randn([batch_size, 3, 64, 64], device=device)\n        # latents, latents_pos = random_patch(latents, 16, 64)\n\n        class_labels = None\n        if net.label_dim:\n            class_labels = torch.eye(net.label_dim, device=device)[rnd.randint(net.label_dim, size=[batch_size], device=device)]\n        if class_idx is not None:\n            class_labels[:, :] = 0\n            class_labels[:, class_idx] = 1\n\n        # Generate images.\n        sampler_kwargs = {key: value for key, value in sampler_kwargs.items() if value is not None}\n        have_ablation_kwargs = any(x in sampler_kwargs for x in ['solver', 'discretization', 'schedule', 'scaling'])\n        sampler_fn = ablation_sampler if have_ablation_kwargs else edm_sampler\n        images = sampler_fn(net, latents, latents_pos, mask_pos, class_labels, randn_like=rnd.randn_like, **sampler_kwargs)\n\n        if on_latents:\n            images = 1 / 0.18215 * images\n            images = img_vae.decode(images.float()).sample\n\n        # Save images.\n        images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n        for seed, image_np in zip(batch_seeds, images_np):\n            image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if subdirs else outdir\n            os.makedirs(image_dir, exist_ok=True)\n            image_path = os.path.join(image_dir, f'{seed:06d}.png')\n            if image_np.shape[2] == 1:\n                PIL.Image.fromarray(image_np[:, :, 0], 'L').save(image_path)\n            else:\n                PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n\n    # Done.\n    torch.distributed.barrier()\n    dist.print0('Done.')", "\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "training/networks.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Model architectures and preconditioning schemes used in the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_utils import persistence\nfrom torch.nn.functional import silu\n\n#----------------------------------------------------------------------------\n# Unified routine for initializing weights and biases.\n\ndef weight_init(shape, mode, fan_in, fan_out):\n    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n    raise ValueError(f'Invalid init mode \"{mode}\"')", "# Unified routine for initializing weights and biases.\n\ndef weight_init(shape, mode, fan_in, fan_out):\n    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n    raise ValueError(f'Invalid init mode \"{mode}\"')\n\n#----------------------------------------------------------------------------", "\n#----------------------------------------------------------------------------\n# Fully-connected layer.\n\n@persistence.persistent_class\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        x = x @ self.weight.to(x.dtype).t()\n        if self.bias is not None:\n            x = x.add_(self.bias.to(x.dtype))\n        return x", "\n#----------------------------------------------------------------------------\n# Convolutional layer with optional up/downsampling.\n\n@persistence.persistent_class\nclass Conv2d(torch.nn.Module):\n    def __init__(self,\n        in_channels, out_channels, kernel, bias=True, up=False, down=False,\n        resample_filter=[1,1], fused_resample=False, init_mode='kaiming_normal', init_weight=1, init_bias=0,\n    ):\n        assert not (up and down)\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.up = up\n        self.down = down\n        self.fused_resample = fused_resample\n        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel*kernel, fan_out=out_channels*kernel*kernel)\n        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel, kernel], **init_kwargs) * init_weight) if kernel else None\n        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if kernel and bias else None\n        f = torch.as_tensor(resample_filter, dtype=torch.float32)\n        f = f.ger(f).unsqueeze(0).unsqueeze(1) / f.sum().square()\n        self.register_buffer('resample_filter', f if up or down else None)\n\n    def forward(self, x):\n        w = self.weight.to(x.dtype) if self.weight is not None else None\n        b = self.bias.to(x.dtype) if self.bias is not None else None\n        f = self.resample_filter.to(x.dtype) if self.resample_filter is not None else None\n        w_pad = w.shape[-1] // 2 if w is not None else 0\n        f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n\n        if self.fused_resample and self.up and w is not None:\n            x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=max(f_pad - w_pad, 0))\n            x = torch.nn.functional.conv2d(x, w, padding=max(w_pad - f_pad, 0))\n        elif self.fused_resample and self.down and w is not None:\n            x = torch.nn.functional.conv2d(x, w, padding=w_pad+f_pad)\n            x = torch.nn.functional.conv2d(x, f.tile([self.out_channels, 1, 1, 1]), groups=self.out_channels, stride=2)\n        else:\n            if self.up:\n                x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n            if self.down:\n                x = torch.nn.functional.conv2d(x, f.tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n            if w is not None:\n                x = torch.nn.functional.conv2d(x, w, padding=w_pad)\n        if b is not None:\n            x = x.add_(b.reshape(1, -1, 1, 1))\n        return x", "\n#----------------------------------------------------------------------------\n# Group normalization.\n\n@persistence.persistent_class\nclass GroupNorm(torch.nn.Module):\n    def __init__(self, num_channels, num_groups=32, min_channels_per_group=4, eps=1e-5):\n        super().__init__()\n        self.num_groups = min(num_groups, num_channels // min_channels_per_group)\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.ones(num_channels))\n        self.bias = torch.nn.Parameter(torch.zeros(num_channels))\n\n    def forward(self, x):\n        x = torch.nn.functional.group_norm(x, num_groups=self.num_groups, weight=self.weight.to(x.dtype), bias=self.bias.to(x.dtype), eps=self.eps)\n        return x", "\n#----------------------------------------------------------------------------\n# Attention weight computation, i.e., softmax(Q^T * K).\n# Performs all computation using FP32, but uses the original datatype for\n# inputs/outputs/gradients to conserve memory.\n\nclass AttentionOp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k):\n        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n        ctx.save_for_backward(q, k, w)\n        return w\n\n    @staticmethod\n    def backward(ctx, dw):\n        q, k, w = ctx.saved_tensors\n        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n        return dq, dk", "\n#----------------------------------------------------------------------------\n# Unified U-Net block with optional up/downsampling and self-attention.\n# Represents the union of all features employed by the DDPM++, NCSN++, and\n# ADM architectures.\n\n@persistence.persistent_class\nclass UNetBlock(torch.nn.Module):\n    def __init__(self,\n        in_channels, out_channels, emb_channels, up=False, down=False, attention=False,\n        num_heads=None, channels_per_head=64, dropout=0, skip_scale=1, eps=1e-5,\n        resample_filter=[1,1], resample_proj=False, adaptive_scale=True,\n        init=dict(), init_zero=dict(init_weight=0), init_attn=None,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.emb_channels = emb_channels\n        self.num_heads = 0 if not attention else num_heads if num_heads is not None else out_channels // channels_per_head\n        self.dropout = dropout\n        self.skip_scale = skip_scale\n        self.adaptive_scale = adaptive_scale\n\n        self.norm0 = GroupNorm(num_channels=in_channels, eps=eps)\n        self.conv0 = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=3, up=up, down=down, resample_filter=resample_filter, **init)\n        self.affine = Linear(in_features=emb_channels, out_features=out_channels*(2 if adaptive_scale else 1), **init)\n        self.norm1 = GroupNorm(num_channels=out_channels, eps=eps)\n        self.conv1 = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=3, **init_zero)\n\n        self.skip = None\n        if out_channels != in_channels or up or down:\n            kernel = 1 if resample_proj or out_channels!= in_channels else 0\n            self.skip = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=kernel, up=up, down=down, resample_filter=resample_filter, **init)\n\n        if self.num_heads:\n            self.norm2 = GroupNorm(num_channels=out_channels, eps=eps)\n            self.qkv = Conv2d(in_channels=out_channels, out_channels=out_channels*3, kernel=1, **(init_attn if init_attn is not None else init))\n            self.proj = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=1, **init_zero)\n\n    def forward(self, x, emb):\n        orig = x\n        x = self.conv0(silu(self.norm0(x)))\n\n        params = self.affine(emb).unsqueeze(2).unsqueeze(3).to(x.dtype)\n        if self.adaptive_scale:\n            scale, shift = params.chunk(chunks=2, dim=1)\n            x = silu(torch.addcmul(shift, self.norm1(x), scale + 1))\n        else:\n            x = silu(self.norm1(x.add_(params)))\n\n        x = self.conv1(torch.nn.functional.dropout(x, p=self.dropout, training=self.training))\n        x = x.add_(self.skip(orig) if self.skip is not None else orig)\n        x = x * self.skip_scale\n\n        if self.num_heads:\n            q, k, v = self.qkv(self.norm2(x)).reshape(x.shape[0] * self.num_heads, x.shape[1] // self.num_heads, 3, -1).unbind(2)\n            w = AttentionOp.apply(q, k)\n            a = torch.einsum('nqk,nck->ncq', w, v)\n            x = self.proj(a.reshape(*x.shape)).add_(x)\n            x = x * self.skip_scale\n        return x", "\n#----------------------------------------------------------------------------\n# Timestep embedding used in the DDPM++ and ADM architectures.\n\n@persistence.persistent_class\nclass PositionalEmbedding(torch.nn.Module):\n    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n        super().__init__()\n        self.num_channels = num_channels\n        self.max_positions = max_positions\n        self.endpoint = endpoint\n\n    def forward(self, x):\n        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n        freqs = (1 / self.max_positions) ** freqs\n        x = x.ger(freqs.to(x.dtype))\n        x = torch.cat([x.cos(), x.sin()], dim=1)\n        return x", "\n#----------------------------------------------------------------------------\n# Timestep embedding used in the NCSN++ architecture.\n\n@persistence.persistent_class\nclass FourierEmbedding(torch.nn.Module):\n    def __init__(self, num_channels, scale=16):\n        super().__init__()\n        self.register_buffer('freqs', torch.randn(num_channels // 2) * scale)\n\n    def forward(self, x):\n        x = x.ger((2 * np.pi * self.freqs).to(x.dtype))\n        x = torch.cat([x.cos(), x.sin()], dim=1)\n        return x", "\n#----------------------------------------------------------------------------\n# Reimplementation of the DDPM++ and NCSN++ architectures from the paper\n# \"Score-Based Generative Modeling through Stochastic Differential\n# Equations\". Equivalent to the original implementation by Song et al.,\n# available at https://github.com/yang-song/score_sde_pytorch\n\n@persistence.persistent_class\nclass SongUNet(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution at input/output.\n        in_channels,                        # Number of color channels at input.\n        out_channels,                       # Number of color channels at output.\n        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\n        model_channels      = 128,          # Base multiplier for the number of channels.\n        channel_mult        = [1,2,2,2],    # Per-resolution multipliers for the number of channels.\n        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n        num_blocks          = 4,            # Number of residual blocks per resolution.\n        attn_resolutions    = [16],         # List of resolutions with self-attention.\n        dropout             = 0.10,         # Dropout probability of intermediate activations.\n        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n\n        embedding_type      = 'positional', # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n        channel_mult_noise  = 1,            # Timestep embedding size: 1 for DDPM++, 2 for NCSN++.\n        encoder_type        = 'standard',   # Encoder architecture: 'standard' for DDPM++, 'residual' for NCSN++.\n        decoder_type        = 'standard',   # Decoder architecture: 'standard' for both DDPM++ and NCSN++.\n        resample_filter     = [1,1],        # Resampling filter: [1,1] for DDPM++, [1,3,3,1] for NCSN++.\n        implicit_mlp        = False,        # enable implicit coordinate encoding\n    ):\n        assert embedding_type in ['fourier', 'positional']\n        assert encoder_type in ['standard', 'skip', 'residual']\n        assert decoder_type in ['standard', 'skip']\n\n        super().__init__()\n        self.label_dropout = label_dropout\n        emb_channels = model_channels * channel_mult_emb\n        noise_channels = model_channels * channel_mult_noise\n        init = dict(init_mode='xavier_uniform')\n        init_zero = dict(init_mode='xavier_uniform', init_weight=1e-5)\n        init_attn = dict(init_mode='xavier_uniform', init_weight=np.sqrt(0.2))\n        block_kwargs = dict(\n            emb_channels=emb_channels, num_heads=1, dropout=dropout, skip_scale=np.sqrt(0.5), eps=1e-6,\n            resample_filter=resample_filter, resample_proj=True, adaptive_scale=False,\n            init=init, init_zero=init_zero, init_attn=init_attn,\n        )\n\n        # Mapping.\n        self.map_noise = PositionalEmbedding(num_channels=noise_channels, endpoint=True) if embedding_type == 'positional' else FourierEmbedding(num_channels=noise_channels)\n        self.map_label = Linear(in_features=label_dim, out_features=noise_channels, **init) if label_dim else None\n        self.map_augment = Linear(in_features=augment_dim, out_features=noise_channels, bias=False, **init) if augment_dim else None\n        self.map_layer0 = Linear(in_features=noise_channels, out_features=emb_channels, **init)\n        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n\n        # Encoder.\n        self.enc = torch.nn.ModuleDict()\n        cout = in_channels\n        caux = in_channels\n        for level, mult in enumerate(channel_mult):\n            res = img_resolution >> level\n            if level == 0:\n                cin = cout\n                cout = model_channels\n                if implicit_mlp:\n                    self.enc[f'{res}x{res}_conv'] = torch.nn.Sequential(\n                                                        Conv2d(in_channels=cin, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=3, **init),\n                                                    )\n                    self.enc[f'{res}x{res}_conv'].out_channels = cout\n                else:\n                    self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n            else:\n                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n                if encoder_type == 'skip':\n                    self.enc[f'{res}x{res}_aux_down'] = Conv2d(in_channels=caux, out_channels=caux, kernel=0, down=True, resample_filter=resample_filter)\n                    self.enc[f'{res}x{res}_aux_skip'] = Conv2d(in_channels=caux, out_channels=cout, kernel=1, **init)\n                if encoder_type == 'residual':\n                    self.enc[f'{res}x{res}_aux_residual'] = Conv2d(in_channels=caux, out_channels=cout, kernel=3, down=True, resample_filter=resample_filter, fused_resample=True, **init)\n                    caux = cout\n            for idx in range(num_blocks):\n                cin = cout\n                cout = model_channels * mult\n                attn = (res in attn_resolutions)\n                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n        skips = [block.out_channels for name, block in self.enc.items() if 'aux' not in name]\n\n        # Decoder.\n        self.dec = torch.nn.ModuleDict()\n        for level, mult in reversed(list(enumerate(channel_mult))):\n            res = img_resolution >> level\n            if level == len(channel_mult) - 1:\n                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n            else:\n                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n            for idx in range(num_blocks + 1):\n                cin = cout + skips.pop()\n                cout = model_channels * mult\n                attn = (idx == num_blocks and res in attn_resolutions)\n                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n            if decoder_type == 'skip' or level == 0:\n                if decoder_type == 'skip' and level < len(channel_mult) - 1:\n                    self.dec[f'{res}x{res}_aux_up'] = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=0, up=True, resample_filter=resample_filter)\n                self.dec[f'{res}x{res}_aux_norm'] = GroupNorm(num_channels=cout, eps=1e-6)\n                self.dec[f'{res}x{res}_aux_conv'] = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\n    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n        # Mapping.\n        emb = self.map_noise(noise_labels)\n        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n        if self.map_label is not None:\n            tmp = class_labels\n            if self.training and self.label_dropout:\n                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n            emb = emb + self.map_label(tmp * np.sqrt(self.map_label.in_features))\n        if self.map_augment is not None and augment_labels is not None:\n            emb = emb + self.map_augment(augment_labels)\n        emb = silu(self.map_layer0(emb))\n        emb = silu(self.map_layer1(emb))\n\n        # Encoder.\n        skips = []\n        aux = x\n        for name, block in self.enc.items():\n            if 'aux_down' in name:\n                aux = block(aux)\n            elif 'aux_skip' in name:\n                x = skips[-1] = x + block(aux)\n            elif 'aux_residual' in name:\n                x = skips[-1] = aux = (x + block(aux)) / np.sqrt(2)\n            else:\n                x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n                skips.append(x)\n\n        # Decoder.\n        aux = None\n        tmp = None\n        for name, block in self.dec.items():\n            if 'aux_up' in name:\n                aux = block(aux)\n            elif 'aux_norm' in name:\n                tmp = block(x)\n            elif 'aux_conv' in name:\n                tmp = block(silu(tmp))\n                aux = tmp if aux is None else tmp + aux\n            else:\n                if x.shape[1] != block.in_channels:\n                    x = torch.cat([x, skips.pop()], dim=1)\n                x = block(x, emb)\n        return aux", "class SongUNet(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution at input/output.\n        in_channels,                        # Number of color channels at input.\n        out_channels,                       # Number of color channels at output.\n        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\n        model_channels      = 128,          # Base multiplier for the number of channels.\n        channel_mult        = [1,2,2,2],    # Per-resolution multipliers for the number of channels.\n        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n        num_blocks          = 4,            # Number of residual blocks per resolution.\n        attn_resolutions    = [16],         # List of resolutions with self-attention.\n        dropout             = 0.10,         # Dropout probability of intermediate activations.\n        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n\n        embedding_type      = 'positional', # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n        channel_mult_noise  = 1,            # Timestep embedding size: 1 for DDPM++, 2 for NCSN++.\n        encoder_type        = 'standard',   # Encoder architecture: 'standard' for DDPM++, 'residual' for NCSN++.\n        decoder_type        = 'standard',   # Decoder architecture: 'standard' for both DDPM++ and NCSN++.\n        resample_filter     = [1,1],        # Resampling filter: [1,1] for DDPM++, [1,3,3,1] for NCSN++.\n        implicit_mlp        = False,        # enable implicit coordinate encoding\n    ):\n        assert embedding_type in ['fourier', 'positional']\n        assert encoder_type in ['standard', 'skip', 'residual']\n        assert decoder_type in ['standard', 'skip']\n\n        super().__init__()\n        self.label_dropout = label_dropout\n        emb_channels = model_channels * channel_mult_emb\n        noise_channels = model_channels * channel_mult_noise\n        init = dict(init_mode='xavier_uniform')\n        init_zero = dict(init_mode='xavier_uniform', init_weight=1e-5)\n        init_attn = dict(init_mode='xavier_uniform', init_weight=np.sqrt(0.2))\n        block_kwargs = dict(\n            emb_channels=emb_channels, num_heads=1, dropout=dropout, skip_scale=np.sqrt(0.5), eps=1e-6,\n            resample_filter=resample_filter, resample_proj=True, adaptive_scale=False,\n            init=init, init_zero=init_zero, init_attn=init_attn,\n        )\n\n        # Mapping.\n        self.map_noise = PositionalEmbedding(num_channels=noise_channels, endpoint=True) if embedding_type == 'positional' else FourierEmbedding(num_channels=noise_channels)\n        self.map_label = Linear(in_features=label_dim, out_features=noise_channels, **init) if label_dim else None\n        self.map_augment = Linear(in_features=augment_dim, out_features=noise_channels, bias=False, **init) if augment_dim else None\n        self.map_layer0 = Linear(in_features=noise_channels, out_features=emb_channels, **init)\n        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n\n        # Encoder.\n        self.enc = torch.nn.ModuleDict()\n        cout = in_channels\n        caux = in_channels\n        for level, mult in enumerate(channel_mult):\n            res = img_resolution >> level\n            if level == 0:\n                cin = cout\n                cout = model_channels\n                if implicit_mlp:\n                    self.enc[f'{res}x{res}_conv'] = torch.nn.Sequential(\n                                                        Conv2d(in_channels=cin, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n                                                        torch.nn.SiLU(),\n                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=3, **init),\n                                                    )\n                    self.enc[f'{res}x{res}_conv'].out_channels = cout\n                else:\n                    self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n            else:\n                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n                if encoder_type == 'skip':\n                    self.enc[f'{res}x{res}_aux_down'] = Conv2d(in_channels=caux, out_channels=caux, kernel=0, down=True, resample_filter=resample_filter)\n                    self.enc[f'{res}x{res}_aux_skip'] = Conv2d(in_channels=caux, out_channels=cout, kernel=1, **init)\n                if encoder_type == 'residual':\n                    self.enc[f'{res}x{res}_aux_residual'] = Conv2d(in_channels=caux, out_channels=cout, kernel=3, down=True, resample_filter=resample_filter, fused_resample=True, **init)\n                    caux = cout\n            for idx in range(num_blocks):\n                cin = cout\n                cout = model_channels * mult\n                attn = (res in attn_resolutions)\n                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n        skips = [block.out_channels for name, block in self.enc.items() if 'aux' not in name]\n\n        # Decoder.\n        self.dec = torch.nn.ModuleDict()\n        for level, mult in reversed(list(enumerate(channel_mult))):\n            res = img_resolution >> level\n            if level == len(channel_mult) - 1:\n                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n            else:\n                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n            for idx in range(num_blocks + 1):\n                cin = cout + skips.pop()\n                cout = model_channels * mult\n                attn = (idx == num_blocks and res in attn_resolutions)\n                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n            if decoder_type == 'skip' or level == 0:\n                if decoder_type == 'skip' and level < len(channel_mult) - 1:\n                    self.dec[f'{res}x{res}_aux_up'] = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=0, up=True, resample_filter=resample_filter)\n                self.dec[f'{res}x{res}_aux_norm'] = GroupNorm(num_channels=cout, eps=1e-6)\n                self.dec[f'{res}x{res}_aux_conv'] = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\n    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n        # Mapping.\n        emb = self.map_noise(noise_labels)\n        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n        if self.map_label is not None:\n            tmp = class_labels\n            if self.training and self.label_dropout:\n                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n            emb = emb + self.map_label(tmp * np.sqrt(self.map_label.in_features))\n        if self.map_augment is not None and augment_labels is not None:\n            emb = emb + self.map_augment(augment_labels)\n        emb = silu(self.map_layer0(emb))\n        emb = silu(self.map_layer1(emb))\n\n        # Encoder.\n        skips = []\n        aux = x\n        for name, block in self.enc.items():\n            if 'aux_down' in name:\n                aux = block(aux)\n            elif 'aux_skip' in name:\n                x = skips[-1] = x + block(aux)\n            elif 'aux_residual' in name:\n                x = skips[-1] = aux = (x + block(aux)) / np.sqrt(2)\n            else:\n                x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n                skips.append(x)\n\n        # Decoder.\n        aux = None\n        tmp = None\n        for name, block in self.dec.items():\n            if 'aux_up' in name:\n                aux = block(aux)\n            elif 'aux_norm' in name:\n                tmp = block(x)\n            elif 'aux_conv' in name:\n                tmp = block(silu(tmp))\n                aux = tmp if aux is None else tmp + aux\n            else:\n                if x.shape[1] != block.in_channels:\n                    x = torch.cat([x, skips.pop()], dim=1)\n                x = block(x, emb)\n        return aux", "\n#----------------------------------------------------------------------------\n# Reimplementation of the ADM architecture from the paper\n# \"Diffusion Models Beat GANS on Image Synthesis\". Equivalent to the\n# original implementation by Dhariwal and Nichol, available at\n# https://github.com/openai/guided-diffusion\n\n@persistence.persistent_class\nclass DhariwalUNet(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution at input/output.\n        in_channels,                        # Number of color channels at input.\n        out_channels,                       # Number of color channels at output.\n        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\n        model_channels      = 192,          # Base multiplier for the number of channels.\n        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n        num_blocks          = 3,            # Number of residual blocks per resolution.\n        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n        dropout             = 0.10,         # List of resolutions with self-attention.\n        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n    ):\n        super().__init__()\n        self.label_dropout = label_dropout\n        emb_channels = model_channels * channel_mult_emb\n        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n\n        # Mapping.\n        self.map_noise = PositionalEmbedding(num_channels=model_channels)\n        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n\n        # Encoder.\n        self.enc = torch.nn.ModuleDict()\n        cout = in_channels\n        for level, mult in enumerate(channel_mult):\n            res = img_resolution >> level\n            if level == 0:\n                cin = cout\n                cout = model_channels * mult\n                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n            else:\n                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n            for idx in range(num_blocks):\n                cin = cout\n                cout = model_channels * mult\n                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n        skips = [block.out_channels for block in self.enc.values()]\n\n        # Decoder.\n        self.dec = torch.nn.ModuleDict()\n        for level, mult in reversed(list(enumerate(channel_mult))):\n            res = img_resolution >> level\n            if level == len(channel_mult) - 1:\n                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n            else:\n                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n            for idx in range(num_blocks + 1):\n                cin = cout + skips.pop()\n                cout = model_channels * mult\n                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n        self.out_norm = GroupNorm(num_channels=cout)\n        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\n    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n        # Mapping.\n        emb = self.map_noise(noise_labels)\n        if self.map_augment is not None and augment_labels is not None:\n            emb = emb + self.map_augment(augment_labels)\n        emb = silu(self.map_layer0(emb))\n        emb = self.map_layer1(emb)\n        if self.map_label is not None:\n            tmp = class_labels\n            if self.training and self.label_dropout:\n                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n            emb = emb + self.map_label(tmp)\n        emb = silu(emb)\n\n        # Encoder.\n        skips = []\n        for block in self.enc.values():\n            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n            skips.append(x)\n\n        # Decoder.\n        for block in self.dec.values():\n            if x.shape[1] != block.in_channels:\n                x = torch.cat([x, skips.pop()], dim=1)\n            x = block(x, emb)\n        x = self.out_conv(silu(self.out_norm(x)))\n        return x", "class DhariwalUNet(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution at input/output.\n        in_channels,                        # Number of color channels at input.\n        out_channels,                       # Number of color channels at output.\n        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\n        model_channels      = 192,          # Base multiplier for the number of channels.\n        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n        num_blocks          = 3,            # Number of residual blocks per resolution.\n        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n        dropout             = 0.10,         # List of resolutions with self-attention.\n        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n    ):\n        super().__init__()\n        self.label_dropout = label_dropout\n        emb_channels = model_channels * channel_mult_emb\n        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n\n        # Mapping.\n        self.map_noise = PositionalEmbedding(num_channels=model_channels)\n        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n\n        # Encoder.\n        self.enc = torch.nn.ModuleDict()\n        cout = in_channels\n        for level, mult in enumerate(channel_mult):\n            res = img_resolution >> level\n            if level == 0:\n                cin = cout\n                cout = model_channels * mult\n                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n            else:\n                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n            for idx in range(num_blocks):\n                cin = cout\n                cout = model_channels * mult\n                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n        skips = [block.out_channels for block in self.enc.values()]\n\n        # Decoder.\n        self.dec = torch.nn.ModuleDict()\n        for level, mult in reversed(list(enumerate(channel_mult))):\n            res = img_resolution >> level\n            if level == len(channel_mult) - 1:\n                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n            else:\n                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n            for idx in range(num_blocks + 1):\n                cin = cout + skips.pop()\n                cout = model_channels * mult\n                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n        self.out_norm = GroupNorm(num_channels=cout)\n        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\n    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n        # Mapping.\n        emb = self.map_noise(noise_labels)\n        if self.map_augment is not None and augment_labels is not None:\n            emb = emb + self.map_augment(augment_labels)\n        emb = silu(self.map_layer0(emb))\n        emb = self.map_layer1(emb)\n        if self.map_label is not None:\n            tmp = class_labels\n            if self.training and self.label_dropout:\n                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n            emb = emb + self.map_label(tmp)\n        emb = silu(emb)\n\n        # Encoder.\n        skips = []\n        for block in self.enc.values():\n            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n            skips.append(x)\n\n        # Decoder.\n        for block in self.dec.values():\n            if x.shape[1] != block.in_channels:\n                x = torch.cat([x, skips.pop()], dim=1)\n            x = block(x, emb)\n        x = self.out_conv(silu(self.out_norm(x)))\n        return x", "\n#----------------------------------------------------------------------------\n# Preconditioning corresponding to the variance preserving (VP) formulation\n# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".\n\n@persistence.persistent_class\nclass VPPrecond(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                 # Image resolution.\n        img_channels,                   # Number of color channels.\n        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n        beta_d          = 19.9,         # Extent of the noise level schedule.\n        beta_min        = 0.1,          # Initial slope of the noise level schedule.\n        M               = 1000,         # Original number of timesteps in the DDPM formulation.\n        epsilon_t       = 1e-5,         # Minimum t-value used during training.\n        model_type      = 'SongUNet',   # Class name of the underlying model.\n        **model_kwargs,                 # Keyword arguments for the underlying model.\n    ):\n        super().__init__()\n        self.img_resolution = img_resolution\n        self.img_channels = img_channels\n        self.label_dim = label_dim\n        self.use_fp16 = use_fp16\n        self.beta_d = beta_d\n        self.beta_min = beta_min\n        self.M = M\n        self.epsilon_t = epsilon_t\n        self.sigma_min = float(self.sigma(epsilon_t))\n        self.sigma_max = float(self.sigma(1))\n        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\n    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n        x = x.to(torch.float32)\n        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\n        c_skip = 1\n        c_out = -sigma\n        c_in = 1 / (sigma ** 2 + 1).sqrt()\n        c_noise = (self.M - 1) * self.sigma_inv(sigma)\n\n        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n        assert F_x.dtype == dtype\n        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n        return D_x\n\n    def sigma(self, t):\n        t = torch.as_tensor(t)\n        return ((0.5 * self.beta_d * (t ** 2) + self.beta_min * t).exp() - 1).sqrt()\n\n    def sigma_inv(self, sigma):\n        sigma = torch.as_tensor(sigma)\n        return ((self.beta_min ** 2 + 2 * self.beta_d * (1 + sigma ** 2).log()).sqrt() - self.beta_min) / self.beta_d\n\n    def round_sigma(self, sigma):\n        return torch.as_tensor(sigma)", "\n#----------------------------------------------------------------------------\n# Preconditioning corresponding to the variance exploding (VE) formulation\n# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".\n\n@persistence.persistent_class\nclass VEPrecond(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                 # Image resolution.\n        img_channels,                   # Number of color channels.\n        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n        sigma_min       = 0.02,         # Minimum supported noise level.\n        sigma_max       = 100,          # Maximum supported noise level.\n        model_type      = 'SongUNet',   # Class name of the underlying model.\n        **model_kwargs,                 # Keyword arguments for the underlying model.\n    ):\n        super().__init__()\n        self.img_resolution = img_resolution\n        self.img_channels = img_channels\n        self.label_dim = label_dim\n        self.use_fp16 = use_fp16\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\n    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n        x = x.to(torch.float32)\n        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\n        c_skip = 1\n        c_out = sigma\n        c_in = 1\n        c_noise = (0.5 * sigma).log()\n\n        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n        assert F_x.dtype == dtype\n        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n        return D_x\n\n    def round_sigma(self, sigma):\n        return torch.as_tensor(sigma)", "\n#----------------------------------------------------------------------------\n# Preconditioning corresponding to improved DDPM (iDDPM) formulation from\n# the paper \"Improved Denoising Diffusion Probabilistic Models\".\n\n@persistence.persistent_class\nclass iDDPMPrecond(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution.\n        img_channels,                       # Number of color channels.\n        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n        C_1             = 0.001,            # Timestep adjustment at low noise levels.\n        C_2             = 0.008,            # Timestep adjustment at high noise levels.\n        M               = 1000,             # Original number of timesteps in the DDPM formulation.\n        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n        **model_kwargs,                     # Keyword arguments for the underlying model.\n    ):\n        super().__init__()\n        self.img_resolution = img_resolution\n        self.img_channels = img_channels\n        self.label_dim = label_dim\n        self.use_fp16 = use_fp16\n        self.C_1 = C_1\n        self.C_2 = C_2\n        self.M = M\n        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels*2, label_dim=label_dim, **model_kwargs)\n\n        u = torch.zeros(M + 1)\n        for j in range(M, 0, -1): # M, ..., 1\n            u[j - 1] = ((u[j] ** 2 + 1) / (self.alpha_bar(j - 1) / self.alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n        self.register_buffer('u', u)\n        self.sigma_min = float(u[M - 1])\n        self.sigma_max = float(u[0])\n\n    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n        x = x.to(torch.float32)\n        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\n        c_skip = 1\n        c_out = -sigma\n        c_in = 1 / (sigma ** 2 + 1).sqrt()\n        c_noise = self.M - 1 - self.round_sigma(sigma, return_index=True).to(torch.float32)\n\n        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n        assert F_x.dtype == dtype\n        D_x = c_skip * x + c_out * F_x[:, :self.img_channels].to(torch.float32)\n        return D_x\n\n    def alpha_bar(self, j):\n        j = torch.as_tensor(j)\n        return (0.5 * np.pi * j / self.M / (self.C_2 + 1)).sin() ** 2\n\n    def round_sigma(self, sigma, return_index=False):\n        sigma = torch.as_tensor(sigma)\n        index = torch.cdist(sigma.to(self.u.device).to(torch.float32).reshape(1, -1, 1), self.u.reshape(1, -1, 1)).argmin(2)\n        result = index if return_index else self.u[index.flatten()].to(sigma.dtype)\n        return result.reshape(sigma.shape).to(sigma.device)", "\n#----------------------------------------------------------------------------\n# Improved preconditioning proposed in the paper \"Elucidating the Design\n# Space of Diffusion-Based Generative Models\" (EDM).\n\n@persistence.persistent_class\nclass EDMPrecond(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution.\n        img_channels,                       # Number of color channels.\n        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n        sigma_min       = 0,                # Minimum supported noise level.\n        sigma_max       = float('inf'),     # Maximum supported noise level.\n        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n        **model_kwargs,                     # Keyword arguments for the underlying model.\n    ):\n        super().__init__()\n        self.img_resolution = img_resolution\n        self.img_channels = img_channels\n        self.label_dim = label_dim\n        self.use_fp16 = use_fp16\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n        self.sigma_data = sigma_data\n        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\n    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n        x = x.to(torch.float32)\n        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\n        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n        c_noise = sigma.log() / 4\n\n        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n        assert F_x.dtype == dtype\n        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n        return D_x\n\n    def round_sigma(self, sigma):\n        return torch.as_tensor(sigma)", "\n#----------------------------------------------------------------------------\n# Patch Version of EDMPrecond.\n\n@persistence.persistent_class\nclass Patch_EDMPrecond(torch.nn.Module):\n    def __init__(self,\n        img_resolution,                     # Image resolution.\n        img_channels,                       # Number of color channels.\n        out_channels    = None,\n        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n        sigma_min       = 0,                # Minimum supported noise level.\n        sigma_max       = float('inf'),     # Maximum supported noise level.\n        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n        **model_kwargs,                     # Keyword arguments for the underlying model.\n    ):\n        super().__init__()\n        self.img_resolution = img_resolution\n        self.img_channels = img_channels\n        self.label_dim = label_dim\n        self.use_fp16 = use_fp16\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n        self.sigma_data = sigma_data\n        self.out_channels = img_channels if out_channels is None else out_channels\n        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=self.out_channels, label_dim=label_dim, **model_kwargs)\n\n    def forward(self, x, sigma, x_pos=None, class_labels=None, force_fp32=False, **model_kwargs):\n        x = x.to(torch.float32)\n        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\n        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n        c_noise = sigma.log() / 4\n\n        x_in = torch.cat([c_in * x, x_pos], dim=1) if x_pos is not None else c_in * x\n        F_x = self.model((x_in).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n        assert F_x.dtype == dtype\n        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n        return D_x\n\n    def round_sigma(self, sigma):\n        return torch.as_tensor(sigma)", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "training/loss.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Loss functions used in the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_utils import persistence\n\n#----------------------------------------------------------------------------\n# Loss function corresponding to the variance preserving (VP) formulation\n# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".", "# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".\n\n@persistence.persistent_class\nclass VPLoss:\n    def __init__(self, beta_d=19.9, beta_min=0.1, epsilon_t=1e-5):\n        self.beta_d = beta_d\n        self.beta_min = beta_min\n        self.epsilon_t = epsilon_t\n\n    def __call__(self, net, images, labels, augment_pipe=None):\n        rnd_uniform = torch.rand([images.shape[0], 1, 1, 1], device=images.device)\n        sigma = self.sigma(1 + rnd_uniform * (self.epsilon_t - 1))\n        weight = 1 / sigma ** 2\n        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n        n = torch.randn_like(y) * sigma\n        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n        loss = weight * ((D_yn - y) ** 2)\n        return loss\n\n    def sigma(self, t):\n        t = torch.as_tensor(t)\n        return ((0.5 * self.beta_d * (t ** 2) + self.beta_min * t).exp() - 1).sqrt()", "\n#----------------------------------------------------------------------------\n# Loss function corresponding to the variance exploding (VE) formulation\n# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".\n\n@persistence.persistent_class\nclass VELoss:\n    def __init__(self, sigma_min=0.02, sigma_max=100):\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n\n    def __call__(self, net, images, labels, augment_pipe=None):\n        rnd_uniform = torch.rand([images.shape[0], 1, 1, 1], device=images.device)\n        sigma = self.sigma_min * ((self.sigma_max / self.sigma_min) ** rnd_uniform)\n        weight = 1 / sigma ** 2\n        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n        n = torch.randn_like(y) * sigma\n        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n        loss = weight * ((D_yn - y) ** 2)\n        return loss", "\n#----------------------------------------------------------------------------\n# Improved loss function proposed in the paper \"Elucidating the Design Space\n# of Diffusion-Based Generative Models\" (EDM).\n\n@persistence.persistent_class\nclass EDMLoss:\n    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5):\n        self.P_mean = P_mean\n        self.P_std = P_std\n        self.sigma_data = sigma_data\n\n    def __call__(self, net, images, labels=None, augment_pipe=None):\n        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n        n = torch.randn_like(y) * sigma\n        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n        loss = weight * ((D_yn - y) ** 2)\n        return loss", "\n#----------------------------------------------------------------------------\n\n"]}
{"filename": "training/patch_loss.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Loss functions used in the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_utils import persistence\n\n#----------------------------------------------------------------------------\n# Loss function corresponding to the variance preserving (VP) formulation\n# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".", "# from the paper \"Score-Based Generative Modeling through Stochastic\n# Differential Equations\".\n\n\n@persistence.persistent_class\nclass Patch_EDMLoss:\n    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5):\n        self.P_mean = P_mean\n        self.P_std = P_std\n        self.sigma_data = sigma_data\n\n    def pachify(self, images, patch_size, padding=None):\n        device = images.device\n        batch_size, resolution = images.size(0), images.size(2)\n\n        if padding is not None:\n            padded = torch.zeros((images.size(0), images.size(1), images.size(2) + padding * 2,\n                                  images.size(3) + padding * 2), dtype=images.dtype, device=device)\n            padded[:, :, padding:-padding, padding:-padding] = images\n        else:\n            padded = images\n\n        h, w = padded.size(2), padded.size(3)\n        th, tw = patch_size, patch_size\n        if w == tw and h == th:\n            i = torch.zeros((batch_size,), device=device).long()\n            j = torch.zeros((batch_size,), device=device).long()\n        else:\n            i = torch.randint(0, h - th + 1, (batch_size,), device=device)\n            j = torch.randint(0, w - tw + 1, (batch_size,), device=device)\n\n        rows = torch.arange(th, dtype=torch.long, device=device) + i[:, None]\n        columns = torch.arange(tw, dtype=torch.long, device=device) + j[:, None]\n        padded = padded.permute(1, 0, 2, 3)\n        padded = padded[:, torch.arange(batch_size)[:, None, None], rows[:, torch.arange(th)[:, None]],\n                 columns[:, None]]\n        padded = padded.permute(1, 0, 2, 3)\n\n        x_pos = torch.arange(tw, dtype=torch.long, device=device).unsqueeze(0).repeat(th, 1).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n        y_pos = torch.arange(th, dtype=torch.long, device=device).unsqueeze(1).repeat(1, tw).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n        x_pos = x_pos + j.view(-1, 1, 1, 1)\n        y_pos = y_pos + i.view(-1, 1, 1, 1)\n        x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n        y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n        images_pos = torch.cat((x_pos, y_pos), dim=1)\n\n        return padded, images_pos\n\n    def __call__(self, net, images, patch_size, resolution, labels=None, augment_pipe=None):\n        images, images_pos = self.pachify(images, patch_size)\n\n        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n\n        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n        n = torch.randn_like(y) * sigma\n        yn = y + n\n\n        D_yn = net(yn, sigma, x_pos=images_pos, class_labels=labels, augment_labels=augment_labels)\n        loss = weight * ((D_yn - y) ** 2)\n        return loss", "\n#----------------------------------------------------------------------------\n\n"]}
{"filename": "training/dataset.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\nimport PIL.Image\nimport json\nimport torch\nimport dnnlib\n\ntry:\n    import pyspng\nexcept ImportError:\n    pyspng = None", "\ntry:\n    import pyspng\nexcept ImportError:\n    pyspng = None\n\n#----------------------------------------------------------------------------\n# Abstract base class for datasets.\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self,\n        name,                   # Name of the dataset.\n        raw_shape,              # Shape of the raw image data (NCHW).\n        max_size    = None,     # Artificially limit the size of the dataset. None = no limit. Applied before xflip.\n        use_labels  = False,    # Enable conditioning labels? False = label dimension is zero.\n        xflip       = False,    # Artificially double the size of the dataset via x-flips. Applied after max_size.\n        random_seed = 0,        # Random seed to use when applying max_size.\n        cache       = False,    # Cache images in CPU memory?\n    ):\n        self._name = name\n        self._raw_shape = list(raw_shape)\n        self._use_labels = use_labels\n        self._cache = cache\n        self._cached_images = dict() # {raw_idx: np.ndarray, ...}\n        self._raw_labels = None\n        self._label_shape = None\n\n        # Apply max_size.\n        self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)\n        if (max_size is not None) and (self._raw_idx.size > max_size):\n            np.random.RandomState(random_seed % (1 << 31)).shuffle(self._raw_idx)\n            self._raw_idx = np.sort(self._raw_idx[:max_size])\n\n        # Apply xflip.\n        self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)\n        if xflip:\n            self._raw_idx = np.tile(self._raw_idx, 2)\n            self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])\n\n    def _get_raw_labels(self):\n        if self._raw_labels is None:\n            self._raw_labels = self._load_raw_labels() if self._use_labels else None\n            if self._raw_labels is None:\n                self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)\n            assert isinstance(self._raw_labels, np.ndarray)\n            assert self._raw_labels.shape[0] == self._raw_shape[0]\n            assert self._raw_labels.dtype in [np.float32, np.int64]\n            if self._raw_labels.dtype == np.int64:\n                assert self._raw_labels.ndim == 1\n                assert np.all(self._raw_labels >= 0)\n        return self._raw_labels\n\n    def close(self): # to be overridden by subclass\n        pass\n\n    def _load_raw_image(self, raw_idx): # to be overridden by subclass\n        raise NotImplementedError\n\n    def _load_raw_labels(self): # to be overridden by subclass\n        raise NotImplementedError\n\n    def __getstate__(self):\n        return dict(self.__dict__, _raw_labels=None)\n\n    def __del__(self):\n        try:\n            self.close()\n        except:\n            pass\n\n    def __len__(self):\n        return self._raw_idx.size\n\n    def __getitem__(self, idx):\n        raw_idx = self._raw_idx[idx]\n        image = self._cached_images.get(raw_idx, None)\n        if image is None:\n            image = self._load_raw_image(raw_idx)\n            if self._cache:\n                self._cached_images[raw_idx] = image\n        assert isinstance(image, np.ndarray)\n        assert list(image.shape) == self.image_shape\n        assert image.dtype == np.uint8\n        if self._xflip[idx]:\n            assert image.ndim == 3 # CHW\n            image = image[:, :, ::-1]\n        return image.copy(), self.get_label(idx)\n\n    def get_label(self, idx):\n        label = self._get_raw_labels()[self._raw_idx[idx]]\n        if label.dtype == np.int64:\n            onehot = np.zeros(self.label_shape, dtype=np.float32)\n            onehot[label] = 1\n            label = onehot\n        return label.copy()\n\n    def get_details(self, idx):\n        d = dnnlib.EasyDict()\n        d.raw_idx = int(self._raw_idx[idx])\n        d.xflip = (int(self._xflip[idx]) != 0)\n        d.raw_label = self._get_raw_labels()[d.raw_idx].copy()\n        return d\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def image_shape(self):\n        return list(self._raw_shape[1:])\n\n    @property\n    def num_channels(self):\n        assert len(self.image_shape) == 3 # CHW\n        return self.image_shape[0]\n\n    @property\n    def resolution(self):\n        assert len(self.image_shape) == 3 # CHW\n        assert self.image_shape[1] == self.image_shape[2]\n        return self.image_shape[1]\n\n    @property\n    def label_shape(self):\n        if self._label_shape is None:\n            raw_labels = self._get_raw_labels()\n            if raw_labels.dtype == np.int64:\n                self._label_shape = [int(np.max(raw_labels)) + 1]\n            else:\n                self._label_shape = raw_labels.shape[1:]\n        return list(self._label_shape)\n\n    @property\n    def label_dim(self):\n        assert len(self.label_shape) == 1\n        return self.label_shape[0]\n\n    @property\n    def has_labels(self):\n        return any(x != 0 for x in self.label_shape)\n\n    @property\n    def has_onehot_labels(self):\n        return self._get_raw_labels().dtype == np.int64", "\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self,\n        name,                   # Name of the dataset.\n        raw_shape,              # Shape of the raw image data (NCHW).\n        max_size    = None,     # Artificially limit the size of the dataset. None = no limit. Applied before xflip.\n        use_labels  = False,    # Enable conditioning labels? False = label dimension is zero.\n        xflip       = False,    # Artificially double the size of the dataset via x-flips. Applied after max_size.\n        random_seed = 0,        # Random seed to use when applying max_size.\n        cache       = False,    # Cache images in CPU memory?\n    ):\n        self._name = name\n        self._raw_shape = list(raw_shape)\n        self._use_labels = use_labels\n        self._cache = cache\n        self._cached_images = dict() # {raw_idx: np.ndarray, ...}\n        self._raw_labels = None\n        self._label_shape = None\n\n        # Apply max_size.\n        self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)\n        if (max_size is not None) and (self._raw_idx.size > max_size):\n            np.random.RandomState(random_seed % (1 << 31)).shuffle(self._raw_idx)\n            self._raw_idx = np.sort(self._raw_idx[:max_size])\n\n        # Apply xflip.\n        self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)\n        if xflip:\n            self._raw_idx = np.tile(self._raw_idx, 2)\n            self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])\n\n    def _get_raw_labels(self):\n        if self._raw_labels is None:\n            self._raw_labels = self._load_raw_labels() if self._use_labels else None\n            if self._raw_labels is None:\n                self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)\n            assert isinstance(self._raw_labels, np.ndarray)\n            assert self._raw_labels.shape[0] == self._raw_shape[0]\n            assert self._raw_labels.dtype in [np.float32, np.int64]\n            if self._raw_labels.dtype == np.int64:\n                assert self._raw_labels.ndim == 1\n                assert np.all(self._raw_labels >= 0)\n        return self._raw_labels\n\n    def close(self): # to be overridden by subclass\n        pass\n\n    def _load_raw_image(self, raw_idx): # to be overridden by subclass\n        raise NotImplementedError\n\n    def _load_raw_labels(self): # to be overridden by subclass\n        raise NotImplementedError\n\n    def __getstate__(self):\n        return dict(self.__dict__, _raw_labels=None)\n\n    def __del__(self):\n        try:\n            self.close()\n        except:\n            pass\n\n    def __len__(self):\n        return self._raw_idx.size\n\n    def __getitem__(self, idx):\n        raw_idx = self._raw_idx[idx]\n        image = self._cached_images.get(raw_idx, None)\n        if image is None:\n            image = self._load_raw_image(raw_idx)\n            if self._cache:\n                self._cached_images[raw_idx] = image\n        assert isinstance(image, np.ndarray)\n        assert list(image.shape) == self.image_shape\n        assert image.dtype == np.uint8\n        if self._xflip[idx]:\n            assert image.ndim == 3 # CHW\n            image = image[:, :, ::-1]\n        return image.copy(), self.get_label(idx)\n\n    def get_label(self, idx):\n        label = self._get_raw_labels()[self._raw_idx[idx]]\n        if label.dtype == np.int64:\n            onehot = np.zeros(self.label_shape, dtype=np.float32)\n            onehot[label] = 1\n            label = onehot\n        return label.copy()\n\n    def get_details(self, idx):\n        d = dnnlib.EasyDict()\n        d.raw_idx = int(self._raw_idx[idx])\n        d.xflip = (int(self._xflip[idx]) != 0)\n        d.raw_label = self._get_raw_labels()[d.raw_idx].copy()\n        return d\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def image_shape(self):\n        return list(self._raw_shape[1:])\n\n    @property\n    def num_channels(self):\n        assert len(self.image_shape) == 3 # CHW\n        return self.image_shape[0]\n\n    @property\n    def resolution(self):\n        assert len(self.image_shape) == 3 # CHW\n        assert self.image_shape[1] == self.image_shape[2]\n        return self.image_shape[1]\n\n    @property\n    def label_shape(self):\n        if self._label_shape is None:\n            raw_labels = self._get_raw_labels()\n            if raw_labels.dtype == np.int64:\n                self._label_shape = [int(np.max(raw_labels)) + 1]\n            else:\n                self._label_shape = raw_labels.shape[1:]\n        return list(self._label_shape)\n\n    @property\n    def label_dim(self):\n        assert len(self.label_shape) == 1\n        return self.label_shape[0]\n\n    @property\n    def has_labels(self):\n        return any(x != 0 for x in self.label_shape)\n\n    @property\n    def has_onehot_labels(self):\n        return self._get_raw_labels().dtype == np.int64", "\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\n\nclass ImageFolderDataset(Dataset):\n    def __init__(self,\n        path,                   # Path to directory or zip.\n        resolution      = None, # Ensure specific resolution, None = highest available.\n        use_pyspng      = True, # Use pyspng if available?\n        **super_kwargs,         # Additional arguments for the Dataset base class.\n    ):\n        self._path = path\n        self._use_pyspng = use_pyspng\n        self._zipfile = None\n\n        if os.path.isdir(self._path):\n            self._type = 'dir'\n            self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in os.walk(self._path) for fname in files}\n        elif self._file_ext(self._path) == '.zip':\n            self._type = 'zip'\n            self._all_fnames = set(self._get_zipfile().namelist())\n        else:\n            raise IOError('Path must point to a directory or zip')\n\n        PIL.Image.init()\n        self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)\n        if len(self._image_fnames) == 0:\n            raise IOError('No image files found in the specified path')\n\n        name = os.path.splitext(os.path.basename(self._path))[0]\n        raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)\n        if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):\n            raise IOError('Image files do not match the specified resolution')\n        super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)\n\n    @staticmethod\n    def _file_ext(fname):\n        return os.path.splitext(fname)[1].lower()\n\n    def _get_zipfile(self):\n        assert self._type == 'zip'\n        if self._zipfile is None:\n            self._zipfile = zipfile.ZipFile(self._path)\n        return self._zipfile\n\n    def _open_file(self, fname):\n        if self._type == 'dir':\n            return open(os.path.join(self._path, fname), 'rb')\n        if self._type == 'zip':\n            return self._get_zipfile().open(fname, 'r')\n        return None\n\n    def close(self):\n        try:\n            if self._zipfile is not None:\n                self._zipfile.close()\n        finally:\n            self._zipfile = None\n\n    def __getstate__(self):\n        return dict(super().__getstate__(), _zipfile=None)\n\n    def _load_raw_image(self, raw_idx):\n        fname = self._image_fnames[raw_idx]\n        with self._open_file(fname) as f:\n            if self._use_pyspng and pyspng is not None and self._file_ext(fname) == '.png':\n                image = pyspng.load(f.read())\n            else:\n                image = np.array(PIL.Image.open(f))\n        if image.ndim == 2:\n            image = image[:, :, np.newaxis] # HW => HWC\n        image = image.transpose(2, 0, 1) # HWC => CHW\n        return image\n\n    def _load_raw_labels(self):\n        fname = 'dataset.json'\n        if fname not in self._all_fnames:\n            return None\n        with self._open_file(fname) as f:\n            labels = json.load(f)['labels']\n        if labels is None:\n            return None\n        labels = dict(labels)\n        labels = [labels[fname.replace('\\\\', '/')] for fname in self._image_fnames]\n        labels = np.array(labels)\n        labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n        return labels", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "training/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n# empty\n", ""]}
{"filename": "training/training_loop.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Main training loop.\"\"\"\n\nimport os", "\nimport os\nimport time\nimport copy\nimport json\nimport pickle\nimport psutil\nimport numpy as np\nimport torch\nimport dnnlib", "import torch\nimport dnnlib\nfrom torch_utils import distributed as dist\nfrom torch_utils import training_stats\nfrom torch_utils import misc\n\nfrom diffusers import AutoencoderKL\n\ndef set_requires_grad(model, value):\n    for param in model.parameters():\n        param.requires_grad = value", "def set_requires_grad(model, value):\n    for param in model.parameters():\n        param.requires_grad = value\n\n#----------------------------------------------------------------------------\n\ndef training_loop(\n    run_dir             = '.',      # Output directory.\n    dataset_kwargs      = {},       # Options for training set.\n    data_loader_kwargs  = {},       # Options for torch.utils.data.DataLoader.\n    network_kwargs      = {},       # Options for model and preconditioning.\n    loss_kwargs         = {},       # Options for loss function.\n    optimizer_kwargs    = {},       # Options for optimizer.\n    augment_kwargs      = None,     # Options for augmentation pipeline, None = disable.\n    seed                = 0,        # Global random seed.\n    batch_size          = 512,      # Total batch size for one training iteration.\n    batch_gpu           = None,     # Limit batch size per GPU, None = no limit.\n    total_kimg          = 200000,   # Training duration, measured in thousands of training images.\n    ema_halflife_kimg   = 500,      # Half-life of the exponential moving average (EMA) of model weights.\n    ema_rampup_ratio    = 0.05,     # EMA ramp-up coefficient, None = no rampup.\n    lr_rampup_kimg      = 10000,    # Learning rate ramp-up duration.\n    loss_scaling        = 1,        # Loss scaling factor for reducing FP16 under/overflows.\n    kimg_per_tick       = 50,       # Interval of progress prints.\n    snapshot_ticks      = 50,       # How often to save network snapshots, None = disable.\n    state_dump_ticks    = 500,      # How often to dump training state, None = disable.\n    resume_pkl          = None,     # Start from the given network snapshot, None = random initialization.\n    resume_state_dump   = None,     # Start from the given training state, None = reset training state.\n    resume_kimg         = 0,        # Start from the given training progress.\n    cudnn_benchmark     = True,     # Enable torch.backends.cudnn.benchmark?\n    real_p              = 0.5,\n    train_on_latents    = False,\n    progressive         = False,\n    device              = torch.device('cuda'),\n):\n    # Initialize.\n    start_time = time.time()\n    np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n    torch.manual_seed(np.random.randint(1 << 31))\n    torch.backends.cudnn.benchmark = cudnn_benchmark\n    torch.backends.cudnn.allow_tf32 = False\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n    # Select batch size per GPU.\n    batch_gpu_total = batch_size // dist.get_world_size()\n    if batch_gpu is None or batch_gpu > batch_gpu_total:\n        batch_gpu = batch_gpu_total\n    num_accumulation_rounds = batch_gpu_total // batch_gpu\n    assert batch_size == batch_gpu * num_accumulation_rounds * dist.get_world_size()\n\n    # Load dataset.\n    dist.print0('Loading dataset...')\n    dataset_obj = dnnlib.util.construct_class_by_name(**dataset_kwargs) # subclass of training.dataset.Dataset\n    dataset_sampler = misc.InfiniteSampler(dataset=dataset_obj, rank=dist.get_rank(), num_replicas=dist.get_world_size(), seed=seed)\n    dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, sampler=dataset_sampler, batch_size=batch_gpu, **data_loader_kwargs))\n\n    img_resolution, img_channels = dataset_obj.resolution, dataset_obj.num_channels\n\n    if train_on_latents:\n        # img_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n        img_vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n        img_vae.eval()\n        set_requires_grad(img_vae, False)\n        latent_scale_factor = 0.18215\n        img_resolution, img_channels = dataset_obj.resolution // 8, 4\n    else:\n        img_vae = None\n\n    # Construct network.\n    dist.print0('Constructing network...')\n    net_input_channels = img_channels + 2\n    interface_kwargs = dict(img_resolution=img_resolution,\n                            img_channels=net_input_channels,\n                            out_channels=4 if train_on_latents else dataset_obj.num_channels,\n                            label_dim=dataset_obj.label_dim)\n    net = dnnlib.util.construct_class_by_name(**network_kwargs, **interface_kwargs) # subclass of torch.nn.Module\n    net.train().requires_grad_(True).to(device)\n    if dist.get_rank() == 0:\n        with torch.no_grad():\n            images = torch.zeros([batch_gpu, img_channels, net.img_resolution, net.img_resolution], device=device)\n            sigma = torch.ones([batch_gpu], device=device)\n            x_pos = torch.zeros([batch_gpu, 2, net.img_resolution, net.img_resolution], device=device)\n            labels = torch.zeros([batch_gpu, net.label_dim], device=device)\n            misc.print_module_summary(net, [images, sigma, x_pos, labels], max_nesting=2)\n\n    # Setup optimizer.\n    dist.print0('Setting up optimizer...')\n    loss_fn = dnnlib.util.construct_class_by_name(**loss_kwargs) # training.loss.(VP|VE|EDM)Loss\n    optimizer = dnnlib.util.construct_class_by_name(params=net.parameters(), **optimizer_kwargs) # subclass of torch.optim.Optimizer\n    augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs) if augment_kwargs is not None else None # training.augment.AugmentPipe\n    ddp = torch.nn.parallel.DistributedDataParallel(net, device_ids=[device], broadcast_buffers=False)\n    ema = copy.deepcopy(net).eval().requires_grad_(False)\n\n    # Resume training from previous snapshot.\n    if resume_pkl is not None:\n        dist.print0(f'Loading network weights from \"{resume_pkl}\"...')\n        if dist.get_rank() != 0:\n            torch.distributed.barrier() # rank 0 goes first\n        with dnnlib.util.open_url(resume_pkl, verbose=(dist.get_rank() == 0)) as f:\n            data = pickle.load(f)\n        if dist.get_rank() == 0:\n            torch.distributed.barrier() # other ranks follow\n        misc.copy_params_and_buffers(src_module=data['ema'], dst_module=net, require_all=False)\n        misc.copy_params_and_buffers(src_module=data['ema'], dst_module=ema, require_all=False)\n        del data # conserve memory\n    if resume_state_dump:\n        dist.print0(f'Loading training state from \"{resume_state_dump}\"...')\n        data = torch.load(resume_state_dump, map_location=torch.device('cpu'))\n        misc.copy_params_and_buffers(src_module=data['net'], dst_module=net, require_all=True)\n        optimizer.load_state_dict(data['optimizer_state'])\n        del data # conserve memory\n\n    # Train.\n    dist.print0(f'Training for {total_kimg} kimg...')\n    dist.print0()\n    cur_nimg = resume_kimg * 1000\n    cur_tick = 0\n    tick_start_nimg = cur_nimg\n    tick_start_time = time.time()\n    maintenance_time = tick_start_time - start_time\n    dist.update_progress(cur_nimg // 1000, total_kimg)\n    stats_jsonl = None\n    batch_mul_dict = {512: 1, 256: 2, 128: 4, 64: 16, 32: 32, 16: 64}\n    if train_on_latents:\n        p_list = np.array([(1 - real_p), real_p])\n        patch_list = np.array([img_resolution // 2, img_resolution])\n        batch_mul_avg = np.sum(p_list * np.array([2, 1]))\n    else:\n        p_list = np.array([(1-real_p)*2/5, (1-real_p)*3/5, real_p])\n        patch_list = np.array([img_resolution//4, img_resolution//2, img_resolution])\n        batch_mul_avg = np.sum(np.array(p_list) * np.array([4, 2, 1]))  # 2\n    while True:\n\n        # Accumulate gradients.\n        optimizer.zero_grad(set_to_none=True)\n        for round_idx in range(num_accumulation_rounds):\n            with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n                if progressive:\n                    p_cumsum = p_list.cumsum()\n                    p_cumsum[-1] = 10.\n                    prog_mask = (cur_nimg // 1000 / total_kimg) <= p_cumsum\n                    patch_size = int(patch_list[prog_mask][0])\n                    batch_mul_avg = batch_mul_dict[patch_size] // batch_mul_dict[img_resolution]\n                else:\n                    patch_size = int(np.random.choice(patch_list, p=p_list))\n\n                batch_mul = batch_mul_dict[patch_size] // batch_mul_dict[img_resolution]\n                images, labels = [], []\n                for _ in range(batch_mul):\n                    images_, labels_ = next(dataset_iterator)\n                    images.append(images_), labels.append(labels_)\n                images, labels = torch.cat(images, dim=0), torch.cat(labels, dim=0)\n                del images_, labels_\n                images = images.to(device).to(torch.float32) / 127.5 - 1\n\n                if train_on_latents:\n                    with torch.no_grad():\n                        images = img_vae.encode(images)['latent_dist'].sample()\n                        images = latent_scale_factor * images\n\n                labels = labels.to(device)\n                loss = loss_fn(net=ddp, images=images, patch_size=patch_size, resolution=img_resolution,\n                               labels=labels, augment_pipe=augment_pipe)\n                training_stats.report('Loss/loss', loss)\n                loss.sum().mul(loss_scaling / batch_gpu_total / batch_mul).backward()\n                # loss.mean().mul(loss_scaling / batch_mul).backward()\n\n        # Update weights.\n        for g in optimizer.param_groups:\n            g['lr'] = optimizer_kwargs['lr'] * min(cur_nimg / max(lr_rampup_kimg * 1000, 1e-8), 1)\n        for param in net.parameters():\n            if param.grad is not None:\n                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n        optimizer.step()\n\n        # Update EMA.\n        ema_halflife_nimg = ema_halflife_kimg * 1000\n        if ema_rampup_ratio is not None:\n            ema_halflife_nimg = min(ema_halflife_nimg, cur_nimg * ema_rampup_ratio)\n        ema_beta = 0.5 ** (batch_size * batch_mul_avg / max(ema_halflife_nimg, 1e-8))\n        for p_ema, p_net in zip(ema.parameters(), net.parameters()):\n            p_ema.copy_(p_net.detach().lerp(p_ema, ema_beta))\n\n        # Perform maintenance tasks once per tick.\n        cur_nimg += int(batch_size * batch_mul_avg)\n        done = (cur_nimg >= total_kimg * 1000)\n        if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n            continue\n\n        # Print status line, accumulating the same information in training_stats.\n        tick_end_time = time.time()\n        fields = []\n        fields += [f\"tick {training_stats.report0('Progress/tick', cur_tick):<5d}\"]\n        fields += [f\"kimg {training_stats.report0('Progress/kimg', cur_nimg / 1e3):<9.1f}\"]\n        fields += [f\"loss {loss.mean().item():<9.3f}\"]\n        fields += [f\"time {dnnlib.util.format_time(training_stats.report0('Timing/total_sec', tick_end_time - start_time)):<12s}\"]\n        fields += [f\"sec/tick {training_stats.report0('Timing/sec_per_tick', tick_end_time - tick_start_time):<7.1f}\"]\n        fields += [f\"sec/kimg {training_stats.report0('Timing/sec_per_kimg', (tick_end_time - tick_start_time) / (cur_nimg - tick_start_nimg) * 1e3):<7.2f}\"]\n        fields += [f\"maintenance {training_stats.report0('Timing/maintenance_sec', maintenance_time):<6.1f}\"]\n        fields += [f\"cpumem {training_stats.report0('Resources/cpu_mem_gb', psutil.Process(os.getpid()).memory_info().rss / 2**30):<6.2f}\"]\n        fields += [f\"gpumem {training_stats.report0('Resources/peak_gpu_mem_gb', torch.cuda.max_memory_allocated(device) / 2**30):<6.2f}\"]\n        fields += [f\"reserved {training_stats.report0('Resources/peak_gpu_mem_reserved_gb', torch.cuda.max_memory_reserved(device) / 2**30):<6.2f}\"]\n        torch.cuda.reset_peak_memory_stats()\n        dist.print0(' '.join(fields))\n\n        # Check for abort.\n        if (not done) and dist.should_stop():\n            done = True\n            dist.print0()\n            dist.print0('Aborting...')\n\n        # Save network snapshot.\n        if (snapshot_ticks is not None) and (done or cur_tick % snapshot_ticks == 0):\n            data = dict(ema=ema, loss_fn=loss_fn, augment_pipe=augment_pipe, dataset_kwargs=dict(dataset_kwargs))\n            for key, value in data.items():\n                if isinstance(value, torch.nn.Module):\n                    value = copy.deepcopy(value).eval().requires_grad_(False)\n                    misc.check_ddp_consistency(value)\n                    data[key] = value.cpu()\n                del value # conserve memory\n            if dist.get_rank() == 0:\n                with open(os.path.join(run_dir, f'network-snapshot-{cur_nimg//1000:06d}.pkl'), 'wb') as f:\n                    pickle.dump(data, f)\n            del data # conserve memory\n\n        # Save full dump of the training state.\n        if (state_dump_ticks is not None) and (done or cur_tick % state_dump_ticks == 0) and cur_tick != 0 and dist.get_rank() == 0:\n            torch.save(dict(net=net, optimizer_state=optimizer.state_dict()), os.path.join(run_dir, f'training-state-{cur_nimg//1000:06d}.pt'))\n\n        # Update logs.\n        training_stats.default_collector.update()\n        if dist.get_rank() == 0:\n            if stats_jsonl is None:\n                stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'at')\n            stats_jsonl.write(json.dumps(dict(training_stats.default_collector.as_dict(), timestamp=time.time())) + '\\n')\n            stats_jsonl.flush()\n        dist.update_progress(cur_nimg // 1000, total_kimg)\n\n        # Update state.\n        cur_tick += 1\n        tick_start_nimg = cur_nimg\n        tick_start_time = time.time()\n        maintenance_time = tick_start_time - tick_end_time\n        if done:\n            break\n\n    # Done.\n    dist.print0()\n    dist.print0('Exiting...')", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "training/pos_embedding.py", "chunked_list": ["import torch\nfrom torch_utils import persistence\n\n@persistence.persistent_class\nclass Pos_Embedding(torch.nn.Module):\n    def __init__(self, num_freqs=16, input_dim=2, log_sampling=True):\n        super().__init__()\n        self.num_freqs = num_freqs\n        self.max_freq = num_freqs - 1\n        self.input_dim = input_dim\n        self.log_sampling = log_sampling\n\n        if self.log_sampling:\n            self.freq_bands = 2. ** torch.linspace(0., self.max_freq, steps=self.num_freqs)\n        else:\n            self.freq_bands = torch.linspace(2. ** 0., 2. ** self.max_freq, steps=self.num_freqs)\n\n        # embed_fns = []\n        # out_dim = 0\n        # d = input_dim\n        # for freq in freq_bands:\n        #     for p_fn in self.periodic_fns:\n        #         embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n        #         out_dim += d\n        #\n        # self.embed_fns = embed_fns\n        self.out_dim = int(input_dim * self.num_freqs * 2) # 2 is for [sin, cos] function list\n\n    def forward(self, x):\n        # concatenate in the channel dim\n        # assert x.shape[0] == self.input_dim\n        output = []\n        for freq in self.freq_bands:\n            for p_fn in [torch.sin, torch.cos]:\n                output.append(p_fn(x * freq))\n\n        return torch.cat(output, 1)"]}
{"filename": "training/augment.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Augmentation pipeline used in the paper\n\"Elucidating the Design Space of Diffusion-Based Generative Models\".\nBuilt around the same concepts that were originally proposed in the paper", "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\nBuilt around the same concepts that were originally proposed in the paper\n\"Training Generative Adversarial Networks with Limited Data\".\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_utils import persistence\nfrom torch_utils import misc\n\n#----------------------------------------------------------------------------", "\n#----------------------------------------------------------------------------\n# Coefficients of various wavelet decomposition low-pass filters.\n\nwavelets = {\n    'haar': [0.7071067811865476, 0.7071067811865476],\n    'db1':  [0.7071067811865476, 0.7071067811865476],\n    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],", "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],", "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n}\n\n#----------------------------------------------------------------------------\n# Helpers for constructing transformation matrices.\n\ndef matrix(*rows, device=None):\n    assert all(len(row) == len(rows[0]) for row in rows)\n    elems = [x for row in rows for x in row]\n    ref = [x for x in elems if isinstance(x, torch.Tensor)]\n    if len(ref) == 0:\n        return misc.constant(np.asarray(rows), device=device)\n    assert device is None or device == ref[0].device\n    elems = [x if isinstance(x, torch.Tensor) else misc.constant(x, shape=ref[0].shape, device=ref[0].device) for x in elems]\n    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))", "# Helpers for constructing transformation matrices.\n\ndef matrix(*rows, device=None):\n    assert all(len(row) == len(rows[0]) for row in rows)\n    elems = [x for row in rows for x in row]\n    ref = [x for x in elems if isinstance(x, torch.Tensor)]\n    if len(ref) == 0:\n        return misc.constant(np.asarray(rows), device=device)\n    assert device is None or device == ref[0].device\n    elems = [x if isinstance(x, torch.Tensor) else misc.constant(x, shape=ref[0].shape, device=ref[0].device) for x in elems]\n    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))", "\ndef translate2d(tx, ty, **kwargs):\n    return matrix(\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1],\n        **kwargs)\n\ndef translate3d(tx, ty, tz, **kwargs):\n    return matrix(\n        [1, 0, 0, tx],\n        [0, 1, 0, ty],\n        [0, 0, 1, tz],\n        [0, 0, 0, 1],\n        **kwargs)", "def translate3d(tx, ty, tz, **kwargs):\n    return matrix(\n        [1, 0, 0, tx],\n        [0, 1, 0, ty],\n        [0, 0, 1, tz],\n        [0, 0, 0, 1],\n        **kwargs)\n\ndef scale2d(sx, sy, **kwargs):\n    return matrix(\n        [sx, 0,  0],\n        [0,  sy, 0],\n        [0,  0,  1],\n        **kwargs)", "def scale2d(sx, sy, **kwargs):\n    return matrix(\n        [sx, 0,  0],\n        [0,  sy, 0],\n        [0,  0,  1],\n        **kwargs)\n\ndef scale3d(sx, sy, sz, **kwargs):\n    return matrix(\n        [sx, 0,  0,  0],\n        [0,  sy, 0,  0],\n        [0,  0,  sz, 0],\n        [0,  0,  0,  1],\n        **kwargs)", "\ndef rotate2d(theta, **kwargs):\n    return matrix(\n        [torch.cos(theta), torch.sin(-theta), 0],\n        [torch.sin(theta), torch.cos(theta),  0],\n        [0,                0,                 1],\n        **kwargs)\n\ndef rotate3d(v, theta, **kwargs):\n    vx = v[..., 0]; vy = v[..., 1]; vz = v[..., 2]\n    s = torch.sin(theta); c = torch.cos(theta); cc = 1 - c\n    return matrix(\n        [vx*vx*cc+c,    vx*vy*cc-vz*s, vx*vz*cc+vy*s, 0],\n        [vy*vx*cc+vz*s, vy*vy*cc+c,    vy*vz*cc-vx*s, 0],\n        [vz*vx*cc-vy*s, vz*vy*cc+vx*s, vz*vz*cc+c,    0],\n        [0,             0,             0,             1],\n        **kwargs)", "def rotate3d(v, theta, **kwargs):\n    vx = v[..., 0]; vy = v[..., 1]; vz = v[..., 2]\n    s = torch.sin(theta); c = torch.cos(theta); cc = 1 - c\n    return matrix(\n        [vx*vx*cc+c,    vx*vy*cc-vz*s, vx*vz*cc+vy*s, 0],\n        [vy*vx*cc+vz*s, vy*vy*cc+c,    vy*vz*cc-vx*s, 0],\n        [vz*vx*cc-vy*s, vz*vy*cc+vx*s, vz*vz*cc+c,    0],\n        [0,             0,             0,             1],\n        **kwargs)\n\ndef translate2d_inv(tx, ty, **kwargs):\n    return translate2d(-tx, -ty, **kwargs)", "\ndef translate2d_inv(tx, ty, **kwargs):\n    return translate2d(-tx, -ty, **kwargs)\n\ndef scale2d_inv(sx, sy, **kwargs):\n    return scale2d(1 / sx, 1 / sy, **kwargs)\n\ndef rotate2d_inv(theta, **kwargs):\n    return rotate2d(-theta, **kwargs)\n", "\n#----------------------------------------------------------------------------\n# Augmentation pipeline main class.\n# All augmentations are disabled by default; individual augmentations can\n# be enabled by setting their probability multipliers to 1.\n\n@persistence.persistent_class\nclass AugmentPipe:\n    def __init__(self, p=1,\n        xflip=0, yflip=0, rotate_int=0, translate_int=0, translate_int_max=0.125,\n        scale=0, rotate_frac=0, aniso=0, translate_frac=0, scale_std=0.2, rotate_frac_max=1, aniso_std=0.2, aniso_rotate_prob=0.5, translate_frac_std=0.125,\n        brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0, brightness_std=0.2, contrast_std=0.5, hue_max=1, saturation_std=1,\n    ):\n        super().__init__()\n        self.p                  = float(p)                  # Overall multiplier for augmentation probability.\n\n        # Pixel blitting.\n        self.xflip              = float(xflip)              # Probability multiplier for x-flip.\n        self.yflip              = float(yflip)              # Probability multiplier for y-flip.\n        self.rotate_int         = float(rotate_int)         # Probability multiplier for integer rotation.\n        self.translate_int      = float(translate_int)      # Probability multiplier for integer translation.\n        self.translate_int_max  = float(translate_int_max)  # Range of integer translation, relative to image dimensions.\n\n        # Geometric transformations.\n        self.scale              = float(scale)              # Probability multiplier for isotropic scaling.\n        self.rotate_frac        = float(rotate_frac)        # Probability multiplier for fractional rotation.\n        self.aniso              = float(aniso)              # Probability multiplier for anisotropic scaling.\n        self.translate_frac     = float(translate_frac)     # Probability multiplier for fractional translation.\n        self.scale_std          = float(scale_std)          # Log2 standard deviation of isotropic scaling.\n        self.rotate_frac_max    = float(rotate_frac_max)    # Range of fractional rotation, 1 = full circle.\n        self.aniso_std          = float(aniso_std)          # Log2 standard deviation of anisotropic scaling.\n        self.aniso_rotate_prob  = float(aniso_rotate_prob)  # Probability of doing anisotropic scaling w.r.t. rotated coordinate frame.\n        self.translate_frac_std = float(translate_frac_std) # Standard deviation of frational translation, relative to image dimensions.\n\n        # Color transformations.\n        self.brightness         = float(brightness)         # Probability multiplier for brightness.\n        self.contrast           = float(contrast)           # Probability multiplier for contrast.\n        self.lumaflip           = float(lumaflip)           # Probability multiplier for luma flip.\n        self.hue                = float(hue)                # Probability multiplier for hue rotation.\n        self.saturation         = float(saturation)         # Probability multiplier for saturation.\n        self.brightness_std     = float(brightness_std)     # Standard deviation of brightness.\n        self.contrast_std       = float(contrast_std)       # Log2 standard deviation of contrast.\n        self.hue_max            = float(hue_max)            # Range of hue rotation, 1 = full circle.\n        self.saturation_std     = float(saturation_std)     # Log2 standard deviation of saturation.\n\n    def __call__(self, images):\n        N, C, H, W = images.shape\n        device = images.device\n        labels = [torch.zeros([images.shape[0], 0], device=device)]\n\n        # ---------------\n        # Pixel blitting.\n        # ---------------\n\n        if self.xflip > 0:\n            w = torch.randint(2, [N, 1, 1, 1], device=device)\n            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.xflip * self.p, w, torch.zeros_like(w))\n            images = torch.where(w == 1, images.flip(3), images)\n            labels += [w]\n\n        if self.yflip > 0:\n            w = torch.randint(2, [N, 1, 1, 1], device=device)\n            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.yflip * self.p, w, torch.zeros_like(w))\n            images = torch.where(w == 1, images.flip(2), images)\n            labels += [w]\n\n        if self.rotate_int > 0:\n            w = torch.randint(4, [N, 1, 1, 1], device=device)\n            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.rotate_int * self.p, w, torch.zeros_like(w))\n            images = torch.where((w == 1) | (w == 2), images.flip(3), images)\n            images = torch.where((w == 2) | (w == 3), images.flip(2), images)\n            images = torch.where((w == 1) | (w == 3), images.transpose(2, 3), images)\n            labels += [(w == 1) | (w == 2), (w == 2) | (w == 3)]\n\n        if self.translate_int > 0:\n            w = torch.rand([2, N, 1, 1, 1], device=device) * 2 - 1\n            w = torch.where(torch.rand([1, N, 1, 1, 1], device=device) < self.translate_int * self.p, w, torch.zeros_like(w))\n            tx = w[0].mul(W * self.translate_int_max).round().to(torch.int64)\n            ty = w[1].mul(H * self.translate_int_max).round().to(torch.int64)\n            b, c, y, x = torch.meshgrid(*(torch.arange(x, device=device) for x in images.shape), indexing='ij')\n            x = W - 1 - (W - 1 - (x - tx) % (W * 2 - 2)).abs()\n            y = H - 1 - (H - 1 - (y + ty) % (H * 2 - 2)).abs()\n            images = images.flatten()[(((b * C) + c) * H + y) * W + x]\n            labels += [tx.div(W * self.translate_int_max), ty.div(H * self.translate_int_max)]\n\n        # ------------------------------------------------\n        # Select parameters for geometric transformations.\n        # ------------------------------------------------\n\n        I_3 = torch.eye(3, device=device)\n        G_inv = I_3\n\n        if self.scale > 0:\n            w = torch.randn([N], device=device)\n            w = torch.where(torch.rand([N], device=device) < self.scale * self.p, w, torch.zeros_like(w))\n            s = w.mul(self.scale_std).exp2()\n            G_inv = G_inv @ scale2d_inv(s, s)\n            labels += [w]\n\n        if self.rotate_frac > 0:\n            w = (torch.rand([N], device=device) * 2 - 1) * (np.pi * self.rotate_frac_max)\n            w = torch.where(torch.rand([N], device=device) < self.rotate_frac * self.p, w, torch.zeros_like(w))\n            G_inv = G_inv @ rotate2d_inv(-w)\n            labels += [w.cos() - 1, w.sin()]\n\n        if self.aniso > 0:\n            w = torch.randn([N], device=device)\n            r = (torch.rand([N], device=device) * 2 - 1) * np.pi\n            w = torch.where(torch.rand([N], device=device) < self.aniso * self.p, w, torch.zeros_like(w))\n            r = torch.where(torch.rand([N], device=device) < self.aniso_rotate_prob, r, torch.zeros_like(r))\n            s = w.mul(self.aniso_std).exp2()\n            G_inv = G_inv @ rotate2d_inv(r) @ scale2d_inv(s, 1 / s) @ rotate2d_inv(-r)\n            labels += [w * r.cos(), w * r.sin()]\n\n        if self.translate_frac > 0:\n            w = torch.randn([2, N], device=device)\n            w = torch.where(torch.rand([1, N], device=device) < self.translate_frac * self.p, w, torch.zeros_like(w))\n            G_inv = G_inv @ translate2d_inv(w[0].mul(W * self.translate_frac_std), w[1].mul(H * self.translate_frac_std))\n            labels += [w[0], w[1]]\n\n        # ----------------------------------\n        # Execute geometric transformations.\n        # ----------------------------------\n\n        if G_inv is not I_3:\n            cx = (W - 1) / 2\n            cy = (H - 1) / 2\n            cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device) # [idx, xyz]\n            cp = G_inv @ cp.t() # [batch, xyz, idx]\n            Hz = np.asarray(wavelets['sym6'], dtype=np.float32)\n            Hz_pad = len(Hz) // 4\n            margin = cp[:, :2, :].permute(1, 0, 2).flatten(1) # [xy, batch * idx]\n            margin = torch.cat([-margin, margin]).max(dim=1).values # [x0, y0, x1, y1]\n            margin = margin + misc.constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)\n            margin = margin.max(misc.constant([0, 0] * 2, device=device))\n            margin = margin.min(misc.constant([W - 1, H - 1] * 2, device=device))\n            mx0, my0, mx1, my1 = margin.ceil().to(torch.int32)\n\n            # Pad image and adjust origin.\n            images = torch.nn.functional.pad(input=images, pad=[mx0,mx1,my0,my1], mode='reflect')\n            G_inv = translate2d((mx0 - mx1) / 2, (my0 - my1) / 2) @ G_inv\n\n            # Upsample.\n            conv_weight = misc.constant(Hz[None, None, ::-1], dtype=images.dtype, device=images.device).tile([images.shape[1], 1, 1])\n            conv_pad = (len(Hz) + 1) // 2\n            images = torch.stack([images, torch.zeros_like(images)], dim=4).reshape(N, C, images.shape[2], -1)[:, :, :, :-1]\n            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(2), groups=images.shape[1], padding=[0,conv_pad])\n            images = torch.stack([images, torch.zeros_like(images)], dim=3).reshape(N, C, -1, images.shape[3])[:, :, :-1, :]\n            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(3), groups=images.shape[1], padding=[conv_pad,0])\n            G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)\n            G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)\n\n            # Execute transformation.\n            shape = [N, C, (H + Hz_pad * 2) * 2, (W + Hz_pad * 2) * 2]\n            G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)\n            grid = torch.nn.functional.affine_grid(theta=G_inv[:,:2,:], size=shape, align_corners=False)\n            images = torch.nn.functional.grid_sample(images, grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n\n            # Downsample and crop.\n            conv_weight = misc.constant(Hz[None, None, :], dtype=images.dtype, device=images.device).tile([images.shape[1], 1, 1])\n            conv_pad = (len(Hz) - 1) // 2\n            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(2), groups=images.shape[1], stride=[1,2], padding=[0,conv_pad])[:, :, :, Hz_pad : -Hz_pad]\n            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(3), groups=images.shape[1], stride=[2,1], padding=[conv_pad,0])[:, :, Hz_pad : -Hz_pad, :]\n\n        # --------------------------------------------\n        # Select parameters for color transformations.\n        # --------------------------------------------\n\n        I_4 = torch.eye(4, device=device)\n        M = I_4\n        luma_axis = misc.constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device)\n\n        if self.brightness > 0:\n            w = torch.randn([N], device=device)\n            w = torch.where(torch.rand([N], device=device) < self.brightness * self.p, w, torch.zeros_like(w))\n            b = w * self.brightness_std\n            M = translate3d(b, b, b) @ M\n            labels += [w]\n\n        if self.contrast > 0:\n            w = torch.randn([N], device=device)\n            w = torch.where(torch.rand([N], device=device) < self.contrast * self.p, w, torch.zeros_like(w))\n            c = w.mul(self.contrast_std).exp2()\n            M = scale3d(c, c, c) @ M\n            labels += [w]\n\n        if self.lumaflip > 0:\n            w = torch.randint(2, [N, 1, 1], device=device)\n            w = torch.where(torch.rand([N, 1, 1], device=device) < self.lumaflip * self.p, w, torch.zeros_like(w))\n            M = (I_4 - 2 * luma_axis.ger(luma_axis) * w) @ M\n            labels += [w]\n\n        if self.hue > 0:\n            w = (torch.rand([N], device=device) * 2 - 1) * (np.pi * self.hue_max)\n            w = torch.where(torch.rand([N], device=device) < self.hue * self.p, w, torch.zeros_like(w))\n            M = rotate3d(luma_axis, w) @ M\n            labels += [w.cos() - 1, w.sin()]\n\n        if self.saturation > 0:\n            w = torch.randn([N, 1, 1], device=device)\n            w = torch.where(torch.rand([N, 1, 1], device=device) < self.saturation * self.p, w, torch.zeros_like(w))\n            M = (luma_axis.ger(luma_axis) + (I_4 - luma_axis.ger(luma_axis)) * w.mul(self.saturation_std).exp2()) @ M\n            labels += [w]\n\n        # ------------------------------\n        # Execute color transformations.\n        # ------------------------------\n\n        if M is not I_4:\n            images = images.reshape([N, C, H * W])\n            if C == 3:\n                images = M[:, :3, :3] @ images + M[:, :3, 3:]\n            elif C == 1:\n                M = M[:, :3, :].mean(dim=1, keepdims=True)\n                images = images * M[:, :, :3].sum(dim=2, keepdims=True) + M[:, :, 3:]\n            else:\n                raise ValueError('Image must be RGB (3 channels) or L (1 channel)')\n            images = images.reshape([N, C, H, W])\n\n        labels = torch.cat([x.to(torch.float32).reshape(N, -1) for x in labels], dim=1)\n        return images, labels", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/distributed.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\nimport os\nimport torch\nfrom . import training_stats", "import torch\nfrom . import training_stats\n\n#----------------------------------------------------------------------------\n\ndef init():\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n    if 'MASTER_PORT' not in os.environ:\n        os.environ['MASTER_PORT'] = '29500'\n    if 'RANK' not in os.environ:\n        os.environ['RANK'] = '0'\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = '0'\n    if 'WORLD_SIZE' not in os.environ:\n        os.environ['WORLD_SIZE'] = '1'\n\n    backend = 'gloo' if os.name == 'nt' else 'nccl'\n    torch.distributed.init_process_group(backend=backend, init_method='env://')\n    torch.cuda.set_device(int(os.environ.get('LOCAL_RANK', '0')))\n\n    sync_device = torch.device('cuda') if get_world_size() > 1 else None\n    training_stats.init_multiprocessing(rank=get_rank(), sync_device=sync_device)", "\n#----------------------------------------------------------------------------\n\ndef get_rank():\n    return torch.distributed.get_rank() if torch.distributed.is_initialized() else 0\n\n#----------------------------------------------------------------------------\n\ndef get_world_size():\n    return torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1", "def get_world_size():\n    return torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n\n#----------------------------------------------------------------------------\n\ndef should_stop():\n    return False\n\n#----------------------------------------------------------------------------\n\ndef update_progress(cur, total):\n    _ = cur, total", "#----------------------------------------------------------------------------\n\ndef update_progress(cur, total):\n    _ = cur, total\n\n#----------------------------------------------------------------------------\n\ndef print0(*args, **kwargs):\n    if get_rank() == 0:\n        print(*args, **kwargs)", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/training_stats.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Facilities for reporting and collecting training statistics across\nmultiple processes and devices. The interface is designed to minimize\nsynchronization overhead as well as the amount of boilerplate in user", "multiple processes and devices. The interface is designed to minimize\nsynchronization overhead as well as the amount of boilerplate in user\ncode.\"\"\"\n\nimport re\nimport numpy as np\nimport torch\nimport dnnlib\n\nfrom . import misc", "\nfrom . import misc\n\n#----------------------------------------------------------------------------\n\n_num_moments    = 3             # [num_scalars, sum_of_scalars, sum_of_squares]\n_reduce_dtype   = torch.float32 # Data type to use for initial per-tensor reduction.\n_counter_dtype  = torch.float64 # Data type to use for the internal counters.\n_rank           = 0             # Rank of the current process.\n_sync_device    = None          # Device to use for multiprocess communication. None = single-process.", "_rank           = 0             # Rank of the current process.\n_sync_device    = None          # Device to use for multiprocess communication. None = single-process.\n_sync_called    = False         # Has _sync() been called yet?\n_counters       = dict()        # Running counters on each device, updated by report(): name => device => torch.Tensor\n_cumulative     = dict()        # Cumulative counters on the CPU, updated by _sync(): name => torch.Tensor\n\n#----------------------------------------------------------------------------\n\ndef init_multiprocessing(rank, sync_device):\n    r\"\"\"Initializes `torch_utils.training_stats` for collecting statistics\n    across multiple processes.\n\n    This function must be called after\n    `torch.distributed.init_process_group()` and before `Collector.update()`.\n    The call is not necessary if multi-process collection is not needed.\n\n    Args:\n        rank:           Rank of the current process.\n        sync_device:    PyTorch device to use for inter-process\n                        communication, or None to disable multi-process\n                        collection. Typically `torch.device('cuda', rank)`.\n    \"\"\"\n    global _rank, _sync_device\n    assert not _sync_called\n    _rank = rank\n    _sync_device = sync_device", "def init_multiprocessing(rank, sync_device):\n    r\"\"\"Initializes `torch_utils.training_stats` for collecting statistics\n    across multiple processes.\n\n    This function must be called after\n    `torch.distributed.init_process_group()` and before `Collector.update()`.\n    The call is not necessary if multi-process collection is not needed.\n\n    Args:\n        rank:           Rank of the current process.\n        sync_device:    PyTorch device to use for inter-process\n                        communication, or None to disable multi-process\n                        collection. Typically `torch.device('cuda', rank)`.\n    \"\"\"\n    global _rank, _sync_device\n    assert not _sync_called\n    _rank = rank\n    _sync_device = sync_device", "\n#----------------------------------------------------------------------------\n\n@misc.profiled_function\ndef report(name, value):\n    r\"\"\"Broadcasts the given set of scalars to all interested instances of\n    `Collector`, across device and process boundaries.\n\n    This function is expected to be extremely cheap and can be safely\n    called from anywhere in the training loop, loss function, or inside a\n    `torch.nn.Module`.\n\n    Warning: The current implementation expects the set of unique names to\n    be consistent across processes. Please make sure that `report()` is\n    called at least once for each unique name by each process, and in the\n    same order. If a given process has no scalars to broadcast, it can do\n    `report(name, [])` (empty list).\n\n    Args:\n        name:   Arbitrary string specifying the name of the statistic.\n                Averages are accumulated separately for each unique name.\n        value:  Arbitrary set of scalars. Can be a list, tuple,\n                NumPy array, PyTorch tensor, or Python scalar.\n\n    Returns:\n        The same `value` that was passed in.\n    \"\"\"\n    if name not in _counters:\n        _counters[name] = dict()\n\n    elems = torch.as_tensor(value)\n    if elems.numel() == 0:\n        return value\n\n    elems = elems.detach().flatten().to(_reduce_dtype)\n    moments = torch.stack([\n        torch.ones_like(elems).sum(),\n        elems.sum(),\n        elems.square().sum(),\n    ])\n    assert moments.ndim == 1 and moments.shape[0] == _num_moments\n    moments = moments.to(_counter_dtype)\n\n    device = moments.device\n    if device not in _counters[name]:\n        _counters[name][device] = torch.zeros_like(moments)\n    _counters[name][device].add_(moments)\n    return value", "\n#----------------------------------------------------------------------------\n\ndef report0(name, value):\n    r\"\"\"Broadcasts the given set of scalars by the first process (`rank = 0`),\n    but ignores any scalars provided by the other processes.\n    See `report()` for further details.\n    \"\"\"\n    report(name, value if _rank == 0 else [])\n    return value", "\n#----------------------------------------------------------------------------\n\nclass Collector:\n    r\"\"\"Collects the scalars broadcasted by `report()` and `report0()` and\n    computes their long-term averages (mean and standard deviation) over\n    user-defined periods of time.\n\n    The averages are first collected into internal counters that are not\n    directly visible to the user. They are then copied to the user-visible\n    state as a result of calling `update()` and can then be queried using\n    `mean()`, `std()`, `as_dict()`, etc. Calling `update()` also resets the\n    internal counters for the next round, so that the user-visible state\n    effectively reflects averages collected between the last two calls to\n    `update()`.\n\n    Args:\n        regex:          Regular expression defining which statistics to\n                        collect. The default is to collect everything.\n        keep_previous:  Whether to retain the previous averages if no\n                        scalars were collected on a given round\n                        (default: True).\n    \"\"\"\n    def __init__(self, regex='.*', keep_previous=True):\n        self._regex = re.compile(regex)\n        self._keep_previous = keep_previous\n        self._cumulative = dict()\n        self._moments = dict()\n        self.update()\n        self._moments.clear()\n\n    def names(self):\n        r\"\"\"Returns the names of all statistics broadcasted so far that\n        match the regular expression specified at construction time.\n        \"\"\"\n        return [name for name in _counters if self._regex.fullmatch(name)]\n\n    def update(self):\n        r\"\"\"Copies current values of the internal counters to the\n        user-visible state and resets them for the next round.\n\n        If `keep_previous=True` was specified at construction time, the\n        operation is skipped for statistics that have received no scalars\n        since the last update, retaining their previous averages.\n\n        This method performs a number of GPU-to-CPU transfers and one\n        `torch.distributed.all_reduce()`. It is intended to be called\n        periodically in the main training loop, typically once every\n        N training steps.\n        \"\"\"\n        if not self._keep_previous:\n            self._moments.clear()\n        for name, cumulative in _sync(self.names()):\n            if name not in self._cumulative:\n                self._cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n            delta = cumulative - self._cumulative[name]\n            self._cumulative[name].copy_(cumulative)\n            if float(delta[0]) != 0:\n                self._moments[name] = delta\n\n    def _get_delta(self, name):\n        r\"\"\"Returns the raw moments that were accumulated for the given\n        statistic between the last two calls to `update()`, or zero if\n        no scalars were collected.\n        \"\"\"\n        assert self._regex.fullmatch(name)\n        if name not in self._moments:\n            self._moments[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n        return self._moments[name]\n\n    def num(self, name):\n        r\"\"\"Returns the number of scalars that were accumulated for the given\n        statistic between the last two calls to `update()`, or zero if\n        no scalars were collected.\n        \"\"\"\n        delta = self._get_delta(name)\n        return int(delta[0])\n\n    def mean(self, name):\n        r\"\"\"Returns the mean of the scalars that were accumulated for the\n        given statistic between the last two calls to `update()`, or NaN if\n        no scalars were collected.\n        \"\"\"\n        delta = self._get_delta(name)\n        if int(delta[0]) == 0:\n            return float('nan')\n        return float(delta[1] / delta[0])\n\n    def std(self, name):\n        r\"\"\"Returns the standard deviation of the scalars that were\n        accumulated for the given statistic between the last two calls to\n        `update()`, or NaN if no scalars were collected.\n        \"\"\"\n        delta = self._get_delta(name)\n        if int(delta[0]) == 0 or not np.isfinite(float(delta[1])):\n            return float('nan')\n        if int(delta[0]) == 1:\n            return float(0)\n        mean = float(delta[1] / delta[0])\n        raw_var = float(delta[2] / delta[0])\n        return np.sqrt(max(raw_var - np.square(mean), 0))\n\n    def as_dict(self):\n        r\"\"\"Returns the averages accumulated between the last two calls to\n        `update()` as an `dnnlib.EasyDict`. The contents are as follows:\n\n            dnnlib.EasyDict(\n                NAME = dnnlib.EasyDict(num=FLOAT, mean=FLOAT, std=FLOAT),\n                ...\n            )\n        \"\"\"\n        stats = dnnlib.EasyDict()\n        for name in self.names():\n            stats[name] = dnnlib.EasyDict(num=self.num(name), mean=self.mean(name), std=self.std(name))\n        return stats\n\n    def __getitem__(self, name):\n        r\"\"\"Convenience getter.\n        `collector[name]` is a synonym for `collector.mean(name)`.\n        \"\"\"\n        return self.mean(name)", "\n#----------------------------------------------------------------------------\n\ndef _sync(names):\n    r\"\"\"Synchronize the global cumulative counters across devices and\n    processes. Called internally by `Collector.update()`.\n    \"\"\"\n    if len(names) == 0:\n        return []\n    global _sync_called\n    _sync_called = True\n\n    # Collect deltas within current rank.\n    deltas = []\n    device = _sync_device if _sync_device is not None else torch.device('cpu')\n    for name in names:\n        delta = torch.zeros([_num_moments], dtype=_counter_dtype, device=device)\n        for counter in _counters[name].values():\n            delta.add_(counter.to(device))\n            counter.copy_(torch.zeros_like(counter))\n        deltas.append(delta)\n    deltas = torch.stack(deltas)\n\n    # Sum deltas across ranks.\n    if _sync_device is not None:\n        torch.distributed.all_reduce(deltas)\n\n    # Update cumulative values.\n    deltas = deltas.cpu()\n    for idx, name in enumerate(names):\n        if name not in _cumulative:\n            _cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n        _cumulative[name].add_(deltas[idx])\n\n    # Return name-value pairs.\n    return [(name, _cumulative[name]) for name in names]", "\n#----------------------------------------------------------------------------\n# Convenience.\n\ndefault_collector = Collector()\n\n#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/persistence.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Facilities for pickling Python code alongside other data.\n\nThe pickled code is automatically imported into a separate Python module", "\nThe pickled code is automatically imported into a separate Python module\nduring unpickling. This way, any previously exported pickles will remain\nusable even if the original code is no longer available, or if the current\nversion of the code is not consistent with what was originally pickled.\"\"\"\n\nimport sys\nimport pickle\nimport io\nimport inspect", "import io\nimport inspect\nimport copy\nimport uuid\nimport types\nimport dnnlib\n\n#----------------------------------------------------------------------------\n\n_version            = 6         # internal version number", "\n_version            = 6         # internal version number\n_decorators         = set()     # {decorator_class, ...}\n_import_hooks       = []        # [hook_function, ...]\n_module_to_src_dict = dict()    # {module: src, ...}\n_src_to_module_dict = dict()    # {src: module, ...}\n\n#----------------------------------------------------------------------------\n\ndef persistent_class(orig_class):\n    r\"\"\"Class decorator that extends a given class to save its source code\n    when pickled.\n\n    Example:\n\n        from torch_utils import persistence\n\n        @persistence.persistent_class\n        class MyNetwork(torch.nn.Module):\n            def __init__(self, num_inputs, num_outputs):\n                super().__init__()\n                self.fc = MyLayer(num_inputs, num_outputs)\n                ...\n\n        @persistence.persistent_class\n        class MyLayer(torch.nn.Module):\n            ...\n\n    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n    source code alongside other internal state (e.g., parameters, buffers,\n    and submodules). This way, any previously exported pickle will remain\n    usable even if the class definitions have been modified or are no\n    longer available.\n\n    The decorator saves the source code of the entire Python module\n    containing the decorated class. It does *not* save the source code of\n    any imported modules. Thus, the imported modules must be available\n    during unpickling, also including `torch_utils.persistence` itself.\n\n    It is ok to call functions defined in the same module from the\n    decorated class. However, if the decorated class depends on other\n    classes defined in the same module, they must be decorated as well.\n    This is illustrated in the above example in the case of `MyLayer`.\n\n    It is also possible to employ the decorator just-in-time before\n    calling the constructor. For example:\n\n        cls = MyLayer\n        if want_to_make_it_persistent:\n            cls = persistence.persistent_class(cls)\n        layer = cls(num_inputs, num_outputs)\n\n    As an additional feature, the decorator also keeps track of the\n    arguments that were used to construct each instance of the decorated\n    class. The arguments can be queried via `obj.init_args` and\n    `obj.init_kwargs`, and they are automatically pickled alongside other\n    object state. This feature can be disabled on a per-instance basis\n    by setting `self._record_init_args = False` in the constructor.\n\n    A typical use case is to first unpickle a previous instance of a\n    persistent class, and then upgrade it to use the latest version of\n    the source code:\n\n        with open('old_pickle.pkl', 'rb') as f:\n            old_net = pickle.load(f)\n        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n    \"\"\"\n    assert isinstance(orig_class, type)\n    if is_persistent(orig_class):\n        return orig_class\n\n    assert orig_class.__module__ in sys.modules\n    orig_module = sys.modules[orig_class.__module__]\n    orig_module_src = _module_to_src(orig_module)\n\n    class Decorator(orig_class):\n        _orig_module_src = orig_module_src\n        _orig_class_name = orig_class.__name__\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            record_init_args = getattr(self, '_record_init_args', True)\n            self._init_args = copy.deepcopy(args) if record_init_args else None\n            self._init_kwargs = copy.deepcopy(kwargs) if record_init_args else None\n            assert orig_class.__name__ in orig_module.__dict__\n            _check_pickleable(self.__reduce__())\n\n        @property\n        def init_args(self):\n            assert self._init_args is not None\n            return copy.deepcopy(self._init_args)\n\n        @property\n        def init_kwargs(self):\n            assert self._init_kwargs is not None\n            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n\n        def __reduce__(self):\n            fields = list(super().__reduce__())\n            fields += [None] * max(3 - len(fields), 0)\n            if fields[0] is not _reconstruct_persistent_obj:\n                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n                fields[0] = _reconstruct_persistent_obj # reconstruct func\n                fields[1] = (meta,) # reconstruct args\n                fields[2] = None # state dict\n            return tuple(fields)\n\n    Decorator.__name__ = orig_class.__name__\n    Decorator.__module__ = orig_class.__module__\n    _decorators.add(Decorator)\n    return Decorator", "\ndef persistent_class(orig_class):\n    r\"\"\"Class decorator that extends a given class to save its source code\n    when pickled.\n\n    Example:\n\n        from torch_utils import persistence\n\n        @persistence.persistent_class\n        class MyNetwork(torch.nn.Module):\n            def __init__(self, num_inputs, num_outputs):\n                super().__init__()\n                self.fc = MyLayer(num_inputs, num_outputs)\n                ...\n\n        @persistence.persistent_class\n        class MyLayer(torch.nn.Module):\n            ...\n\n    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n    source code alongside other internal state (e.g., parameters, buffers,\n    and submodules). This way, any previously exported pickle will remain\n    usable even if the class definitions have been modified or are no\n    longer available.\n\n    The decorator saves the source code of the entire Python module\n    containing the decorated class. It does *not* save the source code of\n    any imported modules. Thus, the imported modules must be available\n    during unpickling, also including `torch_utils.persistence` itself.\n\n    It is ok to call functions defined in the same module from the\n    decorated class. However, if the decorated class depends on other\n    classes defined in the same module, they must be decorated as well.\n    This is illustrated in the above example in the case of `MyLayer`.\n\n    It is also possible to employ the decorator just-in-time before\n    calling the constructor. For example:\n\n        cls = MyLayer\n        if want_to_make_it_persistent:\n            cls = persistence.persistent_class(cls)\n        layer = cls(num_inputs, num_outputs)\n\n    As an additional feature, the decorator also keeps track of the\n    arguments that were used to construct each instance of the decorated\n    class. The arguments can be queried via `obj.init_args` and\n    `obj.init_kwargs`, and they are automatically pickled alongside other\n    object state. This feature can be disabled on a per-instance basis\n    by setting `self._record_init_args = False` in the constructor.\n\n    A typical use case is to first unpickle a previous instance of a\n    persistent class, and then upgrade it to use the latest version of\n    the source code:\n\n        with open('old_pickle.pkl', 'rb') as f:\n            old_net = pickle.load(f)\n        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n    \"\"\"\n    assert isinstance(orig_class, type)\n    if is_persistent(orig_class):\n        return orig_class\n\n    assert orig_class.__module__ in sys.modules\n    orig_module = sys.modules[orig_class.__module__]\n    orig_module_src = _module_to_src(orig_module)\n\n    class Decorator(orig_class):\n        _orig_module_src = orig_module_src\n        _orig_class_name = orig_class.__name__\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            record_init_args = getattr(self, '_record_init_args', True)\n            self._init_args = copy.deepcopy(args) if record_init_args else None\n            self._init_kwargs = copy.deepcopy(kwargs) if record_init_args else None\n            assert orig_class.__name__ in orig_module.__dict__\n            _check_pickleable(self.__reduce__())\n\n        @property\n        def init_args(self):\n            assert self._init_args is not None\n            return copy.deepcopy(self._init_args)\n\n        @property\n        def init_kwargs(self):\n            assert self._init_kwargs is not None\n            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n\n        def __reduce__(self):\n            fields = list(super().__reduce__())\n            fields += [None] * max(3 - len(fields), 0)\n            if fields[0] is not _reconstruct_persistent_obj:\n                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n                fields[0] = _reconstruct_persistent_obj # reconstruct func\n                fields[1] = (meta,) # reconstruct args\n                fields[2] = None # state dict\n            return tuple(fields)\n\n    Decorator.__name__ = orig_class.__name__\n    Decorator.__module__ = orig_class.__module__\n    _decorators.add(Decorator)\n    return Decorator", "\n#----------------------------------------------------------------------------\n\ndef is_persistent(obj):\n    r\"\"\"Test whether the given object or class is persistent, i.e.,\n    whether it will save its source code when pickled.\n    \"\"\"\n    try:\n        if obj in _decorators:\n            return True\n    except TypeError:\n        pass\n    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck", "\n#----------------------------------------------------------------------------\n\ndef import_hook(hook):\n    r\"\"\"Register an import hook that is called whenever a persistent object\n    is being unpickled. A typical use case is to patch the pickled source\n    code to avoid errors and inconsistencies when the API of some imported\n    module has changed.\n\n    The hook should have the following signature:\n\n        hook(meta) -> modified meta\n\n    `meta` is an instance of `dnnlib.EasyDict` with the following fields:\n\n        type:       Type of the persistent object, e.g. `'class'`.\n        version:    Internal version number of `torch_utils.persistence`.\n        module_src  Original source code of the Python module.\n        class_name: Class name in the original Python module.\n        state:      Internal state of the object.\n\n    Example:\n\n        @persistence.import_hook\n        def wreck_my_network(meta):\n            if meta.class_name == 'MyNetwork':\n                print('MyNetwork is being imported. I will wreck it!')\n                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n            return meta\n    \"\"\"\n    assert callable(hook)\n    _import_hooks.append(hook)", "\n#----------------------------------------------------------------------------\n\ndef _reconstruct_persistent_obj(meta):\n    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n    a persistent object.\n    \"\"\"\n    meta = dnnlib.EasyDict(meta)\n    meta.state = dnnlib.EasyDict(meta.state)\n    for hook in _import_hooks:\n        meta = hook(meta)\n        assert meta is not None\n\n    assert meta.version == _version\n    module = _src_to_module(meta.module_src)\n\n    assert meta.type == 'class'\n    orig_class = module.__dict__[meta.class_name]\n    decorator_class = persistent_class(orig_class)\n    obj = decorator_class.__new__(decorator_class)\n\n    setstate = getattr(obj, '__setstate__', None)\n    if callable(setstate):\n        setstate(meta.state) # pylint: disable=not-callable\n    else:\n        obj.__dict__.update(meta.state)\n    return obj", "\n#----------------------------------------------------------------------------\n\ndef _module_to_src(module):\n    r\"\"\"Query the source code of a given Python module.\n    \"\"\"\n    src = _module_to_src_dict.get(module, None)\n    if src is None:\n        src = inspect.getsource(module)\n        _module_to_src_dict[module] = src\n        _src_to_module_dict[src] = module\n    return src", "\ndef _src_to_module(src):\n    r\"\"\"Get or create a Python module for the given source code.\n    \"\"\"\n    module = _src_to_module_dict.get(src, None)\n    if module is None:\n        module_name = \"_imported_module_\" + uuid.uuid4().hex\n        module = types.ModuleType(module_name)\n        sys.modules[module_name] = module\n        _module_to_src_dict[module] = src\n        _src_to_module_dict[src] = module\n        exec(src, module.__dict__) # pylint: disable=exec-used\n    return module", "\n#----------------------------------------------------------------------------\n\ndef _check_pickleable(obj):\n    r\"\"\"Check that the given object is pickleable, raising an exception if\n    it is not. This function is expected to be considerably more efficient\n    than actually pickling the object.\n    \"\"\"\n    def recurse(obj):\n        if isinstance(obj, (list, tuple, set)):\n            return [recurse(x) for x in obj]\n        if isinstance(obj, dict):\n            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n            return None # Python primitive types are pickleable.\n        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor', 'torch.nn.parameter.Parameter']:\n            return None # NumPy arrays and PyTorch tensors are pickleable.\n        if is_persistent(obj):\n            return None # Persistent objects are pickleable, by virtue of the constructor check.\n        return obj\n    with io.BytesIO() as f:\n        pickle.dump(recurse(obj), f)", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n# empty\n", ""]}
{"filename": "torch_utils/misc.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\nimport re\nimport contextlib\nimport numpy as np", "import contextlib\nimport numpy as np\nimport torch\nimport warnings\nimport dnnlib\n\n#----------------------------------------------------------------------------\n# Cached construction of constant tensors. Avoids CPU=>GPU copy when the\n# same constant is used multiple times.\n", "# same constant is used multiple times.\n\n_constant_cache = dict()\n\ndef constant(value, shape=None, dtype=None, device=None, memory_format=None):\n    value = np.asarray(value)\n    if shape is not None:\n        shape = tuple(shape)\n    if dtype is None:\n        dtype = torch.get_default_dtype()\n    if device is None:\n        device = torch.device('cpu')\n    if memory_format is None:\n        memory_format = torch.contiguous_format\n\n    key = (value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format)\n    tensor = _constant_cache.get(key, None)\n    if tensor is None:\n        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n        if shape is not None:\n            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n        tensor = tensor.contiguous(memory_format=memory_format)\n        _constant_cache[key] = tensor\n    return tensor", "\n#----------------------------------------------------------------------------\n# Replace NaN/Inf with specified numerical values.\n\ntry:\n    nan_to_num = torch.nan_to_num # 1.8.0a0\nexcept AttributeError:\n    def nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None): # pylint: disable=redefined-builtin\n        assert isinstance(input, torch.Tensor)\n        if posinf is None:\n            posinf = torch.finfo(input.dtype).max\n        if neginf is None:\n            neginf = torch.finfo(input.dtype).min\n        assert nan == 0\n        return torch.clamp(input.unsqueeze(0).nansum(0), min=neginf, max=posinf, out=out)", "\n#----------------------------------------------------------------------------\n# Symbolic assert.\n\ntry:\n    symbolic_assert = torch._assert # 1.8.0a0 # pylint: disable=protected-access\nexcept AttributeError:\n    symbolic_assert = torch.Assert # 1.7.0\n\n#----------------------------------------------------------------------------", "\n#----------------------------------------------------------------------------\n# Context manager to temporarily suppress known warnings in torch.jit.trace().\n# Note: Cannot use catch_warnings because of https://bugs.python.org/issue29672\n\n@contextlib.contextmanager\ndef suppress_tracer_warnings():\n    flt = ('ignore', None, torch.jit.TracerWarning, None, 0)\n    warnings.filters.insert(0, flt)\n    yield\n    warnings.filters.remove(flt)", "\n#----------------------------------------------------------------------------\n# Assert that the shape of a tensor matches the given list of integers.\n# None indicates that the size of a dimension is allowed to vary.\n# Performs symbolic assertion when used in torch.jit.trace().\n\ndef assert_shape(tensor, ref_shape):\n    if tensor.ndim != len(ref_shape):\n        raise AssertionError(f'Wrong number of dimensions: got {tensor.ndim}, expected {len(ref_shape)}')\n    for idx, (size, ref_size) in enumerate(zip(tensor.shape, ref_shape)):\n        if ref_size is None:\n            pass\n        elif isinstance(ref_size, torch.Tensor):\n            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n                symbolic_assert(torch.equal(torch.as_tensor(size), ref_size), f'Wrong size for dimension {idx}')\n        elif isinstance(size, torch.Tensor):\n            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n                symbolic_assert(torch.equal(size, torch.as_tensor(ref_size)), f'Wrong size for dimension {idx}: expected {ref_size}')\n        elif size != ref_size:\n            raise AssertionError(f'Wrong size for dimension {idx}: got {size}, expected {ref_size}')", "\n#----------------------------------------------------------------------------\n# Function decorator that calls torch.autograd.profiler.record_function().\n\ndef profiled_function(fn):\n    def decorator(*args, **kwargs):\n        with torch.autograd.profiler.record_function(fn.__name__):\n            return fn(*args, **kwargs)\n    decorator.__name__ = fn.__name__\n    return decorator", "\n#----------------------------------------------------------------------------\n# Sampler for torch.utils.data.DataLoader that loops over the dataset\n# indefinitely, shuffling items as it goes.\n\nclass InfiniteSampler(torch.utils.data.Sampler):\n    def __init__(self, dataset, rank=0, num_replicas=1, shuffle=True, seed=0, window_size=0.5):\n        assert len(dataset) > 0\n        assert num_replicas > 0\n        assert 0 <= rank < num_replicas\n        assert 0 <= window_size <= 1\n        super().__init__(dataset)\n        self.dataset = dataset\n        self.rank = rank\n        self.num_replicas = num_replicas\n        self.shuffle = shuffle\n        self.seed = seed\n        self.window_size = window_size\n\n    def __iter__(self):\n        order = np.arange(len(self.dataset))\n        rnd = None\n        window = 0\n        if self.shuffle:\n            rnd = np.random.RandomState(self.seed)\n            rnd.shuffle(order)\n            window = int(np.rint(order.size * self.window_size))\n\n        idx = 0\n        while True:\n            i = idx % order.size\n            if idx % self.num_replicas == self.rank:\n                yield order[i]\n            if window >= 2:\n                j = (i - rnd.randint(window)) % order.size\n                order[i], order[j] = order[j], order[i]\n            idx += 1", "\n#----------------------------------------------------------------------------\n# Utilities for operating with torch.nn.Module parameters and buffers.\n\ndef params_and_buffers(module):\n    assert isinstance(module, torch.nn.Module)\n    return list(module.parameters()) + list(module.buffers())\n\ndef named_params_and_buffers(module):\n    assert isinstance(module, torch.nn.Module)\n    return list(module.named_parameters()) + list(module.named_buffers())", "def named_params_and_buffers(module):\n    assert isinstance(module, torch.nn.Module)\n    return list(module.named_parameters()) + list(module.named_buffers())\n\n@torch.no_grad()\ndef copy_params_and_buffers(src_module, dst_module, require_all=False):\n    assert isinstance(src_module, torch.nn.Module)\n    assert isinstance(dst_module, torch.nn.Module)\n    src_tensors = dict(named_params_and_buffers(src_module))\n    for name, tensor in named_params_and_buffers(dst_module):\n        assert (name in src_tensors) or (not require_all)\n        if name in src_tensors:\n            tensor.copy_(src_tensors[name])", "\n#----------------------------------------------------------------------------\n# Context manager for easily enabling/disabling DistributedDataParallel\n# synchronization.\n\n@contextlib.contextmanager\ndef ddp_sync(module, sync):\n    assert isinstance(module, torch.nn.Module)\n    if sync or not isinstance(module, torch.nn.parallel.DistributedDataParallel):\n        yield\n    else:\n        with module.no_sync():\n            yield", "\n#----------------------------------------------------------------------------\n# Check DistributedDataParallel consistency across processes.\n\ndef check_ddp_consistency(module, ignore_regex=None):\n    assert isinstance(module, torch.nn.Module)\n    for name, tensor in named_params_and_buffers(module):\n        fullname = type(module).__name__ + '.' + name\n        if ignore_regex is not None and re.fullmatch(ignore_regex, fullname):\n            continue\n        tensor = tensor.detach()\n        if tensor.is_floating_point():\n            tensor = nan_to_num(tensor)\n        other = tensor.clone()\n        torch.distributed.broadcast(tensor=other, src=0)\n        assert (tensor == other).all(), fullname", "\n#----------------------------------------------------------------------------\n# Print summary table of module hierarchy.\n\ndef print_module_summary(module, inputs, max_nesting=3, skip_redundant=True):\n    assert isinstance(module, torch.nn.Module)\n    assert not isinstance(module, torch.jit.ScriptModule)\n    assert isinstance(inputs, (tuple, list))\n\n    # Register hooks.\n    entries = []\n    nesting = [0]\n    def pre_hook(_mod, _inputs):\n        nesting[0] += 1\n    def post_hook(mod, _inputs, outputs):\n        nesting[0] -= 1\n        if nesting[0] <= max_nesting:\n            outputs = list(outputs) if isinstance(outputs, (tuple, list)) else [outputs]\n            outputs = [t for t in outputs if isinstance(t, torch.Tensor)]\n            entries.append(dnnlib.EasyDict(mod=mod, outputs=outputs))\n    hooks = [mod.register_forward_pre_hook(pre_hook) for mod in module.modules()]\n    hooks += [mod.register_forward_hook(post_hook) for mod in module.modules()]\n\n    # Run module.\n    outputs = module(*inputs)\n    for hook in hooks:\n        hook.remove()\n\n    # Identify unique outputs, parameters, and buffers.\n    tensors_seen = set()\n    for e in entries:\n        e.unique_params = [t for t in e.mod.parameters() if id(t) not in tensors_seen]\n        e.unique_buffers = [t for t in e.mod.buffers() if id(t) not in tensors_seen]\n        e.unique_outputs = [t for t in e.outputs if id(t) not in tensors_seen]\n        tensors_seen |= {id(t) for t in e.unique_params + e.unique_buffers + e.unique_outputs}\n\n    # Filter out redundant entries.\n    if skip_redundant:\n        entries = [e for e in entries if len(e.unique_params) or len(e.unique_buffers) or len(e.unique_outputs)]\n\n    # Construct table.\n    rows = [[type(module).__name__, 'Parameters', 'Buffers', 'Output shape', 'Datatype']]\n    rows += [['---'] * len(rows[0])]\n    param_total = 0\n    buffer_total = 0\n    submodule_names = {mod: name for name, mod in module.named_modules()}\n    for e in entries:\n        name = '<top-level>' if e.mod is module else submodule_names[e.mod]\n        param_size = sum(t.numel() for t in e.unique_params)\n        buffer_size = sum(t.numel() for t in e.unique_buffers)\n        output_shapes = [str(list(t.shape)) for t in e.outputs]\n        output_dtypes = [str(t.dtype).split('.')[-1] for t in e.outputs]\n        rows += [[\n            name + (':0' if len(e.outputs) >= 2 else ''),\n            str(param_size) if param_size else '-',\n            str(buffer_size) if buffer_size else '-',\n            (output_shapes + ['-'])[0],\n            (output_dtypes + ['-'])[0],\n        ]]\n        for idx in range(1, len(e.outputs)):\n            rows += [[name + f':{idx}', '-', '-', output_shapes[idx], output_dtypes[idx]]]\n        param_total += param_size\n        buffer_total += buffer_size\n    rows += [['---'] * len(rows[0])]\n    rows += [['Total', str(param_total), str(buffer_total), '-', '-']]\n\n    # Print table.\n    widths = [max(len(cell) for cell in column) for column in zip(*rows)]\n    print()\n    for row in rows:\n        print('  '.join(cell + ' ' * (width - len(cell)) for cell, width in zip(row, widths)))\n    print()\n    return outputs", "\n#----------------------------------------------------------------------------\n"]}
{"filename": "dnnlib/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\nfrom .util import EasyDict, make_cache_dir_path\n", ""]}
{"filename": "dnnlib/util.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Miscellaneous utility classes and functions.\"\"\"\n\nimport ctypes", "\nimport ctypes\nimport fnmatch\nimport importlib\nimport inspect\nimport numpy as np\nimport os\nimport shutil\nimport sys\nimport types", "import sys\nimport types\nimport io\nimport pickle\nimport re\nimport requests\nimport html\nimport hashlib\nimport glob\nimport tempfile", "import glob\nimport tempfile\nimport urllib\nimport urllib.request\nimport uuid\n\nfrom distutils.util import strtobool\nfrom typing import Any, List, Tuple, Union, Optional\n\n", "\n\n# Util classes\n# ------------------------------------------------------------------------------------------\n\n\nclass EasyDict(dict):\n    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        self[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        del self[name]", "\n\nclass Logger(object):\n    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n\n    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: Union[str, bytes]) -> None:\n        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n        if isinstance(text, bytes):\n            text = text.decode()\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n            self.file = None", "\n\n# Cache directories\n# ------------------------------------------------------------------------------------------\n\n_dnnlib_cache_dir = None\n\ndef set_cache_dir(path: str) -> None:\n    global _dnnlib_cache_dir\n    _dnnlib_cache_dir = path", "\ndef make_cache_dir_path(*paths: str) -> str:\n    if _dnnlib_cache_dir is not None:\n        return os.path.join(_dnnlib_cache_dir, *paths)\n    if 'DNNLIB_CACHE_DIR' in os.environ:\n        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n    if 'HOME' in os.environ:\n        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n    if 'USERPROFILE' in os.environ:\n        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)", "\n# Small util functions\n# ------------------------------------------------------------------------------------------\n\n\ndef format_time(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n    else:\n        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)", "\n\ndef format_time_brief(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n    else:\n        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)", "\n\ndef ask_yes_no(question: str) -> bool:\n    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n    while True:\n        try:\n            print(\"{0} [y/n]\".format(question))\n            return strtobool(input().lower())\n        except ValueError:\n            pass", "\n\ndef tuple_product(t: Tuple) -> Any:\n    \"\"\"Calculate the product of the tuple elements.\"\"\"\n    result = 1\n\n    for v in t:\n        result *= v\n\n    return result", "\n\n_str_to_ctype = {\n    \"uint8\": ctypes.c_ubyte,\n    \"uint16\": ctypes.c_uint16,\n    \"uint32\": ctypes.c_uint32,\n    \"uint64\": ctypes.c_uint64,\n    \"int8\": ctypes.c_byte,\n    \"int16\": ctypes.c_int16,\n    \"int32\": ctypes.c_int32,", "    \"int16\": ctypes.c_int16,\n    \"int32\": ctypes.c_int32,\n    \"int64\": ctypes.c_int64,\n    \"float32\": ctypes.c_float,\n    \"float64\": ctypes.c_double\n}\n\n\ndef get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n    type_str = None\n\n    if isinstance(type_obj, str):\n        type_str = type_obj\n    elif hasattr(type_obj, \"__name__\"):\n        type_str = type_obj.__name__\n    elif hasattr(type_obj, \"name\"):\n        type_str = type_obj.name\n    else:\n        raise RuntimeError(\"Cannot infer type name from input\")\n\n    assert type_str in _str_to_ctype.keys()\n\n    my_dtype = np.dtype(type_str)\n    my_ctype = _str_to_ctype[type_str]\n\n    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\n    return my_dtype, my_ctype", "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n    type_str = None\n\n    if isinstance(type_obj, str):\n        type_str = type_obj\n    elif hasattr(type_obj, \"__name__\"):\n        type_str = type_obj.__name__\n    elif hasattr(type_obj, \"name\"):\n        type_str = type_obj.name\n    else:\n        raise RuntimeError(\"Cannot infer type name from input\")\n\n    assert type_str in _str_to_ctype.keys()\n\n    my_dtype = np.dtype(type_str)\n    my_ctype = _str_to_ctype[type_str]\n\n    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\n    return my_dtype, my_ctype", "\n\ndef is_pickleable(obj: Any) -> bool:\n    try:\n        with io.BytesIO() as stream:\n            pickle.dump(obj, stream)\n        return True\n    except:\n        return False\n", "\n\n# Functionality to import modules/objects by name, and call functions by name\n# ------------------------------------------------------------------------------------------\n\ndef get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n    \"\"\"Searches for the underlying module behind the name to some python object.\n    Returns the module and the object name (original name with module part removed).\"\"\"\n\n    # allow convenience shorthands, substitute them by full names\n    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n\n    # list alternatives for (module_name, local_obj_name)\n    parts = obj_name.split(\".\")\n    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n\n    # try each alternative in turn\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n            return module, local_obj_name\n        except:\n            pass\n\n    # maybe some of the modules themselves contain errors?\n    for module_name, _local_obj_name in name_pairs:\n        try:\n            importlib.import_module(module_name) # may raise ImportError\n        except ImportError:\n            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n                raise\n\n    # maybe the requested attribute is missing?\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n        except ImportError:\n            pass\n\n    # we are out of luck, but we have no idea why\n    raise ImportError(obj_name)", "\n\ndef get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n    if obj_name == '':\n        return module\n    obj = module\n    for part in obj_name.split(\".\"):\n        obj = getattr(obj, part)\n    return obj", "\n\ndef get_obj_by_name(name: str) -> Any:\n    \"\"\"Finds the python object with the given name.\"\"\"\n    module, obj_name = get_module_from_obj_name(name)\n    return get_obj_from_module(module, obj_name)\n\n\ndef call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n    assert func_name is not None\n    func_obj = get_obj_by_name(func_name)\n    assert callable(func_obj)\n    return func_obj(*args, **kwargs)", "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n    assert func_name is not None\n    func_obj = get_obj_by_name(func_name)\n    assert callable(func_obj)\n    return func_obj(*args, **kwargs)\n\n\ndef construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n    return call_func_by_name(*args, func_name=class_name, **kwargs)", "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n    return call_func_by_name(*args, func_name=class_name, **kwargs)\n\n\ndef get_module_dir_by_obj_name(obj_name: str) -> str:\n    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n    module, _ = get_module_from_obj_name(obj_name)\n    return os.path.dirname(inspect.getfile(module))\n", "\n\ndef is_top_level_function(obj: Any) -> bool:\n    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n\n\ndef get_top_level_function_name(obj: Any) -> str:\n    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n    assert is_top_level_function(obj)\n    module = obj.__module__\n    if module == '__main__':\n        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n    return module + \".\" + obj.__name__", "\n\n# File system helpers\n# ------------------------------------------------------------------------------------------\n\ndef list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n    Returns list of tuples containing both absolute and relative paths.\"\"\"\n    assert os.path.isdir(dir_path)\n    base_name = os.path.basename(os.path.normpath(dir_path))\n\n    if ignores is None:\n        ignores = []\n\n    result = []\n\n    for root, dirs, files in os.walk(dir_path, topdown=True):\n        for ignore_ in ignores:\n            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n\n            # dirs need to be edited in-place\n            for d in dirs_to_remove:\n                dirs.remove(d)\n\n            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n\n        absolute_paths = [os.path.join(root, f) for f in files]\n        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n\n        if add_base_to_relative:\n            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n\n        assert len(absolute_paths) == len(relative_paths)\n        result += zip(absolute_paths, relative_paths)\n\n    return result", "\n\ndef copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n    Will create all necessary directories.\"\"\"\n    for file in files:\n        target_dir_name = os.path.dirname(file[1])\n\n        # will create all intermediate-level directories\n        if not os.path.exists(target_dir_name):\n            os.makedirs(target_dir_name)\n\n        shutil.copyfile(file[0], file[1])", "\n\n# URL helpers\n# ------------------------------------------------------------------------------------------\n\ndef is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n    if not isinstance(obj, str) or not \"://\" in obj:\n        return False\n    if allow_file_urls and obj.startswith('file://'):\n        return True\n    try:\n        res = requests.compat.urlparse(obj)\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n    except:\n        return False\n    return True", "\n\ndef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n    assert num_attempts >= 1\n    assert not (return_filename and (not cache))\n\n    # Doesn't look like an URL scheme so interpret it as a local filename.\n    if not re.match('^[a-z]+://', url):\n        return url if return_filename else open(url, \"rb\")\n\n    # Handle file URLs.  This code handles unusual file:// patterns that\n    # arise on Windows:\n    #\n    # file:///c:/foo.txt\n    #\n    # which would translate to a local '/c:/foo.txt' filename that's\n    # invalid.  Drop the forward slash for such pathnames.\n    #\n    # If you touch this code path, you should test it on both Linux and\n    # Windows.\n    #\n    # Some internet resources suggest using urllib.request.url2pathname() but\n    # but that converts forward slashes to backslashes and this causes\n    # its own set of problems.\n    if url.startswith('file://'):\n        filename = urllib.parse.urlparse(url).path\n        if re.match(r'^/[a-zA-Z]:', filename):\n            filename = filename[1:]\n        return filename if return_filename else open(filename, \"rb\")\n\n    assert is_url(url)\n\n    # Lookup from cache.\n    if cache_dir is None:\n        cache_dir = make_cache_dir_path('downloads')\n\n    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n    if cache:\n        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n        if len(cache_files) == 1:\n            filename = cache_files[0]\n            return filename if return_filename else open(filename, \"rb\")\n\n    # Download.\n    url_name = None\n    url_data = None\n    with requests.Session() as session:\n        if verbose:\n            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n        for attempts_left in reversed(range(num_attempts)):\n            try:\n                with session.get(url) as res:\n                    res.raise_for_status()\n                    if len(res.content) == 0:\n                        raise IOError(\"No data received\")\n\n                    if len(res.content) < 8192:\n                        content_str = res.content.decode(\"utf-8\")\n                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n                            if len(links) == 1:\n                                url = requests.compat.urljoin(url, links[0])\n                                raise IOError(\"Google Drive virus checker nag\")\n                        if \"Google Drive - Quota exceeded\" in content_str:\n                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n\n                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n                    url_name = match[1] if match else url\n                    url_data = res.content\n                    if verbose:\n                        print(\" done\")\n                    break\n            except KeyboardInterrupt:\n                raise\n            except:\n                if not attempts_left:\n                    if verbose:\n                        print(\" failed\")\n                    raise\n                if verbose:\n                    print(\".\", end=\"\", flush=True)\n\n    # Save to cache.\n    if cache:\n        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n        safe_name = safe_name[:min(len(safe_name), 128)]\n        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        with open(temp_file, \"wb\") as f:\n            f.write(url_data)\n        os.replace(temp_file, cache_file) # atomic\n        if return_filename:\n            return cache_file\n\n    # Return data as file object.\n    assert not return_filename\n    return io.BytesIO(url_data)", ""]}
