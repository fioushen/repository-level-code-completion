{"filename": "evaluation.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport gym\nimport numpy as np\nimport os\nimport argparse\nimport torch\nimport json\nimport sys", "import json\nimport sys\nimport pybulletgym\n\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\nfrom stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3 import SAC", "from stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3 import SAC\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.logger import configure\n\n\nimport gym\nimport numpy as np\nimport os", "import numpy as np\nimport os\nimport argparse\nimport torch\nimport json\nimport sys\nimport pybulletgym\n\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.noise import NormalActionNoise", "from stable_baselines3 import DDPG\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\nfrom stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3 import SAC\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.logger import configure\n", "from stable_baselines3.common.logger import configure\n\n\nfrom action_constrained_rl.env_wrapper import ConstraintEnvWrapper\nfrom action_constrained_rl.env_wrapper import MemorizeCenterEnvWrapper\nfrom action_constrained_rl.ddpg.projection_ddpg import ProjectionDDPG\nfrom action_constrained_rl.td3.projection_td3 import ProjectionTD3\nfrom action_constrained_rl.sac.projection_sac import ProjectionSAC\nfrom action_constrained_rl.ddpg.noise_insertion_ddpg import NoiseInsertionDDPG\nfrom action_constrained_rl.ddpg.logging_gradient import LoggingGradientDDPG", "from action_constrained_rl.ddpg.noise_insertion_ddpg import NoiseInsertionDDPG\nfrom action_constrained_rl.ddpg.logging_gradient import LoggingGradientDDPG\nfrom action_constrained_rl.ddpg.logging_gradient import DDPGWithOutputPenalty\nfrom action_constrained_rl.ddpg.nfwpo import NFWPO\nfrom action_constrained_rl.ddpg.ddpg_with_penalty import DDPGWithPenalty\nfrom action_constrained_rl.td3.td3_with_penalty import TD3WithPenalty\nfrom action_constrained_rl.td3.td3_output_penalty import TD3WithOutputPenalty\nfrom action_constrained_rl.td3.noise_insertion_td3 import NoiseInsertionTD3\nfrom action_constrained_rl.sac.logging_gradient import LoggingGradientSAC\nfrom action_constrained_rl.sac.logging_gradient import SACWithOutputPenalty", "from action_constrained_rl.sac.logging_gradient import LoggingGradientSAC\nfrom action_constrained_rl.sac.logging_gradient import SACWithOutputPenalty\nfrom action_constrained_rl.sac.safe_sampling_sac import SafeSamplingSAC\nfrom action_constrained_rl.nn.opt_layer.opt_layer import OptLayer\nfrom action_constrained_rl.nn.opt_layer.opt_layer_policy import OptLayerPolicy\nfrom action_constrained_rl.nn.additional_layers.alpha_projection import AlphaProjectionLayer\nfrom action_constrained_rl.nn.additional_layers.radial_squash import SquashLayer\nfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaGaussianDistribution\nfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaStateDependentNoiseDistribution\nfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedGaussianDistribution", "from action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaStateDependentNoiseDistribution\nfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedGaussianDistribution\nfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedStateDependentNoiseDistribution\nfrom action_constrained_rl.nn.additional_layer_policy import AdditionalLayerPolicy\nfrom action_constrained_rl.nn.additional_layer_sac_policy import AdditionalLayerSACPolicy\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\nfrom action_constrained_rl.utils.arithmatic_series import ArithmaticSeries\nfrom action_constrained_rl.utils.geometric_series import GeometricSeries\nfrom action_constrained_rl.utils.log_series import LogSeries\nfrom action_constrained_rl.constraint.box_constraint import BoxConstraint", "from action_constrained_rl.utils.log_series import LogSeries\nfrom action_constrained_rl.constraint.box_constraint import BoxConstraint\nfrom action_constrained_rl.constraint.power_constraint import PowerConstraint\nfrom action_constrained_rl.constraint.power_constraint import OrthoplexConstraint\nfrom action_constrained_rl.constraint.power_constraint import DecelerationConstraint\nfrom action_constrained_rl.constraint.sphere_constraint import SphericalConstraint\nfrom action_constrained_rl.constraint.tip_constraint import TipConstraint\nfrom action_constrained_rl.constraint.MA_constraint import MAConstraint\nfrom action_constrained_rl.constraint.combined_constraint import CombinedConstraint\nfrom action_constrained_rl.constraint.sin2_constraint import Sin2Constraint", "from action_constrained_rl.constraint.combined_constraint import CombinedConstraint\nfrom action_constrained_rl.constraint.sin2_constraint import Sin2Constraint\n\nimport gurobipy as gp\ngp.setParam('OutputFlag', 0)\n\n\n\ndef nameToConstraint(args):\n    name = args.env\n    c_name = args.constraint\n\n    if c_name == \"Box\" or c_name == \"Power\" or c_name == \"Orthoplex\" or c_name == \"Deceleration\" or c_name == \"Sphere\":\n        if name == \"Hopper-v3\":\n            offset = 8\n            scale = (1, 1, 1)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 11\n        elif name == \"ReacherPyBulletEnv-v0\":\n            offset = 6\n            scale = (1, 1)\n            indices = [6, 8]\n            s_dim = 9\n        elif name == 'Ant-v3':\n            offset = 19\n            scale = (1.,1.,1.,1.,1.,1.,1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 27\n        elif name == 'HalfCheetah-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n        elif name == 'Swimmer-v3':\n            offset = 6\n            scale = (1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 8\n        elif name == 'Walker2d-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n\n        if c_name == \"Box\":\n            return BoxConstraint(len(scale)) # R+N\n        elif c_name == \"Power\":\n            return PowerConstraint(indices, scale, args.max_power, s_dim) # R+M, H+M, W+M\n        elif c_name == \"Orthoplex\":\n            return OrthoplexConstraint(indices, scale, args.max_power, s_dim) # R+O03, R+O10, R+O30\n        elif c_name == \"Deceleration\":\n            return DecelerationConstraint(indices, scale, args.max_power, s_dim) #unused\n        elif c_name == \"Sphere\":\n            return SphericalConstraint(len(scale), args.max_power) #R+L2\n        \n    elif c_name == 'Tip':\n        return TipConstraint(args.max_power)# R+T\n    elif c_name == 'MA':\n        return MAConstraint(args.max_power) # HC+MA\n    elif c_name == 'O+S':\n        if name == \"Hopper-v3\":\n            offset_p = 2\n            scale = (1, 1, 1)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 8\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 11\n        elif name == 'Walker2d-v3':\n            offset_p = 2\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 11\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 17\n        return CombinedConstraint(OrthoplexConstraint(indices_v, scale, args.max_power[0], s_dim),\n                                  Sin2Constraint(indices_p, args.max_power[1], s_dim)) # H+O+S, W+O+S\n    else:\n        raise", "def nameToConstraint(args):\n    name = args.env\n    c_name = args.constraint\n\n    if c_name == \"Box\" or c_name == \"Power\" or c_name == \"Orthoplex\" or c_name == \"Deceleration\" or c_name == \"Sphere\":\n        if name == \"Hopper-v3\":\n            offset = 8\n            scale = (1, 1, 1)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 11\n        elif name == \"ReacherPyBulletEnv-v0\":\n            offset = 6\n            scale = (1, 1)\n            indices = [6, 8]\n            s_dim = 9\n        elif name == 'Ant-v3':\n            offset = 19\n            scale = (1.,1.,1.,1.,1.,1.,1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 27\n        elif name == 'HalfCheetah-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n        elif name == 'Swimmer-v3':\n            offset = 6\n            scale = (1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 8\n        elif name == 'Walker2d-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n\n        if c_name == \"Box\":\n            return BoxConstraint(len(scale)) # R+N\n        elif c_name == \"Power\":\n            return PowerConstraint(indices, scale, args.max_power, s_dim) # R+M, H+M, W+M\n        elif c_name == \"Orthoplex\":\n            return OrthoplexConstraint(indices, scale, args.max_power, s_dim) # R+O03, R+O10, R+O30\n        elif c_name == \"Deceleration\":\n            return DecelerationConstraint(indices, scale, args.max_power, s_dim) #unused\n        elif c_name == \"Sphere\":\n            return SphericalConstraint(len(scale), args.max_power) #R+L2\n        \n    elif c_name == 'Tip':\n        return TipConstraint(args.max_power)# R+T\n    elif c_name == 'MA':\n        return MAConstraint(args.max_power) # HC+MA\n    elif c_name == 'O+S':\n        if name == \"Hopper-v3\":\n            offset_p = 2\n            scale = (1, 1, 1)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 8\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 11\n        elif name == 'Walker2d-v3':\n            offset_p = 2\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 11\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 17\n        return CombinedConstraint(OrthoplexConstraint(indices_v, scale, args.max_power[0], s_dim),\n                                  Sin2Constraint(indices_p, args.max_power[1], s_dim)) # H+O+S, W+O+S\n    else:\n        raise", "\ndef nameToEnv(name, seed=0):\n    env = gym.make(name)\n    env.seed(seed)\n    return env\n\n# unused\ndef pickConstraintCoefficient(args):\n    if args.d > 0.0:\n        constraint_penalty = ArithmaticSeries(args.a0, args.d)\n    elif args.r > 0.0:\n        constraint_penalty = GeometricSeries(args.a0, args.r)\n    elif args.use_log_series:\n        constraint_penalty = LogSeries(args.a0)\n    else:\n        constraint_penalty = ConstantFunction(args.c)\n    return constraint_penalty", "\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--log_dir\", action=\"store\", default=\"tmp\")\nparser.add_argument(\"--prob_id\", action=\"store\", default = \"\");\nparser.add_argument(\"--algo_id\", action=\"store\", default = \"\");\nparser.add_argument(\"--env\", action=\"store\", default=\"HalfCheetahDynamic\")\nparser.add_argument(\"--constraint\", action=\"store\", default=\"Normal\")\nparser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\nparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])", "parser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\nparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])\nparser.add_argument(\"--num_time_steps\", action=\"store\", default=1e6, type=int)\nparser.add_argument(\"--batch_size\", action=\"store\", default=100, type=int)\nparser.add_argument(\"--use_env_wrapper\", action=\"store_true\", default=False, help=\"use projection inside environments\")\nparser.add_argument(\"--use_action_restriction\", action=\"store_true\", default=False)\nparser.add_argument(\"--sigma\", action=\"store\", default=0.01, type=float, help=\"stddev for Gaussian Action Noise\")\nparser.add_argument(\"--a0\", action=\"store\", default=0.001, type=float)\nparser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\nparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)", "parser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\nparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)\nparser.add_argument(\"--init_ent_coef\", action=\"store\", default=2.0, type=float)\nparser.add_argument(\"--n\", action=\"store\", default=1, type=int, help=\"Update the penalty coefficient c every n episodes\")\nparser.add_argument(\"--verbose\", action=\"store\", default=1, type=int)\nparser.add_argument(\"--seed\", action=\"store\", default=0, type=int)\nparser.add_argument(\"--eval_freq\", action=\"store\", default=5000, type=int, help=\"run evaluation episodes every eval_freq time steps\")\nparser.add_argument(\"--n_eval_episodes\", action=\"store\", default=5, type=int)\nparser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\nparser.add_argument(\"--device\", action=\"store\", default='auto')", "parser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\nparser.add_argument(\"--device\", action=\"store\", default='auto')\nparser.add_argument(\"--squash_output\", action=\"store_true\", default=False)\nparser.add_argument(\"--use_my_mlppolicy\", action=\"store_true\", default=False)\nparser.add_argument(\"--infinity_action_space\", action=\"store_true\", default=False)\nparser.add_argument(\"--use_NFWPO\", action=\"store_true\", default=False)\nparser.add_argument(\"--fw_learning_rate\", action=\"store\", default=0.01, type=float)\nparser.add_argument(\"--logging_gradient\", action=\"store\", default=True, type=bool)\nparser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--c\", action=\"store\", default=0.0, type=float, help=\"Constant penalty coefficient\")\ngroup.add_argument(\"--d\", action=\"store\", default=-1.0, type=float, help=\"add d to the penalty coefficient\")\ngroup.add_argument(\"--r\", action=\"store\", default=-1.0, type=float)\ngroup.add_argument(\"--use_log_series\", action=\"store_true\", default=False)\n\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)", "group = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_opt_layer\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_alpha_projection_layer\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_squash_layer\", action=\"store_true\", default=False)\n\nparser.add_argument(\"--proj_type\", action=\"store\", default=\"QP\", choices=[\"QP\", \"alpha\", \"squash\"])\nargs = parser.parse_args()\n\n", "\n\n# from problem id, set problem arguments\nif args.prob_id != \"\":\n    if args.prob_id == \"R+N\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Box\"\n    elif args.prob_id == \"R+L2\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Sphere\"\n        args.max_power = 0.05\n    elif args.prob_id == \"R+O03\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 0.3\n    elif args.prob_id == \"R+O10\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 1.0\n    elif args.prob_id == \"R+O30\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 3.0\n    elif args.prob_id == \"R+M\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Power\"\n        args.max_power = 1.0 \n    elif args.prob_id == \"R+T\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Tip\"\n        args.max_power = 0.05 \n    elif args.prob_id == \"HC+O\" or args.prob_id == \"HC+O-16\":\n        args.env = \"HalfCheetah-v3\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 20.\n    elif args.prob_id == \"H+M\" or args.prob_id == \"H+M-16\":\n        args.env = \"Hopper-v3\"\n        args.constraint = \"Power\"\n        args.max_power = 10.\n    elif args.prob_id == \"W+M\" or args.prob_id == \"W+M-16\":\n        args.env = \"Walker2d-v3\"\n        args.constraint = \"Power\"\n        args.max_power = 10.\n    elif args.prob_id == \"HC+MA\":\n        args.env = \"HalfCheetah-v3\"\n        args.constraint = \"MA\"\n        args.max_power = 5.\n    elif args.prob_id == \"H+O+S\":\n        args.env = \"Hopper-v3\"\n        args.constraint = \"O+S\"\n        args.max_power = (10., 0.1)\n    elif args.prob_id == \"W+O+S\":\n        args.env = \"Walker2d-v3\"\n        args.constraint = \"O+S\"\n        args.max_power = (10., 0.1)\n    else: raise ValueError(\"unknown problem id\")", "\n# from algorithm id, set algorithm arguments\nif args.algo_id != \"\":\n    if args.algo_id == \"DPro\":\n        args.use_action_restriction = True\n    elif args.algo_id == \"DPro+\":\n        args.use_action_restriction = True\n        args.c = 1.\n    elif args.algo_id == \"DPre\":\n        args.use_env_wrapper = True\n    elif args.algo_id == \"DPre+\":\n        args.use_env_wrapper = True\n        args.c = 1.\n    elif args.algo_id == \"DOpt\":\n        args.use_opt_layer = True\n        args.squash_output = True\n    elif args.algo_id == \"DOpt+\":\n        args.use_opt_layer = True\n        args.squash_output = True\n        args.c = 1.\n    elif args.algo_id == \"NFW\":\n        args.use_NFWPO = True\n    elif args.algo_id == \"DAlpha\":\n        args.use_alpha_projection_layer = True\n    elif args.algo_id == \"DRad\":\n        args.use_squash_layer = True\n    elif args.algo_id == \"SPre\":\n        args.use_env_wrapper = True\n        args.solver = \"SAC\"\n    elif args.algo_id == \"SPre+\":\n        args.use_env_wrapper = True\n        args.solver = \"SAC\"\n        args.c = 1.\n    elif args.algo_id == \"SAlpha\":\n        args.use_alpha_projection_layer = True\n        args.solver = \"SAC\"\n    elif args.algo_id == \"SRad\":\n        args.use_squash_layer = True\n        args.solver = \"SAC\"\n    else:\n        raise ValueError(\"unknown algo id\")", "\nif args.proj_type == \"squash\":\n    assert args.infinity_action_space\nif args.use_squash_layer:\n    assert not args.squash_output\nif args.use_opt_layer:\n    assert args.squash_output\n\nlog_dir = args.log_dir\nos.makedirs(log_dir, exist_ok=True)\nif not args.output_stdout:\n    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")", "log_dir = args.log_dir\nos.makedirs(log_dir, exist_ok=True)\nif not args.output_stdout:\n    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")\nprint(args)\nwith open(f'{log_dir}/commandline_args.txt', 'w') as f:\n    json.dump(args.__dict__, f, indent=2)\n\nenv = nameToEnv(args.env, args.seed)", "\nenv = nameToEnv(args.env, args.seed)\nconstraint = nameToConstraint(args)\nconstraint.proj_type = args.proj_type\nconstraint_penalty = pickConstraintCoefficient(args) # penalty coefficient function for output penalty\n\nif args.use_alpha_projection_layer or args.use_squash_layer: # wrapper to memorize the centers\n    EnvWrapper = MemorizeCenterEnvWrapper\n    env = EnvWrapper(constraint, env, n=args.n, dual_learning_rate=args.dual_learning_rate)\n    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/monitor.csv\")\n    eval_env = EnvWrapper(constraint, nameToEnv(args.env, args.seed), n=args.n, dual_learning_rate=args.dual_learning_rate)\n    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\nelse:  # wrapper to project actions. We do not use reward penalty\n    env = ConstraintEnvWrapper(constraint, env, constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=log_dir + \"/monitor.csv\", n=args.n, dual_learning_rate=args.dual_learning_rate, normalize=args.normalize_constraint, infinity_action_space = args.infinity_action_space)\n    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/vec_monitor.csv\")\n    eval_env = ConstraintEnvWrapper(constraint, nameToEnv(args.env, args.seed), constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=None, n=args.n, infinity_action_space = args.infinity_action_space)\n    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)", "    \neval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n                             log_path=log_dir, eval_freq=args.eval_freq, n_eval_episodes = args.n_eval_episodes,\n                         deterministic=True, render=False)\n\n# set rl-zoo hyperparameters\nn_actions = env.action_space.shape[-1]\nif args.env == \"ReacherPyBulletEnv-v0\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 3e5\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"gamma\": 0.98, \"buffer_size\": 200000, \"learning_starts\": 10000,\n                 \"action_noise\": action_noise, \"gradient_steps\": -1, \"train_freq\": (1, \"episode\"),\n                 \"learning_rate\": 1e-3, \"policy_kwargs\": {\"net_arch\":[400, 300]}}\n    elif args.solver == \"SAC\":\n        n_timesteps = 3e5\n        kargs = {\"learning_rate\": 7.3e-4, \"buffer_size\": 300000, \"batch_size\": 256,\n                 \"ent_coef\": 'auto', \"gamma\": 0.98, \"tau\": 0.02, \"train_freq\": 8,\n                 \"gradient_steps\": 8, \"learning_starts\": 10000,\n                 \"use_sde\": True, \"policy_kwargs\": dict(log_std_init=-3, net_arch=[400, 300])}\nelif args.env == \"HalfCheetah-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelif args.env == \"Hopper-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise, \"train_freq\": 1,\n                 \"gradient_steps\": 1, \"learning_rate\": 3e-4, \"batch_size\": 256}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelif args.env == \"Walker2d-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelse:\n    raise", "kargs[\"verbose\"]=args.verbose\nif not \"policy_kwargs\" in kargs:\n    kargs[\"policy_kwargs\"]={}\nif args.prob_id[-3:] == \"-16\":\n    print(\"batch_size: 16\")\n    kargs.update({\"batch_size\": 16})\n\ndef pickModel(constraint):\n    # select model according to arguments\n    seed = args.seed\n    \n    if args.use_NFWPO: #NFW\n        if args.prob_id[:2] == \"R+\":\n            fw_learning_rate = 0.05\n        else:\n            fw_learning_rate = 0.01\n        model = NFWPO(constraint, \"MlpPolicy\", env, fw_learning_rate = fw_learning_rate,\n                      device = args.device, seed = seed, **kargs)\n\n    elif args.use_action_restriction: #DPro, DPro+\n        if args.solver == \"DDPG\":\n            algo = ProjectionDDPG\n        elif args.solver == \"TD3\":\n            algo = ProjectionTD3\n        elif args.solver == \"SAC\":\n            algo = ProjectionSAC\n        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\n    elif args.use_alpha_projection_layer or args.use_squash_layer: # DAlpha, DRad, SAlpha, SRad\n        kargs[\"policy_kwargs\"].update({\"constraint\": constraint})\n        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n            if args.solver == \"DDPG\":\n                algo = NoiseInsertionDDPG\n            else:\n                algo = NoiseInsertionTD3\n            policy = AdditionalLayerPolicy\n            if args.use_alpha_projection_layer:\n                layer_type = AlphaProjectionLayer\n            elif args.use_squash_layer:\n                layer_type = SquashLayer\n            kargs[\"policy_kwargs\"].update({\"layer_type\": layer_type, \"squash_output\": args.squash_output})\n        else:\n            algo = SafeSamplingSAC\n            action_noise = None\n            policy = AdditionalLayerSACPolicy\n            if args.use_alpha_projection_layer:\n                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n                    distribution_class = AlphaStateDependentNoiseDistribution\n                else:\n                    distribution_class = AlphaGaussianDistribution\n            if args.use_squash_layer:\n                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n                    distribution_class = ShrinkedStateDependentNoiseDistribution\n                else:\n                    distribution_class = ShrinkedGaussianDistribution\n            kargs[\"policy_kwargs\"].update({\"distribution_class\": distribution_class})\n        model = algo(policy, env, device = args.device, seed = seed, **kargs)\n    elif args.use_opt_layer: # DOpt, DOpt+\n        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n            if args.solver == \"DDPG\":\n                #algo = NoiseInsertionDDPG\n                algo = DDPGWithPenalty\n            else:\n                algo = TD3WithPenalty\n        else:\n            algo = SafeSamplingSAC\n            action_noise = None\n        kargs[\"policy_kwargs\"].update({\"constraint\": constraint, \"squash_output\": args.squash_output})\n        model = algo(constraint, OptLayerPolicy, env, use_center_wrapper = False, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n    else:\n        if args.solver == \"DDPG\":\n            algo = DDPGWithOutputPenalty\n        elif args.solver == \"TD3\":\n            algo = TD3WithOutputPenalty # DPre, DPre+\n        elif args.solver == \"SAC\":\n            algo = SACWithOutputPenalty # SPre, SPre+\n        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty,  device = args.device, seed = seed, **kargs)\n    return model", "\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nmodel = pickModel(constraint)\nrewards = []\nfor seed in range(1,11):\n    ev = np.load(args.log_dir+'-'+str(seed)+\"/evaluations.npz\")\n    model.set_parameters(args.log_dir+'-'+str(seed)+\"/best_model.zip\")\n    rewards.append(evaluate_policy(model, env, n_eval_episodes = 50)[0])\nrewards = np.array(rewards)\nwith open(args.log_dir+\"/result.npy\", \"wb\") as result_file:\n    np.save(result_file, rewards)", "rewards = np.array(rewards)\nwith open(args.log_dir+\"/result.npy\", \"wb\") as result_file:\n    np.save(result_file, rewards)\n"]}
{"filename": "train.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport gym\nimport numpy as np\nimport os\nimport argparse\nimport torch\nimport json\nimport sys", "import json\nimport sys\nimport pybulletgym\n\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv, VecMonitor\nfrom stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3 import SAC", "from stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3 import SAC\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common.callbacks import EvalCallback\nfrom stable_baselines3.common.logger import configure\n\n\nfrom action_constrained_rl.env_wrapper import ConstraintEnvWrapper\nfrom action_constrained_rl.env_wrapper import MemorizeCenterEnvWrapper\nfrom action_constrained_rl.ddpg.projection_ddpg import ProjectionDDPG", "from action_constrained_rl.env_wrapper import MemorizeCenterEnvWrapper\nfrom action_constrained_rl.ddpg.projection_ddpg import ProjectionDDPG\nfrom action_constrained_rl.td3.projection_td3 import ProjectionTD3\nfrom action_constrained_rl.sac.projection_sac import ProjectionSAC\nfrom action_constrained_rl.ddpg.noise_insertion_ddpg import NoiseInsertionDDPG\nfrom action_constrained_rl.ddpg.logging_gradient import LoggingGradientDDPG\nfrom action_constrained_rl.ddpg.logging_gradient import DDPGWithOutputPenalty\nfrom action_constrained_rl.ddpg.nfwpo import NFWPO\nfrom action_constrained_rl.ddpg.ddpg_with_penalty import DDPGWithPenalty\nfrom action_constrained_rl.td3.td3_with_penalty import TD3WithPenalty", "from action_constrained_rl.ddpg.ddpg_with_penalty import DDPGWithPenalty\nfrom action_constrained_rl.td3.td3_with_penalty import TD3WithPenalty\nfrom action_constrained_rl.td3.td3_output_penalty import TD3WithOutputPenalty\nfrom action_constrained_rl.td3.noise_insertion_td3 import NoiseInsertionTD3\nfrom action_constrained_rl.sac.logging_gradient import LoggingGradientSAC\nfrom action_constrained_rl.sac.logging_gradient import SACWithOutputPenalty\nfrom action_constrained_rl.sac.safe_sampling_sac import SafeSamplingSAC\nfrom action_constrained_rl.nn.opt_layer.opt_layer import OptLayer\nfrom action_constrained_rl.nn.opt_layer.opt_layer_policy import OptLayerPolicy\nfrom action_constrained_rl.nn.additional_layers.alpha_projection import AlphaProjectionLayer", "from action_constrained_rl.nn.opt_layer.opt_layer_policy import OptLayerPolicy\nfrom action_constrained_rl.nn.additional_layers.alpha_projection import AlphaProjectionLayer\nfrom action_constrained_rl.nn.additional_layers.radial_squash import SquashLayer\nfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaGaussianDistribution\nfrom action_constrained_rl.nn.additional_layers.alpha_distribution import AlphaStateDependentNoiseDistribution\nfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedGaussianDistribution\nfrom action_constrained_rl.nn.additional_layers.shrinked_distribution import ShrinkedStateDependentNoiseDistribution\nfrom action_constrained_rl.nn.additional_layer_policy import AdditionalLayerPolicy\nfrom action_constrained_rl.nn.additional_layer_sac_policy import AdditionalLayerSACPolicy\nfrom action_constrained_rl.utils.constant_function import ConstantFunction", "from action_constrained_rl.nn.additional_layer_sac_policy import AdditionalLayerSACPolicy\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\nfrom action_constrained_rl.utils.arithmatic_series import ArithmaticSeries\nfrom action_constrained_rl.utils.geometric_series import GeometricSeries\nfrom action_constrained_rl.utils.log_series import LogSeries\nfrom action_constrained_rl.constraint.box_constraint import BoxConstraint\nfrom action_constrained_rl.constraint.power_constraint import PowerConstraint\nfrom action_constrained_rl.constraint.power_constraint import OrthoplexConstraint\nfrom action_constrained_rl.constraint.power_constraint import DecelerationConstraint\nfrom action_constrained_rl.constraint.sphere_constraint import SphericalConstraint", "from action_constrained_rl.constraint.power_constraint import DecelerationConstraint\nfrom action_constrained_rl.constraint.sphere_constraint import SphericalConstraint\nfrom action_constrained_rl.constraint.tip_constraint import TipConstraint\nfrom action_constrained_rl.constraint.MA_constraint import MAConstraint\nfrom action_constrained_rl.constraint.combined_constraint import CombinedConstraint\nfrom action_constrained_rl.constraint.sin2_constraint import Sin2Constraint\n\nimport gurobipy as gp\ngp.setParam('OutputFlag', 0)\n", "gp.setParam('OutputFlag', 0)\n\n\n\ndef nameToConstraint(args):\n    name = args.env\n    c_name = args.constraint\n\n    if c_name == \"Box\" or c_name == \"Power\" or c_name == \"Orthoplex\" or c_name == \"Deceleration\" or c_name == \"Sphere\":\n        if name == \"Hopper-v3\":\n            offset = 8\n            scale = (1, 1, 1)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 11\n        elif name == \"ReacherPyBulletEnv-v0\":\n            offset = 6\n            scale = (1, 1)\n            indices = [6, 8]\n            s_dim = 9\n        elif name == 'Ant-v3':\n            offset = 19\n            scale = (1.,1.,1.,1.,1.,1.,1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 27\n        elif name == 'HalfCheetah-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n        elif name == 'Swimmer-v3':\n            offset = 6\n            scale = (1.,1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 8\n        elif name == 'Walker2d-v3':\n            offset = 11\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices = list(range(offset, offset+len(scale)))\n            s_dim = 17\n\n        if c_name == \"Box\":\n            return BoxConstraint(len(scale)) # R+N\n        elif c_name == \"Power\":\n            return PowerConstraint(indices, scale, args.max_power, s_dim) # R+M, H+M, W+M\n        elif c_name == \"Orthoplex\":\n            return OrthoplexConstraint(indices, scale, args.max_power, s_dim) # R+O03, R+O10, R+O30\n        elif c_name == \"Deceleration\":\n            return DecelerationConstraint(indices, scale, args.max_power, s_dim) #unused\n        elif c_name == \"Sphere\":\n            return SphericalConstraint(len(scale), args.max_power) #R+L2\n        \n    elif c_name == 'Tip':\n        return TipConstraint(args.max_power)# R+T\n    elif c_name == 'MA':\n        return MAConstraint(args.max_power) # HC+MA\n    elif c_name == 'O+S':\n        if name == \"Hopper-v3\":\n            offset_p = 2\n            scale = (1, 1, 1)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 8\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 11\n        elif name == 'Walker2d-v3':\n            offset_p = 2\n            scale = (1., 1., 1., 1., 1., 1.)\n            indices_p = list(range(offset_p, offset_p+len(scale)))\n            offset_v = 11\n            indices_v = list(range(offset_v, offset_v+len(scale)))\n            s_dim = 17\n        return CombinedConstraint(OrthoplexConstraint(indices_v, scale, args.max_power[0], s_dim),\n                                  Sin2Constraint(indices_p, args.max_power[1], s_dim)) # H+O+S, W+O+S\n    else:\n        raise", "\ndef nameToEnv(name, seed=0):\n    env = gym.make(name)\n    env.seed(seed)\n    return env\n\n# unused\ndef pickConstraintCoefficient(args):\n    if args.d > 0.0:\n        constraint_penalty = ArithmaticSeries(args.a0, args.d)\n    elif args.r > 0.0:\n        constraint_penalty = GeometricSeries(args.a0, args.r)\n    elif args.use_log_series:\n        constraint_penalty = LogSeries(args.a0)\n    else:\n        constraint_penalty = ConstantFunction(args.c)\n    return constraint_penalty", "\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--log_dir\", action=\"store\", default=\"tmp\")\nparser.add_argument(\"--prob_id\", action=\"store\", default = \"\");\nparser.add_argument(\"--algo_id\", action=\"store\", default = \"\");\nparser.add_argument(\"--env\", action=\"store\", default=\"HalfCheetahDynamic\")\nparser.add_argument(\"--constraint\", action=\"store\", default=\"Normal\")\nparser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\nparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])", "parser.add_argument(\"--max_power\", action=\"store\", default=1., type=float)\nparser.add_argument(\"--solver\", action=\"store\", default=\"TD3\", choices=[\"DDPG\", \"SAC\", \"TD3\"])\nparser.add_argument(\"--num_time_steps\", action=\"store\", default=1e6, type=int)\nparser.add_argument(\"--batch_size\", action=\"store\", default=100, type=int)\nparser.add_argument(\"--use_env_wrapper\", action=\"store_true\", default=False, help=\"use projection inside environments\")\nparser.add_argument(\"--use_action_restriction\", action=\"store_true\", default=False)\nparser.add_argument(\"--sigma\", action=\"store\", default=0.01, type=float, help=\"stddev for Gaussian Action Noise\")\nparser.add_argument(\"--a0\", action=\"store\", default=0.001, type=float)\nparser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\nparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)", "parser.add_argument(\"--dual_learning_rate\", action=\"store\", default=0.0, type=float)\nparser.add_argument(\"--learning_rate\", action=\"store\", default=1e-3, type=float)\nparser.add_argument(\"--init_ent_coef\", action=\"store\", default=2.0, type=float)\nparser.add_argument(\"--n\", action=\"store\", default=1, type=int, help=\"Update the penalty coefficient c every n episodes\")\nparser.add_argument(\"--verbose\", action=\"store\", default=1, type=int)\nparser.add_argument(\"--seed\", action=\"store\", default=0, type=int)\nparser.add_argument(\"--eval_freq\", action=\"store\", default=5000, type=int, help=\"run evaluation episodes every eval_freq time steps\")\nparser.add_argument(\"--n_eval_episodes\", action=\"store\", default=5, type=int)\nparser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\nparser.add_argument(\"--device\", action=\"store\", default='auto')", "parser.add_argument(\"--normalize_constraint\", action=\"store_true\", default=False)\nparser.add_argument(\"--device\", action=\"store\", default='auto')\nparser.add_argument(\"--squash_output\", action=\"store_true\", default=False)\nparser.add_argument(\"--use_my_mlppolicy\", action=\"store_true\", default=False)\nparser.add_argument(\"--infinity_action_space\", action=\"store_true\", default=False)\nparser.add_argument(\"--use_NFWPO\", action=\"store_true\", default=False)\nparser.add_argument(\"--fw_learning_rate\", action=\"store\", default=0.01, type=float)\nparser.add_argument(\"--logging_gradient\", action=\"store\", default=True, type=bool)\nparser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--output_stdout\", action=\"store_true\", default=False)\n\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--c\", action=\"store\", default=0.0, type=float, help=\"Constant penalty coefficient\")\ngroup.add_argument(\"--d\", action=\"store\", default=-1.0, type=float, help=\"add d to the penalty coefficient\")\ngroup.add_argument(\"--r\", action=\"store\", default=-1.0, type=float)\ngroup.add_argument(\"--use_log_series\", action=\"store_true\", default=False)\n\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)", "group = parser.add_mutually_exclusive_group()\ngroup.add_argument(\"--use_static_constraint_net\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_opt_layer\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_alpha_projection_layer\", action=\"store_true\", default=False)\ngroup.add_argument(\"--use_squash_layer\", action=\"store_true\", default=False)\n\nparser.add_argument(\"--proj_type\", action=\"store\", default=\"QP\", choices=[\"QP\", \"alpha\", \"squash\"])\nargs = parser.parse_args()\n\n", "\n\n# from problem id, set problem arguments\nif args.prob_id != \"\":\n    if args.prob_id == \"R+N\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Box\"\n    elif args.prob_id == \"R+L2\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Sphere\"\n        args.max_power = 0.05\n    elif args.prob_id == \"R+O03\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 0.3\n    elif args.prob_id == \"R+O10\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 1.0\n    elif args.prob_id == \"R+O30\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 3.0\n    elif args.prob_id == \"R+M\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Power\"\n        args.max_power = 1.0 \n    elif args.prob_id == \"R+T\":\n        args.env = \"ReacherPyBulletEnv-v0\"\n        args.constraint = \"Tip\"\n        args.max_power = 0.05 \n    elif args.prob_id == \"HC+O\" or args.prob_id == \"HC+O-16\":\n        args.env = \"HalfCheetah-v3\"\n        args.constraint = \"Orthoplex\"\n        args.max_power = 20.\n    elif args.prob_id == \"H+M\" or args.prob_id == \"H+M-16\":\n        args.env = \"Hopper-v3\"\n        args.constraint = \"Power\"\n        args.max_power = 10.\n    elif args.prob_id == \"W+M\" or args.prob_id == \"W+M-16\":\n        args.env = \"Walker2d-v3\"\n        args.constraint = \"Power\"\n        args.max_power = 10.\n    elif args.prob_id == \"HC+MA\":\n        args.env = \"HalfCheetah-v3\"\n        args.constraint = \"MA\"\n        args.max_power = 5.\n    elif args.prob_id == \"H+O+S\":\n        args.env = \"Hopper-v3\"\n        args.constraint = \"O+S\"\n        args.max_power = (10., 0.1)\n    elif args.prob_id == \"W+O+S\":\n        args.env = \"Walker2d-v3\"\n        args.constraint = \"O+S\"\n        args.max_power = (10., 0.1)\n    else: raise ValueError(\"unknown problem id\")", "\n# from algorithm id, set algorithm arguments\nif args.algo_id != \"\":\n    if args.algo_id == \"DPro\":\n        args.use_action_restriction = True\n    elif args.algo_id == \"DPro+\":\n        args.use_action_restriction = True\n        args.c = 1.\n    elif args.algo_id == \"DPre\":\n        args.use_env_wrapper = True\n    elif args.algo_id == \"DPre+\":\n        args.use_env_wrapper = True\n        args.c = 1.\n    elif args.algo_id == \"DOpt\":\n        args.use_opt_layer = True\n        args.squash_output = True\n    elif args.algo_id == \"DOpt+\":\n        args.use_opt_layer = True\n        args.squash_output = True\n        args.c = 1.\n    elif args.algo_id == \"NFW\":\n        args.use_NFWPO = True\n    elif args.algo_id == \"DAlpha\":\n        args.use_alpha_projection_layer = True\n    elif args.algo_id == \"DRad\":\n        args.use_squash_layer = True\n    elif args.algo_id == \"SPre\":\n        args.use_env_wrapper = True\n        args.solver = \"SAC\"\n    elif args.algo_id == \"SPre+\":\n        args.use_env_wrapper = True\n        args.solver = \"SAC\"\n        args.c = 1.\n    elif args.algo_id == \"SAlpha\":\n        args.use_alpha_projection_layer = True\n        args.solver = \"SAC\"\n    elif args.algo_id == \"SRad\":\n        args.use_squash_layer = True\n        args.solver = \"SAC\"\n    else:\n        raise ValueError(\"unknown algo id\")", "\nif args.proj_type == \"squash\":\n    assert args.infinity_action_space\nif args.use_squash_layer:\n    assert not args.squash_output\nif args.use_opt_layer:\n    assert args.squash_output\n\nlog_dir = args.log_dir\nos.makedirs(log_dir, exist_ok=True)\nif not args.output_stdout:\n    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")", "log_dir = args.log_dir\nos.makedirs(log_dir, exist_ok=True)\nif not args.output_stdout:\n    sys.stdout = open(log_dir+\"/log.txt\", \"w\")\n    sys.stderr = open(log_dir+\"/error_log.txt\", \"w\")\nprint(args)\nwith open(f'{log_dir}/commandline_args.txt', 'w') as f:\n    json.dump(args.__dict__, f, indent=2)\n\nenv = nameToEnv(args.env, args.seed)", "\nenv = nameToEnv(args.env, args.seed)\nconstraint = nameToConstraint(args)\nconstraint.proj_type = args.proj_type\nconstraint_penalty = pickConstraintCoefficient(args) # penalty coefficient function for output penalty\n\nif args.use_alpha_projection_layer or args.use_squash_layer: # wrapper to memorize the centers\n    EnvWrapper = MemorizeCenterEnvWrapper\n    env = EnvWrapper(constraint, env, n=args.n, dual_learning_rate=args.dual_learning_rate)\n    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/monitor.csv\")\n    eval_env = EnvWrapper(constraint, nameToEnv(args.env, args.seed), n=args.n, dual_learning_rate=args.dual_learning_rate)\n    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)\nelse:  # wrapper to project actions. We do not use reward penalty\n    env = ConstraintEnvWrapper(constraint, env, constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=log_dir + \"/monitor.csv\", n=args.n, dual_learning_rate=args.dual_learning_rate, normalize=args.normalize_constraint, infinity_action_space = args.infinity_action_space)\n    env = VecMonitor(DummyVecEnv([lambda: env]), filename=log_dir + \"/vec_monitor.csv\")\n    eval_env = ConstraintEnvWrapper(constraint, nameToEnv(args.env, args.seed), constraint_penalty=ConstantFunction(0), enforce_constraint=args.use_env_wrapper or args.use_action_restriction, filename=None, n=args.n, infinity_action_space = args.infinity_action_space)\n    eval_env = VecMonitor(DummyVecEnv([lambda: eval_env]), filename=None)", "    \neval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n                             log_path=log_dir, eval_freq=args.eval_freq, n_eval_episodes = args.n_eval_episodes,\n                         deterministic=True, render=False)\n\n# set rl-zoo hyperparameters\nn_actions = env.action_space.shape[-1]\nif args.env == \"ReacherPyBulletEnv-v0\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 3e5\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"gamma\": 0.98, \"buffer_size\": 200000, \"learning_starts\": 10000,\n                 \"action_noise\": action_noise, \"gradient_steps\": -1, \"train_freq\": (1, \"episode\"),\n                 \"learning_rate\": 1e-3, \"policy_kwargs\": {\"net_arch\":[400, 300]}}\n    elif args.solver == \"SAC\":\n        n_timesteps = 3e5\n        kargs = {\"learning_rate\": 7.3e-4, \"buffer_size\": 300000, \"batch_size\": 256,\n                 \"ent_coef\": 'auto', \"gamma\": 0.98, \"tau\": 0.02, \"train_freq\": 8,\n                 \"gradient_steps\": 8, \"learning_starts\": 10000,\n                 \"use_sde\": True, \"policy_kwargs\": dict(log_std_init=-3, net_arch=[400, 300])}\nelif args.env == \"HalfCheetah-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelif args.env == \"Hopper-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise, \"train_freq\": 1,\n                 \"gradient_steps\": 1, \"learning_rate\": 3e-4, \"batch_size\": 256}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelif args.env == \"Walker2d-v3\":\n    if args.solver == \"TD3\" or args.use_NFWPO:\n        n_timesteps = 1e6\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n        kargs = {\"learning_starts\": 10000, \"action_noise\": action_noise}\n    elif args.solver == \"SAC\":\n        n_timesteps = 1e6\n        kargs = {\"learning_starts\": 10000}\nelse:\n    raise", "kargs[\"verbose\"]=args.verbose\nif not \"policy_kwargs\" in kargs:\n    kargs[\"policy_kwargs\"]={}\nif args.prob_id[-3:] == \"-16\":\n    print(\"batch_size: 16\")\n    kargs.update({\"batch_size\": 16})\n\ndef pickModel(constraint):\n    # select model according to arguments\n    seed = args.seed\n    \n    if args.use_NFWPO: #NFW\n        if args.prob_id[:2] == \"R+\":\n            fw_learning_rate = 0.05\n        else:\n            fw_learning_rate = 0.01\n        model = NFWPO(constraint, \"MlpPolicy\", env, fw_learning_rate = fw_learning_rate,\n                      device = args.device, seed = seed, **kargs)\n\n    elif args.use_action_restriction: #DPro, DPro+\n        if args.solver == \"DDPG\":\n            algo = ProjectionDDPG\n        elif args.solver == \"TD3\":\n            algo = ProjectionTD3\n        elif args.solver == \"SAC\":\n            algo = ProjectionSAC\n        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n\n    elif args.use_alpha_projection_layer or args.use_squash_layer: # DAlpha, DRad, SAlpha, SRad\n        kargs[\"policy_kwargs\"].update({\"constraint\": constraint})\n        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n            if args.solver == \"DDPG\":\n                algo = NoiseInsertionDDPG\n            else:\n                algo = NoiseInsertionTD3\n            policy = AdditionalLayerPolicy\n            if args.use_alpha_projection_layer:\n                layer_type = AlphaProjectionLayer\n            elif args.use_squash_layer:\n                layer_type = SquashLayer\n            kargs[\"policy_kwargs\"].update({\"layer_type\": layer_type, \"squash_output\": args.squash_output})\n        else:\n            algo = SafeSamplingSAC\n            action_noise = None\n            policy = AdditionalLayerSACPolicy\n            if args.use_alpha_projection_layer:\n                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n                    distribution_class = AlphaStateDependentNoiseDistribution\n                else:\n                    distribution_class = AlphaGaussianDistribution\n            if args.use_squash_layer:\n                if \"use_sde\" in kargs and kargs[\"use_sde\"]:\n                    distribution_class = ShrinkedStateDependentNoiseDistribution\n                else:\n                    distribution_class = ShrinkedGaussianDistribution\n            kargs[\"policy_kwargs\"].update({\"distribution_class\": distribution_class})\n        model = algo(policy, env, device = args.device, seed = seed, **kargs)\n    elif args.use_opt_layer: # DOpt, DOpt+\n        if args.solver == \"DDPG\" or args.solver == \"TD3\":\n            if args.solver == \"DDPG\":\n                #algo = NoiseInsertionDDPG\n                algo = DDPGWithPenalty\n            else:\n                algo = TD3WithPenalty\n        else:\n            algo = SafeSamplingSAC\n            action_noise = None\n        kargs[\"policy_kwargs\"].update({\"constraint\": constraint, \"squash_output\": args.squash_output})\n        model = algo(constraint, OptLayerPolicy, env, use_center_wrapper = False, constraint_penalty = constraint_penalty, device = args.device, seed = seed, **kargs)\n    else:\n        if args.solver == \"DDPG\":\n            algo = DDPGWithOutputPenalty\n        elif args.solver == \"TD3\":\n            algo = TD3WithOutputPenalty # DPre, DPre+\n        elif args.solver == \"SAC\":\n            algo = SACWithOutputPenalty # SPre, SPre+\n        model = algo(constraint, \"MlpPolicy\", env, constraint_penalty = constraint_penalty,  device = args.device, seed = seed, **kargs)\n    return model", "\nmodel = pickModel(constraint)\nif args.logging_gradient:\n    logger = configure(args.log_dir)\n    model.set_logger(logger)\nmodel.learn(total_timesteps=n_timesteps, callback=eval_callback)\n\ndel model # remove to demonstrate saving and loading\n", ""]}
{"filename": "action_constrained_rl/cvxpy_variables.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nclass CVXPYVariables:\n    \"\"\"\n    class to store cvxpy proplem data\n    \"\"\"\n    def __init__(self, x, q, s, cons, obj, prob):\n        self.x = x\n        self.q = q\n        self.s = s\n        self.cons = cons\n        self.obj = obj\n        self.prob = prob", ""]}
{"filename": "action_constrained_rl/utils/constant_function.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nclass ConstantFunction:\n    def __init__(self, x):\n        self.x = x\n\n    def __call__(self, t):\n        return self.x\n", ""]}
{"filename": "action_constrained_rl/utils/arithmatic_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nclass ArithmaticSeries:\n    def __init__(self, a0, d):\n        self.a0 = a0\n        self.d = d\n\n    def __call__(self, t):\n        assert(t > 0)\n        return self.a0 + (t-1) * self.d", "\nif __name__ == \"__main__\":\n    a0 = 0.001\n    d = 0.02\n    a = ArithmaticSeries(a0, d)\n    print(a(1))\n    print(a(2))\n    print(a(100))\n", ""]}
{"filename": "action_constrained_rl/utils/geometric_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nclass GeometricSeries:\n    def __init__(self, a0, gamma):\n        self.a0 = a0\n        self.gamma = gamma\n\n    def __call__(self, t):\n        assert(t > 0)\n        return self.a0 * self.gamma ** (t-1)", "\nif __name__ == \"__main__\":\n    a0 = 1.0\n    d = 2.0\n    a = GeometricSeries(a0, d)\n    print(a(1))\n    print(a(2))\n    print(a(10))\n", ""]}
{"filename": "action_constrained_rl/utils/log_series.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nclass LogSeries:\n    def __init__(self, a0):\n        self.a0 = a0\n\n    def __call__(self, t):\n        assert(t > 0)\n        return self.a0 + np.log(t)", "\nif __name__ == \"__main__\":\n    a0 = 0.1\n    a = LogSeries(a0)\n    print(a(1))\n    print(a(2))\n    print(a(3))\n    print(a(100))\n    print(a(1000))\n", ""]}
{"filename": "action_constrained_rl/sac/projection_sac.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.utils import polyak_update", "from stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3 import SAC\n\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass ProjectionSAC(SAC):\n    \"\"\"\n    Unused\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n    \n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n    \n    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n        # Update optimizers learning rate\n        optimizers = [self.actor.optimizer, self.critic.optimizer]\n        if self.ent_coef_optimizer is not None:\n            optimizers += [self.ent_coef_optimizer]\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate(optimizers)\n\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            if self.use_sde:\n                self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.ent_coef_optimizer is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = th.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.ent_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None:\n                self.ent_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.ent_coef_optimizer.step()\n\n            with th.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n            # Mean over all critic networks\n            actions_pi.retain_grad()\n            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\n            # calculate penaty\n            actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, actions_pi).mean()\n            \n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\n            # Logging gradients\n            for tag, value in self.actor.named_parameters():\n                if value.grad is not None:\n                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n            action_grad = actions_pi.grad.norm(dim=1).sum()\n            self.logger.record(\"train/action_grad\", action_grad.item())\n            \n        self._n_updates += gradient_steps\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n        if len(ent_coef_losses) > 0:\n            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))", "\nclass ProjectionSAC(SAC):\n    \"\"\"\n    Unused\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n    \n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n    \n    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n        # Update optimizers learning rate\n        optimizers = [self.actor.optimizer, self.critic.optimizer]\n        if self.ent_coef_optimizer is not None:\n            optimizers += [self.ent_coef_optimizer]\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate(optimizers)\n\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            if self.use_sde:\n                self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.ent_coef_optimizer is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = th.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.ent_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None:\n                self.ent_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.ent_coef_optimizer.step()\n\n            with th.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n            # Mean over all critic networks\n            actions_pi.retain_grad()\n            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\n            # calculate penaty\n            actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, actions_pi).mean()\n            \n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\n            # Logging gradients\n            for tag, value in self.actor.named_parameters():\n                if value.grad is not None:\n                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n            action_grad = actions_pi.grad.norm(dim=1).sum()\n            self.logger.record(\"train/action_grad\", action_grad.item())\n            \n        self._n_updates += gradient_steps\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n        if len(ent_coef_losses) > 0:\n            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))", ""]}
{"filename": "action_constrained_rl/sac/safe_sampling_sac.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport math\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import SAC", "from stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import SAC\nfrom stable_baselines3.common.preprocessing import get_action_dim\n\nfrom .logging_gradient import LoggingGradientSAC\n\nclass SafeSamplingSAC(LoggingGradientSAC):\n    \"\"\"\n    modified SAC to project random sampled actions\n    \"\"\"\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n            scaled_action = self.policy.scale_action(unscaled_action)\n            action_dim = get_action_dim(self.action_space)\n            scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-action_dim], self._last_obs[i,-action_dim:], scaled_action[i]) for i in range(n_envs)])\n\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n            scaled_action = self.policy.scale_action(unscaled_action)\n            buffer_action = scaled_action\n            action = unscaled_action\n\n        return action, buffer_action", ""]}
{"filename": "action_constrained_rl/sac/logging_gradient.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3 import SAC\n\nclass LoggingGradientSAC(SAC):\n    \"\"\"\n    modified SAC to log gradient\n    \"\"\"\n    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n        # Update optimizers learning rate\n        optimizers = [self.actor.optimizer, self.critic.optimizer]\n        if self.ent_coef_optimizer is not None:\n            optimizers += [self.ent_coef_optimizer]\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate(optimizers)\n\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            if self.use_sde:\n                self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.ent_coef_optimizer is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = th.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.ent_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None:\n                self.ent_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.ent_coef_optimizer.step()\n\n            with th.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n            # Mean over all critic networks\n            actions_pi.retain_grad()\n            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\n            # Logging gradients\n            for tag, value in self.actor.named_parameters():\n                if value.grad is not None:\n                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n            action_grad = actions_pi.grad.norm(dim=1).sum()\n            self.logger.record(\"train/action_grad\", action_grad.item())\n            \n        self._n_updates += gradient_steps\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n        if len(ent_coef_losses) > 0:\n            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))", "\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass SACWithOutputPenalty(SAC):\n    \"\"\"\n    modified SAC to add penalty to violation of constraints\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.constraint_penalty = constraint_penalty\n        self.penalty_coeff = constraint_penalty\n    \n    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n        # Update optimizers learning rate\n        optimizers = [self.actor.optimizer, self.critic.optimizer]\n        if self.ent_coef_optimizer is not None:\n            optimizers += [self.ent_coef_optimizer]\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate(optimizers)\n\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            if self.use_sde:\n                self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.ent_coef_optimizer is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = th.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.ent_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None:\n                self.ent_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.ent_coef_optimizer.step()\n\n            with th.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n            # Mean over all critic networks\n            actions_pi.retain_grad()\n            q_values_pi = th.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = th.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\n            # calculate penaty\n            actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, actions_pi).mean()\n            \n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n\n            # Logging gradients\n            for tag, value in self.actor.named_parameters():\n                if value.grad is not None:\n                    self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n            action_grad = actions_pi.grad.norm(dim=1).sum()\n            self.logger.record(\"train/action_grad\", action_grad.item())\n            \n        self._n_updates += gradient_steps\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        self.logger.record(\"train/ent_coef\", np.mean(ent_coefs))\n        self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n        if len(ent_coef_losses) > 0:\n            self.logger.record(\"train/ent_coef_loss\", np.mean(ent_coef_losses))", ""]}
{"filename": "action_constrained_rl/ddpg/ddpg_with_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import DDPG\nfrom .noise_insertion_ddpg import NoiseInsertionDDPG\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass DDPGWithPenalty(NoiseInsertionDDPG):\n    \"\"\"\n    modified DDPG to add penalty to violation of constraints of outputs before final layer\n    This class is used for DOpt+\n    \"\"\"\n    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n        super(DDPGWithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n                \n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nclass DDPGWithPenalty(NoiseInsertionDDPG):\n    \"\"\"\n    modified DDPG to add penalty to violation of constraints of outputs before final layer\n    This class is used for DOpt+\n    \"\"\"\n    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n        super(DDPGWithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n                \n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "action_constrained_rl/ddpg/noise_insertion_ddpg.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nimport numpy as np\nimport math\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.preprocessing import get_action_dim", "from stable_baselines3 import DDPG\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom .logging_gradient import LoggingGradientDDPG\n\nclass NoiseInsertionDDPG(LoggingGradientDDPG):\n    \"\"\"\n    DDPG to project random samplied actions and to add noise before final layer\n    \"\"\"\n    def __init__(self, *args, use_center_wrapper:bool = True, **kwargs):\n        super(NoiseInsertionDDPG, self).__init__(*args, **kwargs)\n        self.action_dim = get_action_dim(self.action_space)\n        self.use_center_wrapper = use_center_wrapper\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n            scaled_action = self.policy.scale_action(unscaled_action)\n            # project actions\n            if self.use_center_wrapper: # use alpha projection\n                scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-self.action_dim], self._last_obs[i,-self.action_dim:], scaled_action[i]) for i in range(n_envs)])\n            else: # use closest-point projection\n                scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n        else:\n            scaled_action = self.policy.actor.undeformed_predict(self._last_obs) # output before final layer\n            # Add noise to the action (improve exploration)\n\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n            \n            # Deform action by final layer\n            scaled_action = self.policy.actor.deform_action(scaled_action, self._last_obs)\n\n        buffer_action = scaled_action\n        action = self.policy.unscale_action(scaled_action)\n\n        return action, buffer_action", ""]}
{"filename": "action_constrained_rl/ddpg/nfwpo.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.utils import get_schedule_fn, update_learning_rate\nfrom stable_baselines3 import TD3\nfrom stable_baselines3.common.type_aliases import Schedule\n\nimport gurobipy as gp", "\nimport gurobipy as gp\n\ndef FW_update(action, state, grad, constraint, lr):\n    ## Solve LP\n\n    a_dim = action.shape[0]\n    with gp.Model() as model:\n        x = []\n        for _ in range(a_dim):\n            x.append(model.addVar(lb=-1, ub =1, vtype = gp.GRB.CONTINUOUS))\n        obj = gp.LinExpr()\n        for i in range(a_dim):\n            obj+=grad[i]*x[i]\n        model.setObjective(obj, sense = gp.GRB.MAXIMIZE)\n        constraint.gp_constraints(model, x, state)\n        model.optimize()\n        x_value = np.array(model.X[0:a_dim])\n\n    return x_value*lr + action * (1-lr)", "\nclass NFWPO(TD3):\n\n    \"\"\"\n    TD3-based implimentation of NFWPO\n    \"\"\"\n\n    def __init__(self, constraint, *args, fw_learning_rate:float = 0.01, actor_learning_rate:float = 1e-3, critic_learning_rate:float = 1e-3, **kargs):\n\n        self.actor_learning_rate = actor_learning_rate\n        self.critic_learning_rate = critic_learning_rate\n        super().__init__(*args, **kargs)\n        self.constraint = constraint\n        self.fw_learning_rate = fw_learning_rate\n\n    def _setup_model(self) -> None:\n        super()._setup_model()\n\n        self.policy.actor.optimizer = self.policy.optimizer_class(self.policy.actor.parameters(), lr=self.actor_lr_schedule(1), **self.policy.optimizer_kwargs)\n        self.policy.critic.optimizer = self.policy.optimizer_class(self.policy.critic.parameters(), lr=self.critic_lr_schedule(1), **self.policy.optimizer_kwargs)\n\n        \n    def _setup_lr_schedule(self) -> None:\n        \"\"\"Transform to callable if needed.\"\"\"\n        self.actor_lr_schedule = get_schedule_fn(self.actor_learning_rate)\n        self.critic_lr_schedule = get_schedule_fn(self.critic_learning_rate)\n\n        self.lr_schedule = self.actor_lr_schedule ## dummy\n\n    def _update_learning_rate(self, optimizers: List[th.optim.Optimizer]) -> None:\n        \"\"\"\n        Update the optimizers learning rate using the current learning rate schedule\n        and the current progress remaining (from 1 to 0).\n        :param optimizers:\n            a list of optimizers.\n        \"\"\"\n        # Log the current learning rate\n        self.logger.record(\"train/actor_learning_rate\", self.actor_lr_schedule(self._current_progress_remaining))\n        self.logger.record(\"train/critic_learning_rate\", self.critic_lr_schedule(self._current_progress_remaining))\n\n        update_learning_rate(optimizers[0], self.actor_lr_schedule(self._current_progress_remaining))\n        update_learning_rate(optimizers[1], self.critic_lr_schedule(self._current_progress_remaining))\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                lr = self.fw_learning_rate\n\n                outputs = self.actor(replay_data.observations)\n\n                # Let actions in constraints\n                actions = outputs.cpu().detach().numpy()                \n                states = replay_data.observations.cpu().detach().numpy()\n                for i in range(batch_size):\n                    actions[i] = self.constraint.enforceConstraintIfNeed(states[i], actions[i])\n        \n                # Compute Q value grad\n                action_tensors = th.tensor(actions, device = outputs.device, dtype = outputs.dtype, requires_grad = True)\n                q_value = self.critic.q1_forward(replay_data.observations, action_tensors).mean()\n                q_value.backward()\n                grads = action_tensors.grad.cpu().detach().numpy()\n\n                # Compute optimized action in CPU\n                action_table = np.zeros(actions.shape)\n                for i in range(batch_size):\n                    action_table[i]=FW_update(actions[i], states[i], grads[i], self.constraint, lr)\n                action_table = th.tensor(action_table, device = outputs.device, dtype = outputs.dtype)\n\n                outputs.retain_grad()\n                actor_loss = F.mse_loss(outputs, action_table)\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().detach().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n            scaled_action = self.policy.scale_action(unscaled_action)\n        else:\n            unscaled_action, _ = self.policy.predict(self._last_obs)\n            scaled_action = self.policy.scale_action(unscaled_action)\n            # Add noise to the action (improve exploration)\n\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n        scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n        \n        buffer_action = scaled_action\n        action = self.policy.unscale_action(scaled_action)\n        return action, buffer_action\n\n    def predict(\n        self,\n        observation: np.ndarray,\n        state: Optional[Tuple[np.ndarray, ...]] = None,\n        episode_start: Optional[np.ndarray] = None,\n        deterministic: bool = False,\n    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n        \"\"\"\n        Get the policy action from an observation (and optional hidden state).\n        Includes sugar-coating to handle different observations (e.g. normalizing images).\n\n        :param observation: the input observation\n        :param state: The last hidden states (can be None, used in recurrent policies)\n        :param episode_start: The last masks (can be None, used in recurrent policies)\n            this correspond to beginning of episodes,\n            where the hidden states of the RNN must be reset.\n        :param deterministic: Whether or not to return deterministic actions.\n        :return: the model's action and the next hidden state\n            (used in recurrent policies)\n        \"\"\"\n        unscaled_action, states = self.policy.predict(observation, state, episode_start, deterministic)\n        scaled_action = self.policy.scale_action(unscaled_action)\n        scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(observation[i], scaled_action[i]) for i in range(len(self.env.envs))])\n        return self.policy.unscale_action(scaled_action), states", "\nif __name__ == \"__main__\":\n    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    import cvxpy as cp\n    cons = HalfCheetahDynamicConstraint()\n    action = np.random.rand(6)\n    state = 10*np.random.rand(17)\n    grad = np.random.rand(6)\n    gp.setParam('OutputFlag', 0)\n    print(action, state[11:], grad)\n    x_value = FW_update(action, state, grad, cons, 1)\n    print(x_value)\n    x = cp.Variable(6)\n    obj = cp.Maximize(grad.T @ x)\n    prob = cp.Problem(obj, cons.cvxpy_constraints(x, state))\n    prob.solve(solver=cp.GUROBI)\n    print(x.value)", ""]}
{"filename": "action_constrained_rl/ddpg/projection_ddpg.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nimport numpy as np\nimport math\nimport gym\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update", "from stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import DDPG\n\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass ProjectionDDPG(DDPG):\n    \"\"\"\n    class for DPro\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nclass ProjectionDDPG(DDPG):\n    \"\"\"\n    class for DPro\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nif __name__ == \"__main__\":\n    import gym\n    import numpy as np\n    import os\n\n    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n    from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n\n    video_folder = 'log/videos/'\n    video_length = 1000\n\n    env = gym.make(\"IdealizedPendulum-v0\", max_speed=8.0, max_torque=2.0, normalization_factor=100.0, l=1.0, initial_state=np.array([0.25 * np.pi, 1.0]), dt=0.01)\n\n#    env = DummyVecEnv([lambda: env])\n#    env = VecVideoRecorder(env, video_folder,\n#                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n#                       name_prefix=f\"ddpg-cgf-pendulum\")\n\n# The noise objects for DDPG\n    n_actions = env.action_space.shape[-1]\n    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n\n    cbf = PendulumCBF(normalization_factor=100.0)\n    model = ProjectionDDPG(cbf, 'MlpPolicy', env, action_noise=action_noise, verbose=1)\n    model.learn(total_timesteps=1000)\n    env = model.get_env()\n\n    del model # remove to demonstrate saving and loading", ""]}
{"filename": "action_constrained_rl/ddpg/logging_gradient.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import DDPG\n\nclass LoggingGradientDDPG(DDPG):\n    \"\"\"\n    modified DDPG to log gradient\n    \"\"\"\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass DDPGWithOutputPenalty(DDPG):\n    \"\"\"\n    modified DDPG to add penalty to violation of constraints\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n    \n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "action_constrained_rl/env_wrapper/memorize_center_env_wrapper.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport gym\nimport numpy as np\nfrom gym import spaces\nimport math\nimport time\nfrom stable_baselines3.common.monitor import ResultsWriter\n", "from stable_baselines3.common.monitor import ResultsWriter\n\n\nclass MemorizeCenterEnvWrapper(gym.Wrapper):\n    \"\"\"\n    env wrapper to calculate centers of action spaces and to concatenate it with observations\n    \"\"\"\n    def __init__(self, constraint, env, n=1, dual_learning_rate=0.0):\n        super().__init__(env)\n        self.env = env\n        self.constraint = constraint\n        self.n=n\n        self.dual_learning_rate = dual_learning_rate\n        act_space = self.env.action_space\n        obs_space = self.env.observation_space\n        self.observation_space = gym.spaces.Box(low = np.concatenate((obs_space.low, act_space.low)),\n                                                high  = np.concatenate((obs_space.high, act_space.high)),\n                                                shape = (obs_space.shape[0] + act_space.shape[0],),\n                                                dtype = obs_space.dtype)\n\n        \n    def observation(self, obs):\n        return np.concatenate((obs, self.constraint.get_center(obs)))\n\n    def reset(self, **kwargs):\n        \"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\n        self.prev_obs = self.env.reset(**kwargs)\n        return self.observation(self.prev_obs)\n\n    def step(self, action):\n        state = self.prev_obs.squeeze()\n        \n        assert self.constraint.isConstraintSatisfied(state, action), f\"constraint violated state={state} action{action} \"\n        \n        next_state, reward, done, info = self.env.step(action)\n\n        self.prev_obs = next_state\n\n        return self.observation(next_state), reward, done, info", ""]}
{"filename": "action_constrained_rl/env_wrapper/__init__.py", "chunked_list": ["from .constraint_wrapper import ConstraintEnvWrapper\nfrom .memorize_center_env_wrapper import MemorizeCenterEnvWrapper\n"]}
{"filename": "action_constrained_rl/env_wrapper/constraint_wrapper.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport gym\nimport numpy as np\nfrom gym import spaces\nimport math\nimport time\nfrom stable_baselines3.common.monitor import ResultsWriter\nfrom ..utils.constant_function import ConstantFunction", "from stable_baselines3.common.monitor import ResultsWriter\nfrom ..utils.constant_function import ConstantFunction\n\ndef quadraticPenalty(x):\n    return np.sum(np.square(x))\n\nclass ConstraintEnvWrapper(gym.Wrapper):\n    \"\"\"\n    wrapper to project actions\n    \"\"\"\n    def __init__(self, constraint, env, constraint_penalty=ConstantFunction(0.0), enforce_constraint=False, filename=None, n=1, dual_learning_rate=0.0, normalize=False, infinity_action_space = False):\n        super().__init__(env)\n        self.env = env\n        self.constraint = constraint\n        self.prev_obs = None\n        self.constraint_penalty = constraint_penalty\n        self.num_pre_projection_constraint_violation = 0\n        self.enforce_constraint = enforce_constraint\n        self.dual_learning_rate = dual_learning_rate\n        self.n = n\n        self.normalize = normalize\n\n        self.t_start = time.time()\n        if filename is not None:\n            self.results_writer = ResultsWriter(\n                filename,\n                header={\"t_start\": self.t_start, \"env_id\": env.spec and env.spec.id},\n                extra_keys=(\"v\",\"c\",\"sum_term\",\"quadratic_sum_term\", \"quadratic_sum_term_normalized\")\n            )\n        else:\n            self.results_writer = None\n        self.rewards = None\n        self.violations = None\n        self.quadratic_sum_violations = None\n        self.quadratic_sum_violations_normalized = None\n        self.needs_reset = True\n        self.total_steps = 0\n        self.lagrange_multiplier = np.zeros(self.constraint.numConstraints())\n        self.episode =1\n        self.infinity_action_space = infinity_action_space\n        \n    def logConstraintViolation(self, state, action):\n        self.violations.append(self.constraint.constraintViolation(state, action))\n        self.quadratic_sum_violations.append(quadraticPenalty(self.constraint.constraintViolation(state, action)))\n        self.quadratic_sum_violations_normalized.append(quadraticPenalty(self.constraint.constraintViolation(state, action, normalize=True)))\n\n    def step(self, action):\n        state = self.prev_obs.squeeze()\n        \n        penalty = 0.0\n        lagrange_term = 0.0\n\n        if self.infinity_action_space:\n            action = np.arctanh(np.clip(action,-1+1e-6,1-1e-6))\n        self.logConstraintViolation(state, action)\n        if self.enforce_constraint and self.constraint.proj_type == \"shrinkage\":\n            action = self.constraint.enforceConstraint(state, action)\n        elif not self.constraint.isConstraintSatisfied(state, action):\n            self.num_pre_projection_constraint_violation += 1\n\n            if self.enforce_constraint:\n                g = self.constraint.constraintViolation(state, action, normalize=self.normalize)\n                penalty -= self.constraint_penalty(self.episode // self.n + 1) * quadraticPenalty(g)\n                action = self.constraint.enforceConstraint(state, action)\n                \n                #lagrange_term += np.dot(self.lagrange_multiplier, g)\n                \n\n        assert self.constraint.isConstraintSatisfied(state, action), f\"constraint violated state={state} action{action} \"\n\n        next_state, reward, done, info = self.env.step(action)\n        self.rewards.append(reward)\n        if done:\n            if self.episode % self.n == 0:\n                pass\n                #self.lagrange_multiplier = np.minimum(0.0, self.lagrange_multiplier -self.dual_learning_rate * sum(self.violations))\n                #print(\"violations: {}\".format(sum(self.constraint_violations)))\n                #print(\"multiplier: {}\".format(self.lagrange_multiplier))\n            self.needs_reset = True\n            ep_info = self.getEpInfo()\n            if self.results_writer:\n                self.results_writer.write_row(ep_info)\n            info[\"episode\"] = ep_info\n            self.episode += 1\n            \n        self.total_steps += 1\n\n        self.prev_obs = next_state\n        return next_state, reward + lagrange_term + penalty, done, info\n    \n    def getEpInfo(self):\n        ep_rew = sum(self.rewards)\n        ep_len = len(self.rewards)\n        ep_info = {\"r\": round(ep_rew, 6), \"l\": ep_len, \"t\": round(time.time() - self.t_start, 6), \"v\": self.num_pre_projection_constraint_violation, \"c\": round(self.constraint_penalty(self.episode // self.n + 1), 6), \"sum_term\": round(sum(sum(self.violations)), 6), \"quadratic_sum_term\": round(sum(self.quadratic_sum_violations), 6), \"quadratic_sum_term_normalized\": round(sum(self.quadratic_sum_violations_normalized), 6)}\n        return ep_info\n\n    def reset(self, **kwargs):\n        self.rewards = []\n        self.violations = []\n        self.quadratic_sum_violations = []\n        self.quadratic_sum_violations_normalized = []\n        self.num_pre_projection_constraint_violation = 0\n        self.needs_reset = False\n        self.prev_obs = self.env.reset()\n        return self.prev_obs", "\n\nif __name__ == \"__main__\":\n    import math\n    import time\n    import gym\n    from ..idealized_pendulum.cbf import PendulumCBF\n    \n    normalization_factor = 500.0 \n    env = ConstraintEnvWrapper(PendulumCBF(normalization_factor=normalization_factor), gym.make(\"IdealizedPendulum-v0\",  normalization_factor=normalization_factor, dt=0.01, max_speed=1000.0))\n    observation = env.reset()\n    observation = env.setState(np.array([0.25 * np.pi, 1.0]))\n    frames = []\n\n    for _ in range(1000):\n        action = np.random.uniform(-1.0, 1.0)\n        observation, reward, done, info = env.step(np.array([action]))\n        print(observation)\n        env.render()\n        time.sleep(0.03)\n#        frames.append(env.render(\"rgb_array\"))\n\n    env.close()", ""]}
{"filename": "action_constrained_rl/constraint/constraint.py", "chunked_list": ["import numpy as np\nimport torch as th\nimport math\nfrom abc import ABC, abstractmethod\n\nfrom .normalize_constraint import normalizeConstraint\nfrom ..nn.additional_layers.chebyshev_center import calc_chebyshev_center\n\nimport cvxpy as cp\nfrom ..cvxpy_variables import CVXPYVariables", "import cvxpy as cp\nfrom ..cvxpy_variables import CVXPYVariables\n\nimport gurobipy as gp\n\ndef to_tensors(a):\n    return th.tensor(np.expand_dims(a,0))\n\nEPS=1e-9\n\nclass Constraint(ABC):\n    \"\"\"\n    Abstract class for all action constraints\n    \"\"\"\n    def __init__(self, a_dim:int, s_dim:int = -1):\n        self.a_dim = a_dim\n        self.s_dim = s_dim\n    @abstractmethod\n    def isConstraintSatisfied(self, state, a):\n        pass\n\n    def enforceConstraint(self, state, a):\n        with gp.Model() as model:\n            x = []\n            for _ in range(self.a_dim):\n                x.append(model.addVar(lb=-1, ub =1, vtype = gp.GRB.CONTINUOUS))\n            obj = gp.QuadExpr()\n            for i in range(self.a_dim):\n                obj+=(0.5*x[i]-a[i])*x[i]\n            model.setObjective(obj, sense = gp.GRB.MINIMIZE)\n\n            self.gp_constraints(model, x, state)\n            \n            model.optimize()\n\n            x_value = np.array(model.X[0:self.a_dim])\n\n        return x_value\n\n    def enforceConstraintIfNeed(self, state, a):\n        if self.isConstraintSatisfied(state, a):\n            return a\n        else:\n            return self.enforceConstraint(state, a)\n    \n    @abstractmethod\n    def numConstraints(self):\n        pass\n    \n    @abstractmethod\n    def getL(self, states, centers, v, get_grad:bool = False):\n        pass\n\n    @abstractmethod\n    def get_center(self, state):\n        pass\n\n    def project(self, state, center, a):\n        L = self.getL(to_tensors(state), to_tensors(center), to_tensors(a-center)).numpy()[0]\n        return center + (a-center) / max(L, 1)\n\n    @abstractmethod\n    def cvxpy_constraints(self, x, state = None):\n        pass\n\n    @abstractmethod\n    def gp_constraints(self, x, state = None):\n        pass\n\n    def cvxpy_variables(self):\n        q = cp.Parameter(self.a_dim)  # input action\n\n        if self.s_dim > 0:\n            s = cp.Parameter(self.s_dim)  # input: state\n        else:\n            s = None\n\n        x = cp.Variable(self.a_dim)  # output\n        obj = cp.Minimize(0.5*cp.sum_squares(x) - q.T @ x)\n        cons = self.cvxpy_constraints(x, s)\n        prob = cp.Problem(obj, cons)\n        return CVXPYVariables(x, q, s, cons, obj, prob)", "EPS=1e-9\n\nclass Constraint(ABC):\n    \"\"\"\n    Abstract class for all action constraints\n    \"\"\"\n    def __init__(self, a_dim:int, s_dim:int = -1):\n        self.a_dim = a_dim\n        self.s_dim = s_dim\n    @abstractmethod\n    def isConstraintSatisfied(self, state, a):\n        pass\n\n    def enforceConstraint(self, state, a):\n        with gp.Model() as model:\n            x = []\n            for _ in range(self.a_dim):\n                x.append(model.addVar(lb=-1, ub =1, vtype = gp.GRB.CONTINUOUS))\n            obj = gp.QuadExpr()\n            for i in range(self.a_dim):\n                obj+=(0.5*x[i]-a[i])*x[i]\n            model.setObjective(obj, sense = gp.GRB.MINIMIZE)\n\n            self.gp_constraints(model, x, state)\n            \n            model.optimize()\n\n            x_value = np.array(model.X[0:self.a_dim])\n\n        return x_value\n\n    def enforceConstraintIfNeed(self, state, a):\n        if self.isConstraintSatisfied(state, a):\n            return a\n        else:\n            return self.enforceConstraint(state, a)\n    \n    @abstractmethod\n    def numConstraints(self):\n        pass\n    \n    @abstractmethod\n    def getL(self, states, centers, v, get_grad:bool = False):\n        pass\n\n    @abstractmethod\n    def get_center(self, state):\n        pass\n\n    def project(self, state, center, a):\n        L = self.getL(to_tensors(state), to_tensors(center), to_tensors(a-center)).numpy()[0]\n        return center + (a-center) / max(L, 1)\n\n    @abstractmethod\n    def cvxpy_constraints(self, x, state = None):\n        pass\n\n    @abstractmethod\n    def gp_constraints(self, x, state = None):\n        pass\n\n    def cvxpy_variables(self):\n        q = cp.Parameter(self.a_dim)  # input action\n\n        if self.s_dim > 0:\n            s = cp.Parameter(self.s_dim)  # input: state\n        else:\n            s = None\n\n        x = cp.Variable(self.a_dim)  # output\n        obj = cp.Minimize(0.5*cp.sum_squares(x) - q.T @ x)\n        cons = self.cvxpy_constraints(x, s)\n        prob = cp.Problem(obj, cons)\n        return CVXPYVariables(x, q, s, cons, obj, prob)", "    \nclass LinearConstraint(Constraint):\n    \"\"\"\n    Abstract class for linear constraints with:\n        Ax = b,\n        Cx <= d\n    When inequality representations are needed, A, b, C, and d are aggregated into Ex <= f\n    \"\"\"\n    def __init__(self, a_dim:int, s_dim:int = -1, proj_type: str = \"QP\"):\n        super().__init__(a_dim, s_dim)\n        self.proj_type = proj_type\n        \n    @abstractmethod\n    def E(self, state):\n        pass\n  \n    @abstractmethod\n    def f(self, state):\n        pass\n\n    def C(self, state):\n        if isinstance(state, np.ndarray):\n            return self.tensor_C(to_tensors(state)).numpy()[0]\n        return self.tensor_C(state)\n  \n    def d(self, state):\n        if isinstance(state, np.ndarray):\n            return self.tensor_d(to_tensors(state)).numpy()[0]\n        return self.tensor_d(state)\n\n    @abstractmethod\n    def tensor_C(self, state):\n        pass\n  \n    @abstractmethod\n    def tensor_d(self, state):\n        pass\n\n    def getL(self, states, centers, v, get_grad:bool = False):\n        C = self.tensor_C(states)\n        d = self.tensor_d(states)\n        div = d-(centers[:,None,:]*C).sum(axis=2)\n        maxs = ((v[:,None,:]*C).sum(axis=2) / (div+EPS)).max(axis = 1)\n        if not get_grad:\n            return maxs[0]\n        else:\n            indices_2 = maxs[1][:,None].expand(-1,1)\n            indices_3 = maxs[1][:,None,None].expand(-1,1,C.shape[2])\n            gradL = th.gather(C, 1, indices_3).squeeze(dim=1) / th.gather(div, 1, indices_2).squeeze(dim=1)[:,None]\n            return maxs[0], gradL\n\n    def get_center(self, state):\n        C = self.C(state)\n        d = self.d(state)\n        return calc_chebyshev_center(C,d)\n\n    def A(self, state):\n        None\n\n    def b(self, state):\n        None\n\n    def isConstraintSatisfied(self, state, a, err=1e-1):\n        if state.ndim == 2:\n            return ((self.E(state) * a[:,None,:]).sum(axis = 2) <= self.f(state) + err).all()\n        else:\n            return (np.matmul(self.E(state), a) <= self.f(state) + err).all()\n\n    def enforceConstraint(self, state, a):\n        if self.proj_type == \"QP\":\n            return super().enforceConstraint(state, a)\n\n        elif self.proj_type == \"alpha\":\n            C = self.C(state)\n            d = self.d(state)\n            center = calc_chebyshev_center(C,d)\n            v = a - center\n            return center + v / max((C @ v / (d - C @ center) ).max(), 1)\n        elif self.proj_type == \"shrinkage\":\n            C = self.C(state)\n            d = self.d(state)\n            center = calc_chebyshev_center(C,d)\n            v = a - center\n            L = (C @ v / (d - C @ center) ).max()\n            return center + math.tanh(L) / L * v\n        else:\n            raise ValueError(self.proj_type)\n                \n    def constraintViolation(self, state, a, err=1e-2, normalize=False):\n        if normalize:\n            E = self.E(state)\n            f = self.f(state)\n            normalized_E, normalized_f = normalizeConstraint(E, f)\n            return np.maximum(0.0, np.matmul(normalized_E, a) - normalized_f - err)\n        else:\n            return np.maximum(0.0, np.matmul(self.E(state), a) - self.f(state) - err)\n\n    def constraintViolationBatch(self, states, actions):\n        C = self.tensor_C(states)\n        d = self.tensor_d(states)\n        return th.maximum(((C*actions[:,None,:]).sum(dim=2)-d)/(C.norm(dim=2)+EPS),th.tensor(0.)).norm(dim=1)", "\n        \nif __name__ == \"__main__\":\n    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    cons_pq = HalfCheetahDynamicConstraint(\"QP\")\n    cons_al = HalfCheetahDynamicConstraint(\"alpha\")\n    cons_sh = HalfCheetahDynamicConstraint(\"shrinkage\")\n    from ..nn.opt_layer.opt_layer import OptLayer\n    from ..nn.additional_layers.alpha_projection import AlphaProjectionLayer\n    from ..nn.additional_layers.radial_shrinkage import ShrinkageLayer\n    layer_pq = OptLayer(cons_pq)\n    layer_al = AlphaProjectionLayer(cons_pq)\n    layer_sh = ShrinkageLayer(cons_pq)\n    actions = th.rand(100, 6)\n    states = th.rand(100, 17)\n    y_pq = layer_pq(actions, states)\n    y_al = layer_al(actions, states)\n    y_sh = layer_sh(actions, states)\n    for i in range(100):\n        state = states[i].numpy()\n        action = actions[i].numpy()\n        projected_pq = cons_pq.enforceConstraint(state, action)\n        projected_al = cons_al.enforceConstraint(state, action)\n        projected_sh = cons_sh.enforceConstraint(state, action)\n        assert cons_pq.isConstraintSatisfied(state, projected_pq) and (not cons_pq.isConstraintSatisfied(state, action) or np.allclose(action, projected_pq, rtol = 1e-3))\n        assert cons_al.isConstraintSatisfied(state, projected_al) and (not cons_al.isConstraintSatisfied(state, action) or np.allclose(action, projected_al))\n        assert cons_al.isConstraintSatisfied(state, projected_sh)\n        assert np.allclose(y_pq[i].numpy(), projected_pq, rtol = 1e-3)\n        assert np.allclose(y_al[i].numpy(), projected_al, rtol = 1e-3)\n        assert np.allclose(y_sh[i].numpy(), projected_sh, rtol = 1e-3)", "    #print(cons.constraintViolation(None, np.array([2.0, 0.0, 3.0, -4.0, 0.0, 5.0])))\n    #print(cons.constraintViolation(None, np.array([2.0, 0.0, 3.0, -4.0, 0.0, 5.0]), normalize=True))\n"]}
{"filename": "action_constrained_rl/constraint/combined_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nimport numpy as np\nimport torch as th\nimport math\nfrom abc import ABC, abstractmethod\n\nfrom .normalize_constraint import normalizeConstraint\nfrom ..nn.additional_layers.chebyshev_center import calc_chebyshev_center\n", "from ..nn.additional_layers.chebyshev_center import calc_chebyshev_center\n\nimport cvxpy as cp\nfrom ..cvxpy_variables import CVXPYVariables\nfrom .constraint import Constraint\n\nimport gurobipy as gp\n\nEPS=1e-9\n\nclass CombinedConstraint(Constraint):\n    \"\"\"\n    class for combined two constraints\n    \"\"\"\n    def __init__(self, cons1, cons2):\n        super().__init__(cons1.a_dim, cons1.s_dim)\n        self.cons1=cons1\n        self.cons2=cons2\n        \n    def isConstraintSatisfied(self, state, a):\n        return self.cons1.isConstraintSatisfied(state, a) and self.cons2.isConstraintSatisfied(state,a)\n    \n    def numConstraints(self):\n        return self.cons1.numConstraints() + self.cons2.numConstraints()\n    \n    def getL(self, states, centers, v, get_grad:bool = False):\n        if get_grad:\n            L1, grad1 = self.cons1.getL(states, centers, v, get_grad = True)\n            L2, grad2 = self.cons2.getL(states, centers, v, get_grad = True)\n            L = th.maximum(L1, L2)\n            grad = th.where(th.ge(L1,L2)[:,None], grad1, grad2)\n            return L, grad\n        else:\n            L1 = self.cons1.getL(states, centers, v)\n            L2 = self.cons2.getL(states, centers, v)\n            return th.maximum(L1, L2)\n\n    def get_center(self, state):\n        return self.cons2.get_center(state)\n\n    def cvxpy_constraints(self, x, state = None):\n        return self.cons1.cvxpy_constraints(x, state)+self.cons2.cvxpy_constraints(x,state)\n\n    def gp_constraints(self, model, x, state = None):\n        self.cons1.gp_constraints(model, x, state)\n        self.cons2.gp_constraints(model, x, state)\n        \n    def constraintViolation(self, state, a, err=1e-2, normalize=False):\n        return self.cons1.constraintViolation(state, a, err, normalize)+self.cons2.constraintViolation(state, a, err, normalize)\n\n    def constraintViolationBatch(self, states, actions):\n        return self.cons1.constraintViolationBatch(states, actions)+self.cons2.constraintViolationBatch(states, actions)", "EPS=1e-9\n\nclass CombinedConstraint(Constraint):\n    \"\"\"\n    class for combined two constraints\n    \"\"\"\n    def __init__(self, cons1, cons2):\n        super().__init__(cons1.a_dim, cons1.s_dim)\n        self.cons1=cons1\n        self.cons2=cons2\n        \n    def isConstraintSatisfied(self, state, a):\n        return self.cons1.isConstraintSatisfied(state, a) and self.cons2.isConstraintSatisfied(state,a)\n    \n    def numConstraints(self):\n        return self.cons1.numConstraints() + self.cons2.numConstraints()\n    \n    def getL(self, states, centers, v, get_grad:bool = False):\n        if get_grad:\n            L1, grad1 = self.cons1.getL(states, centers, v, get_grad = True)\n            L2, grad2 = self.cons2.getL(states, centers, v, get_grad = True)\n            L = th.maximum(L1, L2)\n            grad = th.where(th.ge(L1,L2)[:,None], grad1, grad2)\n            return L, grad\n        else:\n            L1 = self.cons1.getL(states, centers, v)\n            L2 = self.cons2.getL(states, centers, v)\n            return th.maximum(L1, L2)\n\n    def get_center(self, state):\n        return self.cons2.get_center(state)\n\n    def cvxpy_constraints(self, x, state = None):\n        return self.cons1.cvxpy_constraints(x, state)+self.cons2.cvxpy_constraints(x,state)\n\n    def gp_constraints(self, model, x, state = None):\n        self.cons1.gp_constraints(model, x, state)\n        self.cons2.gp_constraints(model, x, state)\n        \n    def constraintViolation(self, state, a, err=1e-2, normalize=False):\n        return self.cons1.constraintViolation(state, a, err, normalize)+self.cons2.constraintViolation(state, a, err, normalize)\n\n    def constraintViolationBatch(self, states, actions):\n        return self.cons1.constraintViolationBatch(states, actions)+self.cons2.constraintViolationBatch(states, actions)", "\n\n\nif __name__ == \"__main__\":\n    from .power_constraint import OrthoplexConstraint\n    from .sin2_constraint import Sin2Constraint\n    offset_p = 2\n    scale = (1., 1., 1., 1., 1., 1.)\n    indices_p = list(range(offset_p, offset_p+len(scale)))\n    offset_v = 11\n    indices_v = list(range(offset_v, offset_v+len(scale)))\n    s_dim = 17\n    cons = CombinedConstraint(OrthoplexConstraint(indices_v, scale, 10., s_dim),\n                              Sin2Constraint(indices_p, 0.1, s_dim))\n    state=np.array([8.07938070e-01,  1.66712368e-01, -3.53352869e-01, -2.05882562e+00, 7.96720395e-01, -8.13423423e-01, -5.51193071e-01, -2.03170841e-01,  9.15822510e-01, -1.39466434e+00, -2.43234536e+00,  3.60834351e+00, -1.00000000e+01,  2.21396068e-06,  9.28677242e-01, -9.96848688e+00,  7.87601474e+00])\n    action=np.array([0.26813674, -0.16751897, 0.7599944, -0.00297695, -0.07156372, -0.1420452 ])\n    print(cons.get_center(state))", ""]}
{"filename": "action_constrained_rl/constraint/power_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .constraint import LinearConstraint\nimport torch\n\nimport cvxpy as cp\nimport gurobipy as gp\n\nfrom ..cvxpy_variables import CVXPYVariables\n\ndef make_compatible(a, b):\n    if a.device != b.device:\n        a=a.to(b.device)\n    if a.dtype != b.dtype:\n        a=a.to(b.dtype)\n    return a", "from ..cvxpy_variables import CVXPYVariables\n\ndef make_compatible(a, b):\n    if a.device != b.device:\n        a=a.to(b.device)\n    if a.dtype != b.dtype:\n        a=a.to(b.dtype)\n    return a\n\nclass PowerConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $`\\sum max{w_i a_i, 0} \\leq M, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n    \"\"\"\n\n    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n        super().__init__(len(scale), s_dim, **kargs)\n        self.indices = torch.tensor(indices)\n        self.K = torch.zeros((2 ** self.a_dim -1, self.a_dim))\n        self.scale = scale\n        self.s_dim = s_dim\n        for i in range(2 ** self.a_dim -1):\n            for j in range(self.a_dim):\n                if i // (2 ** j) % 2 == 0:\n                    self.K[i,j] = scale[j]\n        self.max_power = max_power\n\n        self.d_value = torch.hstack((self.max_power * torch.ones(self.K.shape[0]), torch.ones(2*self.a_dim)))\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.K = make_compatible(self.K, state)\n        if self.indices.device != state.device:\n            self.indices=self.indices.to(state.device)\n        C = self.K[None, :, :] * torch.index_select(state, 1, self.indices)[:,None,:]\n        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n        return torch.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.d_value = make_compatible(self.d_value, state)\n        return self.d_value.repeat(size, 1)\n\n\n    def numConstraints(self):\n        return self.K.shape[0] + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n    \n    def cvxpy_constraints(self, x, state):\n        cons = [sum([cp.maximum(self.scale[j] * x[j] * state[self.indices[j].item()], 0.) for j in range(self.a_dim)]) <= self.max_power]\n        for i in range(self.a_dim):\n            cons.append(x[i] <= 1.)\n            cons.append(-x[i] <= 1.)\n        return cons\n\n    def gp_constraints(self, model, x, s):\n        max_vars = []\n        for i in range(self.a_dim):\n            mul_var = model.addVar(lb = -gp.GRB.INFINITY, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addConstr(mul_var == self.scale[i]*x[i]*s[self.indices[i].item()])\n            max_var = model.addVar(lb=0, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addConstr(max_var == gp.max_(mul_var, 0))\n            max_vars.append(max_var)\n        model.addConstr(sum(max_vars) <= self.max_power)", "\nclass PowerConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $`\\sum max{w_i a_i, 0} \\leq M, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n    \"\"\"\n\n    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n        super().__init__(len(scale), s_dim, **kargs)\n        self.indices = torch.tensor(indices)\n        self.K = torch.zeros((2 ** self.a_dim -1, self.a_dim))\n        self.scale = scale\n        self.s_dim = s_dim\n        for i in range(2 ** self.a_dim -1):\n            for j in range(self.a_dim):\n                if i // (2 ** j) % 2 == 0:\n                    self.K[i,j] = scale[j]\n        self.max_power = max_power\n\n        self.d_value = torch.hstack((self.max_power * torch.ones(self.K.shape[0]), torch.ones(2*self.a_dim)))\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.K = make_compatible(self.K, state)\n        if self.indices.device != state.device:\n            self.indices=self.indices.to(state.device)\n        C = self.K[None, :, :] * torch.index_select(state, 1, self.indices)[:,None,:]\n        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n        return torch.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.d_value = make_compatible(self.d_value, state)\n        return self.d_value.repeat(size, 1)\n\n\n    def numConstraints(self):\n        return self.K.shape[0] + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n    \n    def cvxpy_constraints(self, x, state):\n        cons = [sum([cp.maximum(self.scale[j] * x[j] * state[self.indices[j].item()], 0.) for j in range(self.a_dim)]) <= self.max_power]\n        for i in range(self.a_dim):\n            cons.append(x[i] <= 1.)\n            cons.append(-x[i] <= 1.)\n        return cons\n\n    def gp_constraints(self, model, x, s):\n        max_vars = []\n        for i in range(self.a_dim):\n            mul_var = model.addVar(lb = -gp.GRB.INFINITY, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addConstr(mul_var == self.scale[i]*x[i]*s[self.indices[i].item()])\n            max_var = model.addVar(lb=0, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addConstr(max_var == gp.max_(mul_var, 0))\n            max_vars.append(max_var)\n        model.addConstr(sum(max_vars) <= self.max_power)", "        \nclass OrthoplexConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $`\\sum |w_i a_i| \\leq M, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n    \"\"\"\n\n    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n        super().__init__(len(scale), s_dim, **kargs)\n        self.indices = torch.tensor(indices)\n        self.K = torch.zeros((2 ** self.a_dim, self.a_dim))\n        self.scale = scale\n        self.s_dim = s_dim\n        for i in range(2 ** self.a_dim):\n            for j in range(self.a_dim):\n                if i // (2 ** j) % 2 == 0:\n                    self.K[i,j] = scale[j]\n                else:\n                    self.K[i,j] = -scale[j]\n        self.max_power = max_power\n\n        self.d_value = torch.hstack((self.max_power * torch.ones(self.K.shape[0]), torch.ones(2*self.a_dim)))\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.K = make_compatible(self.K, state)\n        if self.indices.device != state.device:\n            self.indices=self.indices.to(state.device)\n        C = self.K[None, :, :] * torch.index_select(state, 1, self.indices)[:, None, :]\n        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n        return torch.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.d_value = make_compatible(self.d_value, state)\n        return self.d_value.repeat(size, 1)\n\n\n    def numConstraints(self):\n        return self.K.shape[0] + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n\n    def cvxpy_constraints(self, x, state):\n        cons = [sum([cp.abs(self.scale[j] * x[j] * state[self.indices[j].item()]) for j in range(self.a_dim)]) <= self.max_power]\n        for i in range(self.a_dim):\n            cons.append(x[i] <= 1.)\n            cons.append(-x[i] <= 1.)\n        return cons\n\n    def gp_constraints(self, model, x, s):\n        abs_vars = []\n        for i in range(self.a_dim):\n            mul_var = model.addVar(lb = -gp.GRB.INFINITY, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addConstr(mul_var == self.scale[i]*x[i]*s[self.indices[i].item()])\n            abs_var = model.addVar(lb=0, ub = gp.GRB.INFINITY,\n                                   vtype = gp.GRB.CONTINUOUS)\n            model.addGenConstrAbs(abs_var, mul_var)\n            abs_vars.append(abs_var)\n        model.addConstr(sum(abs_vars) <= self.max_power)", "    \nclass DecelerationConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $`\\sum w_i a_i \\leq M - \\sum |w_i|, |a_i| \\leq 1`$ where $w_i$ is a velocity corresponding to $a_i$\n    \"\"\"\n\n    def __init__(self, indices, scale, max_power, s_dim, **kargs):\n        super().__init__(len(scale), s_dim, **kargs)\n        self.indices = torch.tensor(indices)\n        self.scale = torch.tensor(scale)\n        self.s_dim = s_dim\n        self.max_power = max_power\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.scale = make_compatible(self.scale, state)\n        if self.indices.device != state.device:\n            self.indices=self.indices.to(state.device)\n        C = (self.scale[None,:] * torch.index_select(state, 1, self.indices)).unsqueeze(1)\n        eyes = torch.eye(self.a_dim, device = device).repeat(size,1,1) \n        return torch.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        device = state.device\n        self.scale = make_compatible(self.scale, state)\n        d = (self.max_power * torch.ones(size, device = device) - torch.abs(self.scale[None,:] * torch.index_select(state, 1, self.indices)).sum(1)).unsqueeze(1)\n        return torch.concat((d, torch.ones((size, 2*self.a_dim), device = device)), 1)\n\n\n    def numConstraints(self):\n        return 1 + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n\n    def cvxpy_variables(self):\n        q = cp.Parameter(self.a_dim)  # input action\n\n        s = cp.Parameter(self.s_dim)  # input: state\n\n        x = cp.Variable(self.a_dim)  # output\n        obj = cp.Minimize(0.5*cp.sum_squares(x) - q.T @ x)\n        cons = []\n        for i in range(1<<self.a_dim):\n            sg = []\n            for j in range(self.a_dim):\n                if i // (2 ** j) % 2 == 0:\n                    sg.append(1)\n                else:\n                    sg.append(-1)\n            cons.append(sum([x[j]*self.scale[j]*s[self.indices[j].item()] for j in range(self.a_dim)])\n                        <= self.max_power - sum([sg[j]*self.scale[j]*s[self.indices[j].item()] for j in range(self.a_dim)]))\n        prob = cp.Problem(obj, cons)\n        return CVXPYVariables(x, q, s, cons, obj, prob)", "\nif __name__ == \"__main__\":\n    import numpy as np\n    cons = PowerConstraint(6, (1., 1.), 1., 11)\n    action = np.random.rand(2)\n    state = 5*np.random.rand(11)\n\n    print(cons.get_center(state))\n    x = cp.Variable(2)  # output\n    r = cp.Variable()\n    obj = cp.Maximize(r)\n    C = cons.C(state)\n    d = cons.d(state)\n    norm=np.linalg.norm(C, axis =1)\n    cons = [C @ x + norm * r <= d]\n    prob = cp.Problem(obj, cons)\n    prob.solve(solver = cp.GUROBI)\n    print(x.value)\n    exit()\n    \n    p_action = cons.enforceConstraintIfNeed(state, action)\n    print(p_action, 0.5*np.sum(p_action**2)-p_action.dot(action), p_action.dot(state[6:8]))\n    \n    x = cp.Variable(2)  # output\n    obj = cp.Minimize(0.5*cp.sum_squares(x) - action.T @ x)\n    cons = [cp.maximum(x[0]*state[6],0.)+cp.maximum(x[1]*state[7],0)<=1., x[0]<=1., -x[0] <= 1., x[1]<=1., -x[1]<=1.]\n    prob = cp.Problem(obj, cons)\n    prob.solve(solver = cp.GUROBI)\n    print(x.value, 0.5*np.sum(x.value**2)-x.value.dot(action), x.value.dot(state[6:8]))", "    \n"]}
{"filename": "action_constrained_rl/constraint/tip_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .quadratic_constraint import QuadraticConstraint\nimport torch as th\n\nimport cvxpy as cp\nimport gurobipy as gp\nimport math\n\nfrom ..cvxpy_variables import CVXPYVariables", "\nfrom ..cvxpy_variables import CVXPYVariables\nfrom .power_constraint import make_compatible\n\nclass TipConstraint(QuadraticConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $a_0^2+2a_0(a_0+a+1)\\cos\\theta_2+(a_0+a_1)^2$.\n    \"\"\"\n\n    def __init__(self, max_M, **kargs):\n        super().__init__(max_M, 2, 9, **kargs)\n        \n    def getTensorQ(self, states):\n        Q=th.zeros((states.shape[0],2,2),device = states.device)\n        cosg = th.cos(states[:,7])\n        Q[:,0,0] = 2 + 2 *cosg\n        Q[:,0,1]=Q[:,1,0]=1+cosg\n        Q[:,1,1]=1\n        return Q\n\n    \n    def cvxpy_constraints(self, x, state = None):\n        cons = [cp.square(x[0])+cp.square(x[0]+x[1])+2*x[0]*(x[0]+x[1])*\n                (1-cp.power(state[7],2)/2+cp.power(state[7],4)/24\n                 -cp.power(state[7],6)/720+cp.power(state[7],8)/40320\n                 -cp.power(state[7],10)/3628800+cp.power(state[7],12)/479001600)\n                <= self.max_M]\n        print(cons)\n        return cons\n\n    def gp_constraints(self, model, x, s):\n        Sq = gp.QuadExpr()\n        cosg = math.cos(s[7])\n        Sq+=x[0]*x[0]+(x[0]+x[1])*(x[0]+x[1])+2*x[0]*(x[0]+x[1])*cosg\n        model.addConstr(Sq <= self.max_M)", "\n\nif __name__ == \"__main__\":\n    constraint = TipConstraint(0.05)\n    states=th.tensor([[ 1.8320e-01, -1.5764e-01, -3.7405e-02,  1.0267e-02,  5.1187e-01,\n         -8.5906e-01, -4.5078e-01,  1.5164e-01,  8.1063e-01],\n        [-1.2841e-01, -1.0619e-01,  5.6862e-02, -6.6311e-02,  8.9963e-02,\n         -9.9595e-01, -1.0231e-01, -3.3542e-01, -3.3383e-01],\n        [-9.6614e-02,  1.2436e-01, -8.8864e-02, -2.5948e-02, -8.8712e-01,\n          4.6154e-01,  1.6002e-01, -8.9188e-03, -3.7412e-01],\n        [ 1.4109e-01,  4.5607e-02, -1.0881e-01,  2.2218e-02, -6.8323e-01,\n          7.3020e-01, -8.2575e-01, -7.5437e-01,  9.2613e-01],\n        [ 1.3156e-01,  1.1313e-01, -2.3143e-02,  8.6857e-03,  2.8196e-02,\n          9.9960e-01, -1.3445e-01, -4.4995e-01,  9.2404e-02],\n        [ 2.3610e-01,  2.7571e-02, -1.5175e-01,  4.3976e-02, -1.9267e-01,\n          9.8126e-01, -2.0409e-01, -6.7312e-01,  9.9564e-02],\n        [ 9.5906e-02, -7.2765e-02, -1.8310e-01,  1.1592e-01, -7.9309e-01,\n         -6.0911e-01, -4.1237e-01, -7.7382e-01, -8.4190e-01],\n        [-2.6493e-02, -1.1129e-01, -9.7531e-02,  2.8042e-01, -5.3531e-01,\n          8.4466e-01, -8.9416e-02,  4.6762e-02,  2.3578e-01],\n        [ 5.2737e-02, -1.6635e-01,  6.7426e-02,  2.9614e-01,  2.0968e-01,\n          9.7777e-01, -3.9352e-01, -3.8360e-01, -6.4619e-02],\n        [ 4.1588e-02, -9.6503e-02, -1.0680e-01,  4.2423e-02,  3.3904e-01,\n         -9.4077e-01, -2.5552e-01, -7.7012e-01,  3.5925e-02],\n        [-1.9536e-01, -1.0919e-01,  2.3155e-01,  4.2231e-02, -7.3762e-01,\n         -6.7522e-01,  4.2337e-03,  8.1003e-01,  1.4798e-01],\n        [-1.7193e-01,  2.6708e-01,  1.2781e-01, -1.5978e-01,  6.0622e-01,\n          7.9529e-01,  1.0703e-01,  6.4415e-01, -2.5370e-01],\n        [ 2.1081e-01,  1.8435e-01, -3.6944e-02, -6.6586e-02,  8.3082e-01,\n          5.5654e-01,  1.1817e-02, -4.0813e-04, -8.9231e-02],\n        [-2.2798e-01,  2.6716e-02,  2.2169e-01, -1.4165e-02, -3.5069e-01,\n         -9.3649e-01,  1.9723e-02, -1.0159e+00,  0.0000e+00],\n        [ 1.4427e-01,  1.2218e-01,  1.6949e-02, -1.1593e-04,  9.3954e-01,\n          3.4243e-01, -7.1718e-02,  1.8670e-01,  8.1036e-02],\n        [ 2.4308e-01,  1.5988e-01, -7.6241e-02, -5.8548e-02,  9.8955e-01,\n          1.4416e-01, -2.4803e-02,  2.5290e-01,  1.4468e-02],\n        [-2.1995e-01,  2.4489e-01,  4.7054e-02, -2.8255e-01, -9.1998e-01,\n          3.9196e-01, -1.0542e-01,  3.7617e-01, -6.1844e-02],\n        [ 9.5906e-02, -7.2765e-02, -8.4770e-02,  7.7624e-02, -9.5543e-01,\n          2.9520e-01, -4.4436e-01, -1.0253e+00,  0.0000e+00],\n        [-1.6567e-02, -1.1778e-01, -6.0481e-02,  1.3783e-01, -5.1129e-01,\n         -8.5941e-01, -7.7052e-02, -8.3015e-01, -7.3317e-01],\n        [ 1.8572e-01,  2.0678e-01, -7.9348e-03, -1.5137e-01,  9.7423e-01,\n         -2.2557e-01, -1.4413e-01,  3.3027e-01,  1.7400e-01],\n        [ 1.9805e-01, -1.2779e-01, -3.2554e-04,  6.6961e-02,  8.8112e-01,\n         -4.7289e-01, -7.6104e-02,  1.1023e-01, -9.5515e-02],\n        [ 2.4602e-02, -2.1116e-01, -2.6810e-02,  1.9340e-01,  7.7527e-01,\n          6.3163e-01,  1.7385e+00, -1.0000e+00,  3.9291e-14],\n        [ 2.0714e-01, -1.5270e-02, -6.5214e-03, -4.7860e-03,  9.2134e-01,\n         -3.8875e-01, -1.1639e-02,  1.8540e-01, -6.9389e-02],\n        [ 2.0401e-01,  1.3602e-01, -3.3934e-01, -1.8223e-01, -4.0614e-01,\n         -9.1381e-01, -3.0699e-01, -5.8965e-01, -7.4583e-01],\n        [-7.5421e-02, -1.6144e-02,  4.1996e-02,  5.0506e-03,  4.3251e-01,\n         -9.0163e-01,  2.7007e-02, -9.3872e-01,  1.2478e-02],\n        [ 1.0846e-01,  9.9337e-02, -3.3758e-02, -6.2992e-02, -1.1327e-01,\n          9.9356e-01, -2.7986e-01, -7.5972e-01,  3.2931e-01],\n        [-5.8949e-02, -2.0931e-01,  7.5779e-02,  2.1469e-01, -8.3897e-01,\n          5.4417e-01,  9.4286e-01, -1.0007e+00,  3.1399e-03],\n        [ 1.9137e-01, -2.1385e-01, -7.0773e-03,  1.8877e-01,  7.9730e-01,\n         -6.0358e-01, -2.6127e-02,  3.1761e-01, -8.4677e-02],\n        [-2.5788e-01, -1.5271e-01,  2.5780e-01,  2.1453e-01,  9.9917e-01,\n         -4.0851e-02, -1.1021e+00,  9.0781e-01,  1.0428e+00],\n        [-2.2205e-01,  3.0545e-02,  2.3723e-01, -3.8390e-02, -8.8281e-01,\n         -4.6973e-01, -2.8692e-01,  9.9921e-01, -7.1263e-02],\n        [ 1.1406e-01,  3.0687e-02,  1.2989e-03,  2.3440e-02,  1.5227e-01,\n          9.8834e-01, -2.1426e-02, -6.1160e-01,  3.4739e-02],\n        [ 5.9732e-02,  2.5014e-01, -4.0865e-02, -2.4222e-01, -7.4692e-01,\n          6.6491e-01,  1.3502e-01, -1.0045e+00, -2.5752e-01],\n        [-1.9243e-01,  1.4527e-01,  1.9735e-01, -1.6079e-01,  6.6832e-01,\n          7.4387e-01, -4.0629e-01, -1.0051e+00,  2.3042e-02],\n        [ 4.6327e-02,  3.0584e-02, -5.0195e-02, -1.7612e-01, -8.3896e-01,\n         -5.4420e-01, -6.8624e-01,  4.9996e-01, -6.7928e-01],\n        [-2.6227e-01, -1.8429e-01,  2.7370e-01,  1.7832e-01, -9.9454e-01,\n          1.0437e-01, -2.0936e+00,  1.0213e+00,  0.0000e+00],\n        [-1.4551e-01,  4.4989e-02,  1.4488e-01, -1.6085e-01,  8.7471e-01,\n         -4.8465e-01,  6.0875e-02, -6.6676e-01, -1.4619e-01],\n        [ 1.3737e-01,  1.3935e-01,  6.1545e-03, -1.3965e-01,  6.3909e-01,\n         -7.6913e-01, -2.9661e-02,  5.4604e-01, -5.3186e-03],\n        [ 2.5603e-01, -1.4094e-01, -7.4301e-02,  1.7236e-01,  9.3303e-01,\n         -3.5981e-01, -8.5294e-02,  3.2390e-01, -1.6248e-01],\n        [-1.0117e-01, -1.9024e-01,  7.8590e-02,  2.4379e-01, -9.5423e-01,\n         -2.9906e-01,  8.9652e-02, -8.7500e-01, -2.1984e-01],\n        [-1.7206e-01, -7.8571e-02,  1.6677e-01, -4.6847e-02, -8.4842e-01,\n         -5.2933e-01,  1.4997e-01,  6.2589e-01,  8.7443e-02],\n        [ 2.5588e-01, -1.6800e-01, -4.1788e-01,  2.4987e-01, -9.9468e-01,\n         -1.0298e-01,  9.5747e-02, -3.6193e-01, -1.8553e-01],\n        [ 7.6981e-02, -3.5777e-02, -8.0278e-02,  5.5422e-02, -8.6179e-01,\n         -5.0727e-01, -4.6081e-01, -1.0090e+00, -3.0229e-01],\n        [ 3.1120e-02, -2.1374e-02, -1.1562e-01,  6.8736e-02,  1.5641e-01,\n          9.8769e-01, -1.9128e-01,  7.5827e-01,  5.3134e-01],\n        [ 2.6689e-01, -1.3015e-01, -1.5124e-01, -3.3241e-02,  3.0154e-01,\n         -9.5345e-01,  7.7906e-02,  2.0443e-01, -9.1836e-03],\n        [ 1.5794e-01, -1.6047e-01, -1.7740e-01,  1.4733e-01,  8.5832e-01,\n         -5.1312e-01,  6.1850e-01, -9.6909e-01,  1.9071e-01],\n        [ 1.9805e-01, -1.2779e-01, -8.8190e-02, -5.1159e-02,  4.8637e-01,\n         -8.7375e-01, -1.6041e-01,  8.5268e-05, -1.8568e-01],\n        [-2.3627e-01, -1.8825e-01,  2.3476e-01,  3.5532e-01,  7.3609e-01,\n          6.7689e-01, -8.9894e-01,  4.8513e-01,  9.2152e-01],\n        [-8.0491e-04,  2.5744e-01, -3.3972e-02, -2.0605e-01,  7.4207e-01,\n          6.7032e-01, -1.0938e-02,  8.2298e-01, -4.8571e-01],\n        [-1.4558e-01,  5.6405e-02,  9.1604e-03,  8.7207e-02, -9.0843e-01,\n          4.1803e-01,  1.4570e-01, -2.5292e-01, -4.8987e-01],\n        [ 1.5794e-01, -1.6047e-01, -1.6855e-01,  1.7482e-01, -3.4020e-01,\n         -9.4035e-01,  3.9129e-01, -1.0001e+00,  5.8146e-04],\n        [-1.0272e-01,  1.3983e-01,  1.1016e-01, -7.2283e-02, -9.6495e-01,\n          2.6245e-01,  1.8502e-01, -8.6011e-01, -5.5235e-01],\n        [ 1.8644e-01,  1.6413e-02, -1.8327e-01, -2.6501e-02,  1.2385e-01,\n          9.9230e-01, -4.8969e-01,  1.0581e+00,  0.0000e+00],\n        [-2.2467e-01,  1.2656e-01,  1.2444e-01, -1.1338e-01, -2.6591e-01,\n          9.6400e-01, -8.1238e-02,  6.2769e-01, -1.5642e+00],\n        [ 1.0696e-01,  2.5812e-01, -1.1316e-01, -2.5016e-01,  6.6828e-01,\n         -7.4391e-01,  1.3040e+00, -1.0428e+00,  0.0000e+00],\n        [-5.2173e-02, -1.4447e-01,  1.8761e-01,  2.3472e-01,  2.7679e-01,\n          9.6093e-01, -1.2907e-01, -4.3628e-01,  3.6993e-01],\n        [-1.2841e-01, -1.0619e-01,  6.4605e-02,  1.3630e-01, -5.8368e-01,\n         -8.1198e-01,  1.3919e-01, -7.8860e-01,  5.8789e-01],\n        [ 2.1081e-01,  1.8435e-01, -4.8475e-02, -7.2713e-02,  9.6667e-01,\n          2.5603e-01,  1.6961e-01,  2.2385e-01, -2.1960e-01],\n        [ 1.1092e-01,  2.4205e-01, -2.1314e-02, -5.4704e-02,  5.6766e-01,\n          8.2326e-01, -6.6414e-03,  9.6259e-02, -5.9421e-02],\n        [ 1.9046e-01,  1.6985e-01, -2.7190e-02, -4.0215e-02,  8.4937e-01,\n          5.2780e-01,  6.8751e-02,  7.4697e-02, -1.0401e-01],\n        [ 1.0989e-01, -1.0523e-01, -3.6461e-02,  4.2261e-02, -2.8819e-01,\n         -9.5757e-01,  1.9405e-01,  7.3822e-01,  1.5771e-01],\n        [ 1.6965e-01,  2.0781e-01, -6.6745e-02, -3.5913e-01,  8.9288e-01,\n         -4.5029e-01, -2.0470e-01, -3.7416e-01, -5.7983e-01],\n        [-5.1685e-02,  5.0972e-02, -6.5238e-02, -7.5389e-02, -3.5952e-01,\n         -9.3314e-01, -2.3778e-01, -6.4139e-01,  6.4520e-02],\n        [-2.0239e-01, -8.1387e-02,  3.2574e-01,  1.2562e-01,  2.2458e-01,\n          9.7446e-01,  2.5393e-01, -6.0578e-01, -1.2533e-01],\n        [-7.5421e-02, -1.6144e-02,  5.0420e-02,  1.9265e-02,  1.9355e-01,\n         -9.8109e-01,  1.4493e-01, -9.7084e-01,  4.9250e-02],\n        [ 1.6701e-01,  9.1604e-02, -4.0769e-02, -3.0067e-02,  2.4395e-01,\n          9.6979e-01, -1.3173e-01, -5.1368e-01,  8.3635e-01],\n        [ 1.3156e-01,  1.1313e-01, -1.5625e-01, -8.0319e-02, -7.8229e-01,\n         -6.2291e-01, -1.3983e-01, -9.4268e-01, -4.1545e-01],\n        [-5.8949e-02, -2.0931e-01,  3.9744e-02,  2.0774e-01,  4.9230e-01,\n         -8.7042e-01, -1.9549e-01, -9.6385e-01,  5.6265e-01],\n        [-5.2173e-02, -1.4447e-01,  1.2499e-01,  2.6095e-01,  9.9971e-01,\n          2.3921e-02, -4.4858e-01,  6.3995e-01,  1.2230e+00],\n        [ 7.0893e-02,  1.2322e-02, -6.4323e-02, -2.0734e-02, -3.3465e-01,\n          9.4234e-01,  3.2380e-01, -1.0351e+00,  3.5233e-03],\n        [-2.0239e-01, -8.1387e-02,  2.9158e-01,  1.4889e-01, -2.0730e-01,\n          9.7828e-01,  2.7321e-01, -6.8015e-01, -1.1346e-01],\n        [ 5.2737e-02, -1.6635e-01, -5.1120e-02,  1.7999e-01,  5.2518e-01,\n         -8.5099e-01, -7.9028e-01,  1.0173e+00,  0.0000e+00],\n        [ 2.6689e-01, -1.3015e-01, -1.4593e-01, -2.6749e-02,  2.8306e-01,\n         -9.5910e-01, -8.8583e-02,  2.2614e-01,  9.3122e-03],\n        [-1.2841e-01, -1.0619e-01,  1.0166e-01, -9.9029e-02, -3.6151e-01,\n         -9.3237e-01, -3.7250e-01,  2.0520e-01,  1.6625e+00],\n        [ 2.5431e-01, -1.1782e-01, -2.4572e-01,  1.0933e-01, -1.4669e-01,\n          9.8918e-01, -9.8805e-02,  1.0685e+00, -2.9642e-03],\n        [-2.2370e-01, -1.5470e-01,  1.4518e-01, -1.8860e-02,  1.1537e-01,\n         -9.9332e-01,  5.0572e-01, -3.2005e-01, -5.4727e-01],\n        [-1.7176e-01,  1.8518e-01,  1.3073e-01, -9.9114e-02,  6.6989e-01,\n          7.4246e-01,  1.0031e-01,  7.2699e-01, -1.3682e-01],\n        [ 1.8644e-01,  1.6413e-02, -1.9379e-01,  3.6640e-02,  9.8955e-01,\n          1.4419e-01, -3.9230e-01,  8.3653e-01, -7.5699e-01],\n        [ 1.6254e-01, -1.5441e-01, -1.3764e-01, -4.6595e-02, -1.7109e-01,\n         -9.8526e-01, -8.7949e-02,  1.6710e-01, -2.0584e-01],\n        [-1.7772e-02,  5.5986e-02,  4.0756e-02, -1.2489e-01, -8.5372e-01,\n         -5.2073e-01,  3.2339e-02,  7.9590e-01, -3.2529e-01],\n        [ 1.3156e-01,  1.1313e-01,  2.7841e-02,  1.9793e-02,  8.6371e-01,\n          5.0399e-01, -3.9161e-02,  1.0646e-01,  8.0312e-02],\n        [-9.6614e-02,  1.2436e-01, -7.4913e-02, -1.5917e-01, -6.8609e-01,\n         -7.2752e-01, -2.0491e-02, -3.8136e-01,  1.7209e-01],\n        [ 5.3381e-02,  1.5518e-01,  5.2611e-02, -3.5173e-02,  9.9240e-01,\n          1.2306e-01,  1.2814e-01,  4.6558e-01, -7.6158e-02],\n        [ 2.5244e-01,  8.4347e-02, -4.0940e-01, -4.9351e-02, -9.4536e-01,\n         -3.2602e-01, -1.1401e+00, -3.7587e-01,  1.6452e+00],\n        [ 2.5780e-01, -1.5807e-01, -1.0912e-01,  5.0225e-02,  3.6717e-01,\n         -9.3015e-01, -2.0676e-01,  3.2349e-01, -2.6042e-01],\n        [-1.7057e-01, -2.5330e-01,  1.0561e-01,  2.2011e-01,  2.0264e-01,\n         -9.7925e-01, -2.7661e-01, -7.8458e-01,  5.1177e-01],\n        [ 4.8916e-02, -1.0787e-01, -5.0131e-02,  1.2707e-01,  9.3071e-01,\n         -3.6576e-01,  1.7344e-01,  9.8406e-01, -1.9716e-01],\n        [ 2.0202e-01, -3.9029e-03, -8.9125e-02, -1.7311e-01,  5.5159e-01,\n         -8.3411e-01, -4.0858e-02, -3.5048e-03,  2.0575e-01],\n        [ 2.0809e-01,  1.7872e-01, -5.5922e-02, -7.2710e-02,  9.9367e-01,\n          1.1230e-01,  1.1095e-01,  3.2193e-01, -7.3183e-02],\n        [ 1.1997e-01, -1.2303e-01, -6.6191e-02,  2.8993e-02,  9.9807e-01,\n          6.2107e-02,  2.0943e-02, -6.8947e-01, -4.2546e-02],\n        [ 9.4314e-02, -2.0381e-01, -1.2926e-01,  1.8637e-01,  5.5225e-01,\n         -8.3368e-01,  2.9412e-01, -8.9692e-01,  5.3850e-01],\n        [ 1.3156e-01,  1.1313e-01, -6.3705e-02,  6.6367e-03, -3.4719e-01,\n          9.3779e-01, -2.5571e-01, -5.5399e-01,  3.1866e-01],\n        [-2.4580e-01,  9.2489e-02,  4.7536e-02, -2.3359e-02, -9.2019e-01,\n          3.9148e-01, -2.9157e-01,  2.8959e-02,  3.1105e-01],\n        [ 2.4662e-01, -8.7919e-02, -2.9131e-01,  1.0427e-01, -2.7906e-01,\n         -9.6027e-01,  5.1581e-01, -8.8904e-01,  1.6501e-01],\n        [ 4.1381e-02,  9.9785e-02,  1.3328e-01, -1.9956e-01,  9.7123e-01,\n         -2.3813e-01, -1.6613e-01, -1.6541e-01,  5.2801e-01],\n        [ 4.9837e-02, -3.2174e-02, -3.3871e-02,  2.2361e-02,  4.5312e-02,\n          9.9897e-01,  1.2342e-01, -9.9689e-01, -2.0943e-03],\n        [ 4.6537e-02,  2.6297e-01, -3.5891e-02, -5.3864e-02,  1.3673e-01,\n          9.9061e-01, -3.2710e-02,  5.7895e-02,  1.1680e-01],\n        [ 1.9656e-02,  7.1967e-02,  1.8609e-02, -1.0322e-01, -6.0140e-01,\n         -7.9895e-01,  3.1162e-02,  8.7325e-01, -3.4123e-01],\n        [-5.9224e-02, -1.9975e-01,  9.1893e-02,  1.6842e-01,  6.4324e-01,\n          7.6567e-01,  3.6364e-01, -9.6045e-01, -9.9280e-01],\n        [ 4.6537e-02,  2.6297e-01, -9.1370e-02, -2.7300e-01, -1.4171e-01,\n          9.8991e-01, -4.7941e-01,  9.6689e-01,  1.1510e+00],\n        [ 1.5794e-01, -1.6047e-01, -2.7301e-02, -2.8689e-04,  4.5119e-01,\n         -8.9243e-01, -2.4893e-01,  1.0593e-01, -7.7573e-02]])\n\n    Q = constraint.getQ(states)\n\n    v = th.tensor([[ 1.1877, -2.3243],\n        [ 1.7959, -4.3229],\n        [ 0.7542, -1.9867],\n        [ 0.9488, -1.2668],\n        [ 0.3369, -0.3681],\n        [-1.5717,  2.3981],\n        [ 2.3537, -2.5683],\n        [-1.5644,  3.7083],\n        [ 0.7733, -1.5146],\n        [ 4.5917, -8.2694],\n        [-0.1183, -1.0292],\n        [ 1.2631, -2.6133],\n        [-0.5344,  1.4256],\n        [-2.9314,  5.5788],\n        [ 0.4514, -1.0473],\n        [ 0.3028, -0.6701],\n        [-1.3386,  1.1386],\n        [-1.4933,  2.7295],\n        [ 4.0325, -7.6104],\n        [-0.7822,  2.7495],\n        [-0.0188,  0.5993],\n        [-3.4582,  6.0066],\n        [ 0.0167,  0.0568],\n        [-1.6121,  1.3926],\n        [-0.9225,  1.0189],\n        [ 0.6311, -0.5272],\n        [ 1.2427, -2.7783],\n        [ 1.0319, -2.5678],\n        [-5.2975, 10.0474],\n        [ 1.4269, -2.9111],\n        [ 0.3023, -0.9060],\n        [-2.8003,  5.5046],\n        [ 0.3941, -1.4517],\n        [ 1.8764, -3.7361],\n        [-2.7421,  4.9254],\n        [-0.7130,  0.3959],\n        [ 0.9533, -0.9312],\n        [ 0.5112, -1.0822],\n        [ 2.9078, -5.0902],\n        [ 0.8757, -2.0521],\n        [ 3.8278, -7.8939],\n        [ 1.2113, -2.0263],\n        [-2.8169,  5.3518],\n        [ 0.8417, -1.0173],\n        [-3.6530,  5.9265],\n        [-5.9884, 11.9783],\n        [-3.8574,  9.2997],\n        [ 1.2505, -1.8271],\n        [-0.9896,  2.3350],\n        [ 3.4117, -4.8462],\n        [-0.1826,  1.3147],\n        [-0.7548,  1.0839],\n        [-1.2892,  2.9177],\n        [-2.1583,  3.2695],\n        [-2.7347,  5.7706],\n        [-0.6775,  1.0023],\n        [ 0.5251, -1.4952],\n        [-0.0960,  0.5027],\n        [ 0.4441, -1.2032],\n        [ 0.5062, -1.0083],\n        [ 2.8411, -7.2339],\n        [ 0.2819, -2.3598],\n        [ 2.6288, -4.4024],\n        [-2.0797,  3.3121],\n        [ 0.9550, -2.2754],\n        [-1.2370,  0.9994],\n        [ 2.3387, -3.4464],\n        [-2.7986,  5.9041],\n        [-1.1930,  2.0790],\n        [ 2.5329, -4.1267],\n        [ 2.1468, -3.7458],\n        [-0.2907,  1.5725],\n        [ 2.2964, -4.7565],\n        [-6.6629, 12.4310],\n        [ 1.6675, -3.2404],\n        [ 1.9704, -4.1947],\n        [ 0.3274, -0.3166],\n        [-4.1403,  8.6956],\n        [-1.8100,  3.6279],\n        [ 1.1904, -2.6021],\n        [-1.3071,  1.7721],\n        [ 0.8844, -1.8934],\n        [ 1.8460, -3.0603],\n        [-2.9637,  6.1326],\n        [ 2.1313, -3.7318],\n        [-3.3370,  5.8759],\n        [ 0.0935,  0.0552],\n        [ 0.8720, -2.2909],\n        [-0.8439,  2.3420],\n        [-1.0208,  0.9889],\n        [ 0.3767, -0.4332],\n        [-0.0614,  0.7950],\n        [ 3.1372, -5.7051],\n        [-1.2770,  3.3321],\n        [-1.0459,  1.6059],\n        [ 0.3424, -0.7144],\n        [-0.5434,  2.0249],\n        [-3.5058,  5.6909],\n        [-4.1882,  7.5286],\n        [-5.5352, 11.1384]])\n    \n    value = (v[:,:,None]*Q*v[:,None,:]).sum(dim=2).sum(dim=1)\n    L = th.sqrt((value+1e-9)/constraint.max_M)\n    print(value, L)", "    \n"]}
{"filename": "action_constrained_rl/constraint/sphere_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .constraint import Constraint\n\nimport numpy as np\nimport torch as th\nimport cvxpy as cp\nimport gurobipy as gp\n\nclass SphericalConstraint(Constraint):\n\n    \"\"\"\n    Action Constraints with the from $|a| \\leq M$\n    \"\"\"\n    \n    def __init__(self, a_dim, r2):\n        super().__init__(a_dim)\n        self.r2 = r2\n        self.r = r2 ** 0.5\n    \n    def isConstraintSatisfied(self, state, a, err=1e-3):\n        return np.sum(np.square(a)) <= self.r2 + err\n\n    def enforceConstraint(self, state, a):\n        return min(self.r / np.linalg.norm(a), 1.) * a\n\n    def numConstraints(self):\n        return 1\n    \n    def getL(self, states, centers, v, get_grad:bool = False):\n        L = v.norm(dim=1)/self.r\n        if not get_grad:\n            return L\n        else:\n            return L, v/L[:,None]/self.r**2\n\n    def constraintViolation(self, state, a, err=1e-3, normalize=False):\n        return np.expand_dims(np.maximum(0.0, np.sqrt(np.sum(np.square(a))) - self.r - err),0)\n        \n    def constraintViolationBatch(self, states, actions):\n        return th.maximum(actions.norm(dim=1)-self.r, th.tensor(0.))\n        \n    def get_center(self, state):\n        return np.zeros(self.a_dim)\n\n    def cvxpy_constraints(self, x, state = None):\n        return [cp.sum_squares(x) <= self.r2]\n\n    def gp_constraints(self, model, x, state = None):\n        Sq = gp.QuadExpr()\n        for i in range(self.a_dim):\n            Sq+=x[i]*x[i]\n        model.addConstr(Sq <= self.r2)", "\nclass SphericalConstraint(Constraint):\n\n    \"\"\"\n    Action Constraints with the from $|a| \\leq M$\n    \"\"\"\n    \n    def __init__(self, a_dim, r2):\n        super().__init__(a_dim)\n        self.r2 = r2\n        self.r = r2 ** 0.5\n    \n    def isConstraintSatisfied(self, state, a, err=1e-3):\n        return np.sum(np.square(a)) <= self.r2 + err\n\n    def enforceConstraint(self, state, a):\n        return min(self.r / np.linalg.norm(a), 1.) * a\n\n    def numConstraints(self):\n        return 1\n    \n    def getL(self, states, centers, v, get_grad:bool = False):\n        L = v.norm(dim=1)/self.r\n        if not get_grad:\n            return L\n        else:\n            return L, v/L[:,None]/self.r**2\n\n    def constraintViolation(self, state, a, err=1e-3, normalize=False):\n        return np.expand_dims(np.maximum(0.0, np.sqrt(np.sum(np.square(a))) - self.r - err),0)\n        \n    def constraintViolationBatch(self, states, actions):\n        return th.maximum(actions.norm(dim=1)-self.r, th.tensor(0.))\n        \n    def get_center(self, state):\n        return np.zeros(self.a_dim)\n\n    def cvxpy_constraints(self, x, state = None):\n        return [cp.sum_squares(x) <= self.r2]\n\n    def gp_constraints(self, model, x, state = None):\n        Sq = gp.QuadExpr()\n        for i in range(self.a_dim):\n            Sq+=x[i]*x[i]\n        model.addConstr(Sq <= self.r2)", ""]}
{"filename": "action_constrained_rl/constraint/box_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .constraint import LinearConstraint\nimport torch\n\nimport cvxpy as cp\nimport gurobipy as gp\n\nfrom ..cvxpy_variables import CVXPYVariables\nfrom .power_constraint import make_compatible", "from ..cvxpy_variables import CVXPYVariables\nfrom .power_constraint import make_compatible\n\nclass BoxConstraint(LinearConstraint):\n\n    \"\"\"\n    Action Constraints with the from\n    $|a_i| \\leq 1`$ \n    \"\"\"\n\n    def __init__(self, a_dim):\n        super().__init__(a_dim, -1)\n        eyes = torch.eye(self.a_dim)\n        self.C_value = torch.concat((eyes, -eyes), axis = 0)\n\n        self.d_value = torch.ones(2*self.a_dim)\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        self.C_value = make_compatible(self.C_value, state)\n        return self.C_value.repeat(size, 1, 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        self.d_value = make_compatible(self.d_value, state)\n        return self.d_value.repeat(size, 1)\n\n\n    def numConstraints(self):\n        return 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n\n    def cvxpy_constraints(self, x, state):\n        cons = []\n        for i in range(self.a_dim):\n            cons.append(x[i] <= 1.)\n            cons.append(-x[i] <= 1.)\n        return cons\n\n    def gp_constraints(self, model, x, s):\n        pass", ""]}
{"filename": "action_constrained_rl/constraint/sin2_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .quadratic_constraint import QuadraticConstraint\nimport torch as th\n\nimport cvxpy as cp\nimport gurobipy as gp\nimport math\n\nfrom ..cvxpy_variables import CVXPYVariables", "\nfrom ..cvxpy_variables import CVXPYVariables\nfrom .power_constraint import make_compatible\n\nclass Sin2Constraint(QuadraticConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $\\sum a_i^2\\sin^2\\theta_i  \\leq M$ where $\\theta_i$ is the angle corresponding to $a_i$\n    \"\"\"\n\n    def __init__(self, index, max_M, s_dim, **kargs):\n        self.index = index\n        super().__init__(max_M, len(index), s_dim, **kargs)\n        \n    def getTensorQ(self, states):\n        Q=th.zeros((states.shape[0],self.a_dim,self.a_dim),device = states.device)\n        for i in range(self.a_dim):\n            sin2 = th.sin(states[:,self.index[i]])**2\n            Q[:,i,i] = sin2\n        return Q\n\n    \n    def cvxpy_constraints(self, x, state = None):\n        pass\n\n    def gp_constraints(self, model, x, s):\n        Sq = gp.QuadExpr()\n        for i in range(self.a_dim):\n            sin2 = math.sin(s[self.index[i]])**2\n            Sq+=sin2*x[i]*x[i]\n        model.addConstr(Sq <= self.max_M)", ""]}
{"filename": "action_constrained_rl/constraint/normalize_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nimport numpy as np\ndef normalizeConstraint(A, b):\n    norms = (np.linalg.norm(A, axis=1))\n    return (A/norms[:,None], b/norms)\n\n\nif __name__ == \"__main__\":\n    import numpy as np\n    A = np.array([[3.0, 4.0], [50.0, 3.0]])\n    b = np.array([1.0, 2.0])\n    norms = (np.linalg.norm(A, axis=1))\n    print(A / norms)\n    print(b / norms)", "if __name__ == \"__main__\":\n    import numpy as np\n    A = np.array([[3.0, 4.0], [50.0, 3.0]])\n    b = np.array([1.0, 2.0])\n    norms = (np.linalg.norm(A, axis=1))\n    print(A / norms)\n    print(b / norms)\n"]}
{"filename": "action_constrained_rl/constraint/MA_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .constraint import LinearConstraint\nimport torch as th\n\nimport cvxpy as cp\nimport gurobipy as gp\n\nimport math\n\nclass MAConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $w_0a_0\\sin(theta_0+theta_1+theta_2)+w_3a_3\\sin(\\theta_3+\\theta_4+\\theta_5) \\leq M, |a_i| \\leq 1`$\n    \"\"\"\n\n    def __init__(self, max_power, **kargs):\n        super().__init__(6, 17, **kargs)\n        self.max_power = max_power\n\n        self.d_value = th.hstack((self.max_power * th.ones(1), th.ones(2*self.a_dim)))\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        C = th.zeros((size, 1, 6), device = device)\n        C[:,0,0] = state[:,11]*th.sin(state[:,2]+state[:,3]+state[:,4])\n        C[:,0,3] = state[:,14]*th.sin(state[:,5]+state[:,6]+state[:,7])\n        eyes = th.eye(self.a_dim, device = device).repeat(size,1,1) \n        return th.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        if self.d_value.device != state.device:\n            self.d_value = self.d_value.to(state.device)\n        return self.d_value.repeat(size, 1)\n\n    def numConstraints(self):\n        return 1 + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n    \n    def f(self, state):\n        return self.d(state)\n    \n    def cvxpy_constraints(self, x, state):\n        pass\n    \n    def numConstraints(self):\n        return 1 + 2 * self.a_dim\n\n    def gp_constraints(self, model, x, s):\n        model.addConstr(s[11]*math.sin(s[2]+s[3]+s[4])*x[0]\n                        + s[14]*math.sin(s[5]+s[6]+s[7])*x[3]<= self.max_power)", "import math\n\nclass MAConstraint(LinearConstraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $w_0a_0\\sin(theta_0+theta_1+theta_2)+w_3a_3\\sin(\\theta_3+\\theta_4+\\theta_5) \\leq M, |a_i| \\leq 1`$\n    \"\"\"\n\n    def __init__(self, max_power, **kargs):\n        super().__init__(6, 17, **kargs)\n        self.max_power = max_power\n\n        self.d_value = th.hstack((self.max_power * th.ones(1), th.ones(2*self.a_dim)))\n\n    def tensor_C(self, state):\n        size = state.shape[0]\n        device = state.device\n        C = th.zeros((size, 1, 6), device = device)\n        C[:,0,0] = state[:,11]*th.sin(state[:,2]+state[:,3]+state[:,4])\n        C[:,0,3] = state[:,14]*th.sin(state[:,5]+state[:,6]+state[:,7])\n        eyes = th.eye(self.a_dim, device = device).repeat(size,1,1) \n        return th.concat((C, eyes, -eyes), axis = 1)\n    \n    def tensor_d(self, state):\n        size = state.shape[0]\n        if self.d_value.device != state.device:\n            self.d_value = self.d_value.to(state.device)\n        return self.d_value.repeat(size, 1)\n\n    def numConstraints(self):\n        return 1 + 2 * self.a_dim\n\n    def E(self, state):\n        return self.C(state)\n    \n    def f(self, state):\n        return self.d(state)\n    \n    def cvxpy_constraints(self, x, state):\n        pass\n    \n    def numConstraints(self):\n        return 1 + 2 * self.a_dim\n\n    def gp_constraints(self, model, x, s):\n        model.addConstr(s[11]*math.sin(s[2]+s[3]+s[4])*x[0]\n                        + s[14]*math.sin(s[5]+s[6]+s[7])*x[3]<= self.max_power)", ""]}
{"filename": "action_constrained_rl/constraint/quadratic_constraint.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\nfrom .constraint import Constraint, to_tensors\nimport torch as th\nimport numpy as np\nimport math\nfrom abc import abstractmethod\n\nimport cvxpy as cp\nimport gurobipy as gp", "import cvxpy as cp\nimport gurobipy as gp\n\nfrom ..cvxpy_variables import CVXPYVariables\nfrom .power_constraint import make_compatible\n\nclass QuadraticConstraint(Constraint):\n\n    \"\"\"\n    State-dependent Action Constraints with the from\n    $`\\sum Q_ij a_i a_j \\leq M`$ \n    \"\"\"\n\n    def __init__(self, max_M, a_dim, s_dim, **kargs):\n        super().__init__(a_dim, s_dim, **kargs)\n        self.max_M = max_M\n        self.sr_max_M = math.sqrt(max_M)\n\n    @abstractmethod\n    def getTensorQ(self, state):\n        pass\n    \n    def getQ(self, state):\n        if isinstance(state, np.ndarray):\n            return self.getTensorQ(to_tensors(state)).numpy()[0]\n        return self.getTensorQ(state)    \n\n    def isConstraintSatisfied(self, state, a, err=1e-2):\n        Q = self.getQ(state)\n        return a.transpose()@Q@a <= self.max_M + err\n\n    def enforceConstraint(self, state, a):\n        Q = self.getQ(state)\n        value = a.transpose()@Q@a\n        if value <= self.max_M:\n            return a\n        else:\n            return math.sqrt(self.max_M / value) * a\n\n    def numConstraints(self):\n        return 1\n    \n    def getL(self, states, centers, v, get_grad:bool = False):\n        Q = self.getQ(states)\n        value = (v[:,:,None]*Q*v[:,None,:]).sum(dim=2).sum(dim=1).clamp(min=1e-3)\n        L = th.sqrt(value/self.max_M)\n        \n        if not get_grad:\n            return L\n        else:\n            return L, (Q*v[:,None,:]).sum(dim=2)/L[:,None]/self.max_M\n\n    def constraintViolation(self, state, a, err=1e-3, normalize=False):\n        Q = self.getQ(state)\n        scale = np.sqrt(self.a_dim / np.trace(Q)+1e-6)\n        value = a.transpose()@Q@a\n        return np.expand_dims(np.maximum(0.0, scale*(np.sqrt(value) - self.sr_max_M) - err),0)\n                \n    def constraintViolationBatch(self, states, actions):\n        Q = self.getQ(states)\n        scale = th.sqrt(self.a_dim / Q.diagonal(dim1=1, dim2=2).sum(axis=1)[:,None,None]+1e-6)\n        value = (actions[:,:,None]*Q*actions[:,None,:]).sum(dim=2).sum(dim=1).clamp(min=1e-3)\n        return th.maximum(scale*(th.sqrt(value)-self.sr_max_M), th.tensor(0.))\n\n    def get_center(self, state):\n        return np.zeros(self.a_dim)", ""]}
{"filename": "action_constrained_rl/nn/additional_layer_sac_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Type, Union, Tuple\n\nimport gym\nimport torch as th\nimport numpy as np\nfrom torch import nn\n", "from torch import nn\n\nfrom stable_baselines3.common.type_aliases import Schedule\nfrom stable_baselines3.common.policies import BasePolicy\nfrom stable_baselines3.sac.policies import Actor, SACPolicy\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom stable_baselines3.common.torch_layers import (\n    BaseFeaturesExtractor,\n    FlattenExtractor,\n    create_mlp,", "    FlattenExtractor,\n    create_mlp,\n)\n\nfrom .additional_layer_policy import TruncateExtractor\n\n# CAP the standard deviation of the actor\nLOG_STD_MAX = 2\nLOG_STD_MIN = -20\n\nclass AdditionalLayerSACActor(Actor):\n\n    \"\"\"\n    Actor for SAlpha or SRad\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        use_sde: bool = False,\n        log_std_init: float = -3,\n        full_std: bool = True,\n        use_expln: bool = False,\n        clip_mean: float = 2.0,\n        normalize_images: bool = True,\n        distribution_class = None, # deformed distribution class\n        constraint = None\n    ):\n        assert distribution_class != None\n        super(Actor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=True,\n        )\n\n        # Save arguments to re-create object at loading\n        self.use_sde = use_sde\n        self.sde_features_extractor = None\n        self.net_arch = net_arch\n        action_dim = get_action_dim(self.action_space)\n        self.a_dim = action_dim\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.log_std_init = log_std_init\n        self.use_expln = use_expln\n        self.full_std = full_std\n        self.clip_mean = clip_mean\n\n        latent_pi_net = create_mlp(features_dim, -1, net_arch, activation_fn)\n        self.latent_pi = nn.Sequential(*latent_pi_net)\n        last_layer_dim = net_arch[-1] if len(net_arch) > 0 else features_dim\n\n        if self.use_sde:\n            self.action_dist = distribution_class(\n                action_dim, constraint, full_std=full_std, use_expln=use_expln, learn_features=True, squash_output=False\n            )\n            self.mu, self.log_std = self.action_dist.proba_distribution_net(\n                latent_dim=last_layer_dim, latent_sde_dim=last_layer_dim, log_std_init=log_std_init\n            )\n            # Avoid numerical issues by limiting the mean of the Gaussian\n            # to be in [-clip_mean, clip_mean]\n            if clip_mean > 0.0:\n                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n        else:\n            self.action_dist = distribution_class(action_dim, constraint)\n            self.mu = nn.Linear(last_layer_dim, action_dim)\n            self.log_std = nn.Linear(last_layer_dim, action_dim)\n        self.flatten = nn.Flatten()\n\n    def get_action_dist_params(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Dict[str, th.Tensor]]:\n\n        \"\"\"\n        Get the parameters for the action distribution.\n\n        :param obs:\n        :return:\n            Mean, standard deviation and optional keyword arguments.\n        \"\"\"\n        # split obs to features and centers\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        latent_pi = self.latent_pi(features)\n        mean_actions = self.mu(latent_pi)\n        if self.use_sde:\n            return mean_actions, self.log_std, dict(states=features, centers=centers, latent_sde=latent_pi)\n\n        # Unstructured exploration (Original implementation)\n        log_std = self.log_std(latent_pi)\n        # Original Implementation to cap the standard deviation\n        log_std = th.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n        return mean_actions, log_std, {\"states\": features, \"centers\": centers}", "LOG_STD_MIN = -20\n\nclass AdditionalLayerSACActor(Actor):\n\n    \"\"\"\n    Actor for SAlpha or SRad\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        use_sde: bool = False,\n        log_std_init: float = -3,\n        full_std: bool = True,\n        use_expln: bool = False,\n        clip_mean: float = 2.0,\n        normalize_images: bool = True,\n        distribution_class = None, # deformed distribution class\n        constraint = None\n    ):\n        assert distribution_class != None\n        super(Actor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=True,\n        )\n\n        # Save arguments to re-create object at loading\n        self.use_sde = use_sde\n        self.sde_features_extractor = None\n        self.net_arch = net_arch\n        action_dim = get_action_dim(self.action_space)\n        self.a_dim = action_dim\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.log_std_init = log_std_init\n        self.use_expln = use_expln\n        self.full_std = full_std\n        self.clip_mean = clip_mean\n\n        latent_pi_net = create_mlp(features_dim, -1, net_arch, activation_fn)\n        self.latent_pi = nn.Sequential(*latent_pi_net)\n        last_layer_dim = net_arch[-1] if len(net_arch) > 0 else features_dim\n\n        if self.use_sde:\n            self.action_dist = distribution_class(\n                action_dim, constraint, full_std=full_std, use_expln=use_expln, learn_features=True, squash_output=False\n            )\n            self.mu, self.log_std = self.action_dist.proba_distribution_net(\n                latent_dim=last_layer_dim, latent_sde_dim=last_layer_dim, log_std_init=log_std_init\n            )\n            # Avoid numerical issues by limiting the mean of the Gaussian\n            # to be in [-clip_mean, clip_mean]\n            if clip_mean > 0.0:\n                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n        else:\n            self.action_dist = distribution_class(action_dim, constraint)\n            self.mu = nn.Linear(last_layer_dim, action_dim)\n            self.log_std = nn.Linear(last_layer_dim, action_dim)\n        self.flatten = nn.Flatten()\n\n    def get_action_dist_params(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Dict[str, th.Tensor]]:\n\n        \"\"\"\n        Get the parameters for the action distribution.\n\n        :param obs:\n        :return:\n            Mean, standard deviation and optional keyword arguments.\n        \"\"\"\n        # split obs to features and centers\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        latent_pi = self.latent_pi(features)\n        mean_actions = self.mu(latent_pi)\n        if self.use_sde:\n            return mean_actions, self.log_std, dict(states=features, centers=centers, latent_sde=latent_pi)\n\n        # Unstructured exploration (Original implementation)\n        log_std = self.log_std(latent_pi)\n        # Original Implementation to cap the standard deviation\n        log_std = th.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n        return mean_actions, log_std, {\"states\": features, \"centers\": centers}", "\n        \nclass AdditionalLayerSACPolicy(SACPolicy):\n    \"\"\"\n    Policy for SAlpha or SRad\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        lr_schedule: Schedule,\n        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        use_sde: bool = False,\n        log_std_init: float = -3,\n        sde_net_arch: Optional[List[int]] = None,\n        use_expln: bool = False,\n        clip_mean: float = 2.0,\n        features_extractor_class: Type[BaseFeaturesExtractor] = TruncateExtractor,\n        features_extractor_kwargs: Optional[Dict[str, Any]] = {},\n        normalize_images: bool = True,\n        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n        n_critics: int = 2,\n        share_features_extractor: bool = False,\n        **kwargs\n    ):\n        self.additional_actor_kwargs = kwargs\n        a_dim = get_action_dim(action_space)\n        features_extractor_kwargs.update({\"a_dim\": a_dim})\n        super(AdditionalLayerSACPolicy, self).__init__(\n            observation_space,\n            action_space,\n            lr_schedule,\n            net_arch,\n            activation_fn,\n            use_sde,\n            log_std_init,\n            sde_net_arch,\n            use_expln,\n            clip_mean,\n            features_extractor_class,\n            features_extractor_kwargs,\n            normalize_images,\n            optimizer_class,\n            optimizer_kwargs,\n            n_critics,\n            share_features_extractor,\n        )\n\n    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None) -> Actor:\n        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n        actor_kwargs.update(self.additional_actor_kwargs)\n        return AdditionalLayerSACActor(**actor_kwargs).to(self.device)", "    \n\nif __name__ == \"__main__\":\n    import numpy as np\n    from stable_baselines3 import DDPG\n    from ..ddpg.projection_ddpg import ProjectionDDPG\n    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n    from ..env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n    \n    cons = HalfCheetahDynamicConstraint()\n    env = gym.make(\"HalfCheetah-v2\")\n    env = ConstraintEnvWrapper(cons, env)\n    n_actions = env.action_space.shape[-1]\n\n    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n    model = ProjectionDDPG(cons, AdditionalLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons, \"layer_type\": None})\n    model.learn(total_timesteps=1000)", "    \n"]}
{"filename": "action_constrained_rl/nn/additional_layer_actor.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nimport gym\nimport torch as th\nimport numpy as np\nfrom torch import nn\n", "from torch import nn\n\nfrom stable_baselines3.common.policies import BasePolicy, ContinuousCritic, register_policy\nfrom stable_baselines3.td3.policies import Actor\nfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom stable_baselines3.common.torch_layers import (\n    BaseFeaturesExtractor,\n    CombinedExtractor,\n    FlattenExtractor,", "    CombinedExtractor,\n    FlattenExtractor,\n    NatureCNN,\n    create_mlp,\n    get_actor_critic_arch,\n)\n\n\nclass AdditionalLayerActor(Actor):\n    \"\"\"\n    Actor network (policy) for DAlpha, DRad.\n\n    :param observation_space: Obervation space\n    :param action_space: Action space\n    :param net_arch: Network architecture\n    :param features_extractor: Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: Number of features\n    :param activation_fn: Activation function\n    :param normalize_images: Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        normalize_images: bool = True,\n        squash_output: bool = False,\n        constraint = None,\n        layer_type = None,\n    ):\n        if constraint is None:\n            raise (\"constraint should not be None\")\n\n        super(Actor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=squash_output,\n        )\n\n        self.net_arch = net_arch\n        action_dim = get_action_dim(self.action_space)\n        self.a_dim = action_dim\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.constraint = constraint\n\n        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n        # Deterministic action\n        self.first_layers = nn.Sequential(*actor_net)\n        self.flatten = nn.Flatten()\n        self.additional_layer = layer_type(self.constraint)\n\n    def forward(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        # split obs to features and centers\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        output = self.first_layers(features)\n        output = self.additional_layer(output, features, centers)\n        return output\n\n    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n        # return output before the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output.cpu().detach().numpy()\n\n    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n        # apply the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        actions = th.tensor(actions, device = self.device).float()\n        return self.additional_layer(actions, features, centers).cpu().detach().numpy()", "class AdditionalLayerActor(Actor):\n    \"\"\"\n    Actor network (policy) for DAlpha, DRad.\n\n    :param observation_space: Obervation space\n    :param action_space: Action space\n    :param net_arch: Network architecture\n    :param features_extractor: Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: Number of features\n    :param activation_fn: Activation function\n    :param normalize_images: Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        normalize_images: bool = True,\n        squash_output: bool = False,\n        constraint = None,\n        layer_type = None,\n    ):\n        if constraint is None:\n            raise (\"constraint should not be None\")\n\n        super(Actor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=squash_output,\n        )\n\n        self.net_arch = net_arch\n        action_dim = get_action_dim(self.action_space)\n        self.a_dim = action_dim\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.constraint = constraint\n\n        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n        # Deterministic action\n        self.first_layers = nn.Sequential(*actor_net)\n        self.flatten = nn.Flatten()\n        self.additional_layer = layer_type(self.constraint)\n\n    def forward(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        # split obs to features and centers\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        output = self.first_layers(features)\n        output = self.additional_layer(output, features, centers)\n        return output\n\n    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n        # return output before the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output.cpu().detach().numpy()\n\n    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n        # apply the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        centers = self.flatten(obs)[:,-self.a_dim:].to(features.dtype)\n        actions = th.tensor(actions, device = self.device).float()\n        return self.additional_layer(actions, features, centers).cpu().detach().numpy()", ""]}
{"filename": "action_constrained_rl/nn/__init__.py", "chunked_list": [""]}
{"filename": "action_constrained_rl/nn/additional_layer_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nimport gym\nimport torch as th\nimport numpy as np\nfrom torch import nn\n", "from torch import nn\n\nfrom stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3.common.torch_layers import (\n    BaseFeaturesExtractor,\n)\nfrom stable_baselines3.common.preprocessing import get_flattened_obs_dim, get_action_dim\n    \nfrom stable_baselines3.common.type_aliases import Schedule\nfrom .additional_layer_actor import AdditionalLayerActor", "from stable_baselines3.common.type_aliases import Schedule\nfrom .additional_layer_actor import AdditionalLayerActor\n\nclass TruncateExtractor(BaseFeaturesExtractor):\n    \"\"\"\n    Extractor to take original observations from concatenated observations with centers\n    \"\"\"\n    def __init__(self, observation_space: gym.Space, a_dim: int):\n        super().__init__(observation_space, get_flattened_obs_dim(observation_space) - a_dim)\n        self.flatten = nn.Flatten()\n        self.a_dim = a_dim\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.flatten(observations)[:,:-self.a_dim]", "    \nclass AdditionalLayerPolicy(TD3Policy):\n    \"\"\"\n    policy for DAlpha, DRad\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        lr_schedule: Schedule,\n        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        features_extractor_class: Type[BaseFeaturesExtractor] = TruncateExtractor,\n        features_extractor_kwargs: Optional[Dict[str, Any]] = {},\n        normalize_images: bool = True,\n        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n        n_critics: int = 2,\n        share_features_extractor: bool = True,\n        **kwargs\n    ):\n        self.kwargs = kwargs\n        a_dim = get_action_dim(action_space)\n        features_extractor_kwargs.update({\"a_dim\": a_dim})\n        super(AdditionalLayerPolicy, self).__init__(\n            observation_space,\n            action_space,\n            lr_schedule,\n            net_arch,\n            activation_fn,\n            features_extractor_class,\n            features_extractor_kwargs,\n            normalize_images,\n            optimizer_class,\n            optimizer_kwargs,\n            n_critics,\n            share_features_extractor,\n        )\n\n    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None):\n        self.actor_kwargs.update(self.kwargs)\n        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n        return AdditionalLayerActor(**actor_kwargs).to(self.device)", "\n    \nif __name__ == \"__main__\":\n    import numpy as np\n    from stable_baselines3 import DDPG\n    from ..ddpg.projection_ddpg import ProjectionDDPG\n    from ..half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n    from ..env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n    \n    cons = HalfCheetahDynamicConstraint()\n    env = gym.make(\"HalfCheetah-v2\")\n    env = ConstraintEnvWrapper(cons, env)\n    n_actions = env.action_space.shape[-1]\n\n    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n    model = ProjectionDDPG(cons, AdditionalLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons, \"layer_type\": None})\n    model.learn(total_timesteps=1000)", "    \n"]}
{"filename": "action_constrained_rl/nn/additional_layers/chebyshev_center.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport gurobipy as gp\nimport numpy as np\n\ndef calc_chebyshev_center(C, d):\n    m, n = C.shape\n    with gp.Model() as model:\n        x = []\n        for i in range(n):\n            x.append(model.addVar(lb = -1, ub = 1, vtype = gp.GRB.CONTINUOUS))\n        r = model.addVar(lb = 0, ub = gp.GRB.INFINITY, vtype = gp.GRB.CONTINUOUS)\n        model.setObjective(r, sense = gp.GRB.MAXIMIZE)\n        norms = np.linalg.norm(C, axis=1)\n        for j in range(m):\n            exp = gp.LinExpr()\n            for i in range(n):\n                exp+=C[j,i]*x[i]\n            exp+=norms[j]*r\n            model.addConstr(exp <= d[j])\n        model.optimize()\n        x_value = np.array(model.X[0:n])\n    return x_value", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/radial_squash.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .analytic_center import calc_analytic_center\n\nclass SquashLayer(torch.nn.Module):\n    def __init__(self, cons):\n        super(SquashLayer, self).__init__()\n        self.cons = cons\n\n    def forward(self, actions, states, centers):\n        v = actions - centers\n        L = self.cons.getL(states, centers, v)\n        return centers + (torch.tanh(L) / (L+1e-9))[:,None] * v", "\nclass SquashLayer(torch.nn.Module):\n    def __init__(self, cons):\n        super(SquashLayer, self).__init__()\n        self.cons = cons\n\n    def forward(self, actions, states, centers):\n        v = actions - centers\n        L = self.cons.getL(states, centers, v)\n        return centers + (torch.tanh(L) / (L+1e-9))[:,None] * v", "\n# unused layer\nclass SmoothSquashLayer(torch.nn.Module):\n    def __init__(self, cons):\n        super(SmoothSquashLayer, self).__init__()\n        self.cons = cons\n\n    def forward(self, actions, states, centers):\n        C=self.cons.C(states)\n        d=self.cons.d(states)\n        v = actions - centers\n        b = torch.maximum((v[:,None,:]*C).sum(axis=2) / (d-(centers[:,None,:]*C).sum(axis=2)), torch.tensor(0.))\n        r = torch.linalg.norm(v, axis = 1)\n        p = torch.minimum(2*torch.exp(r).detach(),torch.tensor(10.))\n        return centers + (torch.pow(torch.pow(b,p[:,None]).sum(axis=1),-1/p) * torch.tanh(torch.linalg.norm(b, axis = 1)))[:,None] * v", "\nif __name__ == \"__main__\":\n    import numpy as np\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\n\n    cons = HalfCheetahDynamicConstraint()\n    state = np.repeat(10.0, 17)\n\n    C=cons.C(state)\n    d=cons.d(state)\n    layer=SquashLayer(cons)\n    print(C,d)\n    for t in range(100):\n        action=np.random.rand(6)\n        print(action)\n        result = layer.forward(action, state)\n        print(result)\n        assert cons.isConstraintSatisfied(state, result)", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/visualize.py", "chunked_list": ["import matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport torch\n\nfrom ...constraint import LinearConstraint\n\nclass TestConstraint(LinearConstraint):\n    def __init__(self, err=1e-3):\n        self.err = err\n\n    def numConstraints(self):\n        return 4\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n\n    def tensor_C(self, state):\n        return torch.tensor([[1.,1.],[-2.,1.],[0.,-1.],[1.,0.]]).repeat(state.shape[0], 1, 1)\n  \n    def tensor_d(self, state):\n        return torch.tensor([1.,1.,1.,1.]).repeat(state.shape[0], 1)", "class TestConstraint(LinearConstraint):\n    def __init__(self, err=1e-3):\n        self.err = err\n\n    def numConstraints(self):\n        return 4\n\n    def E(self, state):\n        return self.C(state)\n\n    def f(self, state):\n        return self.d(state)\n\n    def tensor_C(self, state):\n        return torch.tensor([[1.,1.],[-2.,1.],[0.,-1.],[1.,0.]]).repeat(state.shape[0], 1, 1)\n  \n    def tensor_d(self, state):\n        return torch.tensor([1.,1.,1.,1.]).repeat(state.shape[0], 1)", "    \n\nif __name__ == \"__main__\":\n\n    from .alpha_projection import AlphaProjectionLayer\n    from .radial_shrinkage import ShrinkageLayer\n    from .radial_shrinkage import SmoothShrinkageLayer\n    import torch\n    import matplotlib.path\n\n    \n    cons = TestConstraint()\n    a_layer = AlphaProjectionLayer(cons)\n    s_layer = ShrinkageLayer(cons)\n    ss_layer = SmoothShrinkageLayer(cons)\n    N=100\n    actions = torch.Tensor(N, 2)\n    states = torch.Tensor(N, 2)\n    for i in range(N):\n        actions[i] = torch.Tensor(3 * torch.rand(2) - torch.tensor([1.5, 1.5]))\n        states[i] = torch.Tensor([0.,0.])\n    for layer in [ss_layer, s_layer, a_layer]:\n        res = layer.forward(actions, states)\n        fig, ax = plt.subplots()\n\n        ax.add_patch(patches.Polygon([[0,1], [-1,-1], [1, -1], [1,0]], fill = False))\n        plt.xlim(-2,2)\n        plt.ylim(-2,2)\n\n        for i in range(N):\n            ver1 = actions[i].detach().numpy()\n            ver2 = res[i].detach().numpy()\n            path = matplotlib.path.Path([ver1, ver2])\n            ax.add_patch(patches.Circle(ver1, radius = 0.01, color = 'r'))\n            ax.add_patch(patches.Circle(ver2, radius = 0.01, color = 'b'))\n            ax.add_patch(patches.PathPatch(path))\n        plt.show()", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/alpha_distribution.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\nfrom stable_baselines3.common.distributions import DiagGaussianDistribution, StateDependentNoiseDistribution\nimport torch as th\nimport numpy as np\nimport sys\n\nfrom torch.special import erf", "\nfrom torch.special import erf\n\nsqrt2 = np.sqrt(2.)\nlog2 = np.log(2.)\nsqrtpi = np.sqrt(np.pi)\nsqrt2pi = sqrt2*sqrtpi\nlgsqrt2pi = np.log(sqrt2pi)\nlg2sqrtpi = np.log(2*sqrtpi)\n", "lg2sqrtpi = np.log(2*sqrtpi)\n\nLOG_STD_MAX = 2\n# LOG_PROB_MAX = 1e6\n\ndef radial_integral_gaussian(means, log_std, centers, v, epsilon:float = 1e-300):\n    # calculate integration of gaussian distribution\n    \n    dtp=means.dtype\n    log_std=log_std.to(th.float64)\n    v=v.to(th.float64)\n    dim = means.shape[1]\n    std = log_std.exp()\n    a = v / std / sqrt2\n    b = (centers-means).to(th.float64) / std / sqrt2\n    A = a.norm(dim=1)\n    na = a / A[:,None]\n    B = (na*b).sum(dim=1)\n    Cross = na[:,:,None]*b[:,None,:]-b[:,:,None]*na[:,None,:]\n    C = 1./2. * Cross.square().sum(dim=(1,2)) # C = (b*b).sum(axis=1) - B*B\n\n    log_prob = dim*th.log(v.norm(dim=1)+epsilon) - log_std.sum(dim=1) - dim*lgsqrt2pi - C\n    IA = A**-1\n    BA = IA*B\n    D = IA*th.exp(-(A+B)**2)\n    E = sqrtpi * (erf(A+B)-1.)\n    if dim == 1:\n        log_prob += th.log(-IA*E + epsilon) - log2\n    elif dim == 2:\n        log_prob += th.log(IA*D + IA*BA * E + epsilon) - log2\n    elif dim == 3:\n        log_prob += th.log(2 * IA*(IA-BA)*D - IA*(2 * BA ** 2 + IA**2) * E + epsilon) - 2 * log2\n    elif dim == 4:\n        log_prob += th.log(2 * IA*(IA**2 + 1 - BA + BA**2)*D + IA*BA*(2 * BA ** 2 + 3*IA**2) * E + epsilon) - 2 * log2\n    elif dim == 5:\n        log_prob += th.log((6 * IA**3 + 4*IA - 10 * IA**3*BA - 4 * IA*BA + 4*IA*BA**2-4*IA*BA**3)*D - (3*IA**5+ 12 * IA**3*BA ** 2 + 4*IA*BA**4) * E + epsilon) - 3 * log2\n    elif dim == 6:\n        log_prob += th.log(2*IA*(2 + (2+9*IA**2)*BA**2-(2+7*IA**2)*BA+4*IA**2-2*BA**3+2*BA**4+4*IA**4) * D + IA*BA*(4*BA**4+20*IA**2*BA**2+15*IA**4) * E + epsilon) - 3 * log2\n    else:\n        raise ValueError(\"We do not implemented for this action space dimension\")\n     \n    return log_prob.to(dtp)", "\nclass AlphaGaussianDistribution(DiagGaussianDistribution):\n    \"\"\"\n    Gaussian distribution with diagonal covariance matrix, followed by a alpha projection to ensure bounds.\n\n    :param action_dim: Dimension of the action space.\n    \"\"\"\n\n    def __init__(self, action_dim: int, constraint, epsilon: float = 1e-300):\n        super().__init__(action_dim)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self.epsilon = epsilon\n        self.cons = constraint\n        self.gaussian_actions = None\n\n\n    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n        assert states != None and centers != None\n        self.proba_distribution(mean_actions, log_std)\n        if deterministic:\n            self.gaussian_actions = super().mode()\n        else:\n            self.gaussian_actions = super().sample()\n        self.v = self.gaussian_actions - centers\n        if not calc_prob:\n            self.L = self.cons.getL(states, centers, self.v)\n        else:\n            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n        actions = centers + self.v / th.maximum(self.L, th.tensor(1.))[:,None]\n        return actions\n\n    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n        assert states != None and centers != None\n        actions = self.actions_from_params(mean_actions, log_std, states, centers, calc_prob = True)\n        inside = th.lt(self.L, 1.)\n        inside_log_prob = super().log_prob(self.gaussian_actions) # log_prob for inside of constraints\n\n        # calculate log_prob for outside of constraints\n\n        # calculate measure unit\n        v_norm = self.v.norm(dim=1)\n        coss = (self.gradL*self.v).sum(axis=1)/self.gradL.norm(dim=1)/v_norm\n        darea = v_norm/self.L/coss\n\n        outside_log_prob = radial_integral_gaussian(mean_actions, log_std, centers, self.v/self.L[:,None], self.epsilon) - th.log(darea)\n\n        log_prob = th.where(inside, inside_log_prob, outside_log_prob)\n        return actions, log_prob", "\nclass AlphaStateDependentNoiseDistribution(StateDependentNoiseDistribution):\n    \"\"\"\n    Gaussian distribution with diagonal covariance matrix, followed by a alpha projection to ensure bounds.\n    State dependent version\n\n    :param action_dim: Dimension of the action space.\n    \"\"\"\n\n    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-300, **kargs):\n        super().__init__(action_dim, **kargs)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self._epsilon = _epsilon\n        self.cons = constraint\n        self.gaussian_actions = None\n\n\n    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n        assert states != None and centers != None\n#        mean_actions = th.where(mean_actions.isnan(), centers, mean_actions)\n#        log_std = th.where(log_std.isnan(), LOG_STD_MAX, log_std)\n        self.proba_distribution(mean_actions, log_std, latent_sde)\n        if deterministic:\n            self.gaussian_actions = super().mode()\n        else:\n            self.gaussian_actions = super().sample()\n        self.v = self.gaussian_actions - centers\n        if not calc_prob:\n            self.L = self.cons.getL(states, centers, self.v)\n        else:\n            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n        actions = centers + self.v / th.maximum(self.L, th.tensor(1.))[:,None]\n        return actions\n\n    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n        assert states != None and centers != None\n        actions = self.actions_from_params(mean_actions, log_std, latent_sde, states, centers, calc_prob = True)\n        inside = th.lt(self.L, 1.)\n        inside_log_prob = super().log_prob(self.gaussian_actions) # log_prob for inside of constraints\n\n        # calculate log_prob for outside of constraints\n\n        # calculate measure unit\n        v_norm = self.v.norm(dim=1)\n        coss = (self.gradL*self.v).sum(axis=1)/self.gradL.norm(dim=1)/v_norm\n        darea = v_norm/self.L/coss\n\n        self._latent_sde = latent_sde if self.learn_features else latent_sde.detach()\n        variance = th.mm(self._latent_sde**2, self.get_std(log_std) ** 2)\n        distribution_std = th.sqrt(variance + self.epsilon)\n\n        outside_log_prob = radial_integral_gaussian(mean_actions, distribution_std, centers, self.v/self.L[:,None], self._epsilon) - th.log(darea)\n\n        log_prob = th.where(inside, inside_log_prob, outside_log_prob)\n            \n        return actions, log_prob", "\nif __name__ == \"__main__\":\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    from ...constraint.power_constraint import PowerConstraint\n    from torch import autograd\n\n    cons = PowerConstraint(11, (1,1,1,1,1,1), 10.,17)\n    dist = AlphaGaussianDistribution(6, constraint = cons)\n    N=1\n    mean_actions = th.tensor([[-6.2313e+08,  3.6953e+08,  5.6804e+08,  3.2498e+08, -5.9962e+08,\n         3.3148e+08]], dtype = th.float32, requires_grad=True)\n    log_std = th.tensor([[  2.,   2.,   2., -20.,   2., -20.]], dtype = th.float32, requires_grad=True)\n    states = th.tensor([[  1.1587,  -0.6799,  -0.7305,  -0.2797,   0.6688,  -0.4330,  -0.3629,\n          0.6346,  -1.9232,  -2.1278, -10.0000, -10.0000,   7.1536,  10.0000,\n        -10.0000,  -9.6341, -10.0000]], dtype = th.float32)\n    centers = th.tensor([[ 0.1698, -0.1698, -0.1698,  0.1698,  0.1698,  0.1698]], dtype = th.float32)\n    actions, log_prob = dist.log_prob_from_params(mean_actions, log_std, states = states, centers = centers)\n    print(actions, log_prob)\n    with autograd.detect_anomaly():\n        log_prob.backward()\n    print(mean_actions.grad, log_std.grad)\n    exit()\n\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n\n    probs = log_prob.exp().numpy()\n    colors = [[1,0,0,min(i,10.)/10.] for i in probs]\n    plt.scatter(actions[:,0].numpy(), actions[:,1].numpy(), c = colors)\n    plt.xlim([-2,2])\n    plt.ylim([-2,2])\n    plt.show()", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/analytic_center.py", "chunked_list": ["import cvxpy\nimport torch\ndef calc_analytic_center(C, d):\n    n = C.shape[1]\n    x = cvxpy.Variable(n)\n    obj = cvxpy.Maximize(cvxpy.sum(cvxpy.log((d - C @ x))))\n    prob = cvxpy.Problem(obj, [C @ x <= d])\n    prob.solve()\n    return x.value\n", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/alpha_projection.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .analytic_center import calc_analytic_center\nfrom .chebyshev_center import calc_chebyshev_center\n\nclass AlphaProjectionLayer(torch.nn.Module):\n    def __init__(self, cons):\n        super(AlphaProjectionLayer, self).__init__()\n        self.cons = cons\n\n    def forward(self, actions, states, centers):\n        v = actions - centers\n        L = self.cons.getL(states, centers, v)\n        ret = centers + v / torch.maximum(L, torch.tensor(1))[:,None]\n        return centers + v / torch.maximum(L, torch.tensor(1))[:,None]", "from .chebyshev_center import calc_chebyshev_center\n\nclass AlphaProjectionLayer(torch.nn.Module):\n    def __init__(self, cons):\n        super(AlphaProjectionLayer, self).__init__()\n        self.cons = cons\n\n    def forward(self, actions, states, centers):\n        v = actions - centers\n        L = self.cons.getL(states, centers, v)\n        ret = centers + v / torch.maximum(L, torch.tensor(1))[:,None]\n        return centers + v / torch.maximum(L, torch.tensor(1))[:,None]", "\nif __name__ == \"__main__\":\n    import numpy as np\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\n\n    cons = HalfCheetahDynamicConstraint()\n    states = torch.tensor(10.0).repeat(10,17)\n\n    C=cons.C(states)\n    d=cons.d(states)\n    layer=AlphaProjectionLayer(cons)\n    print(C,d)\n    actions = torch.rand((10,6))\n    print(actions)\n    results = layer.forward(actions, states)\n    print(results)\n    for t in range(10):\n        assert cons.isConstraintSatisfied(states[t], results[t])\n        assert not cons.isConstraintSatisfied(states[t], actions[t]) or torch.allclose(actions[t], results[t])", ""]}
{"filename": "action_constrained_rl/nn/additional_layers/shrinked_distribution.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\nfrom stable_baselines3.common.distributions import DiagGaussianDistribution, StateDependentNoiseDistribution\nimport torch as th\nimport numpy as np\n\nfrom torch.special import erf\n\nclass ShrinkedGaussianDistribution(DiagGaussianDistribution):\n    \"\"\"\n    Gaussian distribution with diagonal covariance matrix, followed by a radial shrinkage to ensure bounds.\n\n    :param action_dim: Dimension of the action space.\n    \"\"\"\n\n    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-6):\n        super().__init__(action_dim)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self._epsilon = _epsilon\n        self.cons = constraint\n        self.gaussian_actions = None\n\n\n    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n        assert states != None and centers != None\n        self.proba_distribution(mean_actions, log_std)\n        if deterministic:\n            self.gaussian_actions = super().mode()\n        else:\n            self.gaussian_actions = super().sample()\n        self.v = self.gaussian_actions - centers\n        if not calc_prob:\n            self.L = self.cons.getL(states, centers, self.v)\n        else:\n            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n        self.tanhL = th.tanh(self.L)\n        actions = centers + (self.tanhL / (self.L+1e-9))[:,None] * self.v\n        return actions\n\n    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n        assert states != None and centers != None\n        device = mean_actions.device\n        actions = self.actions_from_params(mean_actions, log_std, states, centers, calc_prob = True)\n        log_prob = super().log_prob(self.gaussian_actions)\n\n        # calculate gradient of L\n        gradsc = ((self.L*(1-self.tanhL**2)-self.tanhL)/((self.L+1e-9)**2))[:,None] * self.gradL\n        jacob = (self.tanhL/(self.L+1e-9))[:,None,None] * th.eye(self.action_dim, device = device)[None,:,:]\n        jacob += gradsc[:,None,:] * self.v[:,:,None]\n        log_prob -= th.log(th.linalg.det(jacob)+self._epsilon)\n        return actions, log_prob", "from torch.special import erf\n\nclass ShrinkedGaussianDistribution(DiagGaussianDistribution):\n    \"\"\"\n    Gaussian distribution with diagonal covariance matrix, followed by a radial shrinkage to ensure bounds.\n\n    :param action_dim: Dimension of the action space.\n    \"\"\"\n\n    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-6):\n        super().__init__(action_dim)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self._epsilon = _epsilon\n        self.cons = constraint\n        self.gaussian_actions = None\n\n\n    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n        assert states != None and centers != None\n        self.proba_distribution(mean_actions, log_std)\n        if deterministic:\n            self.gaussian_actions = super().mode()\n        else:\n            self.gaussian_actions = super().sample()\n        self.v = self.gaussian_actions - centers\n        if not calc_prob:\n            self.L = self.cons.getL(states, centers, self.v)\n        else:\n            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n        self.tanhL = th.tanh(self.L)\n        actions = centers + (self.tanhL / (self.L+1e-9))[:,None] * self.v\n        return actions\n\n    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n        assert states != None and centers != None\n        device = mean_actions.device\n        actions = self.actions_from_params(mean_actions, log_std, states, centers, calc_prob = True)\n        log_prob = super().log_prob(self.gaussian_actions)\n\n        # calculate gradient of L\n        gradsc = ((self.L*(1-self.tanhL**2)-self.tanhL)/((self.L+1e-9)**2))[:,None] * self.gradL\n        jacob = (self.tanhL/(self.L+1e-9))[:,None,None] * th.eye(self.action_dim, device = device)[None,:,:]\n        jacob += gradsc[:,None,:] * self.v[:,:,None]\n        log_prob -= th.log(th.linalg.det(jacob)+self._epsilon)\n        return actions, log_prob", "\nclass ShrinkedStateDependentNoiseDistribution(StateDependentNoiseDistribution):\n    \"\"\"\n    Gaussian distribution with diagonal covariance matrix, followed by a radial shrinkage to ensure bounds.\n    State dependent version\n    :param action_dim: Dimension of the action space.\n    \"\"\"\n\n    def __init__(self, action_dim: int, constraint, _epsilon: float = 1e-6, **kargs):\n        super().__init__(action_dim, **kargs)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self._epsilon = _epsilon\n        self.cons = constraint\n        self.gaussian_actions = None\n\n\n    def actions_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None, deterministic:bool = False, calc_prob:bool = False) -> th.Tensor:\n        assert states != None and centers != None\n        self.proba_distribution(mean_actions, log_std, latent_sde)\n        if deterministic:\n            self.gaussian_actions = super().mode()\n        else:\n            self.gaussian_actions = super().sample()\n        self.v = self.gaussian_actions - centers\n        if not calc_prob:\n            self.L = self.cons.getL(states, centers, self.v)\n        else:\n            self.L, self.gradL = self.cons.getL(states, centers, self.v, get_grad = True)\n        self.tanhL = th.tanh(self.L)\n        actions = centers + (self.tanhL / (self.L+1e-9))[:,None] * self.v\n        return actions\n\n    def log_prob_from_params(self, mean_actions: th.Tensor, log_std: th.Tensor, latent_sde: th.Tensor, states: th.Tensor = None, centers: th.Tensor = None) -> Tuple[th.Tensor, th.Tensor]:\n        assert states != None and centers != None\n        device = mean_actions.device\n        actions = self.actions_from_params(mean_actions, log_std, latent_sde, states, centers, calc_prob = True)\n        log_prob = super().log_prob(self.gaussian_actions)\n\n        # calculate gradient of L\n        gradsc = ((self.L*(1-self.tanhL**2)-self.tanhL)/((self.L+1e-9)**2))[:,None] * self.gradL\n        jacob = (self.tanhL/(self.L+1e-9))[:,None,None] * th.eye(self.action_dim, device = device)[None,:,:]\n        jacob += gradsc[:,None,:] * self.v[:,:,None]\n        log_prob -= th.log(th.linalg.det(jacob)+self._epsilon)\n        return actions, log_prob", "\nif __name__ == \"__main__\":\n\n    from .visualize import TestConstraint\n\n    cons = TestConstraint()\n    dist = ShrinkedGaussianDistribution(2, constraint = cons)\n    N=1000\n    mean_actions = th.tensor([[0.,0.]], dtype = th.float32).repeat(N,1)\n    log_std = th.tensor([[-1,-1]], dtype = th.float32).repeat(N,1)\n    states = th.zeros((N,2))\n    centers = th.zeros((N,2))\n    actions, log_prob = dist.log_prob_from_params(mean_actions, log_std, states = states, centers = centers)\n\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n\n    probs = log_prob.exp().numpy()\n    colors = [[1,0,0,min(i,10.)/10.] for i in probs]\n    plt.scatter(actions[:,0].numpy(), actions[:,1].numpy(), c = colors)\n    plt.xlim([-2,2])\n    plt.ylim([-2,2])\n    plt.show()", ""]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom cvxpylayers.torch import CvxpyLayer\nimport cvxpy as cp\n\nfrom ...cvxpy_variables import CVXPYVariables", "\nfrom ...cvxpy_variables import CVXPYVariables\n\nclass OptLayer(torch.nn.Module):\n    \"\"\"\n    wrapper of cvxpylayer\n    \"\"\"\n    def __init__(self, cons):\n        super(OptLayer, self).__init__()\n        cvxpy_vars = cons.cvxpy_variables()\n\n        if cvxpy_vars.s is None:\n            self.state_dependence = False\n            self.layer = CvxpyLayer(cvxpy_vars.prob, parameters=[cvxpy_vars.q], variables=[cvxpy_vars.x])\n        else:\n            self.state_dependence = True\n            self.layer = CvxpyLayer(cvxpy_vars.prob, parameters=[cvxpy_vars.q, cvxpy_vars.s], variables=[cvxpy_vars.x])\n\n    def forward(self, x, s):\n        if self.state_dependence:\n            return self.layer(x, s)[0]\n        else:\n            return self.layer(x)[0]", "        \n\nif __name__ == \"__main__\":\n    import numpy as np\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\n    cons = HalfCheetahDynamicConstraint()\n    layer = OptLayer(cons)\n\n    x = torch.rand((100, 6))\n    s = torch.rand((100, 17))\n    y = layer(x,s)\n    for t in range(x.shape[0]):\n        assert cons.isConstraintSatisfied(s[t], y[t])\n        assert not cons.isConstraintSatisfied(s[t], x[t]) or torch.allclose(x[t], y[t], rtol = 1e-2)\n\n    # import diffcp\n\n    # diffcp.solve_and_derivative_batch()\n        \n    for param in layer.parameters():\n        print(param)", "\n"]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer_actor.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nimport gym\nimport torch as th\nimport numpy as np\nfrom torch import nn\n", "from torch import nn\n\nfrom stable_baselines3.common.policies import BasePolicy, ContinuousCritic, register_policy\nfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom stable_baselines3.common.torch_layers import (\n    BaseFeaturesExtractor,\n    CombinedExtractor,\n    FlattenExtractor,\n    NatureCNN,", "    FlattenExtractor,\n    NatureCNN,\n    create_mlp,\n    get_actor_critic_arch,\n)\n\nfrom .opt_layer import OptLayer\n\nclass OptLayerActor(BasePolicy):\n    \"\"\"\n    Actor network (policy) for DOpt.\n\n    :param observation_space: Obervation space\n    :param action_space: Action space\n    :param net_arch: Network architecture\n    :param features_extractor: Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: Number of features\n    :param activation_fn: Activation function\n    :param normalize_images: Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        normalize_images: bool = True,\n        squash_output = True,\n        constraint = None,\n    ):\n        if constraint is None:\n            raise (\"constraint should not be None\")\n\n        super(OptLayerActor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=squash_output,\n        )\n\n        self.net_arch = net_arch\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.constraint = constraint\n\n        action_dim = get_action_dim(self.action_space)\n        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n        # Deterministic action\n        self.first_layers = nn.Sequential(*actor_net)\n        self.opt_layer = OptLayer(self.constraint)\n        self.tanh = nn.Tanh()\n\n    def _get_constructor_parameters(self) -> Dict[str, Any]:\n        data = super()._get_constructor_parameters()\n\n        data.update(\n            dict(\n                net_arch=self.net_arch,\n                features_dim=self.features_dim,\n                activation_fn=self.activation_fn,\n                features_extractor=self.features_extractor,\n            )\n        )\n        return data\n\n    def forward(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        output = self.opt_layer(output, features)\n        return output\n\n    def forward_before_projection(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.forward(observation)\n\n    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n        # return output before the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output.cpu().detach().numpy()\n\n    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n        # apply the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        actions = th.tensor(actions, device = self.device).float()\n        return self.opt_layer(actions, features).cpu().detach().numpy()", "class OptLayerActor(BasePolicy):\n    \"\"\"\n    Actor network (policy) for DOpt.\n\n    :param observation_space: Obervation space\n    :param action_space: Action space\n    :param net_arch: Network architecture\n    :param features_extractor: Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: Number of features\n    :param activation_fn: Activation function\n    :param normalize_images: Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        net_arch: List[int],\n        features_extractor: nn.Module,\n        features_dim: int,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        normalize_images: bool = True,\n        squash_output = True,\n        constraint = None,\n    ):\n        if constraint is None:\n            raise (\"constraint should not be None\")\n\n        super(OptLayerActor, self).__init__(\n            observation_space,\n            action_space,\n            features_extractor=features_extractor,\n            normalize_images=normalize_images,\n            squash_output=squash_output,\n        )\n\n        self.net_arch = net_arch\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.constraint = constraint\n\n        action_dim = get_action_dim(self.action_space)\n        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=squash_output)\n        # Deterministic action\n        self.first_layers = nn.Sequential(*actor_net)\n        self.opt_layer = OptLayer(self.constraint)\n        self.tanh = nn.Tanh()\n\n    def _get_constructor_parameters(self) -> Dict[str, Any]:\n        data = super()._get_constructor_parameters()\n\n        data.update(\n            dict(\n                net_arch=self.net_arch,\n                features_dim=self.features_dim,\n                activation_fn=self.activation_fn,\n                features_extractor=self.features_extractor,\n            )\n        )\n        return data\n\n    def forward(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        output = self.opt_layer(output, features)\n        return output\n\n    def forward_before_projection(self, obs: th.Tensor) -> th.Tensor:\n        # assert deterministic, 'The TD3 actor only outputs deterministic actions'\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.forward(observation)\n\n    def undeformed_predict(self, obs: np.ndarray) -> th.Tensor:\n        # return output before the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        output = self.first_layers(features)\n        return output.cpu().detach().numpy()\n\n    def deform_action(self, actions: np.ndarray, obs: np.ndarray) -> th.Tensor:\n        # apply the final layer\n        obs, _ = self.obs_to_tensor(obs)\n        features = self.extract_features(obs)\n        actions = th.tensor(actions, device = self.device).float()\n        return self.opt_layer(actions, features).cpu().detach().numpy()", "\nif __name__ == \"__main__\":\n    import numpy as np\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    \n    env = gym.make(\"HalfCheetah-v2\")\n    cons = HalfCheetahDynamicConstraint()\n\n    actor = OptLayerActor(env.observation_space, env.action_space, [30, 20], FlattenExtractor(env.observation_space), 17, squash_output=False, constraint = cons)\n    print(env.observation_space)\n    obs = th.ones(1, 17)\n    output = actor(obs)\n    output.sum().backward()\n    for param in actor.parameters():\n        print(param)\n        print(param.grad)", ""]}
{"filename": "action_constrained_rl/nn/opt_layer/opt_layer_policy.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nimport gym\nimport torch as th\nimport numpy as np\nfrom torch import nn\n", "from torch import nn\n\nfrom stable_baselines3.td3.policies import TD3Policy\nfrom stable_baselines3.common.torch_layers import (\n    BaseFeaturesExtractor,\n    FlattenExtractor,\n)\nfrom stable_baselines3.common.type_aliases import Schedule\nfrom .opt_layer_actor import OptLayerActor\n\nclass OptLayerPolicy(TD3Policy):\n    \"\"\"\n    Policy for DOpt\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        lr_schedule: Schedule,\n        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n        normalize_images: bool = True,\n        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n        n_critics: int = 2,\n        share_features_extractor: bool = True,\n        **kwargs\n    ):\n        self.kwargs = kwargs\n        super(OptLayerPolicy, self).__init__(\n            observation_space,\n            action_space,\n            lr_schedule,\n            net_arch,\n            activation_fn,\n            features_extractor_class,\n            features_extractor_kwargs,\n            normalize_images,\n            optimizer_class,\n            optimizer_kwargs,\n            n_critics,\n            share_features_extractor,\n        )\n\n    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None):\n        self.actor_kwargs.update(self.kwargs)\n        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n        return OptLayerActor(**actor_kwargs).to(self.device)\n  \n    def forward(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.actor.forward(observation)\n  \n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        # Note: the deterministic deterministic parameter is ignored in the case of TD3.\n        #   Predictions are always deterministic.\n        return self.actor._predict(observation)", "from .opt_layer_actor import OptLayerActor\n\nclass OptLayerPolicy(TD3Policy):\n    \"\"\"\n    Policy for DOpt\n    \"\"\"\n    def __init__(\n        self,\n        observation_space: gym.spaces.Space,\n        action_space: gym.spaces.Space,\n        lr_schedule: Schedule,\n        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n        activation_fn: Type[nn.Module] = nn.ReLU,\n        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n        normalize_images: bool = True,\n        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n        n_critics: int = 2,\n        share_features_extractor: bool = True,\n        **kwargs\n    ):\n        self.kwargs = kwargs\n        super(OptLayerPolicy, self).__init__(\n            observation_space,\n            action_space,\n            lr_schedule,\n            net_arch,\n            activation_fn,\n            features_extractor_class,\n            features_extractor_kwargs,\n            normalize_images,\n            optimizer_class,\n            optimizer_kwargs,\n            n_critics,\n            share_features_extractor,\n        )\n\n    def make_actor(self, features_extractor: Optional[BaseFeaturesExtractor] = None):\n        self.actor_kwargs.update(self.kwargs)\n        actor_kwargs = self._update_features_extractor(self.actor_kwargs, features_extractor)\n        return OptLayerActor(**actor_kwargs).to(self.device)\n  \n    def forward(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.actor.forward(observation)\n  \n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        # Note: the deterministic deterministic parameter is ignored in the case of TD3.\n        #   Predictions are always deterministic.\n        return self.actor._predict(observation)", "\nif __name__ == \"__main__\":\n    import numpy as np\n    from stable_baselines3 import DDPG\n    from ...ddpg.projection_ddpg import ProjectionDDPG\n    from ...half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n    from ...env_wrapper.constraint_wrapper import ConstraintEnvWrapper\n    \n    cons = HalfCheetahDynamicConstraint()\n    env = gym.make(\"HalfCheetah-v2\")\n    env = ConstraintEnvWrapper(cons, env)\n    n_actions = env.action_space.shape[-1]\n\n    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.01 * np.ones(n_actions))\n    model = ProjectionDDPG(cons, OptLayerPolicy, env, action_noise=action_noise, verbose=2, batch_size=8, policy_kwargs = {\"constraint\": cons})\n    model.learn(total_timesteps=1000)", ""]}
{"filename": "action_constrained_rl/nn/opt_layer/tests/test_opt_layer.py", "chunked_list": ["import torch\nfrom ..opt_layer import OptLayer\nfrom pytest import approx\nfrom ....half_cheetah.half_cheetah_dynamic_constraint import HalfCheetahDynamicConstraint\n\ndef test_opt_layer_half_cheetah():\n    import numpy as np\n    cons = HalfCheetahDynamicConstraint()\n    layer = OptLayer(cons)\n    a = torch.tensor(5.0 * np.ones(6))\n    s = torch.tensor(np.ones(17))\n    assert layer(a, s).sum() == approx(20.0)"]}
{"filename": "action_constrained_rl/td3/noise_insertion_td3.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport math\nimport gym\nimport torch as th\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise", "from typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import TD3\n\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom ..nn.additional_layers.alpha_projection import AlphaProjectionLayer\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.utils import polyak_update\n\nclass NoiseInsertionTD3(TD3):\n    \"\"\"\n    TD3 to project random samplied actions and to add noise before final layer\n    \"\"\"\n    def __init__(self, policy, env, use_center_wrapper:bool = True, **kwargs):\n        super(NoiseInsertionTD3, self).__init__(policy, env, **kwargs)\n        self.action_dim = get_action_dim(self.action_space)\n        self.alpha_layer = AlphaProjectionLayer(env.envs[0].constraint)\n        self.use_center_wrapper = use_center_wrapper\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n            scaled_action = self.policy.scale_action(unscaled_action)\n            # project actions\n            if self.use_center_wrapper: # use alpha projection\n                scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-self.action_dim], self._last_obs[i,-self.action_dim:], scaled_action[i]) for i in range(n_envs)])\n            else: # use closest-point projection\n                scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n        else:\n            scaled_action = self.policy.actor.undeformed_predict(self._last_obs) # output before final layer\n            # Add noise to the action (improve exploration)\n\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            # Deform action by final layer\n            scaled_action = self.policy.actor.deform_action(scaled_action, self._last_obs)\n\n        buffer_action = scaled_action\n        action = self.policy.unscale_action(scaled_action)\n\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                # instead of clamping, project by alpha projection layer\n                next_actions = self.actor_target(replay_data.next_observations) + noise\n                next_observations = replay_data.next_observations.to(next_actions.dtype)\n                next_actions = self.alpha_layer(next_actions,\n                                                next_observations[:, :-self.action_dim],\n                                                next_observations[:, -self.action_dim:])\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                actor_loss = -self.critic.q1_forward(replay_data.observations, self.actor(replay_data.observations)).mean()\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "from stable_baselines3.common.utils import polyak_update\n\nclass NoiseInsertionTD3(TD3):\n    \"\"\"\n    TD3 to project random samplied actions and to add noise before final layer\n    \"\"\"\n    def __init__(self, policy, env, use_center_wrapper:bool = True, **kwargs):\n        super(NoiseInsertionTD3, self).__init__(policy, env, **kwargs)\n        self.action_dim = get_action_dim(self.action_space)\n        self.alpha_layer = AlphaProjectionLayer(env.envs[0].constraint)\n        self.use_center_wrapper = use_center_wrapper\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n            scaled_action = self.policy.scale_action(unscaled_action)\n            # project actions\n            if self.use_center_wrapper: # use alpha projection\n                scaled_action = np.array([self.env.envs[i].constraint.project(self._last_obs[i,:-self.action_dim], self._last_obs[i,-self.action_dim:], scaled_action[i]) for i in range(n_envs)])\n            else: # use closest-point projection\n                scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n        else:\n            scaled_action = self.policy.actor.undeformed_predict(self._last_obs) # output before final layer\n            # Add noise to the action (improve exploration)\n\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            # Deform action by final layer\n            scaled_action = self.policy.actor.deform_action(scaled_action, self._last_obs)\n\n        buffer_action = scaled_action\n        action = self.policy.unscale_action(scaled_action)\n\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                # instead of clamping, project by alpha projection layer\n                next_actions = self.actor_target(replay_data.next_observations) + noise\n                next_observations = replay_data.next_observations.to(next_actions.dtype)\n                next_actions = self.alpha_layer(next_actions,\n                                                next_observations[:, :-self.action_dim],\n                                                next_observations[:, -self.action_dim:])\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                actor_loss = -self.critic.q1_forward(replay_data.observations, self.actor(replay_data.observations)).mean()\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "action_constrained_rl/td3/td3_with_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import TD3\nfrom .noise_insertion_td3 import NoiseInsertionTD3\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass TD3WithPenalty(NoiseInsertionTD3):\n    \"\"\"\n    modified TD3 to add penalty to violation of constraints of outputs before final layer\n    This class is used for DOpt+\n    \"\"\"\n    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n        super(TD3WithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n                \n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nclass TD3WithPenalty(NoiseInsertionTD3):\n    \"\"\"\n    modified TD3 to add penalty to violation of constraints of outputs before final layer\n    This class is used for DOpt+\n    \"\"\"\n    def __init__(self, constraint, *args, use_center_wrapper:bool = True, constraint_penalty = ConstantFunction(0), **kwargs):\n        super(TD3WithPenalty, self).__init__(*args, use_center_wrapper = use_center_wrapper, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                before_projection = (self.actor.forward_before_projection(replay_data.observations))\n\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n                \n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n\n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "action_constrained_rl/td3/td3_output_penalty.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\nimport gym", "from stable_baselines3.common.utils import polyak_update\nimport gym\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import TD3\n\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass TD3WithOutputPenalty(TD3):\n    \"\"\"\n    modified TD3 to add penalty to violation of constraints\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n    \n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "\nclass TD3WithOutputPenalty(TD3):\n    \"\"\"\n    modified TD3 to add penalty to violation of constraints\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n    \n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "action_constrained_rl/td3/projection_td3.py", "chunked_list": ["# Copyright (c) 2023 OMRON SINIC X Corporation\n# Author: Shuwa Miura, Kazumi Kasaura\n\nimport numpy as np\nimport math\nimport gym\nimport torch as th\nfrom torch.nn import functional as F\n\nfrom stable_baselines3.common.buffers import ReplayBuffer", "\nfrom stable_baselines3.common.buffers import ReplayBuffer\nfrom stable_baselines3.common.utils import polyak_update\n\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3 import TD3\n\nfrom action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass ProjectionTD3(TD3):\n    \"\"\"\n    class for DPro\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n            assert not np.isnan(unscaled_action).any()\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                assert not th.isnan(actor_loss).any()\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                assert not th.isnan(actor_loss).any()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", "from action_constrained_rl.utils.constant_function import ConstantFunction\n\nclass ProjectionTD3(TD3):\n    \"\"\"\n    class for DPro\n    \"\"\"\n    def __init__(self, constraint, *args, constraint_penalty = ConstantFunction(0), **kwargs):\n        super().__init__(*args, **kwargs)\n        self.constraint = constraint\n        self.penalty_coeff = constraint_penalty\n\n    def _sample_action(\n        self,\n        learning_starts: int,\n        action_noise: Optional[ActionNoise] = None,\n        n_envs: int = 1,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample an action according to the exploration policy.\n        This is either done by sampling the probability distribution of the policy,\n        or sampling a random action (from a uniform distribution over the action space)\n        or by adding noise to the deterministic output.\n\n        :param action_noise: Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: Number of steps before learning for the warm-up phase.\n        :param n_envs:\n        :return: action to take in the environment\n            and scaled action that will be stored in the replay buffer.\n            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n        \"\"\"\n        # Select action randomly or according to policy\n        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n            # Warmup phase\n            unscaled_action = np.array([self.action_space.sample() for _ in range(n_envs)])\n        else:\n            # Note: when using continuous actions,\n            # we assume that the policy uses tanh to scale the action\n            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n            assert not np.isnan(unscaled_action).any()\n\n        # Rescale the action from [low, high] to [-1, 1]\n        if isinstance(self.action_space, gym.spaces.Box):\n            obs = self._last_obs[-1]\n            #print(\"unscaled: {}\".format(unscaled_action))\n            scaled_action = self.policy.scale_action(unscaled_action)\n            #print(\"scaled: {}\".format(scaled_action))\n\n            # Add noise to the action (improve exploration)\n            if action_noise is not None:\n                scaled_action = scaled_action + action_noise()\n\n            scaled_action = np.array([self.env.envs[i].constraint.enforceConstraintIfNeed(self._last_obs[i], scaled_action[i]) for i in range(n_envs)])\n            # We store the scaled action in the buffer\n            buffer_action = scaled_action\n            action = self.policy.unscale_action(scaled_action)\n        else:\n            # Discrete case, no need to normalize or clip\n            buffer_action = unscaled_action\n            action = buffer_action\n        return action, buffer_action\n\n    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n        # Switch to train mode (this affects batch norm / dropout)\n        self.policy.set_training_mode(True)\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        actor_losses, critic_losses = [], []\n\n        for _ in range(gradient_steps):\n\n            self._n_updates += 1\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the next Q-values: min over all critics targets\n                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critics\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if self._n_updates % self.policy_delay == 0:\n                # Compute actor loss\n                outputs = self.actor(replay_data.observations)\n                outputs.retain_grad()\n                actor_loss = -self.critic.q1_forward(replay_data.observations, outputs).mean()\n\n                assert not th.isnan(actor_loss).any()\n                # calculate penaty\n                actor_loss += self.penalty_coeff(self.num_timesteps) * self.constraint.constraintViolationBatch(replay_data.observations, outputs).mean()\n                assert not th.isnan(actor_loss).any()\n                \n                actor_losses.append(actor_loss.item())\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n\n                for tag, value in self.actor.named_parameters():\n                    if value.grad is not None:\n                        self.logger.record(\"train/grad/\" + tag, np.linalg.norm(value.grad.cpu().numpy()))\n\n                action_grad = outputs.grad.norm(dim=1).sum()\n                self.logger.record(\"train/action_grad\", action_grad.item())\n\n        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n        if len(actor_losses) > 0:\n            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))", ""]}
{"filename": "plot_scripts/plot_experiment_reacher.py", "chunked_list": ["from stable_baselines3.common.results_plotter import load_results, ts2xy, window_func\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nfrom scipy.stats import mannwhitneyu\nimport bootstrapped.bootstrap as bs\nfrom bootstrapped import stats_functions\nimport matplotlib\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['ps.fonttype'] = 42", "matplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['ps.fonttype'] = 42\n\ndef loadEvaluations(filename):\n    npz = np.load(filename)\n    return npz['timesteps'], npz['results']\n\ndef loadEvaluationsN(stem, n):\n    rows = []\n    timesteps = None\n    for i in range(1, n+1):\n        filename = f\"{stem}-{i}/evaluations.npz\"\n        timesteps, results = loadEvaluations(filename)\n        rows.append(np.mean(results, axis=1))\n    L=min([len(row) for row in rows])\n    for i in range(len(rows)):\n        rows[i] = rows[i][:L]\n    timesteps=timesteps[:L]\n    matrix = np.vstack(rows)\n    return (timesteps, matrix)", "\n\ndef plot_results(log_folder, title='Learning Curve'):\n    \"\"\"\n    plot the results\n\n    :param log_folder: (str) the save location of the results to plot\n    :param title: (str) the title of the task to plot\n    \"\"\"\n    fig = plt.figure(title, figsize = (6,4))\n\n    N=10\n    R = []\n    labels = []\n    cmap=plt.get_cmap('tab20')\n    index = -1\n    colors=[0,2,4,6,3,5,8,10,12,14,16,18,1]\n    for label in [\"DPro\", \"DPro+\",\"DPre\", \"DPre+\", \"DOpt\", \n                  \"DOpt\", \"NFW\", \"DAlpha\", \"DRad\", \"SPre\", \"SPre+\", \"SAlpha\", \"SRad\",\n    ]:\n        prefix = \"re-exp/logs-R-L2/\" + label\n        index+=1\n        x, y = loadEvaluationsN(prefix, N)\n        #means = [bs.bootstrap(y[:,i], stat_func=stats_functions.mean) for i in range(len(x))]\n        #plt.errorbar(x, [mean.value for mean in means], [[mean.lower_bound for mean in means], [mean.upper_bound for mean in means]], label = label, capsize = 2, elinewidth = 1)\n        #plt.errorbar(x, np.mean(y, axis=0), np.std(y, axis=0)/np.sqrt(N), label = label, capsize = 2, color=cmap(index))\n        plt.plot(x, np.mean(y, axis=0), label = label, color=cmap(colors[index]))\n        #R.append(np.sum(y, axis=1))\n        labels.append(label)\n\n    plt.xlabel('Number of timesteps')\n    plt.ylabel('Rewards')\n    plt.ylim([-10.,25.0])\n    #plt.title(title)\n    plt.legend(ncol=2, loc='lower right')\n    plt.subplots_adjust(left=0.11,right=0.95,top=0.95,bottom=0.13)\n    plt.savefig(\"../../../miura/figures/reacher.pdf\")\n    plt.show()", "        \n    \nplot_results(\"./\", \"Learning Curve Reacher\")\n"]}
{"filename": "plot_scripts/plot_experiment_halfcheetah.py", "chunked_list": ["from stable_baselines3.common.results_plotter import load_results, ts2xy, window_func\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nfrom scipy.stats import mannwhitneyu\nimport bootstrapped.bootstrap as bs\nfrom bootstrapped import stats_functions\nimport matplotlib\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['ps.fonttype'] = 42", "matplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['ps.fonttype'] = 42\n\ndef loadEvaluations(filename):\n    npz = np.load(filename)\n    return npz['timesteps'], npz['results']\n\ndef loadEvaluationsN(stem, n):\n    rows = []\n    timesteps = None\n    for i in range(1, n+1):\n        filename = f\"{stem}-{i}/evaluations.npz\"\n        timesteps, results = loadEvaluations(filename)\n        rows.append(np.mean(results, axis=1))\n    L=min([len(row) for row in rows])\n    for i in range(len(rows)):\n        rows[i] = rows[i][:L]\n    timesteps=timesteps[:L]\n    matrix = np.vstack(rows)\n    return (timesteps, matrix)", "\n\ndef plot_results(log_folder, title='Learning Curve'):\n    \"\"\"\n    plot the results\n\n    :param log_folder: (str) the save location of the results to plot\n    :param title: (str) the title of the task to plot\n    \"\"\"\n    fig = plt.figure(title, figsize = (6,4))\n\n    N=10\n    R = []\n    labels = []\n    cmap=plt.get_cmap('tab20')\n    index = -1\n    for prefix, label in [\n            [\"../logs/main/half_cheetah/stupid_proj\", \"DPro\"], \n            [\"../logs/main/half_cheetah/stupid_proj-P\", \"DPro+\"], \n            [\"../logs/main/half_cheetah/wrapper-DDPG\", \"DPre\"], \n            [\"../logs/main/half_cheetah/wrapper-P-DDPG\", \"DPre+\"], \n                          [\"../logs/main/half_cheetah/OptSq\", \"DOpt\"], \n                          [\"../logs/main/half_cheetah/OptSqP\", \"DOpt+\"], \n                          [\"../logs/main/half_cheetah/NFWPO100\", \"NFW\"],\n                          [\"../logs/main/half_cheetah/NFWPOOriginal100\", \"NFW*\"],\n            [\"../logs/main/half_cheetah/alpha-DDPG\", \"DAlpha\"],\n                          [\"../logs/main/half_cheetah/shrinkage-DDPG\", \"DRad\"],\n                          [\"../logs/main/half_cheetah/wrapper-TrueSAC\", \"SPre\"], \n                          [\"../logs/main/half_cheetah/wraper-P-TrueSAC\", \"SPre+\"], \n                          [\"../logs/main/half_cheetah/alpha-TrueSAC\", \"SAlpha\"],\n                          [\"../logs/main/half_cheetah/shrinkage-TrueSAC\", \"SRad\"],\n   ]:\n        index+=1\n        if label == \"DOpt\" or label == \"DOpt+\":\n            continue\n        x, y = loadEvaluationsN(prefix, N)\n        #means = [bs.bootstrap(y[:,i], stat_func=stats_functions.mean) for i in range(len(x))]\n        #plt.errorbar(x, [mean.value for mean in means], [[mean.lower_bound for mean in means], [mean.upper_bound for mean in means]], label = label, capsize = 2, elinewidth = 1)\n        #plt.errorbar(x, np.mean(y, axis=0), np.std(y, axis=0)/np.sqrt(N), label = label, capsize = 2, color=cmap(index))\n        plt.plot(x, np.mean(y, axis=0), label = label, color=cmap(index))\n        #R.append(np.sum(y, axis=1))\n        labels.append(label)\n\n    plt.xlabel('Number of timesteps')\n    plt.ylabel('Rewards')\n    plt.ticklabel_format(scilimits=[-1,1])\n    plt.ylim([-1000,9000])\n    #plt.title(title)\n    plt.legend(ncol=2, loc='lower right')\n    plt.subplots_adjust(left=0.11,right=0.95,top=0.95,bottom=0.13)\n    plt.savefig(\"../../../miura/figures/half_cheetah.pdf\")\n    plt.show()", "        \n    \nplot_results(\"./\", \"Learning Curve Half_Cheetah\")\n"]}
