{"filename": "db_manager.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom document_db import load_db_with_type, similarity_search_with_score\nimport sys\nimport pandas as pd\n\n\ndef get_qa(db_dir: str, doc_id: str):\n    return load_db_with_type(db_dir + \"/\" + doc_id)", "def get_qa(db_dir: str, doc_id: str):\n    return load_db_with_type(db_dir + \"/\" + doc_id)\n\ndef get_similarity_search_with_scores(db_dir: str, doc_id: str, terms: str, top_k: int):\n    return similarity_search_with_score(db_dir + \"/\" + doc_id, terms, top_k)\n\ndef get_similarity_search_with_average_score(db_dir: str, doc_id: str, terms: str, top_k: int):\n    doc_scores = similarity_search_with_score(db_dir + \"/\" + doc_id, terms, top_k)\n    scores = [score[1] for score in doc_scores]\n    scores_series = pd.Series(scores, dtype='float64')\n    average_score = scores_series.mean()\n    return average_score", "\ndef get_similarity_search_results(doclist: str, db_dir: str, terms: str, top_k: int):\n    with open(doclist, \"r\") as file:\n        lines = file.readlines()\n        total_list = [line.strip() for line in lines]\n\n    scores = []\n    entries = []\n    for entry in total_list:\n        score = get_similarity_search_with_average_score(db_dir, entry, terms, top_k)\n        scores.append(score)  \n        entries.append(entry)\n    \n    df = pd.DataFrame({'title': entries, 'score': scores})\n    top = df.sort_values(by='score', ascending=True).head(top_k)['title'].tolist()\n    return top", "\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 5:\n        print(\"USAGE: \" + sys.argv[0] + \" <db_dir> <doclist> <terms> <num>\")\n        sys.exit(1)\n    \n    db_dir=sys.argv[1]\n    doclist=sys.argv[2]\n    terms=sys.argv[3]\n    num=int(sys.argv[4])\n\n    docs = get_similarity_search_results(doclist, db_dir, terms, num)\n    for entry in docs:\n        print(entry)", ""]}
{"filename": "params.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport json\n\ndef get_param(param_name: str):\n    with open('./params.json', 'r') as file:\n        param = json.load(file)\n        return param.get(param_name)\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <param_name>\")\n        sys.exit(1)\n    param_name = sys.argv[1]\n\n    print(get_param(param_name))", "\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <param_name>\")\n        sys.exit(1)\n    param_name = sys.argv[1]\n\n    print(get_param(param_name))\n", ""]}
{"filename": "document_db.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\n\n# hyper parameters\nllm_name=\"gpt-4-0613\"\n#llm_name=\"gpt-4\"\n#llm_name=\"gpt-3.5-turbo\"\n#llm_name=\"gpt-3.5-turbo-0613\"", "#llm_name=\"gpt-3.5-turbo\"\n#llm_name=\"gpt-3.5-turbo-0613\"\n#llm_name=\"gpt-3.5-turbo-16k\"\nembedding_model='text-embedding-ada-002'\npage_chunk_size = 1024\nmax_token_num = 4096\nconversation_window_size = 3\nconversation_token_num = 1024\nconversation_history_type = \"window\" # token or window\nvector_db = None", "conversation_history_type = \"window\" # token or window\nvector_db = None\n\nif __name__ == \"__main__\":\n    if (len(sys.argv) == 1) or (len(sys.argv) > 4):\n        print(\"USAGE: \" + sys.argv[0] + \" new [<doc_dir> [<db_dir>]]\")\n        print(\"USAGE: \" + sys.argv[0] + \" chat [<db_dir>]\")\n        print(\"USAGE: \" + sys.argv[0] + \" question <db_dir>\")\n        sys.exit(1)\n\n    mode=sys.argv[1]\n    db_dir = \"DB\"\n    doc_dir = \"documents\"\n    ans_dir = \"answer\"\n\n    if mode == \"chat\":\n        if len(sys.argv) != 2 and len(sys.argv) != 3:\n            print(\"USAGE: \" + sys.argv[0] + \" chat [<db_dir>]\")\n            sys.exit(1)\n        if len(sys.argv) == 3:\n            db_dir = sys.argv[2]\n        \n    if mode == \"question\":\n        if len(sys.argv) != 4:\n            print(\"USAGE: \" + sys.argv[0] + \" question <db_dir>\")\n            sys.exit(1)\n        question = sys.argv[2]\n        db_dir = sys.argv[3]\n\n    if mode == \"new\":\n        if len(sys.argv) != 2 and len(sys.argv) != 4:\n            print(\"USAGE: \" + sys.argv[0] + \" new [<doc_dir> [<db_dir>]]\")\n            sys.exit(1)\n        if len(sys.argv) == 4:\n            doc_dir=sys.argv[2]\n            db_dir = sys.argv[3]\n    print(\"DB_DIR =\" + db_dir)\n    print(\"DOC_DIR=\" + doc_dir)\nelse:\n    conversation_history_type=\"window\"\n    conversation_window_size=0", "\nimport os\nimport numpy as np\nimport openai\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import CSVLoader", "from langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.document_loaders import UnstructuredPowerPointLoader\nfrom langchain.document_loaders import UnstructuredURLLoader\nfrom langchain.document_loaders import JSONLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.memory import ConversationBufferWindowMemory, ConversationTokenBufferMemory\n\ndef create_db(doc_dir, db_dir, embedding_model, chunk_size):\n    pdf_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".pdf\")]\n    json_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".json\")]\n    csv_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".csv\")]\n    pptx_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".pptx\")]\n    url_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".url\")]\n    text_splitter = CharacterTextSplitter(\n        separator = \"\\n\",\n        chunk_size = chunk_size,\n        chunk_overlap = 0,\n    )\n    files = pdf_files + csv_files + pptx_files + url_files + json_files\n    pages = []\n    for file in files:\n        print(\"INFO: Loading document=\" + file)\n        if \".pdf\" in file:\n            loader = PyPDFLoader(doc_dir + '/' + file)\n        elif \".csv\" in file:\n            loader = CSVLoader(doc_dir + '/' + file)\n        elif \".pptx\" in file:\n            loader = UnstructuredPowerPointLoader(doc_dir + '/' + file)\n        elif \".json\" in file:\n            loader = JSONLoader(file_path= doc_dir + '/' + file, jq_schema='.messages[].content')\n        elif \".url\" in file:\n            with open(doc_dir + '/' + file, 'r') as file:\n                urls = file.read().splitlines()\n            loader = UnstructuredURLLoader(urls = urls)\n        else:\n            print(\"WARNING: Not supported document=\" + file)\n            continue\n        #print(\"INFO: Spliting document=\" + file)\n        tmp_pages = loader.load_and_split()\n        chanked_pages = text_splitter.split_documents(tmp_pages)\n        pages = pages + chanked_pages\n\n    print(\"INFO: Storing Vector DB:\" + db_dir)\n    embeddings = OpenAIEmbeddings(deployment=embedding_model)\n    vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=db_dir)\n    vectorstore.persist()", "def create_db(doc_dir, db_dir, embedding_model, chunk_size):\n    pdf_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".pdf\")]\n    json_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".json\")]\n    csv_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".csv\")]\n    pptx_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".pptx\")]\n    url_files = [ file for file in os.listdir(doc_dir) if file.endswith(\".url\")]\n    text_splitter = CharacterTextSplitter(\n        separator = \"\\n\",\n        chunk_size = chunk_size,\n        chunk_overlap = 0,\n    )\n    files = pdf_files + csv_files + pptx_files + url_files + json_files\n    pages = []\n    for file in files:\n        print(\"INFO: Loading document=\" + file)\n        if \".pdf\" in file:\n            loader = PyPDFLoader(doc_dir + '/' + file)\n        elif \".csv\" in file:\n            loader = CSVLoader(doc_dir + '/' + file)\n        elif \".pptx\" in file:\n            loader = UnstructuredPowerPointLoader(doc_dir + '/' + file)\n        elif \".json\" in file:\n            loader = JSONLoader(file_path= doc_dir + '/' + file, jq_schema='.messages[].content')\n        elif \".url\" in file:\n            with open(doc_dir + '/' + file, 'r') as file:\n                urls = file.read().splitlines()\n            loader = UnstructuredURLLoader(urls = urls)\n        else:\n            print(\"WARNING: Not supported document=\" + file)\n            continue\n        #print(\"INFO: Spliting document=\" + file)\n        tmp_pages = loader.load_and_split()\n        chanked_pages = text_splitter.split_documents(tmp_pages)\n        pages = pages + chanked_pages\n\n    print(\"INFO: Storing Vector DB:\" + db_dir)\n    embeddings = OpenAIEmbeddings(deployment=embedding_model)\n    vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=db_dir)\n    vectorstore.persist()", "\n\ndef load_db(db_dir, llm_name, embedding_model, token_num, history_type, num):\n    global vector_db\n    print(\"INFO: Setting up LLM:\" + db_dir)\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    llm = ChatOpenAI(\n        temperature=0, \n        model_name=llm_name, \n        max_tokens=token_num)\n\n    embeddings = OpenAIEmbeddings(deployment=embedding_model)\n    vectorstore = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n    vector_db = vectorstore\n    if (history_type == \"window\"):\n        memory = ConversationBufferWindowMemory(k=num, memory_key=\"chat_history\", return_messages=True)\n    else:\n        memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=num, memory_key=\"chat_history\", return_messages=True)\n    qa = ConversationalRetrievalChain.from_llm(\n        llm, \n        vectorstore.as_retriever(), \n        memory=memory\n        )\n    return qa", "\ndef load_db_with_type(db_dir):\n    global llm_name, max_token_num, conversation_history_type, conversation_window_size, conversation_token_num\n    if (conversation_history_type == \"window\"):\n        qa = load_db(db_dir, llm_name, embedding_model, max_token_num, conversation_history_type, conversation_window_size)\n    else:\n        qa = load_db(db_dir, llm_name, embedding_model, max_token_num, conversation_history_type, conversation_token_num)\n    return qa\n\ndef embedding(text: str) -> list[float]:\n    result = openai.Embedding.create(input=text, model=embedding_model)\n    if isinstance(result, dict):\n        embedding = result[\"data\"][0][\"embedding\"]\n        return embedding\n    return []", "\ndef embedding(text: str) -> list[float]:\n    result = openai.Embedding.create(input=text, model=embedding_model)\n    if isinstance(result, dict):\n        embedding = result[\"data\"][0][\"embedding\"]\n        return embedding\n    return []\n\ndef cos_sim(a, b) -> float:\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))", "def cos_sim(a, b) -> float:\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef calc_similarity(str1, str2):\n    try:\n        s1 = np.array(embedding(str1))\n        s2 = np.array(embedding(str2))\n        return cos_sim(s1, s2)\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None", "\ndef similarity_search_with_score(db_dir: str, terms: str, top_k: int):\n    #print(f\"db_dir={db_dir} terms={terms} embedding_model={embedding_model}\")\n    embeddings = OpenAIEmbeddings(deployment=embedding_model)\n    vectorstore = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n    vector_db = vectorstore\n    docs = vector_db.similarity_search_with_score(terms, top_k = top_k)\n    #print(str(docs))\n    #print(f\"content: {docs[0][0].page_content}\", f\"score: {docs[0][1]}\")\n    #print(f\"content: {docs[1][0].page_content}\", f\"score: {docs[1][1]}\")\n    return docs", "\n\nif __name__ == \"__main__\":\n    if mode == \"new\":\n        _ = create_db(doc_dir, db_dir, embedding_model, page_chunk_size)\n    elif mode == \"question\":\n        qa = load_db_with_type(db_dir)\n        result = qa({\"question\": question})\n        print(result[\"answer\"])\n    else:\n        qa = load_db_with_type(db_dir)\n        while True:\n            query = input(\"> \")\n            if query == 'exit' or query == 'q' or query == \"quit\":\n                print(\"See you again!\")\n                sys.exit(0)\n            print(\"Q: \" + query)\n\n            result = qa({\"question\": query})\n            print(\"A: \"+result[\"answer\"])", "\n            #docs = vector_db.similarity_search_with_score(query, top_k = 1)\n            #print(str(docs))\n            #print(f\"content: {docs[0][0].page_content}\", f\"score: {docs[0][1]}\")\n            #print(f\"content: {docs[1][0].page_content}\", f\"score: {docs[1][1]}\")"]}
{"filename": "prompt_template.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nclass PromptTemplate:\n    def __init__(self, file_path):\n        try:\n            with open(file_path, 'r') as file:\n                self.template = file.read()\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File '{file_path}' not found.\")\n\n    def get_prompt(self, **kwargs) -> str:\n        return self.template.format(**kwargs)", "\nif __name__ == \"__main__\":\n    from params import get_param\n    prompt_template_path = get_param(\"prompt_templates_path\")\n\n    pt = PromptTemplate(prompt_template_path + \"/ptemplate_query.txt\")\n    while True:\n        target_doc_id = input(\"TargetDocID> \")\n        question = input(\"question> \")\n        reply = input(\"reply> \")\n        point = input(\"point> \")\n        prompt = pt.get_prompt(TargetDocID=target_doc_id, question=question, reply=reply, point=int(point))\n        print(\"PROMPT:\\n\" +prompt)", ""]}
{"filename": "reflection.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom question import get_response\nfrom prompt_template import PromptTemplate\nimport json\nimport traceback\nimport json_utils\n\nclass Reflection:\n    def __init__(self, main_question: str, knowledge_path: str, plan_result_path: str, prompt_template_path: str, document_list_path: str, background_knowledge_path: str):\n        prompt_template =  PromptTemplate(prompt_template_path)\n        with open(knowledge_path, 'r') as file:\n            KnowledgesNeeds = file.read()\n        with open(plan_result_path, 'r') as file:\n            PlanResult = file.read()\n        with open(document_list_path, 'r') as file:\n            DocumentList = file.read()\n        with open(background_knowledge_path, 'r') as file:\n            BackgroundKnowledges = file.read()\n        self.query = prompt_template.get_prompt(\n            MainQuestion=main_question, \n            KnowledgesNeeds=KnowledgesNeeds,\n            PlanResult=PlanResult,\n            DocumentList=DocumentList,\n            BackgroundKnowledges=BackgroundKnowledges\n            )\n\n    def create(self):\n        print(self.query)\n        try:\n            self.reply_raw = get_response(self.query)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n        print(self.reply_raw)\n\n    def save_to_raw(self, file_path):\n        with open(file_path, 'w') as file:\n            file.write(self.reply_raw)\n\n    def save_to_json(self, file_path):\n        with open(file_path, 'w') as file:\n            json.dump(json.loads(self.reply_raw), file, indent=4, ensure_ascii=False)", "\nclass Reflection:\n    def __init__(self, main_question: str, knowledge_path: str, plan_result_path: str, prompt_template_path: str, document_list_path: str, background_knowledge_path: str):\n        prompt_template =  PromptTemplate(prompt_template_path)\n        with open(knowledge_path, 'r') as file:\n            KnowledgesNeeds = file.read()\n        with open(plan_result_path, 'r') as file:\n            PlanResult = file.read()\n        with open(document_list_path, 'r') as file:\n            DocumentList = file.read()\n        with open(background_knowledge_path, 'r') as file:\n            BackgroundKnowledges = file.read()\n        self.query = prompt_template.get_prompt(\n            MainQuestion=main_question, \n            KnowledgesNeeds=KnowledgesNeeds,\n            PlanResult=PlanResult,\n            DocumentList=DocumentList,\n            BackgroundKnowledges=BackgroundKnowledges\n            )\n\n    def create(self):\n        print(self.query)\n        try:\n            self.reply_raw = get_response(self.query)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n        print(self.reply_raw)\n\n    def save_to_raw(self, file_path):\n        with open(file_path, 'w') as file:\n            file.write(self.reply_raw)\n\n    def save_to_json(self, file_path):\n        with open(file_path, 'w') as file:\n            json.dump(json.loads(self.reply_raw), file, indent=4, ensure_ascii=False)", "\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 6:\n        print(\"Usage: <MainQuestion> <DocumentList> <PreviousKnowledge> <BackgroundKnowledge> <TemplatePath>\")\n        sys.exit(1)\n    main_question = sys.argv[1]\n    document_list_path = sys.argv[2]\n    previous_knowledge_path = sys.argv[3]\n    background_knowledge_path = sys.argv[4]\n    template_path = sys.argv[5]\n    think = Reflection(\n        main_question, \n        previous_knowledge_path,\n        \"./test/result/plan_result.json\",\n        template_path,\n        document_list_path,\n        background_knowledge_path)\n    think.create()\n    think.save_to_raw(\"test/result/reflection.json\")", ""]}
{"filename": "planner.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport json\nfrom prompt_template import PromptTemplate\nfrom question import get_response\nfrom plan import Plan\nimport os\nimport traceback", "import os\nimport traceback\nimport json_utils\nfrom check_recover_json import check_json_str, recover_json_str\n\nclass Planner:\n    def __init__(self, main_question, mission_path, strategy_path, query_plan_path, strategy_history_path, background_knowledge_path, acquired_knowledge_path):\n        self.main_question = main_question\n        self.mission_path = mission_path\n        self.strategy_path = strategy_path\n        self.query_plan_path = query_plan_path\n        with open(background_knowledge_path, 'r') as file:\n            self.background_knowledges = file.read()\n        with open(acquired_knowledge_path, 'r') as file:\n            self.acquired_knowledges = file.read()\n        self.strategy_history_path = strategy_history_path\n        if os.path.exists(strategy_history_path):\n            with open(strategy_history_path, 'r') as file:\n                self.strategy_history_json = json.load(file)\n        else:\n            self.strategy_history_json = {}\n        self.plan = Plan()\n\n    def generate_query(self, document_list, history):\n        pmission = PromptTemplate(self.mission_path)\n        self.mission = pmission.get_prompt()\n\n        pstrategy = PromptTemplate(self.strategy_path)\n        self.strategy = pstrategy.get_prompt()\n\n        pquery_plan = PromptTemplate(self.query_plan_path)\n        past_strategies = []\n        if \"Strategies\" in self.strategy_history_json:\n            past_strategies = self.strategy_history_json[\"Strategies\"]\n\n        self.query_plan = pquery_plan.get_prompt(\n            MainQuestion = self.main_question,\n            Mission = self.mission,\n            Strategy = self.strategy,\n            DocumentList = document_list,\n            History = history,\n            PastStrategies = past_strategies,\n            BackgroundKnowledges = self.background_knowledges,\n            AcquiredKnowledges = self.acquired_knowledges\n        )\n        print(self.query_plan)\n\n\n    def create_plan(self):\n        try:\n            self.reply_raw = get_response(self.query_plan)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n\n        #self.reply_raw = json_utils.parse_plan(self.reply_raw)\n        count = 1\n        while True:\n            result, errorcode = check_json_str(self.reply_raw)\n            if result == False and count <= 5:\n                print(self.reply_raw)\n                print(\"ERROR: RECOVER JSON PROCESS of PLAN RETRY_COUNT=\", count)\n                self.reply_raw = recover_json_str(errorcode, self.reply_raw)\n                count += 1\n            else:\n                if result == True:\n                    print(self.reply_raw)\n                    print(\"INFO: PLAN JSON DATA IS OK\")\n                else:\n                    print(self.reply_raw)\n                    print(\"ERROR: SORRY CAN NOT RECOVER JSON DATA..\")\n                break\n\n        try:\n            self.reply_json = json.loads(self.reply_raw)\n        except json.decoder.JSONDecodeError as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n\n        new_strategy = os.getenv(\"NEW_STARTEGY\")\n        print(\"NEW_STRATEGY:\" + new_strategy)\n        if new_strategy is None or len(new_strategy.strip()) == 0:\n            new_strategy = self.reply_json[\"DetailedStrategy\"]\n\n        self.plan.set_strategy(new_strategy)\n        if \"Strategies\" not in self.strategy_history_json:\n            self.strategy_history_json[\"Strategies\"] = []\n        self.strategy_history_json[\"Strategies\"].append(new_strategy)\n\n        for entry in self.reply_json[\"Plan\"]:\n            self.plan.add_data(entry[\"DocumentID\"], entry[\"Purpose\"], entry[\"Perspectives\"])\n\n    def save_to_json(self, file_path):\n        with open(file_path, 'w') as file:\n            json.dump(self.reply_json, file, indent=4, ensure_ascii=False)\n\n    def save_strategy_history(self):\n        with open(self.strategy_history_path, 'w') as file:\n            json.dump(self.strategy_history_json, file, indent=4, ensure_ascii=False)", "\nif __name__ == \"__main__\":\n    import sys\n    from params import get_param\n    prompt_template_path = get_param(\"prompt_templates_path\")\n    if len(sys.argv) != 5:\n        print(\"Usage: <MainQuestion> <doc_list.txt> <background_knowledge_path> <acquired_knowledge_path>\")\n        sys.exit(1)\n    main_question = sys.argv[1]\n    background_knowledge_path = sys.argv[3]\n    acquired_knowledge_path = sys.argv[4]\n    batch_size = 100\n    with open(sys.argv[2], 'r') as file:\n        lines = file.readlines()\n        total_list = [line.strip() for line in lines]\n        batched_list = [total_list[i:i+batch_size] for i in range(0, len(total_list), batch_size)]\n    planner = Planner(\n        main_question = main_question,\n        mission_path= prompt_template_path + \"/ptemplate_mission.txt\",\n        strategy_path= prompt_template_path + \"/ptemplate_strategy.txt\",\n        query_plan_path= prompt_template_path + \"/ptemplate_query_plan.txt\",\n        strategy_history_path=\"./test/strategy_history.json\",\n        background_knowledge_path = background_knowledge_path,\n        acquired_knowledge_path = acquired_knowledge_path\n    )\n    for doc_list in batched_list:\n        planner.generate_query(doc_list, \"\")\n        planner.create_plan()\n        planner.save_to_json(\"test/result/reply.json\")\n    planner.plan.save_to_json(\"test/result/plan.json\")\n    planner.save_strategy_history()", ""]}
{"filename": "history_selector.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom memory_stream import MemoryStream\n\nclass HistorySelector:\n    def __init__(self, memory_stream):\n        self.memory_stream = memory_stream\n        self.history = []\n\n    def select(self, threshold):\n        data = self.memory_stream.get_data()\n        high_point_data = data[data[\"Point\"] >= threshold]\n        self.history = high_point_data\n        return high_point_data\n\n    def get_history(self):\n        return self.history", "\n\nif __name__ == \"__main__\":\n    memory_stream = MemoryStream()\n    data_num = input(\"DataNum> \")\n    count = int(data_num)\n    i = 0\n    while i < count:\n        target_doc_id = input(\"TargetDocID> \")\n        question = input(\"question> \")\n        reply = input(\"reply> \")\n        point = input(\"point> \")\n        memory_stream.add_data(target_doc_id, question, reply, int(point))\n        print(memory_stream.get_data())\n        i += 1\n    threshold = input(\"Threshold> \")\n    history = HistorySelector(memory_stream)\n    print(history.select(int(threshold)))\nelse:\n    pass", ""]}
{"filename": "evaluate_applaud.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nfrom prompt_template import PromptTemplate\nfrom question import get_response\n\ndef do_applaud(name: str, document_path: str, template_path):\n    file_list = os.listdir(document_path)\n    log_data = \"\"\n    for file in file_list:\n        with open(document_path + \"/\" + file) as file:\n            data = file.read()\n            log_data += data\n    prompt = PromptTemplate(template_path)\n    p = prompt.get_prompt(\n        Name = name,\n        log_data = log_data)\n    print(p)\n    return get_response(p)", "\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 4:\n        print(\"Usage: <name> <document_path> <template_path>\")\n        sys.exit(1)\n    result = do_applaud(sys.argv[1], sys.argv[2], sys.argv[3])\n    print(result)\n", ""]}
{"filename": "query.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport openai\nfrom prompt_template import PromptTemplate\nfrom memory_stream import MemoryStream\n\nclass Query:\n    def __init__(self, target_doc_id: str, main_question: str, memory_stream: MemoryStream, qa):\n        self.target_doc_id = target_doc_id\n        self.main_question = main_question\n        self.memory_stream = memory_stream\n        self.qa = qa\n\n    def run(self, prompt_template_path: str, sub_question: str):\n        prompt_query_template = PromptTemplate(prompt_template_path)\n        # Generate the prompt query using the template and inputs\n        query = prompt_query_template.get_prompt(sub_question=sub_question)\n        try:\n            reply = self.qa({\"question\": query})\n        except openai.error.InvalidRequestError as e:\n            print(\"ERROR: can not query:\" + query)\n            print(\"ERROR:\" + e)\n            return -1\n\n        print(reply)\n        #calculate point of reply\n        if reply.get(\"answer\") == None:\n            match = False\n        else:\n            match = re.search(r\"Point: ?([0-9\\.]+)$\", reply[\"answer\"])\n        if match:\n            point = float(match.group(1))\n            # Store the information in the MemoryStream\n            return self._save(sub_question, reply[\"answer\"], point)\n        else:\n            point = -1.0\n            print(\"ERROR: can not find point in reply:\" + reply[\"answer\"])\n            return self._save(sub_question, reply[\"answer\"], point)\n\n    def _save(self, sub_question: str, reply: str, point: int):\n        # Store the information in the MemoryStream\n        return self.memory_stream.add_data(\n            target_doc_id = self.target_doc_id, \n            question = sub_question, \n            reply = reply, \n            point = point)", "\n\nif __name__ == \"__main__\":\n    import sys\n    from db_manager import get_qa\n    from params import get_param\n    param_prompt_template_path = get_param(\"prompt_templates_path\")\n    \n    db_dir = \"..\"\n    doc_id = \"DB\"\n    qa = get_qa(db_dir, doc_id)\n    memory_stream = MemoryStream()\n    prompt_template_path = param_prompt_template_path + \"/ptemplate_query.txt\"\n    query = Query(\"1\", \"Athrill\u3068\u306f\u4f55\u3067\u3059\u304b\uff1f\", memory_stream, qa)\n    while True:\n        question = input(\"question> \")\n        if question == 'exit' or question == 'q' or question == \"quit\":\n            print(\"See you again!\")\n            sys.exit(0)\n        query.run(prompt_template_path, question)\n        print(\"REPLY: \" + memory_stream.get_reply())\n        print(\"POINT: \" + str(memory_stream.get_point()))\nelse:\n    pass"]}
{"filename": "evaluate_results.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom prompt_template import PromptTemplate\nfrom question import get_response\n\ndef evaluate_results(query_path: str, result1_path: str, result2_path: str, template_path):\n    try:\n        with open(query_path, 'r') as file:\n            query = file.read()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File '{query_path}' not found.\")\n    try:\n        with open(result1_path, 'r') as file:\n            result1 = file.read()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File '{result1_path}' not found.\")\n    try:\n        with open(result2_path, 'r') as file:\n            result2 = file.read()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File '{result2_path}' not found.\")\n\n    prompt = PromptTemplate(template_path)\n    p = prompt.get_prompt(\n        MainQuestion = query,\n        Result1 = result1,\n        Result2 = result2)\n    print(p)\n    return get_response(p)", "\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 5:\n        print(\"Usage: <query_path> <result1_path> <result2_path> <template_path>\")\n        sys.exit(1)\n    result = evaluate_results(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n    print(result)\n", ""]}
{"filename": "question.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom langchain.agents import Tool\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\n#from getpass import getpass", "from langchain.agents import AgentType\n#from getpass import getpass\nimport os\nimport openai\nimport traceback\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\ntools = []\n\n# OpenAI API\u3067\u30e2\u30c7\u30eb\u3092\u6307\u5b9a\u3057\u3066\u5fdc\u7b54\u3092\u53d6\u5f97\u3059\u308b\ndef get_response(question):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4-0613\",\n#        model=\"gpt-4\",\n#        model=\"gpt-3.5-turbo\",\n#        model=\"gpt-3.5-turbo-0613\",\n#        model=\"gpt-3.5-turbo-16k\",\n        messages=[\n            {\"role\": \"user\", \"content\": question }\n        ]\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"]", "\n# OpenAI API\u3067\u30e2\u30c7\u30eb\u3092\u6307\u5b9a\u3057\u3066\u5fdc\u7b54\u3092\u53d6\u5f97\u3059\u308b\ndef get_response(question):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4-0613\",\n#        model=\"gpt-4\",\n#        model=\"gpt-3.5-turbo\",\n#        model=\"gpt-3.5-turbo-0613\",\n#        model=\"gpt-3.5-turbo-16k\",\n        messages=[\n            {\"role\": \"user\", \"content\": question }\n        ]\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"]", "\nclass TextQa:\n    def __init__(self, doc_dir: str, doc_id: str):\n        self.doc_dir = doc_dir\n        self.doc_id = doc_id\n        self.filepath = os.path.join(self.doc_dir, self.doc_id)\n    def get_answer(self, prompt: str):\n        res = get_response(prompt)\n        return {\n            \"answer\": res\n        }\n    def qa(self, question):\n        with open(self.filepath, \"r\") as file:\n            text_data = file.read()\n            prompt = f\"Input Question: {question}\\nInput Text Data: {text_data}\\n\"\n            return self.get_answer(prompt)\n\n    @staticmethod\n    def get_qa(doc_dir: str, doc_id: str):\n        text_qa = TextQa(doc_dir, doc_id)\n        func_ptr = text_qa.qa\n        return func_ptr", "\nif __name__ == \"__main__\":\n    import sys\n    if (len(sys.argv) == 1):\n        arg = input(\"> \")\n    else:\n        arg = sys.argv[1]\n    if arg == \"q\" or arg == \"quit\":\n        print(\"See you again!\")\n        sys.exit(0)\n    try:\n        ret = get_response(arg)\n    except Exception as e:\n        traceback_str = traceback.format_exc()\n        error_message = f\"ERROR: {str(e)}\"\n        print(traceback_str + error_message)\n        sys.exit(1)\n\n    print(ret)", ""]}
{"filename": "critical_thinking.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom question import get_response\nfrom prompt_template import PromptTemplate\nimport json\nimport traceback\n\nclass CriticalThinking:\n    def __init__(self, main_question: str, prompt_template_path: str, background_knowledge_path: str):\n        with open(background_knowledge_path, 'r') as file:\n            self.background_knowledge = file.read()\n        prompt_template =  PromptTemplate(prompt_template_path)\n        self.query = prompt_template.get_prompt(MainQuestion=main_question, BackgroundKnowledges = self.background_knowledge)\n\n    def create(self):\n        print(self.query)\n        try:\n            self.reply_raw = get_response(self.query)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n        print(self.reply_raw)\n\n    def save_to_raw(self, file_path):\n        with open(file_path, 'w') as file:\n            file.write(self.reply_raw)\n\n    def save_to_json(self, file_path):\n        with open(file_path, 'w') as file:\n            json.dump(json.loads(self.reply_raw), file, indent=4, ensure_ascii=False)", "class CriticalThinking:\n    def __init__(self, main_question: str, prompt_template_path: str, background_knowledge_path: str):\n        with open(background_knowledge_path, 'r') as file:\n            self.background_knowledge = file.read()\n        prompt_template =  PromptTemplate(prompt_template_path)\n        self.query = prompt_template.get_prompt(MainQuestion=main_question, BackgroundKnowledges = self.background_knowledge)\n\n    def create(self):\n        print(self.query)\n        try:\n            self.reply_raw = get_response(self.query)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n        print(self.reply_raw)\n\n    def save_to_raw(self, file_path):\n        with open(file_path, 'w') as file:\n            file.write(self.reply_raw)\n\n    def save_to_json(self, file_path):\n        with open(file_path, 'w') as file:\n            json.dump(json.loads(self.reply_raw), file, indent=4, ensure_ascii=False)", "\n\nif __name__ == \"__main__\":\n    import sys\n    from params import get_param\n    prompt_template_path = get_param(\"prompt_templates_path\")\n\n    if len(sys.argv) != 3:\n        print(\"Usage: <MainQuestion> <BackgroundKnowledge>\")\n        sys.exit(1)\n    main_question = sys.argv[1]\n    background_knowledge_path = sys.argv[2]\n    think = CriticalThinking(main_question, prompt_template_path + \"/ptemplate_critical_thinking.txt\", background_knowledge_path)\n    think.create()\n    think.save_to_raw(\"test/result/critical_thinking.json\")", ""]}
{"filename": "tactical_plannig.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom plan import Plan\nfrom prompt_template import PromptTemplate\nfrom db_manager import get_qa\nfrom question import TextQa\nimport sys\n\nclass TacticalPlanning:\n    def __init__(self, plan: Plan, db_dir: str):\n        self.plan = plan\n        self.db_dir = db_dir\n\n    def generate_question(self, prompt_templates):\n        prioritized_plan = self._prioritize_plan()\n        if (len(prioritized_plan) == 0):\n            return None\n        \n        #print(prioritized_plan)\n        row = prioritized_plan.head(1).iloc[0]\n        #print(row)\n        plan_id = row[\"PlanID\"]\n        self.plan.update_status_doing(plan_id)\n\n        document_id = row[\"DocumentID\"]\n        purpose = row[\"Purpose\"]\n        perspectives = row[\"Perspectives\"]\n\n        return (plan_id, document_id, self._generate_document_question(prompt_templates, document_id, purpose, perspectives))\n\n    def _prioritize_plan(self):\n        plan_data = self.plan.get_data()\n        prioritized_plan = plan_data.sort_values(by=[\"PlanID\"], ascending=True)\n        prioritized_plan = prioritized_plan.loc[prioritized_plan[\"Status\"].isin([\"Doing\", \"None\"])]\n        return prioritized_plan\n\n    def _generate_document_question(self, prompt_template_path, document_id, purpose, perspectives):\n        prompt_query_template = PromptTemplate(prompt_template_path)\n        query = prompt_query_template.get_prompt(document_id=document_id, purpose=purpose, perspectives=perspectives)\n        return query", "\nclass TacticalPlanning:\n    def __init__(self, plan: Plan, db_dir: str):\n        self.plan = plan\n        self.db_dir = db_dir\n\n    def generate_question(self, prompt_templates):\n        prioritized_plan = self._prioritize_plan()\n        if (len(prioritized_plan) == 0):\n            return None\n        \n        #print(prioritized_plan)\n        row = prioritized_plan.head(1).iloc[0]\n        #print(row)\n        plan_id = row[\"PlanID\"]\n        self.plan.update_status_doing(plan_id)\n\n        document_id = row[\"DocumentID\"]\n        purpose = row[\"Purpose\"]\n        perspectives = row[\"Perspectives\"]\n\n        return (plan_id, document_id, self._generate_document_question(prompt_templates, document_id, purpose, perspectives))\n\n    def _prioritize_plan(self):\n        plan_data = self.plan.get_data()\n        prioritized_plan = plan_data.sort_values(by=[\"PlanID\"], ascending=True)\n        prioritized_plan = prioritized_plan.loc[prioritized_plan[\"Status\"].isin([\"Doing\", \"None\"])]\n        return prioritized_plan\n\n    def _generate_document_question(self, prompt_template_path, document_id, purpose, perspectives):\n        prompt_query_template = PromptTemplate(prompt_template_path)\n        query = prompt_query_template.get_prompt(document_id=document_id, purpose=purpose, perspectives=perspectives)\n        return query", "\nif __name__ == \"__main__\":\n    if len(sys.argv) != 1 and len(sys.argv) != 2:\n        print(\"USAGE: \" + sys.argv[0] + \" [text]\")\n        sys.exit(1)\n    query_mode = \"db_query\"\n    if len(sys.argv) == 2:\n        query_mode = \"text_query\"\n\n    from query import Query\n    from memory_stream import MemoryStream\n    from params import get_param\n    param_prompt_template_path = get_param(\"prompt_templates_path\")\n    param_documents_path = get_param(\"documents_path\")\n\n    plan = Plan()\n    plan.load_from_json(\"./test/result/plan.json\")\n    db_dir = param_documents_path + \"/dbs\"\n    tactical_planning = TacticalPlanning(plan, db_dir)\n    memory_stream = MemoryStream()\n\n    while True:\n        ret = tactical_planning.generate_question(param_prompt_template_path + \"/ptemplate_subq_detail.txt\")\n        if ret == None:\n            print(\"END\")\n            break\n        plan_id = ret[0]\n        doc_id = ret[1]\n        question = ret[2]\n        if query_mode == \"db_query\":\n            qa = get_qa(db_dir, doc_id)\n        else:\n            qa = TextQa.get_qa(db_dir, doc_id)\n        print(\"query_mode=\", query_mode)\n\n        prompt_template_path = param_prompt_template_path + \"/ptemplate_query.txt\"\n        query = Query(doc_id, question, memory_stream, qa)\n        memory_id = query.run(prompt_template_path, question)\n        if memory_id < 0:\n            plan.update_status_done(plan_id, memory_id)\n            continue\n        print(\"REPLY: \" + memory_stream.get_reply())\n        print(\"POINT: \" + str(memory_stream.get_point()))\n        memory_stream.save_to_json(\"test/result/memory.json\")\n\n        plan.update_status_done(plan_id, memory_id)\n        plan.save_to_json(\"./test/result/updated_plan.json\")", "\n"]}
{"filename": "memory_stream.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport json\n\nclass MemoryStream:\n    def __init__(self):\n        self.columns = [\"ID\", \"TargetDocID\", \"Question\", \"Reply\", \"Point\"]\n        self.data = pd.DataFrame(columns=self.columns)\n        self.current_id = 1\n\n    def add_data(self, target_doc_id, question, reply, point):\n        ret_id = self.current_id\n        data = [[self.current_id, target_doc_id, question, reply, point]]\n        new_data = pd.DataFrame(data, columns=self.columns)\n        self.data = pd.concat([self.data, new_data], ignore_index=True)\n        self.current_id += 1\n        return ret_id\n\n    def get_data(self):\n        return self.data\n\n    def get_reply(self, index = None):\n        if (index == None):\n            index = len(self.data) - 1\n        if index >= 0 and index < len(self.data):\n            return self.data.loc[index, \"Reply\"]\n        else:\n            return None\n\n    def get_data(self, id: int):\n        filtered_data = self.data.loc[self.data[\"ID\"] == id]\n        if filtered_data.empty:\n            return None\n        else:\n            return filtered_data.to_dict(orient=\"records\")[0]\n\n    def get_point(self, index = None):\n        if (index == None):\n            index = len(self.data) - 1\n        if index >= 0 and index < len(self.data):\n            return self.data.loc[index, \"Point\"]\n        else:\n            return None\n\n    def save_to_json(self, file_path):\n        json_data = self.data.to_dict(orient=\"records\")\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n\n\n    def load_from_json(self, file_path):\n        with open(file_path, 'r') as file:\n            json_data = json.load(file)\n            self.data = pd.DataFrame(json_data)", "\nif __name__ == \"__main__\":\n    memory_stream = MemoryStream()\n    while True:\n        target_doc_id = input(\"TargetDocID> \")\n        question = input(\"question> \")\n        reply = input(\"reply> \")\n        point = input(\"point> \")\n        memory_stream.add_data(target_doc_id, question, reply, int(point))\n        print(memory_stream.get_data())\nelse:\n    pass", ""]}
{"filename": "json_utils.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ndef fix_quotes(line):\n    fixed_line = \"\"\n    quote_mode = False\n \n    for char in line:\n        if quote_mode == False:\n            if char == '\"':\n                quote_mode = True\n        else:\n            if char == '\"':\n                quote_mode = False\n            elif char == ':' or char == '\\n':\n                fixed_line += '\"'\n                quote_mode = False\n        fixed_line += char\n\n    return fixed_line", "\ndef fix_backslashes(json_string):\n    # \u4e0d\u6b63\u306a\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3092\u4fee\u6b63\u3059\u308b\n    fixed_string = json_string.replace(\"\\\\\", \"\\\\\\\\\")\n    return fixed_string\n\ndef parse_one_entry(line: str, key: str):\n    line = line.split(key)[1].strip()\n    string_without_quotes = line.replace('\"', '')\n    entries = string_without_quotes.split(\":\")\n    contents = []\n    #skip DetailedStrategy\n    #get contents\n    for entry in entries:\n        if line in entry:\n            continue\n        else:\n            contents.append(entry)\n    new_contents = \" \".join(contents)\n    #recreate line\n    new_line = '\"' + key + '\": ' + '\"' + new_contents + '\",'\n    print(new_line)\n    return new_line", "\ndef parse_plan(org_data: str):\n    lines = org_data.split(\"\\n\")\n    output_lines = []\n    start_flag = False\n    for line in lines:\n        if start_flag == False:\n            if \"{\" in line:\n                start_flag = True\n                output_lines.append(fix_quotes(line))\n            else:\n                pass\n        else:\n            if \"DetailedStrategy\" in line:\n                line = parse_one_entry(line, \"DetailedStrategy\")\n            output_lines.append(fix_quotes(fix_backslashes(line)))\n\n    return \"\\n\".join(output_lines)", "\n\ndef parse_json(org_data: str):\n    lines = org_data.split(\"\\n\")\n    output_lines = []\n    start_flag = False\n    nest_count = 0\n    for line in lines:\n        if start_flag == False:\n            if \"{\" in line:\n                start_flag = True\n                nest_count += 1\n                output_lines.append(fix_quotes(line))\n            else:\n                pass\n        else:\n            if \"{\" in line:\n                nest_count += 1\n            elif \"}\" in line:\n                nest_count -= 1\n            output_lines.append(line)\n            if (nest_count == 0):\n                break\n    return \"\\n\".join(output_lines)"]}
{"filename": "plan.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport json\n\nclass Plan:\n    def __init__(self):\n        self.columns = [\"PlanID\", \"DocumentID\", \"Purpose\", \"Perspectives\", \"ResultID\", \"Status\"]\n        self.data = pd.DataFrame(columns=self.columns)\n        self.current_id = 1\n\n    def set_strategy(self, detailed_strategy: str):\n        self.detailed_strategy = detailed_strategy\n\n    def add_data(self, document_id, purpose, perspectives):\n        data = [[self.current_id, document_id, purpose, perspectives, \"\", \"None\"]]\n        new_data = pd.DataFrame(data, columns=self.columns)\n        self.data = pd.concat([self.data, new_data], ignore_index=True)\n        self.current_id += 1\n\n    def update_status_doing(self, plan_id: int):\n        self.data.loc[self.data[\"PlanID\"] == plan_id, \"Status\"] = \"Doing\"\n\n    def update_status_done(self, plan_id: int, memory_id: int):\n        self.data.loc[self.data[\"PlanID\"] == plan_id, \"Status\"] = \"Done\"\n        self.data.loc[self.data[\"PlanID\"] == plan_id, \"ResultID\"] = memory_id\n\n    def save_to_json(self, file_path):\n        json_data = dict()\n        json_data[\"DetailedStrategy\"] = self.detailed_strategy\n        json_data[\"Plan\"] = self.data.to_dict(orient=\"records\")\n\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n\n    def get_json_data(self):\n        self.json_data = dict()\n        self.json_data[\"DetailedStrategy\"] = self.detailed_strategy\n        self.json_data[\"Plan\"] = self.data.to_dict(orient=\"records\")\n        return self.json_data\n\n    def load_from_json(self, file_path):\n        with open(file_path, 'r') as file:\n            self.json_data = json.load(file)\n            self.detailed_strategy = self.json_data[\"DetailedStrategy\"]\n            self.data = pd.DataFrame(self.json_data['Plan'])\n\n    def get_data(self):\n        return self.data\n\n    def get_data_by_id(self, plan_id=None):\n        if plan_id is None:\n            plan_id = self.current_id - 1\n        return self.data.loc[self.data[\"PlanID\"] == plan_id]", "\n\nif __name__ == \"__main__\":\n    plan = Plan()\n    i = 0\n    count = 2\n    while i < count:\n        doc_id = input(\"DocumentID> \")\n        purpose = input(\"Purpose> \")\n        perspectives = input(\"Perspectives> \")\n        ids = input(\"ResultID> \")\n        status = input(\"Status> \")\n        plan.add_data(doc_id, purpose, perspectives, ids, status)\n        print(plan.get_data_by_id())\n        i += 1\n    plan.save_to_json(\"test/result/plan.json\")", ""]}
{"filename": "check_recover_json.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom question import get_response\nfrom json_utils import parse_json\nimport json\nimport traceback\n\ndef check_json_str(json_data: str):\n    try:\n        _ = json.loads(json_data)\n        return (True, \"OK\")\n    except json.JSONDecodeError as e:\n        traceback_str = traceback.format_exc()\n        error_message = f\"ERROR: {str(e)}\"\n        print(traceback_str + error_message)\n        return (False, error_message)", "def check_json_str(json_data: str):\n    try:\n        _ = json.loads(json_data)\n        return (True, \"OK\")\n    except json.JSONDecodeError as e:\n        traceback_str = traceback.format_exc()\n        error_message = f\"ERROR: {str(e)}\"\n        print(traceback_str + error_message)\n        return (False, error_message)\n\ndef check_json(filepath: str):\n    try:\n        with open(filepath, \"r\") as file:\n            json_data = json.load(file)\n        return (True, \"OK\")\n    except json.JSONDecodeError as e:\n        traceback_str = traceback.format_exc()\n        error_message = f\"ERROR: {str(e)}\"\n        print(traceback_str + error_message)\n        return (False, error_message)", "\ndef check_json(filepath: str):\n    try:\n        with open(filepath, \"r\") as file:\n            json_data = json.load(file)\n        return (True, \"OK\")\n    except json.JSONDecodeError as e:\n        traceback_str = traceback.format_exc()\n        error_message = f\"ERROR: {str(e)}\"\n        print(traceback_str + error_message)\n        return (False, error_message)", "\ndef recover_json_str(errcode, data: str):\n    res = get_response(f\"{errcode}\\nPlease fix this json data:\\n {data}\")\n    json_data = parse_json(res)\n    return json_data\n\ndef recover_json(errcode, filepath: str):\n    with open(filepath, \"r\") as file:\n        data = file.read()\n        json_data = recover_json_str(errcode, data)\n        result, _ = check_json_str(json_data)\n        return (result, json_data)", "\n\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <filepath>\")\n        sys.exit(1)\n    filepath = sys.argv[1]\n    count = 1\n    while True:\n        result, errcode = check_json(filepath)\n        if result == False:\n            result, json_data = recover_json(errcode, filepath)\n            if result == True:\n                with open(filepath, \"w\") as file:\n                    file.write(json_data)\n                print(\"INFO: RECOVERED JSON DATA\")\n                break\n            elif count <= 5:\n                print(\"ERROR: RCOVERING JSON DATA: RETRY_COUNT=\", count)\n                count += 1\n            else:\n                print(json_data)\n                print(\"ERROR: can not recover json data...\")\n                sys.exit(1)\n        else:\n            break\n    print(\"OK\")\n    sys.exit(0)", "    \n"]}
{"filename": "evaluator.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport json\nfrom memory_stream import MemoryStream\nfrom plan import Plan\nfrom prompt_template import PromptTemplate\nfrom question import get_response\nfrom plan import Plan", "from question import get_response\nfrom plan import Plan\nimport copy\nimport traceback\n\nclass Evaluator:\n    def __init__(self, main_question, mission_path, plan: Plan, memory_stream: MemoryStream):\n        self.main_question = main_question\n        pmission = PromptTemplate(mission_path)\n        self.mission = pmission.get_prompt()\n        self.plan = plan\n        self.memory_stream = memory_stream\n\n    def merge_data(self):\n        self.merged_data = dict()\n        self.merged_data[\"DetailedStrategy\"] = self.plan.detailed_strategy\n        self.merged_data[\"Plan\"] = []\n        for entry in self.plan.get_json_data()[\"Plan\"]:\n            tmp = copy.deepcopy(entry)\n            new_entry = dict()\n            new_entry[\"DocumentID\"] = tmp[\"DocumentID\"]\n            new_entry[\"Purpose\"] = tmp[\"Purpose\"]\n            new_entry[\"Perspectives\"] = tmp[\"Perspectives\"]\n            #print(new_entry)\n            if isinstance(entry[\"ResultID\"], int) or isinstance(entry[\"ResultID\"], float):\n                if entry[\"ResultID\"] >= 0:\n                    data = self.memory_stream.get_data(entry[\"ResultID\"])\n                    #print(data)\n                    new_entry[\"ResultID\"] = { \"Reply\": data[\"Reply\"], \"Point\": data[\"Point\"] }\n                else:\n                    new_entry[\"ResultID\"] = { \"Reply\": \"No Reply\", \"Point\": 0.0 }\n            else:\n                    new_entry[\"ResultID\"] = { \"Reply\": \"No Reply\", \"Point\": 0.0 }\n            self.merged_data[\"Plan\"].append(new_entry)\n        #print(merged_data)\n        with open(\"./test/result/plan_result.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.merged_data, f, indent=4, ensure_ascii=False)\n\n    def evaluate(self, template_path, ref_json_path):\n        with open(ref_json_path, 'r') as file:\n            reflection = file.read()\n        with open(\"./test/result/plan_result.json\", 'r') as file:\n            PlanExecutedResults = file.read()\n\n        temp = PromptTemplate(template_path)\n        prompt = temp.get_prompt(\n            MainQuestion = self.main_question,\n            Mission = self.mission,\n            PastStrategies = [],\n            PlanExecutedResults = PlanExecutedResults,\n            Reflection = reflection\n        )\n        try:\n            reply = get_response(prompt)\n        except Exception as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            sys.exit(1)\n        print(reply)", "\nif __name__ == \"__main__\":\n    import sys\n    from params import get_param\n    prompt_template_path = get_param(\"prompt_templates_path\")\n\n    if len(sys.argv) != 4 and len(sys.argv) != 5:\n        print(\"Usage: <MainQuestion> <plan> <memory> [<reflection>]\")\n        sys.exit(1)\n    main_question = sys.argv[1]\n    mission_path= prompt_template_path + \"/ptemplate_mission.txt\"\n    if len(sys.argv) == 4:\n        plan_json_path = sys.argv[2]\n        mem_json_path = sys.argv[3]\n        plan = Plan()\n        plan.load_from_json(plan_json_path)\n        memory_stream = MemoryStream()\n        memory_stream.load_from_json(mem_json_path)\n        evaluator = Evaluator(main_question, mission_path, plan, memory_stream)\n        evaluator.merge_data()\n    elif len(sys.argv) == 5:\n        ref_json_path = sys.argv[4]\n        evaluator = Evaluator(main_question, mission_path, None, None)\n        evaluator.evaluate(prompt_template_path + \"/ptemplate_evaluate.txt\", ref_json_path)"]}
{"filename": "tools/evaluate_reflection.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport json\nfrom deepdiff import DeepDiff\nimport jsondiff\nimport pandas as pd\n\ndef get_value(json: dict, term: str, key: str):\n    for entry in json[\"Knowledges\"]:\n        if term == entry[\"Term\"]:\n            return entry[key]", "def get_value(json: dict, term: str, key: str):\n    for entry in json[\"Knowledges\"]:\n        if term == entry[\"Term\"]:\n            return entry[key]\n\ndef get_entry(json: dict, term: str):\n    for entry in json[\"Knowledges\"]:\n        if term == entry[\"Term\"]:\n            return entry\n", "\n\ndef evaluate(json_path: str):\n    with open(json_path, 'r') as file:\n        json_data = file.read()\n\n    json_value = json.loads(json_data)\n    #print(json_value)\n    json_terms = []\n    for entry in json_value[\"Knowledges\"]:\n        json_terms.append(entry[\"Term\"])\n    \n    print(\"TermNum: \", len(json_terms))\n    knowns = []\n    for term in json_terms:\n        known = get_value(json_value, term, \"KnownInfos\")\n        knowns.append(len(known))\n\n    df = pd.DataFrame(knowns, columns=[\"KnownInfos\"])\n    ave_num = df[\"KnownInfos\"].mean()\n    max_num = df[\"KnownInfos\"].max()\n    min_num = df[\"KnownInfos\"].min()\n\n    print(\"AveKnwNum:\", ave_num, \" MaxKnwNum:\", max_num, \" MinKnwNum:\", min_num)\n\n\n    docnums = []\n    for term in json_terms:\n        knowns = get_value(json_value, term, \"KnownInfos\")\n        for known in knowns:\n            term_docids = known[\"DocumentIDs\"]\n            docnums.append(len(term_docids))\n\n    df = pd.DataFrame(docnums, columns=[\"DocNum\"])\n    ave_num = df[\"DocNum\"].mean()\n    max_num = df[\"DocNum\"].max()\n    min_num = df[\"DocNum\"].min()\n\n    print(\"AveDocNum:\", ave_num, \" MaxDocNum:\", max_num, \" MinDocNum:\", min_num)\n\n    points = []\n    for term in json_terms:\n        knowns = get_value(json_value, term, \"KnownInfos\")\n        for known in knowns:\n            points.append(float(known[\"Point\"]))\n\n    df = pd.DataFrame(points, columns=[\"Point\"])\n    ave_num = df[\"Point\"].mean()\n    max_num = df[\"Point\"].max()\n    min_num = df[\"Point\"].min()\n\n    print(\"AvePoint:\", ave_num, \" MaxPoint:\", max_num, \" MinPointm:\", min_num)\n\n    known_lens = []\n    for term in json_terms:\n        knowns = get_value(json_value, term, \"KnownInfos\")\n        for known in knowns:\n            known_lens.append(len(known[\"KnownInfo\"]))\n\n    df = pd.DataFrame(known_lens, columns=[\"KnownInfo\"])\n    ave_num = df[\"KnownInfo\"].mean()\n    max_num = df[\"KnownInfo\"].max()\n    min_num = df[\"KnownInfo\"].min()\n\n    print(\"AveknwLen:\", ave_num, \" MaxknwLen:\", max_num, \" MinknwLen:\", min_num)\n\n\n    relations = []\n    for term in json_terms:\n        term_relations = get_entry(json_value, term)\n        if \"Relations\" in term_relations:\n            #print(term_relations[\"Relations\"])\n            relations.append(len(term_relations[\"Relations\"]))\n\n    df = pd.DataFrame(relations, columns=[\"RelationNum\"])\n    ave_num = df[\"RelationNum\"].mean()\n    max_num = df[\"RelationNum\"].max()\n    min_num = df[\"RelationNum\"].min()\n    print(\"AveRelNum:\", ave_num, \" MaxRelNum:\", max_num, \" MinRelNum:\", min_num)\n\n    unknowns = []\n    for term in json_terms:\n        terms = get_entry(json_value, term)\n        if \"UnknownInfo\" in terms:\n            #print(term_relations[\"Relations\"])\n            unknowns.append(len(term_relations[\"UnknownInfo\"]))\n\n    df = pd.DataFrame(unknowns, columns=[\"UnknownInfo\"])\n    ave_num = df[\"UnknownInfo\"].mean()\n    max_num = df[\"UnknownInfo\"].max()\n    min_num = df[\"UnknownInfo\"].min()\n    print(\"AveUnkwnNum:\", ave_num, \" MaxUnkwnNum:\", max_num, \" MinUnkwnNum:\", min_num)", "\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <reflection_path>\")\n        sys.exit(1)\n    json_path = sys.argv[1]\n    evaluate(json_path)\n", ""]}
{"filename": "data_model/reflection_data_cleaner.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nimport traceback\nfrom data_model_accessor import DataModelAccessor\nfrom reflection_data_model import ReflectionDataModel\nfrom reflection_contents_similarity_merge import merge_and_save_known_infos_json\n\nclass ReflectionDataCleaner:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def clean_empty_data_models(self):\n        clean_names = []\n        for name in self.accessor.get_filelist():\n            filepath = self.accessor.get_data_model_filepath(name)\n            model = ReflectionDataModel.load_json_file(filepath)\n            data_model = model.get_model()\n            if data_model.is_empty_content() == True:\n                print(f\"INFO: REMOVING EMPTY MODEL({data_model.get_name()})\")\n                clean_names.append(name)\n        self.accessor.remove_models(clean_names)\n\n    def merge_same_data_models(self):\n        for name in self.accessor.get_filelist():\n            print(\"INFO: name=\", name)\n            filepath = self.accessor.get_data_model_filepath(name)\n            ret = merge_and_save_known_infos_json(filepath)\n            if ret == False:\n                print(\"INFO: skip merge...error\")", "from reflection_contents_similarity_merge import merge_and_save_known_infos_json\n\nclass ReflectionDataCleaner:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def clean_empty_data_models(self):\n        clean_names = []\n        for name in self.accessor.get_filelist():\n            filepath = self.accessor.get_data_model_filepath(name)\n            model = ReflectionDataModel.load_json_file(filepath)\n            data_model = model.get_model()\n            if data_model.is_empty_content() == True:\n                print(f\"INFO: REMOVING EMPTY MODEL({data_model.get_name()})\")\n                clean_names.append(name)\n        self.accessor.remove_models(clean_names)\n\n    def merge_same_data_models(self):\n        for name in self.accessor.get_filelist():\n            print(\"INFO: name=\", name)\n            filepath = self.accessor.get_data_model_filepath(name)\n            ret = merge_and_save_known_infos_json(filepath)\n            if ret == False:\n                print(\"INFO: skip merge...error\")", "                #sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: <dir>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    accessor = DataModelAccessor(dir)\n\n    cleaner = ReflectionDataCleaner(accessor)\n    cleaner.clean_empty_data_models()\n    print(\"INFO: MERGING REFLECTIONS\")\n    cleaner.merge_same_data_models()", "    "]}
{"filename": "data_model/similarity_extractor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom data_model_accessor import DataModelAccessor\nimport json\nfrom openai_libs import get_score, get_tokenlen\n\nclass SimilarityExtractor:\n    def __init__(self, accessor: DataModelAccessor, maxtoken_num: int):\n        self.maxtoken_num = maxtoken_num\n        self.accessor = accessor\n\n    def get_filelist(self, query: str):\n        scores = self._calc_scores(query, accessor.get_filelist())\n        result = []\n        token_sum = 0\n        for entry in scores:\n            if token_sum + entry[\"tokens\"] > self.maxtoken_num:\n                break\n            result.append(entry[\"file\"])\n            token_sum += entry[\"tokens\"]\n        return result\n\n    def _calc_scores(self, query: str, filelist: list):\n        scores = []\n        for entry in filelist:\n            #print(\"file:\", entry)\n            json_data = self.accessor.get_data_model(entry).get_json_data()\n            json_str = json.dumps(json_data)\n            score = get_score(query, json_str)\n            tokens = get_tokenlen(json_str)\n            scores.append({\n                \"file\": entry,\n                \"tokens\": tokens,\n                \"score\": score\n            })\n        scores.sort(key=lambda x: x[\"score\"], reverse=True)\n        return scores\n\n\n    def extract(self, head_name: str, filelists: list):\n        models = self.accessor.get_json_models(filelists)\n        data = {\n            head_name: models\n        }\n        return data", "\n\nif __name__ == \"__main__\":\n    import sys\n    import json\n    if len(sys.argv) != 3:\n        print(\"Usage: <query> <dir>\")\n        sys.exit(1)\n    query = sys.argv[1]\n    dir = sys.argv[2]\n\n    accessor = DataModelAccessor(dir)\n    extractor = SimilarityExtractor(accessor, 2048)\n\n    filelist = extractor.get_filelist(query)\n    data = extractor.extract(\"inputs\", filelist)\n    data_str = json.dumps(data, indent=4, ensure_ascii=False)\n    print(data_str)", ""]}
{"filename": "data_model/reflection_data_persistentor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nimport traceback\nfrom data_model_accessor import DataModelAccessor\nfrom reflection_data_model import ReflectionDataModel\n\nclass ReflectionDataPersistentor:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def save_reflection_data(self):\n        for model in self.models:\n            data_model = model.get_model()\n            self.accessor.add_data_model(data_model)\n\n    def load_reflection_data(self, reflection_data_path: str):\n        try:\n            #print(\"filepath=\", reflection_data_path)\n            with open(reflection_data_path, \"r\") as file:\n                json_data = json.load(file)\n        except json.JSONDecodeError as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            return\n        #print(\"json_data:\", json.dumps(json_data))\n        if json_data.get(\"Knowledges\") is None:\n            return\n        \n        self.models = []\n        for entry in json_data.get(\"Knowledges\"):\n            #print(\"Term:\", entry.get(\"Term\"))\n            model = ReflectionDataModel.create_from_entry(\n                        entry.get(\"Term\").replace(\" \", \"_\").replace(\"/\", \"_\"), entry)\n            self.models.append(model)", "\nclass ReflectionDataPersistentor:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def save_reflection_data(self):\n        for model in self.models:\n            data_model = model.get_model()\n            self.accessor.add_data_model(data_model)\n\n    def load_reflection_data(self, reflection_data_path: str):\n        try:\n            #print(\"filepath=\", reflection_data_path)\n            with open(reflection_data_path, \"r\") as file:\n                json_data = json.load(file)\n        except json.JSONDecodeError as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            return\n        #print(\"json_data:\", json.dumps(json_data))\n        if json_data.get(\"Knowledges\") is None:\n            return\n        \n        self.models = []\n        for entry in json_data.get(\"Knowledges\"):\n            #print(\"Term:\", entry.get(\"Term\"))\n            model = ReflectionDataModel.create_from_entry(\n                        entry.get(\"Term\").replace(\" \", \"_\").replace(\"/\", \"_\"), entry)\n            self.models.append(model)", "\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: <dir> <filepath>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    filepath = sys.argv[2]\n    accessor = DataModelAccessor(dir)\n\n    persistentor = ReflectionDataPersistentor(accessor)\n    persistentor.load_reflection_data(filepath)\n    persistentor.save_reflection_data()", "    "]}
{"filename": "data_model/data_model_storage.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport os\nimport json\n\nfrom data_model import DataModel\n\nclass DataModelStorage:\n    def __init__(self, directory: str):\n        self.directory = directory\n        \n        if not os.path.exists(directory):\n            os.makedirs(directory)\n    \n    def save_data_model(self, new_model: DataModel, merge: bool  = True):\n        filename = f\"{new_model.get_name()}.json\"\n        filepath = os.path.join(self.directory, filename)\n\n        old_model = DataModel.load_json_file(filepath)\n        if merge and old_model is not None:\n            new_model.merge(old_model)\n\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(new_model.get_json_data(), f, indent=4, ensure_ascii=False)\n    \n    def load_data_model(self, name: str) -> DataModel:\n        filename = f\"{name}.json\"\n        filepath = os.path.join(self.directory, filename)\n        return DataModel.load_json_file(filepath)\n    \n    def remove_data_model(self, name: str):\n        filename = f\"{name}.json\"\n        filepath = os.path.join(self.directory, filename)\n        os.remove(filepath)", "\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 4:\n        print(\"Usage: <dir> <name> <contents>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    storage = DataModelStorage(dir)\n    model = DataModel(sys.argv[2], sys.argv[3])\n    storage.save_data_model(model)", "    "]}
{"filename": "data_model/document_data_persistentor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nimport traceback\nfrom data_model_accessor import DataModelAccessor\nfrom document_data_model import DocumentDataModel\n\nclass DocumentDataPersistentor:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def save_document_data(self):\n        for model in self.models:\n            data_model = model.get_model()\n            self.accessor.add_data_model(data_model)\n\n    def load_document_data(self, plan_data_path: str):\n        try:\n            with open(plan_data_path, \"r\") as file:\n                json_data = json.load(file)\n        except json.JSONDecodeError as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            return\n        if json_data.get(\"Plan\") is None:\n            return\n        \n        document_ids = []\n        for entry in json_data.get(\"Plan\"):\n            if entry.get(\"DocumentID\") not in document_ids:\n                #print(\"doc:\", entry.get(\"DocumentID\"))\n                document_ids.append(entry.get(\"DocumentID\"))\n\n        self.models = []\n        for entry in document_ids:\n            model = DocumentDataModel.create_from_plans(entry, json_data)\n            if model.is_empty() == False:\n                self.models.append(model)", "\nclass DocumentDataPersistentor:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def save_document_data(self):\n        for model in self.models:\n            data_model = model.get_model()\n            self.accessor.add_data_model(data_model)\n\n    def load_document_data(self, plan_data_path: str):\n        try:\n            with open(plan_data_path, \"r\") as file:\n                json_data = json.load(file)\n        except json.JSONDecodeError as e:\n            traceback_str = traceback.format_exc()\n            error_message = f\"ERROR: {str(e)}\"\n            print(traceback_str + error_message)\n            return\n        if json_data.get(\"Plan\") is None:\n            return\n        \n        document_ids = []\n        for entry in json_data.get(\"Plan\"):\n            if entry.get(\"DocumentID\") not in document_ids:\n                #print(\"doc:\", entry.get(\"DocumentID\"))\n                document_ids.append(entry.get(\"DocumentID\"))\n\n        self.models = []\n        for entry in document_ids:\n            model = DocumentDataModel.create_from_plans(entry, json_data)\n            if model.is_empty() == False:\n                self.models.append(model)", "\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: <dir> <filepath>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    filepath = sys.argv[2]\n    accessor = DataModelAccessor(dir)\n\n    persistentor = DocumentDataPersistentor(accessor)\n    persistentor.load_document_data(filepath)\n    persistentor.save_document_data()", "    "]}
{"filename": "data_model/reflection_similarity_extractor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nfrom data_model_accessor import DataModelAccessor\nfrom reflection_data_model import ReflectionDataModel\nfrom openai_libs import get_score, get_tokenlen\n\nclass ReflectionSimilarityExtractor:\n    def __init__(self, accessor: DataModelAccessor, maxtoken_num: int):\n        self.maxtoken_num = maxtoken_num\n        self.accessor = accessor\n        self.load()\n\n    def load(self):\n        filelist = self.accessor.get_filelist()\n        self.models = []\n        for entry in filelist:\n            filepath = self.accessor.get_data_model_filepath(entry)\n            model = ReflectionDataModel.load_json_file(filepath)\n            self.models.append(model)\n\n    def _calc_scores(self, query: str):\n        self.scores = []\n        for model in self.models:\n            #print(\"entry_name:\", model.get_term())\n            #print(\" known_infos:\", model.get_known_infos_num())\n            for entry in model.get_known_infos():\n                #print(\"info:\", entry)\n                data = model.get_term() + \":\" + json.dumps(entry)\n                score = get_score(query, data)\n                tokens = get_tokenlen(data)\n                self.scores.append({\n                    \"term\": model.get_term(),\n                    \"info\": entry,\n                    \"tokens\": tokens,\n                    \"score\": score\n                })\n        self.scores.sort(key=lambda x: x[\"score\"], reverse=True)\n\n    def extract(self, query: str):\n        self._calc_scores(query)\n        terms = {}\n        token_sum = 0\n        for entry in self.scores:\n            if token_sum + entry[\"tokens\"] > self.maxtoken_num:\n                break\n            #print(\"data:\", entry[\"term\"])\n            if terms.get(entry[\"term\"]) is None:\n                terms[entry[\"term\"]] = []\n            terms[entry[\"term\"]].append(entry[\"info\"])\n            token_sum += entry[\"tokens\"]\n\n        data = {\n            \"Knowledges\": terms\n        }\n        return data", "\nclass ReflectionSimilarityExtractor:\n    def __init__(self, accessor: DataModelAccessor, maxtoken_num: int):\n        self.maxtoken_num = maxtoken_num\n        self.accessor = accessor\n        self.load()\n\n    def load(self):\n        filelist = self.accessor.get_filelist()\n        self.models = []\n        for entry in filelist:\n            filepath = self.accessor.get_data_model_filepath(entry)\n            model = ReflectionDataModel.load_json_file(filepath)\n            self.models.append(model)\n\n    def _calc_scores(self, query: str):\n        self.scores = []\n        for model in self.models:\n            #print(\"entry_name:\", model.get_term())\n            #print(\" known_infos:\", model.get_known_infos_num())\n            for entry in model.get_known_infos():\n                #print(\"info:\", entry)\n                data = model.get_term() + \":\" + json.dumps(entry)\n                score = get_score(query, data)\n                tokens = get_tokenlen(data)\n                self.scores.append({\n                    \"term\": model.get_term(),\n                    \"info\": entry,\n                    \"tokens\": tokens,\n                    \"score\": score\n                })\n        self.scores.sort(key=lambda x: x[\"score\"], reverse=True)\n\n    def extract(self, query: str):\n        self._calc_scores(query)\n        terms = {}\n        token_sum = 0\n        for entry in self.scores:\n            if token_sum + entry[\"tokens\"] > self.maxtoken_num:\n                break\n            #print(\"data:\", entry[\"term\"])\n            if terms.get(entry[\"term\"]) is None:\n                terms[entry[\"term\"]] = []\n            terms[entry[\"term\"]].append(entry[\"info\"])\n            token_sum += entry[\"tokens\"]\n\n        data = {\n            \"Knowledges\": terms\n        }\n        return data", "\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        print(\"Usage: <query> <dir> <max_tokens>\")\n        sys.exit(1)\n    query = sys.argv[1]\n    dir = sys.argv[2]\n    max_tokens = int(sys.argv[3])\n\n    accessor = DataModelAccessor(dir)\n    extractor = ReflectionSimilarityExtractor(accessor, max_tokens)\n    data = extractor.extract(query)\n    data_str = json.dumps(data, indent=4, ensure_ascii=False)\n    print(data_str)", ""]}
{"filename": "data_model/document_data_model.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\n\nfrom data_model import DataModel\n\nclass DocumentDataModel:\n    def __init__(self, title: str):\n        self.title = title\n        self.results = []\n\n    def get_title(self):\n        return self.title\n    \n    def is_empty(self):\n        if len(self.results) == 0:\n            return True\n        else:\n            return False\n\n    def merge(self, old_model: DataModel):\n        old_contents = old_model.get_contents()\n        if old_contents is None:\n            return\n    \n        exist_contents = []\n        for old_data in old_contents:\n            if all(old_data.get(\"Answer\") != entry.get(\"Answer\") for entry in self.results):\n                exist_contents.append(old_data)\n\n        self.results += exist_contents\n\n    def add_info(self, purpose: str, perspectives: str, answer: str, point: float):\n        if not isinstance(point, float) or float(point) < 60.0:\n            return\n        data = {\n            \"Purpose\": purpose,\n            \"Perspectives\": perspectives,\n            \"Answer\": answer,\n            \"Point\": point\n        }\n        self.results.append(data)           \n\n    def get_contents(self):\n        if self.is_empty():\n            return None\n        return self.results\n    def get_conents_num(self):\n        if self.is_empty():\n            return 0\n        return len(self.results)\n    \n    def is_empty_content(self):\n        return self.is_empty()\n\n    def get_model(self) -> DataModel:\n        data_model = DataModel(self.get_title(), self.get_contents())\n        data_model.set_concrete_model(self)\n        return data_model\n\n\n    @staticmethod\n    def create_from_plans(name: str, plans: dict):\n        model = DocumentDataModel(name)\n        if plans is not None and plans.get(\"Plan\") is not None:\n            for plan in plans.get(\"Plan\"):\n                if plan.get(\"DocumentID\") == name:\n                    #print(plan)\n                    model.add_info(plan.get(\"Purpose\"), \n                                   plan.get(\"Perspectives\"), \n                                   plan.get(\"ResultID\").get(\"Reply\"),\n                                   plan.get(\"ResultID\").get(\"Point\"))\n        return model\n\n    @staticmethod\n    def load_plan_json_file(name: str, filepath: str):\n        with open(filepath, \"r\") as file:\n            plan_data = json.load(file)\n            model = DocumentDataModel.create_from_plans(name, plan_data)\n            return model\n\n    @staticmethod\n    def create_from_entry(name: str, entry: dict):\n        model = DocumentDataModel(name)\n        if entry is not None:\n            for result in entry:\n                model.add_info( result.get(\"Purpose\"), \n                                result.get(\"Perspectives\"), \n                                result.get(\"Answer\"),\n                                result.get(\"Point\"))\n        return model\n    \n    @staticmethod\n    def load_json_file(filepath: str):\n        data_model = DataModel.load_json_file(filepath)\n        if data_model == None:\n            return None\n        model = DocumentDataModel.create_from_entry(\n                    data_model.get_name(), \n                    data_model.get_contents())\n        return model", "class DocumentDataModel:\n    def __init__(self, title: str):\n        self.title = title\n        self.results = []\n\n    def get_title(self):\n        return self.title\n    \n    def is_empty(self):\n        if len(self.results) == 0:\n            return True\n        else:\n            return False\n\n    def merge(self, old_model: DataModel):\n        old_contents = old_model.get_contents()\n        if old_contents is None:\n            return\n    \n        exist_contents = []\n        for old_data in old_contents:\n            if all(old_data.get(\"Answer\") != entry.get(\"Answer\") for entry in self.results):\n                exist_contents.append(old_data)\n\n        self.results += exist_contents\n\n    def add_info(self, purpose: str, perspectives: str, answer: str, point: float):\n        if not isinstance(point, float) or float(point) < 60.0:\n            return\n        data = {\n            \"Purpose\": purpose,\n            \"Perspectives\": perspectives,\n            \"Answer\": answer,\n            \"Point\": point\n        }\n        self.results.append(data)           \n\n    def get_contents(self):\n        if self.is_empty():\n            return None\n        return self.results\n    def get_conents_num(self):\n        if self.is_empty():\n            return 0\n        return len(self.results)\n    \n    def is_empty_content(self):\n        return self.is_empty()\n\n    def get_model(self) -> DataModel:\n        data_model = DataModel(self.get_title(), self.get_contents())\n        data_model.set_concrete_model(self)\n        return data_model\n\n\n    @staticmethod\n    def create_from_plans(name: str, plans: dict):\n        model = DocumentDataModel(name)\n        if plans is not None and plans.get(\"Plan\") is not None:\n            for plan in plans.get(\"Plan\"):\n                if plan.get(\"DocumentID\") == name:\n                    #print(plan)\n                    model.add_info(plan.get(\"Purpose\"), \n                                   plan.get(\"Perspectives\"), \n                                   plan.get(\"ResultID\").get(\"Reply\"),\n                                   plan.get(\"ResultID\").get(\"Point\"))\n        return model\n\n    @staticmethod\n    def load_plan_json_file(name: str, filepath: str):\n        with open(filepath, \"r\") as file:\n            plan_data = json.load(file)\n            model = DocumentDataModel.create_from_plans(name, plan_data)\n            return model\n\n    @staticmethod\n    def create_from_entry(name: str, entry: dict):\n        model = DocumentDataModel(name)\n        if entry is not None:\n            for result in entry:\n                model.add_info( result.get(\"Purpose\"), \n                                result.get(\"Perspectives\"), \n                                result.get(\"Answer\"),\n                                result.get(\"Point\"))\n        return model\n    \n    @staticmethod\n    def load_json_file(filepath: str):\n        data_model = DataModel.load_json_file(filepath)\n        if data_model == None:\n            return None\n        model = DocumentDataModel.create_from_entry(\n                    data_model.get_name(), \n                    data_model.get_contents())\n        return model", "\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: <name> <filepath>\")\n        sys.exit(1)\n    name = sys.argv[1]\n    filepath = sys.argv[2]\n    print(\"name=\", name)\n    print(\"filepath=\", filepath)\n    model = DocumentDataModel.load_plan_json_file(name, filepath)\n\n    with open(\"./doc.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model.get_model().get_json_data(), f, indent=4, ensure_ascii=False)", "\n"]}
{"filename": "data_model/document_contents_similarity_merge.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport sys\nfrom question import get_response\nfrom json_utils import parse_json\nfrom check_recover_json import check_json_str, recover_json_str\nfrom document_data_model import DocumentDataModel\n\ndef merge_answers_str(data: str):\n    res = get_response(f\"For the given json data, compare each Answer, and if it matches the following conditions, merge them, otherwise output as is.:\\n 1. The content of the Answer is the same meaning. Let's think step by step.:\\n {data}\")\n    json_data = parse_json(res)\n    return json_data", "def merge_answers_str(data: str):\n    res = get_response(f\"For the given json data, compare each Answer, and if it matches the following conditions, merge them, otherwise output as is.:\\n 1. The content of the Answer is the same meaning. Let's think step by step.:\\n {data}\")\n    json_data = parse_json(res)\n    return json_data\n\ndef merge_answers_json(filepath: str):\n    with open(filepath, \"r\") as file:\n        data = file.read()\n        json_data = merge_answers_str(data)\n        return json_data", "\ndef merge_and_save_answers_json(filepath: str):\n    model = DocumentDataModel.load_json_file(filepath)\n    if model.get_conents_num() < 2:\n        print(f\"INFO: SKIP MERGING MODEL: {filepath} info_num={model.get_conents_num()}\")\n        return\n    print(\"INFO: MERGING MODEL: \", filepath)\n    json_data = merge_answers_json(filepath)\n    result, errcode = check_json_str(json_data)\n    if result == False:\n        json_data = recover_json_str(errcode, json_data)\n        result, _ = check_json_str(json_data)\n        if (result == False):\n            print(\"ERROR: can not recover json_data...\")\n            return False\n    #print(json_data)\n    with open(filepath, \"w\") as file:\n        file.write(json_data)\n    return True", "\n\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <filepath>\")\n        sys.exit(1)\n    filepath = sys.argv[1]\n    ret = merge_and_save_answers_json(filepath)\n    if ret == False:\n        sys.exit(1)\n    sys.exit(0)", ""]}
{"filename": "data_model/document_similarity_extractor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nfrom data_model_accessor import DataModelAccessor\nfrom document_data_model import DocumentDataModel\nfrom openai_libs import get_score, get_tokenlen\n\nclass DocumentSimilarityExtractor:\n    def __init__(self, accessor: DataModelAccessor, maxtoken_num: int):\n        self.maxtoken_num = maxtoken_num\n        self.accessor = accessor\n        self.load()\n\n    def load(self):\n        filelist = self.accessor.get_filelist()\n        self.models = []\n        for entry in filelist:\n            filepath = self.accessor.get_data_model_filepath(entry)\n            model = DocumentDataModel.load_json_file(filepath)\n            self.models.append(model)\n\n    def _calc_scores(self, query: str):\n        self.scores = []\n        for model in self.models:\n            contents = model.get_contents()\n            if contents is None:\n                continue\n            for entry in model.get_contents():\n                #print(\"info:\", entry)\n                data = json.dumps(entry[\"Answer\"])\n                score = get_score(query, data)\n                tokens = get_tokenlen(data)\n                self.scores.append({\n                    \"term\": model.get_title(),\n                    \"info\": entry[\"Answer\"],\n                    \"tokens\": tokens,\n                    \"score\": score\n                })\n        self.scores.sort(key=lambda x: x[\"score\"], reverse=True)\n\n    def extract(self, query: str):\n        self._calc_scores(query)\n        terms = {}\n        token_sum = 0\n        for entry in self.scores:\n            if token_sum + entry[\"tokens\"] > self.maxtoken_num:\n                break\n            if terms.get(entry[\"term\"]) is None:\n                terms[entry[\"term\"]] = []\n            terms[entry[\"term\"]].append(entry[\"info\"])\n            token_sum += entry[\"tokens\"]\n\n        data = {\n            \"DocumentIDs\": terms\n        }\n        return data", "\nclass DocumentSimilarityExtractor:\n    def __init__(self, accessor: DataModelAccessor, maxtoken_num: int):\n        self.maxtoken_num = maxtoken_num\n        self.accessor = accessor\n        self.load()\n\n    def load(self):\n        filelist = self.accessor.get_filelist()\n        self.models = []\n        for entry in filelist:\n            filepath = self.accessor.get_data_model_filepath(entry)\n            model = DocumentDataModel.load_json_file(filepath)\n            self.models.append(model)\n\n    def _calc_scores(self, query: str):\n        self.scores = []\n        for model in self.models:\n            contents = model.get_contents()\n            if contents is None:\n                continue\n            for entry in model.get_contents():\n                #print(\"info:\", entry)\n                data = json.dumps(entry[\"Answer\"])\n                score = get_score(query, data)\n                tokens = get_tokenlen(data)\n                self.scores.append({\n                    \"term\": model.get_title(),\n                    \"info\": entry[\"Answer\"],\n                    \"tokens\": tokens,\n                    \"score\": score\n                })\n        self.scores.sort(key=lambda x: x[\"score\"], reverse=True)\n\n    def extract(self, query: str):\n        self._calc_scores(query)\n        terms = {}\n        token_sum = 0\n        for entry in self.scores:\n            if token_sum + entry[\"tokens\"] > self.maxtoken_num:\n                break\n            if terms.get(entry[\"term\"]) is None:\n                terms[entry[\"term\"]] = []\n            terms[entry[\"term\"]].append(entry[\"info\"])\n            token_sum += entry[\"tokens\"]\n\n        data = {\n            \"DocumentIDs\": terms\n        }\n        return data", "\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        print(\"Usage: <query> <dir> <max_tokens>\")\n        sys.exit(1)\n    query = sys.argv[1]\n    dir = sys.argv[2]\n    max_tokens = int(sys.argv[3])\n\n    accessor = DataModelAccessor(dir)\n    extractor = DocumentSimilarityExtractor(accessor, max_tokens)\n    data = extractor.extract(query)\n    data_str = json.dumps(data, indent=4, ensure_ascii=False)\n    print(data_str)", ""]}
{"filename": "data_model/openai_libs.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport openai\nimport tiktoken\nfrom tiktoken.core import Encoding\nfrom openai.embeddings_utils import cosine_similarity\n\nllm_model = \"gpt-4-0613\"\nembedding_model = \"text-embedding-ada-002\"", "llm_model = \"gpt-4-0613\"\nembedding_model = \"text-embedding-ada-002\"\n\n# Embedding\ndef get_embedding(text_input: str):\n    global embedding_model\n    \n    # \u30d9\u30af\u30c8\u30eb\u5909\u63db\n    response  = openai.Embedding.create(\n                    input = text_input.replace(\"\\n\", \" \"),   # \u5165\u529b\u6587\u7ae0\n                    model = embedding_model,        # GPT\u30e2\u30c7\u30eb\n                 )\n    \n    # \u51fa\u529b\u7d50\u679c\u53d6\u5f97\n    embeddings = response['data'][0]['embedding']\n    \n    return embeddings", "\ndef get_score(text1: str, text2: str):\n    vec1 = get_embedding(text1)\n    vec2 = get_embedding(text2)\n    result = cosine_similarity(vec1, vec2)\n    return result\n\n\ndef get_tokenlen(data: str):\n    encoding: Encoding = tiktoken.encoding_for_model(llm_model)\n    tokens = encoding.encode(data)\n    return len(tokens)", "def get_tokenlen(data: str):\n    encoding: Encoding = tiktoken.encoding_for_model(llm_model)\n    tokens = encoding.encode(data)\n    return len(tokens)\n"]}
{"filename": "data_model/data_model.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport json\n\nclass DataModel:\n    def __init__(self, name: str, contents):\n        self._name = name\n        self._contents = contents\n        self._concrete_model = None\n    \n    def set_concrete_model(self, concrete_model):\n        self._concrete_model = concrete_model\n\n    def merge(self, old_model):\n        self._concrete_model.merge(old_model)\n        self._contents = self._concrete_model.get_contents()\n\n    def get_name(self) -> str:\n        return self._name\n\n    def get_contents(self):\n        return self._contents\n\n    def set_content(self, content):\n        self._contents = content\n\n    def get_json_data(self):\n        return {\n            \"name\": self._name,\n            \"contents\": self._contents\n        }\n    def is_empty_content(self):\n        if self._contents == None:\n            return True\n        return self._concrete_model.is_empty_content()\n    \n    @staticmethod\n    def load_json_file(filepath: str):\n        if os.path.exists(filepath):\n            #print(\"filepath=\", filepath)\n            with open(filepath, \"r\") as file:\n                data = json.load(file)\n                model = DataModel(data.get(\"name\"), None)\n                model._contents = data.get(\"contents\")\n                return model\n        else:\n            return None", ""]}
{"filename": "data_model/reflection_contents_similarity_merge.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport sys\nfrom question import get_response\nfrom json_utils import parse_json\nfrom check_recover_json import check_json_str, recover_json_str\nfrom reflection_data_model import ReflectionDataModel\n\ndef merge_known_infos_str(data: str):\n    res = get_response(f\"For the given json data, compare each KnownInfo, and if it matches the following conditions, merge them, otherwise output as is.:\\n 1. The DocumentIDs are the same, and\\n2. The content of the KnownInfo is the same meaning. Let's think step by step.:\\n {data}\")\n    json_data = parse_json(res)\n    return json_data", "def merge_known_infos_str(data: str):\n    res = get_response(f\"For the given json data, compare each KnownInfo, and if it matches the following conditions, merge them, otherwise output as is.:\\n 1. The DocumentIDs are the same, and\\n2. The content of the KnownInfo is the same meaning. Let's think step by step.:\\n {data}\")\n    json_data = parse_json(res)\n    return json_data\n\ndef merge_known_infos_json(filepath: str):\n    with open(filepath, \"r\") as file:\n        data = file.read()\n        json_data = merge_known_infos_str(data)\n        return json_data", "\ndef merge_and_save_known_infos_json(filepath: str):\n    model = ReflectionDataModel.load_json_file(filepath)\n    if model.get_known_infos_num() < 2:\n        print(f\"INFO: SKIP MERGING MODEL: {filepath} info_num={model.get_known_infos_num()}\")\n        return\n    print(\"INFO: MERGING MODEL: \", filepath)\n    json_data = merge_known_infos_json(filepath)\n    result, errcode = check_json_str(json_data)\n    if result == False:\n        json_data = recover_json_str(errcode, json_data)\n        result, _ = check_json_str(json_data)\n        if (result == False):\n            print(\"ERROR: can not recover json_data...\")\n            return False\n    #print(json_data)\n    with open(filepath, \"w\") as file:\n        file.write(json_data)\n    return True", "\n\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <filepath>\")\n        sys.exit(1)\n    filepath = sys.argv[1]\n    ret = merge_and_save_known_infos_json(filepath)\n    if ret == False:\n        sys.exit(1)\n    sys.exit(0)", ""]}
{"filename": "data_model/document_data_cleaner.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport json\nimport traceback\nfrom data_model_accessor import DataModelAccessor\nfrom document_data_model import DocumentDataModel\nfrom document_contents_similarity_merge import merge_and_save_answers_json\n\nclass DocumentDataCleaner:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def clean_empty_data_models(self):\n        clean_names = []\n        for name in self.accessor.get_filelist():\n            filepath = self.accessor.get_data_model_filepath(name)\n            model = DocumentDataModel.load_json_file(filepath)\n            data_model = model.get_model()\n            if data_model.is_empty_content() == True:\n                print(f\"INFO: REMOVING EMPTY MODEL({data_model.get_name()})\")\n                clean_names.append(name)\n        self.accessor.remove_models(clean_names)\n\n    def merge_same_data_models(self):\n        for name in self.accessor.get_filelist():\n            print(\"INFO: name=\", name)\n            filepath = self.accessor.get_data_model_filepath(name)\n            ret = merge_and_save_answers_json(filepath)\n            if ret == False:\n                print(\"INFO: skip merge...error\")", "from document_contents_similarity_merge import merge_and_save_answers_json\n\nclass DocumentDataCleaner:\n    def __init__(self, accessor: DataModelAccessor):\n        self.accessor = accessor\n\n    def clean_empty_data_models(self):\n        clean_names = []\n        for name in self.accessor.get_filelist():\n            filepath = self.accessor.get_data_model_filepath(name)\n            model = DocumentDataModel.load_json_file(filepath)\n            data_model = model.get_model()\n            if data_model.is_empty_content() == True:\n                print(f\"INFO: REMOVING EMPTY MODEL({data_model.get_name()})\")\n                clean_names.append(name)\n        self.accessor.remove_models(clean_names)\n\n    def merge_same_data_models(self):\n        for name in self.accessor.get_filelist():\n            print(\"INFO: name=\", name)\n            filepath = self.accessor.get_data_model_filepath(name)\n            ret = merge_and_save_answers_json(filepath)\n            if ret == False:\n                print(\"INFO: skip merge...error\")", "                #sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: <dir>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    accessor = DataModelAccessor(dir)\n\n    cleaner = DocumentDataCleaner(accessor)\n    cleaner.clean_empty_data_models()\n    print(\"INFO: MERGING REFLECTIONS\")\n    cleaner.merge_same_data_models()", "    "]}
{"filename": "data_model/reflection_data_model.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\nimport json\n\nfrom data_model import DataModel\n\nclass ReflectionDataModel:\n    def __init__(self, term: str):\n        self.term = term\n        self.known_infos = []\n        self.relations = []\n        self.unknown_infos = []\n\n    def is_empty(self):\n        if len(self.known_infos) == 0 and len(self.unknown_infos) == 0:\n            return True\n        else:\n            return False\n\n    def merge(self, old_model: DataModel):\n        # merge KnownInfos\n        old_contents = old_model.get_contents()\n        if old_contents is None:\n            return\n        if old_contents.get(\"KnownInfos\") is not None:\n            old_known_infos = []\n            for old_data in old_contents.get(\"KnownInfos\"):\n                if all(old_data.get(\"KnownInfo\") != entry.get(\"KnownInfo\") for entry in self.known_infos):\n                    old_known_info = {\n                        \"KnownInfo\": old_data.get(\"KnownInfo\"),\n                        \"DocumentIDs\": old_data.get(\"DocumentIDs\"),\n                        \"Point\": old_data.get(\"Point\")\n                    }\n                    old_known_infos.append(old_known_info)\n            self.known_infos += old_known_infos\n        \n        # merge Relations\n        if old_contents.get(\"Relations\") is not None:\n            old_relations = []\n            for old_data in old_contents.get(\"Relations\"):\n                if all(old_data.get(\"Term\") != entry.get(\"Term\") for entry in self.relations):\n                    old_relations.append(old_data)\n            \n            self.relations += old_relations\n\n    def add_info(self, known_info: str, document_ids: list, point: float):\n        #print(\"known_info:\", known_info)\n        if len(known_info) == 0:\n            #print(\"skip1\")\n            return\n        if document_ids is None or len(document_ids) == 0:\n            #unreliable info\n            #print(\"skip2\")\n            return\n        if point is not None and float(point) < 60.0:\n            #print(\"skip3:\", type(point))\n            #unreliable info\n            return\n        data = {\n            \"KnownInfo\": known_info,\n            \"DocumentIDs\": document_ids,\n            \"Point\": point\n        }\n        if all(data.get(\"KnownInfo\") != entry.get(\"KnownInfo\") for entry in self.known_infos):\n            self.known_infos.append(data)\n\n    def update_unknown_info(self, unknwon_infos: list):\n        self.unknown_infos = unknwon_infos\n\n    def get_known_infos_num(self):\n        return len(self.known_infos)\n\n\n    def add_relations(self, relations):\n        self.relations += relations\n\n    def get_term(self) -> str:\n        return self.term\n    \n    def get_contents(self):\n        if self.is_empty():\n            return None\n        data = {\n            \"Term\": self.term,\n            \"KnownInfos\": self.known_infos,\n            \"UnknownInfo\": self.unknown_infos,\n            \"Relations\": self.relations\n        }\n        return data\n    def get_known_infos(self):\n        if self.known_infos == None:\n            return []\n        return self.known_infos\n    \n    def is_empty_content(self):\n        if len(self.known_infos) == 0:\n            return True\n        else:\n            return False\n\n    def get_model(self) -> DataModel:\n        data_model = DataModel(self.get_term(), self.get_contents())\n        data_model.set_concrete_model(self)\n        return data_model\n\n    @staticmethod\n    def create_from_entry(name: str, entry: dict):\n        model = ReflectionDataModel(name)\n        if entry is not None and entry.get(\"KnownInfos\") is not None:\n            for known_info in entry.get(\"KnownInfos\"):\n                #print(\"KnownInfo:\", str(known_info.get(\"KnownInfo\")))\n                #print(\"DocumentIDs:\", str(known_info.get(\"DocumentIDs\")))\n                #print(\"Point:\", str(known_info.get(\"Point\")))\n                model.add_info(known_info.get(\"KnownInfo\"), known_info.get(\"DocumentIDs\"), known_info.get(\"Point\"))\n            if entry.get(\"Relations\") is not None:\n                model.add_relations(entry.get(\"Relations\"))\n            if entry.get(\"UnknownInfo\") is not None:\n                model.update_unknown_info(entry.get(\"UnknownInfo\"))\n            if entry.get(\"Point\") is not None:\n                model.point = entry.get(\"Point\")\n        return model\n    \n    @staticmethod\n    def load_json_file(filepath: str):\n        #print(\"filepath:\", filepath)\n        data_model = DataModel.load_json_file(filepath)\n        if data_model == None:\n            #print(\"data_model == None\")\n            return None\n        #print(\"name:\", data_model.get_name())\n        #print(\"get_contents:\", data_model.get_contents())\n        model = ReflectionDataModel.create_from_entry(\n                    data_model.get_name(), \n                    data_model.get_contents())\n        return model", "class ReflectionDataModel:\n    def __init__(self, term: str):\n        self.term = term\n        self.known_infos = []\n        self.relations = []\n        self.unknown_infos = []\n\n    def is_empty(self):\n        if len(self.known_infos) == 0 and len(self.unknown_infos) == 0:\n            return True\n        else:\n            return False\n\n    def merge(self, old_model: DataModel):\n        # merge KnownInfos\n        old_contents = old_model.get_contents()\n        if old_contents is None:\n            return\n        if old_contents.get(\"KnownInfos\") is not None:\n            old_known_infos = []\n            for old_data in old_contents.get(\"KnownInfos\"):\n                if all(old_data.get(\"KnownInfo\") != entry.get(\"KnownInfo\") for entry in self.known_infos):\n                    old_known_info = {\n                        \"KnownInfo\": old_data.get(\"KnownInfo\"),\n                        \"DocumentIDs\": old_data.get(\"DocumentIDs\"),\n                        \"Point\": old_data.get(\"Point\")\n                    }\n                    old_known_infos.append(old_known_info)\n            self.known_infos += old_known_infos\n        \n        # merge Relations\n        if old_contents.get(\"Relations\") is not None:\n            old_relations = []\n            for old_data in old_contents.get(\"Relations\"):\n                if all(old_data.get(\"Term\") != entry.get(\"Term\") for entry in self.relations):\n                    old_relations.append(old_data)\n            \n            self.relations += old_relations\n\n    def add_info(self, known_info: str, document_ids: list, point: float):\n        #print(\"known_info:\", known_info)\n        if len(known_info) == 0:\n            #print(\"skip1\")\n            return\n        if document_ids is None or len(document_ids) == 0:\n            #unreliable info\n            #print(\"skip2\")\n            return\n        if point is not None and float(point) < 60.0:\n            #print(\"skip3:\", type(point))\n            #unreliable info\n            return\n        data = {\n            \"KnownInfo\": known_info,\n            \"DocumentIDs\": document_ids,\n            \"Point\": point\n        }\n        if all(data.get(\"KnownInfo\") != entry.get(\"KnownInfo\") for entry in self.known_infos):\n            self.known_infos.append(data)\n\n    def update_unknown_info(self, unknwon_infos: list):\n        self.unknown_infos = unknwon_infos\n\n    def get_known_infos_num(self):\n        return len(self.known_infos)\n\n\n    def add_relations(self, relations):\n        self.relations += relations\n\n    def get_term(self) -> str:\n        return self.term\n    \n    def get_contents(self):\n        if self.is_empty():\n            return None\n        data = {\n            \"Term\": self.term,\n            \"KnownInfos\": self.known_infos,\n            \"UnknownInfo\": self.unknown_infos,\n            \"Relations\": self.relations\n        }\n        return data\n    def get_known_infos(self):\n        if self.known_infos == None:\n            return []\n        return self.known_infos\n    \n    def is_empty_content(self):\n        if len(self.known_infos) == 0:\n            return True\n        else:\n            return False\n\n    def get_model(self) -> DataModel:\n        data_model = DataModel(self.get_term(), self.get_contents())\n        data_model.set_concrete_model(self)\n        return data_model\n\n    @staticmethod\n    def create_from_entry(name: str, entry: dict):\n        model = ReflectionDataModel(name)\n        if entry is not None and entry.get(\"KnownInfos\") is not None:\n            for known_info in entry.get(\"KnownInfos\"):\n                #print(\"KnownInfo:\", str(known_info.get(\"KnownInfo\")))\n                #print(\"DocumentIDs:\", str(known_info.get(\"DocumentIDs\")))\n                #print(\"Point:\", str(known_info.get(\"Point\")))\n                model.add_info(known_info.get(\"KnownInfo\"), known_info.get(\"DocumentIDs\"), known_info.get(\"Point\"))\n            if entry.get(\"Relations\") is not None:\n                model.add_relations(entry.get(\"Relations\"))\n            if entry.get(\"UnknownInfo\") is not None:\n                model.update_unknown_info(entry.get(\"UnknownInfo\"))\n            if entry.get(\"Point\") is not None:\n                model.point = entry.get(\"Point\")\n        return model\n    \n    @staticmethod\n    def load_json_file(filepath: str):\n        #print(\"filepath:\", filepath)\n        data_model = DataModel.load_json_file(filepath)\n        if data_model == None:\n            #print(\"data_model == None\")\n            return None\n        #print(\"name:\", data_model.get_name())\n        #print(\"get_contents:\", data_model.get_contents())\n        model = ReflectionDataModel.create_from_entry(\n                    data_model.get_name(), \n                    data_model.get_contents())\n        return model", ""]}
{"filename": "data_model/data_model_accessor.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom data_model import DataModel\nfrom data_model_storage import DataModelStorage\n\nclass DataModelAccessor:\n    def __init__(self, dir: str):\n        self.directory = dir\n        if not os.path.exists(self.directory):\n            os.makedirs(self.directory)\n        self.datamodel_path = os.path.join(self.directory, \"data_model\")\n        self.data_model_storage = DataModelStorage(self.datamodel_path)\n        self._load_file_cache()\n\n    def get_data_model_filepath(self, name: str):\n        return os.path.join(self.datamodel_path, name + \".json\")\n\n    def get_filelist(self):\n        return self.file_cache\n\n    def _load_file_cache(self):\n        self.file_cache = []\n        for file_name in os.listdir(self.datamodel_path):\n            if file_name.endswith(\".json\"):\n                name = file_name[:-5]  # .json \u62e1\u5f35\u5b50\u3092\u9664\u53bb\n                self.file_cache.append(name)\n\n    def remove_models(self, model_names: list):\n        for model_name in model_names:\n            self.data_model_storage.remove_data_model(model_name)\n        self._load_file_cache()\n\n    def add_data_model(self, name: str, contents: str):\n        data_model = DataModel(name, contents)\n        self.data_model_storage.save_data_model(data_model)\n\n        if name not in self.file_cache:\n            self.file_cache.append(name)\n\n    def add_data_model(self, model: DataModel):\n        self.data_model_storage.save_data_model(model)\n\n        if model.get_name() not in self.file_cache:\n            self.file_cache.append(model.get_name())\n\n\n    def get_data_model(self, name: str) -> DataModel:\n        if name in self.file_cache:\n            return self.data_model_storage.load_data_model(name)\n        else:\n            return None\n\n    def get_json_models(self, filelist: list):\n        models = []\n        for entry in filelist:\n            models.append(self.get_data_model(entry).get_json_data())\n        return models\n\n    def get_data_models(self, filelist: list):\n        models = []\n        for entry in filelist:\n            models.append(self.get_data_model(entry))\n        return models", "class DataModelAccessor:\n    def __init__(self, dir: str):\n        self.directory = dir\n        if not os.path.exists(self.directory):\n            os.makedirs(self.directory)\n        self.datamodel_path = os.path.join(self.directory, \"data_model\")\n        self.data_model_storage = DataModelStorage(self.datamodel_path)\n        self._load_file_cache()\n\n    def get_data_model_filepath(self, name: str):\n        return os.path.join(self.datamodel_path, name + \".json\")\n\n    def get_filelist(self):\n        return self.file_cache\n\n    def _load_file_cache(self):\n        self.file_cache = []\n        for file_name in os.listdir(self.datamodel_path):\n            if file_name.endswith(\".json\"):\n                name = file_name[:-5]  # .json \u62e1\u5f35\u5b50\u3092\u9664\u53bb\n                self.file_cache.append(name)\n\n    def remove_models(self, model_names: list):\n        for model_name in model_names:\n            self.data_model_storage.remove_data_model(model_name)\n        self._load_file_cache()\n\n    def add_data_model(self, name: str, contents: str):\n        data_model = DataModel(name, contents)\n        self.data_model_storage.save_data_model(data_model)\n\n        if name not in self.file_cache:\n            self.file_cache.append(name)\n\n    def add_data_model(self, model: DataModel):\n        self.data_model_storage.save_data_model(model)\n\n        if model.get_name() not in self.file_cache:\n            self.file_cache.append(model.get_name())\n\n\n    def get_data_model(self, name: str) -> DataModel:\n        if name in self.file_cache:\n            return self.data_model_storage.load_data_model(name)\n        else:\n            return None\n\n    def get_json_models(self, filelist: list):\n        models = []\n        for entry in filelist:\n            models.append(self.get_data_model(entry).get_json_data())\n        return models\n\n    def get_data_models(self, filelist: list):\n        models = []\n        for entry in filelist:\n            models.append(self.get_data_model(entry))\n        return models", "\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 2:\n        print(\"Usage: <dir>\")\n        sys.exit(1)\n    dir = sys.argv[1]\n    accessor = DataModelAccessor(dir)\n    #accessor.add_data_model(\"term1\", \"info1\")\n    #accessor.add_data_model(\"term1\", \"info2\")\n    #accessor.add_data_model(\"term2\", \"info1\")\n\n    model = accessor.get_data_model(\"term3\")\n    if model is not None:\n        print(model.get_json_data())\n    else:\n        print(\"Not Found\")", ""]}
