{"filename": "create_agent.py", "chunked_list": ["\"\"\"\ncreate_agent.py\nThis file sets up the agent executor for the chatbot application.\n\"\"\"\n\nimport logging\nimport os\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.chat_models import ChatOpenAI", "from langchain.callbacks.manager import CallbackManager\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\n\nfrom chatweb3.agents.agent_toolkits.snowflake.base import (\n    create_snowflake_chat_agent, create_snowflake_conversational_chat_agent)\nfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n    CUSTOM_CONV_SNOWFLAKE_PREFIX, CUSTOM_CONV_SNOWFLAKE_SUFFIX,\n    CUSTOM_FORMAT_INSTRUCTIONS, CUSTOM_SNOWFLAKE_PREFIX,\n    CUSTOM_SNOWFLAKE_SUFFIX)", "    CUSTOM_FORMAT_INSTRUCTIONS, CUSTOM_SNOWFLAKE_PREFIX,\n    CUSTOM_SNOWFLAKE_SUFFIX)\nfrom chatweb3.agents.agent_toolkits.snowflake.toolkit_custom import \\\n    CustomSnowflakeDatabaseToolkit\nfrom chatweb3.callbacks.logger_callback import LoggerCallbackHandler\nfrom chatweb3.snowflake_database import SnowflakeContainer\nfrom config.config import agent_config\nfrom config.logging_config import get_logger\n\nlogger = get_logger(", "\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\nPROJ_ROOT_DIR = agent_config.get(\"proj_root_dir\")\nLOCAL_INDEX_FILE_PATH = os.path.join(\n    PROJ_ROOT_DIR, agent_config.get(\"metadata.context_ethereum_core_file\")\n)\nINDEX_ANNOTATION_FILE_PATH = os.path.join(", ")\nINDEX_ANNOTATION_FILE_PATH = os.path.join(\n    PROJ_ROOT_DIR, agent_config.get(\"metadata.annotation_ethereum_core_file\")\n)\nQUERY_DATABASE_TOOL_TOP_K = agent_config.get(\"tool.query_database_tool_top_k\")\n# AGENT_EXECUTOR_RETURN_INTERMEDIDATE_STEPS = agent_config.get(\n#    \"agent_chain.agent_executor_return_intermediate_steps\"\n# )\n\n\ndef create_agent_executor(conversation_mode=False):\n    \"\"\"\n    Creates and returns an agent executor.\n\n    Returns:\n    agent_executor: The created agent executor\n    \"\"\"\n    callbacks = CallbackManager([LoggerCallbackHandler()])\n\n    llm = ChatOpenAI(\n        model_name=agent_config.get(\"model.llm_name\"),\n        temperature=0,\n        callbacks=callbacks,\n        max_tokens=256,\n        verbose=True,\n    )\n\n    snowflake_container_eth_core = SnowflakeContainer(\n        **agent_config.get(\"snowflake_params\")\n        if agent_config.get(\"snowflake_params\")\n        else {},\n        **agent_config.get(\"shroomdk_params\")\n        if agent_config.get(\"shroomdk_params\")\n        else {},\n        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n        verbose=False,\n    )\n\n    snowflake_toolkit = CustomSnowflakeDatabaseToolkit(\n        db=snowflake_container_eth_core,\n        llm=llm,\n        readonly_shared_memory=None,\n        verbose=True,\n    )\n\n    if conversation_mode:\n        snowflake_toolkit.instructions = \"\"\n        memory = ConversationBufferMemory(\n            memory_key=\"chat_history\", return_messages=True\n        )\n\n        agent_executor = create_snowflake_conversational_chat_agent(\n            llm=llm,\n            toolkit=snowflake_toolkit,\n            prefix=CUSTOM_CONV_SNOWFLAKE_PREFIX,\n            suffix=CUSTOM_CONV_SNOWFLAKE_SUFFIX,\n            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n            memory=memory,\n            return_intermediate_steps=False,\n            top_k=QUERY_DATABASE_TOOL_TOP_K,\n            max_iterations=15,\n            max_execution_time=300,\n            early_stopping_method=\"force\",\n            callbacks=callbacks,\n            verbose=True,\n        )\n\n    else:\n        agent_executor = create_snowflake_chat_agent(\n            llm=llm,\n            toolkit=snowflake_toolkit,\n            prefix=CUSTOM_SNOWFLAKE_PREFIX,\n            suffix=CUSTOM_SNOWFLAKE_SUFFIX,\n            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n            return_intermediate_steps=True,\n            top_k=QUERY_DATABASE_TOOL_TOP_K,\n            max_iterations=15,\n            max_execution_time=300,\n            early_stopping_method=\"generate\",\n            callbacks=callbacks,\n            verbose=True,\n        )\n\n    return agent_executor", "\n\ndef create_agent_executor(conversation_mode=False):\n    \"\"\"\n    Creates and returns an agent executor.\n\n    Returns:\n    agent_executor: The created agent executor\n    \"\"\"\n    callbacks = CallbackManager([LoggerCallbackHandler()])\n\n    llm = ChatOpenAI(\n        model_name=agent_config.get(\"model.llm_name\"),\n        temperature=0,\n        callbacks=callbacks,\n        max_tokens=256,\n        verbose=True,\n    )\n\n    snowflake_container_eth_core = SnowflakeContainer(\n        **agent_config.get(\"snowflake_params\")\n        if agent_config.get(\"snowflake_params\")\n        else {},\n        **agent_config.get(\"shroomdk_params\")\n        if agent_config.get(\"shroomdk_params\")\n        else {},\n        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n        verbose=False,\n    )\n\n    snowflake_toolkit = CustomSnowflakeDatabaseToolkit(\n        db=snowflake_container_eth_core,\n        llm=llm,\n        readonly_shared_memory=None,\n        verbose=True,\n    )\n\n    if conversation_mode:\n        snowflake_toolkit.instructions = \"\"\n        memory = ConversationBufferMemory(\n            memory_key=\"chat_history\", return_messages=True\n        )\n\n        agent_executor = create_snowflake_conversational_chat_agent(\n            llm=llm,\n            toolkit=snowflake_toolkit,\n            prefix=CUSTOM_CONV_SNOWFLAKE_PREFIX,\n            suffix=CUSTOM_CONV_SNOWFLAKE_SUFFIX,\n            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n            memory=memory,\n            return_intermediate_steps=False,\n            top_k=QUERY_DATABASE_TOOL_TOP_K,\n            max_iterations=15,\n            max_execution_time=300,\n            early_stopping_method=\"force\",\n            callbacks=callbacks,\n            verbose=True,\n        )\n\n    else:\n        agent_executor = create_snowflake_chat_agent(\n            llm=llm,\n            toolkit=snowflake_toolkit,\n            prefix=CUSTOM_SNOWFLAKE_PREFIX,\n            suffix=CUSTOM_SNOWFLAKE_SUFFIX,\n            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n            return_intermediate_steps=True,\n            top_k=QUERY_DATABASE_TOOL_TOP_K,\n            max_iterations=15,\n            max_execution_time=300,\n            early_stopping_method=\"generate\",\n            callbacks=callbacks,\n            verbose=True,\n        )\n\n    return agent_executor", ""]}
{"filename": "chat_ui.py", "chunked_list": ["\"\"\"\nchat.py\nThis file contains the main code for the chatbot application.\n\"\"\"\n\nimport datetime\nimport logging\nimport os\nimport re\n", "import re\n\n# from dotenv import load_dotenv\nimport gradio as gr  # type: ignore\nfrom dotenv import load_dotenv\n\n# from config import config\nfrom config.logging_config import get_logger\nfrom create_agent import create_agent_executor\n", "from create_agent import create_agent_executor\n\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\nload_dotenv()\n\nCONVERSATION_MODE = False\n", "CONVERSATION_MODE = False\n\n\ndef set_openai_api_key(api_key, agent):\n    \"\"\"\n    Set the OpenAI API Key to the provided value and create an agent executor.\n\n    Parameters:\n    api_key (str): OpenAI API Key\n    agent: The agent to execute tasks\n\n    Returns:\n    agent_executor: The created agent executor\n    \"\"\"\n    if api_key:\n        # set the OpenAI API Key\n        os.environ[\"OPENAI_API_KEY\"] = api_key\n        agent_executor = create_agent_executor(conversation_mode=CONVERSATION_MODE)\n        os.environ[\"OPENAI_API_KEY\"] = \"\"\n        return agent_executor", "\n\ndef format_response(response: dict) -> str:\n    \"\"\"\n    Formats the response dictionary into a readable string.\n\n    Parameters:\n    response (dict): The response dictionary\n\n    Returns:\n    formatted_output (str): The formatted output string\n    \"\"\"\n    output = response[\"output\"]\n    intermediate_steps = response[\"intermediate_steps\"]\n    logger.debug(f\"intermediate_steps: {intermediate_steps}\")\n\n    formatted_steps = []\n    for i, step in enumerate(intermediate_steps, start=1):\n        agent_action, text = step\n        text = re.sub(r\"`|\\\\\", \"\", str(text))  # remove problematic characters\n        text = text.strip().replace(\"\\n\", \"\\n  \")\n        log = agent_action.log\n        thought, action = log.strip().split(\"\\nAction:\")\n        thought = thought.replace(\"Thought: \", \"\") if i == 1 else thought\n        formatted_steps.append(\n            f\"**Thought {i}**: {thought}\\n\\n*Action:*\\n\\n\\tTool: {agent_action.tool}\"\n            f\"\\n\\n\\tTool input: {agent_action.tool_input}\\n\\n*Observation:*\\n\\n{text}\"\n        )\n\n    formatted_output = \"\\n\\n\".join(formatted_steps)\n    formatted_output += f\"\\n\\n**Final answer**: {output}\"\n\n    return formatted_output", "\n\ndef split_thought_process_text(text: str):\n    \"\"\"\n    Splits the thought process text into sections.\n\n    Parameters:\n    text (str): The thought process text\n\n    Returns:\n    sections: The sections of the thought process\n    final_answer: The final answer from the thought process\n    \"\"\"\n    thoughts = text.split(\"_Thought\")\n    sections = []\n    for t in thoughts[1:]:\n        t = t.split(\"_Final answer\")[0]\n        thought, action, observation = (\n            t.split(\"\\n\\nAction:\\n\\tTool:\")[0],\n            \"Tool:\" + t.split(\"Tool:\")[1].split(\"\\n\\nObservation:\\n\\t\")[0],\n            t.split(\"Observation:\\n\\t\")[1],\n        )\n        sections.append((thought, action, observation))\n    final_answer = text.split(\"_Final answer_: \")[1]\n    return sections, final_answer", "\n\ndef chat(inp, history, agent):\n    \"\"\"\n    Handles the chat conversation. If the agent is None,\n    it adds a message to the history asking for the OpenAI API Key.\n    If the agent is set, it generates a response to the user's input and\n    adds it to the history.\n\n    Parameters:\n    inp (str): The user's input\n    history (list): The chat history\n    agent: The chat agent\n\n    Returns:\n    history: The updated chat history\n    thought_process_text: The formatted thought process text\n    \"\"\"\n    history = history or []\n    if agent is None:\n        history.append((inp, \"Please paste your OpenAI API Key to use\"))\n        thought_process_text = \"Please paste your OpenAI API Key to use\"\n        return history, history, thought_process_text\n    else:\n        print(\"\\n==== date/time: \" + str(datetime.datetime.now()) + \" ====\")\n        print(\"inp: \" + inp)\n        response = agent(inp)\n        answer = str(response[\"output\"])\n        thought_process_text = format_response(response)\n        history.append((inp, answer))\n\n    # Debugging: log the types and values of the returned variables\n    logger.debug(\n        f\"Returning from chat(): history type: {type(history)}, history value: {history}\"\n    )\n    logger.debug(\n        f\"Returning from chat(): thought_process_text type: {type(thought_process_text)}, thought_process_text value: {thought_process_text}\"\n    )\n\n    return history, history, thought_process_text", "\n    # return history, history, \"\\n\".join(thought_process_text)\n\n\nblock = gr.Blocks(css=\".gradio-container {background-color: #f5f5f5;}\")\n\nwith block:\n    # first row contains the title and the api key textbox\n    with gr.Row():\n        gr.Markdown(\"<h3><center>Let's ChatWeb3 !</center></h3>\")\n\n        openai_api_key_textbox = gr.Textbox(\n            placeholder=\"Paste your OpenAI API Key here\",\n            show_label=False,\n            lines=1,\n            type=\"password\",\n        )\n\n    # second row contains the chatbot\n    chatbot = gr.Chatbot()\n\n    # third row contains the question textbox and the submit button\n    with gr.Row():\n        message = gr.Textbox(\n            label=\"What's your question?\",\n            placeholder=\"What is the total daily trading volume on Uniswap in USD \"\n            \"in the last 7 days?\",\n            lines=1,\n        )\n        submit = gr.Button(value=\"Send\", variant=\"Secondary\").style(full_width=False)\n\n    gr.Examples(\n        examples=[\n            \"What is yesterday's total trading volume on Uniswap in USD?\",\n            \"What is the total daily trading volume on Uniswap in USD \"\n            \"in the last 7 days?\",\n        ],\n        inputs=message,\n    )\n\n    # gr.HTML(\n    #     \"<center> Built by <a href='https://inweb3.com/'>inWeb3</a>\"\n    # )\n\n    gr.HTML(\n        \"<center> Built by <a href='https://inweb3.com/'>inWeb3</a>, \"\n        \"Powered by <a href='https://openai.com/'>OpenAI</a>, \"\n        \"<a href='https://github.com/hwchase17/langchain'>LangChain \ud83e\udd9c\ufe0f\ud83d\udd17</a>, \"\n        \"<a href='https://flipsidecrypto.xyz/'>Flipsidecrypto</a></center>\"\n    )\n\n    state = gr.State()  # this holds the chat history\n    agent_state = gr.State()  # this is the agent\n\n    with gr.Row():\n        with gr.Column():\n            thought_process_label = gr.Markdown(\"<h4>Thought process:</h4>\")\n            thought_process_text = gr.Markdown(value=\"\")\n\n    # \"submit\" Button.click is triggered when the user clicks the button\n    submit.click(\n        chat,\n        inputs=[message, state, agent_state],\n        # outputs=[chatbot, state, thought_process_textbox],\n        outputs=[chatbot, state, thought_process_text],\n    )\n\n    message.submit(\n        chat,\n        inputs=[message, state, agent_state],\n        outputs=[chatbot, state, thought_process_text],\n    )\n\n    # configure the \"openai_api_key_textbox\" textbox to change\n    openai_api_key_textbox.change(\n        set_openai_api_key,\n        inputs=[openai_api_key_textbox, agent_state],\n        outputs=[agent_state],\n    )", "\nif __name__ == \"__main__\":\n    block.launch(debug=True)\n"]}
{"filename": "chatweb3/snowflake_database.py", "chunked_list": ["# %%\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom langchain.sql_database import SQLDatabase\nfrom shroomdk import ShroomDK\nfrom sqlalchemy import MetaData, create_engine, text\nfrom sqlalchemy.engine import Engine\n\n# from sqlalchemy.engine.cursor import LegacyCursorResult", "\n# from sqlalchemy.engine.cursor import LegacyCursorResult\nfrom sqlalchemy.engine.cursor import CursorResult\nfrom sqlalchemy.engine.row import Row\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.schema import CreateTable\n\nfrom config.logging_config import get_logger\n\nlogger = get_logger(", "\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\n\nclass SnowflakeDatabase(SQLDatabase):\n    def __init__(\n        self,\n        engine: Engine,\n        # schema: Optional[str] = None,\n        schema: str,\n        metadata: Optional[MetaData] = None,\n        ignore_tables: Optional[List[str]] = None,\n        include_tables: Optional[List[str]] = None,\n        sample_rows_in_table_info: int = 3,\n        indexes_in_table_info: bool = False,\n        custom_table_info: Optional[dict] = None,\n        view_support: bool = True,\n    ):\n        \"\"\"\n        We make schema a required parameter for SnowflakeDatabase, because we know it is needed for our databases.\n        We also make view_support required and default to True, because we know it is needed for our databases.\n        \"\"\"\n\n        super().__init__(\n            engine=engine,\n            schema=schema,\n            metadata=metadata,\n            ignore_tables=ignore_tables,\n            include_tables=include_tables,\n            sample_rows_in_table_info=sample_rows_in_table_info,\n            indexes_in_table_info=indexes_in_table_info,\n            custom_table_info=custom_table_info,\n            view_support=view_support,\n        )\n\n    def run(  # type: ignore\n        self, command: str, fetch: str = \"all\", return_string: bool = True\n    ) -> Union[str, CursorResult]:\n        \"\"\"Execute a SQL command and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        # logger.debug(f\"Entering run with command: {command}\")\n\n        with self._engine.begin() as connection:\n            if self._schema is not None:\n                # Set the session-level default schema\n\n                if self.dialect == \"snowflake\":\n                    set_schema_command = f\"USE SCHEMA {self._schema}\"\n                    connection.execute(text(set_schema_command))\n                else:\n                    connection.exec_driver_sql(f\"SET search_path TO {self._schema}\")\n\n            cursor: CursorResult = connection.execute(text(command))\n\n            if cursor.returns_rows:\n                if return_string:\n                    if fetch == \"all\":\n                        result_all: List[Row] = cursor.fetchall()\n                        return str(result_all)\n                    elif fetch == \"one\":\n                        fetched = cursor.fetchone()\n                        result_one: Optional[Tuple[Any, ...]] = (\n                            fetched[0] if fetched else None\n                        )\n                        return str(result_one)\n                    else:\n                        raise ValueError(\n                            \"Fetch parameter must be either 'one' or 'all'\"\n                        )\n                else:\n                    return cursor\n        return \"\" if return_string else cursor\n\n    def run_no_throw(  # type: ignore\n        self, command: str, fetch: str = \"all\", return_string: bool = True\n    ) -> Union[str, CursorResult]:\n        \"\"\"Execute a SQL command and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n\n        If the statement throws an error, the error message is returned.\n        \"\"\"\n        try:\n            return self.run(command, fetch, return_string)\n        except SQLAlchemyError as e:\n            \"\"\"Format the error message\"\"\"\n            return f\"Error: {e}\"\n\n    def get_table_info_no_throw(  # type: ignore\n        self, table_names: Optional[List[str]] = None, as_dict: bool = False\n    ) -> Union[str, Dict[str, str]]:\n        \"\"\"\n        Get the table info for the given table names.\n        This is a temperary hack to get around the fact that the parent method returns a string, but we want to return a dictionary of table names to table info strings\n        We duplicate the parent method here to avoid having to modify the parent method.\n        \"\"\"\n        try:\n            all_table_names = self.get_usable_table_names()\n            if table_names is not None:\n                missing_tables = set(table_names).difference(all_table_names)\n                if missing_tables:\n                    raise ValueError(\n                        f\"table_names {missing_tables} not found in database\"\n                    )\n                all_table_names = table_names\n\n            meta_tables = [\n                tbl\n                for tbl in self._metadata.sorted_tables\n                if tbl.name in set(all_table_names)\n                and not (self.dialect == \"sqlite\" and tbl.name.startswith(\"sqlite_\"))\n            ]\n\n            tables = []\n            for table in meta_tables:\n                if self._custom_table_info and table.name in self._custom_table_info:\n                    tables.append(self._custom_table_info[table.name])\n                    continue\n\n                # add create table command\n                create_table = str(CreateTable(table).compile(self._engine))\n                table_info = f\"{create_table.rstrip()}\"\n                has_extra_info = (\n                    self._indexes_in_table_info or self._sample_rows_in_table_info\n                )\n                if has_extra_info:\n                    table_info += \"\\n\\n/*\"\n                if self._indexes_in_table_info:\n                    table_info += f\"\\n{self._get_table_indexes(table)}\\n\"\n                if self._sample_rows_in_table_info:\n                    table_info += f\"\\n{self._get_sample_rows(table)}\\n\"\n                if has_extra_info:\n                    table_info += \"*/\"\n                tables.append(table_info)\n            if as_dict:\n                return {\n                    table.name: table_info\n                    for table, table_info in zip(meta_tables, tables)\n                }\n            else:\n                final_str = \"\\n\\n\".join(tables)\n                return final_str\n\n        except ValueError as e:\n            if table_names is not None and as_dict:\n                return {table_name: str(e) for table_name in table_names}\n            else:\n                return f\"Error: {e}\"", "\n\nclass SnowflakeContainer:\n    def __init__(\n        self,\n        user: str,\n        password: str,\n        account_identifier: str,\n        local_index_file_path: Optional[str] = None,\n        index_annotation_file_path: Optional[str] = None,\n        shroomdk_api_key: Optional[str] = None,\n        verbose: bool = False,\n    ):\n        \"\"\"Create a Snowflake container.\n        It stores the user, password, and account identifier for a Snowflake account.\n        It has a _databases attribute that stores SQLDatabase objects, which are created on demand.\n        These databases can be accessed via the (database, schema) key pair\n        It can later append the database and schema to the URL to create a Snowflake db engine.\n        \"\"\"\n        # delay import to avoid circular import\n        from chatweb3.metadata_parser import MetadataParser\n\n        self._user = user\n        self._password = password\n        self._account_identifier = account_identifier\n        # Store SQLDatabase objects with (database, schema) as the key\n        self._databases: Dict[str, SnowflakeDatabase] = {}\n        # Keep the dialect attribute for compatibility with SQLDatabase object\n        self.dialect = \"snowflake\"\n        self.metadata_parser = MetadataParser(\n            file_path=local_index_file_path,\n            annotation_file_path=index_annotation_file_path,\n            verbose=verbose,\n        )\n        self._shroomdk = (\n            ShroomDK(shroomdk_api_key) if shroomdk_api_key is not None else None\n        )\n\n    @property\n    def shroomdk(self):\n        if self._shroomdk is None:\n            raise AttributeError(\n                \"Shroomdk attribute is not found in the SnowflakeContainer; please double check whether your SHROOMDK_API_KEY is set correctly in the .env file\"\n            )\n        return self._shroomdk\n\n    def _create_engine(self, database: str) -> Engine:\n        \"\"\"Create a Snowflake engine with the given database\n        We do not need to specify the schema here, since we can specify it when we create the SQLDatabase object\n        \"\"\"\n        # create the base Snowflake URL\n        logger.debug(f\"Creating Snowflake engine for {database=}\")\n        engine_url = (\n            f\"snowflake://{self._user}:{self._password}@{self._account_identifier}/\"\n        )\n        engine_url += f\"{database}/\"\n        engine = create_engine(\n            engine_url,\n            connect_args={\n                \"client_session_keep_alive\": True,\n            },\n            # echo=True,\n        )\n        logger.debug(f\"to return {engine=}\")\n        return engine\n\n    def get_database(self, database: str, schema: str) -> SnowflakeDatabase:\n        key = f\"{database}.{schema}\"\n\n        if key in self._databases:\n            return self._databases[key]\n        else:\n            try:\n                engine = self._create_engine(database)\n                snowflake_database = SnowflakeDatabase(engine=engine, schema=schema)\n                logger.debug(f\"Created {snowflake_database=}\")\n                # sql_database = SQLDatabase(engine=engine, schema=schema)\n                self._databases[key] = snowflake_database\n                return snowflake_database\n            except Exception as e:\n                raise ValueError(\n                    f\"Error getting snowflake database for {key}: {str(e)}\"\n                )\n\n    def run_no_throw(\n        self,\n        command: str,\n        database: str,\n        schema: str,\n        fetch: str = \"all\",\n        return_string: bool = True,\n    ) -> Union[str, CursorResult]:\n        \"\"\"\n        submit a query to Snowflake by providing a database and a schema\n        \"\"\"\n        try:\n            return self.get_database(database=database, schema=schema).run_no_throw(\n                command=command, fetch=fetch, return_string=return_string\n            )\n        except Exception as e:\n            return f\"Error snowflake container running {command=} on {database}.{schema}: {str(e)}\"", ""]}
{"filename": "chatweb3/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/utils.py", "chunked_list": ["\"\"\"\nutils.py\nThis file contains the utility functions for the chatweb3 package.\n\"\"\"\nimport datetime\nimport re\nimport warnings\nfrom decimal import Decimal\nfrom typing import Dict, List, Optional\n", "from typing import Dict, List, Optional\n\n\ndef check_table_long_name(table_name_input):\n    parts = table_name_input.split(\".\")\n    if len(parts) != 3:\n        raise ValueError(\n            f\"Invalid table name format. Expected table long name format: 'database_name.schema_name.table_name', got: {table_name_input}\"\n        )\n", "\n\ndef parse_table_long_name(table_long_name: str):\n    pattern = r\"(?P<database>[\\w]+)\\.(?P<schema>[\\w]+)\\.(?P<table>[\\w]+)\"\n    match = re.match(pattern, table_long_name)\n\n    if match:\n        database = match.group(\"database\")\n        schema = match.group(\"schema\")\n        table = match.group(\"table\")\n        return database, schema, table\n    else:\n        raise ValueError(\"Invalid table long name format.\")", "\n\ndef format_dict_to_string(d: dict) -> str:\n    return \", \".join([f\"{k}={v!r}\" for k, v in d.items()])\n\n\ndef convert_rows_to_serializable(rows):\n    converted_rows = []\n    for row in rows:\n        converted_row = []\n        for item in row:\n            if isinstance(item, datetime.datetime):\n                item = item.isoformat()\n            elif isinstance(item, datetime.date):\n                item = item.isoformat()\n            elif isinstance(item, Decimal):\n                item = float(item)\n            # Add more type conversions here as needed\n            converted_row.append(item)\n        converted_rows.append(converted_row)\n    return converted_rows", "\n\ndef parse_table_long_name_to_json_list(table_long_names: str):\n    \"\"\"\n    Parse the table long names into a list of jsons with their database_name, schema_name and table_names.\n    The list should be sorted based on the database_name, then schema_name, then table_names.\n    Combine the tables with the same database_name and schema_name into one json.\n\n    Examples:\n\n    input1: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n    output1: \"[{\"database\": \"ethereum\", \"schema\": \"core\", \"table_names\": [\"ez_dex_swaps\", \"ez_nft_mints\", \"ez_nft_transfers\"]}]\"\n\n    input2: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.uniswapv3.ez_swaps, ethereum.beacon_chain.fact_deposits, ethereum.uniswapv3.ez_pools\"\n    output2: \"[\n        {\"database\": \"ethereum\", \"schema\": \"beacon_chain\", \"tables\": [\"fact_deposits\"]},\n        {\"database\": \"ethereum\", \"schema\": \"core\", \"tables\": [\"ez_dex_swaps\", \"ez_nft_mints\"]},\n        {\"database\": \"ethereum\", \"schema\": \"uniswapv3\", \"tables\": [\"ez_swaps\", \"ez_pools\"]}]\"\n    \"\"\"\n\n    table_names = re.split(r\",\\s*\", table_long_names)\n    table_data: Dict[str, Dict[str, List[str]]] = {}\n\n    for name in table_names:\n        check_table_long_name(name)\n        database, schema, table = name.split(\".\")\n        if database not in table_data:\n            table_data[database] = {}\n        if schema not in table_data[database]:\n            table_data[database][schema] = []\n        table_data[database][schema].append(table)\n\n    result = []\n    for database, schemas in table_data.items():\n        for schema, tables in schemas.items():\n            result.append(\n                {\"database\": database, \"schema\": schema, \"tables\": sorted(tables)}\n            )\n\n    result.sort(key=lambda x: (x[\"database\"], x[\"schema\"]))\n    return result", "\n\ndef parse_str_to_dict(\n    input_str: str, expected_keys: Optional[List[str]] = None\n) -> Dict[str, str]:\n    \"\"\"Parse a string to a dictionary\n    The string contains key-value pairs separated by a colon and comma.\n    The keys are expected to be in the expected_keys list.\n    The values are expected to be strings.\n    If the value contains multiple sub-elements separated by a comma,\n    the entire value is needs to be enclosed in a pair of square brackets, or quotes\n    For example: tables: [table1, table2], or tables: \"table1, table2\", or tables: 'table1, table2'\n    \"\"\"\n\n    if expected_keys is None:\n        expected_keys = []\n\n    # Split the string into key-value pairs\n    # The regex is to split the string on commas\n    # but not if the comma is inside square brackets or inside quotes\n    key_value_pairs = [\n        pair.strip()\n        for pair in re.split(\n            r\",(?![^[]*\\])(?=(?:[^']*'[^']*')*[^']*$)(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\",\n            input_str,\n        )\n    ]\n\n    input_dict = {}\n\n    for pair in key_value_pairs:\n        try:\n            key, value = [s.strip() for s in pair.split(\":\")]\n        except ValueError:\n            raise ValueError(\n                f\"Error extracting key, value pair: {pair=}, {key_value_pairs=}, {input_str=}\"\n            )\n\n        if (\n            value.startswith(\"[\")\n            and value.endswith(\"]\")\n            or value.startswith(\"'\")\n            and value.endswith(\"'\")\n            or value.startswith('\"')\n            and value.endswith('\"')\n        ):\n            # remove the square brackets or quotes if they exist\n            value = value[1:-1].strip()\n        input_dict[key] = value\n\n    # Check for missing expected keys\n    missing_keys = set(expected_keys) - set(input_dict.keys())\n    if missing_keys:\n        raise ValueError(f\"Missing expected keys: {', '.join(missing_keys)}\")\n\n    # Warn for unexpected keys\n    unexpected_keys = set(input_dict.keys()) - set(expected_keys)\n    if unexpected_keys:\n        warnings.warn(f\"Unexpected keys: {', '.join(unexpected_keys)}\")\n\n    return input_dict", ""]}
{"filename": "chatweb3/metadata_parser.py", "chunked_list": ["# %%\nimport json\nimport logging\nimport re\nfrom collections import defaultdict\nfrom typing import Dict, List, Optional\n\nfrom chatweb3.utils import (\n    parse_table_long_name,\n    parse_table_long_name_to_json_list,", "    parse_table_long_name,\n    parse_table_long_name_to_json_list,\n)\nfrom config.logging_config import get_logger\n\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\n\ndef nested_dict_to_dict(d):\n    return {\n        key: nested_dict_to_dict(value) if isinstance(value, defaultdict) else value\n        for key, value in d.items()\n    }", "\n\ndef nested_dict_to_dict(d):\n    return {\n        key: nested_dict_to_dict(value) if isinstance(value, defaultdict) else value\n        for key, value in d.items()\n    }\n\n\ndef dict_to_nested_dict(d):\n    nested = nested_dict()\n    for key, value in d.items():\n        if isinstance(value, dict):\n            nested[key] = dict_to_nested_dict(value)\n        else:\n            nested[key] = value\n    return nested", "\ndef dict_to_nested_dict(d):\n    nested = nested_dict()\n    for key, value in d.items():\n        if isinstance(value, dict):\n            nested[key] = dict_to_nested_dict(value)\n        else:\n            nested[key] = value\n    return nested\n", "\n\ndef len_nested_dict(nested_dict):\n    count = 0\n    for value in nested_dict.values():\n        if isinstance(value, dict):\n            count += len_nested_dict(value)\n        else:\n            count += 1\n    return count", "\n\ndef nested_dict():\n    return defaultdict(nested_dict)\n\n\nclass Column:\n    def __init__(\n        self,\n        name,\n        table_name=None,\n        schema_name=None,\n        database_name=None,\n        data_type=None,\n        comment=None,\n        verbose=False,\n    ):\n        self.name = name.lower()\n        self.table_name = table_name.lower() if table_name else None\n        self.schema_name = schema_name.lower() if schema_name else None\n        self.database_name = database_name.lower() if database_name else None\n        self.data_type = data_type\n        self.comment = comment\n        self.sample_values_list = []\n        self.verbose = verbose\n\n    def __repr__(self) -> str:\n        return f\"Column('{self.database_name}, {self.schema_name}, {self.table_name}, {self.name}, {self.data_type}, {self.comment}')\"\n\n    def __eq__(self, other):\n        if isinstance(other, Column):\n            return (\n                self.name == other.name\n                and self.table_name == other.table_name\n                and self.schema_name == other.schema_name\n                and self.database_name == other.database_name\n                and self.data_type == other.data_type\n                and self.comment == other.comment\n            )\n        return False\n\n    # property for verbose\n    @property\n    def verbose(self):\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, value):\n        self._verbose = value\n\n    def _parse_data_type_from_create_table_stmt(self, create_table_stmt):\n        pattern = rf\"{self.name}\\s+(?P<data_type>.+?(\\([^\\)]+\\))?(?=\\s*(,|\\n\\s*\\))))\"\n\n        match = re.search(pattern, create_table_stmt, re.IGNORECASE)\n\n        if match:\n            data_type = match.group(\"data_type\")\n        else:\n            if self.verbose:\n                logger.warning(\n                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} data type not parsed from create table statement.\"\n                )\n            data_type = None\n        return data_type\n\n    def _parse_comment_from_ddl(self, get_ddl_create_table):\n        pattern = rf\"{self.name}\\s+([\\w\\(\\),]*)?\\s*COMMENT\\s+'(?P<comment>.*?)'\"\n        match = re.search(pattern, get_ddl_create_table, re.IGNORECASE | re.DOTALL)\n\n        if match:\n            comment = match.group(\"comment\")\n        else:\n            if self.verbose:\n                logger.warning(\n                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} comment not parsed from get_ddl_create_table.\"\n                )\n            comment = None\n        return comment\n\n    def _parse_value_from_sample_rows(self, sample_row_column_names, sample_rows):\n        if self.name in sample_row_column_names:\n            index = sample_row_column_names.index(self.name)\n            values = [row[index] for row in sample_rows]\n            sample_values_list = list(values)\n        else:\n            if self.verbose:\n                logger.warning(\n                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} value not parsed from sample rows.\"\n                )\n            sample_values_list = None\n        return sample_values_list\n\n    def to_dict(self):\n        return {\n            \"name\": self.name,\n            \"table_name\": self.table_name,\n            \"schema_name\": self.schema_name,\n            \"database_name\": self.database_name,\n            \"data_type\": self.data_type,\n            \"comment\": self.comment,\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls(\n            data.get(\"name\"),\n            data.get(\"table_name\"),\n            data.get(\"schema_name\"),\n            data.get(\"database_name\"),\n            data.get(\"data_type\"),\n            data.get(\"comment\"),\n        )", "\n\nclass Table:\n    def __init__(self, table_name, schema_name, database_name, verbose=False):\n        self.name = table_name.lower()\n        self.schema_name = schema_name.lower()\n        self.database_name = database_name.lower()\n        self.long_name = f\"{database_name}.{schema_name}.{table_name}\".lower()\n        self.comment = \"\"\n        self.summary = \"\"\n        self.column_names = []\n        self.columns = {}\n        self.create_table_stmt = None\n        self.select_sample_rows_stmt = None\n        self.sample_row_column_names = None\n        self.sample_rows = None\n        self.select_get_ddl_table_stmt = None\n        self.get_ddl_create_table = None\n        self.select_information_schema_columns_stmt = None\n        self.information_schema_columns_names = None\n        self.information_schema_columns_values = None\n        self.verbose = verbose\n\n    def __repr__(self) -> str:\n        return f\"Table(name='{self.name}', long_name='{self.long_name}', database_name='{self.database_name}', schema_name='{self.schema_name}', column_names={self.column_names})\"\n\n    def __eq__(self, other):\n        if isinstance(other, Table):\n            return (\n                self.name == other.name\n                and self.database_name == other.database_name\n                and self.schema_name == other.schema_name\n                and self.long_name == other.long_name\n                and self.comment == other.comment\n                and self.summary == other.summary\n                and self.column_names == other.column_names\n                and self.create_table_stmt == other.create_table_stmt\n                and self.select_sample_rows_stmt == other.select_sample_rows_stmt\n                and self.sample_row_column_names == other.sample_row_column_names\n                and self.sample_rows == other.sample_rows\n                and self.select_get_ddl_table_stmt == other.select_get_ddl_table_stmt\n                and self.get_ddl_create_table == other.get_ddl_create_table\n                and self.select_information_schema_columns_stmt\n                == other.select_information_schema_columns_stmt\n                and self.information_schema_columns_names\n                == other.information_schema_columns_names\n                and self.information_schema_columns_values\n                == other.information_schema_columns_values\n                and self.columns == other.columns\n            )\n        return False\n\n    # create a property for summary\n    @property\n    def summary(self):\n        return self._summary\n\n    # setter for summary for the table and its columns\n    @summary.setter\n    def summary(self, summary):\n        self._summary = summary\n        # for column in self.columns.values():\n        #     column.summary = summary\n\n    # create a property for verbose\n    @property\n    def verbose(self):\n        return self._verbose\n\n    # setter for verbose for the table and its columns\n    @verbose.setter\n    def verbose(self, verbose):\n        self._verbose = verbose\n        for column in self.columns.values():\n            column.verbose = verbose\n\n    def _parse_comment_from_ddl(self, get_ddl_create_table):\n        comment_pattern = re.compile(\n            r\"COMMENT='{1,3}(.*?)'{1,3}(?:[\\s\\\\n]*as[\\s\\\\n]*\\((?:.|[\\r\\n])*?SELECT|[\\s\\\\n]*;)\",\n            re.IGNORECASE,\n        )\n        match = comment_pattern.search(get_ddl_create_table)\n        if match:\n            comment = match.group(1).strip().replace(\"\\\\n\", \"\\n\")\n        else:\n            if self.verbose:\n                logger.warning(\n                    f\"Could not parse comment for table {self.database_name}.{self.schema_name}.{self.name}\"\n                )\n            comment = \"\"\n        return comment\n\n    def _create_columns(self):\n        \"\"\"Create Column objects for each column in the table if they do not already exist.\"\"\"\n        for column_name in self.column_names:\n            if column_name not in self.columns:\n                self.columns[column_name] = Column(\n                    name=column_name,\n                    table_name=self.name,\n                    schema_name=self.schema_name,\n                    database_name=self.database_name,\n                )\n\n    @staticmethod\n    def _format_value(value):\n        if isinstance(value, str):\n            replaced_value = value.replace(\"\\n\", \" \")\n            # return f\"'{replaced_value}'\"\n            return replaced_value\n        return value\n\n    def _get_metadata(\n        self,\n        include_table_name: bool = True,\n        include_table_summary: bool = True,\n        include_column_names: bool = False,\n        include_column_info: bool = True,\n        column_info_format: Optional[List[str]] = None,\n    ) -> str:\n        \"\"\"\n        By default, this method returns a string with the table name, summary.\n        Optionally it can return column names, or more detailed column information.\n        \"\"\"\n        output = \"\"\n        if include_table_name:\n            output += f\"'{self.database_name}.{self.schema_name}.{self.name}': the '{self.name}' table in '{self.schema_name}' schema of '{self.database_name}' database. \"\n        if include_table_summary and not include_column_info:\n            # if include_column_info if True, then we will use the table comment as the summary and not the table summary\n            output += f\"{self.summary}\"\n\n        if include_column_names and not include_column_info:\n            column_names = \", \".join(self.column_names)\n            output += f\"This table has the following columns: '{column_names}'\\n\"\n\n        if include_column_info:\n            if column_info_format is None:\n                column_info_format = [\n                    \"name\",\n                    \"comment\",\n                    \"data_type\",\n                    \"sample_values_list\",\n                ]\n            column_info = {\n                \"name\": \"Name\",\n                \"comment\": \"Comment\",\n                \"data_type\": \"Data type\",\n                \"sample_values_list\": \"List of sample values\",\n            }\n\n            output += f\"\\nComment: {self.comment}\\n\"\n\n            if len(column_info_format) > 0:\n                output += \"Columns in this table:\\n\"\n\n                headers = [column_info[col] for col in column_info_format]\n                output += \"\\t\" + \" | \".join(headers) + \"\\n\"\n                output += \"\\t\" + \"--- | \" * (len(column_info_format) - 1) + \"---\\n\"\n\n                for column in self.columns.values():\n                    column_values = [getattr(column, col) for col in column_info_format]\n\n                    formatted_values = [\n                        \", \".join(str(self._format_value(v)) for v in value)\n                        if isinstance(value, list)\n                        else self._format_value(value)\n                        for value in column_values\n                    ]\n\n                    output += (\n                        \"\\t\" + \" | \".join([str(val) for val in formatted_values]) + \"\\n\"\n                    )\n\n        return output.strip()\n\n    def to_dict(self):\n        return {\n            key: value\n            for key, value in {\n                \"name\": self.name,\n                \"database_name\": self.database_name,\n                \"schema_name\": self.schema_name,\n                \"long_name\": self.long_name,\n                \"comment\": self.comment,\n                \"summary\": self.summary,\n                \"column_names\": self.column_names,\n                \"columns\": {\n                    name: column.to_dict() for name, column in self.columns.items()\n                },\n                \"create_table_stmt\": self.create_table_stmt,\n                \"select_sample_rows_stmt\": self.select_sample_rows_stmt,\n                \"sample_row_column_names\": self.sample_row_column_names,\n                \"sample_rows\": self.sample_rows,\n                \"select_get_ddl_table_stmt\": self.select_get_ddl_table_stmt,\n                \"get_ddl_create_table\": self.get_ddl_create_table,\n                \"select_information_schema_columns_stmt\": self.select_information_schema_columns_stmt,\n                \"information_schema_columns_names\": self.information_schema_columns_names,\n                \"information_schema_columns_values\": self.information_schema_columns_values,\n            }.items()\n            if value\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        table = cls(\n            data.get(\"name\"), data.get(\"schema_name\"), data.get(\"database_name\")\n        )\n        # data[\"name\"], data[\"schema_name\"], data[\"database_name\"])\n        table.long_name = data.get(\"long_name\").lower()\n        table.comment = data.get(\"comment\")\n        table.summary = data.get(\"summary\")\n        table.column_names = [x.lower() for x in data.get(\"column_names\")]\n        table.columns = {\n            name.lower(): Column.from_dict(col_data)\n            for name, col_data in data.get(\"columns\", {}).items()\n        }\n        table.create_table_stmt = data.get(\"create_table_stmt\")\n        table.select_sample_rows_stmt = data.get(\"select_sample_rows_stmt\")\n        table.sample_row_column_names = data.get(\"sample_row_column_names\")\n        table.sample_rows = data.get(\"sample_rows\")\n        table.select_get_ddl_table_stmt = data.get(\"select_get_ddl_table_stmt\")\n        table.get_ddl_create_table = data.get(\"get_ddl_create_table\")\n        table.select_information_schema_columns_stmt = data.get(\n            \"select_information_schema_columns_stmt\"\n        )\n        table.information_schema_columns_names = data.get(\n            \"information_schema_columns_names\"\n        )\n        table.information_schema_columns_values = data.get(\n            \"information_schema_columns_values\"\n        )\n        # adjust for columns\n        return table", "\n\nclass Schema:\n    def __init__(self, schema_name, database_name, verbose=False):\n        self.name = schema_name\n        self.database_name = database_name.lower()\n        self.long_name = f\"{database_name}.{schema_name}\".lower()\n        self.tables = {}\n        self.verbose = verbose\n\n    def __repr__(self) -> str:\n        return f\"Schema(name='{self.name}', long_name='{self.long_name}', database_name='{self.database_name}', tables={self.tables})\"\n\n    def __eq__(self, other):\n        if isinstance(other, Schema):\n            return (\n                self.name == other.name\n                and self.database_name == other.database_name\n                and self.long_name == other.long_name\n                and self.tables == other.tables\n            )\n        return False\n\n    # property for verbose\n    @property\n    def verbose(self):\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, value):\n        self._verbose = value\n        for table in self.tables.values():\n            table.verbose = value\n\n    def to_dict(self):\n        return {\n            \"name\": self.name,\n            \"database_name\": self.database_name,\n            \"tables\": {name: table.to_dict() for name, table in self.tables.items()},\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        schema = cls(data[\"name\"], data[\"database_name\"])\n        schema.tables = {\n            name.lower(): Table.from_dict(table_data)\n            for name, table_data in data[\"tables\"].items()\n        }\n        return schema", "\n\nclass Database:\n    def __init__(self, database_name, verbose=False):\n        self.name = database_name\n        self.schemas = {}\n        self.verbose = verbose\n\n    def __repr__(self) -> str:\n        return f\"Database(name='{self.name}', schemas={self.schemas})\"\n\n    def __eq__(self, other):\n        if isinstance(other, Database):\n            return self.name == other.name and self.schemas == other.schemas\n        return False\n\n    # property for verbose\n    @property\n    def verbose(self):\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, value):\n        self._verbose = value\n        for schema in self.schemas.values():\n            schema.verbose = value\n\n    def to_dict(self):\n        return {\n            \"name\": self.name,\n            \"schemas\": {\n                name: schema.to_dict() for name, schema in self.schemas.items()\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        database = cls(data.get(\"name\"))\n        # [\"name\"])\n        database.schemas = {\n            name.lower(): Schema.from_dict(schema_data)\n            for name, schema_data in data[\"schemas\"].items()\n        }\n        return database", "\n\nclass RootSchema:\n    def __init__(self):\n        self.databases = {}  # dictionary of databases\n        self.verbose = False\n\n    def __repr__(self) -> str:\n        return f\"RootSchema(databases={self.databases})\"\n\n    def __eq__(self, other):\n        if isinstance(other, RootSchema):\n            return self.databases == other.databases\n        return False\n\n    # property for verbose\n    @property\n    def verbose(self):\n        return self._verbose\n\n    @verbose.setter\n    def verbose(self, value):\n        self._verbose = value\n        for database in self.databases.values():\n            database.verbose = value\n\n    def to_dict(self):\n        return {\n            \"databases\": {\n                name: database.to_dict() for name, database in self.databases.items()\n            },\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        root_schema = cls()\n        root_schema.databases = {\n            name.lower(): Database.from_dict(cat_data)\n            for name, cat_data in data[\"databases\"].items()\n        }\n        return root_schema", "\n\nclass MetadataParser:\n    def __init__(\n        self,\n        file_path: Optional[str] = None,\n        annotation_file_path: Optional[str] = None,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Note: the verbose flag is only effective when the file_path is provided. Otherwise, we have to manually set it after the contents of the root_schema_obj is set.\n        The verbose flag is useful when we want to print out the processing warning messages, e.g., parsing issues for comments and other fields of metadata.\n        \"\"\"\n        self.file_path = file_path\n        self._verbose = verbose\n\n        self._initialize_nested_dicts()\n\n        if file_path is not None:\n            # If a file_path is provided, load metadata from the file\n            data = self.load_metadata_from_json()\n            # then deserialize the metadata into the RootSchema object\n            self.from_dict(data, verbose=verbose)\n\n            if annotation_file_path is not None:\n                # If an annotation_file_path is provided, load the annotation file and add the summary to the RootSchema object\n                self.add_table_summary(file_path_json=annotation_file_path)\n\n            # set the verbose flag for the RootSchema object\n            # self.verbose = verbose\n        else:\n            # If no file_path is provided, create an empty RootSchema object\n            self.root_schema_obj = RootSchema()\n\n    def from_dict(self, data, verbose: bool = False):\n        \"\"\"Takes in the metadata dictionary loaded from the JSON file and deserializes it into the RootSchema object\"\"\"\n        self.root_schema_obj = RootSchema.from_dict(data=data[\"root_schema_obj\"])\n        self._unify_names_to_lower_cases()\n        # Create the column objects for each table\n        self._create_table_columns()\n        if verbose:\n            self.root_schema_obj.verbose = verbose\n\n        # Populate the comment for each table\n        self._populate_table_comment()\n        # Populate various column attributes\n        self._populate_column_table_schema_database_names()\n        self._populate_column_data_type()\n        self._populate_column_comment()\n        self._populate_column_sample_values_list()\n\n        # NOTE: the use of nested dicts will be removed in the future\n        # Initialize the nested dictionaries\n        self._initialize_nested_dicts()\n        # Populate the nested dictionaries\n        self._populate_nested_dicts()\n\n    def to_dict(self):\n        self._unify_names_to_lower_cases()\n        data = {\n            \"root_schema_obj\": self.root_schema_obj.to_dict(),\n        }\n        return data\n\n    def _initialize_nested_dicts(self):\n        self.create_table_stmt = nested_dict()\n        self.select_sample_rows_stmt = nested_dict()\n        self.table_sample_rows = nested_dict()\n        self.table_sample_row_column_names = nested_dict()\n        self.select_get_ddl_table_stmt = nested_dict()\n        self.get_ddl_create_table = nested_dict()\n        self.select_information_schema_columns_stmt = nested_dict()\n        self.information_schema_columns_names = nested_dict()\n        self.information_schema_columns_values = nested_dict()\n\n    def _populate_nested_dicts(self):\n        for database_name, database in self.root_schema_obj.databases.items():\n            for schema_name, schema in database.schemas.items():\n                for table_name, table in schema.tables.items():\n                    self.create_table_stmt[database_name][schema_name][table_name] = (\n                        table.create_table_stmt or None\n                    )\n\n                    self.select_sample_rows_stmt[database_name][schema_name][\n                        table_name\n                    ] = (table.select_sample_rows_stmt or None)\n\n                    self.table_sample_row_column_names[database_name][schema_name][\n                        table_name\n                    ] = (table.sample_row_column_names or None)\n\n                    self.table_sample_rows[database_name][schema_name][table_name] = (\n                        table.sample_rows or None\n                    )\n\n                    self.select_get_ddl_table_stmt[database_name][schema_name][\n                        table_name\n                    ] = (table.select_get_ddl_table_stmt or None)\n\n                    self.get_ddl_create_table[database_name][schema_name][\n                        table_name\n                    ] = (table.get_ddl_create_table or None)\n\n                    self.select_information_schema_columns_stmt[database_name][\n                        schema_name\n                    ][table_name] = (\n                        table.select_information_schema_columns_stmt or None\n                    )\n\n                    self.information_schema_columns_names[database_name][schema_name][\n                        table_name\n                    ] = (table.information_schema_columns_names or None)\n\n                    self.information_schema_columns_values[database_name][schema_name][\n                        table_name\n                    ] = (table.information_schema_columns_values or None)\n\n    def _populate_column_table_schema_database_names(self):\n        for database_name, database in self.root_schema_obj.databases.items():\n            for schema_name, schema in database.schemas.items():\n                for table_name, table in schema.tables.items():\n                    for column in table.columns.values():\n                        column.table_name = table_name\n                        column.schema_name = schema_name\n                        column.database_name = database_name\n\n    def _create_table_columns(self):\n        \"\"\"Create Column objects for each column in each table in each schema in each database, if they do not already exists.\"\"\"\n        for database_name, database in self.root_schema_obj.databases.items():\n            for schema_name, schema in database.schemas.items():\n                for table_name, table in schema.tables.items():\n                    table._create_columns()\n\n    def _populate_column_comment(self):\n        for database_name, database in self.root_schema_obj.databases.items():\n            for schema_name, schema in database.schemas.items():\n                for table_name, table in schema.tables.items():\n                    ddl_create_table = table.get_ddl_create_table\n                    for column in table.columns.values():\n                        column.comment = (\n                            column._parse_comment_from_ddl(ddl_create_table)\n                            if ddl_create_table\n                            else None\n                        )\n\n    #                    ddl_create_table = self.get_ddl_create_table[database_name][ schema_name ][table_name]\n\n    def _populate_column_data_type(self):\n        for database_name, database in self.root_schema_obj.databases.items():\n            for schema_name, schema in database.schemas.items():\n                for table_name, table in schema.tables.items():\n                    create_table_stmt = table.create_table_stmt\n                    for column in table.columns.values():\n                        column.data_type = (\n                            (\n                                column._parse_data_type_from_create_table_stmt(\n                                    create_table_stmt\n                                )\n                            )\n                            if create_table_stmt\n                            else None\n                        )\n\n                    # create_table_stmt = self.create_table_stmt[database_name][ schema_name ][table_name]\n\n    def _populate_column_sample_values_list(self):\n        for database in self.root_schema_obj.databases.values():\n            for schema in database.schemas.values():\n                for table in schema.tables.values():\n                    for column in table.columns.values():\n                        column.sample_values_list = (\n                            column._parse_value_from_sample_rows(\n                                table.sample_row_column_names,\n                                table.sample_rows,\n                            )\n                            if table.sample_rows\n                            else None\n                        )\n\n    def _populate_table_comment(self):\n        for db in self.root_schema_obj.databases.values():\n            for schema in db.schemas.values():\n                for table in schema.tables.values():\n                    table.comment = (\n                        table._parse_comment_from_ddl(table.get_ddl_create_table)\n                        if table.get_ddl_create_table\n                        else None\n                    )\n\n    def _unify_names_to_lower_cases(self):\n        for database in self.root_schema_obj.databases.values():\n            database.name = database.name.lower()\n            for schema in database.schemas.values():\n                schema.name = schema.name.lower()\n                for table in schema.tables.values():\n                    table.name = table.name.lower()\n                    for column in table.columns.values():\n                        column.name = column.name.lower()\n\n    def add_table_summary(\n        self,\n        table_summary_json: Optional[Dict] = None,\n        file_path_json: Optional[str] = None,\n    ):\n        \"\"\"Add a table summary to the metadata.\n        Input file must be a JSON file with the following structure:\n        {\n            \"table_summary\": {\n            \"table_long_name\": \"table_summary\",\n            \"table_long_name\": \"table_summary\",\n            ...\n            }\n        }\n        \"\"\"\n        if table_summary_json is None and file_path_json is None:\n            raise ValueError(\n                \"Either table_summary_json or file_path_json must be provided.\"\n            )\n\n        if table_summary_json and file_path_json:\n            logging.warning(\n                \"Both table_summary_json and file_path_json are provided, use file_path_json only.\"\n            )\n\n        if file_path_json:\n            try:\n                with open(file_path_json, \"r\") as f:\n                    data = json.load(f)\n                    table_summary = data.get(\"table_summary\", None)\n            except FileNotFoundError:\n                print(f\"File not found: {file_path_json}\")\n        elif table_summary_json:\n            # table_summary = json.loads(table_summary_json)\n            table_summary = table_summary_json\n        else:\n            raise ValueError(\n                f\"Unable to load table summary from {file_path_json} or {table_summary_json}.\"\n            )\n\n        for table_long_name, summary in table_summary.items():\n            # parse the table long name\n            database_name, schema_name, table_name = parse_table_long_name(\n                table_long_name\n            )\n            # add the summary to the table\n            self.root_schema_obj.databases[database_name].schemas[schema_name].tables[\n                table_name\n            ].summary = summary\n\n    # create a property to access the verbose attribute\n    @property\n    def verbose(self):\n        return self._verbose\n\n    # create a setter for the verbose attribute\n    @verbose.setter\n    def verbose(self, value: bool):\n        self._verbose = value\n        self.root_schema_obj.verbose = value\n\n    def save_metadata_to_json(self, file_path):\n        with open(file_path, \"w\") as f:\n            json.dump(self.to_dict(), f)\n\n    def load_metadata_from_json(self, file_path=None):\n        \"\"\"Load metadata from a JSON file and returns it.\"\"\"\n        if file_path is None:\n            file_path = self.file_path\n\n        with open(file_path, \"r\") as f:\n            metadata = json.load(f)\n        return metadata\n\n    def get_tables_from_database_schema_table_names(\n        self, database_name=None, schema_name=None, table_name=None\n    ):\n        \"\"\"return a table or a list of tables\n        filtered by database_name, schema_name, and table_name if provided return all table names if database_name or schema_name is None\n        \"\"\"\n        # check for error conditions in database_name\n        if database_name is not None and not isinstance(database_name, str):\n            raise ValueError(\"database_name must be a string\")\n        if schema_name is not None and not isinstance(schema_name, str):\n            raise ValueError(\"schema_name must be a string\")\n        if table_name is not None and not isinstance(schema_name, str):\n            raise ValueError(\"schema_name must be a string\")\n\n        tables = []\n\n        if table_name:\n            # if table_name is provided, then we only return one table\n            if database_name is None or schema_name is None:\n                raise ValueError(\n                    \"database_name and schema_name must be provided if table_name is provided\"\n                )\n            table = (\n                self.root_schema_obj.databases[database_name]\n                .schemas[schema_name]\n                .tables[table_name]\n            )\n            tables.append(table)\n        else:\n            for (\n                cat_name,\n                database,\n            ) in (\n                self.root_schema_obj.databases.items()\n            ):  # database could be e.g., \"ethereum\"\n                if database_name is None or cat_name == database_name:\n                    for sch_name, schema in database.schemas.items():\n                        if schema_name is None or sch_name == schema_name:\n                            for table in schema.tables.values():\n                                tables.append(table)\n        return tables\n\n    def get_table_metadata(\n        self,\n        database: Optional[str] = None,\n        schema: Optional[str] = None,\n        tables: Optional[List] = None,\n        include_table_name: Optional[bool] = True,\n        include_table_summary: Optional[bool] = True,\n        include_column_names: Optional[bool] = False,\n        include_column_info: Optional[bool] = True,\n        column_info_format: Optional[List] = None,\n    ):\n        \"\"\"\n        Process and return metadata information given database, schema and table.\n\n        Args:\n            database (str, optional): The database of the table.\n            schema (str, optional): The schema of the table.\n            tables (list of str, optional): The names of the tables.\n\n        Returns:\n            str: The concatenated table information.\n\n        \"\"\"\n        target_tables = self._find_target_tables(database, schema, tables)\n\n        output = \"\"\n        for table in target_tables:\n            output += table._get_metadata(\n                include_table_name=include_table_name,\n                include_table_summary=include_table_summary,\n                include_column_names=include_column_names,\n                include_column_info=include_column_info,\n                column_info_format=column_info_format,\n            )\n\n            output += \"\\n\\n\"\n\n        return output.strip()\n\n    def _find_target_tables(self, database=None, schema=None, tables=None):\n        matched_tables = []\n\n        if database is not None and database not in self.root_schema_obj.databases:\n            logger.warning(f\"Database '{database}' does not exist.\")\n\n        for db_name, db in self.root_schema_obj.databases.items():\n            if database is None or db_name == database:\n                for sch_name, sch in db.schemas.items():\n                    if schema is None or sch_name == schema:\n                        if tables is None:\n                            matched_tables.extend(list(sch.tables.values()))\n                        else:\n                            for table_name in tables:\n                                if table_name in sch.tables:\n                                    matched_tables.append(sch.tables[table_name])\n\n        return matched_tables\n\n    def get_metadata_by_table_long_names(\n        self,\n        table_long_names: str,\n        include_table_name: Optional[bool] = True,\n        include_table_summary: Optional[bool] = True,\n        include_column_names: Optional[bool] = False,\n        include_column_info: Optional[bool] = True,\n        column_info_format: Optional[List] = None,\n    ) -> str:\n        \"\"\"\n        Process and return metadata information given a list of table long names in the form of\n        database_name.schema_name.table_name.\n\n        Args:\n            table_long_names (str): The list of table long names.\n\n        Returns:\n            str: The concatenated table information.\n        \"\"\"\n\n        parsed_table_info = parse_table_long_name_to_json_list(table_long_names)\n\n        output = \"\"\n        for info in parsed_table_info:\n            database = info[\"database\"]\n            schema = info[\"schema\"]\n            tables = info[\"tables\"]\n            output += self.get_table_metadata(\n                database,\n                schema,\n                tables,\n                include_table_name=include_table_name,\n                include_table_summary=include_table_summary,\n                include_column_names=include_column_names,\n                include_column_info=include_column_info,\n                column_info_format=column_info_format,\n            )\n            output += \"\\n\\n\"\n\n        return output.strip()", ""]}
{"filename": "chatweb3/tools/base.py", "chunked_list": ["from pydantic import BaseModel\n\n\nclass BaseToolInput(BaseModel):\n    \"\"\"A base class for tool input models.\"\"\"\n\n    pass\n"]}
{"filename": "chatweb3/tools/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/tools/snowflake_database/tool_custom.py", "chunked_list": ["\"\"\"\ntool_custom.py\nThis file contains the custom tools for the snowflake_database toolkit.\n\"\"\"\nfrom typing import Any, List, Optional, Union\n\nfrom langchain.callbacks.manager import (\n    CallbackManagerForToolRun,\n)\n", ")\n\nfrom chatweb3.tools.snowflake_database.tool import (\n    GetSnowflakeDatabaseTableMetadataTool,\n    ListSnowflakeDatabaseTableNamesTool,\n    QuerySnowflakeDatabaseTool,\n    SnowflakeQueryCheckerTool,\n)\nfrom config.config import agent_config\n", "from config.config import agent_config\n\nQUERY_DATABASE_TOOL_MODE = agent_config.get(\"tool.query_database_tool_mode\")\nCHECK_TABLE_SUMMARY_TOOL_MODE = agent_config.get(\"tool.check_table_summary_tool_mode\")\nCHECK_TABLE_METADATA_TOOL_MODE = agent_config.get(\"tool.check_table_metadata_tool_mode\")\n\nCHECK_TABLE_SUMMARY_TOOL_NAME = \"check_available_tables_summary\"\nCHECK_TABLE_METADATA_TOOL_NAME = \"check_table_metadata_details\"\nCHECK_QUERY_SYNTAX_TOOL_NAME = \"check_snowflake_query_syntax\"\nQUERY_DATABASE_TOOL_NAME = \"query_snowflake_database\"", "CHECK_QUERY_SYNTAX_TOOL_NAME = \"check_snowflake_query_syntax\"\nQUERY_DATABASE_TOOL_NAME = \"query_snowflake_database\"\nTOOLKIT_INSTRUCTIONS = f\"\"\"\nWhen using these tools, you MUST follow the instructions below:\n1. You MUST always start with the {CHECK_TABLE_SUMMARY_TOOL_NAME} tool to check the available tables in the databases, and make selection of one or multiple tables you want to work with if applicable.\n2. If you need to construct any SQL query for any table, you MUST always use the {CHECK_TABLE_METADATA_TOOL_NAME} tool to check the metadata detail of the table before you can create a query for that table. Note that you can check the metadata details of multiple tables at the same time.\n3. When constructing a query containing a token or NFT, if you have both its address and and its symbol, you MUST always prefer using the token address over the token symbol since token symbols are often not unique.\n4. If you receive and error from  {QUERY_DATABASE_TOOL_NAME} tool, you MUST always analyze the error message and determine how to resolve it. If it is a general syntax error, you MUST use the {CHECK_QUERY_SYNTAX_TOOL_NAME} tool to double check the query before you can run it again through the {QUERY_DATABASE_TOOL_NAME} tool. If it is due to invalid table or column names, you MUST double check the {CHECK_TABLE_METADATA_TOOL_NAME} tool and re-construct the query accordingly.\n\"\"\"\n", "\"\"\"\n\n\nclass CheckTableSummaryTool(ListSnowflakeDatabaseTableNamesTool):\n    name = CHECK_TABLE_SUMMARY_TOOL_NAME\n    description = \"\"\"\n    Input is an empty string.\n    Output is the list of available tables in their full names (database.schema.table), accompanied by their summary descriptions to help you understand what each table is about.\n    \"\"\"\n\n    def _run(  # type: ignore[override]\n        self,\n        tool_input: str = \"\",\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        mode: str = CHECK_TABLE_SUMMARY_TOOL_MODE,\n    ) -> str:\n        return super()._run(tool_input=tool_input, mode=mode)", "\n\nclass CheckTableMetadataTool(GetSnowflakeDatabaseTableMetadataTool):\n    name = CHECK_TABLE_METADATA_TOOL_NAME\n    description = \"\"\"\n    Input is one or more table names specified in their full names (database.schema.table) and seperated by a COMMA.\n    Output is the detailed metadata including column specifics of those tables so that you can construct SQL query to them.\n    \"\"\"\n\n    def _run(self, table_names: str, run_manager: Optional[CallbackManagerForToolRun] = None, mode: str = CHECK_TABLE_METADATA_TOOL_MODE) -> str:  # type: ignore[override]\n        return super()._run(table_names=table_names, run_manager=run_manager, mode=mode)", "\n\nclass CheckQuerySyntaxTool(SnowflakeQueryCheckerTool):\n    name = CHECK_QUERY_SYNTAX_TOOL_NAME\n    description = \"\"\"\n    Input is a Snowflake SQL query.\n    Output is the syntax check result of the query.\n    \"\"\"\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        return super()._run(query=query, run_manager=run_manager)", "\n\nclass QueryDatabaseTool(QuerySnowflakeDatabaseTool):\n    name = QUERY_DATABASE_TOOL_NAME\n    description = \"\"\"\n    Input to this tool contains a Snowflake SQL query in correct syntax. It should be in JSON format with EXACTLY ONE key \"query\" that has the value of the query string.\n    Output is the query result from the database.\n    \"\"\"\n\n    def _run(  # type: ignore[override]\n        self,\n        *args,\n        mode: str = QUERY_DATABASE_TOOL_MODE,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        **kwargs,\n    ) -> Union[str, List[Any]]:\n        return super()._run(*args, mode=mode, run_manager=run_manager, **kwargs)", ""]}
{"filename": "chatweb3/tools/snowflake_database/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/tools/snowflake_database/prompt.py", "chunked_list": ["# flake8: noqa\nSNOWFLAKE_QUERY_CHECKER = \"\"\"\nDouble check the {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type", "- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\"\"\"\n"]}
{"filename": "chatweb3/tools/snowflake_database/tool.py", "chunked_list": ["import json\nimport logging\nimport re\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (AsyncCallbackManagerForToolRun,\n                                         CallbackManagerForToolRun)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models.base import BaseChatModel", "from langchain.chains.llm import LLMChain\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.prompts.chat import (BaseMessagePromptTemplate,\n                                    ChatPromptTemplate,\n                                    HumanMessagePromptTemplate,\n                                    SystemMessagePromptTemplate)\nfrom langchain.schema import BaseMessage\nfrom langchain.tools.sql_database.tool import (InfoSQLDatabaseTool,\n                                               ListSQLDatabaseTool,\n                                               QueryCheckerTool,", "                                               ListSQLDatabaseTool,\n                                               QueryCheckerTool,\n                                               QuerySQLDataBaseTool)\nfrom pydantic import Field, root_validator\n\nfrom chatweb3.snowflake_database import SnowflakeContainer\nfrom chatweb3.tools.base import BaseToolInput\nfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\nfrom chatweb3.utils import \\\n    parse_table_long_name_to_json_list  # parse_str_to_dict", "from chatweb3.utils import \\\n    parse_table_long_name_to_json_list  # parse_str_to_dict\nfrom config.config import agent_config\nfrom config.logging_config import get_logger\n\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\n", "\n\nLIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME = \"list_snowflake_db_table_names\"\nGET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME = \"get_snowflake_db_table_metadata\"\nQUERY_SNOWFLAKE_DATABASE_TOOL_NAME = \"query_snowflake_db\"\nSNOWFLAKE_QUERY_CHECKER_TOOL_NAME = \"snowflake_query_checker\"\nSELECT_SNOWFLAKE_DATABASE_SCHEMA_TOOL_NAME = \"select_snowflake_db_schema\"\n\nDEFAULT_DATABASE = agent_config.get(\"database.default_database\")\nDEFAULT_SCHEMA = agent_config.get(\"database.default_schema\")", "DEFAULT_DATABASE = agent_config.get(\"database.default_database\")\nDEFAULT_SCHEMA = agent_config.get(\"database.default_schema\")\n\n\nclass ListSnowflakeDatabaseTableNamesToolInput(BaseToolInput):\n    database_name: str = Field(\n        alias=\"database\", description=\"The name of the database.\"\n    )\n    schema_name: str = Field(alias=\"schema\", description=\"The name of the schema.\")\n", "\n\nclass ListSnowflakeDatabaseTableNamesTool(ListSQLDatabaseTool):\n    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\n    name = LIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME\n    description = \"\"\"\n    Input is an empty string, output is a list of tables in long form, e.g., 'ethereum.core.ez_nft_sales' means the 'ez_nft_sales' table in 'core' schema of 'ethereum' database, and optionally with their brief summary information.\n    \"\"\"\n\n    def _get_table_long_names_from_snowflake(self, tool_input: str) -> str:\n        database = DEFAULT_DATABASE\n        schema = DEFAULT_SCHEMA\n\n        # Locate the SQLDatabase object with the given database and schema\n        snowflake_database = self.db.get_database(database, schema)\n\n        # Call its get_table_names() method\n        table_names = snowflake_database.get_usable_table_names()\n\n        # Add the database and schema to the table names\n        table_long_names = [f\"{database}.{schema}.{table}\" for table in table_names]\n\n        return \", \".join(table_long_names)\n\n    def _run(\n        self,\n        tool_input: str = \"\",\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        mode: str = \"default\",\n    ) -> str:\n        \"\"\"Get available tables in the databases\n\n        mode:\n        - \"default\": use local index to get the info, if not found, use snowflake as fallback\n        - \"snowflake\": use snowflake to get the info\n        - \"local\": use local index to get the info\n\n        Note: since local index is currently enforced by the table_long_names_enabled variable, while snowflake is enforced by its own Magicdatabase and MagicSchema, the two modes often produce different results.\n\n        \"\"\"\n        logger.debug(\n            f\"Entering list snowflake database table names tool _run with tool_input: {tool_input} and mode: {mode}\"\n        )\n\n        if mode not in [\"local\", \"snowflake\", \"default\"]:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        table_long_names_enabled_list = agent_config.get(\n            \"ethereum_core_table_long_name.enabled_list\"\n        )\n\n        table_long_names_enabled = \", \".join(table_long_names_enabled_list)\n        include_column_names = False\n        # include_column_names = True\n        include_column_info = False\n\n        if mode == \"local\":\n            # use local index to get the info\n            return self.db.metadata_parser.get_metadata_by_table_long_names(\n                table_long_names=table_long_names_enabled,\n                include_column_names=include_column_names,\n                include_column_info=include_column_info,\n            )\n\n        if mode == \"snowflake\":\n            return self._get_table_long_names_from_snowflake(tool_input=tool_input)\n\n        # default mode\n        # use local index to get the info, if not found, use snowflake as fallback\n        if mode == \"default\":\n            try:\n                # logger.debug(f\"mode: {mode}\")\n                result = self.db.metadata_parser.get_metadata_by_table_long_names(\n                    table_long_names=table_long_names_enabled,\n                    include_column_names=include_column_names,\n                    include_column_info=include_column_info,\n                )\n                if result:\n                    return result\n                else:\n                    raise Exception(\"Table name list not found in local index\")\n            except Exception as e:\n                logger.debug(f\"Table name list not found in local index: {e}\")\n                return self._get_table_long_names_from_snowflake(tool_input=tool_input)\n\n        return \"\"  # this is a dummy return, just to make mypy happy\n\n    async def _arun(\n        self,\n        tool_input: str = \"\",\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n    ) -> str:\n        raise NotImplementedError(\"ListSQLDatabaseNameTool does not support async\")", "\n\nclass GetSnowflakeDatabaseTableMetadataTool(InfoSQLDatabaseTool):\n    \"\"\"Tool for getting metadata about a SQL database schema.\"\"\"\n\n    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\n    name = GET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME\n    description = f\"\"\"\n    Input to this tool is a list of tables specified by the long_name format (database_name.schema_name.table_name), make sure they are separated by a COMMA, not a colon or other characters!\n\n    Output is the metadata including schema and sample rows for those tables.\n    Be sure that the tables actually exist by calling {LIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME} first!\n\n    Example Input: \"database_name.schema_name.table_name1: database_name.schema_name.table_name2: database_name.schema_name.table_name3\"\n    \"\"\"\n\n    def _get_metadata_from_snowflake(self, tool_input: str) -> str:\n        \"\"\"First parse the input into a list of jsons with database and schema separated.\n            Example:\n            input: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.uniswapv3.ez_swaps, ethereum.beacon_chain.fact_deposits, ethereum.uniswapv3.ez_pools\"\n            parsed_input: \"[\n                {\"database\": \"ethereum\", \"schema\": \"beacon_chain\", \"tables\": [\"fact_deposits\"]},\n                {\"database\": \"ethereum\", \"schema\": \"core\", \"tables\": [\"ez_dex_swaps\", \"ez_nft_mints\"]},\n                {\"database\": \"ethereum\", \"schema\": \"uniswapv3\", \"tables\": [\"ez_swaps\", \"ez_pools\"]}\n            ]\"\n\n        Then iterate through the list and call self.db['database']['schema'].get_table_info_no_throw(tables)\n\n        Sample final output:\n        \"database_name.schema_name.table_name1: metadata1;\\n database_name.schema_name.table_name2: metadata2;\\n database_name.schema_name.table_name3: metadata3\"\n        \"\"\"\n        input_table_json_list = parse_table_long_name_to_json_list(tool_input)\n        logger.debug(\n            f\"Entering _get_metadata_from_snowflake with {input_table_json_list=}\"\n        )\n        metadata_list = []\n        for input_dict in input_table_json_list:\n            logging.debug(f\"\\n{input_dict=}\")\n            database, schema, tables = (\n                input_dict[\"database\"],\n                input_dict[\"schema\"],\n                input_dict[\"tables\"],\n            )\n            snowflake_database = self.db.get_database(database, schema)\n            # table_names = \", \".join(tables)\n            metadata = snowflake_database.get_table_info_no_throw(\n                table_names=tables, as_dict=True\n            )\n            assert isinstance(\n                metadata, dict\n            ), \"Expected a dict from get_table_info_no_throw\"  # make mypy happy\n\n            logger.debug(f\"\\n Retrieved {metadata=}\")\n\n            # Add the formatted metadata string for each table to metadata_list\n            for table, table_metadata in metadata.items():\n                table_parts = table.split(\".\")\n                table_name = table_parts[-1]\n                table_schema = table_parts[-2] if len(table_parts) > 1 else schema\n                table_database = table_parts[-3] if len(table_parts) > 2 else database\n                # logger.debug( f\"\\n{table=}, {table_parts=}, {table_name=}, {table_schema=}, {table_database=}\")\n\n                formatted_metadata = (\n                    f\"{table_database}.{table_schema}.{table_name}: {table_metadata}\"\n                )\n                metadata_list.append(formatted_metadata)\n\n        return (\n            \";\\n\\n\\n\".join(metadata_list)\n            if len(metadata_list) > 1\n            else metadata_list[0]\n        )\n\n    def _run(\n        self,\n        table_names: str,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        mode: str = \"local\",\n    ) -> str:\n        # def _run(self, tool_input: str, mode: str = \"default\") -> str:\n        \"\"\"Get the metadata for tables in a comma-separated list.\n\n        mode:\n        - \"default\": use local index to get metadata, if not found, use snowflake to get metadata\n        - \"snowflake\": use snowflake to get metadata\n        - \"local\": use local index to get metadata\n\n        \"\"\"\n        logger.debug(f\"\\n Entering get metadata tool _run with {table_names=}, {mode=}\")\n\n        if mode not in [\"local\", \"snowflake\", \"default\"]:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        if mode == \"local\":\n            # use local index to get metadata\n            return self.db.metadata_parser.get_metadata_by_table_long_names(table_names)\n\n        if mode == \"snowflake\":\n            # use snowflake to get metadata\n            return self._get_metadata_from_snowflake(table_names)\n\n        if mode == \"default\":\n            try:\n                # use local index to get metadata\n                logger.debug(f\"{self.db.metadata_parser=}\")\n                result = self.db.metadata_parser.get_metadata_by_table_long_names(\n                    table_names\n                )\n                if result:\n                    return result\n                else:\n                    raise Exception(\"Not found in local index\")\n            except Exception:\n                # if not found in local index, use snowflake to get metadata\n                logger.warning(\n                    \"Metadata not found in local index, retrieving using snowflake\"\n                )\n                return self._get_metadata_from_snowflake(table_names)\n\n        return \"\"  # dummy return, just to make mypy happy", "\n\nclass QuerySnowflakeDatabaseTool(QuerySQLDataBaseTool):\n    \"\"\"Tool for querying a Snowflake database.\"\"\"\n\n    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\n    name = QUERY_SNOWFLAKE_DATABASE_TOOL_NAME\n    description = f\"\"\"\n    Never call this tool directly,\n    - always use the {GET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME} to get the metadata of the tables you want to query first, and make sure your query does NOT have any non-existent column names and other identifiers!\n    - always use the {SNOWFLAKE_QUERY_CHECKER_TOOL_NAME} to check if your query is correct before executing it!\n\n    Input to this tool is a database and a schema, along with a detailed and correct SQL query, output is a result from the database.\n    If the query is not correct, an error message will be returned.\n    If an error is returned, rewrite the query, check the query, and try again.\n\n    Example Input: 'database: database_name, schema: schema_name, query: SELECT * FROM table_name'\n    \"\"\"\n\n    def _process_tool_input(self, tool_input: Union[str, Dict]) -> Dict[str, str]:\n        logger.debug(f\"\\nEntering with {tool_input=}\")\n        if isinstance(tool_input, str):\n            pattern = r\"database: (.*?), schema: (.*?), query: (.*)\"\n            match = re.search(pattern, tool_input)\n\n            if match:\n                input_dict = {\n                    \"database\": match.group(1),\n                    \"schema\": match.group(2),\n                    \"query\": match.group(3),\n                }\n            else:\n                # # try to see if the input is a query only\n                # input_dict = {\n                #     \"database\": DEFAULT_DATABASE,\n                #     \"schema\": DEFAULT_SCHEMA,\n                #     \"query\": tool_input,\n                # }\n                # try to see if the input is a string of a dict\n                try:\n                    input_dict = json.loads(tool_input)\n                    if \"query\" in input_dict.keys():\n                        if \"database\" not in input_dict.keys():\n                            input_dict[\"database\"] = DEFAULT_DATABASE\n                        if \"schema\" not in input_dict.keys():\n                            input_dict[\"schema\"] = DEFAULT_SCHEMA\n                except json.JSONDecodeError:\n                    # try to see if the input is a query only\n                    input_dict = {\n                        \"database\": DEFAULT_DATABASE,\n                        \"schema\": DEFAULT_SCHEMA,\n                        \"query\": tool_input,\n                    }\n        elif isinstance(tool_input, dict):\n            full_keys = {\"database\", \"schema\", \"query\"}\n            if full_keys.issubset(tool_input.keys()):\n                input_dict = tool_input\n            elif \"query\" in tool_input.keys():\n                # try to see if the input is a query only\n                input_dict = {\n                    \"database\": DEFAULT_DATABASE,\n                    \"schema\": DEFAULT_SCHEMA,\n                    \"query\": tool_input.get(\"query\"),\n                }\n            # required_keys = {\"database\", \"schema\", \"query\"}\n            # if required_keys.issubset(tool_input.keys()):\n            #     input_dict = tool_input\n            else:\n                raise ValueError(\n                    f\"Invalid tool_input. Unable to parse 'query' from dictionary {tool_input=}\"\n                )\n        else:\n            raise ValueError(\"Invalid tool_input. Expected a string or a dictionary.\")\n        return input_dict\n\n    #    def _run(self, *args, **kwargs) -> str:\n    def _run(  # type: ignore\n        self,\n        *args,\n        mode=\"default\",\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        **kwargs,\n    ) -> Union[str, List[Any]]:\n        \"\"\"Execute the query, return the results or an error message.\"\"\"\n\n        if mode not in [\"shroomdk\", \"snowflake\", \"default\"]:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        if args:\n            tool_input = args[0]\n        else:\n            tool_input = kwargs\n\n        input_dict = self._process_tool_input(tool_input)\n\n        logger.debug(f\"\\nParsed input: {input_dict=}\")\n\n        # Retrieve values\n        database = input_dict[\"database\"]\n        schema = input_dict[\"schema\"]\n        query = input_dict[\"query\"]\n\n        if mode == \"snowflake\":\n            snowflake_database = self.db.get_database(database, schema)\n            result_snowflake = snowflake_database.run_no_throw(query)\n            assert isinstance(result_snowflake, str)\n            logger.debug(f\"snowflake {result_snowflake=}\")\n            return result_snowflake\n\n        if mode == \"shroomdk\":\n            result_set = self.db.shroomdk.query(query)\n            logger.debug(f\"shroomdk {result_set.rows=}\")\n            result_shroomdk: List[Any] = result_set.rows\n            return result_shroomdk\n\n        if mode == \"default\":\n            # try to use shroomdk first\n            try:\n                result_set = self.db.shroomdk.query(query)\n                logger.debug(\n                    f\"shroomdk result: {result_set=}, to return {result_set.rows=}\"\n                )\n                result_shroomdk = result_set.rows\n                return result_shroomdk\n            except Exception:\n                # if shroomdk fails, use snowflake\n                try:\n                    snowflake_database = self.db.get_database(database, schema)\n                    result_snowflake = snowflake_database.run_no_throw(query)\n                    assert isinstance(result_snowflake, str)\n                    return result_snowflake\n                except Exception:\n                    raise Exception(\n                        f\"Unable to execute query {query=} on {database=}.{schema=} via either shroomdk or snowflake.\"\n                    )\n\n        return \"\"  # dummy return, just to make mypy happy", "\n\nclass SnowflakeQueryCheckerTool(QueryCheckerTool):\n    \"\"\"Use an LLM to check if a query is correct.\n    Adapted from https://www.patterns.app/blog/2023/01/18/crunchbot-sql-analyst-gpt/\"\"\"\n\n    template: str = SNOWFLAKE_QUERY_CHECKER\n    llm: BaseChatModel\n    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n    name = SNOWFLAKE_QUERY_CHECKER_TOOL_NAME\n\n    description = f\"\"\"\n    Use this tool to double check if your snowflake query is correct before executing it.\n    Always use this tool before executing a query with {QUERY_SNOWFLAKE_DATABASE_TOOL_NAME}!\n    \"\"\"\n\n    @root_validator(pre=True)\n    def initialize_llm_chain(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        if \"llm_chain\" not in values:\n            input_variables = [\"query\", \"dialect\"]\n            messages = [\n                SystemMessagePromptTemplate.from_template(\n                    template=SNOWFLAKE_QUERY_CHECKER\n                ),\n                HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n            ]\n            assert isinstance(messages, list) and all(\n                isinstance(item, (BaseMessagePromptTemplate, BaseMessage))\n                for item in messages\n            ), \"Expected a list of BaseMessagePromptTemplate or BaseMessage\"\n\n            llm = values.get(\"llm\")\n            assert isinstance(llm, BaseLanguageModel)\n            values[\"llm_chain\"] = LLMChain(\n                llm=llm,\n                prompt=ChatPromptTemplate(\n                    input_variables=input_variables,\n                    messages=cast(\n                        List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n                    ),  # casting it for now\n                ),\n            )\n\n        if values[\"llm_chain\"].prompt.input_variables != [\"query\", \"dialect\"]:\n            raise ValueError(\n                \"LLM chain for QueryCheckerTool must have input variables ['query', 'dialect']\"\n            )\n\n        return values", ""]}
{"filename": "chatweb3/agents/__init__.py", "chunked_list": ["\"\"\"Interface for agents.\"\"\"\n\nfrom chatweb3.agents.agent_toolkits import (\n    create_snowflake_agent,\n    create_snowflake_chat_agent,\n    create_sql_chat_agent,\n)\n\n__all__ = [\"create_snowflake_agent\", \"create_snowflake_chat_agent\", \"create_sql_chat_agent\"]  # type: ignore\n", "__all__ = [\"create_snowflake_agent\", \"create_snowflake_chat_agent\", \"create_sql_chat_agent\"]  # type: ignore\n"]}
{"filename": "chatweb3/agents/conversational_chat/base.py", "chunked_list": ["\"\"\"Base class for snowflake conversational chat agents.\"\"\"\nfrom typing import Sequence, Optional, List\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.tools import BaseTool", ")\nfrom langchain.tools import BaseTool\nfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n    CONV_SNOWFLAKE_PREFIX,\n    CONV_SNOWFLAKE_SUFFIX,\n)\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.schema import BaseOutputParser\n\n\nclass SnowflakeConversationalChatAgent(ConversationalChatAgent):\n    @classmethod\n    def create_prompt(\n        cls,\n        tools: Sequence[BaseTool],\n        toolkit_instructions: Optional[str] = \"\",\n        system_message: str = CONV_SNOWFLAKE_PREFIX,\n        human_message: str = CONV_SNOWFLAKE_SUFFIX,\n        input_variables: Optional[List[str]] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        format_instructions: Optional[str] = None,\n    ) -> BasePromptTemplate:\n        \"\"\"Create a prompt template for the Snowflake conversational chat agent.\"\"\"\n        # tool descriptions\n        tool_strings = \"\\n\".join(\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n        )\n        if toolkit_instructions:\n            tool_strings += f\"\\n{toolkit_instructions}\"\n        # tool names\n        tool_names = \", \".join([tool.name for tool in tools])\n        # set up the output parser\n        _output_parser = output_parser or cls._get_default_output_parser()\n        human_message_final_prompt = human_message\n        system_message_final_prompt = system_message\n        # fill in the format instructions\n        if \"{format_instructions}\" in human_message:\n            human_message_format_instructions = human_message.format(\n                format_instructions=_output_parser.get_format_instructions()\n                if not format_instructions\n                else format_instructions\n            )\n            # fill in the tools\n            if \"{tools}\" in human_message:\n                human_message_final_prompt = human_message_format_instructions.format(\n                    tool_names=tool_names, tools=tool_strings\n                )\n            else:\n                raise ValueError(\n                    \"{format_instructions} in human message but {tools} are not, yet to be implemented\"\n                )\n        elif \"{format_instructions}\" in system_message:\n            system_message_format_instructions = system_message.format(\n                format_instructions=_output_parser.get_format_instructions()\n                if not format_instructions\n                else format_instructions\n            )\n            # fill in the tools\n            if \"{tools}\" in system_message:\n                system_message_final_prompt = system_message_format_instructions.format(\n                    tool_names=tool_names, tools=tool_strings\n                )\n            else:\n                raise ValueError(\n                    \"{format_instructions} in system message but {tools} are not, yet to be implemented\"\n                )\n        else:\n            Warning(\"Format_instructions not found in system_message or human_message\")\n\n        if input_variables is None:\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n        messages = [\n            SystemMessagePromptTemplate.from_template(system_message_final_prompt),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            HumanMessagePromptTemplate.from_template(human_message_final_prompt),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)", "\n\nclass SnowflakeConversationalChatAgent(ConversationalChatAgent):\n    @classmethod\n    def create_prompt(\n        cls,\n        tools: Sequence[BaseTool],\n        toolkit_instructions: Optional[str] = \"\",\n        system_message: str = CONV_SNOWFLAKE_PREFIX,\n        human_message: str = CONV_SNOWFLAKE_SUFFIX,\n        input_variables: Optional[List[str]] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        format_instructions: Optional[str] = None,\n    ) -> BasePromptTemplate:\n        \"\"\"Create a prompt template for the Snowflake conversational chat agent.\"\"\"\n        # tool descriptions\n        tool_strings = \"\\n\".join(\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n        )\n        if toolkit_instructions:\n            tool_strings += f\"\\n{toolkit_instructions}\"\n        # tool names\n        tool_names = \", \".join([tool.name for tool in tools])\n        # set up the output parser\n        _output_parser = output_parser or cls._get_default_output_parser()\n        human_message_final_prompt = human_message\n        system_message_final_prompt = system_message\n        # fill in the format instructions\n        if \"{format_instructions}\" in human_message:\n            human_message_format_instructions = human_message.format(\n                format_instructions=_output_parser.get_format_instructions()\n                if not format_instructions\n                else format_instructions\n            )\n            # fill in the tools\n            if \"{tools}\" in human_message:\n                human_message_final_prompt = human_message_format_instructions.format(\n                    tool_names=tool_names, tools=tool_strings\n                )\n            else:\n                raise ValueError(\n                    \"{format_instructions} in human message but {tools} are not, yet to be implemented\"\n                )\n        elif \"{format_instructions}\" in system_message:\n            system_message_format_instructions = system_message.format(\n                format_instructions=_output_parser.get_format_instructions()\n                if not format_instructions\n                else format_instructions\n            )\n            # fill in the tools\n            if \"{tools}\" in system_message:\n                system_message_final_prompt = system_message_format_instructions.format(\n                    tool_names=tool_names, tools=tool_strings\n                )\n            else:\n                raise ValueError(\n                    \"{format_instructions} in system message but {tools} are not, yet to be implemented\"\n                )\n        else:\n            Warning(\"Format_instructions not found in system_message or human_message\")\n\n        if input_variables is None:\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n        messages = [\n            SystemMessagePromptTemplate.from_template(system_message_final_prompt),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            HumanMessagePromptTemplate.from_template(human_message_final_prompt),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)", ""]}
{"filename": "chatweb3/agents/chat/base.py", "chunked_list": ["\"\"\"Base class for snowflake chat agents.\"\"\"\nfrom typing import List, Optional, Sequence, Union, cast\n\nfrom langchain.agents.chat.base import ChatAgent\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    BaseMessagePromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,", "    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import BaseMessage\nfrom langchain.tools import BaseTool\n\nfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n    SNOWFLAKE_PREFIX,\n    SNOWFLAKE_SUFFIX,", "    SNOWFLAKE_PREFIX,\n    SNOWFLAKE_SUFFIX,\n)\n\n\nclass SnowflakeChatAgent(ChatAgent):\n    @classmethod\n    def create_prompt(  # type: ignore[override]\n        cls,\n        tools: Sequence[BaseTool],\n        prefix: str = SNOWFLAKE_PREFIX,\n        suffix: str = SNOWFLAKE_SUFFIX,\n        format_instructions: str = FORMAT_INSTRUCTIONS,\n        toolkit_instructions: Optional[str] = \"\",\n        input_variables: Optional[List[str]] = None,\n        system_template: Optional[str] = None,\n        human_template: Optional[str] = None,\n        # **kwargs: Any,\n    ) -> BasePromptTemplate:\n        \"\"\"Create a prompt template for the Snowflake chat agent.\"\"\"\n        tool_strings = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n        if toolkit_instructions:\n            tool_strings += f\"\\n{toolkit_instructions}\"\n        tool_names = \", \".join([tool.name for tool in tools])\n        format_instructions = format_instructions.format(tool_names=tool_names)\n        if system_template is None:\n            system_template = \"\\n\\n\".join(\n                [prefix, tool_strings, format_instructions, suffix]\n            )\n        if human_template is None:\n            human_template = \"Question: {input}\\n\\n{agent_scratchpad}\"\n        messages = [\n            SystemMessagePromptTemplate.from_template(system_template),\n            HumanMessagePromptTemplate.from_template(human_template),\n        ]\n        if input_variables is None:\n            input_variables = [\"input\", \"agent_scratchpad\"]\n        return ChatPromptTemplate(\n            input_variables=input_variables,\n            messages=cast(\n                List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n            ),\n        )", ""]}
{"filename": "chatweb3/agents/chat/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/agents/chat/prompt.py", "chunked_list": ["# flake8: noqa\nCUSTOM_FORMAT_INSTRUCTIONS = \"\"\"The way you use the tools is by specifying a json blob.\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n\nThe ONLY values that should be in the 'action' field are: {tool_names}\n\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\n```\n{{{{", "```\n{{{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}}}\n```\n\nSince all the tools you have access to requires this $JSON_BLOB format, you MUST ALWAYS return the above $JSON_BLOB for any follow-up tools. \n\nMeanwhile, ALWAYS use the following format:", "\nMeanwhile, ALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction:\n```\n$JSON_BLOB\n```\nObservation: the result of the action", "```\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\"\"\"\n"]}
{"filename": "chatweb3/agents/agent_toolkits/__init__.py", "chunked_list": ["\"\"\"Agent toolkits.\"\"\"\n\nfrom chatweb3.agents.agent_toolkits.snowflake.base import (\n    create_snowflake_agent,\n    create_snowflake_chat_agent,\n    create_sql_chat_agent,\n)\nfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import (\n    SnowflakeDatabaseToolkit,\n)", "    SnowflakeDatabaseToolkit,\n)\n\n__all__ = [\n    \"create_sql_chat_agent\",\n    \"create_snowflake_agent\",\n    \"create_snowflake_chat_agent\",\n    \"SnowflakeDatabaseToolkit\",\n]\n", "]\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/base.py", "chunked_list": ["\"\"\"Snowflake agent that uses a Chat model\"\"\"\nfrom typing import Any, List, Optional, Sequence\n\nfrom langchain.agents.agent import AgentExecutor, AgentOutputParser\nfrom langchain.agents.agent_toolkits.sql.base import create_sql_agent\nfrom langchain.agents.agent_toolkits.sql.prompt import SQL_PREFIX, SQL_SUFFIX\nfrom langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\nfrom langchain.agents.chat.base import ChatAgent\nfrom langchain.agents.chat.output_parser import ChatOutputParser\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS", "from langchain.agents.chat.output_parser import ChatOutputParser\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.llms.base import BaseLLM\nfrom langchain.schema import BaseMemory\nfrom langchain.tools import BaseTool\n\nfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (", "\nfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n    CONV_SNOWFLAKE_PREFIX, CONV_SNOWFLAKE_SUFFIX, SNOWFLAKE_PREFIX,\n    SNOWFLAKE_SUFFIX)\nfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import \\\n    SnowflakeDatabaseToolkit\nfrom chatweb3.agents.agent_toolkits.snowflake.toolkit_custom import \\\n    CustomSnowflakeDatabaseToolkit\nfrom chatweb3.agents.chat.base import SnowflakeChatAgent\nfrom chatweb3.agents.conversational_chat.base import \\", "from chatweb3.agents.chat.base import SnowflakeChatAgent\nfrom chatweb3.agents.conversational_chat.base import \\\n    SnowflakeConversationalChatAgent\n\n\ndef create_snowflake_conversational_chat_agent(\n    # for llm chain of agent\n    llm: BaseChatModel,\n    # for prompt of llm chain\n    prefix: str = CONV_SNOWFLAKE_PREFIX,\n    suffix: str = CONV_SNOWFLAKE_SUFFIX,\n    format_instructions: Optional[str] = None,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    # system_template: Optional[str] = None,\n    # human_template: Optional[str] = None,\n    # shared by agent and agent executor chain\n    tools: Optional[Sequence[BaseTool]] = None,\n    toolkit: Optional[SnowflakeDatabaseToolkit] = None,\n    # for agent\n    output_parser: Optional[AgentOutputParser] = None,\n    # for agent executor\n    max_iterations: Optional[int] = 10,\n    max_execution_time: Optional[float] = 20,\n    early_stopping_method: Optional[str] = \"force\",\n    return_intermediate_steps: Optional[bool] = False,\n    # for chains\n    callbacks: Optional[BaseCallbackManager] = None,\n    verbose: Optional[bool] = False,\n    memory: Optional[BaseMemory] = None,\n    # additional kwargs\n    toolkit_kwargs: Optional[dict] = None,\n    prompt_kwargs: Optional[dict] = None,\n    agent_executor_kwargs: Optional[dict] = None,\n    agent_kwargs: Optional[dict] = None,\n    llm_chain_kwargs: Optional[dict] = None,\n    # **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"\n    Construct a sql chat agent from an LLM and tools.\n    \"\"\"\n    # get tools\n    # if tools is None then get tools from toolkit\n    if tools is None:\n        if toolkit is None:\n            raise ValueError(\"Must provide either tools or toolkit\")\n        else:\n            toolkit_kwargs = toolkit_kwargs or {}\n            tools = toolkit.get_tools(\n                callbacks=callbacks, verbose=verbose, **toolkit_kwargs\n            )\n    else:\n        if toolkit is not None:\n            raise ValueError(\"Cannot provide both tools and toolkit, choose one\")\n    # output parser\n    output_parser = (\n        output_parser or SnowflakeConversationalChatAgent._get_default_output_parser()\n    )\n    # prompt for llm chain\n    prompt_kwargs = prompt_kwargs or {}\n\n    if toolkit is not None:\n        prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n        toolkit_instructions = toolkit.instructions\n    else:\n        toolkit_instructions = None\n\n    prompt = SnowflakeConversationalChatAgent.create_prompt(\n        tools,\n        toolkit_instructions=toolkit_instructions,\n        system_message=prefix,\n        human_message=suffix,\n        input_variables=input_variables,\n        output_parser=output_parser,\n        # prefix=prefix,\n        # suffix=suffix,\n        format_instructions=format_instructions if format_instructions else None\n        # system_template=system_template,\n        # human_template=human_template,\n        ** prompt_kwargs,\n    )\n    # llm chain\n    llm_chain_kwargs = llm_chain_kwargs or {}\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        # memory=memory,\n        callbacks=callbacks,\n        verbose=verbose,\n        **llm_chain_kwargs,\n    )\n    # agent\n    agent_kwargs = agent_kwargs or {}\n    tool_names = [tool.name for tool in tools]\n    agent = SnowflakeConversationalChatAgent(\n        llm_chain=llm_chain,\n        allowed_tools=tool_names,\n        output_parser=output_parser,\n        **agent_kwargs,\n        # **kwargs,\n    )\n    # agent executor\n    agent_executor_kwargs = agent_executor_kwargs or {}\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        return_intermediate_steps=return_intermediate_steps,\n        memory=memory,\n        callbacks=callbacks,\n        verbose=verbose,\n        **agent_executor_kwargs,\n    )", "\n\ndef create_snowflake_chat_agent(\n    # shared by agent and agent executor chain\n    toolkit: SnowflakeDatabaseToolkit,\n    # for llm chain of agent\n    llm: BaseChatModel,\n    # for prompt of llm chain\n    prefix: str = SNOWFLAKE_PREFIX,\n    suffix: str = SNOWFLAKE_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    system_template: Optional[str] = None,\n    human_template: Optional[str] = None,\n    # for agent\n    output_parser: AgentOutputParser = ChatOutputParser(),\n    # for agent executor\n    max_iterations: Optional[int] = 10,\n    max_execution_time: Optional[float] = 20,\n    early_stopping_method: Optional[str] = \"force\",\n    return_intermediate_steps: Optional[bool] = False,\n    # for chains\n    callbacks: Optional[BaseCallbackManager] = None,\n    verbose: bool = False,\n    memory: Optional[BaseMemory] = None,\n    # additional kwargs\n    toolkit_kwargs: Optional[dict] = None,\n    prompt_kwargs: Optional[dict] = None,\n    agent_executor_kwargs: Optional[dict] = None,\n    agent_kwargs: Optional[dict] = None,\n    llm_chain_kwargs: Optional[dict] = None,\n    # **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"\n    Construct a sql chat agent from an LLM and tools.\n    \"\"\"\n    # tools from toolkit\n    toolkit_kwargs = toolkit_kwargs or {}\n    tools = toolkit.get_tools(callbacks=callbacks, verbose=verbose, **toolkit_kwargs)\n    # prompt for llm chain\n    prompt_kwargs = prompt_kwargs or {}\n    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n    if isinstance(toolkit, CustomSnowflakeDatabaseToolkit):\n        instructions = toolkit.instructions\n    else:\n        instructions = \"\"\n    prompt = SnowflakeChatAgent.create_prompt(\n        tools,\n        toolkit_instructions=instructions,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n        system_template=system_template,\n        human_template=human_template,\n        **prompt_kwargs,\n    )\n    # llm chain\n    llm_chain_kwargs = llm_chain_kwargs or {}\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        memory=memory,\n        callbacks=callbacks,\n        verbose=verbose,\n        **llm_chain_kwargs,\n    )\n    # agent\n    agent_kwargs = agent_kwargs or {}\n    tool_names = [tool.name for tool in tools]\n    agent = SnowflakeChatAgent(\n        llm_chain=llm_chain,\n        allowed_tools=tool_names,\n        output_parser=output_parser,\n        **agent_kwargs,\n        # **kwargs,\n    )\n    # agent executor\n    agent_executor_kwargs = agent_executor_kwargs or {}\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        return_intermediate_steps=return_intermediate_steps,\n        memory=memory,\n        callbacks=callbacks,\n        verbose=verbose,\n        **agent_executor_kwargs,\n    )", "\n\ndef create_snowflake_agent(\n    llm: BaseLLM,\n    toolkit: SnowflakeDatabaseToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = SNOWFLAKE_PREFIX,\n    suffix: str = SNOWFLAKE_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    max_iterations: Optional[int] = 15,\n    max_execution_time: Optional[float] = None,\n    early_stopping_method: str = \"force\",\n    verbose: bool = False,\n    **kwargs: Any,\n) -> AgentExecutor:\n    return create_sql_agent(\n        llm=llm,\n        toolkit=toolkit,\n        callback_manager=callback_manager,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n        top_k=top_k,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        verbose=verbose,\n        **kwargs,\n    )", "\n\ndef create_sql_chat_agent(\n    # shared by agent and agent executor chain\n    toolkit: SQLDatabaseToolkit,\n    # for llm chain of agent\n    llm: BaseChatModel,\n    # for prompt of llm chain\n    prefix: str = SQL_PREFIX,\n    suffix: str = SQL_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    # for agent\n    output_parser: AgentOutputParser = ChatOutputParser(),\n    # for agent executor\n    max_iterations: Optional[int] = 10,\n    max_execution_time: Optional[float] = 20,\n    early_stopping_method: Optional[str] = \"force\",\n    return_intermeidate_steps: Optional[bool] = False,\n    # for chains\n    callback_manager: Optional[BaseCallbackManager] = None,\n    verbose: bool = False,\n    # memory: Optional[BaseMemory] = None,\n    # additional kwargs\n    # toolkit_kwargs: Optional[dict] = None,\n    # prompt_kwargs: Optional[dict] = None,\n    # agent_executor_kwargs: Optional[dict] = None,\n    # agent_kwargs: Optional[dict] = None,\n    # llm_chain_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"\n    Construct a sql chat agent from an LLM and tools.\n    \"\"\"\n    tools = toolkit.get_tools()\n    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n    prompt = ChatAgent.create_prompt(\n        tools,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n        #    **prompt_kwargs,\n    )\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n        verbose=verbose,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ChatAgent(\n        llm_chain=llm_chain,\n        allowed_tools=tool_names,\n        output_parser=output_parser,\n    )\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        return_intermeidate_steps=return_intermeidate_steps,\n        callback_manager=callback_manager,\n        verbose=verbose,\n    )", ""]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/toolkit.py", "chunked_list": ["\"\"\"Toolkit for interacting with Snowflake databases.\"\"\"\n\nfrom typing import List, Optional, Union, cast\n\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.memory.readonly import ReadOnlySharedMemory", "from langchain.chat_models.base import BaseChatModel\nfrom langchain.memory.readonly import ReadOnlySharedMemory\nfrom langchain.prompts.chat import (\n    BaseMessagePromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import BaseMessage\nfrom langchain.tools import BaseTool", "from langchain.schema import BaseMessage\nfrom langchain.tools import BaseTool\nfrom pydantic import Field\n\nfrom chatweb3.snowflake_database import SnowflakeContainer\nfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\nfrom chatweb3.tools.snowflake_database.tool import (\n    GetSnowflakeDatabaseTableMetadataTool,\n    ListSnowflakeDatabaseTableNamesTool,\n    QuerySnowflakeDatabaseTool,", "    ListSnowflakeDatabaseTableNamesTool,\n    QuerySnowflakeDatabaseTool,\n    SnowflakeQueryCheckerTool,\n)\n\n\nclass SnowflakeDatabaseToolkit(SQLDatabaseToolkit):\n    \"\"\"Snowflake database toolkit.\"\"\"\n\n    # override the db attribute to be a SnowflakeContainer\n    # it contains a dictionary of SnowflakeDatabase objects\n    db: SnowflakeContainer = Field(exclude=True)  # type: ignore[assignment]\n    # override the llm attribute to be a ChatOpenAI object\n    llm: BaseChatModel = Field(default_factory=lambda: ChatOpenAI(temperature=0))  # type: ignore[call-arg]\n    readonly_shared_memory: ReadOnlySharedMemory = Field(default=None)\n\n    def get_tools(\n        self,\n        callback_manager: Optional[BaseCallbackManager] = None,\n        verbose: bool = False,\n        **kwargs,\n    ) -> List[BaseTool]:\n        \"\"\"Get the tools available in the toolkit.\n\n        Returns:\n            The tools available in the toolkit.\n        \"\"\"\n\n        # if input_variables is None:\n        messages = [\n            SystemMessagePromptTemplate.from_template(template=SNOWFLAKE_QUERY_CHECKER),\n            HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n        ]\n        input_variables = [\"query\", \"dialect\"]\n        checker_llm_chain = LLMChain(\n            llm=self.llm,\n            prompt=ChatPromptTemplate(\n                input_variables=input_variables,\n                messages=cast(\n                    List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n                ),  # casting it for now XXX\n            ),\n            callback_manager=callback_manager,\n            memory=self.readonly_shared_memory,\n            verbose=verbose,\n        )\n\n        return [\n            QuerySnowflakeDatabaseTool(db=self.db),  # type: ignore[arg-type]\n            ListSnowflakeDatabaseTableNamesTool(db=self.db),  # type: ignore[arg-type]\n            GetSnowflakeDatabaseTableMetadataTool(db=self.db),  # type: ignore[arg-type]\n            SnowflakeQueryCheckerTool(\n                db=self.db,  # type: ignore[arg-type]\n                template=SNOWFLAKE_QUERY_CHECKER,\n                llm=self.llm,\n                llm_chain=checker_llm_chain,\n                callback_manager=callback_manager,\n                verbose=verbose,\n            ),  # type: ignore[call-arg]\n        ]", ""]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/prompt.py", "chunked_list": ["# flake8: noqa\n\n# these prompts have been tuned for the chat model output parser\n\nSNOWFLAKE_PREFIX = \"\"\"You are an agent designed to interact with Snowflake databases.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\nYou have access to tools for interacting with the database.", "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\nIf the question does not seem related to the database, just return \"Sorry I don't know ...\" as the answer.\n\"\"\"\n", "\"\"\"\n\nSNOWFLAKE_SUFFIX = \"\"\"\nYou MUST ALWAYS first find out what database(s), schema(s) and table(s) are available before you take any other actions such as retrieving metadata about tables or creating queries for tables. \nYou MUST ALWAYS first confirm that a table exists in the database AND then retrieve and examined the table's metadata BEFORE you can create a query for that table and submit to the query checker.\n\nBegin!\n\n\"\"\"\n", "\"\"\"\n\nCUSTOM_SNOWFLAKE_PREFIX = \"\"\"You are an agent designed to interact with Snowflake databases.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\nYou have access to specific tools for interacting with the database.\nYou must ONLY use these specified tools. Only use the information returned by the below tools to construct your final answer.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. You can use `date_trunc` to group dates in the queries when you need to.", "\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. You can use `date_trunc` to group dates in the queries when you need to.\n\nIf the question does not seem related to the database, just return 'I don't know' as the answer.\n\"\"\"\n\nCUSTOM_SNOWFLAKE_SUFFIX = \"\"\"\nFinally, remember to use concise responses so you have space to include the action and action inputs in the response whenever needed.  \n\nBegin!", "\nBegin!\n\n\"\"\"\nCONV_SNOWFLAKE_PREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n", "Assistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n\nWhen generating {dialect} queries: \n1. Unless the human specifies a specific number of examples they wish to obtain, assistant will always limit its query to at most {top_k} results. \n2. Assistant's query may order the results by a relevant column to return the most interesting examples in the database. \n3. Assistant will never query for all the columns from a specific table, only ask for the relevant columns given the question. \n4. Assistant must ONLY use tools specified by the users and MUST follow the tool usage instructions provided by human.\n5. Assistant MUST NOT generate any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \n6. Assistant may use `date_trunc` to group dates in the queries when you need to. \n", "6. Assistant may use `date_trunc` to group dates in the queries when you need to. \n\nAssistant MUST ONLY generate {dialect} queries based on database metadata information as human provided. If it is not possible to generate {dialect} queries based on the provided database metadata information, assistant can just return 'I don't know' as the answer.\n\"\"\"\n\nCONV_SNOWFLAKE_SUFFIX = \"\"\"TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{{tools}}", "\n{{tools}}\n\n{format_instructions}\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{{{input}}}}", "\n{{{{input}}}}\n\"\"\"\n\nCUSTOM_CONV_SNOWFLAKE_PREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n", "Assistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n\nWhen generating {dialect} queries: \n- Unless the human specifies a specific number of examples they wish to obtain, assistant will always limit its query to at most {top_k} results. \n- Assistant's query may order the results by a relevant column to return the most interesting examples in the database. \n- Assistant MUST NOT generate any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \n- Assistant may use `date_trunc` to group dates in the queries when you need to. \n\nAssistant can ask the human to use the following database related tools to help generate {dialect} queries to answer the human's original question. \n", "Assistant can ask the human to use the following database related tools to help generate {dialect} queries to answer the human's original question. \n\n{{{{tools}}}}\n\nIMPORTANT: \n1. Assistant must ALWAYS check available tables first! That is, NEVER EVER start with checking metadata tools or query database tools, ALWAYS start with the tool that tells you what tables are available in the database. \n2. Before generating ANY {dialect} query, assistant MUST first check the metadata of the table the query will be run against. NEVER EVER generate a {dialect} query without checking the metadata of the table first.\n3. If the assistant checked the tables in the database and found no table is related to the the human's specific question, assistant MUST NOT generate any {dialect} queries, and MUST respond 'I don't know' as the answer, and ask the human to provide more information.\n\n------", "\n------\n\n{{format_instructions}}\n\n\"\"\"\n\nCUSTOM_FORMAT_INSTRUCTIONS = \"\"\"RESPONSE FORMAT INSTRUCTIONS\n----------------------------\n", "----------------------------\n\nWhen responding to me, please output a response in one of two formats:\n\n**Option 1:**\nUse this if you want the human to use a tool.\nMarkdown code snippet formatted in the following schema:\n\n```json\n{{{{", "```json\n{{{{\n    \"action\": string \\\\ The action to take. Must be one of {tool_names}\n    \"action_input\": string \\\\ The input to the action\n}}}}\n```\nBoth `action` and `action_input` MUST be provided, even if  `action_input` is an empty string!\n\n**Option #2:**\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:", "**Option #2:**\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n\n```json\n{{{{\n    \"action\": \"Final Answer\",\n    \"action_input\": string \\\\ You should put what you want to return to use here\n}}}}\n```\"\"\"\n", "```\"\"\"\n\nCUSTOM_CONV_SNOWFLAKE_SUFFIX = \"\"\"\n------\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{input}", "\n{input}\n\"\"\"\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/toolkit_custom.py", "chunked_list": ["from typing import List, Optional, Union, cast\n\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.chat import (\n    BaseMessagePromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)", "    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import BaseMessage\nfrom langchain.tools import BaseTool\nfrom pydantic import Field\n\nfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import (\n    SnowflakeDatabaseToolkit,\n)\nfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER", ")\nfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\nfrom chatweb3.tools.snowflake_database.tool_custom import (\n    TOOLKIT_INSTRUCTIONS,\n    CheckQuerySyntaxTool,\n    CheckTableMetadataTool,\n    CheckTableSummaryTool,\n    QueryDatabaseTool,\n)\nfrom config.config import agent_config", ")\nfrom config.config import agent_config\n\n\nclass CustomSnowflakeDatabaseToolkit(SnowflakeDatabaseToolkit):\n    \"\"\"Toolkit for interacting with FPS databases.\"\"\"\n\n    instructions = Field(default=TOOLKIT_INSTRUCTIONS)\n\n    def get_tools(\n        self,\n        callback_manager: Optional[BaseCallbackManager] = None,\n        verbose: bool = False,\n        # input_variables: Optional[List[str]] = None,\n        **kwargs,\n    ) -> List[BaseTool]:\n        \"\"\"Get the tools available in the toolkit.\n        Returns:\n            The tools available in the toolkit.\n        \"\"\"\n\n        # if input_variables is None:\n        input_variables = [\"query\", \"dialect\"]\n        messages = [\n            SystemMessagePromptTemplate.from_template(template=SNOWFLAKE_QUERY_CHECKER),\n            HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n        ]\n        checker_llm_chain = LLMChain(\n            llm=self.llm,\n            prompt=ChatPromptTemplate(\n                input_variables=input_variables,\n                messages=cast(\n                    List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n                ),\n                # messages=messages\n            ),\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n\n        return [\n            CheckTableSummaryTool(\n                db=self.db, callback_manager=callback_manager, verbose=verbose  # type: ignore[call-arg, arg-type]\n            ),\n            CheckTableMetadataTool(\n                db=self.db, callback_manager=callback_manager, verbose=verbose  # type: ignore[call-arg, arg-type]\n            ),\n            CheckQuerySyntaxTool(  # type: ignore[call-arg]\n                db=self.db,  # type: ignore[arg-type]\n                template=SNOWFLAKE_QUERY_CHECKER,\n                llm=self.llm,\n                llm_chain=checker_llm_chain,\n                callback_manager=callback_manager,\n                verbose=verbose,\n            ),\n            QueryDatabaseTool(  # type: ignore[call-arg]\n                db=self.db,  # type: ignore[arg-type]\n                return_direct=agent_config.get(\n                    \"tool.query_database_tool_return_direct\"\n                ),\n                callback_manager=callback_manager,\n                verbose=verbose,\n            ),\n        ]", ""]}
{"filename": "chatweb3/callbacks/__init__.py", "chunked_list": [""]}
{"filename": "chatweb3/callbacks/logger_callback.py", "chunked_list": ["\"\"\"Callback Handler that logs debugging information\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult\n\nfrom config.logging_config import get_logger\n", "from config.logging_config import get_logger\n\nlogger = get_logger(\n    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n)\n\n\nclass LoggerCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback Handler that prints to std out.\"\"\"\n\n    def __init__(self, color: Optional[str] = None) -> None:\n        \"\"\"Initialize callback handler.\"\"\"\n        self.color = color\n\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out the prompts.\"\"\"\n        class_name = serialized[\"name\"]\n        logger.debug(f\"Starting lLM: {class_name} with prompts: {prompts}\")\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        \"\"\"Print out the response.\"\"\"\n        logger.debug(f\"LLM response: {response}\")\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        \"\"\"Print out new token.\"\"\"\n        logger.debug(f\"LLM new token: {token}\")\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out LLM error.\"\"\"\n        logger.debug(f\"LLM error: {error}\")\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out that we are entering a chain.\"\"\"\n        class_name = serialized[\"name\"]\n        logger.debug(\n            f\"\\n\\n\\033[1m> Entering new {class_name} chain\\033[0m with inputs: {inputs}\"\n        )\n        # print(f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\")\n\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Print out that we finished a chain.\"\"\"\n        logger.debug(f\"\\n\\033[1m> Finished chain.\\033[0m with outputs: {outputs}\")\n        # print(\"\\n\\033[1m> Finished chain.\\033[0m\")\n\n    def on_chain_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out chain error\"\"\"\n        logger.debug(f\"Chain error: {error}\")\n\n    def on_tool_start(\n        self,\n        serialized: Dict[str, Any],\n        input_str: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Print out tool start.\"\"\"\n        # tool_name = serialized[\"name\"]\n        # tool_description = serialized[\"description\"]\n        logger.debug(f\"Starting tool: {serialized} with input: {input_str}\")\n\n    def on_agent_action(\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on agent action.\"\"\"\n        # print_text(action.log, color=color if color else self.color)\n        logger.debug(f\"Agent action: {action}\")\n\n    def on_tool_end(\n        self,\n        output: str,\n        color: Optional[str] = None,\n        observation_prefix: Optional[str] = None,\n        llm_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"If not the final action, print out observation.\"\"\"\n        if observation_prefix is not None:\n            # print_text(f\"\\n{observation_prefix}\")\n            logger.debug(f\"Not final action since {observation_prefix=} is not None\")\n        # print_text(output, color=color if color else self.color)\n        logger.debug(f\"Tool ended with {output=}\")\n        if llm_prefix is not None:\n            # print_text(f\"\\n{llm_prefix}\")\n            logger.debug(f\"{llm_prefix=} is not None\")\n\n    def on_tool_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out tool error.\"\"\"\n        logger.debug(f\"Tool error: {error}\")\n\n    def on_text(\n        self,\n        text: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when agent ends.\"\"\"\n        # print_text(text, color=color if color else self.color, end=end)\n        logger.debug(f\"on text: {text}\")\n\n    def on_agent_finish(\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n    ) -> None:\n        \"\"\"Run on agent end.\"\"\"\n        logger.debug(f\"Agent finished with {finish=}\")\n\n    def log_with_context(\n        self, msg: str, pathname: str, lineno: int, func_name: str\n    ) -> None:\n        extra = {\n            \"pathname\": pathname,\n            \"lineno\": lineno,\n            \"funcName\": func_name,\n        }\n        logger.debug(msg, extra=extra)", ""]}
{"filename": "config/logging_config.py", "chunked_list": ["\"\"\"\nlogging_config.py\nThis file contains the code for configuring the logger.\n\"\"\"\nimport inspect\nimport logging\nimport os\nimport sys\nfrom typing import Optional\n", "from typing import Optional\n\n\n# Define CustomLoggerAdapter class\nclass CustomLoggerAdapter(logging.LoggerAdapter):\n    def process(self, msg, kwargs):\n        class_name = \"\"\n        for frame_info in inspect.stack()[2:]:\n            frame = frame_info.frame\n            local_self = frame.f_locals.get(\"self\")\n\n            if local_self and not isinstance(local_self, CustomLoggerAdapter):\n                class_name = local_self.__class__.__name__\n                break\n\n        if class_name:\n            msg = f\"{class_name}: {msg}\"\n\n        return msg, kwargs", "\n\n# Helper function to configure logger handlers\ndef _configure_handlers(\n    logger,\n    log_level,\n    log_to_console,\n    log_to_file,\n    log_format,\n    date_format,\n    log_file_path,\n):\n    handlers = []\n\n    if log_to_console:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(log_level)\n        console_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n        handlers.append(console_handler)\n\n    if log_to_file:\n        file_handler = logging.FileHandler(log_file_path)\n        file_handler.setLevel(log_level)\n        file_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n        handlers.append(file_handler)\n\n    for handler in handlers:\n        logger.addHandler(handler)", "\n\ndef _get_log_file_path():\n    # Get the path of the current module\n    current_module_path = os.path.abspath(__file__)\n\n    # Get the path of the directory where the current module is located\n    config_directory = os.path.dirname(current_module_path)\n\n    # Get the path to the project root (assuming the config directory is located one level below the project root)\n    project_root = os.path.dirname(config_directory)\n\n    # Define the path for the log directory\n    log_directory = os.path.join(project_root, \"logs\")\n    os.makedirs(log_directory, exist_ok=True)\n\n    # Determine the project name from the project_root path\n    project_name = os.path.basename(project_root)\n\n    # Define the log file name and path\n    log_file_name = f\"{project_name}.log\"\n    log_file_path = os.path.join(log_directory, log_file_name)\n\n    return log_file_path", "\n\n# default parameters\nlog_level = logging.INFO\nlog_to_console = True\nlog_to_file = False\nlog_format = \"%(asctime)s [%(name)s] [%(levelname)s] [%(module)s:%(lineno)d] [%(funcName)s]: %(message)s\"\ndate_format = \"%Y-%m-%d %H:%M:%S\"\nlog_file_path = _get_log_file_path()\n", "log_file_path = _get_log_file_path()\n\n# Configure the root logger\nroot_logger = logging.getLogger()\n_configure_handlers(\n    root_logger,\n    log_level=log_level,\n    log_to_console=log_to_console,\n    log_to_file=log_to_file,\n    log_format=log_format,", "    log_to_file=log_to_file,\n    log_format=log_format,\n    date_format=date_format,\n    log_file_path=log_file_path,\n)\n\nroot_logger_adapter = CustomLoggerAdapter(root_logger, {})\n\n# Log a statement during the root logger adapter initialization\nroot_logger_adapter.info(", "# Log a statement during the root logger adapter initialization\nroot_logger_adapter.info(\n    f\"\"\"LOGGER initialized with the following options:\nLog level: {log_level}\nLog to console: {log_to_console}\nLog to file: {log_to_file}\nLog file path: {log_file_path}\n\"\"\"\n)\n", ")\n\n\n# Child loggers will inherit the attributes from the root logger unless they are explicitly overridden\ndef get_logger(\n    name: str,\n    log_level: Optional[int] = None,\n    log_to_console: Optional[bool] = None,\n    log_to_file: Optional[bool] = None,\n    log_format: Optional[str] = None,\n    date_format: Optional[str] = None,\n    log_file_path: Optional[str] = None,\n):\n    logger = logging.getLogger(name)\n\n    if logger.handlers:\n        return CustomLoggerAdapter(logger, {})\n\n    if log_level is not None:\n        logger.setLevel(log_level)\n    else:\n        logger.setLevel(logging.getLogger().level)  # Inherit log level from root logger\n\n    if log_to_file and log_file_path is None:\n        log_file_path = next(\n            (\n                handler.baseFilename\n                for handler in root_logger.handlers\n                if isinstance(handler, logging.FileHandler)\n            ),\n            None,\n        )  # Use root logger's log_file_path\n\n    if log_to_console is None:\n        log_to_console = any(\n            isinstance(handler, logging.StreamHandler)\n            for handler in root_logger.handlers\n        )\n\n    if log_to_file is None:\n        log_to_file = any(\n            isinstance(handler, logging.FileHandler) for handler in root_logger.handlers\n        )\n\n    if log_format is None:\n        if root_logger.handlers[0].formatter is not None:\n            log_format = root_logger.handlers[0].formatter._fmt\n\n    if date_format is None:\n        if root_logger.handlers[0].formatter is not None:\n            date_format = root_logger.handlers[0].formatter.datefmt\n\n    if log_to_file and log_file_path is None:\n        log_file_path = (\n            _get_log_file_path()\n        )  # Assign default log file path if not found in root_logger and log_to_file is True\n\n    _configure_handlers(\n        logger,\n        log_level=logger.level,\n        log_to_console=log_to_console,\n        log_to_file=log_to_file,\n        log_format=log_format,\n        date_format=date_format,\n        log_file_path=log_file_path,\n    )\n\n    # Add this line to prevent propagation of log messages to ancestor loggers\n    logger.propagate = False\n\n    logger_adapter = CustomLoggerAdapter(logger, {})\n    if logger.level == logging.DEBUG:\n        logger_adapter.info(\n            f\"\\n\\n\\n *** Initialized '{name}' LOGGER with level '{logging.getLevelName(logger.level)}'\"\n        )\n\n    return logger_adapter", ""]}
{"filename": "config/config.py", "chunked_list": ["\"\"\"\nconfig.py\nThis file contains the configuration for the chatbot application.\n\"\"\"\nimport os\n\nimport yaml\nfrom dotenv import load_dotenv\n\nload_dotenv()", "\nload_dotenv()\n\n\nclass Config:\n    def __init__(self, config_file):\n        with open(config_file, \"r\") as f:\n            self.config = yaml.safe_load(f)\n        self.config[\"proj_root_dir\"] = os.path.dirname(\n            os.path.dirname(os.path.abspath(__file__))\n        )\n        self.config[\"tool\"][\"query_database_tool_return_direct\"] = (\n            True if self.config[\"tool\"][\"query_database_tool_top_k\"] > 10 else False\n        )\n\n        # Load from environment variables\n        self.config[\"snowflake_params\"] = {\n            \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n            \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n            \"account_identifier\": os.getenv(\"SNOWFLAKE_ACCOUNT_IDENTIFIER\"),\n        }\n        self.config[\"shroomdk_params\"] = {\n            \"shroomdk_api_key\": os.getenv(\"SHROOMDK_API_KEY\"),\n        }\n\n    def get(self, path, default=None):\n        keys = path.split(\".\")\n        value = self.config\n        for key in keys:\n            if isinstance(value, dict):\n                value = value.get(key)\n            else:\n                return default\n        return value\n\n    def set(self, path, value):\n        keys = path.split(\".\")\n        current_level = self.config\n        for key in keys[:-1]:\n            current_level = current_level.setdefault(key, {})\n        current_level[keys[-1]] = value", "\n\nagent_config = Config(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), \"config.yaml\")\n)\n"]}
{"filename": "config/__init__.py", "chunked_list": [""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"\nconftest.py\nThis file contains the fixtures for the tests.\n\"\"\"\n\nimport pytest\n\nfrom chatweb3.metadata_parser import Column, Database, MetadataParser, Schema, Table\nfrom chatweb3.snowflake_database import SnowflakeContainer\nfrom config.config import agent_config", "from chatweb3.snowflake_database import SnowflakeContainer\nfrom config.config import agent_config\nfrom create_agent import INDEX_ANNOTATION_FILE_PATH, LOCAL_INDEX_FILE_PATH\n\n\n@pytest.fixture(scope=\"module\")\ndef snowflake_params():\n    return {\n        **agent_config.get(\"snowflake_params\"),\n        **agent_config.get(\"shroomdk_params\"),\n    }", "\n\n@pytest.fixture(scope=\"module\")\ndef snowflake_container(snowflake_params):\n    return SnowflakeContainer(**snowflake_params)\n\n\n@pytest.fixture(scope=\"module\")\ndef snowflake_container_eth_core(snowflake_params):\n    return SnowflakeContainer(\n        **snowflake_params,\n        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n    )", "def snowflake_container_eth_core(snowflake_params):\n    return SnowflakeContainer(\n        **snowflake_params,\n        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef eth_core_table_long_names_select_list():\n    return agent_config.get(\"ethereum_core_table_long_name.enabled_list\")", "@pytest.fixture(scope=\"module\")\ndef eth_core_table_long_names_select_list():\n    return agent_config.get(\"ethereum_core_table_long_name.enabled_list\")\n\n\n@pytest.fixture\ndef metadata_parser_with_sample_data():\n    # Create the MetadataParser object\n    metadata_parser = MetadataParser()\n\n    # Add sample data to the parser's custom objects\n    # Database 1: ethereum\n    ethereum_db = Database(\"ethereum\")\n    ethereum_core_schema = Schema(\"core\", \"ethereum\")\n    ethereum_aave_schema = Schema(\"aave\", \"ethereum\")\n\n    # ethereum.core.ez_nft_sales\n    ez_nft_sales = Table(\"ez_nft_sales\", \"core\", \"ethereum\")\n    ez_nft_sales.comment = \"The sales of NFTs.\"\n    ez_nft_sales.summary = \"Summary: This table contains the sales of NFTs.\"\n    columns1 = {\n        \"block_number\": Column(\n            \"block_number\",\n            \"NUMBER(38,0)\",\n            \"The block number at which the NFT event occurred.\",\n        ),\n        \"event_type\": Column(\n            \"event_type\",\n            \"VARCHAR(16777216)\",\n            \"The type of NFT event in this transaction, either sale`, `bid_won` or `redeem`.\",\n        ),\n    }\n    columns1[\"block_number\"].sample_values_list = [1, 2, 3]\n    columns1[\"event_type\"].sample_values_list = [\"sale\", \"bid_won\", \"redeem\"]\n    ez_nft_sales.columns = columns1\n    ez_nft_sales.column_names = [\"block_number\", \"event_type\"]\n\n    ethereum_core_schema.tables[\"ez_nft_sales\"] = ez_nft_sales\n    ethereum_db.schemas[\"core\"] = ethereum_core_schema\n\n    # ethereum.aave.ez_proposals\n    ez_proposals = Table(\"ez_proposals\", \"aave\", \"ethereum\")\n    ez_proposals.comment = \"Aave proposals.\"\n    ez_proposals.summary = \"Summary: This table contains Aave proposals.\"\n    columns2 = {\n        \"block_number\": Column(\n            \"block_number\",\n            \"NUMBER(38,0)\",\n            \"The block number at which the NFT event occurred.\",\n        ),\n    }\n    columns2[\"block_number\"].sample_values_list = [10, 11, 12]\n    ez_proposals.columns = columns2\n    ez_proposals.column_names = [\"block_number\"]\n\n    ethereum_aave_schema.tables[\"ez_proposals\"] = ez_proposals\n    ethereum_db.schemas[\"aave\"] = ethereum_aave_schema\n\n    metadata_parser.root_schema_obj.databases[\"ethereum\"] = ethereum_db\n\n    # Database 2: polygon\n    polygon_db = Database(\"polygon\")\n    polygon_core_schema = Schema(\"core\", \"polygon\")\n\n    # polygon.core.fact_blocks\n    fact_blocks = Table(\"fact_blocks\", \"core\", \"polygon\")\n    fact_blocks.comment = \"The fact blocks on Polygon.\"\n    fact_blocks.summary = \"Summary: This table contains the fact blocks on Polygon.\"\n    columns3 = {\n        \"difficulty\": Column(\n            \"difficulty\", \"NUMBER(38,0)\", \"The effort required to mine the block\"\n        ),\n    }\n    columns3[\"difficulty\"].sample_values_list = [6, 7, 8]\n    fact_blocks.columns = columns3\n    fact_blocks.column_names = [\"difficulty\"]\n\n    polygon_core_schema.tables[\"fact_blocks\"] = fact_blocks\n    polygon_db.schemas[\"core\"] = polygon_core_schema\n\n    metadata_parser.root_schema_obj.databases[\"polygon\"] = polygon_db\n\n    return metadata_parser", ""]}
{"filename": "tests/integration_tests/test_metadata_parser.py", "chunked_list": ["\"\"\"\ntest_metadata_parser.py\nThis file contains the tests for the metadata_parser module.\n\"\"\"\n\n\ndef test_get_metadata_by_table_long_names(metadata_parser_with_sample_data):\n    table_long_names = \"ethereum.core.ez_nft_sales, ethereum.aave.ez_proposals, polygon.core.fact_blocks\"\n    expected_output = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. \\nComment: Aave proposals.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tblock_number | None | None | 10, 11, 12\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. \\nComment: The sales of NFTs.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tblock_number | None | None | 1, 2, 3\\n\\tevent_type | None | None | sale, bid_won, redeem\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. \\nComment: The fact blocks on Polygon.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tdifficulty | None | None | 6, 7, 8\"\n    result = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n        table_long_names\n    )\n    assert result == expected_output\n    assert \"ethereum.aave.ez_proposals\" in result\n    assert \"ethereum.core.ez_nft_sales\" in result\n    assert \"polygon.core.fact_blocks\" in result\n    assert \"Aave proposals\" in result\n    assert \"The sales of NFTs\" in result\n    assert \"The fact blocks on Polygon\" in result\n    assert \"block_number\" in result\n    assert \"event_type\" in result\n    assert \"difficulty\" in result", "\n\ndef test_get_metadata_by_table_long_names_table_summary(\n    metadata_parser_with_sample_data,\n):\n    table_long_names = \"ethereum.core.ez_nft_sales, ethereum.aave.ez_proposals, polygon.core.fact_blocks\"\n    result1 = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n        table_long_names, include_column_info=False\n    )\n\n    result2 = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n        table_long_names, include_column_names=True, include_column_info=False\n    )\n\n    expected_result_1 = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. Summary: This table contains Aave proposals.\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. Summary: This table contains the sales of NFTs.\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. Summary: This table contains the fact blocks on Polygon.\"\n    expected_result_2 = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. Summary: This table contains Aave proposals.This table has the following columns: 'block_number'\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. Summary: This table contains the sales of NFTs.This table has the following columns: 'block_number, event_type'\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. Summary: This table contains the fact blocks on Polygon.This table has the following columns: 'difficulty'\"\n    assert result1 == expected_result_1\n    assert result2 == expected_result_2", ""]}
{"filename": "tests/integration_tests/test_snowflake_database.py", "chunked_list": ["\"\"\"\ntest_snowflake_database.py\nThis file contains the tests for the snowflake_database module.\n\"\"\"\nimport logging\n\nimport pytest\n\nfrom chatweb3.snowflake_database import SnowflakeContainer\nfrom chatweb3.tools.snowflake_database.tool import (", "from chatweb3.snowflake_database import SnowflakeContainer\nfrom chatweb3.tools.snowflake_database.tool import (\n    GetSnowflakeDatabaseTableMetadataTool,\n    ListSnowflakeDatabaseTableNamesTool,\n    QuerySnowflakeDatabaseTool,\n)\n\nlogging.getLogger(\"sqlalchemy.engine\").setLevel(logging.ERROR)\nlogging.getLogger(\"snowflake.connector\").setLevel(logging.ERROR)\n", "logging.getLogger(\"snowflake.connector\").setLevel(logging.ERROR)\n\n\ndef test_snowflake_container_initialization_shroomdk(snowflake_params):\n    container = SnowflakeContainer(**snowflake_params)\n    assert container.shroomdk is not None\n    assert container.metadata_parser is not None\n\n\ndef test_list_snowflake_database_table_name_tool_run_local(\n    snowflake_container_eth_core, eth_core_table_long_names_select_list\n):\n    tool = ListSnowflakeDatabaseTableNamesTool(db=snowflake_container_eth_core)\n    tool_input = \"\"\n    result = tool._run(tool_input, mode=\"local\")\n    for i in range(len(eth_core_table_long_names_select_list)):\n        assert eth_core_table_long_names_select_list[i] in result", "\ndef test_list_snowflake_database_table_name_tool_run_local(\n    snowflake_container_eth_core, eth_core_table_long_names_select_list\n):\n    tool = ListSnowflakeDatabaseTableNamesTool(db=snowflake_container_eth_core)\n    tool_input = \"\"\n    result = tool._run(tool_input, mode=\"local\")\n    for i in range(len(eth_core_table_long_names_select_list)):\n        assert eth_core_table_long_names_select_list[i] in result\n", "\n\ndef test_get_snowflake_database_table_metadata_tool_run_local_one_table(\n    snowflake_container_eth_core,\n):\n    tool = GetSnowflakeDatabaseTableMetadataTool(db=snowflake_container_eth_core)\n    tool_input = \"ethereum.core.ez_nft_transfers\"\n\n    result = tool._run(tool_input, mode=\"local\")\n    # logger.debug(f\"{result=}\")\n    print(f\"{result=}\")\n    assert \"ethereum.core.ez_nft_transfers\" in result", "\n\ndef test_get_snowflake_database_table_metadata_tool_run_local_single_schema(\n    snowflake_container_eth_core,\n):\n    tool = GetSnowflakeDatabaseTableMetadataTool(db=snowflake_container_eth_core)\n    tool_input = \"ethereum.core.ez_dex_swaps,ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n    # tool_input = \"ethereum.core.dim_labels,ethereum.core.ez_dex_swaps\"\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"local\")\n    # logger.debug(f\"{result=}\")\n    print(f\"{result=}\")\n    assert \"ethereum.core.ez_dex_swaps\" in result\n    assert \"ethereum.core.ez_nft_mints\" in result\n    assert \"ethereum.core.ez_nft_transfers\" in result\n    # Test invalid input\n    with pytest.raises(ValueError):\n        tool._run(\"invalid_input\")", "\n\ndef test_query_snowflake_database_tool_dict_shroomdk(snowflake_container):\n    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\n    tool_input = {\n        \"database\": \"ethereum\",\n        \"schema\": \"beacon_chain\",\n        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n    }\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", "\n\ndef test_query_snowflake_database_tool_dict_str_shroomdk(snowflake_container):\n    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\n    tool_input = \"database: ethereum, schema: beacon_chain, query: select * from ethereum.beacon_chain.fact_attestations limit 3\"\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", "\n\ndef test_query_snowflake_database_tool_str_shroomdk(snowflake_container):\n    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\n    tool_input = \"select * from ethereum.beacon_chain.fact_attestations limit 3\"\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", ""]}
{"filename": "tests/integration_tests/test_tool_custom.py", "chunked_list": ["\"\"\"\ntest_tool_custom.py\nThis file contains the tests for the tool_custom module.\n\"\"\"\n\nimport pytest\n\nfrom chatweb3.tools.snowflake_database.tool_custom import (\n    CheckTableMetadataTool, CheckTableSummaryTool, QueryDatabaseTool)\n", "    CheckTableMetadataTool, CheckTableSummaryTool, QueryDatabaseTool)\n\n\ndef test_check_table_summary_tool_local(\n    snowflake_container_eth_core, eth_core_table_long_names_select_list\n):\n    tool = CheckTableSummaryTool(db=snowflake_container_eth_core)\n    tool_input = \"\"\n    result = tool._run(tool_input=tool_input, mode=\"local\")\n    for i in range(len(eth_core_table_long_names_select_list)):\n        assert eth_core_table_long_names_select_list[i] in result", "\n\ndef test_check_table_metadata_tool_local(snowflake_container_eth_core):\n    tool = CheckTableMetadataTool(db=snowflake_container_eth_core)\n    table_names = \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n\n    result = tool._run(table_names=table_names, mode=\"local\")\n    assert \"ethereum.core.ez_dex_swaps\" in result\n    assert \"ethereum.core.ez_nft_mints\" in result\n    assert \"ethereum.core.ez_nft_transfers\" in result\n    with pytest.raises(ValueError):\n        tool._run(\"invalid_input\")", "\n\ndef test_query_database_tool_str_shroomdk(snowflake_container_eth_core):\n    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\n    tool_input = \"select * from ethereum.beacon_chain.fact_attestations limit 3\"\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", "\n\ndef test_query_database_tool_dict_shroomdk(snowflake_container_eth_core):\n    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\n    tool_input = {\n        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n    }\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", "\n\ndef test_query_database_tool_dict_str_shroomdk(snowflake_container_eth_core):\n    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\n    tool_input = (\n        '{\"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\"}'\n    )\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"shroomdk\")\n    print(f\"{result=}\")\n    num_items = len(result)\n    assert num_items == 3", "\n\n@pytest.mark.skip(reason=\"requires snowflake credentials\")\ndef test_query_database_tool_dict_snowflake(snowflake_container_eth_core):\n    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\n    tool_input = {\n        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n    }\n    print(f\"\\n{tool_input=}\")\n\n    result = tool._run(tool_input, mode=\"snowflake\")\n    print(f\"{result=}\")\n    num_items = result.count(\"'), (\") + 1\n    assert num_items == 3", ""]}
{"filename": "tests/unit_tests/test_create_agent.py", "chunked_list": ["\"\"\"\ntest_create_agent.py\nThis file contains the tests for the create_agent module.\n\"\"\"\nfrom unittest.mock import Mock, patch\n\nfrom create_agent import create_agent_executor\n\n\n@patch(\"create_agent.create_snowflake_chat_agent\")", "\n@patch(\"create_agent.create_snowflake_chat_agent\")\n@patch(\"create_agent.CustomSnowflakeDatabaseToolkit\")\n@patch(\"create_agent.SnowflakeContainer\")\n@patch(\"create_agent.ChatOpenAI\")\ndef test_create_agent_executor(\n    mock_chat_openai,\n    mock_snowflake_container,\n    mock_snowflake_toolkit,\n    mock_create_snowflake_chat_agent,\n):\n    mock_agent_executor = Mock()\n    mock_create_snowflake_chat_agent.return_value = mock_agent_executor\n\n    agent_executor = create_agent_executor()\n\n    assert agent_executor == mock_agent_executor", ""]}
{"filename": "tests/unit_tests/test_chat.py", "chunked_list": ["\"\"\"\ntest_chat.py\nThis file contains the tests for the main chat module.\n\"\"\"\nimport os\nfrom unittest.mock import Mock\n\nfrom chat_ui import (\n    chat,\n    format_response,", "    chat,\n    format_response,\n    set_openai_api_key,\n    split_thought_process_text,\n)\n\n\ndef test_set_openai_api_key():\n    original_key = os.getenv(\"OPENAI_API_KEY\")\n    api_key = \"test_key\"\n    agent = \"test_agent\"\n    set_openai_api_key(api_key, agent)\n    assert os.getenv(\"OPENAI_API_KEY\") == \"\"\n\n    if original_key is None:\n        # if the original API key was None, delete the environment variable\n        del os.environ[\"OPENAI_API_KEY\"]\n    else:\n        # otherwise, restore the original API key\n        os.environ[\"OPENAI_API_KEY\"] = original_key", "\n\ndef test_format_response():\n    step1 = Mock()\n    step1.log = \"Thought: step1\\nAction: \"\n    step1.tool = None\n    step1.tool_input = None\n\n    step2 = Mock()\n    step2.log = \"Thought: step2\\nAction: \"\n    step2.tool = None\n    step2.tool_input = None\n\n    response = {\n        \"output\": \"output\",\n        \"intermediate_steps\": [\n            (step1, \"step1_text\"),\n            (step2, \"step2_text\"),\n        ],\n    }\n    expected_output = \"**Thought 1**: step1\\n\\n*Action:*\\n\\n\\tTool: None\\n\\n\\tTool input: None\\n\\n*Observation:*\\n\\nstep1_text\\n\\n**Thought 2**: Thought: step2\\n\\n*Action:*\\n\\n\\tTool: None\\n\\n\\tTool input: None\\n\\n*Observation:*\\n\\nstep2_text\\n\\n**Final answer**: output\"\n    assert format_response(response) == expected_output", "\n\ndef test_split_thought_process_text():\n    text = \"_Thought 1: Thought1\\n\\nAction:\\n\\tTool: Tool1\\n\\nTool input: Input1\\n\\nObservation:\\n\\tObservation1\\n\\n_Thought 2: Thought2\\n\\nAction:\\n\\tTool: Tool2\\n\\nTool input: Input2\\n\\nObservation:\\n\\tObservation2\\n\\n_Final answer_: FinalAnswer\"\n    expected_sections = [\n        (\" 1: Thought1\", \"Tool: Tool1\\n\\nTool input: Input1\", \"Observation1\\n\\n\"),\n        (\" 2: Thought2\", \"Tool: Tool2\\n\\nTool input: Input2\", \"Observation2\\n\\n\"),\n    ]\n    expected_final_answer = \"FinalAnswer\"\n    sections, final_answer = split_thought_process_text(text)\n    assert sections == expected_sections\n    assert final_answer == expected_final_answer", "\n\ndef test_chat():\n    inp = \"test_input\"\n    history = [(\"question\", \"answer\")]\n    agent = None\n    new_history, _, _ = chat(inp, history, agent)\n    assert new_history == [\n        (\"question\", \"answer\"),\n        (\"test_input\", \"Please paste your OpenAI API Key to use\"),\n    ]", ""]}
