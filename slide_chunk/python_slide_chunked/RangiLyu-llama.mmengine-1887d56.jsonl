{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n# Copyright (c) OpenMMLab. All rights reserved.\nimport os\nimport os.path as osp\nimport platform\nimport shutil\nimport sys\nimport warnings\nfrom setuptools import find_packages, setup\n", "from setuptools import find_packages, setup\n\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension, CUDAExtension\n\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        content = f.read()\n    return content", "\n\nversion_file = 'mmllama/version.py'\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n", "\n\ndef make_cuda_ext(name, module, sources, sources_cuda=[]):\n\n    define_macros = []\n    extra_compile_args = {'cxx': []}\n\n    if torch.cuda.is_available() or os.getenv('FORCE_CUDA', '0') == '1':\n        define_macros += [('WITH_CUDA', None)]\n        extension = CUDAExtension\n        extra_compile_args['nvcc'] = [\n            '-D__CUDA_NO_HALF_OPERATORS__',\n            '-D__CUDA_NO_HALF_CONVERSIONS__',\n            '-D__CUDA_NO_HALF2_OPERATORS__',\n        ]\n        sources += sources_cuda\n    else:\n        print(f'Compiling {name} without CUDA')\n        extension = CppExtension\n\n    return extension(\n        name=f'{module}.{name}',\n        sources=[os.path.join(*module.split('.'), p) for p in sources],\n        define_macros=define_macros,\n        extra_compile_args=extra_compile_args)", "\n\ndef parse_requirements(fname='requirements.txt', with_version=True):\n    \"\"\"Parse the package dependencies listed in a requirements file but strips\n    specific versioning information.\n\n    Args:\n        fname (str): path to requirements file\n        with_version (bool, default=False): if True include version specs\n\n    Returns:\n        List[str]: list of requirements items\n\n    CommandLine:\n        python -c \"import setup; print(setup.parse_requirements())\"\n    \"\"\"\n    import re\n    import sys\n    from os.path import exists\n    require_fpath = fname\n\n    def parse_line(line):\n        \"\"\"Parse information from a line in a requirements text file.\"\"\"\n        if line.startswith('-r '):\n            # Allow specifying requirements in other files\n            target = line.split(' ')[1]\n            for info in parse_require_file(target):\n                yield info\n        else:\n            info = {'line': line}\n            if line.startswith('-e '):\n                info['package'] = line.split('#egg=')[1]\n            elif '@git+' in line:\n                info['package'] = line\n            else:\n                # Remove versioning from the package\n                pat = '(' + '|'.join(['>=', '==', '>']) + ')'\n                parts = re.split(pat, line, maxsplit=1)\n                parts = [p.strip() for p in parts]\n\n                info['package'] = parts[0]\n                if len(parts) > 1:\n                    op, rest = parts[1:]\n                    if ';' in rest:\n                        # Handle platform specific dependencies\n                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n                        version, platform_deps = map(str.strip,\n                                                     rest.split(';'))\n                        info['platform_deps'] = platform_deps\n                    else:\n                        version = rest  # NOQA\n                    info['version'] = (op, version)\n            yield info\n\n    def parse_require_file(fpath):\n        with open(fpath, 'r') as f:\n            for line in f.readlines():\n                line = line.strip()\n                if line and not line.startswith('#'):\n                    for info in parse_line(line):\n                        yield info\n\n    def gen_packages_items():\n        if exists(require_fpath):\n            for info in parse_require_file(require_fpath):\n                parts = [info['package']]\n                if with_version and 'version' in info:\n                    parts.extend(info['version'])\n                if not sys.version.startswith('3.4'):\n                    # apparently package_deps are broken in 3.4\n                    platform_deps = info.get('platform_deps')\n                    if platform_deps is not None:\n                        parts.append(';' + platform_deps)\n                item = ''.join(parts)\n                yield item\n\n    packages = list(gen_packages_items())\n    return packages", "\n\ndef add_mim_extension():\n    \"\"\"Add extra files that are required to support MIM into the package.\n\n    These files will be added by creating a symlink to the originals if the\n    package is installed in `editable` mode (e.g. pip install -e .), or by\n    copying from the originals otherwise.\n    \"\"\"\n\n    # parse installment mode\n    if 'develop' in sys.argv:\n        # installed by `pip install -e .`\n        if platform.system() == 'Windows':\n            # set `copy` mode here since symlink fails on Windows.\n            mode = 'copy'\n        else:\n            mode = 'symlink'\n    elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:\n        # installed by `pip install .`\n        # or create source distribution by `python setup.py sdist`\n        mode = 'copy'\n    else:\n        return\n\n    filenames = ['tools', 'configs', 'demo', 'model-index.yml']\n    repo_path = osp.dirname(__file__)\n    mim_path = osp.join(repo_path, 'mmllama', '.mim')\n    os.makedirs(mim_path, exist_ok=True)\n\n    for filename in filenames:\n        if osp.exists(filename):\n            src_path = osp.join(repo_path, filename)\n            tar_path = osp.join(mim_path, filename)\n\n            if osp.isfile(tar_path) or osp.islink(tar_path):\n                os.remove(tar_path)\n            elif osp.isdir(tar_path):\n                shutil.rmtree(tar_path)\n\n            if mode == 'symlink':\n                src_relpath = osp.relpath(src_path, osp.dirname(tar_path))\n                os.symlink(src_relpath, tar_path)\n            elif mode == 'copy':\n                if osp.isfile(src_path):\n                    shutil.copyfile(src_path, tar_path)\n                elif osp.isdir(src_path):\n                    shutil.copytree(src_path, tar_path)\n                else:\n                    warnings.warn(f'Cannot copy file {src_path}.')\n            else:\n                raise ValueError(f'Invalid mode {mode}')", "\n\nif __name__ == '__main__':\n    add_mim_extension()\n    setup(\n        name='mmllama',\n        version=get_version(),\n        description='llama with mmengine',\n        long_description=readme(),\n        long_description_content_type='text/markdown',\n        author='RangiLyu',\n        author_email='lyuchqi@gmail.com',\n        keywords='computer vision, object detection',\n        url='https://github.com/RangiLyu/llama.mmengine',\n        packages=find_packages(exclude=('configs', 'tools', 'demo')),\n        include_package_data=True,\n        classifiers=[\n            'Development Status :: 5 - Production/Stable',\n            'License :: OSI Approved :: Apache Software License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.10',\n        ],\n        license='Apache License 2.0',\n        install_requires=parse_requirements('requirements.txt'),\n        extras_require={\n            'all': parse_requirements('requirements.txt'),\n        },\n        ext_modules=[],\n        cmdclass={'build_ext': BuildExtension},\n        zip_safe=False)", ""]}
{"filename": "tools/train.py", "chunked_list": ["import argparse\nimport os\nimport os.path as osp\n\nfrom datasets import load_dataset\nfrom mmengine.config import Config, DictAction\nfrom transformers import DataCollatorForSeq2Seq, LlamaTokenizer\n\nfrom mmllama.datasets import Prompter, seq2seq_collate\nfrom mmllama.engine import Runner", "from mmllama.datasets import Prompter, seq2seq_collate\nfrom mmllama.engine import Runner\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume',\n        nargs='?',\n        type=str,\n        const='auto',\n        help='If specify checkpoint path, resume from it, while if not '\n        'specify, try to auto resume from the latest checkpoint '\n        'in the work directory.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\n    return args", "\n\ndef main():\n    args = parse_args()\n\n    # load config\n    cfg = Config.fromfile(args.config)\n    cfg.launcher = args.launcher\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = args.work_dir\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(args.config))[0])\n\n\n    if cfg.data_path.endswith('.json') or cfg.data_path.endswith('.jsonl'):\n        data = load_dataset('json', data_files=cfg.data_path)\n    else:\n        data = load_dataset(cfg.data_path)\n\n    tokenizer = LlamaTokenizer(cfg.tokenizer_path)\n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different from the eos token\n    )\n    tokenizer.padding_side = 'left'  # Allow batched inference\n\n    # TODO: move hyps to cfg\n    cutoff_len: int = 256\n    prompt_template_name: str = 'alpaca'\n    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n    prompter = Prompter(prompt_template_name)\n\n\n\n    def tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do this with the tokenizer settings\n        # but again, gotta move fast\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result['input_ids'][-1] != tokenizer.eos_token_id\n            and len(result['input_ids']) < cutoff_len\n            and add_eos_token\n        ):\n            result['input_ids'].append(tokenizer.eos_token_id)\n            result['attention_mask'].append(1)\n\n        result['labels'] = result['input_ids'].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n        full_prompt = prompter.generate_prompt(\n            data_point['instruction'],\n            data_point['input'],\n            data_point['output'],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n        if not train_on_inputs:\n            user_prompt = prompter.generate_prompt(\n                data_point['instruction'], data_point['input']\n            )\n            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n            user_prompt_len = len(tokenized_user_prompt['input_ids'])\n\n            tokenized_full_prompt['labels'] = [\n                -100\n            ] * user_prompt_len + tokenized_full_prompt['labels'][\n                user_prompt_len:\n            ]  # could be sped up, probably\n        return tokenized_full_prompt\n\n    train_val = data['train'].train_test_split(\n        test_size=cfg.val_set_size, shuffle=True, seed=42\n    )\n    train_data = train_val['train'].shuffle().map(generate_and_tokenize_prompt)\n\n    val_data = train_val['test'].shuffle().map(generate_and_tokenize_prompt)\n    # collator = DataCollatorForSeq2Seq(\n    #         tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n    #     )\n    from mmengine.registry import FUNCTIONS\n    FUNCTIONS.register_module(name='seq2seq_collate', module=seq2seq_collate)\n\n    cfg.train_dataloader.dataset = train_data.remove_columns(('instruction', 'input', 'output'))\n    cfg.train_dataloader.collate_fn.tokenizer = tokenizer\n    cfg.val_dataloader.dataset = val_data.remove_columns(('instruction', 'input', 'output'))\n    cfg.val_dataloader.collate_fn.tokenizer = tokenizer\n\n\n    # resume is determined in this priority: resume from > auto_resume\n    if args.resume == 'auto':\n        cfg.resume = True\n        cfg.load_from = None\n    elif args.resume is not None:\n        cfg.resume = True\n        cfg.load_from = args.resume\n\n    runner = Runner.from_cfg(cfg)\n    # start training\n    runner.train()", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "tools/convert_ckeckpoint.py", "chunked_list": ["# from https://github.com/Lightning-AI/lit-llama/blob/main/scripts/convert_checkpoint.py\nimport os\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict\n\nimport torch\nfrom tqdm import tqdm\n\n\"\"\"", "\n\"\"\"\nSample usage:\n```bash\npython -m scripts.convert_checkpoint -h\npython -m scripts.convert_checkpoint converted\n```\n\"\"\"\n\n\ndef convert_state_dict(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    converted = {}\n    converted['transformer.wte.weight'] = state_dict['tok_embeddings.weight']\n    converted['lm_head.weight'] = state_dict['output.weight']\n    converted['transformer.ln_f.scale'] = state_dict['norm.weight']\n\n    for key in [k for k in state_dict if k.startswith('layers')]:\n        layer_idx = key.split('.')[1]\n\n        # attention\n        # the wq, wk, wv from the FB model are stacked in our model as c_attn\n        converted[f'transformer.h.{layer_idx}.attn.c_attn.weight'] = torch.cat(\n            (\n                state_dict[f'layers.{layer_idx}.attention.wq.weight'],\n                state_dict[f'layers.{layer_idx}.attention.wk.weight'],\n                state_dict[f'layers.{layer_idx}.attention.wv.weight'],\n            )\n        )\n        converted[f'transformer.h.{layer_idx}.attn.c_proj.weight'] = state_dict[\n            f'layers.{layer_idx}.attention.wo.weight'\n        ]\n        # mlp\n        converted[f'transformer.h.{layer_idx}.mlp.c_fc1.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w1.weight'\n        ]\n        converted[f'transformer.h.{layer_idx}.mlp.c_proj.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w2.weight'\n        ]\n        converted[f'transformer.h.{layer_idx}.mlp.c_fc2.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w3.weight'\n        ]\n        # rms norm\n        converted[f'transformer.h.{layer_idx}.rms_1.scale'] = state_dict[f'layers.{layer_idx}.attention_norm.weight']\n        converted[f'transformer.h.{layer_idx}.rms_2.scale'] = state_dict[f'layers.{layer_idx}.ffn_norm.weight']\n    return converted", "\n\ndef convert_state_dict(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    converted = {}\n    converted['transformer.wte.weight'] = state_dict['tok_embeddings.weight']\n    converted['lm_head.weight'] = state_dict['output.weight']\n    converted['transformer.ln_f.scale'] = state_dict['norm.weight']\n\n    for key in [k for k in state_dict if k.startswith('layers')]:\n        layer_idx = key.split('.')[1]\n\n        # attention\n        # the wq, wk, wv from the FB model are stacked in our model as c_attn\n        converted[f'transformer.h.{layer_idx}.attn.c_attn.weight'] = torch.cat(\n            (\n                state_dict[f'layers.{layer_idx}.attention.wq.weight'],\n                state_dict[f'layers.{layer_idx}.attention.wk.weight'],\n                state_dict[f'layers.{layer_idx}.attention.wv.weight'],\n            )\n        )\n        converted[f'transformer.h.{layer_idx}.attn.c_proj.weight'] = state_dict[\n            f'layers.{layer_idx}.attention.wo.weight'\n        ]\n        # mlp\n        converted[f'transformer.h.{layer_idx}.mlp.c_fc1.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w1.weight'\n        ]\n        converted[f'transformer.h.{layer_idx}.mlp.c_proj.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w2.weight'\n        ]\n        converted[f'transformer.h.{layer_idx}.mlp.c_fc2.weight'] = state_dict[\n            f'layers.{layer_idx}.feed_forward.w3.weight'\n        ]\n        # rms norm\n        converted[f'transformer.h.{layer_idx}.rms_1.scale'] = state_dict[f'layers.{layer_idx}.attention_norm.weight']\n        converted[f'transformer.h.{layer_idx}.rms_2.scale'] = state_dict[f'layers.{layer_idx}.ffn_norm.weight']\n    return converted", "\n\ndef meta_weights_for_nano_model(\n    *,\n    output_dir: Path = Path('checkpoints/mm-llama'),\n    ckpt_dir: Path = Path('checkpoints/llama/'),\n    tokenizer_path: Path = Path('checkpoints/llama/tokenizer.model'),\n    model_size: str = '7B',\n) -> None:\n    output_dir = output_dir / model_size\n    ckpt_dir = ckpt_dir / model_size\n    os.makedirs(output_dir, exist_ok=True)\n\n    # the tokenizer is the same for all model sizes, so we store it in the parent dir\n    if 'tokenizer.model' not in os.listdir(output_dir.parent):\n        shutil.copy(tokenizer_path, output_dir.parent)\n\n    checkpoint_files = sorted(ckpt_dir.glob('*.pth'))\n\n    # for the bigger models, there are multiple model-parallel checkpoints\n    # and we combine them into one single file\n    combined = {}\n    for file in tqdm(checkpoint_files, total=len(checkpoint_files)):\n        checkpoint = torch.load(file, map_location='cpu')\n        converted = convert_state_dict(checkpoint)\n        combined.update(converted)\n\n    torch.save(combined, Path(output_dir, 'state_dict.pth'))", "\n\nif __name__ == '__main__':\n    from jsonargparse import CLI\n\n    CLI(meta_weights_for_nano_model)\n"]}
{"filename": "tools/generate.py", "chunked_list": ["import argparse\n\nimport torch\nfrom mmengine.config import Config, DictAction\nfrom mmengine.logging import print_log\nfrom transformers import LlamaTokenizer\n\nfrom mmllama.datasets import Prompter\nfrom mmllama.registry import MODELS\n", "from mmllama.registry import MODELS\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet test (and eval) a model')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--instructions',\n        nargs='+',\n        help='instructions to generate responses')\n    args = parser.parse_args()\n    return args", "\n\ndef main():\n    args = parse_args()\n\n    # load config\n    cfg = Config.fromfile(args.config)\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    print_log('Building model', logger='current')\n    model = MODELS.build(cfg.model)\n    model.init_weights()\n    model.load_state_dict(torch.load(args.checkpoint)['state_dict'], strict=False)\n    model.half()\n    model.cuda()\n    model.eval()\n    print_log('Finished building model', logger='current')\n\n    tokenizer = LlamaTokenizer(cfg.tokenizer_path)\n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different from the eos token\n    )\n    tokenizer.padding_side = 'left'  # Allow batched inference\n\n    prompter = Prompter('alpaca')\n\n    def evaluate(\n        instruction,\n        input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        max_new_tokens=128,\n        **kwargs,\n    ):\n        \"\"\"Generate a response to an instruction.\n        Modified from https://github.com/tloen/alpaca-lora\n        \"\"\"\n        prompt = prompter.generate_prompt(instruction, input)\n        inputs = tokenizer(prompt, return_tensors='pt')\n        input_ids = inputs['input_ids'].to('cuda')\n        model.model.test_cfg.temperature=temperature\n        model.model.test_cfg.top_k=top_k\n        model.model.test_cfg.max_new_tokens=max_new_tokens\n        # TODO: beam search\n        with torch.no_grad():\n            generation_output = model(input_ids, mode='predict')\n        s = generation_output[0]\n        output = tokenizer.decode(s)\n        return prompter.get_response(output)\n\n    if args.instructions is not None:\n        instructions = args.instructions\n    else:\n        instructions = [\n        'Tell me about alpacas.',\n        'Tell me about the president of Mexico in 2019.',\n        'Tell me about the king of France in 2019.',\n        'List all Canadian provinces in alphabetical order.',\n        'Write a Python program that prints the first 10 Fibonacci numbers.',\n        \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",  # noqa: E501\n        \"Tell me five words that rhyme with 'shock'.\",\n        \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n        'Count up from 1 to 500.',\n    ]\n    for instruction in instructions:\n        print('Instruction:', instruction)\n        print('Response:', evaluate(instruction))\n        print()", "\n\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "mmllama/version.py", "chunked_list": ["__version__ = '0.0.1'\nshort_version = __version__\n\n\ndef parse_version_info(version_str):\n    \"\"\"Parse a version string into a tuple.\n\n    Args:\n        version_str (str): The version string.\n    Returns:\n        tuple[int | str]: The version info, e.g., \"1.3.0\" is parsed into\n            (1, 3, 0), and \"2.0.0rc1\" is parsed into (2, 0, 0, 'rc1').\n    \"\"\"\n    version_info = []\n    for x in version_str.split('.'):\n        if x.isdigit():\n            version_info.append(int(x))\n        elif x.find('rc') != -1:\n            patch_version = x.split('rc')\n            version_info.append(int(patch_version[0]))\n            version_info.append(f'rc{patch_version[1]}')\n    return tuple(version_info)", "\n\nversion_info = parse_version_info(__version__)\n"]}
{"filename": "mmllama/registry.py", "chunked_list": ["\"\"\"MMEngine provides 17 registry nodes to support using modules across\nprojects. Each node is a child of the root registry in MMEngine.\n\nMore details can be found at\nhttps://mmengine.readthedocs.io/en/latest/tutorials/registry.html.\n\"\"\"\n\nfrom mmengine.registry import DATA_SAMPLERS as MMENGINE_DATA_SAMPLERS\nfrom mmengine.registry import DATASETS as MMENGINE_DATASETS\nfrom mmengine.registry import EVALUATOR as MMENGINE_EVALUATOR", "from mmengine.registry import DATASETS as MMENGINE_DATASETS\nfrom mmengine.registry import EVALUATOR as MMENGINE_EVALUATOR\nfrom mmengine.registry import HOOKS as MMENGINE_HOOKS\nfrom mmengine.registry import LOG_PROCESSORS as MMENGINE_LOG_PROCESSORS\nfrom mmengine.registry import LOOPS as MMENGINE_LOOPS\nfrom mmengine.registry import METRICS as MMENGINE_METRICS\nfrom mmengine.registry import MODEL_WRAPPERS as MMENGINE_MODEL_WRAPPERS\nfrom mmengine.registry import MODELS as MMENGINE_MODELS\nfrom mmengine.registry import \\\n    OPTIM_WRAPPER_CONSTRUCTORS as MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS", "from mmengine.registry import \\\n    OPTIM_WRAPPER_CONSTRUCTORS as MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS\nfrom mmengine.registry import OPTIM_WRAPPERS as MMENGINE_OPTIM_WRAPPERS\nfrom mmengine.registry import OPTIMIZERS as MMENGINE_OPTIMIZERS\nfrom mmengine.registry import PARAM_SCHEDULERS as MMENGINE_PARAM_SCHEDULERS\nfrom mmengine.registry import RUNNER_CONSTRUCTORS as MMENGINE_RUNNER_CONSTRUCTORS\nfrom mmengine.registry import RUNNERS as MMENGINE_RUNNERS\nfrom mmengine.registry import TASK_UTILS as MMENGINE_TASK_UTILS\nfrom mmengine.registry import TRANSFORMS as MMENGINE_TRANSFORMS\nfrom mmengine.registry import VISBACKENDS as MMENGINE_VISBACKENDS", "from mmengine.registry import TRANSFORMS as MMENGINE_TRANSFORMS\nfrom mmengine.registry import VISBACKENDS as MMENGINE_VISBACKENDS\nfrom mmengine.registry import VISUALIZERS as MMENGINE_VISUALIZERS\nfrom mmengine.registry import WEIGHT_INITIALIZERS as MMENGINE_WEIGHT_INITIALIZERS\nfrom mmengine.registry import Registry\n\n# manage all kinds of runners like `EpochBasedRunner` and `IterBasedRunner`\nRUNNERS = Registry(\n    'runner', parent=MMENGINE_RUNNERS, locations=['mmllama.engine.runner'])\n# manage runner constructors that define how to initialize runners", "    'runner', parent=MMENGINE_RUNNERS, locations=['mmllama.engine.runner'])\n# manage runner constructors that define how to initialize runners\nRUNNER_CONSTRUCTORS = Registry(\n    'runner constructor',\n    parent=MMENGINE_RUNNER_CONSTRUCTORS,\n    locations=['mmllama.engine.runner'])\n# manage all kinds of loops like `EpochBasedTrainLoop`\nLOOPS = Registry(\n    'loop', parent=MMENGINE_LOOPS, locations=['mmllama.engine.runner'])\n# manage all kinds of hooks like `CheckpointHook`", "    'loop', parent=MMENGINE_LOOPS, locations=['mmllama.engine.runner'])\n# manage all kinds of hooks like `CheckpointHook`\nHOOKS = Registry(\n    'hook', parent=MMENGINE_HOOKS, locations=['mmllama.engine.hooks'])\n\n# manage data-related modules\nDATASETS = Registry(\n    'dataset', parent=MMENGINE_DATASETS, locations=['mmllama.datasets'])\nDATA_SAMPLERS = Registry(\n    'data sampler',", "DATA_SAMPLERS = Registry(\n    'data sampler',\n    parent=MMENGINE_DATA_SAMPLERS,\n    locations=['mmllama.datasets.samplers'])\nTRANSFORMS = Registry(\n    'transform',\n    parent=MMENGINE_TRANSFORMS,\n    locations=['mmllama.datasets.transforms'])\n\n# manage all kinds of modules inheriting `nn.Module`", "\n# manage all kinds of modules inheriting `nn.Module`\nMODELS = Registry(\n    'model', parent=MMENGINE_MODELS, locations=['mmllama.models'])\n# manage all kinds of model wrappers like 'MMDistributedDataParallel'\nMODEL_WRAPPERS = Registry(\n    'model_wrapper',\n    parent=MMENGINE_MODEL_WRAPPERS,\n    locations=['mmllama.models'])\n# manage all kinds of weight initialization modules like `Uniform`", "    locations=['mmllama.models'])\n# manage all kinds of weight initialization modules like `Uniform`\nWEIGHT_INITIALIZERS = Registry(\n    'weight initializer',\n    parent=MMENGINE_WEIGHT_INITIALIZERS,\n    locations=['mmllama.models'])\n\n# manage all kinds of optimizers like `SGD` and `Adam`\nOPTIMIZERS = Registry(\n    'optimizer',", "OPTIMIZERS = Registry(\n    'optimizer',\n    parent=MMENGINE_OPTIMIZERS,\n    locations=['mmllama.engine.optimizers'])\n# manage optimizer wrapper\nOPTIM_WRAPPERS = Registry(\n    'optim_wrapper',\n    parent=MMENGINE_OPTIM_WRAPPERS,\n    locations=['mmllama.engine.optimizers'])\n# manage constructors that customize the optimization hyperparameters.", "    locations=['mmllama.engine.optimizers'])\n# manage constructors that customize the optimization hyperparameters.\nOPTIM_WRAPPER_CONSTRUCTORS = Registry(\n    'optimizer constructor',\n    parent=MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS,\n    locations=['mmllama.engine.optimizers'])\n# manage all kinds of parameter schedulers like `MultiStepLR`\nPARAM_SCHEDULERS = Registry(\n    'parameter scheduler',\n    parent=MMENGINE_PARAM_SCHEDULERS,", "    'parameter scheduler',\n    parent=MMENGINE_PARAM_SCHEDULERS,\n    locations=['mmllama.engine.schedulers'])\n# manage all kinds of metrics\nMETRICS = Registry(\n    'metric', parent=MMENGINE_METRICS, locations=['mmllama.evaluation'])\n# manage evaluator\nEVALUATOR = Registry(\n    'evaluator', parent=MMENGINE_EVALUATOR, locations=['mmllama.evaluation'])\n", "    'evaluator', parent=MMENGINE_EVALUATOR, locations=['mmllama.evaluation'])\n\n# manage task-specific modules like anchor generators and box coders\nTASK_UTILS = Registry(\n    'task util', parent=MMENGINE_TASK_UTILS, locations=['mmllama.models'])\n\n# manage visualizer\nVISUALIZERS = Registry(\n    'visualizer',\n    parent=MMENGINE_VISUALIZERS,", "    'visualizer',\n    parent=MMENGINE_VISUALIZERS,\n    locations=['mmllama.visualization'])\n# manage visualizer backend\nVISBACKENDS = Registry(\n    'vis_backend',\n    parent=MMENGINE_VISBACKENDS,\n    locations=['mmllama.visualization'])\n\n# manage logprocessor", "\n# manage logprocessor\nLOG_PROCESSORS = Registry(\n    'log_processor',\n    parent=MMENGINE_LOG_PROCESSORS,\n    # TODO: update the location when mmllama has its own log processor\n    locations=['mmllama.engine'])\n"]}
{"filename": "mmllama/__init__.py", "chunked_list": ["import mmengine\nfrom mmengine.utils import digit_version\n\nfrom .version import __version__, version_info\n\nmmengine_minimum_version = '0.7.0'\nmmengine_maximum_version = '1.0.0'\nmmengine_version = digit_version(mmengine.__version__)\n\n", "\n\nassert (mmengine_version >= digit_version(mmengine_minimum_version)\n        and mmengine_version < digit_version(mmengine_maximum_version)), \\\n    f'MMEngine=={mmengine.__version__} is used but incompatible. ' \\\n    f'Please install mmengine>={mmengine_minimum_version}, ' \\\n    f'<{mmengine_maximum_version}.'\n\n__all__ = ['__version__', 'version_info', 'digit_version']\n", "__all__ = ['__version__', 'version_info', 'digit_version']\n"]}
{"filename": "mmllama/evaluation/metrics.py", "chunked_list": ["from typing import Dict, List, Optional, Sequence, Union\n\nfrom mmengine.evaluator import BaseMetric\n\nfrom mmllama.registry import METRICS\n\n\n@METRICS.register_module()\nclass DummyMetric(BaseMetric):\n    \"\"\"\n    TODO: implement a metric\n    \"\"\"\n    default_prefix: Optional[str] = 'metric'\n\n    def __init__(self,\n                 collect_device: str = 'cpu',\n                 prefix: Optional[str] = None) -> None:\n        super().__init__(collect_device=collect_device, prefix=prefix)\n\n    # TODO: data_batch is no longer needed, consider adjusting the\n    #  parameter position\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:\n        \"\"\"Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader.\n            data_samples (Sequence[dict]): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n        for data_sample in data_samples:\n            result = dict()\n            self.results.append(result)\n\n    def compute_metrics(self, results: list) -> Dict[str, float]:\n\n        return {}", "class DummyMetric(BaseMetric):\n    \"\"\"\n    TODO: implement a metric\n    \"\"\"\n    default_prefix: Optional[str] = 'metric'\n\n    def __init__(self,\n                 collect_device: str = 'cpu',\n                 prefix: Optional[str] = None) -> None:\n        super().__init__(collect_device=collect_device, prefix=prefix)\n\n    # TODO: data_batch is no longer needed, consider adjusting the\n    #  parameter position\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:\n        \"\"\"Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader.\n            data_samples (Sequence[dict]): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n        for data_sample in data_samples:\n            result = dict()\n            self.results.append(result)\n\n    def compute_metrics(self, results: list) -> Dict[str, float]:\n\n        return {}", ""]}
{"filename": "mmllama/evaluation/__init__.py", "chunked_list": ["from .metrics import DummyMetric\n"]}
{"filename": "mmllama/engine/runner.py", "chunked_list": ["import logging\nimport time\nimport warnings\nfrom typing import Dict, List, Optional, Union\n\nimport mmengine\nfrom mmengine.config import Config, ConfigDict\nfrom mmengine.dist import master_only\nfrom mmengine.fileio import FileClient, join_path\nfrom mmengine.logging import print_log", "from mmengine.fileio import FileClient, join_path\nfrom mmengine.logging import print_log\nfrom mmengine.model import is_model_wrapper\nfrom mmengine.optim import OptimWrapper, OptimWrapperDict, _ParamScheduler\nfrom mmengine.runner.checkpoint import save_checkpoint, weights_to_cpu\nfrom mmengine.utils import get_git_hash\n\nConfigType = Union[Dict, Config, ConfigDict]\nParamSchedulerType = Union[List[_ParamScheduler], Dict[str,\n                                                       List[_ParamScheduler]]]", "ParamSchedulerType = Union[List[_ParamScheduler], Dict[str,\n                                                       List[_ParamScheduler]]]\nOptimWrapperType = Union[OptimWrapper, OptimWrapperDict]\n\n\nfrom mmengine.runner import Runner as _Runner\n\n\nclass Runner(_Runner):\n\n    @master_only\n    def save_checkpoint(\n        self,\n        out_dir: str,\n        filename: str,\n        file_client_args: Optional[dict] = None,\n        save_optimizer: bool = True,\n        save_param_scheduler: bool = True,\n        meta: dict = None,\n        by_epoch: bool = True,\n        backend_args: Optional[dict] = None,\n    ):\n        \"\"\"Save checkpoints.\n\n        ``CheckpointHook`` invokes this method to save checkpoints\n        periodically.\n\n        Args:\n            out_dir (str): The directory that checkpoints are saved.\n            filename (str): The checkpoint filename.\n            file_client_args (dict, optional): Arguments to instantiate a\n                FileClient. See :class:`mmengine.fileio.FileClient` for\n                details. Defaults to None. It will be deprecated in future.\n                Please use `backend_args` instead.\n            save_optimizer (bool): Whether to save the optimizer to\n                the checkpoint. Defaults to True.\n            save_param_scheduler (bool): Whether to save the param_scheduler\n                to the checkpoint. Defaults to True.\n            meta (dict, optional): The meta information to be saved in the\n                checkpoint. Defaults to None.\n            by_epoch (bool): Whether the scheduled momentum is updated by\n                epochs. Defaults to True.\n            backend_args (dict, optional): Arguments to instantiate the\n                prefix of uri corresponding backend. Defaults to None.\n                New in v0.2.0.\n        \"\"\"\n        if meta is None:\n            meta = {}\n        elif not isinstance(meta, dict):\n            raise TypeError(\n                f'meta should be a dict or None, but got {type(meta)}')\n\n        if by_epoch:\n            # self.epoch increments 1 after\n            # `self.call_hook('after_train_epoch)` but `save_checkpoint` is\n            # called by `after_train_epoch`` method of `CheckpointHook` so\n            # `epoch` should be `self.epoch + 1`\n            meta.update(epoch=self.epoch + 1, iter=self.iter)\n        else:\n            meta.update(epoch=self.epoch, iter=self.iter + 1)\n\n        if file_client_args is not None:\n            warnings.warn(\n                '\"file_client_args\" will be deprecated in future. '\n                'Please use \"backend_args\" instead', DeprecationWarning)\n            if backend_args is not None:\n                raise ValueError(\n                    '\"file_client_args\" and \"backend_args\" cannot be set at '\n                    'the same time.')\n\n            file_client = FileClient.infer_client(file_client_args, out_dir)\n            filepath = file_client.join_path(out_dir, filename)\n        else:\n            filepath = join_path(  # type: ignore\n                out_dir, filename, backend_args=backend_args)\n\n        meta.update(\n            cfg=self.cfg.pretty_text,\n            seed=self.seed,\n            experiment_name=self.experiment_name,\n            time=time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n            mmengine_version=mmengine.__version__ + get_git_hash())\n\n        if hasattr(self.train_dataloader.dataset, 'metainfo'):\n            meta.update(dataset_meta=self.train_dataloader.dataset.metainfo)\n\n        if is_model_wrapper(self.model):\n            model = self.model.module\n        else:\n            model = self.model\n\n        checkpoint = {\n            'meta': meta,\n            'state_dict': weights_to_cpu(model.state_dict()),\n            'message_hub': self.message_hub.state_dict()\n        }\n        # save optimizer state dict to checkpoint\n        if save_optimizer:\n            if isinstance(self.optim_wrapper, OptimWrapper):\n                checkpoint['optimizer'] = self.optim_wrapper.state_dict()\n            else:\n                raise TypeError(\n                    'self.optim_wrapper should be an `OptimWrapper` '\n                    'or `OptimWrapperDict` instance, but got '\n                    f'{self.optim_wrapper}')\n\n        # save param scheduler state dict\n        if save_param_scheduler and self.param_schedulers is None:\n            print_log(\n                '`save_param_scheduler` is True but `self.param_schedulers` '\n                'is None, so skip saving parameter schedulers',\n                logger='current',\n                level=logging.WARNING)\n            save_param_scheduler = False\n        if save_param_scheduler:\n            if isinstance(self.param_schedulers, dict):\n                checkpoint['param_schedulers'] = dict()\n                for name, schedulers in self.param_schedulers.items():\n                    checkpoint['param_schedulers'][name] = []\n                    for scheduler in schedulers:\n                        state_dict = scheduler.state_dict()\n                        checkpoint['param_schedulers'][name].append(state_dict)\n            else:\n                checkpoint['param_schedulers'] = []\n                for scheduler in self.param_schedulers:  # type: ignore\n                    state_dict = scheduler.state_dict()  # type: ignore\n                    checkpoint['param_schedulers'].append(state_dict)\n\n        self.call_hook('before_save_checkpoint', checkpoint=checkpoint)\n        save_checkpoint(checkpoint, filepath)", "class Runner(_Runner):\n\n    @master_only\n    def save_checkpoint(\n        self,\n        out_dir: str,\n        filename: str,\n        file_client_args: Optional[dict] = None,\n        save_optimizer: bool = True,\n        save_param_scheduler: bool = True,\n        meta: dict = None,\n        by_epoch: bool = True,\n        backend_args: Optional[dict] = None,\n    ):\n        \"\"\"Save checkpoints.\n\n        ``CheckpointHook`` invokes this method to save checkpoints\n        periodically.\n\n        Args:\n            out_dir (str): The directory that checkpoints are saved.\n            filename (str): The checkpoint filename.\n            file_client_args (dict, optional): Arguments to instantiate a\n                FileClient. See :class:`mmengine.fileio.FileClient` for\n                details. Defaults to None. It will be deprecated in future.\n                Please use `backend_args` instead.\n            save_optimizer (bool): Whether to save the optimizer to\n                the checkpoint. Defaults to True.\n            save_param_scheduler (bool): Whether to save the param_scheduler\n                to the checkpoint. Defaults to True.\n            meta (dict, optional): The meta information to be saved in the\n                checkpoint. Defaults to None.\n            by_epoch (bool): Whether the scheduled momentum is updated by\n                epochs. Defaults to True.\n            backend_args (dict, optional): Arguments to instantiate the\n                prefix of uri corresponding backend. Defaults to None.\n                New in v0.2.0.\n        \"\"\"\n        if meta is None:\n            meta = {}\n        elif not isinstance(meta, dict):\n            raise TypeError(\n                f'meta should be a dict or None, but got {type(meta)}')\n\n        if by_epoch:\n            # self.epoch increments 1 after\n            # `self.call_hook('after_train_epoch)` but `save_checkpoint` is\n            # called by `after_train_epoch`` method of `CheckpointHook` so\n            # `epoch` should be `self.epoch + 1`\n            meta.update(epoch=self.epoch + 1, iter=self.iter)\n        else:\n            meta.update(epoch=self.epoch, iter=self.iter + 1)\n\n        if file_client_args is not None:\n            warnings.warn(\n                '\"file_client_args\" will be deprecated in future. '\n                'Please use \"backend_args\" instead', DeprecationWarning)\n            if backend_args is not None:\n                raise ValueError(\n                    '\"file_client_args\" and \"backend_args\" cannot be set at '\n                    'the same time.')\n\n            file_client = FileClient.infer_client(file_client_args, out_dir)\n            filepath = file_client.join_path(out_dir, filename)\n        else:\n            filepath = join_path(  # type: ignore\n                out_dir, filename, backend_args=backend_args)\n\n        meta.update(\n            cfg=self.cfg.pretty_text,\n            seed=self.seed,\n            experiment_name=self.experiment_name,\n            time=time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n            mmengine_version=mmengine.__version__ + get_git_hash())\n\n        if hasattr(self.train_dataloader.dataset, 'metainfo'):\n            meta.update(dataset_meta=self.train_dataloader.dataset.metainfo)\n\n        if is_model_wrapper(self.model):\n            model = self.model.module\n        else:\n            model = self.model\n\n        checkpoint = {\n            'meta': meta,\n            'state_dict': weights_to_cpu(model.state_dict()),\n            'message_hub': self.message_hub.state_dict()\n        }\n        # save optimizer state dict to checkpoint\n        if save_optimizer:\n            if isinstance(self.optim_wrapper, OptimWrapper):\n                checkpoint['optimizer'] = self.optim_wrapper.state_dict()\n            else:\n                raise TypeError(\n                    'self.optim_wrapper should be an `OptimWrapper` '\n                    'or `OptimWrapperDict` instance, but got '\n                    f'{self.optim_wrapper}')\n\n        # save param scheduler state dict\n        if save_param_scheduler and self.param_schedulers is None:\n            print_log(\n                '`save_param_scheduler` is True but `self.param_schedulers` '\n                'is None, so skip saving parameter schedulers',\n                logger='current',\n                level=logging.WARNING)\n            save_param_scheduler = False\n        if save_param_scheduler:\n            if isinstance(self.param_schedulers, dict):\n                checkpoint['param_schedulers'] = dict()\n                for name, schedulers in self.param_schedulers.items():\n                    checkpoint['param_schedulers'][name] = []\n                    for scheduler in schedulers:\n                        state_dict = scheduler.state_dict()\n                        checkpoint['param_schedulers'][name].append(state_dict)\n            else:\n                checkpoint['param_schedulers'] = []\n                for scheduler in self.param_schedulers:  # type: ignore\n                    state_dict = scheduler.state_dict()  # type: ignore\n                    checkpoint['param_schedulers'].append(state_dict)\n\n        self.call_hook('before_save_checkpoint', checkpoint=checkpoint)\n        save_checkpoint(checkpoint, filepath)", ""]}
{"filename": "mmllama/engine/__init__.py", "chunked_list": ["from .runner import Runner\n\n__all__ = ['Runner']\n"]}
{"filename": "mmllama/models/llama.py", "chunked_list": ["\"\"\"Full definition of a LLaMA Language Model, all of it in this single file.\n\nModified from https://github.com/Lightning-AI/lit-llama/blob/main/lit_llama/model.py\n\"\"\"\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nfrom mmengine import Config\nfrom mmengine.logging import print_log\nfrom mmengine.model import BaseModel\nfrom torch.nn import functional as F\n\nfrom mmllama.registry import MODELS\n\n\ndef build_rope_cache(seq_len: int,\n                     n_elem: int,\n                     dtype: torch.dtype,\n                     base: int = 10000) -> torch.Tensor:\n    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\n    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n    transformers/rope/__init__.py. MIT License:\n    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n    \"\"\"\n    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n    theta = 1.0 / (base**(torch.arange(0, n_elem, 2, dtype=dtype) / n_elem))\n\n    # Create position indexes `[0, 1, ..., seq_len - 1]`\n    seq_idx = torch.arange(seq_len, dtype=dtype)\n\n    # Calculate the product of position index and $\\theta_i$\n    idx_theta = torch.outer(seq_idx, theta)\n\n    # Cache them\n    cache = torch.polar(torch.ones_like(idx_theta), idx_theta)  # complex64\n    return cache", "\n\ndef build_rope_cache(seq_len: int,\n                     n_elem: int,\n                     dtype: torch.dtype,\n                     base: int = 10000) -> torch.Tensor:\n    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\n    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n    transformers/rope/__init__.py. MIT License:\n    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n    \"\"\"\n    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n    theta = 1.0 / (base**(torch.arange(0, n_elem, 2, dtype=dtype) / n_elem))\n\n    # Create position indexes `[0, 1, ..., seq_len - 1]`\n    seq_idx = torch.arange(seq_len, dtype=dtype)\n\n    # Calculate the product of position index and $\\theta_i$\n    idx_theta = torch.outer(seq_idx, theta)\n\n    # Cache them\n    cache = torch.polar(torch.ones_like(idx_theta), idx_theta)  # complex64\n    return cache", "\n\ndef apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n    x = x.transpose(1, 2)\n\n    # truncate to support variable sizes\n    T = x.size(1)\n    rope_cache = rope_cache[:T]\n\n    # cast because `view_as_complex` does not support 16 bit tensors\n    xc = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n    rope_cache = rope_cache.view(1, xc.size(1), 1, xc.size(3))\n    x_out = torch.view_as_real(xc * rope_cache).flatten(3)\n    return x_out.transpose(1, 2).type_as(x)", "\n\nclass RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization.\n\n    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n    \"\"\"\n\n    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(size))\n        self.eps = eps\n        self.dim = dim\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # NOTE: the original RMSNorm paper implementation is not equivalent\n        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n        # rms_x = norm_x * d_x ** (-1. / 2)\n        # x_normed = x / (rms_x + self.eps)\n        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n        x_normed = x * torch.rsqrt(norm_x + self.eps)\n        return self.scale * x_normed", "\n\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, n_embd, n_head, rope_cache: torch.Tensor) -> None:\n        super().__init__()\n        assert n_embd % n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n        # regularization\n        self.n_head = n_head\n        self.n_embd = n_embd\n        self.register_buffer('rope_cache', rope_cache, persistent=False)\n\n    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        B, T, C = x.size(\n        )  # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n\n        head_size = C // self.n_head\n        k = k.view(B, T, self.n_head, head_size).transpose(1,\n                                                           2)  # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, head_size).transpose(1,\n                                                           2)  # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, head_size).transpose(1,\n                                                           2)  # (B, nh, T, hs)\n\n        q = apply_rope(q, self.rope_cache)\n        k = apply_rope(k, self.rope_cache)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        #  att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        #  att = F.softmax(att, dim=-1)\n        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\n        # efficient attention using Flash Attention CUDA kernels\n        y = F.scaled_dot_product_attention(\n            q, k, v, attn_mask=attention_mask, dropout_p=0.0, is_causal=True)\n\n        y = y.transpose(1, 2).contiguous().view(\n            B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.c_proj(y)\n\n        return y", "\n\nclass MLP(nn.Module):\n\n    def __init__(self, n_embd, expand=4) -> None:\n        super().__init__()\n        hidden_dim = expand * n_embd\n        n_hidden = int(2 * hidden_dim / 3)\n        N = 256\n        # ensure n_hidden is multiple of N\n        n_hidden = ((n_hidden - 1) // N) * N + N\n\n        self.c_fc1 = nn.Linear(n_embd, n_hidden, bias=False)\n        self.c_fc2 = nn.Linear(n_embd, n_hidden, bias=False)\n        self.c_proj = nn.Linear(n_hidden, n_embd, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n        x = self.c_proj(x)\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self, n_embd, n_head, rope_cache: torch.Tensor) -> None:\n        super().__init__()\n        self.rms_1 = RMSNorm(n_embd)\n        self.attn = CausalSelfAttention(\n            n_embd=n_embd, n_head=n_head, rope_cache=rope_cache)\n        self.rms_2 = RMSNorm(n_embd)\n        self.mlp = MLP(n_embd)\n\n    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        x = x + self.attn(self.rms_1(x), attention_mask)\n        x = x + self.mlp(self.rms_2(x))\n        return x", "\n\n@MODELS.register_module()\nclass LLaMA(BaseModel):\n\n    def __init__(self,\n                 block_size: int = 4096,\n                 vocab_size: int = 32000,\n                 n_layer: int = 32,\n                 n_head: int = 32,\n                 n_embd: int = 4096,\n                 pretrained=None,\n                 test_cfg=dict(\n                    max_new_tokens=50,\n                    temperature=1.0,\n                    top_k=200,)) -> None:\n        super().__init__()\n        assert vocab_size is not None\n        assert block_size is not None\n        self.block_size = block_size\n        self.vocab_size = vocab_size\n        self.n_layer = n_layer\n        self.pretrained = pretrained\n        self.test_cfg = Config(test_cfg)\n\n        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n\n        rope_cache = build_rope_cache(\n            seq_len=block_size,\n            n_elem=n_embd // n_head,\n            dtype=self.lm_head.weight.dtype)\n\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(vocab_size, n_embd),\n                h=nn.ModuleList([\n                    Block(n_embd, n_head, rope_cache) for _ in range(n_layer)\n                ]),\n                ln_f=RMSNorm(n_embd),\n            ))\n\n    def init_weights(self):\n        if self.pretrained is not None:\n            checkpoint = torch.load(self.pretrained)\n            self.load_state_dict(checkpoint, strict=False)\n            print_log(f'Load pretrained model from {self.pretrained}')\n\n\n    # def _init_weights(self, module: nn.Module) -> None:\n    #     if isinstance(module, nn.Linear):\n    #         torch.nn.init.normal_(\n    #             module.weight,\n    #             mean=0.0,\n    #             std=0.02 / math.sqrt(2 * self.n_layer))\n    #     elif isinstance(module, nn.Embedding):\n    #         torch.nn.init.normal_(\n    #             module.weight,\n    #             mean=0.0,\n    #             std=0.02 / math.sqrt(2 * self.n_layer))\n\n    def forward(self,\n                input_ids: torch.Tensor,\n                attention_mask=None,\n                labels=None,\n                mode='tensor') -> torch.Tensor:\n\n\n        if mode == 'tensor':\n            return self._forward(input_ids, attention_mask)\n        elif mode == 'loss':\n            return self.loss(input_ids, attention_mask, labels)\n        elif mode == 'predict':\n            return self.predict(input_ids)\n\n    def _forward(self, input_ids, attention_mask=None):\n        _, t = input_ids.size()\n        assert (\n            t <= self.block_size\n        ), f'Cannot forward sequence of length {t}, block size is only {self.block_size}'\n\n        # forward the LLaMA model itself\n        x = self.transformer.wte(\n            input_ids)  # token embeddings of shape (b, t, n_embd)\n\n        # TODO: prepare attn mask\n        if attention_mask is not None:\n            attention_mask = None\n\n        for block in self.transformer.h:\n            x = block(x, attention_mask)\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)  # (b, t, vocab_size)\n\n        return logits\n\n    def loss(self, input_ids, attention_mask, labels):\n        logits = self._forward(input_ids, attention_mask)\n        # Shift so that tokens < n predict n\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        # Flatten the tokens\n        loss_fct = nn.CrossEntropyLoss()\n        shift_logits = shift_logits.view(-1, self.vocab_size)\n        shift_labels = shift_labels.view(-1)\n        # Enable model parallelism\n        shift_labels = shift_labels.to(shift_logits.device)\n        loss = loss_fct(shift_logits, shift_labels)\n        return dict(loss=loss)\n\n    @torch.no_grad()\n    def predict(self, input_ids):\n        logits = self._forward(input_ids)\n        # create an empty tensor of the expected final shape and fill in the current tokens\n        B, T = input_ids.shape\n        T_new = T + self.test_cfg.max_new_tokens\n        empty = torch.empty(B, T_new, dtype=input_ids.dtype, device=input_ids.device)\n        empty[:, :T] = input_ids\n        input_ids = empty\n        max_seq_length = self.block_size\n\n        # generate max_new_tokens tokens\n        for t in range(T, T_new):\n            # ignore the not-filled-yet tokens\n            idx_cond = input_ids[:, :t]\n            # if the sequence context is growing too long we must crop it at max_seq_length\n            idx_cond = idx_cond if T <= max_seq_length else idx_cond[:, -max_seq_length:]\n\n            # forward\n            logits = self._forward(idx_cond)\n            logits = logits[:, -1] / self.test_cfg.temperature\n\n            # optionally crop the logits to only the top k options\n            if self.test_cfg.get('top_k', None) is not None:\n                v, _ = torch.topk(logits, min(self.test_cfg.top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n\n            # concatenate the new column\n            input_ids[:, t:] = idx_next\n\n        return input_ids", "\n\nllama_configs = {\n    'toy': dict(n_layer=1, n_head=1,\n                n_embd=128, block_size=1024,\n                vocab_size=32000, pretrained=None),  # for debug\n    '7B': dict(n_layer=32, n_head=32,\n               n_embd=4096, block_size=4096,\n               vocab_size=32000, pretrained='checkpoints/mm-llama/7B/state_dict.pth'),\n    '13B': dict(n_layer=40, n_head=40,", "               vocab_size=32000, pretrained='checkpoints/mm-llama/7B/state_dict.pth'),\n    '13B': dict(n_layer=40, n_head=40,\n                n_embd=5120, block_size=4096,\n                vocab_size=32000, pretrained='checkpoints/mm-llama/13B/state_dict.pth'),\n    '30B': dict(n_layer=60, n_head=52,\n                n_embd=6656, block_size=4096,\n                vocab_size=32000, pretrained='checkpoints/mm-llama/30B/state_dict.pth'),\n    '65B': dict(n_layer=80, n_head=64,\n                n_embd=8192, block_size=4096,\n                vocab_size=32000, pretrained='checkpoints/mm-llama/65B/state_dict.pth'),", "                n_embd=8192, block_size=4096,\n                vocab_size=32000, pretrained='checkpoints/mm-llama/65B/state_dict.pth'),\n}\n\n\n# TODO: mmengine support for partial\n# MODELS.register_module('LLaMA-toy', module=partial(LLaMA, **llama_configs['toy']))  # for debug\n# MODELS.register_module('LLaMA-7B', module=partial(LLaMA, **llama_configs['7B']))\n# MODELS.register_module('LLaMA-13B', module=partial(LLaMA, **llama_configs['13B']))\n# MODELS.register_module('LLaMA-30B', module=partial(LLaMA, **llama_configs['30B']))", "# MODELS.register_module('LLaMA-13B', module=partial(LLaMA, **llama_configs['13B']))\n# MODELS.register_module('LLaMA-30B', module=partial(LLaMA, **llama_configs['30B']))\n# MODELS.register_module('LLaMA-65B', module=partial(LLaMA, **llama_configs['65B']))\n\n\n@MODELS.register_module()\nclass LLaMAToy(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['toy'], **kwargs)\n", "\n@MODELS.register_module()\nclass LLaMA7B(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['7B'], **kwargs)\n\n\n@MODELS.register_module()\nclass LLaMA13B(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['13B'], **kwargs)", "class LLaMA13B(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['13B'], **kwargs)\n\n\n@MODELS.register_module()\nclass LLaMA30B(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['30B'], **kwargs)\n", "\n\n@MODELS.register_module()\nclass LLaMA65B(LLaMA):\n    def __init__(self, **kwargs):\n        super().__init__(**llama_configs['65B'], **kwargs)\n"]}
{"filename": "mmllama/models/__init__.py", "chunked_list": ["from .llama import LLaMA, llama_configs\nfrom .lora import LoRAModel\n\n__all__ = ['LLaMA', 'llama_configs', 'LoRAModel']\n"]}
{"filename": "mmllama/models/lora.py", "chunked_list": ["# Derived from https://github.com/microsoft/LoRA\n#  ------------------------------------------------------------------------------------------\n#  Copyright (c) Microsoft Corporation. All rights reserved.\n#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------\nimport math\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n", "from typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmengine.logging import print_log\nfrom mmengine.model import BaseModel\n\nimport mmllama.models.llama as llama\nfrom mmllama.registry import MODELS", "import mmllama.models.llama as llama\nfrom mmllama.registry import MODELS\n\n\nclass LoRALayer():\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights", "\n\nclass MergedLinear(nn.Linear, LoRALayer):\n    # LoRA implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.,\n        enable_lora: List[bool] = [False],\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        **kwargs\n    ):\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n                           merge_weights=merge_weights)\n        assert out_features % len(enable_lora) == 0, \\\n            'The length of enable_lora must divide out_features'\n        self.enable_lora = enable_lora\n        self.fan_in_fan_out = fan_in_fan_out\n        # Actual trainable parameters\n        if r > 0 and any(enable_lora):\n            self.lora_A = nn.Parameter(\n                self.weight.new_zeros((r * sum(enable_lora), in_features)))\n            self.lora_B = nn.Parameter(\n                self.weight.new_zeros((out_features // len(enable_lora) * sum(enable_lora), r))\n            ) # weights for Conv1D with groups=sum(enable_lora)\n            self.scaling = self.lora_alpha / self.r\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n            # Compute the indices\n            self.lora_ind = self.weight.new_zeros(\n                (out_features, ), dtype=torch.bool\n            ).view(len(enable_lora), -1)\n            self.lora_ind[enable_lora, :] = True\n            self.lora_ind = self.lora_ind.view(-1)\n        self.reset_parameters()\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, 'lora_A'):\n            # initialize A the same way as the default for nn.Linear and B to zero\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B)\n\n    def zero_pad(self, x):\n        result = x.new_zeros((*x.shape[:-1], self.out_features))\n        result = result.view(-1, self.out_features)\n        result[:, self.lora_ind] = x.reshape(\n            -1, self.out_features // len(self.enable_lora) * sum(self.enable_lora)\n        )\n        return result.view((*x.shape[:-1], self.out_features))\n\n    def train(self, mode: bool = True):\n        def T(w):\n            return w.T if self.fan_in_fan_out else w\n        nn.Linear.train(self, mode)\n        if self.merge_weights and self.merged:\n            # Make sure that the weights are not merged\n            if self.r > 0 and any(self.enable_lora):\n                delta_w = F.conv1d(\n                    self.lora_A.data.unsqueeze(0),\n                    self.lora_B.data.unsqueeze(-1),\n                    groups=sum(self.enable_lora)\n                ).squeeze(0)\n                self.weight.data -= self.zero_pad(T(delta_w * self.scaling))\n            self.merged = False\n\n    def eval(self):\n        def T(w):\n            return w.T if self.fan_in_fan_out else w\n        nn.Linear.eval(self)\n        if self.merge_weights and not self.merged:\n            # Merge the weights and mark it\n            if self.r > 0 and any(self.enable_lora):\n                delta_w = F.conv1d(\n                    self.lora_A.data.unsqueeze(0),\n                    self.lora_B.data.unsqueeze(-1),\n                    groups=sum(self.enable_lora)\n                ).squeeze(0)\n                self.weight.data += self.zero_pad(T(delta_w * self.scaling))\n            self.merged = True\n\n    def forward(self, x: torch.Tensor):\n        def T(w):\n            return w.T if self.fan_in_fan_out else w\n        if self.merged:\n            return F.linear(x, T(self.weight), bias=self.bias)\n        else:\n            result = F.linear(x, T(self.weight), bias=self.bias)\n            if self.r > 0:\n                after_A = F.linear(self.lora_dropout(x), self.lora_A)\n                after_B = F.conv1d(\n                    after_A.transpose(-2, -1),\n                    self.lora_B.unsqueeze(-1),\n                    groups=sum(self.enable_lora)\n                ).transpose(-2, -1)\n                result += self.zero_pad(after_B) * self.scaling\n            return result", "\n\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n    for n, p in model.named_parameters():\n        if 'lora_' not in n:\n            p.requires_grad = False\n    if bias == 'none':\n        return\n    elif bias == 'all':\n        for n, p in model.named_parameters():\n            if 'bias' in n:\n                p.requires_grad = True\n    elif bias == 'lora_only':\n        for m in model.modules():\n            if isinstance(m, LoRALayer) and \\\n                hasattr(m, 'bias') and \\\n                m.bias is not None:\n                    m.bias.requires_grad = True\n    else:\n        raise NotImplementedError", "\n\ndef lora_state_dict(model: nn.Module, bias: str = 'none') -> Dict[str, torch.Tensor]:\n    my_state_dict = model.state_dict()\n    if bias == 'none':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k}\n    elif bias == 'all':\n        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k or 'bias' in k}\n    elif bias == 'lora_only':\n        to_return = {}\n        for k in my_state_dict:\n            if 'lora_' in k:\n                to_return[k] = my_state_dict[k]\n                bias_name = k.split('lora_')[0]+'bias'\n                if bias_name in my_state_dict:\n                    to_return[bias_name] = my_state_dict[bias_name]\n        return to_return\n    else:\n        raise NotImplementedError", "\n\n@dataclass\nclass LoRAConfig:\n    r: float = 0.0\n    alpha: float = 1.0\n    dropout: float = 0.0\n\n\nclass CausalSelfAttention(llama.CausalSelfAttention):\n    lora_config = None\n\n    def __init__(self,  n_embd, n_head, rope_cache: torch.Tensor) -> None:\n        # Skip the parent class __init__ altogether and replace it to avoid\n        # useless allocations\n        nn.Module.__init__(self)\n        assert n_embd % n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = MergedLinear(\n            in_features=n_embd,\n            out_features=3 * n_embd,\n            r=self.lora_config.r,\n            lora_alpha=self.lora_config.alpha,\n            lora_dropout=self.lora_config.dropout,\n            enable_lora=[True, False, True],\n            fan_in_fan_out = False,\n            merge_weights=True,\n            bias=False)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n        # regularization\n        self.n_head = n_head\n        self.n_embd = n_embd\n        self.register_buffer('rope_cache', rope_cache, persistent=False)", "\nclass CausalSelfAttention(llama.CausalSelfAttention):\n    lora_config = None\n\n    def __init__(self,  n_embd, n_head, rope_cache: torch.Tensor) -> None:\n        # Skip the parent class __init__ altogether and replace it to avoid\n        # useless allocations\n        nn.Module.__init__(self)\n        assert n_embd % n_head == 0\n\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = MergedLinear(\n            in_features=n_embd,\n            out_features=3 * n_embd,\n            r=self.lora_config.r,\n            lora_alpha=self.lora_config.alpha,\n            lora_dropout=self.lora_config.dropout,\n            enable_lora=[True, False, True],\n            fan_in_fan_out = False,\n            merge_weights=True,\n            bias=False)\n        # output projection\n        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n        # regularization\n        self.n_head = n_head\n        self.n_embd = n_embd\n        self.register_buffer('rope_cache', rope_cache, persistent=False)", "\n\n@contextmanager\ndef lora(r, alpha, dropout, enabled: bool = True):\n    \"\"\"A context manager under which you can instantiate the model with\n    LoRA.\"\"\"\n    if not enabled:\n        yield\n        return\n\n    CausalSelfAttention.lora_config = LoRAConfig(r=r, alpha=alpha, dropout=dropout)\n\n    causal_self_attention = llama.CausalSelfAttention\n    llama.CausalSelfAttention = CausalSelfAttention\n    yield\n    llama.CausalSelfAttention = causal_self_attention\n\n    CausalSelfAttention.lora_config = None", "\n\n@MODELS.register_module()\nclass LoRAModel(BaseModel):\n    def __init__(self,\n                 model: dict,\n                 r: float,\n                 alpha: float,\n                 dropout: float):\n        super().__init__()\n        with lora(r=r, alpha=alpha, dropout=dropout, enabled=True):\n            print_log('Building model with LoRA', logger='current')\n            self.model = MODELS.build(model)\n            mark_only_lora_as_trainable(self.model)\n        self.data_preprocessor = self.model.data_preprocessor\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    def state_dict(self, *args, **kwargs):\n        return lora_state_dict(self.model)\n\n    def load_state_dict(self, *args, **kwargs):\n        return self.model.load_state_dict(*args, **kwargs)", ""]}
{"filename": "mmllama/datasets/prompter.py", "chunked_list": ["\"\"\"A dedicated helper to manage templates and prompt building.\n\nModified from https://github.com/tloen/alpaca-lora/tree/main/utils\n\"\"\"\n\nimport json\nimport os.path as osp\nfrom typing import Union\n\n\nclass Prompter(object):\n    __slots__ = ('template', '_verbose')\n\n    def __init__(self, template_name: str = '', verbose: bool = False):\n        self._verbose = verbose\n        if not template_name:\n            # Enforce the default here, so the constructor can be called with '' and will not break.\n            template_name = 'alpaca'\n        file_name = osp.join('templates', f'{template_name}.json')\n        if not osp.exists(file_name):\n            raise ValueError(f\"Can't read {file_name}\")\n        with open(file_name) as fp:\n            self.template = json.load(fp)\n        if self._verbose:\n            print(\n                f\"Using prompt template {template_name}: {self.template['description']}\"\n            )\n\n    def generate_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,\n        label: Union[None, str] = None,\n    ) -> str:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.template['prompt_input'].format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = self.template['prompt_no_input'].format(\n                instruction=instruction\n            )\n        if label:\n            res = f'{res}{label}'\n        if self._verbose:\n            print(res)\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(self.template['response_split'])[1].strip()", "\n\nclass Prompter(object):\n    __slots__ = ('template', '_verbose')\n\n    def __init__(self, template_name: str = '', verbose: bool = False):\n        self._verbose = verbose\n        if not template_name:\n            # Enforce the default here, so the constructor can be called with '' and will not break.\n            template_name = 'alpaca'\n        file_name = osp.join('templates', f'{template_name}.json')\n        if not osp.exists(file_name):\n            raise ValueError(f\"Can't read {file_name}\")\n        with open(file_name) as fp:\n            self.template = json.load(fp)\n        if self._verbose:\n            print(\n                f\"Using prompt template {template_name}: {self.template['description']}\"\n            )\n\n    def generate_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,\n        label: Union[None, str] = None,\n    ) -> str:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.template['prompt_input'].format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = self.template['prompt_no_input'].format(\n                instruction=instruction\n            )\n        if label:\n            res = f'{res}{label}'\n        if self._verbose:\n            print(res)\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(self.template['response_split'])[1].strip()", ""]}
{"filename": "mmllama/datasets/collate_fn.py", "chunked_list": ["import numpy as np\nfrom mmengine.registry import FUNCTIONS\n\nFUNCTIONS.register_module()\ndef seq2seq_collate(features, tokenizer):\n    # import ipdb; ipdb.set_trace()\n    return_tensors = 'pt'\n    label_pad_token_id: int = -100\n    pad_to_multiple_of = 8\n    labels = [feature['labels'] for feature in features] if 'labels' in features[0].keys() else None\n    # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n    # same length to return tensors.\n    if labels is not None:\n        max_label_length = max(len(l) for l in labels)\n        if pad_to_multiple_of is not None:\n            max_label_length = (\n                (max_label_length + pad_to_multiple_of - 1)\n                // pad_to_multiple_of\n                * pad_to_multiple_of\n            )\n\n        padding_side = tokenizer.padding_side\n        for feature in features:\n            remainder = [label_pad_token_id] * (max_label_length - len(feature['labels']))\n            if isinstance(feature['labels'], list):\n                feature['labels'] = (\n                    feature['labels'] + remainder if padding_side == 'right' else remainder + feature['labels']\n                )\n            elif padding_side == 'right':\n                feature['labels'] = np.concatenate([feature['labels'], remainder]).astype(np.int64)\n            else:\n                feature['labels'] = np.concatenate([remainder, feature['labels']]).astype(np.int64)\n    features = tokenizer.pad(\n        features,\n        padding=True,\n        max_length=None,\n        pad_to_multiple_of=pad_to_multiple_of,\n        return_tensors=return_tensors,\n    )\n\n    return features", ""]}
{"filename": "mmllama/datasets/__init__.py", "chunked_list": ["from .collate_fn import seq2seq_collate\nfrom .prompter import Prompter\n\n__all__ = ['Prompter', 'seq2seq_collate']\n"]}
{"filename": "configs/llama-7B_finetune_3e.py", "chunked_list": ["_base_ = [\n    './_base_/schedules/finetune-3e.py', './_base_/default_runtime.py'\n]\n\nmodel = dict(type='LoRAModel',\n             model=dict(type='LLaMA7B'),\n             r=8,\n             alpha=16,\n             dropout=0.05)\n", "             dropout=0.05)\n\ndata_path = 'yahma/alpaca-cleaned'\nval_set_size = 2000\ntokenizer_path = 'checkpoints/mm-llama/tokenizer.model'\n\ntrain_dataloader = dict(\n    batch_size=4,\n    num_workers=4,\n    persistent_workers=True,", "    num_workers=4,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    collate_fn=dict(type='seq2seq_collate'))\nval_dataloader = dict(\n    batch_size=4,\n    num_workers=4,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),", "    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    collate_fn=dict(type='seq2seq_collate'))\n\nval_evaluator = dict(type='DummyMetric')\n\noptim_wrapper = dict(\n    type='AmpOptimWrapper',\n    optimizer=dict(type='AdamW', lr=3e-4),\n    accumulative_counts=128//4  # TODO minibatch=4", "    optimizer=dict(type='AdamW', lr=3e-4),\n    accumulative_counts=128//4  # TODO minibatch=4\n    )\n# Default setting for scaling LR automatically\n#   - `enable` means enable scaling LR automatically\n#       or not by default.\n#   - `base_batch_size` = 4 samples per GPU).\nauto_scale_lr = dict(enable=False, base_batch_size=4)\n", ""]}
{"filename": "configs/_base_/default_runtime.py", "chunked_list": ["default_scope = 'mmllama'\n\ndefault_hooks = dict(\n    timer=dict(type='IterTimerHook'),\n    logger=dict(type='LoggerHook', interval=10),\n    param_scheduler=dict(type='ParamSchedulerHook'),\n    checkpoint=dict(type='CheckpointHook', interval=1),  # TODO\n    sampler_seed=dict(type='DistSamplerSeedHook'),\n    visualization=dict(type='NaiveVisualizationHook'))\n", "    visualization=dict(type='NaiveVisualizationHook'))\n\nenv_cfg = dict(\n    cudnn_benchmark=False,\n    mp_cfg=dict(mp_start_method='fork'),\n    dist_cfg=dict(backend='nccl'),\n)\n\nvis_backends = [dict(type='LocalVisBackend'), dict(type='TensorboardVisBackend')]\nvisualizer = dict(", "vis_backends = [dict(type='LocalVisBackend'), dict(type='TensorboardVisBackend')]\nvisualizer = dict(\n    type='Visualizer', vis_backends=vis_backends, name='visualizer')\nlog_processor = dict(type='LogProcessor', window_size=10, by_epoch=True)\n\nlog_level = 'INFO'\nload_from = None\nresume = False\n", ""]}
{"filename": "configs/_base_/schedules/finetune-3e.py", "chunked_list": ["# training schedule for 1x\ntrain_cfg = dict(type='EpochBasedTrainLoop', max_epochs=3, val_interval=10)\nval_cfg = dict(type='ValLoop')\n\n# learning rate\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=100),\n    dict(\n        type='LinearLR',", "    dict(\n        type='LinearLR',\n        begin=0,\n        end=3,\n        by_epoch=True,\n        start_factor=1.0,\n        end_factor=0.0,\n        convert_to_iter_based=True\n        )\n]", "        )\n]\n\n# optimizer\noptim_wrapper = dict(\n    type='AmpOptimWrapper',\n    optimizer=dict(type='AdamW', lr=3e-4),\n    clip_grad=dict(max_norm=1.0, norm_type=2),\n    )\n", "    )\n"]}
