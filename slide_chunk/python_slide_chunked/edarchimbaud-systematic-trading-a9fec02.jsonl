{"filename": "systematic_trading/helpers.py", "chunked_list": ["import json\nimport subprocess\nimport time\n\nimport requests\nfrom requests.exceptions import ConnectionError, HTTPError, ReadTimeout\nfrom requests.models import Response\n\n\ndef is_valid_json(data):\n    try:\n        json.loads(data)\n        return True\n    except json.JSONDecodeError:\n        return False", "\ndef is_valid_json(data):\n    try:\n        json.loads(data)\n        return True\n    except json.JSONDecodeError:\n        return False\n\n\ndef nasdaq_headers():\n    return {\n        \"authority\": \"api.nasdaq.com\",\n        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n        \"accept-language\": \"en-US,en;q=0.5\",\n        \"cache-control\": \"max-age=0\",\n        \"sec-ch-ua\": '\"Brave\";v=\"113\", \"Chromium\";v=\"113\", \"Not-A.Brand\";v=\"24\"',\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": '\"macOS\"',\n        \"sec-fetch-dest\": \"document\",\n        \"sec-fetch-mode\": \"navigate\",\n        \"sec-fetch-site\": \"none\",\n        \"sec-fetch-user\": \"?1\",\n        \"sec-gpc\": \"1\",\n        \"upgrade-insecure-requests\": \"1\",\n        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n    }", "\ndef nasdaq_headers():\n    return {\n        \"authority\": \"api.nasdaq.com\",\n        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n        \"accept-language\": \"en-US,en;q=0.5\",\n        \"cache-control\": \"max-age=0\",\n        \"sec-ch-ua\": '\"Brave\";v=\"113\", \"Chromium\";v=\"113\", \"Not-A.Brand\";v=\"24\"',\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": '\"macOS\"',\n        \"sec-fetch-dest\": \"document\",\n        \"sec-fetch-mode\": \"navigate\",\n        \"sec-fetch-site\": \"none\",\n        \"sec-fetch-user\": \"?1\",\n        \"sec-gpc\": \"1\",\n        \"upgrade-insecure-requests\": \"1\",\n        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n    }", "\n\ndef retry_get(\n    url,\n    headers={\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\",\n    },\n    retries=10,\n    delay=300,\n    mode=\"default\",\n):\n    if mode == \"curl\":\n        curl_headers = []\n        for k, v in headers.items():\n            curl_headers += [\"-H\", f\"{k}: {v}\"]\n        curl_command = [\n            \"curl\",\n            url,\n            *curl_headers,\n            \"--compressed\",\n        ]\n        for _ in range(retries):\n            result = subprocess.run(curl_command, capture_output=True, text=True)\n            content = result.stdout\n            if result.returncode == 0 and is_valid_json(content):\n                response = Response()\n                response.status_code = 200\n                response._content = content.encode(\"utf-8\")\n                return response\n            else:\n                print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n                time.sleep(delay)\n        raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")\n    for _ in range(retries):\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()  # Raise an exception for 4xx/5xx status codes\n            return response\n        except (ConnectionError, HTTPError, ReadTimeout):\n            print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n            time.sleep(delay)\n    raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")", ""]}
{"filename": "systematic_trading/strategies/momentum.py", "chunked_list": ["from datetime import datetime\nfrom dateutil import relativedelta\nimport os\n\nimport backtrader as bt\nimport backtrader.feeds as btfeeds\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd", "import numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\n\n\nclass MomentumStrategy(bt.Strategy):\n    \"\"\"\n    A momentum strategy that goes long the top quantile of stocks\n    and short the bottom quantile of stocks.\n    \"\"\"\n\n    params = (\n        (\"long_quantile\", 0.8),  # Long quantile threshold (e.g., top 80%)\n        (\"short_quantile\", 0.2),  # Short quantile threshold (e.g., bottom 20%)\n    )\n\n    def __init__(self):\n        self.ret = np.zeros(len(self.datas))\n\n    def log(self, txt, dt=None):\n        dt = dt or self.datas[0].datetime.date(0)\n        print(f\"{dt.isoformat()},{txt}\")\n\n    def is_first_business_day(self, dt=None):\n        dt = dt or self.datas[0].datetime.date(0)\n        first_day = dt.replace(day=1)\n        first_business_day = pd.date_range(first_day, periods=1, freq=\"BMS\")[0]\n        return dt == first_business_day.date()\n\n    def next(self):\n        \"\"\"\n        Execute trades based on the momentum strategy.\n        \"\"\"\n        if not self.is_first_business_day():\n            return\n\n        # Calculate returns for all stocks\n        self.ret = np.array(\n            [\n                (d.close[-20] / d.close[-252] - 1 if len(d) > 252 else np.NaN)\n                for d in self.datas\n            ]\n        )\n        self.log(self.broker.getvalue())\n\n        # Count the number of stocks that have a valid momentum predictor\n        num_stocks = np.count_nonzero(~np.isnan(self.ret))\n\n        # Compute the quantile thresholds\n        long_threshold = np.nanquantile(self.ret, self.params.long_quantile)\n        short_threshold = np.nanquantile(self.ret, self.params.short_quantile)\n\n        for i, d in enumerate(self.datas):\n            if self.ret[i] > long_threshold:  # Long the top quantile stocks\n                self.order_target_percent(\n                    data=d,\n                    target=0.7 / num_stocks,\n                )\n            elif self.ret[i] < short_threshold:  # Short the bottom quantile stocks\n                self.order_target_percent(\n                    data=d,\n                    target=-0.7 / num_stocks,\n                )\n            else:  # Close positions that don't meet the long or short criteria\n                self.close(data=d)", "\n\nclass CashNav(bt.analyzers.Analyzer):\n    \"\"\"\n    Analyzer returning cash and market values\n    \"\"\"\n\n    def create_analysis(self):\n        self.rets = {}\n        self.vals = 0.0\n\n    def notify_cashvalue(self, cash, value):\n        self.vals = (\n            self.strategy.datetime.datetime(),\n            cash,\n            value,\n        )\n        self.rets[len(self)] = self.vals\n\n    def get_analysis(self):\n        return self.rets", "\n\ndef main():\n    # Load Data\n    path = os.path.join(\"/tmp\", \"momentum.pkl\")\n    if os.path.exists(path):\n        df = pickle.load(open(path, \"rb\"))\n    else:\n        dataset = load_dataset(\"edarchimbaud/timeseries-daily-sp500\", split=\"train\")\n        df = pd.DataFrame(dataset)\n        pickle.dump(df, open(path, \"wb\"))\n\n    # Data Preparation\n    symbols = df[\"symbol\"].unique()\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n\n    # Create a Cerebro object\n    cerebro = bt.Cerebro()\n    cerebro.broker.set_cash(1000000)\n\n    starting_date = datetime(1990, 1, 1)\n\n    # Add Data Feeds to Cerebro\n    for _, symbol in enumerate(tqdm(symbols)):\n        df_symbol = df[df[\"symbol\"] == symbol].copy()\n        if df_symbol[\"date\"].min() > starting_date:\n            continue\n        factor = df_symbol[\"adj_close\"] / df_symbol[\"close\"]\n        df_symbol[\"open\"] = df_symbol[\"open\"] * factor\n        df_symbol[\"high\"] = df_symbol[\"high\"] * factor\n        df_symbol[\"low\"] = df_symbol[\"low\"] * factor\n        df_symbol[\"close\"] = df_symbol[\"close\"] * factor\n        df_symbol.drop([\"symbol\", \"adj_close\"], axis=1, inplace=True)\n        df_symbol.set_index(\"date\", inplace=True)\n        data = btfeeds.PandasData(dataname=df_symbol)\n        cerebro.adddata(data, name=symbol)\n\n    # Add Strategy to Cerebro\n    cerebro.addstrategy(\n        MomentumStrategy, long_quantile=0.8, short_quantile=0.2\n    )  # Adjust parameters as desired\n\n    cerebro.addanalyzer(CashNav, _name=\"cash_nav\")\n\n    # Run the Strategy\n    results = cerebro.run()\n    print(\"Final Portfolio Value: %.2f\" % cerebro.broker.getvalue())\n\n    dictionary = results[0].analyzers.getbyname(\"cash_nav\").get_analysis()\n    df = pd.DataFrame(dictionary).T\n    df.columns = [\"Date\", \"Cash\", \"Nav\"]\n    df.set_index(\"Date\", inplace=True)\n    df.loc[df.index >= starting_date, [\"Nav\"]].plot()\n    plt.show()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/features/__main__.py", "chunked_list": ["from datetime import date, timedelta\n\nimport click\n\nfrom systematic_trading.features.predictors.predictors_monthly import PredictorsMonthly\nfrom systematic_trading.features.targets.targets_monthly import TargetsMonthly\n\n\n@click.command()\n@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\ndef main(suffix: str, username: str):\n    tag_date = date.today() - timedelta(days=3)\n    print(\"Updating feature and target datasets...\")\n    features = {\n        \"predictors-monthly-stocks\": PredictorsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n        \"targets-monthly-stocks\": TargetsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n    }\n    for name in features:\n        features[name].set_dataset_df()\n        features[name].to_hf_datasets()", "@click.command()\n@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\ndef main(suffix: str, username: str):\n    tag_date = date.today() - timedelta(days=3)\n    print(\"Updating feature and target datasets...\")\n    features = {\n        \"predictors-monthly-stocks\": PredictorsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n        \"targets-monthly-stocks\": TargetsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n    }\n    for name in features:\n        features[name].set_dataset_df()\n        features[name].to_hf_datasets()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/features/targets/targets_monthly.py", "chunked_list": ["from datetime import date\n\nfrom datasets import load_dataset\nimport numpy as np\nimport pandas as pd\n\nfrom systematic_trading.datasets.dataset import Dataset\n\n\nclass TargetsMonthly(Dataset):\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"targets-monthly-{self.suffix}\"\n        self.expected_columns = [\"symbol\", \"date\", \"return\", \"return_quintile\"]\n\n    def __return_to_quintile(self, returns_arr):\n        # I am allowed to use the future to calculate the percentiles\n        percentiles = np.percentile(returns_arr, [20, 40, 60, 80])\n        quintile_id = []\n        for ret in returns_arr:\n            if ret <= percentiles[0]:\n                quintile_id.append(-2)\n            elif ret <= percentiles[1]:\n                quintile_id.append(-1)\n            elif ret <= percentiles[2]:\n                quintile_id.append(0)\n            elif ret <= percentiles[3]:\n                quintile_id.append(1)\n            else:\n                quintile_id.append(2)\n        return quintile_id\n\n    def set_dataset_df(self):\n        \"\"\"\n        Compute the dataset.\n        \"\"\"\n        timeseries_daily_df = pd.DataFrame(\n            load_dataset(\n                f\"{self.username}/timeseries-daily-{self.suffix}\",\n                revision=self.tag_date.isoformat(),\n                split=\"train\",\n            ),\n        )\n        timeseries_daily_df[\"date\"] = pd.to_datetime(timeseries_daily_df[\"date\"])\n        timeseries_daily_df.set_index(\"date\", inplace=True)\n        # Cross-sectional returns\n        monthly_df = (\n            timeseries_daily_df.groupby(\"symbol\")[\"close\"]\n            .resample(\"M\")\n            .last()\n            .pct_change()\n            .shift(-1)\n        )\n        monthly_df = monthly_df.reset_index(level=[\"symbol\", \"date\"]).dropna()\n        monthly_df.rename(columns={\"close\": \"return\"}, inplace=True)\n        monthly_df[\"return_quintile\"] = monthly_df.groupby(\"date\")[\"return\"].transform(\n            lambda x: pd.qcut(x, 5, labels=False)\n        )\n        monthly_df.reset_index(drop=True, inplace=True)\n        self.dataset_df = monthly_df", "\nclass TargetsMonthly(Dataset):\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"targets-monthly-{self.suffix}\"\n        self.expected_columns = [\"symbol\", \"date\", \"return\", \"return_quintile\"]\n\n    def __return_to_quintile(self, returns_arr):\n        # I am allowed to use the future to calculate the percentiles\n        percentiles = np.percentile(returns_arr, [20, 40, 60, 80])\n        quintile_id = []\n        for ret in returns_arr:\n            if ret <= percentiles[0]:\n                quintile_id.append(-2)\n            elif ret <= percentiles[1]:\n                quintile_id.append(-1)\n            elif ret <= percentiles[2]:\n                quintile_id.append(0)\n            elif ret <= percentiles[3]:\n                quintile_id.append(1)\n            else:\n                quintile_id.append(2)\n        return quintile_id\n\n    def set_dataset_df(self):\n        \"\"\"\n        Compute the dataset.\n        \"\"\"\n        timeseries_daily_df = pd.DataFrame(\n            load_dataset(\n                f\"{self.username}/timeseries-daily-{self.suffix}\",\n                revision=self.tag_date.isoformat(),\n                split=\"train\",\n            ),\n        )\n        timeseries_daily_df[\"date\"] = pd.to_datetime(timeseries_daily_df[\"date\"])\n        timeseries_daily_df.set_index(\"date\", inplace=True)\n        # Cross-sectional returns\n        monthly_df = (\n            timeseries_daily_df.groupby(\"symbol\")[\"close\"]\n            .resample(\"M\")\n            .last()\n            .pct_change()\n            .shift(-1)\n        )\n        monthly_df = monthly_df.reset_index(level=[\"symbol\", \"date\"]).dropna()\n        monthly_df.rename(columns={\"close\": \"return\"}, inplace=True)\n        monthly_df[\"return_quintile\"] = monthly_df.groupby(\"date\")[\"return\"].transform(\n            lambda x: pd.qcut(x, 5, labels=False)\n        )\n        monthly_df.reset_index(drop=True, inplace=True)\n        self.dataset_df = monthly_df", ""]}
{"filename": "systematic_trading/features/predictors/__main__.py", "chunked_list": ["from datetime import date, timedelta\n\nimport click\n\nfrom systematic_trading.features.predictors.predictors_monthly import PredictorsMonthly\nfrom systematic_trading.features.targets.targets_monthly import TargetsMonthly\n\n\n@click.command()\n@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\ndef main(suffix: str, username: str):\n    tag_date = date.today() - timedelta(days=3)\n    print(\"Updating feature and target datasets...\")\n    features = {\n        \"predictors-monthly-stocks\": PredictorsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n        \"targets-monthly-stocks\": TargetsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n    }\n    for name in features:\n        features[name].set_dataset_df()\n        features[name].to_hf_datasets()", "@click.command()\n@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\ndef main(suffix: str, username: str):\n    tag_date = date.today() - timedelta(days=3)\n    print(\"Updating feature and target datasets...\")\n    features = {\n        \"predictors-monthly-stocks\": PredictorsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n        \"targets-monthly-stocks\": TargetsMonthly(\n            suffix=\"stocks\", tag_date=tag_date, username=username\n        ),\n    }\n    for name in features:\n        features[name].set_dataset_df()\n        features[name].to_hf_datasets()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/features/predictors/predictors_monthly.py", "chunked_list": ["from datetime import date\nimport os\n\nfrom datasets import load_dataset\nfrom numba import jit\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom systematic_trading.datasets.dataset import Dataset\nfrom systematic_trading.datasets.predictors.estimators.slope import (\n    bayesian_slope,\n    linear_regression_slope,\n    median_of_local_slopes,\n    median_of_progressive_slopes,\n    barycentre_of_progressive_slopes,\n)", "    barycentre_of_progressive_slopes,\n)\n\n\nclass SignalsMonthly(Dataset):\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"signals-monthly-{self.suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"bayesian_slope_12m\",\n            \"linear_regression_slope_12m\",\n            \"median_of_progressive_slopes_12m\",\n            \"median_of_local_slopes_12m\",\n            \"barycentre_of_progressive_slopes_12m\",\n            \"bayesian_slope_12m_quintile\",\n            \"linear_regression_slope_12m_quintile\",\n            \"median_of_progressive_slopes_12m_quintile\",\n            \"median_of_local_slopes_12m_quintile\",\n            \"barycentre_of_progressive_slopes_12m_quintile\",\n        ]\n\n    def set_dataset_df(self):\n        path = os.path.join(os.getenv(\"HOME\"), \"Downloads\", \"timeseries_daily_df.pkl\")\n        if os.path.exists(path):\n            with open(path, \"rb\") as handler:\n                timeseries_daily_df = pickle.load(handler)\n        else:\n            timeseries_daily_df = pd.DataFrame(\n                load_dataset(\n                    f\"{self.username}/timeseries-daily-{self.suffix}\",\n                    revision=self.tag_date.isoformat(),\n                    split=\"train\",\n                ),\n            )\n            with open(path, \"wb\") as handler:\n                pickle.dump(timeseries_daily_df, handler)\n        timeseries_daily_df[\"date\"] = pd.to_datetime(timeseries_daily_df[\"date\"])\n        timeseries_daily_df.set_index(\"date\", inplace=True)\n        BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M = \"barycentre_of_progressive_slopes_12m\"\n        signals = {\n            \"bayesian_slope_12m\": bayesian_slope,\n            \"linear_regression_slope_12m\": linear_regression_slope,\n            \"median_of_progressive_slopes_12m\": median_of_progressive_slopes,\n            \"median_of_local_slopes_12m\": median_of_local_slopes,\n            BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M: \"custom\",\n        }\n        frames = []\n        for signal_name, signal in tqdm(signals.items()):\n            if signal != \"custom\":\n                monthly_df = (\n                    timeseries_daily_df.groupby(\"symbol\")[\"close\"]\n                    .resample(\"M\")\n                    .last()\n                    .rolling(window=12)\n                    .apply(signal)\n                )\n                monthly_df.rename(signal_name, inplace=True)\n            elif signal_name == BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M:\n                monthly_close_df = (\n                    timeseries_daily_df.groupby(\"symbol\")[\"close\"].resample(\"M\").last()\n                )\n                monthly_volume_df = (\n                    timeseries_daily_df.groupby(\"symbol\")[\"volume\"].resample(\"M\").sum()\n                )\n                monthly_df = pd.concat([monthly_close_df, monthly_volume_df], axis=1)\n                monthly_df = monthly_df.rolling(window=12, method=\"table\").apply(\n                    barycentre_of_progressive_slopes,\n                    raw=True,\n                    engine=\"numba\",\n                )[[\"close\"]]\n                monthly_df.rename(\n                    columns={\"close\": BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M},\n                    inplace=True,\n                )\n            frames.append(monthly_df)\n        monthly_df = pd.concat(frames, axis=1)\n        monthly_df = monthly_df.reset_index(level=[\"symbol\", \"date\"]).dropna()\n        for signal_name in signals.keys():\n            monthly_df[f\"{signal_name}_quintile\"] = monthly_df.groupby(\"date\")[\n                signal_name\n            ].transform(lambda x: pd.qcut(x, 5, labels=False))\n        monthly_df.reset_index(drop=True, inplace=True)\n        self.dataset_df = monthly_df", ""]}
{"filename": "systematic_trading/features/predictors/estimators/slope.py", "chunked_list": ["from numba import jit\nimport numpy as np\n\n\ndef bayesian_slope(x) -> float:\n    \"\"\"\n    Bayesian slope: (S(12)-S(1)) / S(1)\n    \"\"\"\n    if len(x) < 12:\n        return np.NaN\n    else:\n        return (x[-1] - x[0]) / 12 / np.median(x)", "\n\ndef linear_regression_slope(x) -> float:\n    \"\"\"\n    Slope of the linear regression: slope(close(1), ..., close(12))\n    \"\"\"\n    if len(x) < 12:\n        return np.NaN\n    else:\n        return np.polyfit(range(12), x, 1)[0] / np.median(x)", "\n\ndef median_of_local_slopes(x) -> float:\n    \"\"\"\n    Median of local slopes: median(close(12)-close(11), close(11)-close(10), ..., close(2)-close(1))\n    \"\"\"\n    if len(x) < 12:\n        return np.NaN\n    else:\n        return np.median(np.diff(x)) / np.median(x)", "\n\ndef median_of_progressive_slopes(x) -> float:\n    \"\"\"\n    Median of progressive slopes: median(close(2)-close(1), (close(3)-close(1)) / 2, ..., (close(12)-close(1)) / 11)\n    \"\"\"\n    if len(x) < 12:\n        return np.NaN\n    else:\n        return np.median((x[1:] - x[0]) / np.arange(1, 12)) / np.median(x)", "\n\n@jit(nopython=True)\ndef barycentre_of_progressive_slopes(x) -> float:\n    \"\"\"\n    Barycentre of progressive slopes: sum(volume(n)*(close(n)-close(1)/n)/sum(volume(n))\n    \"\"\"\n    if len(x) < 12:\n        return np.NaN\n    else:\n        returns = (x[1:, 0] - x[0, 0]) / np.arange(1, 12) / np.median(x[:, 0])\n        volumes = x[1:, 1]\n        return np.sum(returns / volumes) / np.sum(1 / volumes)", ""]}
{"filename": "systematic_trading/models/momentum.py", "chunked_list": ["import click\nfrom datasets import load_dataset\nimport ffn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score", "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\ndef train_test_split_v2(X, y, dt=None):\n    if dt is None:\n        return train_test_split(X, y, test_size=0.5, random_state=42)\n    X_train = X.index.get_level_values(\"date\") < dt\n    y_train = y.index.get_level_values(\"date\") < dt\n    return X.loc[X_train, :], X.loc[~X_train, :], y.loc[y_train, :], y.loc[~y_train, :]", "\n\n@click.command()\n@click.option(\n    \"--test_start_date\", help=\"Starting date of the out of sample (yyyy-mm-dd)\"\n)\ndef main(test_start_date: str):\n    predictors_df = pd.DataFrame(\n        load_dataset(\"edarchimbaud/signals-monthly-sp500\", split=\"train\")\n    )\n    targets_df = pd.DataFrame(\n        load_dataset(\"edarchimbaud/targets-monthly-sp500\", split=\"train\")\n    )\n    print(predictors_df.iloc[0, :])\n    print(targets_df.iloc[0, :])\n\n    # merging the two dataframes on the 'symbol' and 'date' columns\n    merged_df = pd.merge(predictors_df, targets_df, on=[\"symbol\", \"date\"])\n    merged_df.set_index([\"symbol\", \"date\"], inplace=True)\n\n    # filter the merged dataframe to include only \"*quintile\" columns from the first dataset\n    quintile_cols = [\n        col\n        for col in merged_df.columns\n        if \"quintile\" in col and col != \"return_quintile\"\n    ]\n    X = merged_df[quintile_cols]\n\n    # target variable is 'return_quintile' from the second dataset\n    y = merged_df[[\"return_quintile\", \"return\"]]\n\n    # split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split_v2(X, y, dt=test_start_date)\n\n    # use a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train.return_quintile)\n\n    # Predict the 'return_quintile' for the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test.return_quintile, y_pred)\n    precision = precision_score(\n        y_test.return_quintile, y_pred, average=\"weighted\"\n    )  # you can choose another averaging method if you prefer\n    recall = recall_score(y_test.return_quintile, y_pred, average=\"weighted\")\n    f1 = f1_score(y_test.return_quintile, y_pred, average=\"weighted\")\n\n    # Print metrics\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F1 Score: {f1}\")\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test.return_quintile, y_pred)\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Truth\")\n    plt.show()\n\n    # Plot cumulative returns\n    fig = plt.figure(figsize=(10, 7))\n    for n in range(5):\n        y_test.loc[y_pred == n, \"return\"].groupby(\"date\").mean().cumsum().plot()\n    plt.legend([f\"Predicted {n}\" for n in range(5)])\n    plt.show()\n\n    # Plot cumulative returns for long and short positions\n    fig = plt.figure(figsize=(10, 7))\n    df = pd.concat(\n        [\n            y_test.loc[y_pred == 4, \"return\"].groupby(\"date\").mean(),\n            y_test.loc[y_pred == 1, \"return\"].groupby(\"date\").mean(),\n        ],\n        axis=1,\n        keys=[\"long\", \"short\"],\n    )\n    df = df.fillna(0)\n    df[\"return\"] = df[\"long\"] - df[\"short\"]\n    df[\"return\"].cumsum().plot()\n    plt.show()\n\n    # Display performance metrics\n    nav = (df[\"return\"] + 1).cumprod()\n    stats = nav.calc_stats()\n    stats.display()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract_crawler.py", "chunked_list": ["from collections import Counter\nimport os\nimport re\nimport time\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options", "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport textract\nfrom tqdm import tqdm\nfrom webdriver_manager.chrome import ChromeDriverManager\n\nfrom systematic_trading.strategy_ideas.ssrn_abstract import SsrnAbstract\n\n\nclass SsrnAbstractCrawler:\n    def __init__(\n        self,\n        kili_project_id: Optional[str] = None,\n        is_strategy: Optional[str] = None,\n    ):\n        self.kili_project_id = kili_project_id\n        self.is_strategy = is_strategy\n        self._driver = None\n\n    def __from_url(self, url: str):\n        return int(url.split(\"=\")[-1])\n\n    def __download_and_save_to_kili(self, abstract_ids: list):\n        for abstract_id in tqdm(abstract_ids):\n            abstract = SsrnAbstract(abstract_id)\n            if (\n                abstract.exists_in_kili(self.kili_project_id)\n                or not abstract.exists_in_ssrn()\n            ):\n                continue\n            abstract.from_ssrn()\n            if self.is_strategy is not None:\n                abstract.is_strategy = self.is_strategy\n            abstract.to_kili(self.kili_project_id)\n\n    def __go_to_page(self, page: int):\n        self._driver.find_element(\n            \"xpath\", '//input[@aria-label=\"Go to page\"]'\n        ).send_keys(page)\n        self._driver.find_element(\"xpath\", '//button[@aria-label=\"Go\"]').click()\n\n    def from_jel_code(self, jel_code: str, from_page: int = 1):\n        \"\"\"\n        List all abstract ids from SSRN\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        options = Options()\n        options.headless = True\n        options.add_argument(\"user-agent=\" + headers[\"User-Agent\"])\n        self._driver = webdriver.Chrome(\n            ChromeDriverManager().install(), options=options\n        )\n        self._driver.get(\"https://papers.ssrn.com/sol3/DisplayAbstractSearch.cfm\")\n        self._driver.find_element_by_id(\"onetrust-accept-btn-handler\").click()\n        self._driver.find_element_by_id(\"advanced_search\").send_keys(jel_code)\n        self._driver.find_element_by_id(\"srchCrit2\").find_element_by_xpath(\"..\").click()\n        self._driver.find_element(\n            \"xpath\", '//button[contains(@class, \"primary\")]'\n        ).click()\n        for page in range(from_page, 200 + 1):\n            self.__go_to_page(page)\n            print(f\"{page} / 200\")\n            body = self._driver.find_element(\"xpath\", \"//body\")\n            body_html = body.get_attribute(\"innerHTML\")\n            soup = BeautifulSoup(body_html, \"html.parser\")\n            a_tags = soup.find_all(\"a\", {\"class\": \"title optClickTitle\"})\n            abstract_ids = [self.__from_url(a_tag[\"href\"]) for a_tag in a_tags]\n            abstract_ids = list(set(abstract_ids))\n            time.sleep(10)\n            self.__download_and_save_to_kili(abstract_ids)", "\nclass SsrnAbstractCrawler:\n    def __init__(\n        self,\n        kili_project_id: Optional[str] = None,\n        is_strategy: Optional[str] = None,\n    ):\n        self.kili_project_id = kili_project_id\n        self.is_strategy = is_strategy\n        self._driver = None\n\n    def __from_url(self, url: str):\n        return int(url.split(\"=\")[-1])\n\n    def __download_and_save_to_kili(self, abstract_ids: list):\n        for abstract_id in tqdm(abstract_ids):\n            abstract = SsrnAbstract(abstract_id)\n            if (\n                abstract.exists_in_kili(self.kili_project_id)\n                or not abstract.exists_in_ssrn()\n            ):\n                continue\n            abstract.from_ssrn()\n            if self.is_strategy is not None:\n                abstract.is_strategy = self.is_strategy\n            abstract.to_kili(self.kili_project_id)\n\n    def __go_to_page(self, page: int):\n        self._driver.find_element(\n            \"xpath\", '//input[@aria-label=\"Go to page\"]'\n        ).send_keys(page)\n        self._driver.find_element(\"xpath\", '//button[@aria-label=\"Go\"]').click()\n\n    def from_jel_code(self, jel_code: str, from_page: int = 1):\n        \"\"\"\n        List all abstract ids from SSRN\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        options = Options()\n        options.headless = True\n        options.add_argument(\"user-agent=\" + headers[\"User-Agent\"])\n        self._driver = webdriver.Chrome(\n            ChromeDriverManager().install(), options=options\n        )\n        self._driver.get(\"https://papers.ssrn.com/sol3/DisplayAbstractSearch.cfm\")\n        self._driver.find_element_by_id(\"onetrust-accept-btn-handler\").click()\n        self._driver.find_element_by_id(\"advanced_search\").send_keys(jel_code)\n        self._driver.find_element_by_id(\"srchCrit2\").find_element_by_xpath(\"..\").click()\n        self._driver.find_element(\n            \"xpath\", '//button[contains(@class, \"primary\")]'\n        ).click()\n        for page in range(from_page, 200 + 1):\n            self.__go_to_page(page)\n            print(f\"{page} / 200\")\n            body = self._driver.find_element(\"xpath\", \"//body\")\n            body_html = body.get_attribute(\"innerHTML\")\n            soup = BeautifulSoup(body_html, \"html.parser\")\n            a_tags = soup.find_all(\"a\", {\"class\": \"title optClickTitle\"})\n            abstract_ids = [self.__from_url(a_tag[\"href\"]) for a_tag in a_tags]\n            abstract_ids = list(set(abstract_ids))\n            time.sleep(10)\n            self.__download_and_save_to_kili(abstract_ids)", ""]}
{"filename": "systematic_trading/strategy_ideas/__main__.py", "chunked_list": ["import click\n\nfrom systematic_trading.strategy_ideas.ssrn_abstract_crawler import SsrnAbstractCrawler\nfrom systematic_trading.strategy_ideas.ssrn_paper_crawler import SsrnPaperCrawler\nfrom systematic_trading.strategy_ideas.ssrn_paper_summary_crawler import (\n    SsrnPaperSummaryCrawler,\n)\n\n\n@click.command()", "\n@click.command()\n@click.option(\"--mode\")\n@click.option(\"--kili-project-id\", help=\"Kili project id to save the data\")\n@click.option(\"--from-page\", default=1, help=\"Starting from 1\")\n@click.option(\"--jel-code\", default=\"G14\", help=\"JEL code: G14, G12, G11\")\n@click.option(\n    \"--src-kili-project-id\", default=\"\", help=\"Kili project id to read the data\"\n)\n@click.option(", ")\n@click.option(\n    \"--tgt-folder\", default=\"data/summaries\", help=\"Folder to save the summaries\"\n)\ndef main(\n    mode: str,\n    kili_project_id: str,\n    from_page: int,\n    jel_code: str,\n    src_kili_project_id: str,\n    target_folder: str,\n):\n    \"\"\"\n    Main entrypoint.\n    \"\"\"\n    if mode == \"abstract\":\n        SsrnAbstractCrawler(kili_project_id=kili_project_id).from_jel_code(\n            jel_code,\n            from_page,\n        )\n    elif mode == \"paper\":\n        SsrnPaperCrawler(tgt_kili_project_id=kili_project_id).from_kili(\n            src_kili_project_id=src_kili_project_id,\n        )\n    elif mode == \"summary\":\n        ssrn_paper_summarizer = SsrnPaperSummarizer()\n        ssrn_paper_summarizer.predict(\n            kili_project_id,\n            target_folder,\n        )", "\n\nif __name__ == \"__main__\":\n    main()  # pylint: disable=no-value-for-parameter\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper_summarizer.py", "chunked_list": ["\"\"\"\nSSRN Paper Summarizer.\n\"\"\"\nimport os\nimport json\nfrom pprint import pprint\n\nfrom kili.client import Kili\nfrom langchain.prompts import (\n    ChatPromptTemplate,", "from langchain.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom systematic_trading.strategy_ideas.ssrn_abstract import SsrnAbstract\nfrom systematic_trading.strategy_ideas.ssrn_strategy import SsrnStrategy", "from systematic_trading.strategy_ideas.ssrn_abstract import SsrnAbstract\nfrom systematic_trading.strategy_ideas.ssrn_strategy import SsrnStrategy\n\n\nclass SsrnPaperSummarizer:\n    \"\"\"\n    SSRN Paper Summarizer.\n    \"\"\"\n\n    def __init__(self):\n        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n        self._openai_client = ChatOpenAI(\n            model_name=\"gpt-3.5-turbo\",\n            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n            temperature=0,\n        )\n\n    def __parse_label(self, asset):\n        label = asset[\"labels\"][-1][\"jsonResponse\"]\n        is_strategy = label[\"IS_STRATEGY\"][\"categories\"][0][\"name\"]\n        key_elements_annotations = label[\"KEY_ELEMENTS\"][\"annotations\"]\n        key_elements = []\n        for annotation in key_elements_annotations:\n            key_elements.append(\n                {\n                    \"category\": annotation[\"categories\"][0][\"name\"],\n                    \"content\": annotation[\"content\"],\n                    \"page_number\": min(annotation[\"annotations\"][0][\"pageNumberArray\"]),\n                }\n            )\n        key_elements = sorted(key_elements, key=lambda x: x[\"page_number\"])\n        aggregated_key_elements = {}\n        for item in key_elements:\n            category = item[\"category\"]\n            content = item[\"content\"]\n            if category in aggregated_key_elements:\n                aggregated_key_elements[category].append(content)\n            else:\n                aggregated_key_elements[category] = [content]\n        return aggregated_key_elements, is_strategy\n\n    def __query_chatgpt(self, instructions: str, text: str):\n        system_message_prompt = SystemMessagePromptTemplate.from_template(instructions)\n        human_template = \"{text}\"\n        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n        chat_prompt = ChatPromptTemplate.from_messages(\n            [system_message_prompt, human_message_prompt]\n        )\n        response = self._openai_client(\n            chat_prompt.format_prompt(\n                text=text,\n            ).to_messages()\n        )\n        return response.content\n\n    def __predict_trading_rules(self, key_elements):\n        trading_rules = \" \".join(key_elements.get(\"TRADING_RULES\", []))\n        if trading_rules == \"\":\n            return \"\"\n        instructions = \"\"\"You are a helpful assistant that extract the rules of the following trading strategy as bullet points.\nHere is an example:\n- Investment universe: 54 countries' 10-year government bonds\n- Sort assets into quintiles based on past month return\n- Long top quintile assets (highest returns from previous month)\n- Short bottom quintile assets (lowest returns from previous month)\n- Utilize equal weighting for assets\n- Rebalance strategy on a monthly basis\"\"\"\n        return self.__query_chatgpt(instructions=instructions, text=trading_rules)\n\n    def __predict_backtrader(self, key_elements):\n        trading_rules = \" \".join(key_elements.get(\"TRADING_RULES\", []))\n        if trading_rules == \"\":\n            return \"\"\n        instructions = (\n            \"Write the python code with Backtrader for the following strategy.\"\n        )\n        return self.__query_chatgpt(instructions=instructions, text=trading_rules)\n\n    def __predict_markets_traded(self, key_elements):\n        markets_traded = \" \".join(key_elements.get(\"MARKETS_TRADED\", []))\n        if markets_traded == \"\":\n            return \"\"\n        instructions = (\n            \"Extract the list of markets traded.\"\n            \" It can be one or more of the following: equities, bonds, bills, commodities, currencies, cryptos.\"\n        )\n        return self.__query_chatgpt(instructions=instructions, text=markets_traded)\n\n    def __predict_period_of_rebalancing(self, key_elements):\n        period_of_rebalancing = \" \".join(key_elements.get(\"PERIOD_OF_REBALANCING\", []))\n        if period_of_rebalancing == \"\":\n            return \"\"\n        instructions = \"Extract the period of rebalancing. It can be: daily, weekly, quarterly, yearly.\"\n        return self.__query_chatgpt(\n            instructions=instructions, text=period_of_rebalancing\n        )\n\n    def __predict_backtest_period(self, key_elements):\n        backtest_period = \" \".join(key_elements.get(\"BACKTEST_PERIOD\", []))\n        if backtest_period == \"\":\n            return \"\"\n        instructions = \"Extract the backtest_period. Example: 1961-2018.\"\n        return self.__query_chatgpt(instructions=instructions, text=backtest_period)\n\n    def __format_percent(self, text):\n        if \"%\" not in text:\n            return f\"{text}%\"\n        return text\n\n    def predict(self, kili_project_id: str, target_folder: str):\n        \"\"\"\n        Run predictions.\n        \"\"\"\n        assets = self._kili_client.assets(\n            project_id=kili_project_id,\n            fields=[\"id\", \"externalId\", \"labels.jsonResponse\"],\n            status_in=[\"LABELED\"],\n            disable_tqdm=True,\n        )\n        if not os.path.exists(target_folder):\n            os.makedirs(target_folder)\n        for asset in tqdm(assets):\n            key_elements, is_strategy = self.__parse_label(asset)\n            if is_strategy == \"NO\":\n                continue\n            abstract_id = int(asset[\"externalId\"])\n            path = os.path.join(target_folder, f\"{abstract_id}.md\")\n            if os.path.exists(path):\n                continue\n            abstract = SsrnAbstract(abstract_id)\n            abstract.from_ssrn()\n            strategy = SsrnStrategy(abstract)\n            strategy.trading_rules = self.__predict_trading_rules(key_elements)\n            strategy.backtrader = self.__predict_backtrader(key_elements)\n            strategy.markets_traded = self.__predict_markets_traded(key_elements)\n            strategy.period_of_rebalancing = self.__predict_period_of_rebalancing(\n                key_elements\n            )\n            strategy.backtest_period = self.__predict_backtest_period(key_elements)\n            annual_return = \" \".join(key_elements.get(\"ANNUAL_RETURN\", []))\n            strategy.annual_return = self.__format_percent(annual_return)\n            maximum_drawdown = \" \".join(key_elements.get(\"MAXIMUM_DRAWDOWN\", []))\n            strategy.maximum_drawdown = self.__format_percent(maximum_drawdown)\n            strategy.sharpe_ratio = \" \".join(key_elements.get(\"SHARPE_RATIO\", []))\n            annual_standard_deviation = \" \".join(\n                key_elements.get(\"ANNUAL_STANDARD_DEVIATION\", [])\n            )\n            strategy.annual_standard_deviation = self.__format_percent(\n                annual_standard_deviation\n            )\n            print(path)\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                markdown = strategy.to_markdown()\n                f.write(markdown)\n                print(markdown)", ""]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract_classifier.py", "chunked_list": ["import os\nimport requests\n\nimport click\nfrom datasets import Dataset\nimport evaluate\nfrom kili.client import Kili\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer", "from tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom transformers import pipeline\n\nfrom ssrn_abstract import SsrnAbstract\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"", "\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\nclass SsrnAbstractClassifier:\n    def __init__(self, kili_project_id: str):\n        self.kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n        self.kili_project_id = kili_project_id\n        self.zeroshot_author_id = \"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n        self.id2label = {0: \"NO\", 1: \"YES\"}\n        self.label2id = {\"NO\": 0, \"YES\": 1}\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            \"distilbert-base-uncased\",\n            num_labels=2,\n            id2label=self.id2label,\n            label2id=self.label2id,\n        )\n        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n        self.metric = evaluate.load(\"f1\")\n        self.model_name = \"ssrn-abstract-classifier\"\n\n    def __preprocess_function(self, examples):\n        return self.tokenizer(examples[\"text\"], truncation=True)\n\n    def __compute_metrics(self, eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=1)\n        return self.metric.compute(predictions=predictions, references=labels)\n\n    def train(self):\n        assets = self.kili_client.assets(\n            project_id=self.kili_project_id,\n            fields=[\"id\", \"externalId\", \"labels.jsonResponse\", \"labels.labelType\"],\n            status_in=[\"LABELED\"],\n        )\n        labels = []\n        texts = []\n        for asset in tqdm(assets):\n            groundtruth_labels = [\n                l for l in asset[\"labels\"] if l[\"labelType\"] == \"DEFAULT\"\n            ]\n            if len(groundtruth_labels) == 0:\n                continue\n            groundtruth_category = groundtruth_labels[-1][\"jsonResponse\"][\n                \"IS_STRATEGY\"\n            ][\"categories\"][0][\"name\"]\n            labels.append(self.label2id[groundtruth_category])\n            abstract_id = int(asset[\"externalId\"])\n            abstract = SsrnAbstract(abstract_id)\n            abstract.from_kili(project_id=self.kili_project_id)\n            text = str(abstract)\n            texts.append(text)\n        if len(labels) == 0 or len(texts) == 0:\n            print(\"There is no data for training. Please check the assets list.\")\n            return\n        dataset_dict = {\"label\": labels, \"text\": texts}\n        dataset = Dataset.from_dict(dataset_dict)\n        dataset = dataset.train_test_split(test_size=0.2)\n        tokenized_dataset = dataset.map(self.__preprocess_function, batched=True)\n\n        training_args = TrainingArguments(\n            output_dir=self.model_name,\n            learning_rate=2e-5,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=3,\n            weight_decay=0.01,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            push_to_hub=True,\n        )\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=tokenized_dataset[\"train\"],\n            eval_dataset=tokenized_dataset[\"test\"],\n            tokenizer=self.tokenizer,\n            data_collator=self.data_collator,\n            compute_metrics=self.__compute_metrics,\n        )\n        trainer.train()\n\n    def predict(self):\n        \"\"\"\n        Predicts the category of a text.\n        \"\"\"\n        classifier = pipeline(\n            \"text-classification\", model=self.model_name, tokenizer=self.tokenizer\n        )\n        assets = self.kili_client.assets(\n            project_id=self.kili_project_id,\n            fields=[\"id\", \"externalId\"],\n            status_in=[\"TODO\"],\n        )\n        for asset in tqdm(assets):\n            abstract_id = int(asset[\"externalId\"])\n            abstract = SsrnAbstract(abstract_id)\n            abstract.from_kili(project_id=self.kili_project_id)\n            text = str(abstract)\n            try:\n                prediction = classifier(text)\n            except RuntimeError:\n                continue\n            predicted_label = {\n                \"IS_STRATEGY\": {\"categories\": [{\"name\": prediction[0][\"label\"]}]}\n            }\n            self.kili_client.append_labels(\n                asset_id_array=[asset[\"id\"]],\n                json_response_array=[predicted_label],\n                model_name=self.model_name,\n                disable_tqdm=True,\n                label_type=\"PREDICTION\",\n            )\n            priority = int(100 * (1 - prediction[0][\"score\"]))\n            self.kili_client.update_properties_in_assets(\n                asset_ids=[asset[\"id\"]],\n                priorities=[priority],\n            )", "\n\n@click.command()\n@click.option(\"--mode\", default=\"train\")\n@click.option(\"--kili-project-id\")\ndef main(mode, kili_project_id):\n    \"\"\"\n    Main function.\n    \"\"\"\n    ssrn_abstract_classifier = SsrnAbstractClassifier(kili_project_id=kili_project_id)\n    if mode == \"train\":\n        ssrn_abstract_classifier.train()\n    elif mode == \"predict\":\n        ssrn_abstract_classifier.predict()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper_crawler.py", "chunked_list": ["from collections import Counter\nimport os\nimport re\nimport time\nfrom typing import Optional\n\nfrom bs4 import BeautifulSoup\nfrom kili.client import Kili\nimport requests\nfrom selenium import webdriver", "import requests\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport textract\nfrom tqdm import tqdm\nfrom webdriver_manager.chrome import ChromeDriverManager\n\nfrom systematic_trading.strategy_ideas.ssrn_paper import SsrnPaper\n\n\nclass SsrnPaperCrawler:\n    def __init__(\n        self,\n        project_id: Optional[str] = None,\n    ):\n        self.tgt_kili_project_id = tgt_kili_project_id\n\n    def __from_url(self, url: str):\n        return int(url.split(\"=\")[-1])\n\n    def from_kili(self, src_kili_project_id: str):\n        \"\"\"\n        List all abstract ids from Kili\n        \"\"\"\n        kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n        assets = kili_client.assets(\n            project_id=src_kili_project_id,\n            fields=[\"externalId\", \"labels.jsonResponse\", \"labels.labelType\"],\n            disable_tqdm=True,\n        )\n        for asset in assets:\n            labels = [\n                label\n                for label in asset[\"labels\"]\n                if label[\"labelType\"] in [\"DEFAULT\", \"REVIEW\"]\n            ]\n            if len(labels) == 0:\n                continue\n            is_strategy = labels[-1][\"jsonResponse\"][\"IS_STRATEGY\"][\"categories\"][0][\n                \"name\"\n            ]\n            if is_strategy != \"Yes\":\n                continue\n            abstract_id = int(asset[\"externalId\"])\n            paper = SsrnPaper(abstract_id)\n            if paper.exists_in_kili(self.tgt_kili_project_id):\n                continue\n            paper.from_ssrn()\n            if paper.pdf_path is None:\n                continue\n            paper.to_kili(self.tgt_kili_project_id, metadata={\"text\": filename})", "\n\nclass SsrnPaperCrawler:\n    def __init__(\n        self,\n        project_id: Optional[str] = None,\n    ):\n        self.tgt_kili_project_id = tgt_kili_project_id\n\n    def __from_url(self, url: str):\n        return int(url.split(\"=\")[-1])\n\n    def from_kili(self, src_kili_project_id: str):\n        \"\"\"\n        List all abstract ids from Kili\n        \"\"\"\n        kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n        assets = kili_client.assets(\n            project_id=src_kili_project_id,\n            fields=[\"externalId\", \"labels.jsonResponse\", \"labels.labelType\"],\n            disable_tqdm=True,\n        )\n        for asset in assets:\n            labels = [\n                label\n                for label in asset[\"labels\"]\n                if label[\"labelType\"] in [\"DEFAULT\", \"REVIEW\"]\n            ]\n            if len(labels) == 0:\n                continue\n            is_strategy = labels[-1][\"jsonResponse\"][\"IS_STRATEGY\"][\"categories\"][0][\n                \"name\"\n            ]\n            if is_strategy != \"Yes\":\n                continue\n            abstract_id = int(asset[\"externalId\"])\n            paper = SsrnPaper(abstract_id)\n            if paper.exists_in_kili(self.tgt_kili_project_id):\n                continue\n            paper.from_ssrn()\n            if paper.pdf_path is None:\n                continue\n            paper.to_kili(self.tgt_kili_project_id, metadata={\"text\": filename})", ""]}
{"filename": "systematic_trading/strategy_ideas/ssrn_strategy.py", "chunked_list": ["import os\n\nfrom kili.client import Kili\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom ssrn_abstract import SsrnAbstract\n\n\nclass SsrnStrategy:\n    def __init__(self, abstract: SsrnAbstract):\n        self.abstract = abstract\n        self.trading_rules = \"\"\n        self.backtrader = \"\"\n        self.markets_traded = \"\"\n        self.period_of_rebalancing = \"\"\n        self.backtest_period = \"\"\n        self.annual_return = \"\"\n        self.maximum_drawdown = \"\"\n        self.sharpe_ratio = \"\"\n        self.annual_standard_deviation = \"\"\n\n    def to_markdown(self):\n        return f\"\"\"# {self.abstract.title}\n\nA python [Backtrader](https://www.backtrader.com/) implementation of the algorithmic trading strategy described in the following paper.\n\n# Original paper\n\n\ud83d\udcd5 [Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract.abstract_id})\n\n# Trading rules\n\n{self.trading_rules}\n\n# Statistics\n\n- **Markets Traded:** {self.markets_traded}\n- **Period of Rebalancing:** {self.period_of_rebalancing}\n- **Backtest period:** {self.backtest_period}\n- **Annual Return:** {self.annual_return}\n- **Maximum Drawdown:** {self.maximum_drawdown}\n- **Sharpe Ratio:** {self.sharpe_ratio}\n- **Annual Standard Deviation:** {self.annual_standard_deviation}\n\n# Python code\n\n## Backtrader\n\n```python\n{self.backtrader}\n```\"\"\"", "\nclass SsrnStrategy:\n    def __init__(self, abstract: SsrnAbstract):\n        self.abstract = abstract\n        self.trading_rules = \"\"\n        self.backtrader = \"\"\n        self.markets_traded = \"\"\n        self.period_of_rebalancing = \"\"\n        self.backtest_period = \"\"\n        self.annual_return = \"\"\n        self.maximum_drawdown = \"\"\n        self.sharpe_ratio = \"\"\n        self.annual_standard_deviation = \"\"\n\n    def to_markdown(self):\n        return f\"\"\"# {self.abstract.title}\n\nA python [Backtrader](https://www.backtrader.com/) implementation of the algorithmic trading strategy described in the following paper.\n\n# Original paper\n\n\ud83d\udcd5 [Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract.abstract_id})\n\n# Trading rules\n\n{self.trading_rules}\n\n# Statistics\n\n- **Markets Traded:** {self.markets_traded}\n- **Period of Rebalancing:** {self.period_of_rebalancing}\n- **Backtest period:** {self.backtest_period}\n- **Annual Return:** {self.annual_return}\n- **Maximum Drawdown:** {self.maximum_drawdown}\n- **Sharpe Ratio:** {self.sharpe_ratio}\n- **Annual Standard Deviation:** {self.annual_standard_deviation}\n\n# Python code\n\n## Backtrader\n\n```python\n{self.backtrader}\n```\"\"\"", ""]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract.py", "chunked_list": ["import json\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom kili.client import Kili\n\n\nclass SsrnAbstract:\n    def __init__(self, abstract_id: int):\n        self.abstract_id = abstract_id\n        self.url = (\n            f\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract_id}\"\n        )\n        self.abstract = \"\"\n        self.authors = []\n        self.jel_classification = []\n        self.keywords = []\n        self.online_date = \"\"\n        self.publication_date = \"\"\n        self.title = \"\"\n        self.is_strategy = \"\"\n        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n        self._external_id = str(abstract_id)\n        self._soup = None\n\n    def __ssrn_page(self):\n        if self._soup is not None:\n            return self._soup\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        response = requests.get(self.url, headers=headers)\n        self._soup = BeautifulSoup(response.content, \"html.parser\")\n        return self._soup\n\n    def exists_in_ssrn(self):\n        soup = self.__ssrn_page()\n        abstract_tag = soup.find(\"div\", {\"class\": \"abstract-text\"})\n        title = \"The submitter of this work did not provide a PDF file for download\"\n        no_download_tag = soup.find(\"a\", {\"title\": title})\n        return abstract_tag is not None and no_download_tag is None\n\n    def from_ssrn(self):\n        soup = self.__ssrn_page()\n        # abstract\n        abstract_tag = soup.find(\"div\", {\"class\": \"abstract-text\"})\n        self.abstract = abstract_tag.find(\"p\").text.strip()\n        # authors\n        author_tags = soup.find_all(\"meta\", {\"name\": \"citation_author\"})\n        self.authors = [tag[\"content\"] for tag in author_tags]\n        # JEL classification\n        p_tags = soup.find_all(\"p\")\n        pattern = \"JEL Classification:\"\n        jel_tag = [tag for tag in p_tags if pattern in tag.text]\n        if len(jel_tag) == 1:\n            self.jel_classification = (\n                jel_tag[0].text.replace(pattern, \"\").strip().split(\", \")\n            )\n        # keywords\n        keywords_tag = soup.find(\"meta\", {\"name\": \"citation_keywords\"})\n        self.keywords = keywords_tag[\"content\"].split(\", \")\n        # online date\n        online_date_tag = soup.find(\"meta\", {\"name\": \"citation_online_date\"})\n        self.online_date = online_date_tag[\"content\"]\n        # publication date\n        publication_date_tag = soup.find(\"meta\", {\"name\": \"citation_publication_date\"})\n        self.publication_date = publication_date_tag[\"content\"]\n        # title\n        title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n        self.title = title_tag[\"content\"]\n\n    def __find_json_content_element(self, json_obj, target_id: str):\n        results = []\n        if isinstance(json_obj, dict):\n            if json_obj.get(\"id\") == target_id:\n                results.append(json_obj)\n            for value in json_obj.values():\n                if isinstance(value, (dict, list)):\n                    results.extend(self.__find_json_content_element(value, target_id))\n        elif isinstance(json_obj, list):\n            for item in json_obj:\n                if isinstance(item, (dict, list)):\n                    results.extend(self.__find_json_content_element(item, target_id))\n        return results\n\n    def from_kili(self, project_id: str):\n        assets = self._kili_client.assets(\n            project_id=project_id,\n            external_id_strictly_in=[self._external_id],\n            fields=[\"jsonContent\", \"labels.jsonResponse\", \"labels.labelType\"],\n            disable_tqdm=True,\n        )\n        assert len(assets) == 1\n        asset = assets[0]\n        asset[\"jsonContent\"] = json.loads(requests.get(asset[\"jsonContent\"]).content)\n        # abstract\n        abstract_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"abstract\"\n        )\n        assert len(abstract_blocks) == 1\n        self.abstract = abstract_blocks[0][\"text\"]\n        # authors\n        authors_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"authors\"\n        )\n        assert len(authors_blocks) == 1\n        self.authors = authors_blocks[0][\"text\"].split(\" | \")\n        # JEL classification\n        jel_classification_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"jel-classification\"\n        )\n        assert len(jel_classification_blocks) == 1\n        self.jel_classification = jel_classification_blocks[0][\"text\"].split(\" | \")\n        # keywords\n        keywords_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"keywords\"\n        )\n        assert len(keywords_blocks) == 1\n        self.keywords = keywords_blocks[0][\"text\"].split(\" | \")\n        # online date\n        online_date_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"online-date\"\n        )\n        assert len(online_date_blocks) == 1\n        self.online_date = online_date_blocks[0][\"text\"]\n        # publication date\n        publication_date_blocks = self.__find_json_content_element(\n            asset[\"jsonContent\"], \"publication-date\"\n        )\n        assert len(publication_date_blocks) == 1\n        self.publication_date = publication_date_blocks[0][\"text\"]\n        # title\n        title_blocks = self.__find_json_content_element(asset[\"jsonContent\"], \"title\")\n        assert len(title_blocks) == 1\n        self.title = title_blocks[0][\"text\"]\n        labels = [\n            label\n            for label in asset[\"labels\"]\n            if label[\"labelType\"] in [\"DEFAULT\", \"REVIEW\"]\n        ]\n        if len(labels) > 0:\n            self.is_strategy = labels[-1][\"jsonResponse\"][\"IS_STRATEGY\"][\"categories\"][\n                0\n            ][\"name\"]\n\n    def __json_content_children(self, tag_id: str, title: str, text: str):\n        return [\n            {\n                \"type\": \"h3\",\n                \"children\": [\n                    {\n                        \"id\": f\"{tag_id}-h\",\n                        \"text\": title,\n                    }\n                ],\n            },\n            {\n                \"type\": \"p\",\n                \"children\": [\n                    {\n                        \"id\": tag_id,\n                        \"text\": text,\n                    }\n                ],\n            },\n        ]\n\n    def exists_in_folder(self, path: str):\n        return os.path.exists(os.path.join(path, f\"{self._external_id}.json\"))\n\n    def exists_in_kili(self, project_id: str):\n        assets = self._kili_client.assets(\n            project_id=project_id,\n            external_id_strictly_in=[self._external_id],\n            fields=[\"id\"],\n            disable_tqdm=True,\n        )\n        return len(assets) == 1\n\n    def to_folder(self, path: str):\n        with open(os.path.join(path, f\"{self._external_id}.json\"), \"w\") as f:\n            json.dump(self.__dict__(), f, indent=4, sort_keys=True)\n\n    def to_kili(self, project_id: str):\n        children = (\n            self.__json_content_children(\n                tag_id=\"title\",\n                title=\"Title\",\n                text=self.title,\n            )\n            + self.__json_content_children(\n                tag_id=\"abstract\",\n                title=\"Abstract\",\n                text=self.abstract,\n            )\n            + self.__json_content_children(\n                tag_id=\"keywords\",\n                title=\"Keywords\",\n                text=\" | \".join(self.keywords),\n            )\n            + self.__json_content_children(\n                tag_id=\"jel-classification\",\n                title=\"JEL classification\",\n                text=\" | \".join(self.jel_classification),\n            )\n            + self.__json_content_children(\n                tag_id=\"authors\",\n                title=\"Authors\",\n                text=\" | \".join(self.authors),\n            )\n            + self.__json_content_children(\n                tag_id=\"url\",\n                title=\"Url\",\n                text=self.url,\n            )\n            + self.__json_content_children(\n                tag_id=\"publication-date\",\n                title=\"Publication date\",\n                text=self.publication_date,\n            )\n            + self.__json_content_children(\n                tag_id=\"online-date\",\n                title=\"Online date\",\n                text=self.online_date,\n            )\n        )\n        json_content = [\n            {\n                \"children\": children,\n            }\n        ]\n        self._kili_client.append_many_to_dataset(\n            project_id=project_id,\n            json_content_array=[json_content],\n            external_id_array=[self._external_id],\n            disable_tqdm=True,\n        )\n        if self.is_strategy != \"\":\n            json_response = {\n                \"IS_STRATEGY\": {\"categories\": [{\"name\": self.is_strategy}]}\n            }\n            self._kili_client.append_labels(\n                project_id=project_id,\n                asset_external_id_array=[self._external_id],\n                json_response_array=[json_response],\n                label_type=\"DEFAULT\",\n                disable_tqdm=True,\n            )\n\n    def __dict__(self):\n        return {\n            \"abstract\": self.abstract,\n            \"authors\": self.authors,\n            \"external_id\": self._external_id,\n            \"jel_classification\": self.jel_classification,\n            \"keywords\": self.keywords,\n            \"online_date\": self.online_date,\n            \"publication_date\": self.publication_date,\n            \"title\": self.title,\n            \"url\": self.url,\n        }\n\n    def __str__(self):\n        text = \"\\n\\n\".join(\n            [\n                self.title,\n                self.abstract,\n                \" | \".join(self.keywords),\n                \" | \".join(self.jel_classification),\n                \" | \".join(self.authors),\n            ]\n        )\n        return text", ""]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper.py", "chunked_list": ["import os\n\nfrom kili.client import Kili\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass SsrnPaper:\n    def __init__(self, abstract_id: int):\n        self.abstract_id = abstract_id\n        self._external_id = str(abstract_id)\n        self.pdf_path = None\n        self.url = (\n            f\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract_id}\"\n        )\n        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n\n    def exists_in_kili(self, project_id: str):\n        assets = self._kili_client.assets(\n            project_id=project_id,\n            external_id_strictly_in=[self._external_id],\n            fields=[\"id\"],\n            disable_tqdm=True,\n        )\n        return len(assets) == 1\n\n    def from_ssrn(self):\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        html = requests.get(self.url, headers=headers).content\n        soup = BeautifulSoup(html, \"html.parser\")\n        hrefs = [\n            tag[\"href\"]\n            for tag in soup.find_all(\"a\")\n            if tag.has_attr(\"href\") and tag[\"href\"].startswith(\"Delivery.cfm/\")\n        ]\n        if len(hrefs) == 0:\n            return\n        pdf_url = \"https://papers.ssrn.com/sol3/\" + hrefs[0]\n        folder = os.path.join(os.getenv(\"HOME\"), \"Downloads\")\n        filename = f\"{self.abstract_id}.pdf\"\n        self.pdf_path = os.path.join(folder, filename)\n        response = requests.get(pdf_url, headers={**headers, \"Referer\": self.url})\n        with open(self.pdf_path, \"wb\") as handler:\n            handler.write(response.content)\n\n    def to_kili(self, project_id: str, metadata: dict = {}):\n        self._kili_client.append_many_to_dataset(\n            project_id=project_id,\n            content_array=[self.pdf_path],\n            external_id_array=[self._external_id],\n            disable_tqdm=True,\n            json_metadata_array=[metadata],\n        )", ""]}
{"filename": "systematic_trading/datasets/__main__.py", "chunked_list": ["from datetime import date, datetime, timedelta\nfrom typing import List\n\nimport click\nfrom tqdm import tqdm\n\nfrom systematic_trading.datasets.dataset import Dataset\nfrom systematic_trading.datasets.index_constituents import IndexConstituents\nfrom systematic_trading.datasets.index_constituents.sp500 import SP500\nfrom systematic_trading.datasets.knowledge_graph.stocks import Stocks", "from systematic_trading.datasets.index_constituents.sp500 import SP500\nfrom systematic_trading.datasets.knowledge_graph.stocks import Stocks\nfrom systematic_trading.datasets.raw.analysis.earnings_estimate import EarningsEstimate\nfrom systematic_trading.datasets.raw.analysis.eps_revisions import EPSRevisions\nfrom systematic_trading.datasets.raw.analysis.eps_trend import EPSTrend\nfrom systematic_trading.datasets.raw.analysis.revenue_estimate import RevenueEstimate\nfrom systematic_trading.datasets.raw.earnings import Earnings\nfrom systematic_trading.datasets.raw.earnings_forecast import EarningsForecast\nfrom systematic_trading.datasets.raw.earnings_surprise import EarningsSurprise\nfrom systematic_trading.datasets.raw.extended_trading import ExtendedTrading", "from systematic_trading.datasets.raw.earnings_surprise import EarningsSurprise\nfrom systematic_trading.datasets.raw.extended_trading import ExtendedTrading\nfrom systematic_trading.datasets.raw.news import News\nfrom systematic_trading.datasets.raw.short_interest import ShortInterest\nfrom systematic_trading.datasets.raw.timeseries_daily import TimeseriesDaily\nfrom systematic_trading.datasets.raw.timeseries_1mn import Timeseries1mn\n\n\n@click.command()\n@click.option(\"--mode\", default=\"\", help=\"Mode to use, daily / on-demand\")", "@click.command()\n@click.option(\"--mode\", default=\"\", help=\"Mode to use, daily / on-demand\")\n@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\ndef main(mode: str, username: str):\n    \"\"\"\n    Main function.\n    \"\"\"\n    if mode == \"daily\":\n        now = datetime.now()\n        if now.hour > 21:\n            tag_date = date.today()\n        elif now.hour < 10:\n            tag_date = date.today() - timedelta(days=1)\n        else:\n            raise ValueError(\"This script should be run between 21:00 and 10:00\")\n        tag = tag_date.isoformat()\n        print(\"Updating index constituents...\")\n        index_constituents = SP500(tag_date=tag_date, username=username)\n        if not index_constituents.check_file_exists(tag=tag):\n            index_constituents.set_dataset_df()\n            index_constituents.to_hf_datasets()\n        print(\"Updating raw datasets...\")\n        raw_datasets = {\n            \"earnings-stocks\": Earnings(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"earnings-estimate-stocks\": EarningsEstimate(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"earnings-forecast-stocks\": EarningsForecast(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"earnings-surprise-stocks\": EarningsSurprise(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"extended-trading-stocks\": ExtendedTrading(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"eps-revisions-stocks\": EPSRevisions(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"eps-trend-stocks\": EPSTrend(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"news-stocks\": News(suffix=\"stocks\", tag_date=tag_date, username=username),\n            \"revenue-estimate-stocks\": RevenueEstimate(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"short-interest-stocks\": ShortInterest(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"timeseries-daily-stocks\": TimeseriesDaily(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n            \"timeseries-1mn-stocks\": Timeseries1mn(\n                suffix=\"stocks\", tag_date=tag_date, username=username\n            ),\n        }\n        dataset_names = [\n            name\n            for name in raw_datasets\n            if not raw_datasets[name].check_file_exists(tag=tag)\n        ]\n        for name in dataset_names:\n            raw_datasets[name].load_frames()\n        for symbol in tqdm(index_constituents.symbols):\n            for name in dataset_names:\n                if symbol in raw_datasets[name].frames:\n                    continue\n                raw_datasets[name].append_frame(symbol)\n                raw_datasets[name].save_frames()\n        for name in dataset_names:\n            raw_datasets[name].set_dataset_df()\n            raw_datasets[name].to_hf_datasets()\n    elif mode == \"on-demand\":\n        print(\"Updating list of stocks...\")\n        stocks = Stocks(username=username)\n        stocks.set_dataset_df()\n        stocks.to_hf_datasets()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "systematic_trading/datasets/dataset.py", "chunked_list": ["from datetime import date, timedelta\nfrom typing import Optional\n\nfrom datasets import Dataset as HFDataset, load_dataset\nimport huggingface_hub\nimport pandas as pd\nimport re\n\n\nclass Dataset:\n    \"\"\"\n    Dataset.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        self.suffix: str = suffix\n        self.tag_date = tag_date\n        self.username: str = username\n        self.expected_columns = []\n        self.dataset_df: pd.DataFrame = pd.DataFrame(columns=self.expected_columns)\n        self.name: str = None\n        self.symbols = self.get_scope_symbols()\n\n    def add_previous_data(self):\n        \"\"\"\n        Add previous data to the current data.\n        \"\"\"\n        prev_data = pd.DataFrame(\n            load_dataset(f\"{self.username}/{self.name}\")[\"train\"],\n        )\n        # filter out news that are not related to the index\n        still_in_scope = prev_data.symbol.isin(self.symbols)\n        prev_data = prev_data.loc[still_in_scope]\n        self.dataset_df = pd.concat([prev_data, self.dataset_df])\n        self.dataset_df.drop_duplicates(inplace=True)\n\n    def check_file_exists(self, tag: Optional[str] = None) -> bool:\n        \"\"\"\n        Check if file exists.\n        \"\"\"\n        try:\n            load_dataset(\n                f\"{self.username}/{self.name}\",\n                revision=tag,\n                verification_mode=\"no_checks\",\n            )\n            return True\n        except FileNotFoundError:\n            return False\n\n    def set_dataset_df(self):\n        \"\"\"\n        Frames to dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_scope_symbols(self) -> list:\n        if self.check_file_exists():\n            return load_dataset(f\"{self.username}/{self.suffix}\")[\"train\"][\"symbol\"]\n        return []\n\n    def symbol_to_ticker(self, symbol: str) -> str:\n        \"\"\"\n        Convert a symbol to a ticker.\n        \"\"\"\n        pattern = re.compile(r\"\\.B$\")\n        return pattern.sub(\"-B\", symbol)\n\n    def to_hf_datasets(self) -> None:\n        \"\"\"\n        To Hugging Face datasets.\n        \"\"\"\n        if self.dataset_df.columns.tolist() != self.expected_columns:\n            raise ValueError(\n                f\"self.dataset_df must have the right columns\\n{self.dataset_df.columns.tolist()}\\n!=\\n{self.expected_columns}\"\n            )\n        if len(self.dataset_df) == 0:\n            raise ValueError(\"self.dataset_df must be set\")\n        tag = self.tag_date.isoformat()\n        dataset = HFDataset.from_pandas(self.dataset_df)\n        repo_id: str = f\"edarchimbaud/{self.name}\"\n        dataset.push_to_hub(repo_id, private=False)\n        huggingface_hub.create_tag(repo_id, tag=tag, repo_type=\"dataset\")", "\nclass Dataset:\n    \"\"\"\n    Dataset.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        self.suffix: str = suffix\n        self.tag_date = tag_date\n        self.username: str = username\n        self.expected_columns = []\n        self.dataset_df: pd.DataFrame = pd.DataFrame(columns=self.expected_columns)\n        self.name: str = None\n        self.symbols = self.get_scope_symbols()\n\n    def add_previous_data(self):\n        \"\"\"\n        Add previous data to the current data.\n        \"\"\"\n        prev_data = pd.DataFrame(\n            load_dataset(f\"{self.username}/{self.name}\")[\"train\"],\n        )\n        # filter out news that are not related to the index\n        still_in_scope = prev_data.symbol.isin(self.symbols)\n        prev_data = prev_data.loc[still_in_scope]\n        self.dataset_df = pd.concat([prev_data, self.dataset_df])\n        self.dataset_df.drop_duplicates(inplace=True)\n\n    def check_file_exists(self, tag: Optional[str] = None) -> bool:\n        \"\"\"\n        Check if file exists.\n        \"\"\"\n        try:\n            load_dataset(\n                f\"{self.username}/{self.name}\",\n                revision=tag,\n                verification_mode=\"no_checks\",\n            )\n            return True\n        except FileNotFoundError:\n            return False\n\n    def set_dataset_df(self):\n        \"\"\"\n        Frames to dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_scope_symbols(self) -> list:\n        if self.check_file_exists():\n            return load_dataset(f\"{self.username}/{self.suffix}\")[\"train\"][\"symbol\"]\n        return []\n\n    def symbol_to_ticker(self, symbol: str) -> str:\n        \"\"\"\n        Convert a symbol to a ticker.\n        \"\"\"\n        pattern = re.compile(r\"\\.B$\")\n        return pattern.sub(\"-B\", symbol)\n\n    def to_hf_datasets(self) -> None:\n        \"\"\"\n        To Hugging Face datasets.\n        \"\"\"\n        if self.dataset_df.columns.tolist() != self.expected_columns:\n            raise ValueError(\n                f\"self.dataset_df must have the right columns\\n{self.dataset_df.columns.tolist()}\\n!=\\n{self.expected_columns}\"\n            )\n        if len(self.dataset_df) == 0:\n            raise ValueError(\"self.dataset_df must be set\")\n        tag = self.tag_date.isoformat()\n        dataset = HFDataset.from_pandas(self.dataset_df)\n        repo_id: str = f\"edarchimbaud/{self.name}\"\n        dataset.push_to_hub(repo_id, private=False)\n        huggingface_hub.create_tag(repo_id, tag=tag, repo_type=\"dataset\")", ""]}
{"filename": "systematic_trading/datasets/knowledge_graph/wikipedia.py", "chunked_list": ["\"\"\"\ncurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream.xml.bz2\"\ncurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream-index.txt.bz2\"\n\"\"\"\nimport bz2\nimport os\nimport pickle\nimport re\n\nfrom bs4 import BeautifulSoup", "\nfrom bs4 import BeautifulSoup\nimport click\nfrom tqdm import tqdm\n\n\nDRIVE = \"/Users/edba/Downloads\"\n\n\nclass Wikipedia:\n    def __init__(self):\n        path = os.path.join(\n            DRIVE,\n            \"Raw\",\n            \"Wikipedia\",\n            \"enwiki-20230620-pages-articles-multistream.xml.bz2\",\n        )\n        self.handler = bz2.BZ2File(path, \"r\")\n\n    def __del__(self):\n        self.handler.close()\n\n    def select_pages(self, titles: list[str]):\n        \"\"\"\n        Returns the Wikipedia pages of companies that are traded.\n        \"\"\"\n        pages = {}\n        for line in tqdm(self.handler, total=22962775):\n            line = line.decode(\"utf-8\")\n            if \"<page>\" in line:\n                page_content = []\n                page_content.append(line)\n                while \"</page>\" not in line:\n                    line = next(self.handler).decode(\"utf-8\")\n                    page_content.append(line)\n                page = \"\".join(page_content).strip()\n                soup = BeautifulSoup(page, \"xml\")\n                title_tag = soup.find(\"title\")\n                title = title_tag.get_text()\n                if title in titles:\n                    text_tag = soup.find(\"text\")\n                    text = text_tag.get_text()\n                    pages[title] = text\n        return pages", "\nclass Wikipedia:\n    def __init__(self):\n        path = os.path.join(\n            DRIVE,\n            \"Raw\",\n            \"Wikipedia\",\n            \"enwiki-20230620-pages-articles-multistream.xml.bz2\",\n        )\n        self.handler = bz2.BZ2File(path, \"r\")\n\n    def __del__(self):\n        self.handler.close()\n\n    def select_pages(self, titles: list[str]):\n        \"\"\"\n        Returns the Wikipedia pages of companies that are traded.\n        \"\"\"\n        pages = {}\n        for line in tqdm(self.handler, total=22962775):\n            line = line.decode(\"utf-8\")\n            if \"<page>\" in line:\n                page_content = []\n                page_content.append(line)\n                while \"</page>\" not in line:\n                    line = next(self.handler).decode(\"utf-8\")\n                    page_content.append(line)\n                page = \"\".join(page_content).strip()\n                soup = BeautifulSoup(page, \"xml\")\n                title_tag = soup.find(\"title\")\n                title = title_tag.get_text()\n                if title in titles:\n                    text_tag = soup.find(\"text\")\n                    text = text_tag.get_text()\n                    pages[title] = text\n        return pages", ""]}
{"filename": "systematic_trading/datasets/knowledge_graph/stocks.py", "chunked_list": ["\"\"\"\ncurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream.xml.bz2\"\ncurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream-index.txt.bz2\"\n\"\"\"\n\nfrom datetime import date\nimport json\nimport os\nimport pickle\nimport re", "import pickle\nimport re\nimport time\nfrom urllib.parse import quote_plus\n\nfrom bs4 import BeautifulSoup\nimport click\nfrom datasets import load_dataset\nimport pandas as pd\nimport requests", "import pandas as pd\nimport requests\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom tqdm import tqdm\nfrom webdriver_manager.chrome import ChromeDriverManager\n\nfrom systematic_trading.helpers import nasdaq_headers\nfrom systematic_trading.datasets.knowledge_graph import KnowledgeGraph\nfrom systematic_trading.datasets.knowledge_graph.wikipedia import Wikipedia", "from systematic_trading.datasets.knowledge_graph import KnowledgeGraph\nfrom systematic_trading.datasets.knowledge_graph.wikipedia import Wikipedia\n\n\nclass Stocks(KnowledgeGraph):\n    def __init__(self, tag_date: date = None, username: str = None):\n        super().__init__(\"stocks\", tag_date, username)\n        self.name = f\"stocks\"\n\n    def __download_nasdaq(self) -> pd.DataFrame:\n        \"\"\"\n        Returns a DataFrame of NASDAQ stocks\n        \"\"\"\n        url = \"https://api.nasdaq.com/api/screener/stocks?tableonly=true&download=true\"\n        response = requests.get(url, headers=nasdaq_headers())\n        json_data = response.json()\n        df = pd.DataFrame(data=json_data[\"data\"][\"rows\"])\n        df = df[[\"symbol\", \"name\", \"country\", \"sector\", \"industry\"]]\n        # filter common stocks\n        index = df.name.apply(lambda x: x.endswith(\"Common Stock\"))\n        df = df.loc[index, :]\n        df.reset_index(drop=True, inplace=True)\n        nasdaq_names = df.name.apply(\n            lambda x: x.replace(\" Common Stock\", \"\")\n            .replace(\" Inc.\", \"\")\n            .replace(\" Inc\", \"\")\n            .replace(\" Class A\", \"\")\n        )\n        df.name = nasdaq_names\n        df.rename(\n            columns={\n                \"name\": \"security\",\n                \"sector\": \"gics_sector\",\n                \"industry\": \"gics_sub_industry\",\n            },\n            inplace=True,\n        )\n        return df\n\n    def __download_sp500(self) -> pd.DataFrame:\n        dataset = load_dataset(\"edarchimbaud/index-constituents-sp500\")\n        df = dataset[\"train\"].to_pandas()\n        df = df[[\"symbol\", \"security\", \"gics_sector\", \"gics_sub_industry\"]]\n        df.loc[:, \"country\"] = \"United States\"\n        return df\n\n    def __download(self) -> pd.DataFrame:\n        path_tgt = os.path.join(\"data\", \"stocks.raw.csv\")\n        if os.path.exists(path_tgt):\n            return\n        self.dataset_df = pd.concat(\n            [\n                self.__download_nasdaq(),\n                self.__download_sp500(),\n            ]\n        )\n        self.dataset_df = self.dataset_df.drop_duplicates(\n            subset=[\"symbol\"], keep=\"first\"\n        )\n        self.dataset_df.sort_values(by=[\"symbol\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)\n        self.__save(path=path_tgt)\n\n    def __load(self, path):\n        if path.endswith(\".csv\"):\n            self.dataset_df = pd.read_csv(path)\n        elif path.endswith(\".pkl\"):\n            self.dataset_df = pd.read_pickle(path)\n\n    def __save(self, path):\n        if path.endswith(\".csv\"):\n            self.dataset_df.to_csv(\n                path,\n                index=False,\n            )\n        elif path.endswith(\".pkl\"):\n            self.dataset_df.to_pickle(path)\n\n    def __add_wikipedia_title(self):\n        \"\"\"\n        Add wikipedia title to the DataFrame.\n        \"\"\"\n        path_src = os.path.join(\"data\", \"stocks.raw.csv\")\n        path_tgt = os.path.join(\"data\", \"stocks.title.csv\")\n        if os.path.exists(path_tgt):\n            return\n        self.__load(path=path_src)\n        self.dataset_df.fillna(\"\", inplace=True)\n        self.dataset_df.loc[:, \"wikipedia_title\"] = \"\"\n        # Match with Google search\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        options = Options()\n        options.headless = False\n        options.add_argument(\"user-agent=\" + headers[\"User-Agent\"])\n        driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n        driver.get(\"https://www.google.com/\")\n        input(\"Cookies accepted?\")\n        path = os.path.join(\"data\", \"stocks.csv\")\n        for index, row in tqdm(self.dataset_df.iterrows(), total=len(self.dataset_df)):\n            if index < 6:\n                continue\n            if row[\"wikipedia_title\"]:\n                continue\n            encoded_query = quote_plus(row[\"security\"] + \" company\")\n            url = \"https://www.google.com/search?hl=en&q=\" + encoded_query\n            driver.get(url)\n            time.sleep(60)\n            body = driver.find_element(\"xpath\", \"//body\")\n            body_html = body.get_attribute(\"innerHTML\")\n            soup = BeautifulSoup(body_html, \"html.parser\")\n            hrefs = [\n                a[\"href\"]\n                for a in soup.find_all(\"a\")\n                if a.has_attr(\"href\")\n                and a[\"href\"].startswith(\"https://en.wikipedia.org/\")\n                and a.text.strip() == \"Wikipedia\"\n            ]\n            if len(hrefs) == 0:\n                continue\n            href = hrefs[0]\n            wikipedia_name = href.split(\"/\")[-1].replace(\"_\", \" \")\n            self.dataset_df.loc[index, \"wikipedia_title\"] = wikipedia_name\n            self.__save(path=path_tgt)\n        self.__save(path=path_tgt)\n\n    def __add_wikipedia_page(self):\n        \"\"\"\n        Add wikipedia page to the DataFrame.\n        \"\"\"\n        path_src = os.path.join(\"data\", \"stocks.title.csv\")\n        path_tgt = os.path.join(\"data\", \"stocks.page.pkl\")\n        if os.path.exists(path_tgt):\n            return\n        self.__load(path=path_src)\n        titles = self.dataset_df.wikipedia_title.tolist()\n        wikipedia = Wikipedia()\n        pages = wikipedia.select_pages(titles)\n        self.dataset_df.loc[:, \"wikipedia_page\"] = \"\"\n        for index, row in self.dataset_df.iterrows():\n            title = row[\"wikipedia_title\"]\n            if title == \"\" or title not in pages:\n                continue\n            row[\"wikipedia_page\"] = pages[title]\n        self.__save(path=path_tgt)\n\n    def __add_relationships(self):\n        \"\"\"\n        Add relationships to the DataFrame.\n        \"\"\"\n        path_src = os.path.join(\"data\", \"stocks.page.pkl\")\n        path_tgt = os.path.join(\"data\", \"stocks.csv\")\n        if os.path.exists(path_tgt):\n            return\n        self.__load(path=path_src)\n        self.dataset_df.loc[:, \"categories\"] = \"\"\n        pattern = r\"\\[\\[Category:(.*?)\\]\\]\"\n        for index, row in self.dataset_df.iterrows():\n            text = row[\"wikipedia_page\"]\n            if text == \"\":\n                continue\n            categories = list(re.findall(pattern, text))\n            self.dataset_df.loc[index, \"categories\"] = json.dumps(categories)\n        self.dataset_df = self.dataset_df[self.expected_columns]\n        self.__save(path=path_tgt)\n\n    def set_dataset_df(self):\n        \"\"\"\n        Frames to dataset.\n        \"\"\"\n        self.dataset_df = self.__download()\n        self.__add_wikipedia_title()\n        self.__add_wikipedia_page()\n        self.__add_relationships()", ""]}
{"filename": "systematic_trading/datasets/knowledge_graph/__init__.py", "chunked_list": ["\"\"\"\nIndex constituents data.\n\"\"\"\nfrom datetime import date\n\nimport pandas as pd\n\nfrom systematic_trading.datasets.dataset import Dataset\n\n\nclass KnowledgeGraph(Dataset):\n    \"\"\"\n    Index constituents data.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.expected_columns = [\n            \"symbol\",\n            \"security\",\n            \"country\",\n            \"gics_sector\",\n            \"gics_sub_industry\",\n            \"categories\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)", "\n\nclass KnowledgeGraph(Dataset):\n    \"\"\"\n    Index constituents data.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.expected_columns = [\n            \"symbol\",\n            \"security\",\n            \"country\",\n            \"gics_sector\",\n            \"gics_sub_industry\",\n            \"categories\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)", ""]}
{"filename": "systematic_trading/datasets/index_constituents/__init__.py", "chunked_list": ["\"\"\"\nIndex constituents data.\n\"\"\"\nfrom datetime import date\n\nimport pandas as pd\n\nfrom systematic_trading.datasets.dataset import Dataset\n\n\nclass IndexConstituents(Dataset):\n    \"\"\"\n    Index constituents data.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.expected_columns = [\n            \"symbol\",\n            \"security\",\n            \"gics_sector\",\n            \"gics_sub_industry\",\n            \"headquarters_location\",\n            \"date_added\",\n            \"cik\",\n            \"founded\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)", "\n\nclass IndexConstituents(Dataset):\n    \"\"\"\n    Index constituents data.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.expected_columns = [\n            \"symbol\",\n            \"security\",\n            \"gics_sector\",\n            \"gics_sub_industry\",\n            \"headquarters_location\",\n            \"date_added\",\n            \"cik\",\n            \"founded\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)", ""]}
{"filename": "systematic_trading/datasets/index_constituents/sp500.py", "chunked_list": ["\"\"\"\nIndex constituents S&P 500.\n\"\"\"\nfrom datetime import date\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom systematic_trading.datasets.index_constituents import IndexConstituents\nfrom systematic_trading.helpers import retry_get\n\n\nclass SP500(IndexConstituents):\n    \"\"\"\n    Index constituents S&P 500.\n    \"\"\"\n\n    def __init__(self, tag_date: date = None, username: str = None):\n        super().__init__(\"stocks\", tag_date, username)\n        self.name = f\"index-constituents-sp500\"\n\n    def set_dataset_df(self):\n        \"\"\"\n        Download the list of S&P 500 constituents from Wikipedia.\n        \"\"\"\n        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n        response = retry_get(url)\n        body_html = response.content.decode(\"utf-8\")\n        soup = BeautifulSoup(body_html, features=\"lxml\")\n        table = soup.find(\"table\", {\"id\": \"constituents\"})\n        header = [th.text.strip() for th in table.find_all(\"th\")]\n        assert len(header) == 8, f\"len(header)=={len(header)}\"\n        tbody = table.find(\"tbody\")\n        data = []\n        for row in tqdm(tbody.find_all(\"tr\")):\n            td_tags = row.find_all(\"td\")\n            if len(td_tags) != len(header):\n                continue\n            data.append(\n                {\n                    \"symbol\": td_tags[header.index(\"Symbol\")].text.strip(),\n                    \"security\": td_tags[header.index(\"Security\")].text.strip(),\n                    \"gics_sector\": td_tags[header.index(\"GICS Sector\")].text.strip(),\n                    \"gics_sub_industry\": td_tags[\n                        header.index(\"GICS Sub-Industry\")\n                    ].text.strip(),\n                    \"headquarters_location\": td_tags[\n                        header.index(\"Headquarters Location\")\n                    ].text.strip(),\n                    \"date_added\": td_tags[header.index(\"Date added\")].text.strip(),\n                    \"cik\": td_tags[header.index(\"CIK\")].text.strip(),\n                    \"founded\": td_tags[header.index(\"Founded\")].text.strip(),\n                }\n            )\n        self.dataset_df = pd.DataFrame(data=data)\n        self.dataset_df.sort_values(by=[\"symbol\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/short_interest.py", "chunked_list": ["\"\"\"\nShort interest from Nasdaq.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport numpy as np", "from datasets import load_dataset\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import nasdaq_headers, retry_get\n\n\nclass ShortInterest(Raw):\n    \"\"\"\n    Short interest from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"short-interest-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"settlement_date\",\n            \"interest\",\n            \"avg_daily_share_volume\",\n            \"days_to_cover\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/quote/{ticker}/short-interest?assetClass=stocks\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        short_interest_table = json_data[\"data\"][\"shortInterestTable\"]\n        if short_interest_table is None:\n            self.frames[symbol] = None\n            return\n        data = short_interest_table[\"rows\"]\n        df = pd.DataFrame(data=data)\n        df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"])\n        df[\"interest\"] = df[\"interest\"].apply(\n            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n        )\n        df[\"avgDailyShareVolume\"] = df[\"avgDailyShareVolume\"].apply(\n            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n        )\n        df.rename(\n            columns={\n                \"settlementDate\": \"settlement_date\",\n                \"avgDailyShareVolume\": \"avg_daily_share_volume\",\n                \"daysToCover\": \"days_to_cover\",\n            },\n            inplace=True,\n        )\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\nclass ShortInterest(Raw):\n    \"\"\"\n    Short interest from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"short-interest-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"settlement_date\",\n            \"interest\",\n            \"avg_daily_share_volume\",\n            \"days_to_cover\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/quote/{ticker}/short-interest?assetClass=stocks\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        short_interest_table = json_data[\"data\"][\"shortInterestTable\"]\n        if short_interest_table is None:\n            self.frames[symbol] = None\n            return\n        data = short_interest_table[\"rows\"]\n        df = pd.DataFrame(data=data)\n        df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"])\n        df[\"interest\"] = df[\"interest\"].apply(\n            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n        )\n        df[\"avgDailyShareVolume\"] = df[\"avgDailyShareVolume\"].apply(\n            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n        )\n        df.rename(\n            columns={\n                \"settlementDate\": \"settlement_date\",\n                \"avgDailyShareVolume\": \"avg_daily_share_volume\",\n                \"daysToCover\": \"days_to_cover\",\n            },\n            inplace=True,\n        )\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\n\nif __name__ == \"__main__\":\n    symbol = \"AAPL\"\n    suffix = \"stocks\"\n    tag_date = datetime(2023, 5, 26).date()\n    username = \"edarchimbaud\"\n    dataset = ShortInterest(suffix=suffix, tag_date=tag_date, username=username)\n    dataset.append_frame(symbol)\n", ""]}
{"filename": "systematic_trading/datasets/raw/timeseries_daily.py", "chunked_list": ["\"\"\"\nTimeseries daily from Yahoo Finance.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport pandas as pd", "from datasets import load_dataset\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\n\n\nclass TimeseriesDaily(Raw):\n    \"\"\"\n    Timeseries daily from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"timeseries-daily-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"open\",\n            \"high\",\n            \"low\",\n            \"close\",\n            \"adj_close\",\n            \"volume\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def __get_timeseries_daily_with_retry(self, ticker: str, retries=10, delay=300):\n        from_timestamp = int(datetime.timestamp(datetime(1980, 1, 1)))\n        to_timestamp = int(datetime.timestamp(datetime.now()))\n        url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={from_timestamp}&period2={to_timestamp}&interval=1d&events=history&includeAdjustedClose=true\"\n        for _ in range(retries):\n            try:\n                df = pd.read_csv(url)\n                df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date.apply(\n                    lambda x: x.isoformat()\n                )\n                return df\n            except urllib.error.HTTPError:\n                print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n                time.sleep(delay)\n        raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        df = self.__get_timeseries_daily_with_retry(ticker)\n        if df is None:\n            self.frames[symbol] = None\n            return\n        df.rename(\n            columns={\n                \"Date\": \"date\",\n                \"Open\": \"open\",\n                \"High\": \"high\",\n                \"Low\": \"low\",\n                \"Close\": \"close\",\n                \"Adj Close\": \"adj_close\",\n                \"Volume\": \"volume\",\n            },\n            inplace=True,\n        )\n        df[\"symbol\"] = symbol\n        # use reindex() to set 'symbol' as the first column\n        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/earnings.py", "chunked_list": ["\"\"\"\nEarnings data from Yahoo Finance.\n\"\"\"\nfrom datetime import datetime, date\nfrom typing import Union\n\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nimport pandas as pd\nimport pytz", "import pandas as pd\nimport pytz\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import retry_get\n\n\nclass Earnings(Raw):\n    \"\"\"\n    Earnings data from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"eps_estimate\",\n            \"reported_eps\",\n            \"surprise\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def __format_field(self, key: str, value: str):\n        \"\"\"\n        Format a field.\n        \"\"\"\n        if key == \"Earnings Date\":\n            date_str = value[:-3]\n            datetime_obj = datetime.strptime(date_str, \"%b %d, %Y, %I %p\")\n            tz_str = value[-3:]\n            if tz_str in [\"EST\", \"EDT\"]:\n                tz = pytz.timezone(\"EST\")\n            else:\n                raise ValueError(f\"Unknown timezone: {tz_str}\")\n            return tz.localize(datetime_obj)\n        elif key in [\"EPS Estimate\", \"Reported EPS\", \"Surprise(%)\"]:\n            if value == \"-\":\n                return None\n            return float(value)\n        elif key in [\"Symbol\", \"Company\"]:\n            return value\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def __get_earnings(self, ticker: str) -> Union[pd.DataFrame, None]:\n        \"\"\"\n        Get earnings for a given ticker.\n        \"\"\"\n        url = f\"https://finance.yahoo.com/calendar/earnings?symbol={ticker}\"\n        response = retry_get(url)\n        soup = BeautifulSoup(response.text, features=\"lxml\")\n        div = soup.find(\"div\", {\"id\": \"cal-res-table\"})\n        if div is None:\n            raise ValueError(f\"Could not find earnings for {ticker}\")\n        table = div.find(\"table\")\n        thead = table.find(\"thead\")\n        header = [th.text for th in thead.find_all(\"th\")]\n        expected_header = [\n            \"Symbol\",\n            \"Company\",\n            \"Earnings Date\",\n            \"EPS Estimate\",\n            \"Reported EPS\",\n            \"Surprise(%)\",\n        ]\n        assert header == expected_header\n        tbody = table.find(\"tbody\")\n        data = []\n        for row in tbody.find_all(\"tr\"):\n            data.append(\n                {\n                    key: self.__format_field(\n                        key,\n                        value=row.find_all(\"td\")[index].text,\n                    )\n                    for index, key in enumerate(header)\n                }\n            )\n        assert len(data) > 0\n        df = pd.DataFrame(data)\n        return df\n\n    def append_frame(self, symbol: str):\n        \"\"\"\n        Append a dataframe for a given symbol.\n        \"\"\"\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            df = self.__get_earnings(ticker)\n        except ValueError as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        df.drop(columns=[\"Symbol\", \"Company\"], inplace=True)\n        df.rename(\n            columns={\n                \"Earnings Date\": \"date\",\n                \"EPS Estimate\": \"eps_estimate\",\n                \"Reported EPS\": \"reported_eps\",\n                \"Surprise(%)\": \"surprise\",\n            },\n            inplace=True,\n        )\n        df[\"symbol\"] = symbol\n        # use reindex() to set 'symbol' as the first column\n        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        \"\"\"\n        Set the dataset dataframe.\n        \"\"\"\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "class Earnings(Raw):\n    \"\"\"\n    Earnings data from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"eps_estimate\",\n            \"reported_eps\",\n            \"surprise\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def __format_field(self, key: str, value: str):\n        \"\"\"\n        Format a field.\n        \"\"\"\n        if key == \"Earnings Date\":\n            date_str = value[:-3]\n            datetime_obj = datetime.strptime(date_str, \"%b %d, %Y, %I %p\")\n            tz_str = value[-3:]\n            if tz_str in [\"EST\", \"EDT\"]:\n                tz = pytz.timezone(\"EST\")\n            else:\n                raise ValueError(f\"Unknown timezone: {tz_str}\")\n            return tz.localize(datetime_obj)\n        elif key in [\"EPS Estimate\", \"Reported EPS\", \"Surprise(%)\"]:\n            if value == \"-\":\n                return None\n            return float(value)\n        elif key in [\"Symbol\", \"Company\"]:\n            return value\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def __get_earnings(self, ticker: str) -> Union[pd.DataFrame, None]:\n        \"\"\"\n        Get earnings for a given ticker.\n        \"\"\"\n        url = f\"https://finance.yahoo.com/calendar/earnings?symbol={ticker}\"\n        response = retry_get(url)\n        soup = BeautifulSoup(response.text, features=\"lxml\")\n        div = soup.find(\"div\", {\"id\": \"cal-res-table\"})\n        if div is None:\n            raise ValueError(f\"Could not find earnings for {ticker}\")\n        table = div.find(\"table\")\n        thead = table.find(\"thead\")\n        header = [th.text for th in thead.find_all(\"th\")]\n        expected_header = [\n            \"Symbol\",\n            \"Company\",\n            \"Earnings Date\",\n            \"EPS Estimate\",\n            \"Reported EPS\",\n            \"Surprise(%)\",\n        ]\n        assert header == expected_header\n        tbody = table.find(\"tbody\")\n        data = []\n        for row in tbody.find_all(\"tr\"):\n            data.append(\n                {\n                    key: self.__format_field(\n                        key,\n                        value=row.find_all(\"td\")[index].text,\n                    )\n                    for index, key in enumerate(header)\n                }\n            )\n        assert len(data) > 0\n        df = pd.DataFrame(data)\n        return df\n\n    def append_frame(self, symbol: str):\n        \"\"\"\n        Append a dataframe for a given symbol.\n        \"\"\"\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            df = self.__get_earnings(ticker)\n        except ValueError as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        df.drop(columns=[\"Symbol\", \"Company\"], inplace=True)\n        df.rename(\n            columns={\n                \"Earnings Date\": \"date\",\n                \"EPS Estimate\": \"eps_estimate\",\n                \"Reported EPS\": \"reported_eps\",\n                \"Surprise(%)\": \"surprise\",\n            },\n            inplace=True,\n        )\n        df[\"symbol\"] = symbol\n        # use reindex() to set 'symbol' as the first column\n        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        \"\"\"\n        Set the dataset dataframe.\n        \"\"\"\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/news.py", "chunked_list": ["\"\"\"\nNews from Yahoo Finance.\n\"\"\"\nfrom datetime import datetime, date\n\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nimport pandas as pd\nimport pytz\nimport requests", "import pytz\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import retry_get\n\n\nclass Article:\n    \"\"\"\n    Article.\n    \"\"\"\n\n    def __init__(self, url, uuid, title):\n        self.url = url\n        self.uuid = uuid\n        self.title = title\n        soup = self.__get_soup(url)\n        publisher_tag = soup.find(\"span\", {\"class\": \"caas-attr-provider\"})\n        self.publisher = publisher_tag.text if publisher_tag is not None else None\n        self.publish_time = self.__format_date(\n            soup.find(\"div\", {\"class\": \"caas-attr-time-style\"}).find(\"time\")[\n                \"datetime\"\n            ],\n        )\n        self.text = soup.find(\"div\", {\"class\": \"caas-body\"}).text\n\n    def __format_date(self, date_str):\n        \"\"\"\n        Format date.\n        \"\"\"\n        date_obj = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        tz = pytz.timezone(\"GMT\")\n        date_obj = tz.localize(date_obj)\n        return date_obj\n\n    def __get_soup(self, url):\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n        }\n        response = retry_get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        return soup\n\n    def to_json(self):\n        return {\n            \"body\": self.text,\n            \"publisher\": self.publisher,\n            \"publish_time\": self.publish_time,\n            \"title\": self.title,\n            \"url\": self.url,\n            \"uuid\": self.uuid,\n        }", "\n\nclass News(Raw):\n    \"\"\"\n    News from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"news-{self.suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"body\",\n            \"publisher\",\n            \"publish_time\",\n            \"title\",\n            \"url\",\n            \"uuid\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def __get_news(self, ticker: str) -> pd.DataFrame:\n        \"\"\"\n        Get news for a given ticker.\n        \"\"\"\n        url = f\"https://finance.yahoo.com/quote/{ticker}/?p={ticker}\"\n        response = retry_get(url)\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        news_tag = soup.find(\"div\", {\"id\": \"quoteNewsStream-0-Stream\"})\n        data = []\n        for h3_tag in news_tag.find_all(\"h3\"):\n            a_tag = h3_tag.find(\"a\")\n            if not a_tag.has_attr(\"data-uuid\"):\n                continue\n            article = Article(\n                url=\"https://finance.yahoo.com\" + a_tag[\"href\"],\n                uuid=a_tag[\"data-uuid\"],\n                title=a_tag.text,\n            )\n            data.append(article.to_json())\n        df = pd.DataFrame(data)\n        return df\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            df = self.__get_news(ticker)\n        except AttributeError as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        df[\"symbol\"] = symbol\n        # use reindex() to set 'symbol' as the first column\n        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"publish_time\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/earnings_forecast.py", "chunked_list": ["\"\"\"\nEarnings forecast from Nasdaq.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport pandas as pd", "from datasets import load_dataset\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import nasdaq_headers, retry_get\n\n\nclass EarningsForecast(Raw):\n    \"\"\"\n    Earnings forecast from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-forecast-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"fiscal_end\",\n            \"consensus_eps_forecast\",\n            \"high_eps_forecast\",\n            \"low_eps_forecast\",\n            \"no_of_estimates\",\n            \"up\",\n            \"down\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/analyst/{ticker}/earnings-forecast\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        quarterly_forecast = json_data[\"data\"][\"quarterlyForecast\"]\n        if quarterly_forecast is None:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=quarterly_forecast[\"rows\"])\n        df.rename(\n            columns={\n                \"fiscalEnd\": \"fiscal_end\",\n                \"consensusEPSForecast\": \"consensus_eps_forecast\",\n                \"highEPSForecast\": \"high_eps_forecast\",\n                \"lowEPSForecast\": \"low_eps_forecast\",\n                \"noOfEstimates\": \"no_of_estimates\",\n            },\n            inplace=True,\n        )\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "class EarningsForecast(Raw):\n    \"\"\"\n    Earnings forecast from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-forecast-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"fiscal_end\",\n            \"consensus_eps_forecast\",\n            \"high_eps_forecast\",\n            \"low_eps_forecast\",\n            \"no_of_estimates\",\n            \"up\",\n            \"down\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/analyst/{ticker}/earnings-forecast\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        quarterly_forecast = json_data[\"data\"][\"quarterlyForecast\"]\n        if quarterly_forecast is None:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=quarterly_forecast[\"rows\"])\n        df.rename(\n            columns={\n                \"fiscalEnd\": \"fiscal_end\",\n                \"consensusEPSForecast\": \"consensus_eps_forecast\",\n                \"highEPSForecast\": \"high_eps_forecast\",\n                \"lowEPSForecast\": \"low_eps_forecast\",\n                \"noOfEstimates\": \"no_of_estimates\",\n            },\n            inplace=True,\n        )\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\n\nif __name__ == \"__main__\":\n    symbol = \"AAPL\"\n    suffix = \"stocks\"\n    tag_date = datetime(2023, 5, 26).date()\n    username = \"edarchimbaud\"\n    dataset = EarningsForecast(suffix=suffix, tag_date=tag_date, username=username)\n    dataset.append_frame(symbol)\n", ""]}
{"filename": "systematic_trading/datasets/raw/__init__.py", "chunked_list": ["from datetime import date\nimport os\nimport pickle\n\nfrom systematic_trading.datasets.dataset import Dataset\n\n\nclass Raw(Dataset):\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.cache_dir = os.getenv(\"CACHE_DIR\", \"/tmp\")\n        self.frames = {}\n\n    def append_frame(self, symbol: str):\n        \"\"\"\n        Append frame.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_cache_path(self):\n        \"\"\"\n        Get cache path.\n        \"\"\"\n        tag = self.tag_date.isoformat()\n        return os.path.join(\n            self.cache_dir,\n            self.username,\n            self.name,\n            f\"{tag}.pkl\",\n        )\n\n    def load_frames(self):\n        file_path = self.get_cache_path()\n        if os.path.exists(file_path):\n            with open(file_path, \"rb\") as file:\n                self.frames = pickle.load(file)\n\n    def save_frames(self):\n        file_path = self.get_cache_path()\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, \"wb\") as file:\n            pickle.dump(self.frames, file)", ""]}
{"filename": "systematic_trading/datasets/raw/extended_trading.py", "chunked_list": ["\"\"\"\nExtended trading from Nasdaq.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport pandas as pd", "from datasets import load_dataset\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import nasdaq_headers, retry_get\n\n\nclass ExtendedTrading(Raw):\n    \"\"\"\n    Extended trading from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"extended-trading-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"time\",\n            \"price\",\n            \"share_volume\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        data = []\n        for n in range(1, 12):\n            url = f\"https://api.nasdaq.com/api/quote/{ticker}/extended-trading?markettype=pre&assetclass=stocks&time={n}\"\n            response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n            json_data = response.json()\n            if json_data[\"data\"] is None:\n                continue\n            trade_detail_table = json_data[\"data\"][\"tradeDetailTable\"]\n            if trade_detail_table[\"rows\"] is None:\n                continue\n            data += trade_detail_table[\"rows\"]\n        if len(data) == 0:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=data)\n        df.rename(\n            columns={\n                \"shareVolume\": \"share_volume\",\n            },\n            inplace=True,\n        )\n        df[\"price\"] = df[\"price\"].replace(\"\\$\", \"\", regex=True).astype(float)\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"time\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "class ExtendedTrading(Raw):\n    \"\"\"\n    Extended trading from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"extended-trading-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"time\",\n            \"price\",\n            \"share_volume\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        data = []\n        for n in range(1, 12):\n            url = f\"https://api.nasdaq.com/api/quote/{ticker}/extended-trading?markettype=pre&assetclass=stocks&time={n}\"\n            response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n            json_data = response.json()\n            if json_data[\"data\"] is None:\n                continue\n            trade_detail_table = json_data[\"data\"][\"tradeDetailTable\"]\n            if trade_detail_table[\"rows\"] is None:\n                continue\n            data += trade_detail_table[\"rows\"]\n        if len(data) == 0:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=data)\n        df.rename(\n            columns={\n                \"shareVolume\": \"share_volume\",\n            },\n            inplace=True,\n        )\n        df[\"price\"] = df[\"price\"].replace(\"\\$\", \"\", regex=True).astype(float)\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"time\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\n\nif __name__ == \"__main__\":\n    symbol = \"AAPL\"\n    suffix = \"stocks\"\n    tag_date = datetime(2023, 5, 26).date()\n    username = \"edarchimbaud\"\n    dataset = ExtendedTrading(suffix=suffix, tag_date=tag_date, username=username)\n    dataset.append_frame(symbol)\n", ""]}
{"filename": "systematic_trading/datasets/raw/timeseries_1mn.py", "chunked_list": ["\"\"\"\nTimeseries 1mn from Yahoo Finance.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport pandas as pd", "from datasets import load_dataset\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import retry_get\n\n\nclass Timeseries1mn(Raw):\n    \"\"\"\n    Timeseries 1mn from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"timeseries-1mn-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"datetime\",\n            \"open\",\n            \"high\",\n            \"low\",\n            \"close\",\n            \"volume\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?region=US&lang=en-US&includePrePost=false&interval=1m&useYfid=true&range=1d&corsDomain=finance.yahoo.com&.tsrc=finance\"\n        response = retry_get(url)\n        json_data = response.json()\n        result = json_data[\"chart\"][\"result\"][0]\n        if \"timestamp\" not in result:\n            self.frames[symbol] = None\n            return\n        timestamp = result[\"timestamp\"]\n        indicators = result[\"indicators\"][\"quote\"][0]\n        data = {\"datetime\": [datetime.fromtimestamp(t) for t in timestamp]}\n        data.update(indicators)\n        df = pd.DataFrame(data=data)\n        df[\"symbol\"] = symbol\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"datetime\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "class Timeseries1mn(Raw):\n    \"\"\"\n    Timeseries 1mn from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"timeseries-1mn-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"datetime\",\n            \"open\",\n            \"high\",\n            \"low\",\n            \"close\",\n            \"volume\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?region=US&lang=en-US&includePrePost=false&interval=1m&useYfid=true&range=1d&corsDomain=finance.yahoo.com&.tsrc=finance\"\n        response = retry_get(url)\n        json_data = response.json()\n        result = json_data[\"chart\"][\"result\"][0]\n        if \"timestamp\" not in result:\n            self.frames[symbol] = None\n            return\n        timestamp = result[\"timestamp\"]\n        indicators = result[\"indicators\"][\"quote\"][0]\n        data = {\"datetime\": [datetime.fromtimestamp(t) for t in timestamp]}\n        data.update(indicators)\n        df = pd.DataFrame(data=data)\n        df[\"symbol\"] = symbol\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"datetime\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\n\nif __name__ == \"__main__\":\n    symbol = \"AAPL\"\n    suffix = \"stocks\"\n    tag_date = datetime(2023, 5, 26).date()\n    username = \"edarchimbaud\"\n    dataset = Timeseries1mn(suffix=suffix, tag_date=tag_date, username=username)\n    dataset.append_frame(symbol)\n", ""]}
{"filename": "systematic_trading/datasets/raw/earnings_surprise.py", "chunked_list": ["\"\"\"\nEarnings surprise from Nasdaq.\n\"\"\"\n\nfrom datetime import date, datetime\nimport time\nimport urllib\n\nfrom datasets import load_dataset\nimport pandas as pd", "from datasets import load_dataset\nimport pandas as pd\nimport requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import nasdaq_headers, retry_get\n\n\nclass EarningsSurprise(Raw):\n    \"\"\"\n    Earnings surprise from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-surprise-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"fiscal_qtr_end\",\n            \"date_reported\",\n            \"eps\",\n            \"consensus_forecast\",\n            \"percentage_surprise\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/company/{ticker}/earnings-surprise\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        earnings_surprise = json_data[\"data\"][\"earningsSurpriseTable\"]\n        if earnings_surprise is None:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=earnings_surprise[\"rows\"])\n        df.rename(\n            columns={\n                \"fiscalQtrEnd\": \"fiscal_qtr_end\",\n                \"dateReported\": \"date_reported\",\n                \"consensusForecast\": \"consensus_forecast\",\n                \"percentageSurprise\": \"percentage_surprise\",\n            },\n            inplace=True,\n        )\n        df[\"date_reported\"] = pd.to_datetime(df[\"date_reported\"])\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "class EarningsSurprise(Raw):\n    \"\"\"\n    Earnings surprise from Nasdaq.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-surprise-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"id\",\n            \"fiscal_qtr_end\",\n            \"date_reported\",\n            \"eps\",\n            \"consensus_forecast\",\n            \"percentage_surprise\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        url = f\"https://api.nasdaq.com/api/company/{ticker}/earnings-surprise\"\n        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n        json_data = response.json()\n        if json_data[\"data\"] is None:\n            self.frames[symbol] = None\n            return\n        earnings_surprise = json_data[\"data\"][\"earningsSurpriseTable\"]\n        if earnings_surprise is None:\n            self.frames[symbol] = None\n            return\n        df = pd.DataFrame(data=earnings_surprise[\"rows\"])\n        df.rename(\n            columns={\n                \"fiscalQtrEnd\": \"fiscal_qtr_end\",\n                \"dateReported\": \"date_reported\",\n                \"consensusForecast\": \"consensus_forecast\",\n                \"percentageSurprise\": \"percentage_surprise\",\n            },\n            inplace=True,\n        )\n        df[\"date_reported\"] = pd.to_datetime(df[\"date_reported\"])\n        df[\"id\"] = range(len(df))\n        df[\"symbol\"] = symbol\n        df[\"date\"] = self.tag_date.isoformat()\n        df = df.reindex(columns=self.expected_columns)\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", "\n\nif __name__ == \"__main__\":\n    symbol = \"AAPL\"\n    suffix = \"stocks\"\n    tag_date = datetime(2023, 5, 26).date()\n    username = \"edarchimbaud\"\n    dataset = EarningsSurprise(suffix=suffix, tag_date=tag_date, username=username)\n    dataset.append_frame(symbol)\n", ""]}
{"filename": "systematic_trading/datasets/raw/analysis/earnings_estimate.py", "chunked_list": ["\"\"\"\nEarnings estimate from Yahoo Finance.\n\"\"\"\nfrom datetime import date\nfrom typing import Union\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nfrom systematic_trading.datasets.raw.analysis import Analysis", "\nfrom systematic_trading.datasets.raw.analysis import Analysis\n\n\nclass EarningsEstimate(Analysis):\n    \"\"\"\n    Earnings estimate from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"earnings-estimate-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"current_qtr\",\n            \"no_of_analysts_current_qtr\",\n            \"next_qtr\",\n            \"no_of_analysts_next_qtr\",\n            \"current_year\",\n            \"no_of_analysts_current_year\",\n            \"next_year\",\n            \"no_of_analysts_next_year\",\n            \"avg_estimate_current_qtr\",\n            \"avg_estimate_next_qtr\",\n            \"avg_estimate_current_year\",\n            \"avg_estimate_next_year\",\n            \"low_estimate_current_qtr\",\n            \"low_estimate_next_qtr\",\n            \"low_estimate_current_year\",\n            \"low_estimate_next_year\",\n            \"high_estimate_current_qtr\",\n            \"high_estimate_next_qtr\",\n            \"high_estimate_current_year\",\n            \"high_estimate_next_year\",\n            \"year_ago_eps_current_qtr\",\n            \"year_ago_eps_next_qtr\",\n            \"year_ago_eps_current_year\",\n            \"year_ago_eps_next_year\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def format_value(self, key: str, value: str) -> Union[int, float]:\n        \"\"\"\n        Format value.\n        \"\"\"\n        if value == \"N/A\":\n            return None\n        elif key.startswith(\"no_of_analysts\"):\n            return int(value)\n        elif key.startswith(\"avg_estimate\"):\n            return float(value)\n        elif key.startswith(\"low_estimate\"):\n            return float(value)\n        elif key.startswith(\"high_estimate\"):\n            return float(value)\n        elif key.startswith(\"year_ago_eps\"):\n            return float(value)\n        elif key == \"current_qtr\" or key == \"next_qtr\":\n            return value\n        elif key == \"current_year\" or key == \"next_year\":\n            return int(value)\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            data = self.get_analysis(ticker)\n            df = self.data_to_df(\n                data=data[0][\"Earnings Estimate\"],\n                field=\"Earnings Estimate\",\n                symbol=symbol,\n            )\n        except (IndexError, ValueError) as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        \"\"\"\n        Convert the frames to a dataset.\n        \"\"\"\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/analysis/revenue_estimate.py", "chunked_list": ["\"\"\"\nRevenue estimate from Yahoo Finance.\n\"\"\"\nfrom datetime import date\nfrom typing import Union\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nfrom systematic_trading.datasets.raw.analysis import Analysis", "\nfrom systematic_trading.datasets.raw.analysis import Analysis\n\n\nclass RevenueEstimate(Analysis):\n    \"\"\"\n    Revenue estimate from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"revenue-estimate-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"current_qtr\",\n            \"no_of_analysts_current_qtr\",\n            \"next_qtr\",\n            \"no_of_analysts_next_qtr\",\n            \"current_year\",\n            \"no_of_analysts_current_year\",\n            \"next_year\",\n            \"no_of_analysts_next_year\",\n            \"avg_estimate_current_qtr\",\n            \"avg_estimate_next_qtr\",\n            \"avg_estimate_current_year\",\n            \"avg_estimate_next_year\",\n            \"low_estimate_current_qtr\",\n            \"low_estimate_next_qtr\",\n            \"low_estimate_current_year\",\n            \"low_estimate_next_year\",\n            \"high_estimate_current_qtr\",\n            \"high_estimate_next_qtr\",\n            \"high_estimate_current_year\",\n            \"high_estimate_next_year\",\n            \"year_ago_sales_current_qtr\",\n            \"year_ago_sales_next_qtr\",\n            \"year_ago_sales_current_year\",\n            \"year_ago_sales_next_year\",\n            \"sales_growth_yearest_current_qtr\",\n            \"sales_growth_yearest_next_qtr\",\n            \"sales_growth_yearest_current_year\",\n            \"sales_growth_yearest_next_year\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def format_value(self, key: str, value: str) -> Union[int, float]:\n        \"\"\"\n        Format value.\n        \"\"\"\n        if value == \"N/A\":\n            return None\n        elif key.startswith(\"no_of_analysts\"):\n            return int(value)\n        elif (\n            key.startswith(\"avg_estimate\")\n            or key.startswith(\"low_estimate\")\n            or key.startswith(\"high_estimate\")\n            or key.startswith(\"year_ago_sales\")\n            or key.startswith(\"sales_growth_yearest\")\n        ):\n            return value\n        elif key == \"current_qtr\" or key == \"next_qtr\":\n            return value\n        elif key == \"current_year\" or key == \"next_year\":\n            return int(value)\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            data = self.get_analysis(ticker)\n            df = self.data_to_df(\n                data=data[1][\"Revenue Estimate\"],\n                field=\"Revenue Estimate\",\n                symbol=symbol,\n            )\n        except (IndexError, ValueError) as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/analysis/__init__.py", "chunked_list": ["\"\"\"\nEarnings data.\n\"\"\"\nfrom typing import Union\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\nimport requests\n", "import requests\n\nfrom systematic_trading.datasets.raw import Raw\nfrom systematic_trading.helpers import retry_get\n\n\nclass Analysis(Raw):\n    \"\"\"\n    Analysis data from Yahoo Finance.\n    \"\"\"\n\n    def get_analysis(self, ticker: str) -> pd.DataFrame:\n        \"\"\"\n        Get analysis for a given ticker.\n        \"\"\"\n        url = f\"https://finance.yahoo.com/quote/{ticker}/analysis?p={ticker}\"\n        response = retry_get(url)\n        soup = BeautifulSoup(response.text, features=\"lxml\")\n        div = soup.find(\"div\", {\"id\": \"Col1-0-AnalystLeafPage-Proxy\"})\n        tables = div.find_all(\"table\") if div is not None else None\n        if tables is None:\n            raise ValueError(f\"No tables found for {ticker}\")\n        expected_table_names = [\n            \"Earnings Estimate\",\n            \"Revenue Estimate\",\n            \"Earnings History\",\n            \"EPS Trend\",\n            \"EPS Revisions\",\n            \"Growth Estimates\",\n        ]\n        data = []\n        for index, expected_table_name in enumerate(expected_table_names):\n            table = tables[index]\n            thead = table.find(\"thead\")\n            header = [th.text for th in thead.find_all(\"th\")]\n            assert len(header) == 5\n            assert header[0] == expected_table_name\n            tbody = table.find(\"tbody\")\n            table_data = []\n            for row in tbody.find_all(\"tr\"):\n                table_data.append(\n                    {k: row.find_all(\"td\")[i].text for i, k in enumerate(header)}\n                )\n            data.append(\n                {\n                    expected_table_name: table_data,\n                }\n            )\n        assert len(data) > 0\n        return data\n\n    def __format_column(self, column: str) -> str:\n        \"\"\"\n        Format column.\n        \"\"\"\n        return re.sub(r\"[^a-z0-9_]\", \"\", column.replace(\" \", \"_\").lower())\n\n    def format_value(self, column: str, value: str) -> Union[float, None]:\n        raise NotImplementedError\n\n    def data_to_df(\n        self,\n        data,\n        field: str,\n        symbol: str,\n    ) -> pd.DataFrame:\n        df = pd.DataFrame(data=data)\n        df.set_index(field, inplace=True, drop=True)\n        data_dict = {}\n        for index, row in df.iterrows():\n            for col in df.columns:\n                period_column = self.__format_column(col.split(\" (\")[0])\n                period = col.split(\" (\")[1].replace(\")\", \"\")\n                data_dict[period_column] = self.format_value(period_column, period)\n                column = self.__format_column(index + \" \" + period_column)\n                data_dict[column] = self.format_value(column, row[col])\n        df = pd.DataFrame(data=[data_dict])\n        df[\"date\"] = self.tag_date.isoformat()\n        df[\"symbol\"] = symbol\n        df = df.reindex(columns=[\"symbol\", \"date\"] + list(df.columns[:-2]))\n        return df", ""]}
{"filename": "systematic_trading/datasets/raw/analysis/eps_revisions.py", "chunked_list": ["\"\"\"\nEPS Revisions from Yahoo Finance.\n\"\"\"\nfrom datetime import date\nfrom typing import Union\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nfrom systematic_trading.datasets.raw.analysis import Analysis", "\nfrom systematic_trading.datasets.raw.analysis import Analysis\n\n\nclass EPSRevisions(Analysis):\n    \"\"\"\n    EPS Revisions from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"eps-revisions-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"current_qtr\",\n            \"up_last_7_days_current_qtr\",\n            \"next_qtr\",\n            \"up_last_7_days_next_qtr\",\n            \"current_year\",\n            \"up_last_7_days_current_year\",\n            \"next_year\",\n            \"up_last_7_days_next_year\",\n            \"up_last_30_days_current_qtr\",\n            \"up_last_30_days_next_qtr\",\n            \"up_last_30_days_current_year\",\n            \"up_last_30_days_next_year\",\n            \"down_last_7_days_current_qtr\",\n            \"down_last_7_days_next_qtr\",\n            \"down_last_7_days_current_year\",\n            \"down_last_7_days_next_year\",\n            \"down_last_30_days_current_qtr\",\n            \"down_last_30_days_next_qtr\",\n            \"down_last_30_days_current_year\",\n            \"down_last_30_days_next_year\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def format_value(self, key: str, value: str) -> Union[int, float]:\n        \"\"\"\n        Format value.\n        \"\"\"\n        if value == \"N/A\":\n            return None\n        elif \"_last_\" in key:\n            return int(value)\n        elif key == \"current_qtr\" or key == \"next_qtr\":\n            return value\n        elif key == \"current_year\" or key == \"next_year\":\n            return int(value)\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def append_frame(self, symbol: str) -> None:\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            data = self.get_analysis(ticker)\n            df = self.data_to_df(\n                data=data[4][\"EPS Revisions\"],\n                field=\"EPS Revisions\",\n                symbol=symbol,\n            )\n        except (IndexError, ValueError) as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
{"filename": "systematic_trading/datasets/raw/analysis/eps_trend.py", "chunked_list": ["\"\"\"\nEPS Trend from Yahoo Finance.\n\"\"\"\nfrom datetime import date\nfrom typing import Union\n\nfrom datasets import load_dataset\nimport pandas as pd\n\nfrom systematic_trading.datasets.raw.analysis import Analysis", "\nfrom systematic_trading.datasets.raw.analysis import Analysis\n\n\nclass EPSTrend(Analysis):\n    \"\"\"\n    EPS Trend from Yahoo Finance.\n    \"\"\"\n\n    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n        super().__init__(suffix, tag_date, username)\n        self.name = f\"eps-trend-{suffix}\"\n        self.expected_columns = [\n            \"symbol\",\n            \"date\",\n            \"current_qtr\",\n            \"current_estimate_current_qtr\",\n            \"next_qtr\",\n            \"current_estimate_next_qtr\",\n            \"current_year\",\n            \"current_estimate_current_year\",\n            \"next_year\",\n            \"current_estimate_next_year\",\n            \"7_days_ago_current_qtr\",\n            \"7_days_ago_next_qtr\",\n            \"7_days_ago_current_year\",\n            \"7_days_ago_next_year\",\n            \"30_days_ago_current_qtr\",\n            \"30_days_ago_next_qtr\",\n            \"30_days_ago_current_year\",\n            \"30_days_ago_next_year\",\n            \"60_days_ago_current_qtr\",\n            \"60_days_ago_next_qtr\",\n            \"60_days_ago_current_year\",\n            \"60_days_ago_next_year\",\n            \"90_days_ago_current_qtr\",\n            \"90_days_ago_next_qtr\",\n            \"90_days_ago_current_year\",\n            \"90_days_ago_next_year\",\n        ]\n        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\n    def format_value(self, key: str, value: str) -> Union[int, float]:\n        \"\"\"\n        Format value.\n        \"\"\"\n        if value == \"N/A\":\n            return None\n        elif key.startswith(\"current_estimate\") or \"_days_ago_\" in key:\n            return float(value)\n        elif key == \"current_qtr\" or key == \"next_qtr\":\n            return value\n        elif key == \"current_year\" or key == \"next_year\":\n            return int(value)\n        else:\n            raise ValueError(f\"Unknown key: {key}\")\n\n    def append_frame(self, symbol: str):\n        ticker = self.symbol_to_ticker(symbol)\n        try:\n            data = self.get_analysis(ticker)\n            df = self.data_to_df(\n                data=data[3][\"EPS Trend\"],\n                field=\"EPS Trend\",\n                symbol=symbol,\n            )\n        except (IndexError, ValueError) as e:\n            print(f\"Exception for {self.name}: {symbol}: {e}\")\n            self.frames[symbol] = None\n            return\n        self.frames[symbol] = df\n\n    def set_dataset_df(self):\n        \"\"\"\n        Download the EPS trend data from Yahoo Finance.\n        \"\"\"\n        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n        if self.check_file_exists():\n            self.add_previous_data()\n        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n        self.dataset_df.reset_index(drop=True, inplace=True)", ""]}
