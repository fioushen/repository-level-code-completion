{"filename": "main.py", "chunked_list": ["import datetime\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n\nclass Gracie(BaseModel):\n    name: str\n    hello_utc: datetime.datetime\n", "\n\napp = FastAPI()\n\n\n@app.get(\"/goodnight\", response_model=Gracie)\nasync def goodnight() -> Gracie:\n    goodnight = Gracie(name=\"Gracie\", hello_utc=datetime.datetime.now())\n\n    return goodnight", "\n    return goodnight\n"]}
{"filename": "tests/test_rico.py", "chunked_list": ["from rico import __version__\n\n\ndef test_version() -> None:\n    assert __version__ == \"0.1.0\"\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"Rico\"\ncopyright = \"2023, Hank Corbett\"", "project = \"Rico\"\ncopyright = \"2023, Hank Corbett\"\nauthor = \"Hank Corbett\"\nrelease = \"0.1\"\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    \"sphinx.ext.todo\",", "extensions = [\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx_click\",\n    \"sphinxcontrib.autodoc_pydantic\",\n]\n\ntemplates_path = [\"_templates\"]", "\ntemplates_path = [\"_templates\"]\nexclude_patterns = []\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"furo\"\nhtml_static_path = [\"_static\"]\nhtml_theme_options = {", "html_static_path = [\"_static\"]\nhtml_theme_options = {\n    \"light_logo\": \"argus_logo_light.png\",\n    \"dark_logo\": \"argus_logo_dark.png\",\n    \"announcement\": \"<em>Important</em>: This package is currently under construction! Contact us for more info.\",\n}\nhtml_title = \"Rico Docs\"\n\nautodoc_pydantic_model_show_json = True\nautodoc_pydantic_settings_show_json = False", "autodoc_pydantic_model_show_json = True\nautodoc_pydantic_settings_show_json = False\n"]}
{"filename": "src/argus_rico/images.py", "chunked_list": ["__all__ = [\"images_containing\", \"get_image_meta\", \"EVRImageProducer\", \"EVRImageLoader\"]\n\nimport datetime\nimport glob\nimport os\nfrom typing import Any, Dict, Union\n\nimport astropy.io.fits as fits\nimport pandas as pd\nimport pymongoarrow.monkey", "import pandas as pd\nimport pymongoarrow.monkey\nfrom pymongo import MongoClient\nfrom pymongoarrow.api import Schema\n\nfrom . import config, models\nfrom .models import fitspath_to_constructor\nfrom .producer import Producer\n\npymongoarrow.monkey.patch_all()", "\npymongoarrow.monkey.patch_all()\n\n\ndef images_containing(\n    ra: float, dec: float, date_start: datetime.datetime, date_end: datetime.datetime\n) -> pd.DataFrame:\n    \"\"\"\n    Retrieve images containing a given position within a specified time range.\n\n    Args:\n        ra (float): Right ascension (ICRS) of the target position.\n        dec (float): Declination (ICRS) of the target position.\n        date_start (datetime.datetime): Start date of the time range.\n        date_end (datetime.datetime): End date of the time range.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the retrieved images.\n\n    \"\"\"\n    client = MongoClient(config.MONGODB_URI)\n    collection = client[config.MONGO_DBNAME].evr_images\n    images = collection.find_pandas_all(\n        {\n            \"$and\": [{\"rawpath\": {\"$ne\": None}}, {\"wcspath\": {\"$ne\": None}}],\n            \"footprint\": {\n                \"$geoIntersects\": {\n                    \"$geometry\": {\"type\": \"Point\", \"coordinates\": [ra - 180.0, dec]}\n                }\n            },\n            \"obstime\": {\"$gte\": date_start, \"$lte\": date_end},\n            \"image_type\": \"object\",\n        },\n        schema=Schema(\n            {\n                \"obstime\": datetime.datetime,\n                \"image_type\": str,\n                \"camera\": str,\n                \"rawpath\": str,\n                \"wcspath\": str,\n            }\n        ),\n    )\n    return images", "\n\ndef get_image_meta(\n    path: str,\n) -> Dict[str, Any]:\n    \"\"\"\n    Retrieve image metadata by filename.\n\n    Args:\n        path (str): Filename for the image\n\n    Returns:\n        Dict: Dictionary of image metadata\n\n    \"\"\"\n\n    basename = os.path.basename(path).split(\".\")[0]\n\n    client = MongoClient(config.MONGODB_URI)\n    collection = client[config.MONGO_DBNAME].evr_images\n    image = collection.find_one({\"basename\": basename})\n    return image", "\n\nclass EVRImageProducer(Producer):\n    \"\"\"\n    A Kafka producer for sending EVR images.\n\n    Args:\n        None\n\n    Attributes:\n        host (str): The Kafka host address.\n        port (int): The Kafka port number.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialize the EVRImageProducer.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        \"\"\"\n        super().__init__(\n            host=config.KAFKA_ADDR,\n            port=config.KAFKA_PORT,\n        )\n\n    def send_image(self, image: Union[str, fits.HDUList]) -> None:\n        \"\"\"\n        Send an image to the EVR image topic.\n\n        Args:\n            image (Union[str, fits.HDUList]): The image to be sent. It can be either a string representing the\n                path to a FITS file or a `fits.HDUList` object.\n\n        Returns:\n            None\n\n        \"\"\"\n        image_dict = fitspath_to_constructor(image)\n        self.send_json(image_dict, config.EVR_IMAGE_TOPIC)", "\n\nclass EVRImageLoader:\n    \"\"\"Class for loading EVRImage data into a MongoDB collection.\"\"\"\n\n    def __init__(self, create_client: bool = True) -> None:\n        \"\"\"\n        Initialize EVRImageLoader.\n\n        Args:\n            create_client (bool): Whether to create a MongoDB client. Default is True.\n\n        Returns:\n            None\n        \"\"\"\n        self.client: MongoClient[Dict[str, Any]]\n        self.client_loaded: bool = create_client\n\n        if create_client:\n            self.client = MongoClient(config.MONGODB_URI, uuidRepresentation=\"standard\")\n\n    def load_fits(self, path: str) -> None:\n        \"\"\"\n        Load FITS file data into the MongoDB collection.\n\n        Args:\n            path (str): Path to the FITS file.\n\n        Returns:\n            None\n        \"\"\"\n        img_coll = self.client[config.MONGO_DBNAME].evr_images\n\n        db_img = img_coll.find_one({\"basename\": os.path.basename(path).split(\".\")[0]})\n        if db_img is None:\n            img_new = models.EVRImage.from_fits(path)\n            _ = img_coll.insert_one(img_new.dict())\n        else:\n            img_existing = models.EVRImageUpdate.from_fits(path)\n            _ = img_coll.update_one(\n                {\"_id\": db_img[\"_id\"]}, {\"$set\": img_existing.dict(exclude_unset=True)}\n            )\n\n    def load_directory(self, dirname: str) -> None:\n        \"\"\"\n        Load all FITS files from a directory into the MongoDB collection.\n\n        Args:\n            dirname (str): Directory path containing FITS files.\n\n        Returns:\n            None\n        \"\"\"\n        if not self.client_loaded:\n            self.client = MongoClient(config.MONGODB_URI, uuidRepresentation=\"standard\")\n        images = glob.glob(os.path.join(dirname, \"*.fits\"))\n\n        for image in images:\n            self.load_fits(image)", ""]}
{"filename": "src/argus_rico/slack.py", "chunked_list": ["__all__ = [\"slack_app\", \"slack_app_starter\"]\nimport datetime\nimport os\nimport time\nfrom typing import Any, Callable, Dict\n\nimport slack_sdk as sk\nfrom sanitize_filename import sanitize\nfrom slack_bolt import App\n", "from slack_bolt import App\n\nfrom . import config, get_logger\nfrom . import images as rimages\nfrom .efte import EFTERunner\n\nlog = get_logger(\"slack_bot\")\n\nif config.SLACK_BOT_TOKEN is not None:\n    slack_app = App(\n        token=config.SLACK_BOT_TOKEN,\n        signing_secret=config.SLACK_SIGNING_SECRET,\n    )", "if config.SLACK_BOT_TOKEN is not None:\n    slack_app = App(\n        token=config.SLACK_BOT_TOKEN,\n        signing_secret=config.SLACK_SIGNING_SECRET,\n    )\n\n\n@slack_app.event(\"message\")\ndef handle_message_events(body: Dict[str, Any]) -> None:\n    \"\"\"Handle message events in the Slack app.\n\n    Args:\n        body (Dict[str, Any]): The payload containing the event data.\n\n    Returns:\n        None.\n    \"\"\"\n    pass", "def handle_message_events(body: Dict[str, Any]) -> None:\n    \"\"\"Handle message events in the Slack app.\n\n    Args:\n        body (Dict[str, Any]): The payload containing the event data.\n\n    Returns:\n        None.\n    \"\"\"\n    pass", "\n\n@slack_app.message(\"hi Rico\")\ndef message_hello(message: Dict[str, Any], say: Callable[..., None]) -> None:\n    \"\"\"Handle the \"hi Rico\" message event in the Slack app.\n\n    Args:\n        message (Dict[str, Any]): The incoming message data.\n        say (Callable[..., None]): Function to send a message as a response.\n\n    Returns:\n        None.\n    \"\"\"\n    say(f\"Hey there <@{message['user']}>!\", thread_ts=message[\"ts\"])", "\n\n@slack_app.event(\"reaction_added\")\ndef telescope_trigger(\n    body: Dict[str, Any], ack: Callable[..., None], say: Callable[..., None]\n) -> None:\n    \"\"\"\n    Triggered when a reaction is added in the Slack app.\n\n    Args:\n        body (dict): The payload containing the event data.\n        ack (function): Acknowledgment function to confirm the event was received.\n        say (function): Function to send a message as a response.\n\n    Returns:\n        None.\n    \"\"\"\n    # Acknowledge the action\n    ack()\n    if \"telescope\" in body[\"event\"][\"reaction\"]:\n        event = body[\"event\"]\n        thread_ts = event[\"item\"][\"ts\"]\n        say(\n            f\"<@{event['user']}> initiated a follow-up request.\",\n            thread_ts=thread_ts,\n        )", "\n\n@slack_app.shortcut(\"rico_lightcurve_req\")\ndef open_lcv_modal(ack, shortcut, client):\n    \"\"\"Open a modal for requesting a lightcurve in the Slack app.\n\n    Args:\n        ack (Callable[..., None]): Acknowledgment function to confirm the shortcut request.\n        shortcut (Dict[str, Any]): The shortcut data.\n        client: The Slack WebClient instance.\n\n    Returns:\n        None.\n    \"\"\"\n    # Acknowledge the shortcut request\n    ack()\n    # Call the views_open method using the built-in WebClient\n    client.views_open(\n        trigger_id=shortcut[\"trigger_id\"],\n        # A simple view payload for a modal\n        view={\n            \"type\": \"modal\",\n            \"callback_id\": \"modal-lcv-req\",\n            \"title\": {\"type\": \"plain_text\", \"text\": \"Forced Photometry\", \"emoji\": True},\n            \"submit\": {\"type\": \"plain_text\", \"text\": \"Submit\", \"emoji\": True},\n            \"close\": {\"type\": \"plain_text\", \"text\": \"Cancel\", \"emoji\": True},\n            \"blocks\": [\n                {\n                    \"type\": \"input\",\n                    \"block_id\": \"target_name\",\n                    \"element\": {\n                        \"type\": \"plain_text_input\",\n                        \"action_id\": \"title\",\n                        \"placeholder\": {\"type\": \"plain_text\", \"text\": \"TRAPPIST-1\"},\n                    },\n                    \"label\": {\"type\": \"plain_text\", \"text\": \"Target Name\"},\n                },\n                {\n                    \"type\": \"actions\",\n                    \"block_id\": \"time_range\",\n                    \"elements\": [\n                        {\n                            \"type\": \"datepicker\",\n                            \"initial_date\": \"2015-07-01\",\n                            \"placeholder\": {\n                                \"type\": \"plain_text\",\n                                \"text\": \"Select a date\",\n                                \"emoji\": True,\n                            },\n                            \"action_id\": \"start_date\",\n                        },\n                        {\n                            \"type\": \"datepicker\",\n                            \"initial_date\": \"2030-01-01\",\n                            \"placeholder\": {\n                                \"type\": \"plain_text\",\n                                \"text\": \"Select a date\",\n                                \"emoji\": True,\n                            },\n                            \"action_id\": \"end_date\",\n                        },\n                    ],\n                },\n                {\n                    \"type\": \"input\",\n                    \"block_id\": \"right_ascension\",\n                    \"element\": {\n                        \"type\": \"number_input\",\n                        \"is_decimal_allowed\": True,\n                        \"action_id\": \"number_input-action\",\n                    },\n                    \"label\": {\n                        \"type\": \"plain_text\",\n                        \"text\": \"Right Ascension\",\n                        \"emoji\": True,\n                    },\n                },\n                {\n                    \"type\": \"input\",\n                    \"block_id\": \"declination\",\n                    \"element\": {\n                        \"type\": \"number_input\",\n                        \"is_decimal_allowed\": True,\n                        \"action_id\": \"number_input-action\",\n                    },\n                    \"label\": {\n                        \"type\": \"plain_text\",\n                        \"text\": \"Declination\",\n                        \"emoji\": True,\n                    },\n                },\n            ],\n        },\n    )", "\n\n@slack_app.view(\"modal-lcv-req\")\ndef handle_submission(\n    ack: Callable[..., None], body: Dict[str, Any], client: sk.WebClient\n) -> None:\n    \"\"\"Handle the submission of the lightcurve request modal in the Slack app.\n\n    Args:\n        ack (Callable[..., None]): Acknowledgment function to confirm the submission.\n        body (Dict[str, Any]): The payload containing the submission data.\n        client: The Slack WebClient instance.\n\n    Returns:\n        None.\n    \"\"\"\n    values = body[\"view\"][\"state\"][\"values\"]\n    user = body[\"user\"][\"id\"]\n\n    print(values)\n\n    target_name = sanitize(values[\"target_name\"][\"title\"][\"value\"])\n    start_date = values[\"time_range\"][\"start_date\"][\"selected_date\"]\n    end_date = values[\"time_range\"][\"end_date\"][\"selected_date\"]\n    right_ascension = float(values[\"right_ascension\"][\"number_input-action\"][\"value\"])\n    declination = float(values[\"declination\"][\"number_input-action\"][\"value\"])\n\n    errors = {}\n    if (right_ascension < 0) or (right_ascension > 360):\n        errors[\"right_ascension\"] = \"Right ascension out of range.\"\n    if (declination < -90) or (declination > 90):\n        errors[\"declination\"] = \"Declination out of range.\"\n    if len(errors) > 0:\n        ack(response_action=\"errors\", errors=errors)\n        return\n    # Acknowledge the view_submission request and close the modal\n    ack()\n    # Do whatever you want with the input data - here we're querying the DB and\n    # then passing off the lcv gen to a background worker\n\n    # TODO: probably can refactor this to only hit the DB once, but what's 15\n    # seconds out of 5 hours?\n    nimages = len(\n        rimages.images_containing(\n            right_ascension,\n            declination,\n            datetime.datetime.strptime(start_date, \"%Y-%m-%d\"),\n            datetime.datetime.strptime(end_date, \"%Y-%m-%d\"),\n        )\n    )\n    uncompleteable = False\n    # Message to send user\n    msg = \"\"\n    try:\n        if nimages >= 1:\n            msg = f\"Hi <@{user}>! I have received your request to produce the following lightcurve:\\n*{target_name}* at RA: {right_ascension}, Dec: {declination}.\\nFrom {start_date} to {end_date}.\\n\\n *Please note that I don't know how long this will take! Your table will be uploaded here ASAP.*\"\n        else:\n            msg = f\"Hi <@{user}>! I received your forced photometry request, but didn't find any images.\"\n            uncompleteable = True\n    except Exception as e:\n        # Handle error\n        msg = f\"There was an error with your submission: {e}\"\n    #\n    # Message the user\n    try:\n        response = client.chat_postMessage(channel=user, text=msg)\n    except Exception as e:\n        print(f\"Failed to post a message {e}\")\n        return\n    if uncompleteable:\n        return\n\n    dm_channel = response[\"channel\"]\n    print(\n        f\"efte -n 72 autophot --mindate {start_date} --maxdate {end_date} -o {target_name} {right_ascension} -- {declination}\"\n    )\n\n    er = EFTERunner()\n    er.run(\n        f\"efte -n 72 autophot --mindate {start_date} --maxdate {end_date} -o {target_name} {right_ascension} -- {declination}\"\n    )\n    output = (\n        target_name\n        + f\"_{right_ascension:.2f}_{declination:.2f}\".replace(\".\", \"d\").replace(\n            \"-\", \"m\"\n        )\n        + \".fits\"\n    )\n    while not os.path.isfile(output):\n        time.sleep(60)\n\n    client.files_upload_v2(\n        file=f\"./{output}\",\n        channel=dm_channel,\n        title=output,\n        initial_comment=\"Your lightcurve is ready!\",\n    )", "\n\ndef slack_app_starter() -> None:\n    log.info(\"Slack integration starting up...\")\n    slack_app.start(port=int(config.SLACK_PORT))\n"]}
{"filename": "src/argus_rico/producer.py", "chunked_list": ["from typing import Any, Dict, Optional, Union\nfrom uuid import uuid4\n\nimport confluent_kafka as ck\nimport orjson\n\nfrom . import get_logger\n\n\nclass Producer:\n    def __init__(\n        self,\n        host: str,\n        port: Union[int, str],\n    ) -> None:\n        \"\"\"\n        Initialize the Producer instance.\n\n        Args:\n            host (str): The hostname or IP address of the Kafka broker.\n            port (Union[int, str]): The port number of the Kafka broker.\n            topic (str): The name of the topic to produce messages to.\n        \"\"\"\n        self.p = ck.Producer({\"bootstrap.servers\": f\"{host}:{port}\"})\n        self.log = get_logger(__name__)\n\n    def delivery_report(self, err: Optional[ck.KafkaError], msg: ck.Message) -> None:\n        \"\"\"\n        Reports the failure or success of a message delivery.\n\n        Args:\n            err (Optional[ck.KafkaError]): The error that occurred, or None on success.\n            msg (ck.Message): The message that was produced or failed.\n\n        Note:\n            In the delivery report callback, the Message.key() and Message.value()\n            will be in binary format as encoded by any configured Serializers and\n            not the same object that was passed to produce().\n            If you wish to pass the original object(s) for key and value to the delivery\n            report callback, we recommend using a bound callback or lambda where you pass\n            the objects along.\n        \"\"\"\n        if err is not None:\n            self.log.error(f\"Delivery failed for User record {msg.key()}: {err}\")\n            return\n        self.log.info(\n            f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\"\n        )\n\n    def send_json(self, message: Dict[Any, Any], topic: str) -> None:\n        \"\"\"\n        Send a JSON message to the Kafka topic.\n\n        Args:\n            message (dict): The message to be sent, represented as a dictionary.\n            topic (str): The name of the topic to send the message to.\n        \"\"\"\n        self.p.produce(\n            topic=topic,\n            key=str(uuid4()),\n            value=orjson.dumps(message),\n            on_delivery=self.delivery_report,\n        )\n        self.p.flush()\n\n    def send_binary(self, payload: bytes, topic: str) -> None:\n        \"\"\"\n        Send a binary payload to the Kafka topic.\n\n        Args:\n            payload (bytes): The payload to be sent, represented as a bytestring.\n            topic (str): The name of the topic to send the payload to.\n        \"\"\"\n        self.p.produce(\n            topic=topic,\n            key=str(uuid4()),\n            value=payload,\n            on_delivery=self.delivery_report,\n        )\n        self.p.flush()", "\nclass Producer:\n    def __init__(\n        self,\n        host: str,\n        port: Union[int, str],\n    ) -> None:\n        \"\"\"\n        Initialize the Producer instance.\n\n        Args:\n            host (str): The hostname or IP address of the Kafka broker.\n            port (Union[int, str]): The port number of the Kafka broker.\n            topic (str): The name of the topic to produce messages to.\n        \"\"\"\n        self.p = ck.Producer({\"bootstrap.servers\": f\"{host}:{port}\"})\n        self.log = get_logger(__name__)\n\n    def delivery_report(self, err: Optional[ck.KafkaError], msg: ck.Message) -> None:\n        \"\"\"\n        Reports the failure or success of a message delivery.\n\n        Args:\n            err (Optional[ck.KafkaError]): The error that occurred, or None on success.\n            msg (ck.Message): The message that was produced or failed.\n\n        Note:\n            In the delivery report callback, the Message.key() and Message.value()\n            will be in binary format as encoded by any configured Serializers and\n            not the same object that was passed to produce().\n            If you wish to pass the original object(s) for key and value to the delivery\n            report callback, we recommend using a bound callback or lambda where you pass\n            the objects along.\n        \"\"\"\n        if err is not None:\n            self.log.error(f\"Delivery failed for User record {msg.key()}: {err}\")\n            return\n        self.log.info(\n            f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\"\n        )\n\n    def send_json(self, message: Dict[Any, Any], topic: str) -> None:\n        \"\"\"\n        Send a JSON message to the Kafka topic.\n\n        Args:\n            message (dict): The message to be sent, represented as a dictionary.\n            topic (str): The name of the topic to send the message to.\n        \"\"\"\n        self.p.produce(\n            topic=topic,\n            key=str(uuid4()),\n            value=orjson.dumps(message),\n            on_delivery=self.delivery_report,\n        )\n        self.p.flush()\n\n    def send_binary(self, payload: bytes, topic: str) -> None:\n        \"\"\"\n        Send a binary payload to the Kafka topic.\n\n        Args:\n            payload (bytes): The payload to be sent, represented as a bytestring.\n            topic (str): The name of the topic to send the payload to.\n        \"\"\"\n        self.p.produce(\n            topic=topic,\n            key=str(uuid4()),\n            value=payload,\n            on_delivery=self.delivery_report,\n        )\n        self.p.flush()", ""]}
{"filename": "src/argus_rico/__init__.py", "chunked_list": ["\"\"\"Top-level package for Evryscope-Argus Transient Reporter.\"\"\"\n\n__author__ = \"\"\"Hank Corbett\"\"\"\n__email__ = \"htc@unc.edu\"\n__all__ = [\"get_logger\", \"config\"]\n\nimport importlib.metadata\nimport logging\nimport os\n", "import os\n\nfrom dotenv import load_dotenv\n\n__version__ = importlib.metadata.version(\"argus-rico\")\n\nbasedir = os.path.abspath(os.path.dirname(__file__))\n\nif os.path.isfile(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\")):\n    load_dotenv(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\"))\nelse:\n    load_dotenv(os.path.join(basedir, \".env\"))", "if os.path.isfile(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\")):\n    load_dotenv(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\"))\nelse:\n    load_dotenv(os.path.join(basedir, \".env\"))\n\n\nclass Config(object):\n    \"\"\"\n    Configuration class for managing application settings.\n\n    Attributes:\n        LOCAL_ADDR (str): The local address to bind the application to. Defaults to \"127.0.0.1\" if not provided in the environment.\n        KAFKA_ADDR (str): The address of the Kafka server. Defaults to \"152.2.38.172\" if not provided in the environment.\n        KAFKA_PORT (str): The port of the Kafka server. Defaults to \"9092\" if not provided in the environment.\n        HBEAT_TOPIC (str): The topic name for heartbeat messages in Kafka. Defaults to \"rico-hearbeat\" if not provided in the environment.\n    \"\"\"\n\n    SLACK_SIGNING_SECRET = os.environ.get(\"SLACK_SIGNING_SECRET\") or None\n    SLACK_BOT_TOKEN = os.environ.get(\"SLACK_BOT_TOKEN\") or None\n    SLACK_PORT = os.environ.get(\"SLACK_PORT\") or 3000\n\n    LOCAL_ADDR = os.environ.get(\"LOCAL_ADDR\") or \"127.0.0.1\"\n\n    KAFKA_ADDR = os.environ.get(\"KAFKA_ADDR\") or \"127.0.0.1\"\n    KAFKA_PORT = os.environ.get(\"KAFKA_PORT\") or \"9092\"\n    HBEAT_TOPIC = os.environ.get(\"HBEAT_TOPIC\") or \"rico.heartbeat\"\n    RAW_TOPIC_BASE = os.environ.get(\"RAW_TOPIC_BASE\") or \"rico.candidates.raw\"\n    EFTE_TOPIC_BASE = os.environ.get(\"EFTE_TOPIC_BASE\") or \"rico.efte.alerts\"\n    EVR_IMAGE_TOPIC = os.environ.get(\"EVR_IMAGE_TOPIC\") or \"rico.images.evr\"\n\n    MONGODB_URI = os.environ.get(\"MONGODB_URI\") or None\n    MONGO_DBNAME = os.environ.get(\"MONGO_DBNAME\") or \"hdps\"\n\n    EFTE_DB_ADDR = os.environ.get(\"EFTE_DB_ADDR\") or \"127.0.0.1\"\n    EFTE_DB_PORT = os.environ.get(\"EFTE_DB_PORT\") or 5432\n    EFTE_DB_NAME = os.environ.get(\"EFTE_DB_NAME\") or \"transients\"\n    EFTE_DB_USER = os.environ.get(\"EFTE_DB_USER\") or None\n    EFTE_DB_PASS = os.environ.get(\"EFTE_DB_PASS\") or None\n\n    WASABI_KEY_ID = os.environ.get(\"WASABI_KEY_ID\") or None\n    WASABI_SECRET_KEY = os.environ.get(\"WASABI_SECRET_KEY\") or None\n    WASABI_ENDPOINT = os.environ.get(\"WASABI_ENDPOINT\") or None\n    RICO_CACHE_DIR = os.environ.get(\"RICO_CACHE_DIR\") or None\n    if RICO_CACHE_DIR is None:\n        RICO_CACHE_DIR = os.path.join(os.path.expanduser(\"~\"), \".rico_cache\")\n    if not os.path.isdir(RICO_CACHE_DIR):\n        os.makedirs(RICO_CACHE_DIR)", "\n\nconfig = Config()\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"\n    Shortcut to grab a logger instance with the specified name.\n\n    Args:\n        name (str): The name of the logger.\n\n    Returns:\n        logging.Logger: A logger instance configured with the specified name.\n    \"\"\"\n\n    logger = logging.getLogger(name)\n    if not logger.handlers:\n        # Prevent logging from propagating to the root logger\n        logger.propagate = False\n        logger.setLevel((\"INFO\"))\n        console = logging.StreamHandler()\n        logger.addHandler(console)\n        formatter = logging.Formatter(\n            \"%(asctime)s \u2014 %(name)s \u2014 %(levelname)s \u2014 %(funcName)s:%(lineno)d \u2014 %(message)s\"\n        )\n        console.setFormatter(formatter)\n    return logger", "\n\n# For testing\nget_logger(__name__).addHandler(logging.StreamHandler())\n\n# Silenced\n# get_logger(__name__).addHandler(logging.NullHandler())\n"]}
{"filename": "src/argus_rico/utils.py", "chunked_list": ["__all__ = [\"Timer\"]\n\nimport time\n\n\nclass Timer:\n    def __init__(self, log=None):\n        self.start_time = time.perf_counter()\n        self.last_ping = time.perf_counter()\n        self.log = log\n\n    def start(self):\n        \"\"\"Total runtime start.\"\"\"\n        self.start_time = time.perf_counter()\n\n    @property\n    def t(self):\n        \"\"\"Runtime relative to start.\"\"\"\n        return time.perf_counter() - self.start_time\n\n    @property\n    def t_since_last_ping(self):\n        \"\"\"Runtime relative to the last ping.\"\"\"\n        return time.perf_counter() - self.last_ping\n\n    def ping(self, message):\n        if self.log is not None:\n            self.log.info(\n                f\"@ {self.t * 1000:0.0f} ms ({self.t_since_last_ping * 1000:0.0f} ms delta): {message}\",\n            )\n        self.last_ping = time.perf_counter()", ""]}
{"filename": "src/argus_rico/heartbeat.py", "chunked_list": ["__all__ = [\"RicoHeartBeat\"]\n\nimport datetime\nimport threading\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\nfrom . import config\nfrom .producer import Producer\n\n\nclass RepeatTimer(threading.Timer):\n    finished: threading.Event\n    interval: float\n    function: Callable[..., Any]\n    args: Tuple[Any, ...]\n    kwargs: Dict[str, Any]\n\n    def run(self) -> None:\n        while not self.finished.wait(self.interval):\n            assert self.function is not None\n            assert self.args is not None\n            assert self.kwargs is not None\n            self.function(*self.args, **self.kwargs)", "\n\nclass RepeatTimer(threading.Timer):\n    finished: threading.Event\n    interval: float\n    function: Callable[..., Any]\n    args: Tuple[Any, ...]\n    kwargs: Dict[str, Any]\n\n    def run(self) -> None:\n        while not self.finished.wait(self.interval):\n            assert self.function is not None\n            assert self.args is not None\n            assert self.kwargs is not None\n            self.function(*self.args, **self.kwargs)", "\n\nclass RicoHeartBeat(Producer):\n    def __init__(self) -> None:\n        \"\"\"\n        Initialize the RicoHeartBeat instance.\n\n        It inherits from the Producer class and configures the Kafka connection parameters\n        using values from the config dictionary.\n\n        The host, port, and topic are retrieved from the config dictionary\n        using the keys 'KAFKA_ADDR', 'KAFKA_PORT', and 'HBEAT_TOPIC' respectively.\n        \"\"\"\n        super().__init__(\n            host=config.KAFKA_ADDR,\n            port=config.KAFKA_PORT,\n        )\n        self.heartbeat_thread: Optional[RepeatTimer] = None\n\n    def send_heartbeat(self) -> None:\n        \"\"\"\n        Send a heartbeat message to the Kafka topic.\n\n        The message is a dictionary containing the current UTC time.\n        \"\"\"\n        self.send_json({\"utc\": datetime.datetime.utcnow()}, config.HBEAT_TOPIC)\n\n    def start(self, rate: int = 30) -> None:\n        \"\"\"\n        Start sending heartbeat messages periodically.\n\n        Args:\n            rate (int): The rate at which heartbeat messages should be sent, in seconds.\n                        Defaults to 30 seconds.\n        \"\"\"\n        self.heartbeat_thread = RepeatTimer(rate, self.send_heartbeat)\n        self.heartbeat_thread.start()\n\n    def stop(self) -> None:\n        \"\"\"\n        Stop sending heartbeat messages periodically.\n        \"\"\"\n        if self.heartbeat_thread is not None:\n            self.heartbeat_thread.cancel()", ""]}
{"filename": "src/argus_rico/s3.py", "chunked_list": ["__all__ = [\"S3AuthException\", \"S3Share\"]\n\"\"\"S3 Share module for handling Wasabi S3 interactions.\"\"\"\nimport os\nimport tarfile\nfrom typing import NoReturn\n\nimport boto3\n\nfrom . import config\n", "from . import config\n\n\nclass S3AuthException(Exception):\n    \"\"\"Exception raised for S3 authentication errors.\"\"\"\n\n    pass\n\n\nclass S3Share:\n    def __init__(self) -> None:\n        \"\"\"Initialize the S3Share class.\n\n        Raises:\n            S3AuthException: If Wasabi credentials are missing in ricoenv.\n        \"\"\"\n        if config.WASABI_KEY_ID is None or config.WASABI_SECRET_KEY is None:\n            raise S3AuthException(\"No Wasabi credentials in ricoenv\")\n\n        self.s3 = boto3.resource(\n            \"s3\",\n            aws_access_key_id=config.WASABI_KEY_ID,\n            aws_secret_access_key=config.WASABI_SECRET_KEY,\n            endpoint_url=config.WASABI_ENDPOINT,\n        )\n\n        self.vetnet_bucket = self.s3.Bucket(\"efte.vetnet\")\n        self.catalog_bucket = self.s3.Bucket(\"hera.catalogs\")\n        self.stamps_bucket = \"efte.stamps\"\n\n    def upload_stamp(self, id: str) -> str:\n        \"\"\"\n        Upload a stamp for to S3.\n\n        Args:\n            id (str): S3 key for the stamp (candidate ID)\n\n        Returns:\n            str: Pre-authenticated URL for the stamp file.\n        \"\"\"\n        url = self.s3.generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\"Bucket\": self.stamps_bucket, \"Key\": \"invoice.pdf\"},\n        )\n        return url\n\n    def download_vetnet(self) -> NoReturn:\n        \"\"\"Download Vetnet data from S3.\n\n        Downloads the hypersky_v7_v0.tar.gz file from the S3 bucket,\n        extracts its contents, and saves it in the cache directory.\n\n        Raises:\n            botocore.exceptions.ClientError: If the file download fails.\n        \"\"\"\n        cached_path = os.path.join(\n            os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0.tar.gz\"\n        )\n\n        self.vetnet_bucket.download_file(\n            \"hypersky_v7_v0.tar.gz\",\n            cached_path,\n        )\n        outpath = os.path.dirname(cached_path)\n        file = tarfile.open(cached_path)\n        file.extractall(outpath)\n        file.close()\n\n    def download_atlas(self) -> NoReturn:\n        \"\"\"Download ATLAS-RefCat 2 data from S3.\n\n        Downloads the atlas_feathercat.tar.gz file from the S3 bucket,\n        extracts its contents, and saves it in the cache directory.\n\n        Raises:\n            botocore.exceptions.ClientError: If the file download fails.\n        \"\"\"\n        cached_path = os.path.join(config.RICO_CACHE_DIR, \"atlas_feathercat.tar.gz\")\n\n        self.catalog_bucket.download_file(\n            \"atlas_feathercat.tar.gz\",\n            cached_path,\n        )\n        outpath = os.path.dirname(cached_path)\n        file = tarfile.open(cached_path)\n        file.extractall(outpath)\n        file.close()", "\nclass S3Share:\n    def __init__(self) -> None:\n        \"\"\"Initialize the S3Share class.\n\n        Raises:\n            S3AuthException: If Wasabi credentials are missing in ricoenv.\n        \"\"\"\n        if config.WASABI_KEY_ID is None or config.WASABI_SECRET_KEY is None:\n            raise S3AuthException(\"No Wasabi credentials in ricoenv\")\n\n        self.s3 = boto3.resource(\n            \"s3\",\n            aws_access_key_id=config.WASABI_KEY_ID,\n            aws_secret_access_key=config.WASABI_SECRET_KEY,\n            endpoint_url=config.WASABI_ENDPOINT,\n        )\n\n        self.vetnet_bucket = self.s3.Bucket(\"efte.vetnet\")\n        self.catalog_bucket = self.s3.Bucket(\"hera.catalogs\")\n        self.stamps_bucket = \"efte.stamps\"\n\n    def upload_stamp(self, id: str) -> str:\n        \"\"\"\n        Upload a stamp for to S3.\n\n        Args:\n            id (str): S3 key for the stamp (candidate ID)\n\n        Returns:\n            str: Pre-authenticated URL for the stamp file.\n        \"\"\"\n        url = self.s3.generate_presigned_url(\n            ClientMethod=\"get_object\",\n            Params={\"Bucket\": self.stamps_bucket, \"Key\": \"invoice.pdf\"},\n        )\n        return url\n\n    def download_vetnet(self) -> NoReturn:\n        \"\"\"Download Vetnet data from S3.\n\n        Downloads the hypersky_v7_v0.tar.gz file from the S3 bucket,\n        extracts its contents, and saves it in the cache directory.\n\n        Raises:\n            botocore.exceptions.ClientError: If the file download fails.\n        \"\"\"\n        cached_path = os.path.join(\n            os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0.tar.gz\"\n        )\n\n        self.vetnet_bucket.download_file(\n            \"hypersky_v7_v0.tar.gz\",\n            cached_path,\n        )\n        outpath = os.path.dirname(cached_path)\n        file = tarfile.open(cached_path)\n        file.extractall(outpath)\n        file.close()\n\n    def download_atlas(self) -> NoReturn:\n        \"\"\"Download ATLAS-RefCat 2 data from S3.\n\n        Downloads the atlas_feathercat.tar.gz file from the S3 bucket,\n        extracts its contents, and saves it in the cache directory.\n\n        Raises:\n            botocore.exceptions.ClientError: If the file download fails.\n        \"\"\"\n        cached_path = os.path.join(config.RICO_CACHE_DIR, \"atlas_feathercat.tar.gz\")\n\n        self.catalog_bucket.download_file(\n            \"atlas_feathercat.tar.gz\",\n            cached_path,\n        )\n        outpath = os.path.dirname(cached_path)\n        file = tarfile.open(cached_path)\n        file.extractall(outpath)\n        file.close()", ""]}
{"filename": "src/argus_rico/consumer.py", "chunked_list": ["from typing import Union\n\nimport confluent_kafka as ck\n\nfrom . import get_logger\n\n\nclass Consumer:\n    def __init__(\n        self,\n        host: str,\n        port: Union[int, str],\n        group_id: str,\n        topic: str,\n    ) -> None:\n        \"\"\"Initialize the Consumer class.\n\n        Args:\n            host (str): The Kafka broker host.\n            port (Union[int, str]): The Kafka broker port.\n            group_id (str): The Kafka consumer group ID.\n            topic (str): The Kafka topic to consume messages from.\n        \"\"\"\n        # Unlike producer, consumer is created on poll\n        self.config = {\n            \"bootstrap.servers\": f\"{host}:{port}\",\n            \"group.id\": group_id,\n            \"enable.auto.commit\": False,\n            \"auto.offset.reset\": \"latest\",\n        }\n\n        self.log = get_logger(__name__)\n        self.topic = topic\n        self.polling = False\n\n    def get_consumer(self) -> ck.Consumer:\n        \"\"\"Create and return a new Kafka consumer instance.\n\n        Returns:\n            ck.Consumer: A new Kafka consumer instance configured with the specified settings.\n        \"\"\"\n        return ck.Consumer(self.config)", ""]}
{"filename": "src/argus_rico/cli.py", "chunked_list": ["import sys\nfrom typing import Sequence\n\nimport click\n\n\n@click.group()\ndef main() -> None:\n    \"\"\"Console scripts for rico.\"\"\"\n    pass", "\n\nclass Config(object):\n    \"\"\"Contains global options used for all pipeline actions.\"\"\"\n\n    def __init__(self) -> None:\n        self.verbose = False\n        self.ncpus = 72\n\n", "\n\npass_config = click.make_pass_decorator(Config, ensure=True)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())  # pragma: no cover\n\n\n@click.command(\"slack\", short_help=\"Start Rico Slack server.\")\ndef slack() -> None:\n    \"\"\"Starts the Rico Slack server.\"\"\"\n    from .slack import slack_app_starter\n\n    slack_app_starter()", "\n@click.command(\"slack\", short_help=\"Start Rico Slack server.\")\ndef slack() -> None:\n    \"\"\"Starts the Rico Slack server.\"\"\"\n    from .slack import slack_app_starter\n\n    slack_app_starter()\n\n\n@click.command(\"loadcats\", short_help=\"Start Rico monitor for EFTE catalogs.\")", "\n@click.command(\"loadcats\", short_help=\"Start Rico monitor for EFTE catalogs.\")\n@click.argument(\"directory\", type=click.Path(exists=True))\ndef loadcats(directory: str) -> None:\n    \"\"\"Starts the Rico directory monitor.\"\"\"\n    from .efte.watchdog import EFTEWatcher\n\n    ew = EFTEWatcher(watch_path=directory)\n    ew.watch()\n", "\n\n@click.command(\"stream_json\", short_help=\"Record event stream to local JSON.\")\n@click.option(\n    \"--filter\",\n    \"-f\",\n    type=click.Path(exists=True, dir_okay=False),\n    default=None,\n    help=\"File containing filters to apply to alert stream\",\n)", "    help=\"File containing filters to apply to alert stream\",\n)\n@click.option(\n    \"--outdir\",\n    \"-o\",\n    type=click.Path(file_okay=False),\n    default=\"rico_alerts\",\n    help=\"Output directory\",\n)\n@click.option(\"--group\", \"-g\", type=str, default=\"argus-spec\", help=\"Kafka group ID\")\ndef stream_json(filter: str, outdir: str, group: str) -> None:\n    \"\"\"Monitor the alert stream and record candidates to local disk in JSON\n    format. Optionally, apply a filter to the events before recording, based on\n    xmatch info.\"\"\"\n    import os\n\n    from .efte.stream import EFTEAlertReceiver\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    ear = EFTEAlertReceiver(\n        output_path=outdir,\n        filter_path=filter,\n        group=group,\n    )\n    ear.poll_and_record()", ")\n@click.option(\"--group\", \"-g\", type=str, default=\"argus-spec\", help=\"Kafka group ID\")\ndef stream_json(filter: str, outdir: str, group: str) -> None:\n    \"\"\"Monitor the alert stream and record candidates to local disk in JSON\n    format. Optionally, apply a filter to the events before recording, based on\n    xmatch info.\"\"\"\n    import os\n\n    from .efte.stream import EFTEAlertReceiver\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    ear = EFTEAlertReceiver(\n        output_path=outdir,\n        filter_path=filter,\n        group=group,\n    )\n    ear.poll_and_record()", "\n\n@click.command(\"index_images\", short_help=\"Index EVR images directly into MongoDB.\")\n@click.argument(\"directories\", nargs=-1, type=click.Path(exists=True))\n@pass_config\ndef index_images(cli_config: Config, directories: Sequence[str]) -> None:\n    \"\"\"Indexes EVR images from the specified directories.\n\n    Args:\n        cli_config: Config object containing global options.\n        directories: Iterable of directories containing EVR images.\n\n    \"\"\"\n    import multiprocessing as mp\n\n    from . import EVRImageLoader\n\n    image_loader = EVRImageLoader(create_client=False)\n\n    pool = mp.Pool(cli_config.ncpus)\n\n    directories = sorted(directories)\n    n_directories = len(directories)\n    out = []\n    with click.progressbar(\n        pool.imap_unordered(image_loader.load_directory, directories),\n        label=\"Loading: \",\n        length=n_directories,\n    ) as pbar:\n        for _ in pbar:\n            out.append(_)\n\n    pool.close()\n    pool.join()", "\n\nmain.add_command(slack)\nmain.add_command(index_images)\nmain.add_command(loadcats)\nmain.add_command(stream_json)\n"]}
{"filename": "src/argus_rico/catalogs/__init__.py", "chunked_list": ["__all__ = [\n    \"ATLASRefcat2\",\n]\n\nfrom .atlas import ATLASRefcat2\n"]}
{"filename": "src/argus_rico/catalogs/atlas.py", "chunked_list": ["__all__ = [\"MissingDirectoryError\", \"ATLASRefcat2\"]\n\nimport glob\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple, Union\n\nimport astropy.coordinates as crds\nimport astropy.units as u\nimport click", "import astropy.units as u\nimport click\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.dataset as pds\nimport pyarrow.feather as pf\nimport pyarrow.parquet as pq\nfrom astropy_healpix import HEALPix\n", "from astropy_healpix import HEALPix\n\nfrom .. import config, s3\n\n\nclass MissingDirectoryError(Exception):\n    \"\"\"Exception raised when a directory is missing.\n\n    Attributes:\n        message (str): Explanation of the error.\n    \"\"\"\n\n    def __init__(self, message: str = \"Tree basepath is not set!\"):\n        self.message = message\n        super().__init__(self.message)", "\n\ndef haversine(\n    lon1: np.ndarray, lat1: np.ndarray, lon2: np.ndarray, lat2: np.ndarray\n) -> np.ndarray:\n    \"\"\"Calculate the great circle distance between two points on the earth.\n\n    Args:\n        lon1 (np.ndarray): Longitudes of the first points in decimal degrees.\n        lat1 (np.ndarray): Latitudes of the first points in decimal degrees.\n        lon2 (np.ndarray): Longitudes of the second points in decimal degrees.\n        lat2 (np.ndarray): Latitudes of the second points in decimal degrees.\n\n    Returns:\n        np.ndarray: The great circle distances in degrees.\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    return np.rad2deg(c)", "\n\nclass ATLASRefcat2:\n    def __init__(self):\n        self.h4 = HEALPix(nside=4, order=\"nested\", frame=crds.ICRS())\n        self.h16 = HEALPix(nside=16, order=\"nested\", frame=crds.ICRS())\n\n        self.table: pa.Table = None\n        self.dataset: str = None\n\n        if not os.path.isdir(os.path.join(config.RICO_CACHE_DIR, \"atlas_refcat2\")):\n            if click.confirm(\n                \"ATLAS RefCat not found, do you want to download it (82 GB)?\"\n            ):\n                s3share = s3.S3Share()\n                s3share.download_atlas()\n\n            else:\n                raise MissingDirectoryError(\"ATLAS Refcat is not installed\")\n        self.dataset = os.path.join(config.RICO_CACHE_DIR, \"atlas_refcat2\")\n\n    def radial(\n        self,\n        ra: float,\n        dec: float,\n        radius: float,\n        min_g: float = 11.0,\n        max_g: float = 16.0,\n        return_area: bool = False,\n        grab_closest: bool = False,\n        as_pandas: bool = True,\n    ) -> Union[pd.DataFrame, pa.Table]:\n        \"\"\"Perform radial search on the dataset.\n\n        Args:\n            ra (float): Right ascension in degrees.\n            dec (float): Declination in degrees.\n            radius (float): Radius of the search in degrees.\n            min_g (float, optional): Minimum 'g' value for filtering. Defaults to 11.0.\n            max_g (float, optional): Maximum 'g' value for filtering. Defaults to 16.0.\n            return_area (bool, optional): Whether to return the area covered by the search.\n                Defaults to True.\n            grab_closest (bool, optional): Whether to return only the closest match.\n                Defaults to False.\n            as_pandas (bool, optional): Whether to return the result as a pandas DataFrame.\n                If False, the result is returned as a pyarrow Table. Defaults to True.\n\n        Returns:\n            Union[pd.DataFrame, pa.Table]: The filtered dataset within the specified radius.\n        \"\"\"\n        hpx_4 = self.h4.cone_search_lonlat(ra * u.deg, dec * u.deg, radius * u.deg)\n        hpx_16 = self.h16.cone_search_lonlat(ra * u.deg, dec * u.deg, radius * u.deg)\n\n        imfiles = []\n        for i4 in hpx_4:\n            files_4 = [\n                glob.glob(\n                    os.path.join(\n                        self.dataset, str(i4) + \"/\" + str(i) + \"/\" + \"*.feather\"\n                    )\n                )\n                for i in hpx_16\n            ]\n            imfiles += files_4\n        imfiles = [x[0] for x in imfiles if len(x) > 0]\n\n        if as_pandas:\n            with ThreadPoolExecutor() as threads:\n                t_res = threads.map(\n                    self._from_featherfile_pandas,\n                    zip(\n                        imfiles,\n                        [\n                            [min_g, max_g],\n                        ]\n                        * len(imfiles),\n                    ),\n                )\n            t = pd.concat(t_res)\n            separations = haversine(t[\"ra\"], t[\"dec\"], ra, dec)\n            t[\"separation\"] = separations\n\n            if grab_closest:\n                t = t.iloc[[np.argmin(separations)]]\n            else:\n                t = t[separations < radius]\n\n        else:\n            with ThreadPoolExecutor() as threads:\n                t_res = threads.map(\n                    self._from_featherfile,\n                    zip(\n                        imfiles,\n                        [\n                            [min_g, max_g],\n                        ]\n                        * len(imfiles),\n                    ),\n                )\n\n            t = pa.concat_tables(t_res)\n            separations = haversine(t[\"ra\"].to_numpy(), t[\"dec\"].to_numpy(), ra, dec)\n            t.append_column(\"separation\", [pa.array(separations)])\n\n            t = t.filter(separations < radius)\n\n        if return_area:\n            return t, np.pi * radius**2\n        else:\n            return t\n\n    @staticmethod\n    def _from_featherfile_pandas(packet: Tuple[str, List[float]]) -> pd.DataFrame:\n        \"\"\"Read a feather file and return the DataFrame after filtering.\n\n        Args:\n            packet (Tuple[str, List[float]]): A tuple containing the feather file path\n                and the range of 'g' values for filtering.\n\n        Returns:\n            pd.DataFrame: The filtered DataFrame.\n        \"\"\"\n        path, [min_g, max_g] = packet\n        t = pf.read_feather(path)\n\n        t = t[(t[\"g\"] < max_g) & (t[\"g\"] > min_g)]\n\n        return t\n\n    @staticmethod\n    def _from_featherfile(packet: Tuple[str, List[float]]) -> pa.Table:\n        \"\"\"Read a feather file and return the Table.\n\n        Args:\n            packet (Tuple[str, List[float]]): A tuple containing the feather file path\n                and dummy list.\n\n        Returns:\n            pa.Table: The Table read from the feather file.\n        \"\"\"\n        path, [_, _] = packet\n        t = pf.read_table(path)\n\n        return t\n\n    def to_parquet(self, outpath: str) -> None:\n        \"\"\"Write the Table to a parquet file.\n\n        Args:\n            outpath (str): The output path for the parquet file.\n        \"\"\"\n        pq.write_table(self.table, outpath)\n\n    def from_parquet(self, path: str) -> None:\n        \"\"\"Load a parquet file into the class.\n\n        Args:\n            path (str): The path to the parquet file.\n\n        Raises:\n            FileNotFoundError: If the parquet file at the given path is not found.\n        \"\"\"\n        try:\n            self.table = pq.read_table(path)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Parquet file not found at path: {path}\")\n\n    def to_ds(self, outdir: str) -> None:\n        \"\"\"Write the Table to a dataset in feather format.\n\n        Args:\n            outdir (str): The output directory for the dataset.\n        \"\"\"\n        part = pds.partitioning(\n            pa.schema(\n                [\n                    (\"h4\", pa.int32()),\n                    (\"h16\", pa.int32()),\n                    # (\"h32\", pa.int32()),\n                    # (\"h64\", pa.int32()),\n                    # (\"h256\", pa.int32()),\n                ]\n            ),\n        )\n        pds.write_dataset(\n            self.table,\n            outdir,\n            format=\"feather\",\n            max_partitions=786432,\n            partitioning=part,\n            max_open_files=786432,\n        )\n\n    def to_segment_ds(self, outdir: str, nside_base: int) -> None:\n        \"\"\"Write the Table to a segmented dataset in feather format.\n\n        Args:\n            outdir (str): The output directory for the segmented dataset.\n            nside_base (int): The base nside value for segmentation.\n        \"\"\"\n        part = pds.partitioning(\n            pa.schema(\n                [\n                    (\"h4\", pa.int32()),\n                    (\"h16\", pa.int32()),\n                    (\"h64\", pa.int32()),\n                    (\"h256\", pa.int32()),\n                ]\n            ),\n        )\n        pds.write_dataset(\n            self.table,\n            outdir,\n            format=\"feather\",\n            max_partitions=786432,\n            partitioning=part,\n            max_open_files=786432,\n        )", ""]}
{"filename": "src/argus_rico/efte/vetnet.py", "chunked_list": ["__all__ = [\"VetNet\"]\n\"\"\"Wrapper for the MC EFTE Vetnet model.\"\"\"\nimport os\nfrom typing import Tuple\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport astropy.visualization as viz  # noqa: E402\nimport numpy as np  # noqa: E402\nimport tensorflow as tf  # noqa: E402", "import numpy as np  # noqa: E402\nimport tensorflow as tf  # noqa: E402\nimport tensorflow.keras.models as models  # noqa: E402\n\nfrom .. import s3  # noqa: E402\n\n\nclass VetNet:\n    def __init__(self) -> None:\n        \"\"\"Initialize the Vetnet class.\n\n        Loads the pretrained model and sets class variables.\n        \"\"\"\n        try:\n            if not os.path.isdir(os.path.join(os.path.expanduser(\"~\"), \".rico_cache\")):\n                os.mkdir(os.path.join(os.path.expanduser(\"~\"), \".rico_cache\"))\n\n            self.model = models.load_model(\n                os.path.join(os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0\")\n            )\n        except OSError:\n            s3share = s3.S3Share()\n            s3share.download_vetnet()\n            self.model = models.load_model(\n                os.path.join(os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0\")\n            )\n\n        self.BATCH_SIZE: int  # The batch size for data processing\n        self.AUTOTUNE: int = tf.data.AUTOTUNE  # The buffer size for prefetching\n        self.zed = viz.ZScaleInterval()\n\n    def prep_ds(self, ds: tf.data.Dataset) -> tf.data.Dataset:\n        \"\"\"Preprocess the input dataset.\n\n        Batch the dataset and apply buffered prefetching.\n\n        Args:\n            ds (tf.data.Dataset): The input dataset.\n\n        Returns:\n            tf.data.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Batch all datasets\n        ds = ds.batch(self.BATCH_SIZE)\n\n        # Use buffered prefetching on all datasets\n        return ds.prefetch(buffer_size=self.AUTOTUNE)\n\n    def _normalize_triplet(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Perform stamp normalization. Processes the reference, science, and\n        difference images separately.\n\n\n        Args:\n            data (np.ndarray): The input data for prediction.\n\n        Returns:\n            np.ndarray: Recursively normalized postage stamp cutout.\n        \"\"\"\n        for i in range(data.shape[-1]):\n            data[:, :, i] = self.zed(data[:, :, i])\n\n        return data\n\n    def _normalize_stamps(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Perform recursive stamp normalization. Processes the reference,\n        science, and difference images separately.\n\n\n        Args:\n            data (np.ndarray): The input data for prediction.\n\n        Returns:\n            np.ndarray: Recursively normalized postage stamp cutouts.\n        \"\"\"\n        for i in range(data.shape[0]):\n            data[i, :, :, :] = self._normalize_triplet(data[i, :, :, :])\n\n        return data\n\n    def mc_predict(\n        self, data: np.ndarray, n_posterior_draws: int\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform Monte Carlo prediction.\n\n        Args:\n            data (np.ndarray): The input data for prediction.\n            n_posterior_draws (int): The number of posterior draws.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: A tuple containing:\n            - mean_pred (np.ndarray): The mean prediction.\n            - low_pred (np.ndarray): The lower prediction bound.\n            - high_pred (np.ndarray): The upper prediction bound.\n            - confidence (np.ndarray): The confidence measure.\n        \"\"\"\n        data = self._normalize_stamps(data)\n\n        pred_dists = np.array(\n            [self.model(data, training=False) for _ in range(n_posterior_draws)]\n        )\n\n        pred_dists[pred_dists == 0] += 1e-5\n        pred_dists[pred_dists == 1] -= 1e-5\n\n        entropy = (-1 * pred_dists * np.log2(pred_dists)) - (\n            (1 - pred_dists) * np.log2(1 - pred_dists)\n        )\n        low_pred = np.rint(np.percentile(pred_dists, (50.0 - 95.4 / 2), axis=0))\n        high_pred = np.rint(np.percentile(pred_dists, (50.0 + 95.4 / 2), axis=0))\n        mean_pred = np.rint(np.mean(pred_dists, axis=0))\n\n        confidence = 1 - (1 / n_posterior_draws) * np.sum(entropy, axis=0)\n\n        return mean_pred, low_pred, high_pred, confidence", ""]}
{"filename": "src/argus_rico/efte/efte_runner.py", "chunked_list": ["\"\"\"(very thin) wrapper for running EFTE CLI utilities programmatically in a thread\"\"\"\nimport subprocess\nimport threading\nfrom typing import Optional\n\n\nclass EFTERunner(threading.Thread):\n    \"\"\"A class representing a thread that runs a command using subprocess.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the EFTERunner thread.\"\"\"\n        self.stdout: Optional[bytes] = None\n        self.stderr: Optional[bytes] = None\n        threading.Thread.__init__(self)\n\n    def run(self, command: str) -> None:\n        \"\"\"Execute the specified command in a separate thread.\n\n        Args:\n            command (str): The command to be executed.\n\n        Returns:\n            None.\n        \"\"\"\n        p = subprocess.Popen(\n            command.split(), shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n\n        self.stdout, self.stderr = p.communicate()", ""]}
{"filename": "src/argus_rico/efte/processor.py", "chunked_list": ["\"\"\"Ray-parallelizable EFTE catalog reducer.\"\"\"\nimport os\n\nimport astropy.table as tbl\nimport ray\n\nfrom .. import catalogs, get_logger, utils\nfrom .stream import EFTEAlertStreamer\nfrom .vetnet import VetNet\n", "from .vetnet import VetNet\n\n\n@ray.remote\nclass EFTECatalogProcessor:\n    def __init__(self):\n        \"\"\"Initialize the EFTECatalogProcessor class.\"\"\"\n        self.vetnet = VetNet()\n        self.atlas = catalogs.ATLASRefcat2()\n        self.producer = EFTEAlertStreamer()\n\n        self.log = get_logger(__name__)\n\n    def process(self, filepath: str) -> None:\n        \"\"\"Perform vetting and crossmatching for the given FITS table.\n\n        Args:\n            filepath (str): The path to the FITS table.\n\n        Returns:\n            Table: Returns the FITS table with normalized stamps, scores.\n        \"\"\"\n        clock = utils.Timer(log=self.log)\n        name = os.path.basename(filepath)\n        table: tbl.Table = tbl.Table.read(filepath, format=\"fits\")\n        clock.ping(f\"Read {len(table)} candidates from {name}\")\n\n        stamps = table[\"stamp\"].data\n        mean_pred, _, _, confidence = self.vetnet.mc_predict(stamps, 10)\n        clock.ping(f\"Vetted candidates from {name}\")\n\n        table[\"vetnet_score\"] = confidence[:, 0]\n        table = table[mean_pred[:, 0] > 0.5]\n        table = table[table[\"vetnet_score\"] > 0.4]  # fairly arbitrary...\n\n        clock.ping(f\"Reporting {len(table)} candidates in {name}\")\n\n        if len(table) == 0:\n            return\n        crossmatches = []\n        for r in table:\n            phot = self.atlas.radial(\n                r[\"ra\"], r[\"dec\"], 0.01, max_g=25, return_area=False\n            )\n            phot = phot.sort_values(by=\"separation\")\n            crossmatches.append(phot.to_dict(orient=\"records\"))\n\n        clock.ping(f\"Xmatched {name} with ATLAS Refcat\")\n\n        self.producer.push_alert(table, crossmatches)", ""]}
{"filename": "src/argus_rico/efte/db.py", "chunked_list": ["__all__ = [\"insert_efte_candidates\"]\n\nimport datetime\nimport os\n\nimport astropy.table as tbl\nimport astropy.time as atime\nimport astropy.visualization as viz\nimport numpy as np\nimport psycopg", "import numpy as np\nimport psycopg\nimport skimage as sk\n\nfrom . import config, get_logger\n\nlog = get_logger(\"__name__\")\n\n\ndef insert_efte_candidates(catalog):\n    if len(catalog) == 0:\n        return False\n\n    conn = psycopg.connect(\n        f\"\"\"host='{config.EFTE_DB_ADDR}'\n            port='{config.EFTE_DB_PORT}'\n            dbname='{config.EFTE_DB_NAME}'\n            user='{config.EFTE_DB_USER}'\n            password='{config.EFTE_DB_PASS}'\n        \"\"\",\n        # cursor_factory=psycopg.ClientCursor,\n    )\n\n    c = conn.cursor()\n\n    now = datetime.datetime.utcnow()\n    imgepoch = atime.Time(catalog.meta[\"EPOCH\"], format=\"isot\", scale=\"utc\")\n    epoch = [imgepoch.to_datetime()] * len(catalog)\n    lag = [np.abs((now - imgepoch.to_datetime()).total_seconds())] * len(catalog)\n    camera = [catalog.meta[\"CCD\"]] * len(catalog)\n    ratchet = [catalog.meta[\"ratchetnum\"]] * len(catalog)\n    filename = [catalog.meta[\"FILENAME\"]] * len(catalog)\n\n    zed = viz.ZScaleInterval()\n    stamps = [\n        sk.img_as_ubyte(zed(x)).astype(np.uint8).tobytes()\n        for x in catalog[\"stamp\"].data\n    ]\n    if \"bstamps\" not in catalog.colnames:\n        catalog.add_column(tbl.Column(stamps, name=\"bstamps\"))\n\n    ra = catalog[\"ra\"].data.astype(float)\n    dec = catalog[\"dec\"].data.astype(float)\n    x = catalog[\"xcentroid\"].data.astype(float)\n    y = catalog[\"ycentroid\"].data.astype(float)\n    sign_ratio = catalog[\"sign_ratio\"].data.astype(float)\n    srcsnr = catalog[\"srcsnr\"].data.astype(float)\n    gmag = catalog[\"gmag\"].data.astype(float)\n    difsnr = catalog[\"difsnr\"].data.astype(float)\n    vetscore = catalog[\"vetnet\"].data.astype(float)\n    stamps = catalog[\"bstamps\"].data\n    site = [\"mlo\"] * len(stamps)\n\n    rows = tuple(zip(epoch, stamps))\n\n    try:\n        q = \"\"\"\n               INSERT INTO stamps (epoch, cutout)\n               VALUES (%s, %s)\n               RETURNING id\"\"\"\n        c.executemany(q, rows, returning=True)\n        stamp_ids = []\n        while True:\n            stamp_ids.append(c.fetchone()[0])\n            if not c.nextset():\n                break\n\n    except Exception as e:\n        log.error(\n            \"Stamp insert failed for {}  aborting: {} {}\".format(\n                os.path.basename(catalog.meta[\"FILENAME\"]), q, e\n            )\n        )\n\n        c.close()\n        conn.close()\n        return False\n\n    rows = list(\n        zip(\n            epoch,\n            ra,\n            dec,\n            x,\n            y,\n            sign_ratio,\n            vetscore,\n            srcsnr,\n            difsnr,\n            gmag,\n            camera,\n            ratchet,\n            filename,\n            lag,\n            site,\n            stamp_ids,\n        )\n    )\n\n    q = \"\"\"\n            INSERT INTO  candidates(\n                               epoch,\n                               raj2000,\n                               decj2000,\n                               x_position,\n                               y_position,\n                               sign_ratio,\n                               vetnet_score,\n                               srcsnr,\n                               difsnr,\n                               mag_g,\n                               camera,\n                               ratchet,\n                               detection_image,\n                               detection_lag,\n                               detected_from,\n                               stamp_id)\n            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n            \"\"\"\n    c.executemany(q, rows)\n\n    c.close()\n    conn.commit()\n    conn.close()", "\ndef insert_efte_candidates(catalog):\n    if len(catalog) == 0:\n        return False\n\n    conn = psycopg.connect(\n        f\"\"\"host='{config.EFTE_DB_ADDR}'\n            port='{config.EFTE_DB_PORT}'\n            dbname='{config.EFTE_DB_NAME}'\n            user='{config.EFTE_DB_USER}'\n            password='{config.EFTE_DB_PASS}'\n        \"\"\",\n        # cursor_factory=psycopg.ClientCursor,\n    )\n\n    c = conn.cursor()\n\n    now = datetime.datetime.utcnow()\n    imgepoch = atime.Time(catalog.meta[\"EPOCH\"], format=\"isot\", scale=\"utc\")\n    epoch = [imgepoch.to_datetime()] * len(catalog)\n    lag = [np.abs((now - imgepoch.to_datetime()).total_seconds())] * len(catalog)\n    camera = [catalog.meta[\"CCD\"]] * len(catalog)\n    ratchet = [catalog.meta[\"ratchetnum\"]] * len(catalog)\n    filename = [catalog.meta[\"FILENAME\"]] * len(catalog)\n\n    zed = viz.ZScaleInterval()\n    stamps = [\n        sk.img_as_ubyte(zed(x)).astype(np.uint8).tobytes()\n        for x in catalog[\"stamp\"].data\n    ]\n    if \"bstamps\" not in catalog.colnames:\n        catalog.add_column(tbl.Column(stamps, name=\"bstamps\"))\n\n    ra = catalog[\"ra\"].data.astype(float)\n    dec = catalog[\"dec\"].data.astype(float)\n    x = catalog[\"xcentroid\"].data.astype(float)\n    y = catalog[\"ycentroid\"].data.astype(float)\n    sign_ratio = catalog[\"sign_ratio\"].data.astype(float)\n    srcsnr = catalog[\"srcsnr\"].data.astype(float)\n    gmag = catalog[\"gmag\"].data.astype(float)\n    difsnr = catalog[\"difsnr\"].data.astype(float)\n    vetscore = catalog[\"vetnet\"].data.astype(float)\n    stamps = catalog[\"bstamps\"].data\n    site = [\"mlo\"] * len(stamps)\n\n    rows = tuple(zip(epoch, stamps))\n\n    try:\n        q = \"\"\"\n               INSERT INTO stamps (epoch, cutout)\n               VALUES (%s, %s)\n               RETURNING id\"\"\"\n        c.executemany(q, rows, returning=True)\n        stamp_ids = []\n        while True:\n            stamp_ids.append(c.fetchone()[0])\n            if not c.nextset():\n                break\n\n    except Exception as e:\n        log.error(\n            \"Stamp insert failed for {}  aborting: {} {}\".format(\n                os.path.basename(catalog.meta[\"FILENAME\"]), q, e\n            )\n        )\n\n        c.close()\n        conn.close()\n        return False\n\n    rows = list(\n        zip(\n            epoch,\n            ra,\n            dec,\n            x,\n            y,\n            sign_ratio,\n            vetscore,\n            srcsnr,\n            difsnr,\n            gmag,\n            camera,\n            ratchet,\n            filename,\n            lag,\n            site,\n            stamp_ids,\n        )\n    )\n\n    q = \"\"\"\n            INSERT INTO  candidates(\n                               epoch,\n                               raj2000,\n                               decj2000,\n                               x_position,\n                               y_position,\n                               sign_ratio,\n                               vetnet_score,\n                               srcsnr,\n                               difsnr,\n                               mag_g,\n                               camera,\n                               ratchet,\n                               detection_image,\n                               detection_lag,\n                               detected_from,\n                               stamp_id)\n            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n            \"\"\"\n    c.executemany(q, rows)\n\n    c.close()\n    conn.commit()\n    conn.close()", ""]}
{"filename": "src/argus_rico/efte/__init__.py", "chunked_list": ["__all__ = [\n    \"EFTERunner\",\n    \"EFTECatalogProcessor\",\n    \"EFTECatalogHandler\",\n    \"EFTEWatcher\",\n    \"VetNet\",\n    \"EFTEAlertStreamer\",\n    \"EFTEAlertReceiver\",\n]\n", "]\n\nfrom .efte_runner import EFTERunner\nfrom .processor import EFTECatalogProcessor\nfrom .stream import EFTEAlertReceiver, EFTEAlertStreamer\nfrom .vetnet import VetNet\nfrom .watchdog import EFTECatalogHandler, EFTEWatcher\n"]}
{"filename": "src/argus_rico/efte/stream.py", "chunked_list": ["__all__ = [\"EFTEAlertReceiver\", \"EFTEAlertStreamer\"]\n\nimport base64\nimport io\nimport os\nfrom typing import Any, Dict, List, Optional\nfrom uuid import uuid4\n\nimport astropy.table as tbl\nimport blosc", "import astropy.table as tbl\nimport blosc\nimport fastavro as fa\nimport orjson\nimport pandas as pd\nfrom confluent_kafka import KafkaException\n\nfrom .. import config, get_logger\nfrom ..consumer import Consumer\nfrom ..producer import Producer", "from ..consumer import Consumer\nfrom ..producer import Producer\n\nPATH = os.path.realpath(os.path.dirname(__file__))\n\n\nclass EFTEAlertStreamer(Producer):\n    def __init__(self) -> None:\n        \"\"\"\n        Initialize the EFTEAlertStreamer instance.\n\n        It inherits from the Producer class and configures the Kafka connection parameters\n        using values from the config dictionary.\n\n        This class is used for streaming raw candidate detections from the\n        observatory to the Rico Kafka cluster.\n\n        The host, port, and topic are retrieved from the config dictionary\n        using the keys 'KAFKA_ADDR', 'KAFKA_PORT', and 'HBEAT_TOPIC' respectively.\n        \"\"\"\n        super().__init__(\n            host=config.KAFKA_ADDR,\n            port=config.KAFKA_PORT,\n        )\n        self.topic_base = config.EFTE_TOPIC_BASE\n\n        self.parsed_alert_schema = fa.schema.load_schema(\n            f\"{PATH}/schemas/efte.alert.avsc\"\n        )\n\n    def _parse_catalog(\n        self, catalog: tbl.Table, xmatches: List[Dict[str, List]]\n    ) -> bytes:\n        \"\"\"\n        Parses a catalog file and returns the serialized Avro data.\n\n        Args:\n            catalog_path (str): The path to the catalog file.\n\n        Returns:\n            bytes: The serialized Avro data.\n            dict: Catalog metadata.\n        \"\"\"\n        tab = catalog\n\n        if \"MJD\" not in tab.meta:\n            tab.meta[\"MJD\"] = 60000.1\n        if \"CCDDETID\" not in tab.meta:\n            tab.meta[\"CCDDETID\"] = \"ML3103817\"\n\n        mjd = tab.meta[\"MJD\"]\n        camera_id = tab.meta[\"CCDDETID\"]\n\n        records = []\n        for i, r in enumerate(tab):\n            alert_data = {\n                \"schemavsn\": self.parsed_alert_schema[\"version\"],\n                \"publisher\": \"rico.efte_generator\",\n                \"objectId\": str(uuid4()),\n            }\n\n            stamp = blosc.compress(r[\"stamp\"].data.tobytes())\n\n            candidate = dict(r)\n            candidate[\"stamp_bytes\"] = stamp\n            candidate[\"epoch\"] = mjd\n            candidate[\"camera\"] = camera_id\n\n            alert_data[\"candidate\"] = candidate\n            alert_data[\"xmatch\"] = xmatches[i]\n\n            records.append(alert_data)\n\n        fo = io.BytesIO()\n        fa.writer(fo, self.parsed_alert_schema, records)\n        fo.seek(0)\n\n        return fo.read()\n\n    def push_alert(self, catalog: tbl.Table, xmatches: List[Dict[str, List]]) -> None:\n        \"\"\"\n        Pushes data from a catalog file to a camera-specific topic.\n\n        Args:\n            catalog_path (str): The path to the catalog file.\n\n        Returns:\n            None\n\n        \"\"\"\n        avro_data = self._parse_catalog(catalog, xmatches)\n        topic = self.topic_base\n        self.send_binary(avro_data, topic=topic)", "\n\nclass EFTEAlertReceiver(Consumer):\n    def __init__(\n        self, group: str, output_path: str, filter_path: Optional[str]\n    ) -> None:\n        \"\"\"Initialize the EFTEAlertReceiver class.\n\n        Args:\n            group (str): The Kafka consumer group ID.\n            output_path (str): The path where filtered candidate data will be written.\n            filter_path (Optional[str]): The path to the text file containing filter conditions for xmatch data.\n                If provided, candidates will be filtered based on the conditions in the file.\n                Each condition should be in the format: 'column_name operator value'.\n        \"\"\"\n        super().__init__(\n            host=config.KAFKA_ADDR,\n            port=config.KAFKA_PORT,\n            topic=config.EFTE_TOPIC_BASE,\n            group_id=group,\n        )\n\n        self.parsed_alert_schema = fa.schema.load_schema(\n            f\"{PATH}/schemas/efte.alert.avsc\"\n        )\n        self.filter_path = filter_path\n        self.output_path = output_path\n        self.log = get_logger(__name__)\n\n    def poll_and_record(self) -> None:\n        \"\"\"Start polling for Kafka messages and process the candidates based on filter conditions.\"\"\"\n        c = self.get_consumer()\n\n        c.subscribe(\n            [\n                self.topic,\n            ]\n        )\n        try:\n            while True:\n                event = c.poll(1.0)\n                if event is None:\n                    continue\n                if event.error():\n                    raise KafkaException(event.error())\n                else:\n                    alerts = self._decode(event.value())\n                    self._filter_to_disk(alerts)\n\n        except KeyboardInterrupt:\n            print(\"Canceled by user.\")\n        finally:\n            c.close()\n\n    def _decode(self, message: bytes) -> List[Dict[str, Any]]:\n        \"\"\"Decode the AVRO message into a list of dictionaries.\n\n        Args:\n            message (bytes): The AVRO message received from Kafka.\n\n        Returns:\n            List[Dict[str, Any]]: A list of candidate dictionaries.\n        \"\"\"\n        stringio = io.BytesIO(message)\n        stringio.seek(0)\n\n        records = []\n        for record in fa.reader(stringio):\n            records.append(record)\n        return records\n\n    def _write_candidate(self, alert: Dict[str, Any]) -> None:\n        \"\"\"Write the candidate data to a JSON file.\n\n        Args:\n            candidate (Dict[str, Any]): The candidate dictionary to be written to the JSON file.\n        \"\"\"\n        alert[\"candidate\"][\"stamp_bytes\"] = base64.b64encode(\n            alert[\"candidate\"][\"stamp_bytes\"]\n        ).decode(\"utf-8\")\n        with open(\n            os.path.join(self.output_path, f\"{alert['objectId']}.json\"), \"wb\"\n        ) as f:\n            f.write(orjson.dumps(alert))\n        self.log.info(f'New candidate: {alert[\"objectId\"]}.json')\n\n    def _filter_to_disk(self, alerts: List[Dict[str, Any]]) -> None:\n        \"\"\"Filter the candidates based on the specified filter conditions.\n\n        Args:\n            candidates (List[Dict[str, Any]]): The list of candidate dictionaries to be filtered.\n\n        Note:\n            If the 'filter_path' is provided, the candidates will be filtered based on the conditions\n            specified in the text file. If 'xmatch' field is empty or 'filter_path' is not provided,\n            all candidates will be written to the output.\n        \"\"\"\n        if self.filter_path is not None:\n            with open(self.filter_path, \"r\") as file:\n                filter_conditions = file.read().splitlines()\n            filter_conditions = [f for f in filter_conditions if len(f) > 3]\n\n        for alert in alerts:\n            if self.filter_path is not None:\n                if len(alert[\"xmatch\"]) > 0:\n                    xmatch = pd.DataFrame.from_records(alert[\"xmatch\"])\n                    xmatch[\"g-r\"] = xmatch[\"g\"] - xmatch[\"r\"]\n                    for condition in filter_conditions:\n                        column_name, operator, value = condition.split()\n                        xmatch = xmatch.query(f\"{column_name} {operator} {value}\")\n                    if len(xmatch) > 0:\n                        self._write_candidate(alert)\n                else:\n                    return\n            else:\n                self._write_candidate(alert)", ""]}
{"filename": "src/argus_rico/efte/watchdog.py", "chunked_list": ["__all__ = [\"EFTEWatcher\", \"EFTECatalogHandler\"]\n\nimport os\nimport time\nfrom collections import defaultdict\n\nimport ray\nfrom watchdog.events import FileSystemEvent, FileSystemEventHandler\nfrom watchdog.observers.polling import PollingObserver\n", "from watchdog.observers.polling import PollingObserver\n\nfrom .. import get_logger\nfrom .processor import EFTECatalogProcessor\n\n\nclass EFTEWatcher:\n    def __init__(self, watch_path: str) -> None:\n        \"\"\"Initialize the EFTEWatcher class.\n\n        Args:\n            watch_path (str): The path to the directory to watch for catalog files.\n            format (str, optional): The format of catalog files. Defaults to \"fits\".\n        \"\"\"\n        self.watch_path = os.path.abspath(watch_path)\n\n    def watch(self) -> None:\n        \"\"\"Start watching the specified directory for catalog files.\"\"\"\n        ray.init()\n        event_handler = EFTECatalogHandler()\n        observer = PollingObserver()\n\n        observer.schedule(event_handler, self.watch_path, recursive=False)\n        observer.start()\n\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()", "\n\nclass EFTECatalogHandler(FileSystemEventHandler):\n    def __init__(self):\n        \"\"\"Initialize the EFTECatalogHandler class.\"\"\"\n        self.efte_processors = defaultdict(EFTECatalogProcessor.remote)\n        self.log = get_logger(__name__)\n\n    def on_created(self, event: FileSystemEvent) -> None:\n        \"\"\"Process the newly created catalog file.\n\n        Args:\n            event (FileSystemEvent): The event object representing the file creation.\n\n        Returns:\n            None: This method does not return any value; it processes the catalog file.\n        \"\"\"\n        filepath = event.src_path\n\n        if filepath[-4:] != \".cat\":\n            return\n        camera_id = os.path.basename(filepath)[:9]\n\n        self.log.info(f\"New cat for {camera_id}: {filepath}\")\n\n        self.efte_processors[camera_id].process.remote(filepath)", ""]}
{"filename": "src/argus_rico/models/__init__.py", "chunked_list": ["__all__ = [\n    \"EVRImage\",\n    \"EVRImageUpdate\",\n    \"EVRImageType\",\n    \"EVRImageUpdateType\",\n    \"fitspath_to_constructor\",\n    \"EFTEAlert\",\n    \"EFTECandidate\",\n    \"XMatchItem\",\n]", "    \"XMatchItem\",\n]\n\nfrom .efte_alert import EFTEAlert, EFTECandidate, XMatchItem\nfrom .evr_image import EVRImage, EVRImageUpdate, fitspath_to_constructor\n"]}
{"filename": "src/argus_rico/models/efte_alert.py", "chunked_list": ["import base64\nfrom typing import List, Type, TypeVar\n\nimport blosc\nimport numpy as np\nimport orjson\nfrom pydantic import BaseModel\n\nEFTEAlertType = TypeVar(\"EFTEAlertType\", bound=\"EFTEAlert\")\n", "EFTEAlertType = TypeVar(\"EFTEAlertType\", bound=\"EFTEAlert\")\n\n\nclass EFTECandidate(BaseModel):\n    stamp_bytes: bytes\n    epoch: float\n    camera: str\n    thresh: float\n    vetnet_score: float\n    npix: int\n    tnpix: int\n    xmin: int\n    xmax: int\n    ymin: int\n    ymax: int\n    xcentroid: float\n    ycentroid: float\n    x2: float\n    y2: float\n    xy: float\n    errx2: float\n    erry2: float\n    errxy: float\n    a: float\n    b: float\n    theta: float\n    cxx: float\n    cyy: float\n    cxy: float\n    cflux: float\n    flux: float\n    cpeak: float\n    peak: float\n    xcpeak: int\n    ycpeak: int\n    xpeak: int\n    ypeak: int\n    flag: int\n    aper_sum_bkgsub: float\n    annulus_rms: float\n    difsnr: float\n    hfd: float\n    sign_ratio: float\n    insmag: float\n    zpoint: float\n    gmag: float\n    srcsnr: float\n    ra: float\n    dec: float\n\n    @property\n    def stamp(self):\n        return (\n            np.frombuffer(\n                blosc.decompress(\n                    base64.b64decode(\n                        self.stamp_bytes,\n                    )\n                ),\n                dtype=np.float32,\n            )\n            .reshape((30, 30, 3))\n            .byteswap()\n        )", "\n\n# Create a Pydantic model for the 'xmatch' list of dictionaries\nclass XMatchItem(BaseModel):\n    ra: float\n    dec: float\n    parallax: float\n    pmra: float\n    pmdec: float\n    g: float\n    r: float\n    i: float\n    separation: float", "\n\nclass EFTEAlert(BaseModel):\n    schemavsn: str\n    publisher: str\n    objectId: str\n    candidate: EFTECandidate\n    xmatch: List[XMatchItem]\n\n    @classmethod\n    def from_json(cls: Type[EFTEAlertType], json_path: str) -> EFTEAlertType:\n        with open(json_path, \"rb\") as f:\n            alert = orjson.loads(f.read())\n        return cls(**alert)", ""]}
{"filename": "src/argus_rico/models/evr_image.py", "chunked_list": ["import datetime\nimport os\nimport socket\nimport uuid\nimport warnings\nfrom typing import Any, Dict, Optional, Type, TypeVar, Union\n\nimport astropy.io.fits as fits\nimport astropy.wcs as awcs\nimport numpy as np", "import astropy.wcs as awcs\nimport numpy as np\nimport qlsc\nfrom geojson_pydantic import Polygon\nfrom pydantic import BaseModel, Field, ValidationError\n\nwarnings.simplefilter(\"ignore\", category=awcs.FITSFixedWarning)\n\n\nQ = qlsc.QLSC(depth=30)", "\nQ = qlsc.QLSC(depth=30)\nHOSTNAME = socket.gethostname()\n\n\nEVRImageType = TypeVar(\"EVRImageType\", bound=\"EVRImage\")\nEVRImageUpdateType = TypeVar(\"EVRImageUpdateType\", bound=\"EVRImageUpdate\")\n\n\ndef _wcs_to_footprint(header: fits.Header) -> Dict[str, Any]:\n    \"\"\"\n     Converts the WCS header information to a GeoJSON polygon footprint.\n\n    Args:\n        header: FITS header containing WCS information.\n\n    Returns:\n        Dictionary representing the GeoJSON polygon footprint.\n\n    \"\"\"\n    w = awcs.WCS(header)\n    im_center = w.all_pix2world(3288, 2192, 0)\n    header[\"CRVAL1\"] = float(im_center[0])\n    header[\"CRVAL2\"] = float(im_center[1])\n\n    x = np.arange(0, 6575, 274)\n    y = np.arange(0, 4384, 274)\n\n    bottom_edge = list(zip([0] * len(x), x)) + [(0, 6575)]\n    right_edge = list(zip(y, [6575] * len(y)))[1:] + [(4383, 6575)]\n    top_edge = list(zip([4383] * len(x), x)) + [(4383, 6575)]\n    top_edge = list(reversed(top_edge))[1:]\n    left_edge = list(zip(y, [0] * len(y))) + [(4383, 0)]\n    left_edge = list(reversed(left_edge))[1:]\n\n    poly = np.array(bottom_edge + right_edge + top_edge + left_edge)\n    poly = np.array([poly.T[1], poly.T[0]]).T\n\n    radecs = w.all_pix2world(poly, 0)\n    # Re-orient RA from [-180,180]\n    radecs[:, 0] -= 180\n    radecs = [list(r) for r in radecs]\n\n    footprint = {\"type\": \"Polygon\", \"coordinates\": [radecs]}\n    return footprint", "\ndef _wcs_to_footprint(header: fits.Header) -> Dict[str, Any]:\n    \"\"\"\n     Converts the WCS header information to a GeoJSON polygon footprint.\n\n    Args:\n        header: FITS header containing WCS information.\n\n    Returns:\n        Dictionary representing the GeoJSON polygon footprint.\n\n    \"\"\"\n    w = awcs.WCS(header)\n    im_center = w.all_pix2world(3288, 2192, 0)\n    header[\"CRVAL1\"] = float(im_center[0])\n    header[\"CRVAL2\"] = float(im_center[1])\n\n    x = np.arange(0, 6575, 274)\n    y = np.arange(0, 4384, 274)\n\n    bottom_edge = list(zip([0] * len(x), x)) + [(0, 6575)]\n    right_edge = list(zip(y, [6575] * len(y)))[1:] + [(4383, 6575)]\n    top_edge = list(zip([4383] * len(x), x)) + [(4383, 6575)]\n    top_edge = list(reversed(top_edge))[1:]\n    left_edge = list(zip(y, [0] * len(y))) + [(4383, 0)]\n    left_edge = list(reversed(left_edge))[1:]\n\n    poly = np.array(bottom_edge + right_edge + top_edge + left_edge)\n    poly = np.array([poly.T[1], poly.T[0]]).T\n\n    radecs = w.all_pix2world(poly, 0)\n    # Re-orient RA from [-180,180]\n    radecs[:, 0] -= 180\n    radecs = [list(r) for r in radecs]\n\n    footprint = {\"type\": \"Polygon\", \"coordinates\": [radecs]}\n    return footprint", "\n\ndef fitspath_to_constructor(fitspath: Union[str, fits.HDUList]) -> Dict[str, Any]:\n    \"\"\"\n    Convert a FITS header into a Pydantic constructor dictionary.\n\n    Args:\n        fitspath (str or astropy.io.fits.HDUList): The path to the FITS file or\n        and HDUList object.\n\n    Returns:\n        Dict[str, Any]: The Pydantic constructor dictionary representing the FITS header.\n\n    Raises:\n        KeyError: If any required FITS header fields are missing.\n    \"\"\"\n    if type(fitspath) is str:\n        hdulist = fits.open(fitspath)\n\n    if len(hdulist) == 1:\n        hdu_num = 0\n    else:\n        hdu_num = 1\n    header = hdulist[hdu_num].header.copy()\n    hdulist.close()\n\n    if \"FIELDID\" not in header:\n        header[\"FIELDID\"] = 400\n\n    constructor_dict = {}\n    constructor_dict[\"camera\"] = header[\"CCDDETID\"]\n    constructor_dict[\"filter_name\"] = header[\"FILTER\"]\n    constructor_dict[\"image_type\"] = header[\"IMGTYPE\"].strip()\n    constructor_dict[\"obstime\"] = header[\"DATE-OBS\"] + \"T\" + header[\"TIME-OBS\"]\n    if \"GPSTIME\" in header and \"Z\" in header[\"GPSTIME\"]:\n        constructor_dict[\"gpstime\"] = header[\"GPSTIME\"]\n    constructor_dict[\"ccd_set_temp\"] = header[\"SETTEMP\"]\n    constructor_dict[\"ccd_temp\"] = header[\"CCDTEMP\"]\n    constructor_dict[\"exp_time\"] = header[\"EXPTIME\"]\n    constructor_dict[\"site_name\"] = header[\"TELOBS\"]\n    constructor_dict[\"server_name\"] = HOSTNAME\n    constructor_dict[\"basename\"] = header[\"ORIGNAME\"].split(\".\")[0]\n    constructor_dict[\"fieldid\"] = header[\"FIELDID\"]\n    constructor_dict[\"ratchnum\"] = header[\"RATCHNUM\"]\n    constructor_dict[\"mount_ha\"] = header[\"MOUNTHA\"]\n    constructor_dict[\"sha1\"] = header[\"CHECKSUM\"]\n    constructor_dict[\"ccd_ext_temp\"] = header[\"CCDETEMP\"]\n    constructor_dict[\"wind_dir\"] = header[\"WINDDIR\"]\n    constructor_dict[\"wind_speed\"] = header[\"WINDSPED\"]\n    constructor_dict[\"rel_humidity\"] = header[\"OUTRELHU\"]\n    constructor_dict[\"dew_point\"] = header[\"OUTDEWPT\"]\n    constructor_dict[\"air_pressure\"] = header[\"OUTPRESS\"]\n    constructor_dict[\"mushroom_temp\"] = header[\"INR1TEMP\"]\n    constructor_dict[\"inc_x\"] = header[\"INR1INCX\"]\n    constructor_dict[\"inc_y\"] = header[\"INR1INCY\"]\n    constructor_dict[\"inc_z\"] = header[\"INR1INCZ\"]\n\n    if \"CRVAL1\" in header:\n        constructor_dict[\"qid\"] = Q.ang2ipix(header[\"CRVAL1\"], header[\"CRVAL2\"])\n        constructor_dict[\"footprint\"] = Polygon(**_wcs_to_footprint(header))\n        constructor_dict[\"wcspath\"] = os.path.abspath(fitspath)\n    else:\n        constructor_dict[\"rawpath\"] = os.path.abspath(fitspath)\n    return constructor_dict", "\n\nclass EVRImage(BaseModel):\n    \"\"\"Class representing an EVR image.\"\"\"\n\n    id: str = Field(default_factory=uuid.uuid4, alias=\"_id\")\n    camera: Optional[str] = Field(...)\n    filter_name: str = Field(...)\n    obstime: datetime.datetime = Field(...)\n    gpstime: Optional[datetime.datetime] = None\n    ccd_set_temp: float = Field(...)\n    ccd_temp: float = Field(...)\n    exp_time: float = Field(...)\n\n    site_name: str = Field(...)\n    server_name: str = Field(...)\n    image_type: str = Field(...)\n    rawpath: Optional[str] = None\n    wcspath: Optional[str] = None\n    basename: str = Field(...)\n    fieldid: float = Field(...)\n    ratchnum: str = Field(...)\n    mount_ha: Optional[float] = Field(...)\n    sha1: str = Field(...)\n    ccd_ext_temp: float = Field(...)\n    wind_dir: float = Field(...)\n    wind_speed: float = Field(...)\n    rel_humidity: float = Field(...)\n    dew_point: float = Field(...)\n    air_pressure: float = Field(...)\n    mushroom_temp: float = Field(...)\n    inc_x: Optional[float] = Field(...)\n    inc_y: Optional[float] = Field(...)\n    inc_z: Optional[float] = Field(...)\n\n    footprint: Optional[Polygon] = Field(...)\n    qid: Optional[int] = Field(...)\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n\n        allow_population_by_field_name = True\n\n    @classmethod\n    def from_fits(cls: Type[EVRImageType], fitspath: str) -> EVRImageType:\n        \"\"\"Create an instance of EVRImage from a FITS file.\n\n        Args:\n            fitspath: Path to the FITS file.\n\n        Returns:\n            An instance of EVRImage.\n\n        \"\"\"\n\n        constructor_dict = fitspath_to_constructor(fitspath)\n\n        # This is duplicated boiler plate between the Image/ImageUpdate classes\n        try:\n            inst = cls(**constructor_dict)\n        except ValidationError as e:\n            for error in e.errors():\n                field = error[\"loc\"][0]\n                constructor_dict[field] = None\n            inst = cls(**constructor_dict)\n\n        return inst", "\n\nclass EVRImageUpdate(BaseModel):\n    \"\"\"Class representing an update to an EVR image.\"\"\"\n\n    camera: Optional[str]\n    filter_name: Optional[str]\n    obstime: Optional[datetime.datetime]\n    gpstime: Optional[datetime.datetime]\n    ccd_set_temp: Optional[float]\n    ccd_temp: Optional[float]\n    exp_time: Optional[float]\n    site_name: Optional[str]\n    server_name: Optional[str]\n    image_type: Optional[str]\n    rawpath: Optional[str]\n    wcspath: Optional[str]\n    basename: Optional[str]\n    fieldid: Optional[float]\n    ratchnum: Optional[str]\n    sha1: Optional[str]\n    ccd_ext_temp: Optional[float]\n    wind_dir: Optional[float]\n    wind_speed: Optional[float]\n    rel_humidity: Optional[float]\n    dew_point: Optional[float]\n    air_pressure: Optional[float]\n    mushroom_temp: Optional[float]\n    inc_x: Optional[float]\n    inc_y: Optional[float]\n    inc_z: Optional[float]\n    footprint: Optional[Polygon]\n    qid: Optional[int]\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n\n        allow_population_by_field_name = True\n\n    @classmethod\n    def from_fits(cls: Type[EVRImageUpdateType], fitspath: str) -> EVRImageUpdateType:\n        \"\"\"Create an instance of EVRImageUpdate from a FITS file.\n\n        Args:\n            fitspath: Path to the FITS file.\n\n        Returns:\n            An instance of EVRImageUpdate.\n\n        \"\"\"\n        constructor_dict = fitspath_to_constructor(fitspath)\n\n        # This is duplicated boiler plate between the Image/ImageUpdate classes\n        try:\n            inst = cls(**constructor_dict)\n        except ValidationError as e:\n            for error in e.errors():\n                field = error[\"loc\"][0]\n                constructor_dict[field] = None\n            inst = cls(**constructor_dict)\n\n        return inst", ""]}
