{"filename": "main.py", "chunked_list": ["import logging\nimport os\nimport time\n\nimport hydra\n\nfrom runner import SelfSLRunner, SupervisedRunner\nfrom trainer.utils import fix_seed\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\ndef check_existence_result() -> bool:\n    if os.path.exists(\"results.json\"):\n        print(\"Skip this trial, because it has already been run.\")\n        return True\n    else:\n        return False", "\n\n@hydra.main(config_path=\"conf\", config_name=\"config.yaml\", version_base=None)\ndef main(config):\n    if check_existence_result():\n        return\n\n    fix_seed(config.seed)\n\n    if config.train_mode == \"supervised\":\n        runner = SupervisedRunner(config)\n    elif config.train_mode == \"self_sl\":\n        runner = SelfSLRunner(config)\n    else:\n        raise ValueError(f\"train mode {config.train_mode} is not defined.\")\n\n    runner.run()\n    time.sleep(1)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "data/tabular_datamodule.py", "chunked_list": ["import logging\nfrom typing import Callable, Dict, Optional, Sequence, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import LongTensor, Tensor\nfrom torch.utils.data import DataLoader, Dataset\n", "from torch.utils.data import DataLoader, Dataset\n\nimport data.datasets as datasets\nfrom data.datasets.tabular_dataframe import TabularDataFrame\n\nlogger = logging.getLogger(__name__)\n\n# Copied from https://github.com/pfnet-research/deep-table.\n# Modified by somaonishi and shoyameguro.\nclass TabularDataset(Dataset):\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        task: str = \"binary\",\n        continuous_columns: Optional[Sequence[str]] = None,\n        categorical_columns: Optional[Sequence[str]] = None,\n        target: Optional[Union[str, Sequence[str]]] = None,\n        transform: Optional[Callable] = None,\n        unlabel_data_rate: Optional[float] = None,\n        seed=42,\n    ) -> None:\n        \"\"\"\n        Args:\n            data (pandas.DataFrame): DataFrame.\n            task (str): One of \"binary\", \"multiclass\", \"regression\".\n                Defaults to \"binary\".\n            continuous_cols (sequence of str, optional): Sequence of names of\n                continuous features (columns). Defaults to None.\n            categorical_cols (sequence of str, optional): Sequence of names of\n                categorical features (columns). Defaults to None.\n            target (str, optional): If None, `np.zeros` is set as target.\n                Defaults to None.\n            transform (callable): Method of converting Tensor data.\n                Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.task = task\n        self.num = data.shape[0]\n        self.categorical_columns = categorical_columns if categorical_columns else []\n        self.continuous_columns = continuous_columns if continuous_columns else []\n\n        if unlabel_data_rate is not None:\n            if task == \"regression\":\n                stratify = None\n            else:\n                stratify = data[target]\n            data, unlabeled_data = train_test_split(\n                data, test_size=unlabel_data_rate, stratify=stratify, random_state=seed\n            )\n            self.num = data.shape[0]\n            self.unlabeled_num = unlabeled_data.shape[0]\n            logger.info(f\"labeled data size: {self.num}\")\n            logger.info(f\"unlabeled data size: {self.unlabeled_num}\")\n\n        if self.continuous_columns:\n            self.continuous = data[self.continuous_columns].values\n            if unlabel_data_rate is not None:\n                self.unlabeled_continuous = unlabeled_data[self.continuous_columns].values\n\n        if self.categorical_columns:\n            self.categorical = data[categorical_columns].values\n            if unlabel_data_rate is not None:\n                self.unlabeled_categorical = unlabeled_data[categorical_columns].values\n\n        if target:\n            self.target = data[target].values\n            if isinstance(target, str):\n                self.target = self.target.reshape(-1, 1)\n        else:\n            self.target = np.zeros((self.num, 1))\n\n        self.transform = transform\n        self.unlabel_data_rate = unlabel_data_rate\n\n    def __len__(self) -> int:\n        return self.num\n\n    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n        \"\"\"\n        Args:\n            idx (int): The index of the sample in the dataset.\n\n        Returns:\n            dict[str, Tensor]:\n                The returned dict has the keys {\"target\", \"continuous\", \"categorical\"}\n                and its values. If no continuous/categorical features, the returned value is `[]`.\n        \"\"\"\n        if self.task == \"multiclass\":\n            x = {\n                \"target\": torch.LongTensor(self.target[idx]),\n                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n            }\n        elif self.task in {\"binary\", \"regression\"}:\n            x = {\n                \"target\": torch.Tensor(self.target[idx]),\n                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n            }\n        else:\n            raise ValueError(f\"task: {self.task} must be 'multiclass' or 'binary' or 'regression'\")\n\n        if self.transform is not None:\n            x = self.transform(x)\n\n        if hasattr(self, \"unlabeled_num\"):\n            unlabel_idx = np.random.randint(0, self.unlabeled_num)\n            unlabel = {\n                \"continuous\": Tensor(self.unlabeled_continuous[unlabel_idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.unlabeled_categorical[unlabel_idx]) if self.categorical_columns else [],\n            }\n            return x, unlabel\n\n        else:\n            return x", "# Modified by somaonishi and shoyameguro.\nclass TabularDataset(Dataset):\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        task: str = \"binary\",\n        continuous_columns: Optional[Sequence[str]] = None,\n        categorical_columns: Optional[Sequence[str]] = None,\n        target: Optional[Union[str, Sequence[str]]] = None,\n        transform: Optional[Callable] = None,\n        unlabel_data_rate: Optional[float] = None,\n        seed=42,\n    ) -> None:\n        \"\"\"\n        Args:\n            data (pandas.DataFrame): DataFrame.\n            task (str): One of \"binary\", \"multiclass\", \"regression\".\n                Defaults to \"binary\".\n            continuous_cols (sequence of str, optional): Sequence of names of\n                continuous features (columns). Defaults to None.\n            categorical_cols (sequence of str, optional): Sequence of names of\n                categorical features (columns). Defaults to None.\n            target (str, optional): If None, `np.zeros` is set as target.\n                Defaults to None.\n            transform (callable): Method of converting Tensor data.\n                Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.task = task\n        self.num = data.shape[0]\n        self.categorical_columns = categorical_columns if categorical_columns else []\n        self.continuous_columns = continuous_columns if continuous_columns else []\n\n        if unlabel_data_rate is not None:\n            if task == \"regression\":\n                stratify = None\n            else:\n                stratify = data[target]\n            data, unlabeled_data = train_test_split(\n                data, test_size=unlabel_data_rate, stratify=stratify, random_state=seed\n            )\n            self.num = data.shape[0]\n            self.unlabeled_num = unlabeled_data.shape[0]\n            logger.info(f\"labeled data size: {self.num}\")\n            logger.info(f\"unlabeled data size: {self.unlabeled_num}\")\n\n        if self.continuous_columns:\n            self.continuous = data[self.continuous_columns].values\n            if unlabel_data_rate is not None:\n                self.unlabeled_continuous = unlabeled_data[self.continuous_columns].values\n\n        if self.categorical_columns:\n            self.categorical = data[categorical_columns].values\n            if unlabel_data_rate is not None:\n                self.unlabeled_categorical = unlabeled_data[categorical_columns].values\n\n        if target:\n            self.target = data[target].values\n            if isinstance(target, str):\n                self.target = self.target.reshape(-1, 1)\n        else:\n            self.target = np.zeros((self.num, 1))\n\n        self.transform = transform\n        self.unlabel_data_rate = unlabel_data_rate\n\n    def __len__(self) -> int:\n        return self.num\n\n    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n        \"\"\"\n        Args:\n            idx (int): The index of the sample in the dataset.\n\n        Returns:\n            dict[str, Tensor]:\n                The returned dict has the keys {\"target\", \"continuous\", \"categorical\"}\n                and its values. If no continuous/categorical features, the returned value is `[]`.\n        \"\"\"\n        if self.task == \"multiclass\":\n            x = {\n                \"target\": torch.LongTensor(self.target[idx]),\n                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n            }\n        elif self.task in {\"binary\", \"regression\"}:\n            x = {\n                \"target\": torch.Tensor(self.target[idx]),\n                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n            }\n        else:\n            raise ValueError(f\"task: {self.task} must be 'multiclass' or 'binary' or 'regression'\")\n\n        if self.transform is not None:\n            x = self.transform(x)\n\n        if hasattr(self, \"unlabeled_num\"):\n            unlabel_idx = np.random.randint(0, self.unlabeled_num)\n            unlabel = {\n                \"continuous\": Tensor(self.unlabeled_continuous[unlabel_idx]) if self.continuous_columns else [],\n                \"categorical\": LongTensor(self.unlabeled_categorical[unlabel_idx]) if self.categorical_columns else [],\n            }\n            return x, unlabel\n\n        else:\n            return x", "\n\nclass TabularDatamodule:\n    def __init__(\n        self,\n        dataset: TabularDataFrame,\n        transform: Optional[Callable] = None,\n        train_sampler: Optional[torch.utils.data.Sampler] = None,\n        batch_size: int = 128,\n        num_workers: int = 3,\n        seed: int = 42,\n        val_size: float = 0.1,\n    ) -> None:\n        # self.dataset = dataset\n        self.__num_categories = dataset.num_categories()\n        self.categorical_columns = dataset.categorical_columns\n        self.continuous_columns = dataset.continuous_columns\n        self.cat_cardinalities = dataset.cat_cardinalities(True)\n        self.target = dataset.target_columns\n        self.d_out = dataset.dim_out\n\n        dataframes = dataset.processed_dataframes(val_size=val_size, seed=seed)\n        self.train = dataframes[\"train\"]\n        self.val = dataframes[\"val\"]\n        self.test = dataframes[\"test\"]\n\n        for k, v in dataframes.items():\n            logger.info(f\"{k} dataset shape: {v.shape}\")\n\n        self.task = dataset.task\n        self.transform = transform\n        self.train_sampler = train_sampler\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        if self.task == \"regression\":\n            self.y_std = dataset.y_std\n        \n        self.seed = seed\n\n    @property\n    def num_categories(self) -> int:\n        return self.__num_categories\n\n    @property\n    def num_continuous_features(self) -> int:\n        return len(self.continuous_columns)\n\n    @property\n    def num_categorical_features(self) -> int:\n        return len(self.categorical_columns)\n\n    def dataloader(self, mode: str, batch_size: Optional[int] = None, transform=None, unlabel_data_rate=None):\n        assert mode in {\"train\", \"val\", \"test\"}\n        if not hasattr(self, mode):\n            return None\n        data = getattr(self, mode)\n\n        if mode == \"test\":\n            transform = None\n\n        if transform is None:\n            transform = self.transform\n\n        dataset = TabularDataset(\n            data=data,\n            task=self.task,\n            categorical_columns=self.categorical_columns,\n            continuous_columns=self.continuous_columns,\n            target=self.target,\n            transform=transform,\n            unlabel_data_rate=unlabel_data_rate,\n            seed=self.seed,\n        )\n        return DataLoader(\n            dataset,\n            batch_size if batch_size is not None else self.batch_size,\n            shuffle=True if mode == \"train\" else False,\n            num_workers=self.num_workers,\n            sampler=self.train_sampler if mode == \"train\" else None,\n        )", "\n\ndef get_datamodule(config) -> TabularDatamodule:\n    if type(config.data) == int:\n        dataset = datasets.OpenmlDataset(data_id=config.data, config=config)\n    else:\n        dataset = getattr(datasets, config.data)(config=config)\n\n    return TabularDatamodule(dataset, batch_size=config.batch_size, seed=config.seed, val_size=config.val_size)\n", ""]}
{"filename": "data/__init__.py", "chunked_list": ["from data.tabular_datamodule import TabularDatamodule, get_datamodule\n"]}
{"filename": "data/datasets/openml.py", "chunked_list": ["import openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom .tabular_dataframe import TabularDataFrame\n\nexceptions_binary = [45062]\nexceptions_multiclass = [44135]\n\n\ndef get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col):\n    target_idx = columns.index(target_col)\n\n    if data_id in exceptions_binary:\n        task = \"binary\"\n        dim_out = 1\n    elif data_id in exceptions_multiclass:\n        task = \"multiclass\"\n        dim_out = int(df[target_col].nunique())\n    elif cont_checker(df, target_col, cate_indicator[target_idx]):\n        task = \"regression\"\n        dim_out = 1\n    elif int(df[target_col].nunique()) == 2:\n        task = \"binary\"\n        dim_out = 1\n    else:\n        task = \"multiclass\"\n        dim_out = int(df[target_col].nunique())\n    return task, dim_out", "\n\ndef get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col):\n    target_idx = columns.index(target_col)\n\n    if data_id in exceptions_binary:\n        task = \"binary\"\n        dim_out = 1\n    elif data_id in exceptions_multiclass:\n        task = \"multiclass\"\n        dim_out = int(df[target_col].nunique())\n    elif cont_checker(df, target_col, cate_indicator[target_idx]):\n        task = \"regression\"\n        dim_out = 1\n    elif int(df[target_col].nunique()) == 2:\n        task = \"binary\"\n        dim_out = 1\n    else:\n        task = \"multiclass\"\n        dim_out = int(df[target_col].nunique())\n    return task, dim_out", "\n\ndef cont_checker(df, col, is_cate):\n    return not is_cate and df[col].dtype != bool and df[col].dtype != object\n\n\ndef cate_checker(df, col, is_cate):\n    return is_cate or df[col].dtype == bool or df[col].dtype == object\n\n\ndef get_columns_list(df, columns, cate_indicator, target_col, checker):\n    return [col for col, is_cate in zip(columns, cate_indicator) if col != target_col and checker(df, col, is_cate)]", "\n\ndef get_columns_list(df, columns, cate_indicator, target_col, checker):\n    return [col for col, is_cate in zip(columns, cate_indicator) if col != target_col and checker(df, col, is_cate)]\n\n\ndef print_dataset_details(dataset: openml.datasets.OpenMLDataset, data_id):\n    df, _, cate_indicator, columns = dataset.get_data(dataset_format=\"dataframe\")\n    print(dataset.name)\n    print(dataset.openml_url)\n    print(df)\n\n    target_col = dataset.default_target_attribute\n    print(\"Nan count\", df.isna().sum().sum())\n    print(\"cont\", get_columns_list(df, columns, cate_indicator, target_col, cont_checker))\n    print(\"cate\", get_columns_list(df, columns, cate_indicator, target_col, cate_checker))\n    print(\"target\", target_col)\n\n    task, dim_out = get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col)\n    print(f\"task: {task}\")\n    print(f\"dim_out: {dim_out}\")\n    exit()", "\n\nclass OpenmlDataset(TabularDataFrame):\n    def __init__(self, data_id, config, download: bool = False) -> None:\n        super().__init__(config=config, download=download)\n        dataset = openml.datasets.get_dataset(data_id)\n        if config.show_data_detail:\n            print_dataset_details(dataset, data_id)\n\n        df, _, cate_indicator, columns = dataset.get_data(dataset_format=\"dataframe\")\n\n        self.all_columns = columns\n        target_col = dataset.default_target_attribute\n        self.continuous_columns = get_columns_list(df, columns, cate_indicator, target_col, cont_checker)\n        self.categorical_columns = get_columns_list(df, columns, cate_indicator, target_col, cate_checker)\n\n        self.task, self.dim_out = get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col)\n\n        self.target_columns = [target_col]\n        if self.task != \"regression\":\n            df[target_col] = LabelEncoder().fit_transform(df[target_col])\n            self.train, self.test = train_test_split(\n                df, test_size=0.2, stratify=df[target_col], random_state=self.config.seed\n            )\n        else:\n            self.train, self.test = train_test_split(df, test_size=0.2, random_state=self.config.seed)", ""]}
{"filename": "data/datasets/adult.py", "chunked_list": ["import os\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom .tabular_dataframe import TabularDataFrame\n\n\nclass Adult(TabularDataFrame):\n    dim_out = 1\n\n    all_columns = [\n        \"age\",\n        \"workclass\",\n        \"fnlwgt\",\n        \"education\",\n        \"education-num\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"sex\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n        \"native-country\",\n        \"income\",\n    ]\n\n    continuous_columns = [\n        \"age\",\n        \"fnlwgt\",\n        \"education-num\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n    ]\n\n    categorical_columns = [\n        \"workclass\",\n        \"education\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"sex\",\n        \"native-country\",\n    ]\n\n    target_columns = [\"income\"]\n\n    task = \"binary\"\n\n    def __init__(self, config, download: bool = False) -> None:\n        super().__init__(config=config, download=download)\n        df = pd.read_csv(os.path.join(self.raw_folder, \"adult.csv\"))\n        df.columns = self.all_columns\n        df[\"income\"] = df[\"income\"].replace({\"<=50K\": 1, \">50K\": 0})\n        self.train, self.test = train_test_split(\n            df, test_size=0.2, stratify=df[\"income\"], random_state=self.config.seed\n        )", "class Adult(TabularDataFrame):\n    dim_out = 1\n\n    all_columns = [\n        \"age\",\n        \"workclass\",\n        \"fnlwgt\",\n        \"education\",\n        \"education-num\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"sex\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n        \"native-country\",\n        \"income\",\n    ]\n\n    continuous_columns = [\n        \"age\",\n        \"fnlwgt\",\n        \"education-num\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n    ]\n\n    categorical_columns = [\n        \"workclass\",\n        \"education\",\n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"sex\",\n        \"native-country\",\n    ]\n\n    target_columns = [\"income\"]\n\n    task = \"binary\"\n\n    def __init__(self, config, download: bool = False) -> None:\n        super().__init__(config=config, download=download)\n        df = pd.read_csv(os.path.join(self.raw_folder, \"adult.csv\"))\n        df.columns = self.all_columns\n        df[\"income\"] = df[\"income\"].replace({\"<=50K\": 1, \">50K\": 0})\n        self.train, self.test = train_test_split(\n            df, test_size=0.2, stratify=df[\"income\"], random_state=self.config.seed\n        )", ""]}
{"filename": "data/datasets/__init__.py", "chunked_list": ["from .adult import Adult\nfrom .ca_housing import CAHousing\nfrom .openml import OpenmlDataset\n"]}
{"filename": "data/datasets/utils.py", "chunked_list": ["import numpy as np\nfrom sklearn.preprocessing import QuantileTransformer\n\n\ndef quantiletransform(df_cont, noise=1e-3, seed=42):\n    df_cont = df_cont.copy()\n    sc = QuantileTransformer(\n        output_distribution=\"normal\",\n        n_quantiles=max(min(df_cont.shape[0] // 30, 1000), 10),\n        subsample=int(1e9),\n        random_state=seed,\n    )\n    stds = np.std(df_cont.values, axis=0, keepdims=True)\n    noise_std = noise / np.maximum(stds, noise)\n    normal = np.random.default_rng(seed).standard_normal(df_cont.shape)\n    df_cont += noise_std * normal\n    sc.fit(df_cont)\n    return sc", ""]}
{"filename": "data/datasets/tabular_dataframe.py", "chunked_list": ["import logging\nimport os\nfrom typing import Dict, List, Optional, Sequence, Tuple, Union\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom torchvision.datasets.utils import check_integrity\n\nfrom .utils import quantiletransform", "\nfrom .utils import quantiletransform\n\nlogger = logging.getLogger(__name__)\n\n\n# Copied from https://github.com/pfnet-research/deep-table.\n# Modified by somaonishi and shoyameguro.\nclass TabularDataFrame(object):\n    \"\"\"Base class for datasets\"\"\"\n\n    def __init__(self, config, download: bool = False) -> None:\n        \"\"\"\n        Args:\n            root (str): Path to the root of datasets for saving/loading.\n            download (bool): If True, you must implement `self.download` method\n                in the child class. Defaults to False.\n        \"\"\"\n        self.config = config\n        self.root = config.data_dir\n        if download:\n            self.download()\n\n    @property\n    def mirrors(self) -> None:\n        pass\n\n    @property\n    def resources(self) -> None:\n        pass\n\n    @property\n    def raw_folder(self) -> str:\n        \"\"\"The folder where raw data will be stored\"\"\"\n        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n\n    def _check_exists(self, fpath: Sequence[Tuple[str, Union[str, None]]]) -> bool:\n        \"\"\"\n        Args:\n            fpath (sequence of tuple[str, (str or None)]): Each value has the format\n                [file_path, (md5sum or None)]. Checking if files are correctly\n                stored in `self.raw_folder`. If `md5sum` is provided, checking\n                the file itself is valid.\n        \"\"\"\n        return all(check_integrity(os.path.join(self.raw_folder, path[0]), md5=path[1]) for path in fpath)\n\n    def download(self) -> None:\n        \"\"\"\n        Implement this function if the dataset is downloadable.\n        See :func:`~deep_table.data.datasets.adult.Adult` for an example implementation.\n        \"\"\"\n        raise NotImplementedError\n\n    def cat_cardinalities(self, use_unk: bool = True) -> Optional[List[int]]:\n        \"\"\"List of the numbers of the categories of each column.\n\n        Args:\n            use_unk (bool): If True, each feature (column) has \"unknown\" categories.\n\n        Returns:\n            List[int], optional: List of cardinalities. i-th value denotes\n                the number of categories which i-th column has.\n        \"\"\"\n        cardinalities = []\n        df_train = self.get_dataframe(train=True)\n        df_train_cat = df_train[self.categorical_columns]\n        cardinalities = df_train_cat.nunique().values.astype(int)\n        if use_unk:\n            cardinalities += 1\n        cardinalities_list = cardinalities.tolist()\n        return cardinalities_list\n\n    def get_dataframe(self, train: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Args:\n            train (bool): If True, the returned value is `pd.DataFrame` for train.\n                If False, the returned value is `pd.DataFrame` for test.\n\n        Returns:\n            `pd.DataFrame`\n        \"\"\"\n        if train:\n            return self.train\n        else:\n            return self.test\n\n    def get_classify_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        df_train = self.get_dataframe(train=True)\n        df_test = self.get_dataframe(train=False)\n        df_train, df_val = train_test_split(\n            df_train,\n            test_size=val_size,\n            stratify=df_train[self.target_columns],\n            random_state=seed,\n        )\n        if self.config.train_size < 1:\n            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n            df_train, _ = train_test_split(\n                df_train,\n                train_size=self.config.train_size,\n                stratify=df_train[self.target_columns],\n                random_state=seed,\n            )\n\n        classify_dfs = {\n            \"train\": df_train,\n            \"val\": df_val,\n            \"test\": df_test,\n        }\n\n        if len(classify_dfs[\"val\"]) > 20000:\n            logger.info(\"validation size reduction: {} -> 20000\".format(len(classify_dfs[\"val\"])))\n            classify_dfs[\"val\"], _ = train_test_split(\n                classify_dfs[\"val\"],\n                train_size=20000,\n                stratify=classify_dfs[\"val\"][self.target_columns],\n                random_state=seed,\n            )\n        return classify_dfs\n\n    def get_regression_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        df_train = self.get_dataframe(train=True)\n        df_test = self.get_dataframe(train=False)\n        df_train, df_val = train_test_split(df_train, test_size=val_size, random_state=seed)\n        if self.config.train_size < 1:\n            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n            df_train, _ = train_test_split(\n                df_train,\n                train_size=self.config.train_size,\n                random_state=seed,\n            )\n\n        regression_dfs = {\n            \"train\": df_train,\n            \"val\": df_val,\n            \"test\": df_test,\n        }\n        self.y_mean = regression_dfs[\"train\"][self.target_columns].to_numpy().mean()\n        self.y_std = regression_dfs[\"train\"][self.target_columns].to_numpy().std()\n        for key in regression_dfs.keys():\n            regression_dfs[key] = self._regression_encoder(regression_dfs[key])\n\n        if len(regression_dfs[\"val\"]) > 20000:\n            logger.info(\"validation size reduction: {} -> 20000\".format(len(regression_dfs[\"val\"])))\n            regression_dfs[\"val\"], _ = train_test_split(regression_dfs[\"val\"], train_size=20000, random_state=seed)\n        return regression_dfs\n\n    def processed_dataframes(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Returns:\n            dict[str, DataFrame]: The value has the keys \"train\", \"val\" and \"test\".\n        \"\"\"\n        if self.task == \"regression\":\n            dfs = self.get_regression_dataframe(val_size=val_size, seed=seed)\n        else:\n            dfs = self.get_classify_dataframe(val_size=val_size, seed=seed)\n        # preprocessing\n        if self.categorical_columns != []:\n            categorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n                dfs[\"train\"][self.categorical_columns]\n            )\n        # only apply DL model.\n        if self.continuous_columns != []:\n            continuous_encoder = quantiletransform(dfs[\"train\"][self.continuous_columns], seed=seed)\n\n        for key in dfs.keys():\n            if self.categorical_columns != []:\n                dfs[key][self.categorical_columns] = (\n                    categorical_encoder.transform(dfs[key][self.categorical_columns]) + 1\n                )\n            if self.continuous_columns != []:\n                dfs[key][self.continuous_columns] = continuous_encoder.transform(dfs[key][self.continuous_columns])\n        return dfs\n\n    def _regression_encoder(self, df):\n        df[self.target_columns] = (df[self.target_columns] - self.y_mean) / self.y_std\n        return df\n\n    def num_categories(self, use_unk: bool = True) -> int:\n        \"\"\"Total numbers of categories\n\n        Args:\n            use_unk (bool): If True, the returned value is calculated\n                as there are unknown categories.\n        \"\"\"\n        return sum(self.cat_cardinalities(use_unk=use_unk))", "class TabularDataFrame(object):\n    \"\"\"Base class for datasets\"\"\"\n\n    def __init__(self, config, download: bool = False) -> None:\n        \"\"\"\n        Args:\n            root (str): Path to the root of datasets for saving/loading.\n            download (bool): If True, you must implement `self.download` method\n                in the child class. Defaults to False.\n        \"\"\"\n        self.config = config\n        self.root = config.data_dir\n        if download:\n            self.download()\n\n    @property\n    def mirrors(self) -> None:\n        pass\n\n    @property\n    def resources(self) -> None:\n        pass\n\n    @property\n    def raw_folder(self) -> str:\n        \"\"\"The folder where raw data will be stored\"\"\"\n        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n\n    def _check_exists(self, fpath: Sequence[Tuple[str, Union[str, None]]]) -> bool:\n        \"\"\"\n        Args:\n            fpath (sequence of tuple[str, (str or None)]): Each value has the format\n                [file_path, (md5sum or None)]. Checking if files are correctly\n                stored in `self.raw_folder`. If `md5sum` is provided, checking\n                the file itself is valid.\n        \"\"\"\n        return all(check_integrity(os.path.join(self.raw_folder, path[0]), md5=path[1]) for path in fpath)\n\n    def download(self) -> None:\n        \"\"\"\n        Implement this function if the dataset is downloadable.\n        See :func:`~deep_table.data.datasets.adult.Adult` for an example implementation.\n        \"\"\"\n        raise NotImplementedError\n\n    def cat_cardinalities(self, use_unk: bool = True) -> Optional[List[int]]:\n        \"\"\"List of the numbers of the categories of each column.\n\n        Args:\n            use_unk (bool): If True, each feature (column) has \"unknown\" categories.\n\n        Returns:\n            List[int], optional: List of cardinalities. i-th value denotes\n                the number of categories which i-th column has.\n        \"\"\"\n        cardinalities = []\n        df_train = self.get_dataframe(train=True)\n        df_train_cat = df_train[self.categorical_columns]\n        cardinalities = df_train_cat.nunique().values.astype(int)\n        if use_unk:\n            cardinalities += 1\n        cardinalities_list = cardinalities.tolist()\n        return cardinalities_list\n\n    def get_dataframe(self, train: bool = True) -> pd.DataFrame:\n        \"\"\"\n        Args:\n            train (bool): If True, the returned value is `pd.DataFrame` for train.\n                If False, the returned value is `pd.DataFrame` for test.\n\n        Returns:\n            `pd.DataFrame`\n        \"\"\"\n        if train:\n            return self.train\n        else:\n            return self.test\n\n    def get_classify_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        df_train = self.get_dataframe(train=True)\n        df_test = self.get_dataframe(train=False)\n        df_train, df_val = train_test_split(\n            df_train,\n            test_size=val_size,\n            stratify=df_train[self.target_columns],\n            random_state=seed,\n        )\n        if self.config.train_size < 1:\n            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n            df_train, _ = train_test_split(\n                df_train,\n                train_size=self.config.train_size,\n                stratify=df_train[self.target_columns],\n                random_state=seed,\n            )\n\n        classify_dfs = {\n            \"train\": df_train,\n            \"val\": df_val,\n            \"test\": df_test,\n        }\n\n        if len(classify_dfs[\"val\"]) > 20000:\n            logger.info(\"validation size reduction: {} -> 20000\".format(len(classify_dfs[\"val\"])))\n            classify_dfs[\"val\"], _ = train_test_split(\n                classify_dfs[\"val\"],\n                train_size=20000,\n                stratify=classify_dfs[\"val\"][self.target_columns],\n                random_state=seed,\n            )\n        return classify_dfs\n\n    def get_regression_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        df_train = self.get_dataframe(train=True)\n        df_test = self.get_dataframe(train=False)\n        df_train, df_val = train_test_split(df_train, test_size=val_size, random_state=seed)\n        if self.config.train_size < 1:\n            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n            df_train, _ = train_test_split(\n                df_train,\n                train_size=self.config.train_size,\n                random_state=seed,\n            )\n\n        regression_dfs = {\n            \"train\": df_train,\n            \"val\": df_val,\n            \"test\": df_test,\n        }\n        self.y_mean = regression_dfs[\"train\"][self.target_columns].to_numpy().mean()\n        self.y_std = regression_dfs[\"train\"][self.target_columns].to_numpy().std()\n        for key in regression_dfs.keys():\n            regression_dfs[key] = self._regression_encoder(regression_dfs[key])\n\n        if len(regression_dfs[\"val\"]) > 20000:\n            logger.info(\"validation size reduction: {} -> 20000\".format(len(regression_dfs[\"val\"])))\n            regression_dfs[\"val\"], _ = train_test_split(regression_dfs[\"val\"], train_size=20000, random_state=seed)\n        return regression_dfs\n\n    def processed_dataframes(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Returns:\n            dict[str, DataFrame]: The value has the keys \"train\", \"val\" and \"test\".\n        \"\"\"\n        if self.task == \"regression\":\n            dfs = self.get_regression_dataframe(val_size=val_size, seed=seed)\n        else:\n            dfs = self.get_classify_dataframe(val_size=val_size, seed=seed)\n        # preprocessing\n        if self.categorical_columns != []:\n            categorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n                dfs[\"train\"][self.categorical_columns]\n            )\n        # only apply DL model.\n        if self.continuous_columns != []:\n            continuous_encoder = quantiletransform(dfs[\"train\"][self.continuous_columns], seed=seed)\n\n        for key in dfs.keys():\n            if self.categorical_columns != []:\n                dfs[key][self.categorical_columns] = (\n                    categorical_encoder.transform(dfs[key][self.categorical_columns]) + 1\n                )\n            if self.continuous_columns != []:\n                dfs[key][self.continuous_columns] = continuous_encoder.transform(dfs[key][self.continuous_columns])\n        return dfs\n\n    def _regression_encoder(self, df):\n        df[self.target_columns] = (df[self.target_columns] - self.y_mean) / self.y_std\n        return df\n\n    def num_categories(self, use_unk: bool = True) -> int:\n        \"\"\"Total numbers of categories\n\n        Args:\n            use_unk (bool): If True, the returned value is calculated\n                as there are unknown categories.\n        \"\"\"\n        return sum(self.cat_cardinalities(use_unk=use_unk))", ""]}
{"filename": "data/datasets/ca_housing.py", "chunked_list": ["from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nfrom .tabular_dataframe import TabularDataFrame\n\n\nclass CAHousing(TabularDataFrame):\n    dim_out = 1\n\n    all_columns = [\n        \"MedInc\",\n        \"HouseAge\",\n        \"AveRooms\",\n        \"AveBedrms\",\n        \"Population\",\n        \"AveOccup\",\n        \"Latitude\",\n        \"Longitude\",\n        \"MedHouseVal\",\n    ]\n\n    continuous_columns = [\n        \"MedInc\",\n        \"HouseAge\",\n        \"AveRooms\",\n        \"AveBedrms\",\n        \"Population\",\n        \"AveOccup\",\n        \"Latitude\",\n        \"Longitude\",\n    ]\n\n    categorical_columns = []\n\n    target_columns = [\"MedHouseVal\"]\n\n    task = \"regression\"\n\n    def __init__(self, config, download: bool = False) -> None:\n        super().__init__(config=config, download=download)\n        df = fetch_california_housing(as_frame=True).frame\n        self.train, self.test = train_test_split(df, test_size=0.2, random_state=self.config.seed)\n\n    def download(self) -> None:\n        pass", "\n    # def raw_dataframe(self, train: bool = True) -> pd.DataFrame:\n    #     if train:\n    #         return self.train\n    #     else:\n    #         return self.test\n\n    # def processed_dataframes(self, *args, **kwargs) -> Dict[str, pd.DataFrame]:\n    #     df_train = self.raw_dataframe(train=True)\n    #     df_test = self.raw_dataframe(train=False)", "    #     df_train = self.raw_dataframe(train=True)\n    #     df_test = self.raw_dataframe(train=False)\n    #     df_train, df_val = train_test_split(df_train, **kwargs)\n    #     dfs = {\n    #         \"train\": df_train,\n    #         \"val\": df_val,\n    #         \"test\": df_test,\n    #     }\n    #     # preprocessing\n    #     # only apply DL model.", "    #     # preprocessing\n    #     # only apply DL model.\n    #     sc = quantiletransform(df_train[self.continuous_columns], seed=self.config.seed)\n\n    #     self.y_mean = df_train[self.target_columns].to_numpy().mean()\n    #     self.y_std = df_train[self.target_columns].to_numpy().std()\n    #     for key in dfs.keys():\n    #         dfs[key][self.continuous_columns] = sc.transform(dfs[key][self.continuous_columns])\n    #         dfs[key] = self._label_encoder(dfs[key])\n    #     return dfs", "    #         dfs[key] = self._label_encoder(dfs[key])\n    #     return dfs\n\n    # def _label_encoder(self, df):\n    #     df[self.target_columns] = (df[self.target_columns] - self.y_mean) / self.y_std\n    #     return df\n"]}
{"filename": "runner/self_sl.py", "chunked_list": ["import logging\n\nimport torch.nn as nn\n\nimport model\nimport trainer\nfrom model import FTTransformer\nfrom trainer import FTTransTraniner\nfrom trainer.self_supervised.base import BaseSSLTrainer\n", "from trainer.self_supervised.base import BaseSSLTrainer\n\nfrom .base import BaseRunner\n\nlogger = logging.getLogger(__name__)\n\n\nclass SelfSLRunner(BaseRunner):\n    def __init__(self, config) -> None:\n        super().__init__(config)\n\n    def self_sl_init(self):\n        Model = getattr(model, self.config.model.name)\n        logger.info(f\"Model is {self.config.model.name}.\")\n\n        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n        self.model: FTTransformer = Model.make_default(\n            num_features=num_features,\n            n_num_features=self.datamodule.num_continuous_features,\n            cat_cardinalities=self.datamodule.cat_cardinalities,\n            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n            d_out=self.datamodule.d_out,\n        )\n        self.model.to(self.device)\n\n        optimizer = self.model.make_default_optimizer()\n        criterion = None\n        Trainer = getattr(trainer, self.config.model.trainer)\n        logger.info(f\"Trainer is {self.config.model.trainer}.\")\n\n        assert issubclass(Trainer, BaseSSLTrainer), \"Trainer must be a subclass of BaseSLTrainer.\"\n        del self.config.model.params.p\n        self.self_sl_trainer: BaseSSLTrainer = Trainer(\n            datamodule=self.datamodule,\n            batch_size=self.config.batch_size,\n            eval_batch_size=self.config.eval_batch_size,\n            model=self.model,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=self.device,\n            epochs=200,\n            patience=10,\n            eval_metric=\"val/self-sl-loss\",\n            eval_less_is_better=True,\n            mixed_fp16=self.config.mixed_fp16,\n            **self.config.model.params,\n        )\n\n    def sl_init(self):\n        state_dict = self.model.state_dict()\n        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n        self.model = FTTransformer.make_default(\n            num_features=num_features,\n            n_num_features=self.datamodule.num_continuous_features,\n            cat_cardinalities=self.datamodule.cat_cardinalities,\n            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n            d_out=self.datamodule.d_out,\n        )\n        missing, unexpected = self.model.load_state_dict(state_dict, strict=False)\n        # for p in self.model.parameters():\n        #     p.requires_grad = False\n        # for p in self.model.head.parameters():\n        #     p.requires_grad = True\n\n        self.model.to(self.device)\n\n        optimizer = self.model.make_default_optimizer()\n        criterion = (\n            nn.BCEWithLogitsLoss()\n            if self.datamodule.task == \"binary\"\n            else nn.CrossEntropyLoss()\n            if self.datamodule.task == \"multiclass\"\n            else nn.MSELoss()\n        )\n        self.sl_trainer = FTTransTraniner(\n            datamodule=self.datamodule,\n            batch_size=self.config.batch_size,\n            eval_batch_size=self.config.eval_batch_size,\n            model=self.model,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=self.device,\n            epochs=self.config.epochs,\n            patience=self.config.patience,\n            eval_metric=self.config.eval.metric,\n            eval_less_is_better=self.config.eval.less_is_better,\n            mixed_fp16=self.config.mixed_fp16,\n            save_model=self.config.save_model,\n            tensorbord_dir=\"./self_supervised\",\n        )\n\n    def run(self):\n        self.self_sl_init()\n        self.self_sl_trainer.train()\n        self.sl_init()\n        self.sl_trainer.train()\n        self.sl_trainer.print_evaluate()", ""]}
{"filename": "runner/base.py", "chunked_list": ["import logging\n\nimport torch\nfrom hydra.utils import to_absolute_path\n\nfrom data import get_datamodule\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseRunner:\n    def __init__(self, config) -> None:\n        self.config = config\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        logger.info(f\"Select {self.device}\")\n\n        config.data_dir = to_absolute_path(config.data_dir)\n        self.datamodule = get_datamodule(config)", "\n\nclass BaseRunner:\n    def __init__(self, config) -> None:\n        self.config = config\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        logger.info(f\"Select {self.device}\")\n\n        config.data_dir = to_absolute_path(config.data_dir)\n        self.datamodule = get_datamodule(config)", ""]}
{"filename": "runner/__init__.py", "chunked_list": ["from .self_sl import SelfSLRunner\nfrom .supervised import SupervisedRunner\n"]}
{"filename": "runner/supervised.py", "chunked_list": ["import logging\n\nimport torch.nn as nn\n\nimport model\nimport trainer\nfrom model import FTTransformer\nfrom trainer.supervised.base import BaseSupervisedTrainer\n\nfrom .base import BaseRunner", "\nfrom .base import BaseRunner\n\nlogger = logging.getLogger(__name__)\n\n\nclass SupervisedRunner(BaseRunner):\n    def __init__(self, config) -> None:\n        super().__init__(config)\n\n    def supervised_init(self):\n        Model = getattr(model, self.config.model.name)\n        logger.info(f\"Model is {self.config.model.name}.\")\n        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n        self.model: FTTransformer = Model.make_default(\n            num_features=num_features,\n            n_num_features=self.datamodule.num_continuous_features,\n            cat_cardinalities=self.datamodule.cat_cardinalities,\n            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n            d_out=self.datamodule.d_out,\n        )\n        self.model.to(self.device)\n\n        optimizer = self.model.make_default_optimizer()\n        criterion = (\n            nn.BCEWithLogitsLoss()\n            if self.datamodule.task == \"binary\"\n            else nn.CrossEntropyLoss()\n            if self.datamodule.task == \"multiclass\"\n            else nn.MSELoss()\n        )\n        Trainer = getattr(trainer, self.config.model.trainer)\n        logger.info(f\"Trainer is {self.config.model.trainer}.\")\n\n        self.trainer: BaseSupervisedTrainer = Trainer(\n            datamodule=self.datamodule,\n            batch_size=self.config.batch_size,\n            eval_batch_size=self.config.eval_batch_size,\n            model=self.model,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=self.device,\n            epochs=self.config.epochs,\n            patience=self.config.patience,\n            eval_metric=self.config.eval.metric,\n            eval_less_is_better=self.config.eval.less_is_better,\n            mixed_fp16=self.config.mixed_fp16,\n            save_model=self.config.save_model,\n            **self.config.model.params,\n        )\n\n    def run(self):\n        self.supervised_init()\n        self.trainer.train()\n        self.trainer.print_evaluate()", ""]}
{"filename": "model/__init__.py", "chunked_list": ["from .core.fttrans import FTTransformer\nfrom .hidden_mix_alpha import FTTransformerWithHiddenMix\nfrom .mask_token import FTTransformerWithMaskToken\nfrom .mixup import FTTransformerWithMixup\n"]}
{"filename": "model/mixup.py", "chunked_list": ["from typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom .core import FTTransformer\n\n\ndef mixup_process(x: Tensor, lam: float, target_reweighted: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n    \"\"\"\n    Args:\n        x (tensor): the input of shape (b, n, d)\n        lam (float): the mixing coefficient\n        target_reweighted (tensor): the target labels of shape (b, num_classes)\n\n    Returns:\n        new_x (tensor): the output of shape (b, n, d) representing the mixed input data\n        target_reweighted (tensor): the mixed target labels of shape (b, num_classes)\n    \"\"\"\n    indices = torch.randperm(x.shape[0])\n    new_x = x * (1 - lam) + x[indices] * lam\n    if target_reweighted is not None:\n        target_shuffled_onehot = target_reweighted[indices]\n        target_reweighted = target_reweighted * (1 - lam) + target_shuffled_onehot * lam\n        return new_x, target_reweighted\n    else:\n        return new_x", "def mixup_process(x: Tensor, lam: float, target_reweighted: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n    \"\"\"\n    Args:\n        x (tensor): the input of shape (b, n, d)\n        lam (float): the mixing coefficient\n        target_reweighted (tensor): the target labels of shape (b, num_classes)\n\n    Returns:\n        new_x (tensor): the output of shape (b, n, d) representing the mixed input data\n        target_reweighted (tensor): the mixed target labels of shape (b, num_classes)\n    \"\"\"\n    indices = torch.randperm(x.shape[0])\n    new_x = x * (1 - lam) + x[indices] * lam\n    if target_reweighted is not None:\n        target_shuffled_onehot = target_reweighted[indices]\n        target_reweighted = target_reweighted * (1 - lam) + target_shuffled_onehot * lam\n        return new_x, target_reweighted\n    else:\n        return new_x", "\n\nclass FTTransformerWithMixup(FTTransformer):\n    def forward(\n        self,\n        x_num: Optional[Tensor],\n        x_cat: Optional[Tensor],\n        target: Tensor = None,\n        lam: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            x_num (tensor): the input of shape (b, n) representing the numerical values\n            x_cat (tensor): the input of shape (b, n) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n            lam (tensor): the mixing coefficient\n\n        Returns:\n            x (tensor): the output of shape (b, num_classes) representing the model's prediction\n            new_target (tensor): the mixed target labels of shape (b, num_classes)\n        \"\"\"\n        \"\"\"\n        Supervised\n        \"\"\"\n        x = self.feature_tokenizer(x_num, x_cat)\n        x = x + self.pos_embedding\n        x = self.cls_token(x)\n        x = self.transformer(x)\n        x = x[:, -1]\n        x = self.normalization(x)\n        x = self.activation(x)\n\n        if target is not None:\n            assert lam is not None\n            x, new_target = mixup_process(x, lam, target)\n            x = self.head(x)\n            return x, new_target\n        else:\n            return self.head(x)\n\n    def forward_no_labelmix(\n        self,\n        x_num: Optional[Tensor],\n        x_cat: Optional[Tensor],\n        alpha: float = 0.0,\n    ) -> Tensor:\n        \"\"\"\n        Args:\n            x_num (tensor): the input of shape (b, n) representing the numerical values\n            x_cat (tensor): the input of shape (b, n) representing the categorical values\n            alpha (float): the mixing coefficient\n\n        Returns:\n            tensor: the output of shape (b, num_classes) representing the model's prediction\n        \"\"\"\n        \"\"\"\n        Self-SL\n        0.0      <= alpha <= 1.0\n        original <=   x   <= other\n        \"\"\"\n        x = self.feature_tokenizer(x_num, x_cat)\n        x = x + self.pos_embedding\n        x = self.cls_token(x)\n        x = self.transformer(x)\n        x = x[:, -1]\n        x = self.normalization(x)\n        x = self.activation(x)\n        x = mixup_process(x, alpha)\n        return self.head(x)", ""]}
{"filename": "model/mask_token.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Type, Union\n\nimport torch\nimport torch.nn as nn\nfrom rtdl.modules import _INTERNAL_ERROR_MESSAGE\nfrom torch import Tensor\n\nfrom .core import FeatureTokenizer, FTTransformer, Head, Transformer\n\n\nclass FTTransformerWithMaskToken(FTTransformer):\n    def __init__(\n        self,\n        num_features,\n        feature_tokenizer: FeatureTokenizer,\n        transformer: Transformer,\n        head: Head,\n    ) -> None:\n        super().__init__(num_features, feature_tokenizer, transformer, head)\n        self.mask_token = nn.Parameter(Tensor(1, 1, feature_tokenizer.d_token))\n        self.initialization_.apply(self.mask_token, feature_tokenizer.d_token)\n\n    def optimization_param_groups(self) -> List[Dict[str, Any]]:\n        \"\"\"The replacement for :code:`.parameters()` when creating optimizers.\n\n        Example::\n\n            optimizer = AdamW(\n                model.optimization_param_groups(), lr=1e-4, weight_decay=1e-5\n            )\n        \"\"\"\n        no_wd_names = [\"feature_tokenizer\", \"normalization\", \".bias\", \"pos_embedding\", \"mask_token\"]\n        assert isinstance(getattr(self, no_wd_names[0], None), FeatureTokenizer), _INTERNAL_ERROR_MESSAGE\n        assert (\n            sum(1 for name, _ in self.named_modules() if no_wd_names[1] in name)\n            == len(self.transformer.blocks) * 2\n            - int(\"attention_normalization\" not in self.transformer.blocks[0])  # type: ignore\n            + 1\n        ), _INTERNAL_ERROR_MESSAGE\n\n        def needs_wd(name):\n            return all(x not in name for x in no_wd_names)\n\n        return [\n            {\"params\": [v for k, v in self.named_parameters() if needs_wd(k)]},\n            {\n                \"params\": [v for k, v in self.named_parameters() if not needs_wd(k)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    def random_masking(self, x: Tensor, mask_ratio: float) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n, d).\n            mask_ratio (float): the ratio of data points to be masked\n\n        Returns:\n            x_masked (tensor): the output of shape (b, n, d) representing the masked data\n        \"\"\"\n        b, n, d = x.shape\n        mask = torch.bernoulli(torch.ones(b, n) * mask_ratio)\n        mask = mask.unsqueeze(-1).to(torch.float).to(x.device)\n        mask = mask.repeat(1, 1, d)\n\n        mask_tokens = self.mask_token.repeat(b, n, 1)\n        x_masked = x * (1 - mask) + mask_tokens * mask\n        return x_masked\n\n    def forward(\n        self,\n        x_num: Optional[Tensor],\n        x_cat: Optional[Tensor],\n        mask_ratio: float = 0.0,\n        bias_after_mask: bool = True,\n    ) -> Tensor:\n        \"\"\"\n        Args:\n            x_num (tensor): the input of shape (b, n) representing the numerical values\n            x_cat (tensor): the input of shape (b, n) representing the categorical values\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        Returns:\n            tensor: the output of shape (b, num_classes) representing the model's prediction\n        \"\"\"\n        x = self.feature_tokenizer(x_num, x_cat)\n\n        if not bias_after_mask:\n            x = x + self.pos_embedding\n        if mask_ratio > 0.0:\n            x = self.random_masking(x, mask_ratio)\n        if bias_after_mask:\n            x = x + self.pos_embedding\n\n        x = self.cls_token(x)\n        x = self.transformer(x)\n        x = x[:, -1]\n        x = self.normalization(x)\n        x = self.activation(x)\n        return self.head(x)", "\n\nclass FTTransformerWithMaskToken(FTTransformer):\n    def __init__(\n        self,\n        num_features,\n        feature_tokenizer: FeatureTokenizer,\n        transformer: Transformer,\n        head: Head,\n    ) -> None:\n        super().__init__(num_features, feature_tokenizer, transformer, head)\n        self.mask_token = nn.Parameter(Tensor(1, 1, feature_tokenizer.d_token))\n        self.initialization_.apply(self.mask_token, feature_tokenizer.d_token)\n\n    def optimization_param_groups(self) -> List[Dict[str, Any]]:\n        \"\"\"The replacement for :code:`.parameters()` when creating optimizers.\n\n        Example::\n\n            optimizer = AdamW(\n                model.optimization_param_groups(), lr=1e-4, weight_decay=1e-5\n            )\n        \"\"\"\n        no_wd_names = [\"feature_tokenizer\", \"normalization\", \".bias\", \"pos_embedding\", \"mask_token\"]\n        assert isinstance(getattr(self, no_wd_names[0], None), FeatureTokenizer), _INTERNAL_ERROR_MESSAGE\n        assert (\n            sum(1 for name, _ in self.named_modules() if no_wd_names[1] in name)\n            == len(self.transformer.blocks) * 2\n            - int(\"attention_normalization\" not in self.transformer.blocks[0])  # type: ignore\n            + 1\n        ), _INTERNAL_ERROR_MESSAGE\n\n        def needs_wd(name):\n            return all(x not in name for x in no_wd_names)\n\n        return [\n            {\"params\": [v for k, v in self.named_parameters() if needs_wd(k)]},\n            {\n                \"params\": [v for k, v in self.named_parameters() if not needs_wd(k)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    def random_masking(self, x: Tensor, mask_ratio: float) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n, d).\n            mask_ratio (float): the ratio of data points to be masked\n\n        Returns:\n            x_masked (tensor): the output of shape (b, n, d) representing the masked data\n        \"\"\"\n        b, n, d = x.shape\n        mask = torch.bernoulli(torch.ones(b, n) * mask_ratio)\n        mask = mask.unsqueeze(-1).to(torch.float).to(x.device)\n        mask = mask.repeat(1, 1, d)\n\n        mask_tokens = self.mask_token.repeat(b, n, 1)\n        x_masked = x * (1 - mask) + mask_tokens * mask\n        return x_masked\n\n    def forward(\n        self,\n        x_num: Optional[Tensor],\n        x_cat: Optional[Tensor],\n        mask_ratio: float = 0.0,\n        bias_after_mask: bool = True,\n    ) -> Tensor:\n        \"\"\"\n        Args:\n            x_num (tensor): the input of shape (b, n) representing the numerical values\n            x_cat (tensor): the input of shape (b, n) representing the categorical values\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        Returns:\n            tensor: the output of shape (b, num_classes) representing the model's prediction\n        \"\"\"\n        x = self.feature_tokenizer(x_num, x_cat)\n\n        if not bias_after_mask:\n            x = x + self.pos_embedding\n        if mask_ratio > 0.0:\n            x = self.random_masking(x, mask_ratio)\n        if bias_after_mask:\n            x = x + self.pos_embedding\n\n        x = self.cls_token(x)\n        x = self.transformer(x)\n        x = x[:, -1]\n        x = self.normalization(x)\n        x = self.activation(x)\n        return self.head(x)", ""]}
{"filename": "model/hidden_mix_alpha.py", "chunked_list": ["from typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom .core import FTTransformer\n\n\ndef mask_generator(x: Tensor, lam: float) -> Tensor:\n    \"\"\"\n    Args:\n        x (tensor): the input of shape (b, n, d)\n        lam (float): the scalar coefficient to keep unmasked.\n\n    Returns:\n        mask (tensor): the binary mask of shape (b, n, d)\n    \"\"\"\n    b, n, d = x.shape\n    ids_noise = torch.rand(b, d, device=x.device)\n    ids_shuffle = torch.argsort(ids_noise, dim=1)\n    len_unmask = int(lam * d)\n    ids_unmask = ids_shuffle[:, :len_unmask]\n    mask = torch.zeros(b, d, device=x.device)\n    mask[torch.arange(b)[:, None], ids_unmask] = 1\n    mask = mask.unsqueeze(1)\n    mask = mask.repeat(1, n, 1)\n    return mask", "def mask_generator(x: Tensor, lam: float) -> Tensor:\n    \"\"\"\n    Args:\n        x (tensor): the input of shape (b, n, d)\n        lam (float): the scalar coefficient to keep unmasked.\n\n    Returns:\n        mask (tensor): the binary mask of shape (b, n, d)\n    \"\"\"\n    b, n, d = x.shape\n    ids_noise = torch.rand(b, d, device=x.device)\n    ids_shuffle = torch.argsort(ids_noise, dim=1)\n    len_unmask = int(lam * d)\n    ids_unmask = ids_shuffle[:, :len_unmask]\n    mask = torch.zeros(b, d, device=x.device)\n    mask[torch.arange(b)[:, None], ids_unmask] = 1\n    mask = mask.unsqueeze(1)\n    mask = mask.repeat(1, n, 1)\n    return mask", "\n\ndef hidden_mix(x: Tensor, target: Optional[Tensor], lam: float) -> Tuple[Tensor, Optional[Tensor]]:\n    \"\"\"\n    Args:\n        x (tensor): the input of shape (b, n, d)\n        target (tensor): the target labels of shape (b, num_classes)\n        lam (float): the scalar coefficient to keep unmasked.\n\n    Returns:\n        new_x (tensor): the output of shape (b, n, d) representing the mixed input data\n        label (tensor): the mixed target labels of shape (b, num_classes)\n    \"\"\"\n    mask = mask_generator(x, lam)\n    indices = torch.randperm(x.shape[0])\n    new_x = x * mask + x[indices] * (1 - mask)\n    if target is not None:\n        label = lam * target + (1 - lam) * target[indices]\n        return new_x, label\n    else:\n        return new_x, None", "\n\nclass FTTransformerWithHiddenMix(FTTransformer):\n    def forward(\n        self,\n        x_num: Optional[Tensor],\n        x_cat: Optional[Tensor],\n        target: Tensor = None,\n        lam: Tensor = None,\n    ) -> Tensor:\n        \"\"\"\n        Args:\n            x_num (tensor): the input of shape (b, n) representing the numerical values\n            x_cat (tensor): the input of shape (b, n) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n            lam (tensor): the scalar coefficient to keep unmasked.\n\n        Returns:\n            tensor: the output of shape (b, num_classes) representing the model's prediction\n        \"\"\"\n        x = self.feature_tokenizer(x_num, x_cat)\n        x = x + self.pos_embedding\n        if target is not None or lam is not None:\n            assert lam is not None\n            x, new_target = hidden_mix(x, target, lam)\n            x = self.cls_token(x)\n            x = self.transformer(x)\n            x = x[:, -1]\n            x = self.normalization(x)\n            x = self.activation(x)\n            return self.head(x), new_target\n        else:\n            x = self.cls_token(x)\n            x = self.transformer(x)\n            x = x[:, -1]\n            x = self.normalization(x)\n            x = self.activation(x)\n            return self.head(x)", ""]}
{"filename": "model/core/transformer.py", "chunked_list": ["import math\nimport time\nimport warnings\nfrom typing import Dict, List, Optional, Tuple, Union, cast\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom rtdl.modules import (\n    _INTERNAL_ERROR_MESSAGE,\n    ModuleType,", "    _INTERNAL_ERROR_MESSAGE,\n    ModuleType,\n    MultiheadAttention,\n    _all_or_none,\n    _is_glu_activation,\n    _make_nn_module,\n)\nfrom torch import Tensor\n\n\nclass MultiheadAttentionWithMask(MultiheadAttention):\n    def forward(\n        self,\n        x_q: Tensor,\n        x_kv: Tensor,\n        key_compression: Optional[nn.Linear],\n        value_compression: Optional[nn.Linear],\n        attn_mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n        \"\"\"Perform the forward pass.\n\n        Args:\n            x_q: query tokens\n            x_kv: key-value tokens\n            key_compression: Linformer-style compression for keys\n            value_compression: Linformer-style compression for values\n        Returns:\n            (tokens, attention_stats)\n        \"\"\"\n        assert _all_or_none(\n            [key_compression, value_compression]\n        ), \"If key_compression is (not) None, then value_compression must (not) be None\"\n        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n        for tensor in [q, k, v]:\n            assert tensor.shape[-1] % self.n_heads == 0, _INTERNAL_ERROR_MESSAGE\n        if key_compression is not None:\n            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n            v = value_compression(v.transpose(1, 2)).transpose(1, 2)  # type: ignore\n\n        batch_size = len(q)\n        d_head_key = k.shape[-1] // self.n_heads\n        d_head_value = v.shape[-1] // self.n_heads\n        n_q_tokens = q.shape[1]\n\n        q = self._reshape(q)\n        k = self._reshape(k)\n        attention_logits = q @ k.transpose(1, 2) / math.sqrt(d_head_key)\n\n        if attn_mask is not None:\n            attention_logits = attention_logits.masked_fill(attn_mask, float(\"-inf\"))\n\n        attention_probs = F.softmax(attention_logits, dim=-1)\n        if self.dropout is not None:\n            attention_probs = self.dropout(attention_probs)\n        x = attention_probs @ self._reshape(v)\n        x = (\n            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n            .transpose(1, 2)\n            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n        )\n        if self.W_out is not None:\n            x = self.W_out(x)\n        return x, {\n            \"attention_logits\": attention_logits,\n            \"attention_probs\": attention_probs,\n        }", "\n\nclass MultiheadAttentionWithMask(MultiheadAttention):\n    def forward(\n        self,\n        x_q: Tensor,\n        x_kv: Tensor,\n        key_compression: Optional[nn.Linear],\n        value_compression: Optional[nn.Linear],\n        attn_mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n        \"\"\"Perform the forward pass.\n\n        Args:\n            x_q: query tokens\n            x_kv: key-value tokens\n            key_compression: Linformer-style compression for keys\n            value_compression: Linformer-style compression for values\n        Returns:\n            (tokens, attention_stats)\n        \"\"\"\n        assert _all_or_none(\n            [key_compression, value_compression]\n        ), \"If key_compression is (not) None, then value_compression must (not) be None\"\n        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n        for tensor in [q, k, v]:\n            assert tensor.shape[-1] % self.n_heads == 0, _INTERNAL_ERROR_MESSAGE\n        if key_compression is not None:\n            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n            v = value_compression(v.transpose(1, 2)).transpose(1, 2)  # type: ignore\n\n        batch_size = len(q)\n        d_head_key = k.shape[-1] // self.n_heads\n        d_head_value = v.shape[-1] // self.n_heads\n        n_q_tokens = q.shape[1]\n\n        q = self._reshape(q)\n        k = self._reshape(k)\n        attention_logits = q @ k.transpose(1, 2) / math.sqrt(d_head_key)\n\n        if attn_mask is not None:\n            attention_logits = attention_logits.masked_fill(attn_mask, float(\"-inf\"))\n\n        attention_probs = F.softmax(attention_logits, dim=-1)\n        if self.dropout is not None:\n            attention_probs = self.dropout(attention_probs)\n        x = attention_probs @ self._reshape(v)\n        x = (\n            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n            .transpose(1, 2)\n            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n        )\n        if self.W_out is not None:\n            x = self.W_out(x)\n        return x, {\n            \"attention_logits\": attention_logits,\n            \"attention_probs\": attention_probs,\n        }", "\n\nclass Transformer(nn.Module):\n    \"\"\"Transformer with extra features.\n\n    This module is the backbone of `FTTransformer`.\"\"\"\n\n    WARNINGS = {\"first_prenormalization\": True, \"prenormalization\": True}\n\n    class FFN(nn.Module):\n        \"\"\"The Feed-Forward Network module used in every `Transformer` block.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_token: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout: float,\n            activation: ModuleType,\n        ):\n            super().__init__()\n            self.linear_first = nn.Linear(\n                d_token,\n                d_hidden * (2 if _is_glu_activation(activation) else 1),\n                bias_first,\n            )\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n            self.linear_second = nn.Linear(d_hidden, d_token, bias_second)\n\n        def forward(self, x: Tensor) -> Tensor:\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout(x)\n            x = self.linear_second(x)\n            return x\n\n    def __init__(\n        self,\n        *,\n        d_token: int,\n        n_blocks: int,\n        attention_n_heads: int,\n        attention_dropout: float,\n        attention_initialization: str,\n        attention_normalization: str,\n        ffn_d_hidden: int,\n        ffn_dropout: float,\n        ffn_activation: str,\n        ffn_normalization: str,\n        residual_dropout: float,\n        prenormalization: bool,\n        first_prenormalization: bool,\n        last_layer_query_idx: Union[None, List[int], slice],\n        n_tokens: Optional[int],\n        kv_compression_ratio: Optional[float],\n        kv_compression_sharing: Optional[str],\n    ) -> None:\n        super().__init__()\n        if isinstance(last_layer_query_idx, int):\n            raise ValueError(\n                \"last_layer_query_idx must be None, list[int] or slice. \"\n                f\"Do you mean last_layer_query_idx=[{last_layer_query_idx}] ?\"\n            )\n        if not prenormalization:\n            assert (\n                not first_prenormalization\n            ), \"If `prenormalization` is False, then `first_prenormalization` must be False\"\n        assert _all_or_none([n_tokens, kv_compression_ratio, kv_compression_sharing]), (\n            \"If any of the following arguments is (not) None, then all of them must (not) be None: \"\n            \"n_tokens, kv_compression_ratio, kv_compression_sharing\"\n        )\n        assert kv_compression_sharing in [None, \"headwise\", \"key-value\", \"layerwise\"]\n        if not prenormalization:\n            if self.WARNINGS[\"prenormalization\"]:\n                warnings.warn(\n                    \"prenormalization is set to False. Are you sure about this? \"\n                    \"The training can become less stable. \"\n                    \"You can turn off this warning by tweaking the \"\n                    \"rtdl.Transformer.WARNINGS dictionary.\",\n                    UserWarning,\n                )\n            assert (\n                not first_prenormalization\n            ), \"If prenormalization is False, then first_prenormalization is ignored and must be set to False\"\n        if prenormalization and first_prenormalization and self.WARNINGS[\"first_prenormalization\"]:\n            warnings.warn(\n                \"first_prenormalization is set to True. Are you sure about this? \"\n                \"For example, the vanilla FTTransformer with \"\n                \"first_prenormalization=True performs SIGNIFICANTLY worse. \"\n                \"You can turn off this warning by tweaking the \"\n                \"rtdl.Transformer.WARNINGS dictionary.\",\n                UserWarning,\n            )\n            time.sleep(3)\n\n        def make_kv_compression():\n            assert n_tokens and kv_compression_ratio, _INTERNAL_ERROR_MESSAGE  # for mypy\n            # https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/linformer/linformer_src/modules/multihead_linear_attention.py#L83\n            return nn.Linear(n_tokens, int(n_tokens * kv_compression_ratio), bias=False)\n\n        self.shared_kv_compression = (\n            make_kv_compression() if kv_compression_ratio and kv_compression_sharing == \"layerwise\" else None\n        )\n\n        self.prenormalization = prenormalization\n        self.last_layer_query_idx = last_layer_query_idx\n\n        self.blocks = nn.ModuleList([])\n        for layer_idx in range(n_blocks):\n            layer = nn.ModuleDict(\n                {\n                    \"attention\": MultiheadAttentionWithMask(\n                        d_token=d_token,\n                        n_heads=attention_n_heads,\n                        dropout=attention_dropout,\n                        bias=True,\n                        initialization=attention_initialization,\n                    ),\n                    \"ffn\": Transformer.FFN(\n                        d_token=d_token,\n                        d_hidden=ffn_d_hidden,\n                        bias_first=True,\n                        bias_second=True,\n                        dropout=ffn_dropout,\n                        activation=ffn_activation,\n                    ),\n                    \"attention_residual_dropout\": nn.Dropout(residual_dropout),\n                    \"ffn_residual_dropout\": nn.Dropout(residual_dropout),\n                    \"output\": nn.Identity(),  # for hooks-based introspection\n                }\n            )\n            if layer_idx or not prenormalization or first_prenormalization:\n                layer[\"attention_normalization\"] = _make_nn_module(attention_normalization, d_token)\n            layer[\"ffn_normalization\"] = _make_nn_module(ffn_normalization, d_token)\n            if kv_compression_ratio and self.shared_kv_compression is None:\n                layer[\"key_compression\"] = make_kv_compression()\n                if kv_compression_sharing == \"headwise\":\n                    layer[\"value_compression\"] = make_kv_compression()\n                else:\n                    assert kv_compression_sharing == \"key-value\", _INTERNAL_ERROR_MESSAGE\n            self.blocks.append(layer)\n\n    def _get_kv_compressions(self, layer):\n        return (\n            (self.shared_kv_compression, self.shared_kv_compression)\n            if self.shared_kv_compression is not None\n            else (layer[\"key_compression\"], layer[\"value_compression\"])\n            if \"key_compression\" in layer and \"value_compression\" in layer\n            else (layer[\"key_compression\"], layer[\"key_compression\"])\n            if \"key_compression\" in layer\n            else (None, None)\n        )\n\n    def _start_residual(self, layer, stage, x):\n        assert stage in [\"attention\", \"ffn\"], _INTERNAL_ERROR_MESSAGE\n        x_residual = x\n        if self.prenormalization:\n            norm_key = f\"{stage}_normalization\"\n            if norm_key in layer:\n                x_residual = layer[norm_key](x_residual)\n        return x_residual\n\n    def _end_residual(self, layer, stage, x, x_residual):\n        assert stage in [\"attention\", \"ffn\"], _INTERNAL_ERROR_MESSAGE\n        x_residual = layer[f\"{stage}_residual_dropout\"](x_residual)\n        x = x + x_residual\n        if not self.prenormalization:\n            x = layer[f\"{stage}_normalization\"](x)\n        return x\n\n    def forward(self, x: Tensor, attn_mask: Optional[Tensor] = None) -> Tensor:\n        assert x.ndim == 3, \"The input must have 3 dimensions: (n_objects, n_tokens, d_token)\"\n        for layer_idx, layer in enumerate(self.blocks):\n            layer = cast(nn.ModuleDict, layer)\n\n            query_idx = self.last_layer_query_idx if layer_idx + 1 == len(self.blocks) else None\n            x_residual = self._start_residual(layer, \"attention\", x)\n            x_residual, _ = layer[\"attention\"](\n                x_residual if query_idx is None else x_residual[:, query_idx],\n                x_residual,\n                attn_mask=attn_mask if query_idx is None or attn_mask is None else attn_mask[:, query_idx],\n                *self._get_kv_compressions(layer),\n            )\n            if query_idx is not None:\n                x = x[:, query_idx]\n            x = self._end_residual(layer, \"attention\", x, x_residual)\n\n            x_residual = self._start_residual(layer, \"ffn\", x)\n            x_residual = layer[\"ffn\"](x_residual)\n            x = self._end_residual(layer, \"ffn\", x, x_residual)\n            x = layer[\"output\"](x)\n\n        return x", ""]}
{"filename": "model/core/__init__.py", "chunked_list": ["from .fttrans import FeatureTokenizer, FTTransformer, Head\nfrom .transformer import Transformer\n"]}
{"filename": "model/core/fttrans.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Type, Union\n\nimport rtdl\nimport torch\nimport torch.nn as nn\nfrom rtdl.modules import (\n    _INTERNAL_ERROR_MESSAGE,\n    CategoricalFeatureTokenizer,\n    NumericalFeatureTokenizer,\n    _TokenInitialization,", "    NumericalFeatureTokenizer,\n    _TokenInitialization,\n)\nfrom torch import Tensor\n\nfrom .transformer import Transformer\n\n\nclass FeatureTokenizer(rtdl.modules.FeatureTokenizer):\n    def __init__(\n        self,\n        n_num_features: int,\n        cat_cardinalities: List[int],\n        d_token: int,\n        bias: bool = False,\n    ) -> None:\n        super().__init__(n_num_features, cat_cardinalities, d_token)\n        self.num_tokenizer = (\n            NumericalFeatureTokenizer(\n                n_features=n_num_features,\n                d_token=d_token,\n                bias=bias,\n                initialization=self.initialization,\n            )\n            if n_num_features\n            else None\n        )\n        self.cat_tokenizer = (\n            CategoricalFeatureTokenizer(cat_cardinalities, d_token, bias, self.initialization)\n            if cat_cardinalities\n            else None\n        )", "class FeatureTokenizer(rtdl.modules.FeatureTokenizer):\n    def __init__(\n        self,\n        n_num_features: int,\n        cat_cardinalities: List[int],\n        d_token: int,\n        bias: bool = False,\n    ) -> None:\n        super().__init__(n_num_features, cat_cardinalities, d_token)\n        self.num_tokenizer = (\n            NumericalFeatureTokenizer(\n                n_features=n_num_features,\n                d_token=d_token,\n                bias=bias,\n                initialization=self.initialization,\n            )\n            if n_num_features\n            else None\n        )\n        self.cat_tokenizer = (\n            CategoricalFeatureTokenizer(cat_cardinalities, d_token, bias, self.initialization)\n            if cat_cardinalities\n            else None\n        )", "\n\nclass Head(nn.Module):\n    \"\"\"The final module of the `Transformer` that performs BERT-like inference.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        bias: bool,\n        d_out: int,\n    ):\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.linear(x)\n        return x", "\n\nclass ProjectionHead(nn.Module):\n    def __init__(self, d_in: int, projection_dim=64) -> None:\n        super().__init__()\n        self.f1 = nn.Linear(d_in, d_in, bias=False)\n        self.act = nn.ReLU()\n        self.f2 = nn.Linear(d_in, projection_dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.f1(x)\n        x = self.act(x)\n        x = self.f2(x)\n        return x", "\n\nclass FTTransformer(rtdl.FTTransformer):\n    def __init__(\n        self,\n        num_features,\n        feature_tokenizer: FeatureTokenizer,\n        transformer: Transformer,\n        head: Head,\n    ) -> None:\n        super().__init__(feature_tokenizer, transformer)\n        self.pos_embedding = nn.Parameter(Tensor(num_features, feature_tokenizer.d_token))\n        self.initialization_ = _TokenInitialization.from_str(\"uniform\")\n        self.initialization_.apply(self.pos_embedding, feature_tokenizer.d_token)\n\n        if transformer.prenormalization:\n            self.normalization = nn.LayerNorm(feature_tokenizer.d_token)\n        else:\n            self.normalization = nn.Identity()\n        self.activation = nn.ReLU()\n        self.head = head\n\n    def optimization_param_groups(self) -> List[Dict[str, Any]]:\n        \"\"\"The replacement for :code:`.parameters()` when creating optimizers.\n\n        Example::\n\n            optimizer = AdamW(\n                model.optimization_param_groups(), lr=1e-4, weight_decay=1e-5\n            )\n        \"\"\"\n        no_wd_names = [\"feature_tokenizer\", \"normalization\", \".bias\", \"pos_embedding\"]\n        assert isinstance(getattr(self, no_wd_names[0], None), FeatureTokenizer), _INTERNAL_ERROR_MESSAGE\n        assert (\n            sum(1 for name, _ in self.named_modules() if no_wd_names[1] in name)\n            == len(self.transformer.blocks) * 2\n            - int(\"attention_normalization\" not in self.transformer.blocks[0])  # type: ignore\n            + 1\n        ), _INTERNAL_ERROR_MESSAGE\n\n        def needs_wd(name):\n            return all(x not in name for x in no_wd_names)\n\n        return [\n            {\"params\": [v for k, v in self.named_parameters() if needs_wd(k)]},\n            {\n                \"params\": [v for k, v in self.named_parameters() if not needs_wd(k)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n    def make_default_optimizer(self, lr=1e-4) -> torch.optim.AdamW:\n        \"\"\"Make the optimizer for the default FT-Transformer.\"\"\"\n        return torch.optim.AdamW(\n            self.optimization_param_groups(),\n            lr=lr,\n            weight_decay=1e-5,\n        )\n\n    @classmethod\n    def make_default(\n        cls: Type[\"FTTransformer\"],\n        *,\n        num_features: int,\n        n_num_features: int,\n        cat_cardinalities: Optional[List[int]],\n        n_blocks: int = 3,\n        last_layer_query_idx: Union[None, List[int], slice] = None,\n        kv_compression_ratio: Optional[float] = None,\n        kv_compression_sharing: Optional[str] = None,\n        d_out: int,\n    ) -> \"FTTransformer\":\n        transformer_config = cls.get_default_transformer_config(n_blocks=n_blocks)\n        for arg_name in [\n            \"last_layer_query_idx\",\n            \"kv_compression_ratio\",\n            \"kv_compression_sharing\",\n            \"d_out\",\n        ]:\n            transformer_config[arg_name] = locals()[arg_name]\n        return cls._make(num_features, n_num_features, cat_cardinalities, transformer_config)\n\n    @classmethod\n    def _make(\n        cls,\n        num_features,\n        n_num_features,\n        cat_cardinalities,\n        transformer_config,\n    ):\n        feature_tokenizer = FeatureTokenizer(\n            n_num_features=n_num_features,\n            cat_cardinalities=cat_cardinalities,\n            d_token=transformer_config[\"d_token\"],\n            bias=False,\n        )\n        if transformer_config[\"d_out\"] is None:\n            transformer_config[\"head_activation\"] = None\n        if transformer_config[\"kv_compression_ratio\"] is not None:\n            transformer_config[\"n_tokens\"] = feature_tokenizer.n_tokens + 2\n\n        head = Head(\n            d_in=transformer_config[\"d_token\"],\n            d_out=transformer_config[\"d_out\"],\n            bias=True,\n        )\n        del (\n            transformer_config[\"d_out\"],\n            transformer_config[\"head_activation\"],\n            transformer_config[\"head_normalization\"],\n        )\n        return cls(\n            num_features,\n            feature_tokenizer,\n            transformer=Transformer(**transformer_config),\n            head=head,\n        )\n\n    def forward(self, x_num: Optional[Tensor], x_cat: Optional[Tensor]) -> Tensor:\n        x = self.feature_tokenizer(x_num, x_cat)\n        x = x + self.pos_embedding\n        x = self.cls_token(x)\n        x = self.transformer(x)\n        x = x[:, -1]\n        x = self.normalization(x)\n        x = self.activation(x)\n        return self.head(x)", ""]}
{"filename": "script/self_sl/run_all.py", "chunked_list": ["import argparse\nimport subprocess\nfrom pathlib import Path\n\n\ndef main(args):\n    script_path = Path(__file__).parent / \"all.sh\"\n    if \",\" in args.data:\n        for data in args.data.split(\",\"):\n            cmd = [\n                str(script_path),\n                data,\n                args.train_size,\n                args.max_seed,\n            ]\n            subprocess.run(cmd)\n    else:\n        cmd = [\n            str(script_path),\n            args.data,\n            args.train_size,\n            args.max_seed,\n        ]\n        subprocess.run(cmd)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"data\", type=str)\n    parser.add_argument(\"train_size\", type=str)\n    parser.add_argument(\"-m\", \"--max-seed\", type=str, default=\"10\")\n\n    args = parser.parse_args()\n    main(args)", ""]}
{"filename": "script/sl/run_all.py", "chunked_list": ["import argparse\nimport subprocess\nfrom pathlib import Path\n\n\ndef main(args):\n    script_path = Path(__file__).parent / \"all.sh\"\n    if \",\" in args.data:\n        for data in args.data.split(\",\"):\n            cmd = [\n                str(script_path),\n                data,\n                args.train_size,\n                args.max_seed,\n            ]\n            subprocess.run(cmd)\n    else:\n        cmd = [\n            str(script_path),\n            args.data,\n            args.train_size,\n            args.max_seed,\n        ]\n        subprocess.run(cmd)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"data\", type=str)\n    parser.add_argument(\"train_size\", type=str)\n    parser.add_argument(\"-m\", \"--max-seed\", type=str, default=\"10\")\n\n    args = parser.parse_args()\n    main(args)", ""]}
{"filename": "trainer/base.py", "chunked_list": ["import logging\nimport os\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom timm.scheduler.scheduler import Scheduler\nfrom torch import Tensor\nfrom torch.cuda.amp import GradScaler\nfrom torch.nn.modules.loss import _Loss", "from torch.cuda.amp import GradScaler\nfrom torch.nn.modules.loss import _Loss\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\nfrom data import TabularDatamodule\nfrom model.core.fttrans import FTTransformer\n", "from model.core.fttrans import FTTransformer\n\nfrom .utils import EarlyStopping, auto_batch_size, save_json\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTrainer:\n    def __init__(\n        self,\n        datamodule: TabularDatamodule,\n        batch_size: Union[int, str],\n        eval_batch_size: int,\n        model: FTTransformer,\n        optimizer: Optimizer,\n        criterion: _Loss,\n        epochs: int,\n        device: torch._C.device,\n        patience: int = 16,\n        eval_metric: str = \"val/loss\",\n        eval_less_is_better: bool = True,\n        scheduler: Scheduler = None,\n        mixed_fp16: bool = False,\n        tensorbord_dir: str = \"./supervised\",\n        save_model: bool = False,\n    ) -> None:\n        \"\"\"\n        Args:\n            datamodule (TabularDatamodule): providing the dataset and dataloaders\n            batch_size (Union[int, str]): the batch size used for training\n            eval_batch_size (int): the batch size used for evaluation\n            model (FTTransformer): the FTTransformer model used for training\n            optimizer (Optimizer): used for updating the model parameters during training\n            criterion (_Loss): the loss function\n            epochs (int): the total number of training epochs\n            device (torch._C.device): the device to be used for computations (e.g., \"cpu\", \"cuda\")\n            patience (int): the number of epochs to wait for improvement\n            eval_metric (str): the evaluation metric\n            eval_less_is_better (bool): the flag representing whether the lower value indicates better performance\n            scheduler (Scheduler): used for adjusting the learning rate during training\n            mixed_fp16 (bool): whether to use mixed precision training with FP16\n            tensorbord_dir (str): the directory path to save TensorBoard logs\n            save_model (bool): whether to save the best model during training\n        \"\"\"\n        self.datamodule = datamodule\n\n        if batch_size == \"auto\":\n            batch_size = auto_batch_size(len(datamodule.train))\n            logger.info(f\"Use auto batch size; choose {batch_size}\")\n\n        self.batch_size = batch_size\n        self.eval_batch_size = eval_batch_size\n\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n        self.epochs = epochs\n        self.eval_metric = eval_metric\n        self.eval_less_is_better = eval_less_is_better\n\n        self.early_stopping = EarlyStopping(patience)\n\n        self.scheduler = scheduler\n        if mixed_fp16:\n            self.scaler = GradScaler()\n        else:\n            self.scaler = None\n\n        self.save_model = save_model\n\n        self.device = device\n\n        os.makedirs(tensorbord_dir, exist_ok=True)\n        self.writer = SummaryWriter(tensorbord_dir)\n\n    def apply_device(self, data: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"\n        Args:\n            data (tensor): the input of shape (b, n)\n\n        Returns:\n            cont (tensor): the output of shape (b, num_cont) representing the continuous values\n            cate (tensor): the output of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        cont = data[\"continuous\"]\n        cate = data[\"categorical\"]\n        if \"target\" in data:\n            target = data[\"target\"].to(self.device)\n        else:\n            target = None\n\n        if self.datamodule.task == \"multiclass\" and target is not None:\n            target = target.squeeze(1)\n\n        if cont != []:\n            cont = cont.to(self.device)\n        else:\n            cont = None\n        if cate != []:\n            cate = cate.to(self.device)\n        else:\n            cate = None\n        return cont, cate, target\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None):\n        raise NotImplementedError()\n\n    def train_dataloader(self):\n        return self.datamodule.dataloader(\"train\", batch_size=self.batch_size)\n\n    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int):\n        raise NotImplementedError()\n\n    @torch.no_grad()\n    def eval(self, mode: str = \"test\"):\n        raise NotImplementedError()\n\n    def train(self) -> None:\n        if self.eval_less_is_better:\n            best_score = np.inf\n        else:\n            best_score = -np.inf\n\n        dataloader = self.train_dataloader()\n        for epoch in range(1, self.epochs + 1):\n            with tqdm(total=len(dataloader), bar_format=\"{l_bar}{bar}{r_bar}{bar:-10b}\") as pbar_epoch:\n                scores = self.train_per_epoch(dataloader, pbar_epoch, epoch)\n                scores.update(self.eval(\"val\"))\n                pbar_epoch.set_postfix(scores)\n                for tag, score in scores.items():\n                    self.writer.add_scalar(tag, score, epoch)\n\n            if self.eval_less_is_better:\n                self.early_stopping(scores[self.eval_metric], self.model)\n                if best_score > scores[self.eval_metric]:\n                    best_score = scores[self.eval_metric]\n            else:\n                self.early_stopping(-scores[self.eval_metric], self.model)\n                if best_score < scores[self.eval_metric]:\n                    best_score = scores[self.eval_metric]\n            if self.early_stopping.early_stop:\n                logger.info(f\"early stopping {epoch} / {self.epochs}\")\n                break\n        self.model.load_state_dict(self.early_stopping.best_model)\n\n    def print_evaluate(self) -> None:\n        scores = self.eval()\n        if self.save_model:\n            torch.save(self.early_stopping.best_model, \"./best_model\")\n        save_json(scores, \"./\")\n        for key, score in scores.items():\n            logger.info(f\"{key}: {score}\")", ""]}
{"filename": "trainer/__init__.py", "chunked_list": ["from .self_supervised import (\n    FTTransHiddenMixSSLTrainer,\n    FTTransMaskTokenSSLTrainer,\n    FTTransMixupSSLTrainer,\n    FTTransSCARFSSLTrainer,\n)\nfrom .supervised import (\n    FTTransTraniner,\n    FTTransWithCutmixTraniner,\n    FTTransWithHiddenMixTraniner,", "    FTTransWithCutmixTraniner,\n    FTTransWithHiddenMixTraniner,\n    FTTransWithMaskTokenTraniner,\n    FTTransWithMixupTraniner,\n    FTTransWithSCARFTraniner,\n)\n"]}
{"filename": "trainer/utils.py", "chunked_list": ["import json\nimport os\nimport random\nfrom copy import deepcopy\nfrom typing import Dict, Union\n\nimport numpy as np\nimport torch\n\n\ndef fix_seed(seed=42):\n    # Python random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms = True", "\n\ndef fix_seed(seed=42):\n    # Python random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms = True", "\n\ndef auto_batch_size(train_size: int):\n    if train_size > 50000:\n        return 1024\n    elif 50000 >= train_size > 10000:\n        return 512\n    elif 10000 >= train_size > 5000:\n        return 256\n    elif 5000 >= train_size > 1000:\n        return 128\n    else:\n        return 64", "\n\ndef save_json(data: Dict[str, Union[int, float, str]], save_dir: str):\n    with open(os.path.join(save_dir, \"results.json\"), mode=\"wt\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=2)\n\n\ndef load_json(path) -> Dict[str, Union[int, float, str]]:\n    with open(path, mode=\"rt\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data", "\n\n# Copied from https://github.com/Bjarten/early-stopping-pytorch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\n    def __init__(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.best_model = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta and self.patience is None:\n            return\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n            )\n        # torch.save(model.state_dict(), self.path)\n        self.best_model = deepcopy(model.state_dict())\n        self.val_loss_min = val_loss\n\n    def reset(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func", ""]}
{"filename": "trainer/supervised/fttrans_w_hidden_mix.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom .base_da import BaseDATrainer\n", "from .base_da import BaseDATrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransWithHiddenMixTraniner(BaseDATrainer):\n    def __init__(self, alpha: float = 0.5, label_mix: bool = True, **kwargs) -> None:\n        \"\"\"\n        Args:\n            alpha (float): the parameter used in the hidden-mix augmentation\n            label_mix (bool): whether to apply label mixing\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"Set hidden mix alpha to {alpha}.\")\n        self.alpha = alpha\n        self.label_mix = label_mix\n\n    def get_lambda(self) -> float:\n        \"\"\"Return lambda\"\"\"\n        if self.alpha > 0.0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1.0\n        torch.tensor([lam]).float().to(self.device)\n        return lam\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        lam = self.get_lambda()\n        if self.datamodule.d_out > 1:\n            target = F.one_hot(target, self.datamodule.d_out)\n        if self.label_mix:\n            out, target = self.model(cont, cate, target, lam)\n        else:\n            lam = max(lam, 1 - lam)\n            out, _ = self.model(cont, cate, target, lam)\n            if self.datamodule.d_out > 1:\n                target = torch.argmax(target, dim=1)\n        return out, target", ""]}
{"filename": "trainer/supervised/fttrans_w_scarf.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom .base_da import BaseDATrainer\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass SCARFDA:\n    def __init__(self, df, cont_cols: list, cate_cols: list, mask_ratio: float, device: str) -> None:\n        \"\"\"\n        Args:\n            df (DataFrame): the original dataset\n            cont_cols (List): the column names of continuous values\n            cate_cals (List): the column names of categorica values\n            mask_ratio (float): the ratio of data points to be masked\n            device (str): the device to be used for computations (e.g., \"cpu\", \"cuda\")\n        \"\"\"\n        self.cont = torch.tensor(df[cont_cols].values, device=device)\n        self.cate = torch.tensor(df[cate_cols].values, device=device)\n        self.device = device\n        self.mask_ratio = mask_ratio\n\n    def scarf_augument(self, x: Tensor, all_data: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n            all_data (tensor): the entire dataset\n\n        Returns:\n            x_tilde (tensor): the output of shape (b, n) representing the masked data\n        \"\"\"\n        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n        mask = mask.to(torch.float).to(self.device)\n        batch_size = x.shape[0]\n        no, dim = all_data.shape\n        x_bar = torch.zeros([batch_size, dim]).to(self.device)\n        for i in range(dim):\n            idx = torch.randint(0, no, (batch_size,))\n            x_bar[:, i] = all_data[idx, i]\n        x_tilde = x * (1 - mask) + x_bar * mask\n        return x_tilde\n\n    def __call__(self, x: Tensor, mode: str) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n            mode (str): the mode of data should be either \"cont\" or \"cate\"\n\n        Returns:\n            tensor: the output of shape (b, n) representing the new data after SCARF augmentation\n        \"\"\"\n        if mode == \"cont\":\n            data = self.cont\n        elif mode == \"cate\":\n            data = self.cate\n        else:\n            raise ValueError(f\"unexpected values: {mode}\")\n\n        return self.scarf_augument(x, data)", "\n\nclass FTTransWithSCARFTraniner(BaseDATrainer):\n    def __init__(\n        self,\n        da_mode: str = \"scarf\",\n        mask_ratio: float = 0.2,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            da_mode (str): the data augmentation mode\n            mask_ratio (float): the ratio of data points to be masked\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"DA mode is {da_mode}.\")\n\n        self.mask_ratio = mask_ratio\n        self.da_mode = da_mode\n\n        self.scarf_da_train = SCARFDA(\n            self.datamodule.train,\n            self.datamodule.continuous_columns,\n            self.datamodule.categorical_columns,\n            mask_ratio,\n            self.device,\n        )\n        self.scarf_da_val = SCARFDA(\n            self.datamodule.val,\n            self.datamodule.continuous_columns,\n            self.datamodule.categorical_columns,\n            mask_ratio,\n            self.device,\n        )\n\n    def vime_augument(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n        Returns:\n            x_tilde (tensor): the output of shape (b, n) representing the new data after VIME augmentation\n        \"\"\"\n        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n        mask = mask.to(torch.float).to(self.device)\n\n        no, dim = x.shape\n        x_bar = torch.zeros([no, dim]).to(self.device)\n        for i in range(dim):\n            idx = torch.randperm(no)\n            x_bar[:, i] = x[idx, i]\n        x_tilde = x * (1 - mask) + x_bar * mask\n        return x_tilde\n\n    def apply_data_augmentation(\n        self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n        Returns:\n            cont (tensor): the output of shape (b, num_cont) representing the new data after SCARF augmentation\n            cate (tensor): the output of shape (b, num_cate) representing the new data after SCARF augmentation\n        \"\"\"\n        if self.da_mode == \"vime\":\n            if cont is not None:\n                cont = self.vime_augument(cont).to(torch.float)\n            if cate is not None:\n                cate = self.vime_augument(cate).to(torch.long)\n        elif self.da_mode == \"scarf\":\n            if self.model.train:\n                scarf_da = self.scarf_da_train\n            else:\n                scarf_da = self.scarf_da_val\n\n            if cont is not None:\n                cont = scarf_da(cont, \"cont\").to(torch.float)\n            if cate is not None:\n                cate = scarf_da(cate, \"cate\").to(torch.long)\n        return cont, cate\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        cont, cate = self.apply_data_augmentation(cont, cate)\n        out = self.model(cont, cate)\n        return out, target", ""]}
{"filename": "trainer/supervised/base.py", "chunked_list": ["import logging\nfrom statistics import mean\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\nfrom torch import Tensor\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import DataLoader", "from torch.cuda.amp import autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom ..base import BaseTrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSupervisedTrainer(BaseTrainer):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def forward(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n            loss (float): the model's supervised loss\n        \"\"\"\n        raise NotImplementedError()\n\n    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int) -> dict:\n        self.model.train()\n        all_loss = []\n        if self.scheduler is not None:\n            self.scheduler.step()\n        for batch in dataloader:\n            pbar_epoch.update(1)\n            self.optimizer.zero_grad()\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, target = self.apply_device(batch)\n                _, loss = self.forward(cont, cate, target)\n\n            if self.scaler is not None:\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                self.optimizer.step()\n\n            all_loss.append(loss.item())\n            scores = {\"train/loss\": mean(all_loss)}\n\n            pbar_epoch.set_description(f\"epoch[{epoch} / {self.epochs}]\")\n            pbar_epoch.set_postfix(scores)\n        return scores\n\n    @torch.no_grad()\n    def eval(self, mode: str = \"test\"):\n        self.model.eval()\n        all_target = []\n        all_pred = []\n        all_loss = []\n        for batch in self.datamodule.dataloader(mode, self.eval_batch_size):\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, target = self.apply_device(batch)\n                out = self.model(cont, cate)\n                loss = self.criterion(out, target)\n\n            all_target.append(target.cpu())\n            all_pred.append(out.cpu())\n            all_loss.append(loss.item())\n\n        all_target = torch.cat(all_target, dim=0)\n        all_pred = torch.cat(all_pred, dim=0)\n        mean_loss = mean(all_loss)\n\n        score = {f\"{mode}/loss\": mean_loss}\n        if self.datamodule.task == \"binary\":\n            label = (all_pred.numpy() > 0.5).astype(np.int)\n            all_pred = torch.sigmoid(all_pred.float()).numpy()\n            score.update(\n                {\n                    f\"{mode}/acc\": accuracy_score(all_target, label),\n                    f\"{mode}/auc\": roc_auc_score(all_target, all_pred),\n                }\n            )\n        elif self.datamodule.task == \"multiclass\":\n            label = all_pred.argmax(1).numpy()\n            score.update({f\"{mode}/acc\": accuracy_score(all_target, label)})\n        else:\n            assert self.datamodule.task == \"regression\"\n            score.update(\n                {f\"{mode}/rmse\": mean_squared_error(all_target, all_pred.numpy()) ** 0.5 * self.datamodule.y_std}\n            )\n        return score", "\nclass BaseSupervisedTrainer(BaseTrainer):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def forward(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n            loss (float): the model's supervised loss\n        \"\"\"\n        raise NotImplementedError()\n\n    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int) -> dict:\n        self.model.train()\n        all_loss = []\n        if self.scheduler is not None:\n            self.scheduler.step()\n        for batch in dataloader:\n            pbar_epoch.update(1)\n            self.optimizer.zero_grad()\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, target = self.apply_device(batch)\n                _, loss = self.forward(cont, cate, target)\n\n            if self.scaler is not None:\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                self.optimizer.step()\n\n            all_loss.append(loss.item())\n            scores = {\"train/loss\": mean(all_loss)}\n\n            pbar_epoch.set_description(f\"epoch[{epoch} / {self.epochs}]\")\n            pbar_epoch.set_postfix(scores)\n        return scores\n\n    @torch.no_grad()\n    def eval(self, mode: str = \"test\"):\n        self.model.eval()\n        all_target = []\n        all_pred = []\n        all_loss = []\n        for batch in self.datamodule.dataloader(mode, self.eval_batch_size):\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, target = self.apply_device(batch)\n                out = self.model(cont, cate)\n                loss = self.criterion(out, target)\n\n            all_target.append(target.cpu())\n            all_pred.append(out.cpu())\n            all_loss.append(loss.item())\n\n        all_target = torch.cat(all_target, dim=0)\n        all_pred = torch.cat(all_pred, dim=0)\n        mean_loss = mean(all_loss)\n\n        score = {f\"{mode}/loss\": mean_loss}\n        if self.datamodule.task == \"binary\":\n            label = (all_pred.numpy() > 0.5).astype(np.int)\n            all_pred = torch.sigmoid(all_pred.float()).numpy()\n            score.update(\n                {\n                    f\"{mode}/acc\": accuracy_score(all_target, label),\n                    f\"{mode}/auc\": roc_auc_score(all_target, all_pred),\n                }\n            )\n        elif self.datamodule.task == \"multiclass\":\n            label = all_pred.argmax(1).numpy()\n            score.update({f\"{mode}/acc\": accuracy_score(all_target, label)})\n        else:\n            assert self.datamodule.task == \"regression\"\n            score.update(\n                {f\"{mode}/rmse\": mean_squared_error(all_target, all_pred.numpy()) ** 0.5 * self.datamodule.y_std}\n            )\n        return score", ""]}
{"filename": "trainer/supervised/fttrans_w_mixup.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom .base_da import BaseDATrainer\n", "from .base_da import BaseDATrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransWithMixupTraniner(BaseDATrainer):\n    def __init__(self, alpha: float = 0.1, **kwargs) -> None:\n        \"\"\"\n        Args:\n            alpha (float): the parameter used in the mixup augmentation\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"Set mixup alpha to {alpha}.\")\n        self.alpha = alpha\n\n    def get_lambda(self) -> float:\n        \"\"\"Return lambda\"\"\"\n        if self.alpha > 0.0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 0.0\n        torch.tensor([lam]).float().to(self.device)\n        return lam\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        lam = self.get_lambda()\n        if self.datamodule.d_out > 1:\n            target = F.one_hot(target, self.datamodule.d_out)\n        out, target = self.model(cont, cate, target, lam)\n        return out, target", ""]}
{"filename": "trainer/supervised/fttrans_w_mask_token.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nfrom torch import Tensor\n\nfrom .base_da import BaseDATrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransWithMaskTokenTraniner(BaseDATrainer):\n    def __init__(\n        self,\n        mask_ratio: float = 0.1,\n        bias_after_mask: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.mask_ratio = mask_ratio\n        self.bias_after_mask = bias_after_mask\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        out = self.model(\n            cont,\n            cate,\n            mask_ratio=self.mask_ratio,\n            bias_after_mask=self.bias_after_mask,\n        )\n        return out, target", "\n\nclass FTTransWithMaskTokenTraniner(BaseDATrainer):\n    def __init__(\n        self,\n        mask_ratio: float = 0.1,\n        bias_after_mask: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.mask_ratio = mask_ratio\n        self.bias_after_mask = bias_after_mask\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        out = self.model(\n            cont,\n            cate,\n            mask_ratio=self.mask_ratio,\n            bias_after_mask=self.bias_after_mask,\n        )\n        return out, target", ""]}
{"filename": "trainer/supervised/base_da.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nfrom torch import Tensor\n\nfrom .base import BaseSupervisedTrainer\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass BaseDATrainer(BaseSupervisedTrainer):\n    def __init__(self, p: float = 0.5, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.p = p\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        raise NotImplementedError()\n\n    def forward(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n            loss (float): the model's supervised loss\n        \"\"\"\n        if self.p > np.random.rand():\n            out, target = self.forward_w_da(cont, cate, target)\n        else:\n            out = self.model(cont, cate)\n\n        if target is not None:\n            loss = self.criterion(out, target)\n            return out, loss\n        else:\n            return out", ""]}
{"filename": "trainer/supervised/__init__.py", "chunked_list": ["from .fttrans import FTTransTraniner\nfrom .fttrans_w_cutmix import FTTransWithCutmixTraniner\nfrom .fttrans_w_hidden_mix import FTTransWithHiddenMixTraniner\nfrom .fttrans_w_mask_token import FTTransWithMaskTokenTraniner\nfrom .fttrans_w_mixup import FTTransWithMixupTraniner\nfrom .fttrans_w_scarf import FTTransWithSCARFTraniner\n"]}
{"filename": "trainer/supervised/fttrans.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nfrom torch import Tensor\n\nfrom .base import BaseSupervisedTrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransTraniner(BaseSupervisedTrainer):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def forward(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n            loss (tensor): the model's loss\n        \"\"\"\n        out = self.model(cont, cate)\n        if target is not None:\n            loss = self.criterion(out, target)\n            return out, loss\n        else:\n            return out", "\n\nclass FTTransTraniner(BaseSupervisedTrainer):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def forward(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n            loss (tensor): the model's loss\n        \"\"\"\n        out = self.model(cont, cate)\n        if target is not None:\n            loss = self.criterion(out, target)\n            return out, loss\n        else:\n            return out", ""]}
{"filename": "trainer/supervised/fttrans_w_cutmix.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom .base_da import BaseDATrainer\n", "from .base_da import BaseDATrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransWithCutmixTraniner(BaseDATrainer):\n    def __init__(\n        self,\n        alpha: float = 0.1,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            alpha (float): the parameter used in the cutmix augmentation\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"Set mixup alpha to {alpha}.\")\n        self.alpha = alpha\n\n    def get_lambda(self) -> float:\n        \"\"\"Return lambda\"\"\"\n        if self.alpha > 0.0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1.0\n        torch.tensor([lam]).float().to(self.device)\n        return lam\n\n    def mask_generator(self, x: Tensor, lam: float) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n            lam (float): The ratio to mask\n\n        Returns:\n            mask (tensor): the binary mask of shape (b, n)\n        \"\"\"\n        b, n = x.shape\n        ids_noise = torch.rand(b, n, device=x.device)\n        ids_shuffle = torch.argsort(ids_noise, dim=1)\n        len_keep = int(n * lam)\n        ids_keep = ids_shuffle[:, :len_keep]\n        mask = torch.ones(b, n, device=x.device)\n        mask[torch.arange(b)[:, None], ids_keep] = mask[torch.arange(b)[:, None], ids_keep] * 0.0\n        return mask\n\n    def cutmix(self, x: Tensor, target: Tensor, lam: float) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n            lam (float): The ratio to mask\n\n        Returns:\n            new_x (tensor): the output of shape (b, n) representing the new data after cutmix augmentation\n            label (tensor): the mixed target labels of shape (b, num_classes)\n        \"\"\"\n        indices = torch.randperm(x.shape[0])\n        mask = self.mask_generator(x, lam)\n        new_x = x * (1 - mask) + x[indices] * mask\n        new_mask = (x == new_x).to(torch.float)\n        new_lam = new_mask.mean(1).unsqueeze(1)\n        label = target * new_lam + target[indices] * (1 - new_lam)\n        return new_x, label\n\n    def concat_data(self, cont: Optional[Tensor], cate: Optional[Tensor]) -> Tensor:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, n) representing the continuous values\n            cate (tensor): the input of shape (b, n) representing the categorical values\n\n        Returns:\n            x (tensor): A output of shape (b, n) representing the concatenated input values\n        \"\"\"\n        if cont is not None and cate is not None:\n            x = torch.cat([cont, cate], dim=1)\n        elif cate is None:\n            x = cont\n        else:\n            x = cate\n        return x\n\n    def cutmix_process(\n        self, cont: Optional[Tensor], cate: Optional[Tensor], target: Tensor\n    ) -> Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (Tensor): The input tensor of shape (b, num_cont) representing the continuous values.\n            cate (Tensor): The input tensor of shape (b, num_cate) representing the categorical values.\n            target (Tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            cont (Tensor): the augmented continuous values\n            cate (Tensor): the augmented categorical values\n            target (Tensor): the mixed target labels of shape (b, num_classes)\n        \"\"\"\n        lam = self.get_lambda()\n        if self.datamodule.d_out > 1:\n            target = F.one_hot(target, self.datamodule.d_out)\n        x = self.concat_data(cont, cate)\n        if cont is not None:\n            n_cont = cont.shape[1]\n        else:\n            n_cont = 0\n        x_new, target = self.cutmix(x, target, lam)\n        if cont is not None:\n            cont = x_new[:, :n_cont].to(torch.float)\n        if cate is not None:\n            cate = x_new[:, n_cont:].to(torch.long)\n        return cont, cate, target\n\n    def forward_w_da(\n        self,\n        cont: Optional[Tensor] = None,\n        cate: Optional[Tensor] = None,\n        target: Tensor = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n            target (tensor): the target labels of shape (b, num_classes)\n\n        Returns:\n            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n            target (tensor): the target labels of shape (b, num_classes)\n        \"\"\"\n        cont, cate, target = self.cutmix_process(cont, cate, target)\n        out = self.model(cont, cate)\n        return out, target", ""]}
{"filename": "trainer/self_supervised/fttrans_w_hidden_mix.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\nfrom .base import BaseSSLTrainer\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransHiddenMixSSLTrainer(BaseSSLTrainer):\n    def __init__(\n        self,\n        alpha: float = 0.5,\n        label_mix=None,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            alpha (float): the parameter used in the hidden-mix augmentation\n            label_mix (bool): whether to apply label mixing\n        \"\"\"\n        super().__init__(**kwargs)\n        self.alpha = alpha\n\n    def get_lambda(self) -> float:\n        \"\"\"Return lambda\"\"\"\n        if self.alpha > 0.0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1.0\n        torch.tensor([lam]).float().to(self.device)\n        return lam\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\n        Returns:\n            loss (float): the model's contrastive loss\n        \"\"\"\n        z_0 = self.model(cont, cate)\n        lam = self.get_lambda()\n        lam = max(lam, 1 - lam)\n        z_1, _ = self.model(cont, cate, lam=lam)\n        loss = self.forward_loss(z_0, z_1)\n        return None, loss", ""]}
{"filename": "trainer/self_supervised/fttrans_w_scarf.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom ..supervised.fttrans_w_scarf import SCARFDA\nfrom .base import BaseSSLTrainer\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransSCARFSSLTrainer(BaseSSLTrainer):\n    def __init__(\n        self,\n        da_mode: str = \"scarf\",\n        mask_ratio: float = 0.2,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            da_mode (str): the data augmentation mode\n            mask_ratio (float): the ratio of data points to be masked\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"DA mode is {da_mode}.\")\n\n        self.mask_ratio = mask_ratio\n        self.da_mode = da_mode\n\n        self.scarf_da_train = SCARFDA(\n            self.datamodule.train,\n            self.datamodule.continuous_columns,\n            self.datamodule.categorical_columns,\n            mask_ratio,\n            self.device,\n        )\n        self.scarf_da_val = SCARFDA(\n            self.datamodule.val,\n            self.datamodule.continuous_columns,\n            self.datamodule.categorical_columns,\n            mask_ratio,\n            self.device,\n        )\n\n    def vime_augument(self, x) -> Tensor:\n        \"\"\"\n        Args:\n            x (tensor): the input of shape (b, n)\n        Returns:\n            x_tilde (tensor): the output of shape (b, n) representing the new data after VIME augmentation\n        \"\"\"\n        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n        mask = mask.to(torch.float).to(self.device)\n\n        no, dim = x.shape\n        x_bar = torch.zeros([no, dim]).to(self.device)\n        for i in range(dim):\n            idx = torch.randperm(no)\n            x_bar[:, i] = x[idx, i]\n        x_tilde = x * (1 - mask) + x_bar * mask\n        return x_tilde\n\n    def apply_data_augmentation(\n        self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n        Returns:\n            cont (tensor): the output of shape (b, num_cont) representing the new data after SCARF augmentation\n            cate (tensor): the output of shape (b, num_cate) representing the new data after SCARF augmentation\n        \"\"\"\n        if self.da_mode == \"vime\":\n            if cont is not None:\n                cont = self.vime_augument(cont).to(torch.float)\n            if cate is not None:\n                cate = self.vime_augument(cate).to(torch.long)\n        elif self.da_mode == \"scarf\":\n            if self.model.train:\n                scarf_da = self.scarf_da_train\n            else:\n                scarf_da = self.scarf_da_val\n\n            if cont is not None:\n                cont = scarf_da(cont, \"cont\").to(torch.float)\n            if cate is not None:\n                cate = scarf_da(cate, \"cate\").to(torch.long)\n        return cont, cate\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\n        Returns:\n            loss (float): the model's contrastive loss\n        \"\"\"\n        cont_da, cate_da = self.apply_data_augmentation(cont, cate)\n        z_0 = self.model(cont, cate)\n        z_1 = self.model(cont_da, cate_da)\n        loss = self.forward_loss(z_0, z_1)\n        return None, loss", ""]}
{"filename": "trainer/self_supervised/base.py", "chunked_list": ["from statistics import mean\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom model.core.fttrans import ProjectionHead\n\nfrom ..base import BaseTrainer\n\n# Copied from https://github.com/clabrugere/pytorch-scarf/\nclass NTXent(nn.Module):\n    def __init__(self, temperature=1.0):\n        \"\"\"NT-Xent loss for contrastive learning using cosine distance as similarity metric as used in [SimCLR](https://arxiv.org/abs/2002.05709).\n        Implementation adapted from https://theaisummer.com/simclr/#simclr-loss-implementation\n        Args:\n            temperature (float, optional): scaling factor of the similarity metric. Defaults to 1.0.\n        \"\"\"\n        super().__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i: Tensor, z_j: Tensor):\n        \"\"\"Compute NT-Xent loss using only anchor and positive batches of samples. Negative samples are the 2*(N-1) samples in the batch\n        Args:\n            z_i (torch.tensor): anchor batch of samples\n            z_j (torch.tensor): positive batch of samples\n        Returns:\n            float: loss\n        \"\"\"\n        batch_size = z_i.size(0)\n\n        # compute similarity between the sample's embedding and its corrupted view\n        z = torch.cat([z_i, z_j], dim=0)\n        similarity = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n\n        sim_ij = torch.diag(similarity, batch_size)\n        sim_ji = torch.diag(similarity, -batch_size)\n        positives = torch.cat([sim_ij, sim_ji], dim=0)\n\n        mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool, device=z_i.device)).float()\n        numerator = torch.exp(positives / self.temperature)\n        denominator = mask * torch.exp(similarity / self.temperature)\n\n        all_losses = -torch.log(numerator / torch.sum(denominator, dim=1))\n        loss = torch.sum(all_losses) / (2 * batch_size)\n\n        return loss", "\n\nclass BaseSSLTrainer(BaseTrainer):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs, tensorbord_dir=\"./self_supervised\")\n        new_head = ProjectionHead(self.model.head.linear.in_features)\n        self.model.head = new_head.to(self.device)\n        self.ssl_loss = NTXent()\n\n    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int) -> dict:\n        self.model.train()\n        all_loss = []\n        if self.scheduler is not None:\n            self.scheduler.step()\n        for batch in dataloader:\n            pbar_epoch.update(1)\n            self.optimizer.zero_grad()\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, _ = self.apply_device(batch)\n                _, loss = self.forward(cont, cate)\n\n            if self.scaler is not None:\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                self.optimizer.step()\n\n            all_loss.append(loss.item())\n            scores = {\"train/self-sl-loss\": mean(all_loss)}\n            pbar_epoch.set_description(f\"epoch[{epoch} / {self.epochs}]\")\n            pbar_epoch.set_postfix(scores)\n        return scores\n\n    def forward_loss(self, z_i, z_j) -> float:\n        return self.ssl_loss(z_i, z_j)\n\n    @torch.no_grad()\n    def eval(self, mode: str = \"val\") -> dict:\n        self.model.eval()\n        all_loss = []\n        for batch in self.datamodule.dataloader(mode, self.eval_batch_size):\n            with autocast(enabled=self.scaler is not None):\n                cont, cate, _ = self.apply_device(batch)\n                _, loss = self.forward(cont, cate)\n\n            all_loss.append(loss.item())\n\n        mean_loss = mean(all_loss)\n\n        score = {f\"{mode}/self-sl-loss\": mean_loss}\n        return score", ""]}
{"filename": "trainer/self_supervised/fttrans_w_mixup.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\nfrom .base import BaseSSLTrainer\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransMixupSSLTrainer(BaseSSLTrainer):\n    def __init__(\n        self,\n        alpha: float = 0.1,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            alpha (float): the parameter used in the mixup augmentation\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"Set mixup alpha to {alpha}.\")\n        self.alpha = alpha\n\n    def get_lambda(self) -> float:\n        \"\"\"Return lambda\"\"\"\n        if self.alpha > 0.0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 0.0\n        torch.tensor([lam]).float().to(self.device)\n        return lam\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\n        Returns:\n            loss (float): the model's contrastive loss\n        \"\"\"\n        z_0 = self.model(cont, cate)\n        lam = self.get_lambda()\n        lam = min(lam, 1 - lam)\n        z_1 = self.model.forward_no_labelmix(cont, cate, alpha=lam)\n        loss = self.forward_loss(z_0, z_1)\n        return None, loss", ""]}
{"filename": "trainer/self_supervised/fttrans_w_mask_token.py", "chunked_list": ["import logging\nfrom typing import Optional, Tuple\n\nfrom torch import Tensor\n\nfrom .base import BaseSSLTrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FTTransMaskTokenSSLTrainer(BaseSSLTrainer):\n    def __init__(\n        self,\n        mask_ratio: float = 0.1,\n        bias_after_mask: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.mask_ratio = mask_ratio\n        self.bias_after_mask = bias_after_mask\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\n        Returns:\n            loss (float): the model's contrastive loss\n        \"\"\"\n        z_0 = self.model(cont, cate)\n        z_1 = self.model(\n            cont,\n            cate,\n            mask_ratio=self.mask_ratio,\n            bias_after_mask=self.bias_after_mask,\n        )\n        loss = self.forward_loss(z_0, z_1)\n        return None, loss", "\n\nclass FTTransMaskTokenSSLTrainer(BaseSSLTrainer):\n    def __init__(\n        self,\n        mask_ratio: float = 0.1,\n        bias_after_mask: bool = True,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Args:\n            mask_ratio (float): the ratio of data points to be masked\n            bias_after_mask (bool): whether to add the positional embedding before or after masking\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.mask_ratio = mask_ratio\n        self.bias_after_mask = bias_after_mask\n\n    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n        \"\"\"\n        Args:\n            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\n        Returns:\n            loss (float): the model's contrastive loss\n        \"\"\"\n        z_0 = self.model(cont, cate)\n        z_1 = self.model(\n            cont,\n            cate,\n            mask_ratio=self.mask_ratio,\n            bias_after_mask=self.bias_after_mask,\n        )\n        loss = self.forward_loss(z_0, z_1)\n        return None, loss", ""]}
{"filename": "trainer/self_supervised/__init__.py", "chunked_list": ["from .fttrans_w_hidden_mix import FTTransHiddenMixSSLTrainer\nfrom .fttrans_w_mask_token import FTTransMaskTokenSSLTrainer\nfrom .fttrans_w_mixup import FTTransMixupSSLTrainer\nfrom .fttrans_w_scarf import FTTransSCARFSSLTrainer\n"]}
