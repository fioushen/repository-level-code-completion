{"filename": "utils/distributed.py", "chunked_list": ["import os\nimport torch\nimport torch.distributed as dist\nimport logging\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_for_distributed(is_master):\n    import warnings\n\n    builtin_warn = warnings.warn\n\n    def warn(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_warn(*args, **kwargs)\n\n    # Log warnings only once\n    warnings.warn = warn\n    warnings.simplefilter(\"once\", UserWarning)\n\n    if not is_master:\n        logging.disable()", "\ndef setup_for_distributed(is_master):\n    import warnings\n\n    builtin_warn = warnings.warn\n\n    def warn(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_warn(*args, **kwargs)\n\n    # Log warnings only once\n    warnings.warn = warn\n    warnings.simplefilter(\"once\", UserWarning)\n\n    if not is_master:\n        logging.disable()", "\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()", "\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()", "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)", "\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef is_port_in_use(port):\n    import socket\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex(('localhost', port)) == 0", "\n\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        # job started by torch.distributed.launch\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        # local rank on the current node / global rank\n        local_rank = int(os.environ['SLURM_LOCALID'])\n        global_rank = int(os.environ['SLURM_PROCID'])\n        # number of processes / GPUs per node\n        world_size = int(os.environ[\"SLURM_NNODES\"]) * \\\n            int(os.environ[\"SLURM_TASKS_PER_NODE\"][0])\n\n        args.rank = global_rank\n        args.gpu = local_rank\n        args.world_size = world_size\n    else:\n        logger.info('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n\n    if \"tcp\" in args.dist_url:  # in slurm, multiple program runs in a single node\n        dist_port = int(args.dist_url.split(\":\")[-1])\n        # while is_port_in_use(dist_port):\n        #     dist_port += 10\n        args.dist_url = \":\".join(args.dist_url.split(\":\")[:-1] + [str(dist_port)])\n\n    logger.info('| distributed init (rank {}): {}'.format(\n        args.rank, args.dist_url))\n    if \"SLURM_JOB_ID\" in os.environ:\n        logger.info(f\"SLURM_JOB_ID {os.environ['SLURM_JOB_ID']}\")\n    torch.distributed.init_process_group(\n        backend=args.dist_backend, init_method=args.dist_url,\n        world_size=args.world_size, rank=args.rank)\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)", "\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n# copied from https://github.com/facebookresearch/vissl/blob/master/vissl/utils/distributed_gradients.py\nclass GatherLayer(torch.autograd.Function):\n    \"\"\"\n    Gather tensors from all workers with support for backward propagation:\n    This implementation does not cut the gradients as torch.distributed.all_gather does.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x):\n        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n        dist.all_gather(output, x)\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        all_gradients = torch.stack(grads)\n        dist.all_reduce(all_gradients)\n        return all_gradients[dist.get_rank()]", "\n\n# copied from megavlt\ndef gather_tensor_along_batch_with_backward(tensor, dim=0):\n    world_size = get_world_size()\n\n    if world_size < 2:\n        return tensor\n\n    tensor_list = GatherLayer.apply(tensor)\n    tensor_list = torch.cat(tensor_list, dim=dim)\n    return tensor_list", "\n\n@torch.no_grad()\ndef gather_tensor_along_batch(tensor, dim=0):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    world_size = get_world_size()\n\n    if world_size < 2:\n        return tensor\n\n    with torch.no_grad():\n        tensor_list = []\n\n        for _ in range(world_size):\n            tensor_list.append(torch.zeros_like(tensor))\n\n        dist.all_gather(tensor_list, tensor)\n        tensor_list = torch.cat(tensor_list, dim=dim)\n    return tensor_list", ""]}
{"filename": "utils/basic_utils.py", "chunked_list": ["import numpy as np\nimport io\nimport os\nimport json\nimport logging\nimport random\nimport time\nfrom collections import defaultdict, deque\nimport datetime\nfrom pathlib import Path", "import datetime\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport torch\nimport torch.distributed as dist\nfrom .distributed import is_dist_avail_and_initialized\n\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total],\n                         dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)", "\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            if meter.count == 0:  # skip empty meter\n                loss_str.append(\n                    \"{}: {}\".format(name, \"No data\")\n                )\n            else:\n                loss_str.append(\n                    \"{}: {}\".format(name, str(meter))\n                )\n        return self.delimiter.join(loss_str)\n\n    def global_avg(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            if meter.count == 0:\n                loss_str.append(\n                    \"{}: {}\".format(name, \"No data\")\n                )\n            else:\n                loss_str.append(\n                    \"{}: {:.4f}\".format(name, meter.global_avg)\n                )\n        return self.delimiter.join(loss_str)\n\n    def get_global_avg_dict(self, prefix=\"\"):\n        \"\"\"include a separator (e.g., `/`, or \"_\") at the end of `prefix`\"\"\"\n        d = {f\"{prefix}{k}\": m.global_avg if m.count > 0 else 0. for k, m in self.meters.items()}\n        return d\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, log_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        log_msg = [\n            header,\n            '[{0' + space_fmt + '}/{1}]',\n            'eta: {eta}',\n            '{meters}',\n            'time: {time}',\n            'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            log_msg.append('max mem: {memory:.0f} res mem: {res_mem:.0f}')\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % log_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    logger.info(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB,\n                        res_mem=torch.cuda.max_memory_reserved() / MB,\n                    ))\n                else:\n                    logger.info(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        logger.info('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))", "\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef compute_acc(logits, label, reduction='mean'):\n    ret = (torch.argmax(logits, dim=1) == label).float()\n    if reduction == 'none':\n        return ret.detach()\n    elif reduction == 'mean':\n        return ret.mean().item()", "def compute_acc(logits, label, reduction='mean'):\n    ret = (torch.argmax(logits, dim=1) == label).float()\n    if reduction == 'none':\n        return ret.detach()\n    elif reduction == 'mean':\n        return ret.mean().item()\n\n\ndef compute_n_params(model, return_str=True):\n    tot = 0\n    for p in model.parameters():\n        w = 1\n        for x in p.shape:\n            w *= x\n        tot += w\n    if return_str:\n        if tot >= 1e6:\n            return '{:.1f}M'.format(tot / 1e6)\n        else:\n            return '{:.1f}K'.format(tot / 1e3)\n    else:\n        return tot", "def compute_n_params(model, return_str=True):\n    tot = 0\n    for p in model.parameters():\n        w = 1\n        for x in p.shape:\n            w *= x\n        tot += w\n    if return_str:\n        if tot >= 1e6:\n            return '{:.1f}M'.format(tot / 1e6)\n        else:\n            return '{:.1f}K'.format(tot / 1e3)\n    else:\n        return tot", "\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef remove_files_if_exist(file_paths):\n    for fp in file_paths:\n        if os.path.isfile(fp):\n            os.remove(fp)", "def remove_files_if_exist(file_paths):\n    for fp in file_paths:\n        if os.path.isfile(fp):\n            os.remove(fp)\n\n\ndef save_json(data, filename, save_pretty=False, sort_keys=False):\n    with open(filename, \"w\") as f:\n        if save_pretty:\n            f.write(json.dumps(data, indent=4, sort_keys=sort_keys))\n        else:\n            json.dump(data, f)", "\n\ndef load_json(filename):\n    with open(filename, \"r\") as f:\n        return json.load(f)\n\n\ndef flat_list_of_lists(l):\n    \"\"\"flatten a list of lists [[1,2], [3,4]] to [1,2,3,4]\"\"\"\n    return [item for sublist in l for item in sublist]", "\n\ndef find_files_by_suffix_recursively(root: str, suffix: Union[str, List[str]]):\n    \"\"\"\n    Args:\n        root: path to the directory to start search files\n        suffix: any str as suffix, or can match multiple such strings\n            when input is List[str]. \n            Example 1, e.g., suffix: `.jpg` or [`.jpg`, `.png`]\n            Example 2, e.g., use a `*` in the `suffix`: `START*.jpg.`.\n    \"\"\"\n    if isinstance(suffix, str):\n        suffix = [suffix, ]\n    filepaths = flat_list_of_lists(\n        [list(Path(root).rglob(f\"*{e}\")) for e in suffix])\n    return filepaths", "\n\ndef match_key_and_shape(state_dict1, state_dict2):\n    keys1 = set(state_dict1.keys())\n    keys2 = set(state_dict2.keys())\n    print(f\"keys1 - keys2: {keys1 - keys2}\")\n    print(f\"keys2 - keys1: {keys2 - keys1}\")\n\n    mismatch = 0\n    for k in list(keys1):\n        if state_dict1[k].shape != state_dict2[k].shape:\n            print(\n                f\"k={k}, state_dict1[k].shape={state_dict1[k].shape}, state_dict2[k].shape={state_dict2[k].shape}\")\n            mismatch += 1\n    print(f\"mismatch {mismatch}\")", "\n\ndef merge_dicts(list_dicts):\n    merged_dict = list_dicts[0].copy()\n    for i in range(1, len(list_dicts)):\n        merged_dict.update(list_dicts[i])\n    return merged_dict\n"]}
{"filename": "utils/scheduler.py", "chunked_list": ["\"\"\" Scheduler Factory\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom torch.optim import Optimizer\nimport math\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef create_scheduler(args, optimizer):\n    lr_scheduler = None\n    if args.sched == 'cosine':\n        lr_scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=args.num_warmup_steps,\n            num_training_steps=args.num_training_steps,\n            num_cycles=0.5,\n            min_lr_multi=args.min_lr_multi\n        )\n    return lr_scheduler", "def create_scheduler(args, optimizer):\n    lr_scheduler = None\n    if args.sched == 'cosine':\n        lr_scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=args.num_warmup_steps,\n            num_training_steps=args.num_training_steps,\n            num_cycles=0.5,\n            min_lr_multi=args.min_lr_multi\n        )\n    return lr_scheduler", "\n\ndef get_cosine_schedule_with_warmup(\n        optimizer: Optimizer, num_warmup_steps: int, num_training_steps: int,\n        num_cycles: float = 0.5, min_lr_multi: float = 0., last_epoch: int = -1\n):\n    \"\"\"\n    Modified from https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/optimization.py\n\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_cycles (`float`, *optional*, defaults to 0.5):\n            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n            following a half-cosine).\n        min_lr_multi (`float`, *optional*, defaults to 0):\n            The minimum learning rate multiplier. Thus the minimum learning rate is base_lr * min_lr_multi.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return max(min_lr_multi, float(current_step) / float(max(1, num_warmup_steps)))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(min_lr_multi, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)", ""]}
{"filename": "utils/optimizer.py", "chunked_list": ["\"\"\" Optimizer Factory w/ Custom Weight Decay\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import optim as optim\nimport logging\nlogger = logging.getLogger(__name__)\ntry:\n    from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD\n    has_apex = True\nexcept ImportError:\n    has_apex = False", "\n\ndef add_weight_decay(model, weight_decay, no_decay_list=(), filter_bias_and_bn=True):\n    named_param_tuples = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if filter_bias_and_bn and (len(param.shape) == 1 or name.endswith(\".bias\")):\n            named_param_tuples.append([name, param, 0])\n        elif name in no_decay_list:\n            named_param_tuples.append([name, param, 0])\n        else:\n            named_param_tuples.append([name, param, weight_decay])\n    return named_param_tuples", "\n\ndef add_different_lr(named_param_tuples_or_model, diff_lr_names, diff_lr, default_lr):\n    \"\"\"use lr=diff_lr for modules named found in diff_lr_names,\n    otherwise use lr=default_lr\n\n    Args:\n        named_param_tuples_or_model: List([name, param, weight_decay]), or nn.Module\n        diff_lr_names: List(str)\n        diff_lr: float\n        default_lr: float\n    Returns:\n        named_param_tuples_with_lr: List([name, param, weight_decay, lr])\n    \"\"\"\n    named_param_tuples_with_lr = []\n    for name, p, wd in named_param_tuples_or_model:\n        use_diff_lr = False\n        for diff_name in diff_lr_names:\n            if diff_name in name:\n                use_diff_lr = True\n                break\n\n        named_param_tuples_with_lr.append(\n            [name, p, wd, diff_lr if use_diff_lr else default_lr]\n        )\n    return named_param_tuples_with_lr", "\n\ndef create_optimizer_params_group(named_param_tuples_with_lr):\n    \"\"\"named_param_tuples_with_lr: List([name, param, weight_decay, lr])\"\"\"\n    group = {}\n    for name, p, wd, lr in named_param_tuples_with_lr:\n        if wd not in group:\n            group[wd] = {}\n        if lr not in group[wd]:\n            group[wd][lr] = []\n        group[wd][lr].append(p)\n\n    optimizer_params_group = []\n    for wd, lr_groups in group.items():\n        for lr, p in lr_groups.items():\n            optimizer_params_group.append(dict(\n                params=p,\n                weight_decay=wd,\n                lr=lr\n            ))\n            logger.info(f\"optimizer -- lr={lr} wd={wd} len(p)={len(p)}\")\n    return optimizer_params_group", "\n\ndef create_optimizer(args, model, filter_bias_and_bn=True):\n    opt_lower = args.opt.lower()\n    weight_decay = args.weight_decay\n    # check for modules that requires different lr\n    if hasattr(args, \"different_lr\") and args.different_lr.enable:\n        diff_lr_module_names = args.different_lr.module_names\n        diff_lr = args.different_lr.lr\n    else:\n        diff_lr_module_names = []\n        diff_lr = None\n\n    no_decay = {}\n    if hasattr(model, 'no_weight_decay'):\n        no_decay = model.no_weight_decay()\n    named_param_tuples = add_weight_decay(\n        model, weight_decay, no_decay, filter_bias_and_bn)\n    named_param_tuples = add_different_lr(\n        named_param_tuples, diff_lr_module_names, diff_lr, args.lr)\n    parameters = create_optimizer_params_group(named_param_tuples)\n\n    if 'fused' in opt_lower:\n        assert has_apex and torch.cuda.is_available(), 'APEX and CUDA required for fused optimizers'\n\n    opt_args = dict(lr=args.lr, weight_decay=weight_decay)\n    if hasattr(args, 'opt_eps') and args.opt_eps is not None:\n        opt_args['eps'] = args.opt_eps\n    if hasattr(args, 'opt_betas') and args.opt_betas is not None:\n        opt_args['betas'] = args.opt_betas\n    if hasattr(args, 'opt_args') and args.opt_args is not None:\n        opt_args.update(args.opt_args)\n\n    opt_split = opt_lower.split('_')\n    opt_lower = opt_split[-1]\n    if opt_lower == 'sgd' or opt_lower == 'nesterov':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'momentum':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'adam':\n        optimizer = optim.Adam(parameters, **opt_args)\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, **opt_args)\n    else:\n        assert False and \"Invalid optimizer\"\n        raise ValueError\n    return optimizer", ""]}
{"filename": "utils/logger.py", "chunked_list": ["# from MMF: https://github.com/facebookresearch/mmf/blob/master/mmf/utils/logger.py\n# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport functools\nimport logging\nimport os\nimport sys\nimport time\nimport wandb\nfrom typing import Any, Dict, Union", "import wandb\nfrom typing import Any, Dict, Union\n\nimport torch\nfrom .distributed import get_rank, is_main_process\nfrom termcolor import colored\n\n\ndef log_dict_to_wandb(log_dict, step, prefix=\"\"):\n    \"\"\"include a separator `/` at the end of `prefix`\"\"\"\n    if not is_main_process():\n        return\n\n    log_dict = {f\"{prefix}{k}\": v for k, v in log_dict.items()}\n    wandb.log(log_dict, step)", "def log_dict_to_wandb(log_dict, step, prefix=\"\"):\n    \"\"\"include a separator `/` at the end of `prefix`\"\"\"\n    if not is_main_process():\n        return\n\n    log_dict = {f\"{prefix}{k}\": v for k, v in log_dict.items()}\n    wandb.log(log_dict, step)\n\n\ndef setup_wandb(config):\n    if not (config.wandb.enable and is_main_process()):\n        return\n\n    run = wandb.init(\n        config=config,\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        name=os.path.basename(config.output_dir),\n        reinit=True\n    )\n    return run", "\ndef setup_wandb(config):\n    if not (config.wandb.enable and is_main_process()):\n        return\n\n    run = wandb.init(\n        config=config,\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        name=os.path.basename(config.output_dir),\n        reinit=True\n    )\n    return run", "\n\ndef setup_output_folder(save_dir: str, folder_only: bool = False):\n    \"\"\"Sets up and returns the output file where the logs will be placed\n    based on the configuration passed. Usually \"save_dir/logs/log_<timestamp>.txt\".\n    If env.log_dir is passed, logs will be directly saved in this folder.\n    Args:\n        folder_only (bool, optional): If folder should be returned and not the file.\n            Defaults to False.\n    Returns:\n        str: folder or file path depending on folder_only flag\n    \"\"\"\n    log_filename = \"train_\"\n    log_filename += time.strftime(\"%Y_%m_%dT%H_%M_%S\")\n    log_filename += \".log\"\n\n    log_folder = os.path.join(save_dir, \"logs\")\n\n    if not os.path.exists(log_folder):\n        os.path.mkdirs(log_folder)\n\n    if folder_only:\n        return log_folder\n\n    log_filename = os.path.join(log_folder, log_filename)\n\n    return log_filename", "\n\ndef setup_logger(\n    output: str = None,\n    color: bool = True,\n    name: str = \"mmf\",\n    disable: bool = False,\n    clear_handlers=True,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the MMF logger and set its verbosity level to \"INFO\".\n    Outside libraries shouldn't call this in case they have set there\n    own logging handlers and setup. If they do, and don't want to\n    clear handlers, pass clear_handlers options.\n    The initial version of this function was taken from D2 and adapted\n    for MMF.\n    Args:\n        output (str): a file name or a directory to save log.\n            If ends with \".txt\" or \".log\", assumed to be a file name.\n            Default: Saved to file <save_dir/logs/log_[timestamp].txt>\n        color (bool): If false, won't log colored logs. Default: true\n        name (str): the root module name of this logger. Defaults to \"mmf\".\n        disable: do not use\n        clear_handlers (bool): If false, won't clear existing handlers.\n    Returns:\n        logging.Logger: a logger\n    \"\"\"\n    if disable:\n        return None\n    logger = logging.getLogger(name)\n    logger.propagate = False\n\n    logging.captureWarnings(True)\n    warnings_logger = logging.getLogger(\"py.warnings\")\n\n    plain_formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s : %(message)s\",\n        datefmt=\"%Y-%m-%dT%H:%M:%S\",\n    )\n\n    distributed_rank = get_rank()\n    handlers = []\n\n    logging_level = logging.INFO\n    # logging_level = logging.DEBUG\n\n    if distributed_rank == 0:\n        logger.setLevel(logging_level)\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(logging_level)\n        if color:\n            formatter = ColorfulFormatter(\n                colored(\"%(asctime)s | %(name)s: \", \"green\") + \"%(message)s\",\n                datefmt=\"%Y-%m-%dT%H:%M:%S\",\n            )\n        else:\n            formatter = plain_formatter\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n        warnings_logger.addHandler(ch)\n        handlers.append(ch)\n\n    # file logging: all workers\n    if output is None:\n        output = setup_output_folder()\n\n    if output is not None:\n        if output.endswith(\".txt\") or output.endswith(\".log\"):\n            filename = output\n        else:\n            filename = os.path.join(output, \"train.log\")\n        if distributed_rank > 0:\n            filename = filename + f\".rank{distributed_rank}\"\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n\n        fh = logging.StreamHandler(_cached_log_stream(filename))\n        fh.setLevel(logging_level)\n        fh.setFormatter(plain_formatter)\n        logger.addHandler(fh)\n        warnings_logger.addHandler(fh)\n        handlers.append(fh)\n\n        # Slurm/FB output, only log the main process\n        #     save_dir = get_mmf_env(key=\"save_dir\")\n        if \"train.log\" not in filename and distributed_rank == 0:\n            filename = os.path.join(output, \"train.log\")\n            sh = logging.StreamHandler(_cached_log_stream(filename))\n            sh.setLevel(logging_level)\n            sh.setFormatter(plain_formatter)\n            logger.addHandler(sh)\n            warnings_logger.addHandler(sh)\n            handlers.append(sh)\n\n        logger.info(f\"Logging to: {filename}\")\n\n    # Remove existing handlers to add MMF specific handlers\n    if clear_handlers:\n        for handler in logging.root.handlers[:]:\n            logging.root.removeHandler(handler)\n    # Now, add our handlers.\n    logging.basicConfig(level=logging_level, handlers=handlers)\n\n    return logger", "\n\ndef setup_very_basic_config(color=True):\n    plain_formatter = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s : %(message)s\",\n        datefmt=\"%Y-%m-%dT%H:%M:%S\",\n    )\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.INFO)\n    if color:\n        formatter = ColorfulFormatter(\n            colored(\"%(asctime)s | %(name)s: \", \"green\") + \"%(message)s\",\n            datefmt=\"%Y-%m-%dT%H:%M:%S\",\n        )\n    else:\n        formatter = plain_formatter\n    ch.setFormatter(formatter)\n    # Setup a minimal configuration for logging in case something tries to\n    # log a message even before logging is setup by MMF.\n    logging.basicConfig(level=logging.INFO, handlers=[ch])", "\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    return open(filename, \"a\")\n\n\n# ColorfulFormatter is adopted from Detectron2 and adapted for MMF\nclass ColorfulFormatter(logging.Formatter):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def formatMessage(self, record):\n        log = super().formatMessage(record)\n        if record.levelno == logging.WARNING:\n            prefix = colored(\"WARNING\", \"red\", attrs=[\"blink\"])\n        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:\n            prefix = colored(\"ERROR\", \"red\", attrs=[\"blink\", \"underline\"])\n        else:\n            return log\n        return prefix + \" \" + log", "\n# ColorfulFormatter is adopted from Detectron2 and adapted for MMF\nclass ColorfulFormatter(logging.Formatter):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def formatMessage(self, record):\n        log = super().formatMessage(record)\n        if record.levelno == logging.WARNING:\n            prefix = colored(\"WARNING\", \"red\", attrs=[\"blink\"])\n        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:\n            prefix = colored(\"ERROR\", \"red\", attrs=[\"blink\", \"underline\"])\n        else:\n            return log\n        return prefix + \" \" + log", "\n\nclass TensorboardLogger:\n    def __init__(self, log_folder=\"./logs\", iteration=0):\n        # This would handle warning of missing tensorboard\n        from torch.utils.tensorboard import SummaryWriter\n\n        self.summary_writer = None\n        self._is_master = is_main_process()\n        # self.timer = Timer()\n        self.log_folder = log_folder\n\n        if self._is_master:\n            # current_time = self.timer.get_time_hhmmss(None, format=self.time_format)\n            current_time = time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n            # self.timer.get_time_hhmmss(None, format=self.time_format)\n            tensorboard_folder = os.path.join(\n                self.log_folder, f\"tensorboard_{current_time}\"\n            )\n            self.summary_writer = SummaryWriter(tensorboard_folder)\n\n    def __del__(self):\n        if getattr(self, \"summary_writer\", None) is not None:\n            self.summary_writer.close()\n\n    def _should_log_tensorboard(self):\n        if self.summary_writer is None or not self._is_master:\n            return False\n        else:\n            return True\n\n    def add_scalar(self, key, value, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        self.summary_writer.add_scalar(key, value, iteration)\n\n    def add_scalars(self, scalar_dict, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        for key, val in scalar_dict.items():\n            self.summary_writer.add_scalar(key, val, iteration)\n\n    def add_histogram_for_model(self, model, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        for name, param in model.named_parameters():\n            np_param = param.clone().cpu().data.numpy()\n            self.summary_writer.add_histogram(name, np_param, iteration)", ""]}
{"filename": "utils/eval.py", "chunked_list": ["import torch\n\ndef reorder_frames(video, temporal_order='none'):\n    bs, n_frames = video.shape[:2]\n    if temporal_order == 'none':\n        pass\n    elif temporal_order == 'reverse':\n        video = video.flip(1)\n    elif temporal_order == 'shuffle':\n        # shuffle each example independently\n        idx = torch.argsort(torch.rand(bs, n_frames), dim=1)\n        video = video[torch.arange(bs).unsqueeze(1), idx]\n    elif temporal_order == 'freeze':\n        # take random frame and repeat\n        idx = torch.randint(n_frames, [bs])\n        video = video[torch.arange(bs), idx].unsqueeze(1).expand_as(video).contiguous()\n    else:\n        raise ValueError(f\"Invalid reordering method: {temporal_order}\")\n    return video"]}
{"filename": "utils/visualization.py", "chunked_list": ["import os\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid, save_image\nfrom einops import rearrange\n\n\ndef norm_ip(img, low, high):\n    img.clamp_(min=low, max=high)\n    img.sub_(low).div_(max(high - low, 1e-5))", "\n\ndef save_token_map(img_id, img, token_idx, thw_shape, save_path, bg_val=0.5, layer_id=(0, 1, 2,), texts=None):\n    os.makedirs(save_path, exist_ok=True)\n\n    # normalize image\n    bsz = len(img_id)\n    img_norm = img.detach().clone()\n    img_norm = rearrange(img_norm, '(b k) n c h w -> b (k n) c h w', b=bsz)\n    for t in img_norm:\n        norm_ip(t, float(t.min()), float(t.max()))\n    \n    with torch.no_grad():\n        for k, token_idx_ in enumerate(token_idx):\n            if k not in layer_id:\n                continue\n\n            num_clips = token_idx_.shape[0]\n            mask = torch.full((num_clips, *thw_shape), bg_val, device=img.device)\n            mask_flt = rearrange(mask, 'b t h w -> b (t h w)')\n            mask_flt[torch.arange(num_clips).unsqueeze(1), token_idx_] = 1\n            mask_rsz = F.interpolate(mask, img.shape[-2:], mode='nearest')\n            mask_rsz = rearrange(mask_rsz, '(b k) n h w -> b (k n) 1 h w', b=bsz)\n            img_norm = mask_rsz * img_norm\n\n    # save images\n    for i, id in enumerate(img_id):\n        save_fp = os.path.join(save_path, f'vid{id}.jpg')\n        save_image(img_norm[i], fp=save_fp, nrow=thw_shape[0], normalize=False)\n\n        if texts is not None:\n            save_fp_txt = os.path.join(save_path, f'vid{id}.txt')\n            with open(save_fp_txt, 'w') as txt:\n                txt.write(texts[i] + '\\n')"]}
{"filename": "utils/config_utils.py", "chunked_list": ["import os\nimport sys\nimport logging\nfrom os.path import join, dirname\nfrom omegaconf import OmegaConf, ListConfig, DictConfig\nfrom utils.distributed import init_distributed_mode, is_main_process\nfrom utils.logger import setup_logger\n\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\ndef convert_types(config):\n    \"\"\"Convert `'None'` (str) --> `None` (None). Only supports top-level\"\"\"\n    for k, v in config.items():\n        if isinstance(v, DictConfig):\n            setattr(config, k, convert_types(v))\n\n        # TODO convert types in ListConfig, right now they are ignored\n        # if isinstance(v, ListConfig):\n        #     new_v = ListConfig()\n\n        if v in [\"None\", \"none\"]:\n            setattr(config, k, None)\n    return config", "\n\ndef setup_config():\n    \"\"\"Conbine yaml config and command line config with OmegaConf.\n    Also converts types, e.g., `'None'` (str) --> `None` (None)\n    \"\"\"\n    config_path = sys.argv[1]\n    del sys.argv[1]  # not needed\n    cli_args = sys.argv[1:]\n    yaml_config = OmegaConf.load(config_path)\n    cli_config = OmegaConf.from_cli() if len(cli_args) else OmegaConf.create()\n    # the latter overwrite the former, i.e, cli_config higher priority.\n    logger.info(f\"Command line configs: {cli_config}\")\n    config = OmegaConf.merge(yaml_config, cli_config)\n    config = convert_types(config)\n    if config.debug:\n        config.wandb.enable = False\n    return config", "\n\ndef setup_evaluate_config(config):\n    \"\"\"setup evaluation default settings, e.g., disable wandb\"\"\"\n    assert config.evaluate\n    config.wandb.enable = False\n    if config.output_dir is None:\n        config.output_dir = join(dirname(config.pretrained_path), \"eval\")\n    return config\n", "\n\ndef setup_output_dir(output_dir, excludes=[\"code\"]):\n    \"\"\"ensure not overwritting an exisiting/non-empty output dir\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=False)\n    else:\n        existing_dirs_files = os.listdir(output_dir)  # list\n        remaining = set(existing_dirs_files) - set(excludes)\n        remaining = [e for e in remaining if \"slurm\" not in e]\n        assert len(remaining) == 0, f\"remaining dirs or files: {remaining}\"", "\n\ndef setup_main():\n    \"\"\"\n    Setup config, logger, output_dir, etc. \n    Shared for pretrain and all downstream tasks.\n    \"\"\"\n    config = setup_config()\n    if hasattr(config, \"evaluate\") and config.evaluate:\n        config = setup_evaluate_config(config)    \n    init_distributed_mode(config)\n\n    if is_main_process():\n        setup_output_dir(config.output_dir, excludes=[\"code\"])\n        setup_logger(output=config.output_dir, color=True, name=\"loopitr\")\n        OmegaConf.save(\n            config, open(os.path.join(config.output_dir, 'config.yaml'), 'w'))\n    return config", ""]}
{"filename": "dataset/caption_dataset.py", "chunked_list": ["from dataset.utils import pre_text\nfrom os.path import basename\n\nfrom dataset.base_dataset import ImageVideoBaseDataset\nfrom dataset.utils import load_anno\nfrom dataset.video_utils import VIDEO_READER_FUNCS\nimport logging\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass ImgTxtRetTrainDataset(ImageVideoBaseDataset):\n    media_type = \"image\"\n\n    def __init__(self, ann_file, transform, has_multi_vision_gt=False):\n        super(ImgTxtRetTrainDataset, self).__init__()\n        self.anno_list = load_anno(ann_file)\n        self.transform = transform\n        # each caption has multiple image as ground_truth, e.g., ssv2\n        self.has_multi_vision_gt = has_multi_vision_gt\n        self.match_ids = {}\n\n        n = 0\n        for ann in self.anno_list:\n            key = ann[\"caption\"] if has_multi_vision_gt else basename(ann[\"image\"])\n            if key not in self.match_ids:\n                self.match_ids[key] = n\n                n += 1\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def __getitem__(self, index):\n        ann = self.anno_list[index]\n        image, index = self.load_and_transform_media_data(index)\n        caption = pre_text(ann[\"caption\"])\n        key = ann[\"caption\"] if self.has_multi_vision_gt else basename(ann[\"image\"])\n        return image, caption, self.match_ids[key]", "\n\nclass VidTxtRetTrainDataset(ImgTxtRetTrainDataset):\n    media_type = \"video\"\n\n    def __init__(\n            self, ann_file, transform, num_frames=4,\n            video_reader_type=\"decord\", sample_type=\"rand\", num_tries=3,\n            is_paragraph_retrieval=False, has_multi_vision_gt=False\n    ):\n        super(VidTxtRetTrainDataset, self).__init__(ann_file, transform, has_multi_vision_gt)\n        self.num_frames = num_frames\n        self.video_reader_type = video_reader_type\n        self.video_reader = VIDEO_READER_FUNCS[video_reader_type]\n        self.sample_type = sample_type\n        self.num_tries = num_tries\n        self.is_paragraph_retrieval = is_paragraph_retrieval\n\n        if is_paragraph_retrieval:\n            self.anno_list = preprocess_para_retrieval_data(self.anno_list)", "\n\nclass ImgTxtRetEvalDataset(ImageVideoBaseDataset):\n    media_type = \"image\"\n\n    def __init__(self, ann_file, transform, has_multi_vision_gt=False):\n        super(ImgTxtRetEvalDataset, self).__init__()\n        self.raw_anno_list = load_anno(ann_file)\n        self.transform = transform\n        self.has_multi_vision_gt = has_multi_vision_gt  # each caption has multiple image as ground_truth\n\n        self.text = None\n        self.image = None\n        self.txt2img = None\n        self.img2txt = None\n        self.build_data()\n\n    def build_data(self):\n        self.text = []\n        self.image = []\n        self.txt2img = {}\n        self.img2txt = {}\n        if self.has_multi_vision_gt:\n            self.build_data_multi_img_gt()\n        else:\n            self.build_data_multi_txt_gt()\n        self.anno_list = [dict(image=e) for e in self.image]\n\n    def build_data_multi_img_gt(self):\n        \"\"\"each text may have multiple ground_truth image, e.g., ssv2\"\"\"\n        img_id = 0\n        for txt_id, ann in enumerate(self.raw_anno_list):\n            self.text.append(pre_text(ann[\"caption\"]))\n            self.txt2img[txt_id] = []\n            _images = ann[\"image\"] \\\n                if isinstance(ann[\"image\"], list) else [ann[\"image\"], ]\n            for i, image in enumerate(_images):\n                self.image.append(image)\n                self.txt2img[txt_id].append(img_id)\n                self.img2txt[img_id] = txt_id\n                img_id += 1\n\n    def build_data_multi_txt_gt(self):\n        \"\"\"each image may have multiple ground_truth text\uff0c e.g., COCO and Flickr30K\"\"\"\n        txt_id = 0\n        for img_id, ann in enumerate(self.raw_anno_list):\n            self.image.append(ann[\"image\"])\n            self.img2txt[img_id] = []\n            _captions = ann[\"caption\"] \\\n                if isinstance(ann[\"caption\"], list) else [ann[\"caption\"], ]\n            for i, caption in enumerate(_captions):\n                self.text.append(pre_text(caption))\n                self.img2txt[img_id].append(txt_id)\n                self.txt2img[txt_id] = img_id\n                txt_id += 1\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def __getitem__(self, index):\n        image, index = self.load_and_transform_media_data(index)\n        return image, index", "\n\nclass VidTxtRetEvalDataset(ImgTxtRetEvalDataset):\n    media_type = \"video\"\n\n    def __init__(\n            self, ann_file, transform, num_frames=4,\n            video_reader_type=\"decord\", sample_type=\"rand\", num_tries=1,\n            is_paragraph_retrieval=False, has_multi_vision_gt=False\n    ):\n        super(VidTxtRetEvalDataset, self).__init__(ann_file, transform, has_multi_vision_gt)\n        self.num_frames = num_frames\n        self.video_reader_type = video_reader_type\n        self.video_reader = VIDEO_READER_FUNCS[video_reader_type]\n        self.sample_type = sample_type\n        self.num_tries = num_tries\n        self.is_paragraph_retrieval = is_paragraph_retrieval\n\n        if is_paragraph_retrieval:\n            self.anno_list = preprocess_para_retrieval_data(self.raw_anno_list)\n        self.build_data()", "\n\ndef preprocess_para_retrieval_data(anno_list):\n    processed_anno_list = []\n    for d in anno_list:\n        d[\"caption\"] = \" \".join(d.pop(\"caption\"))\n        processed_anno_list.append(d)\n    return processed_anno_list\n\n\nclass VidTxtRetMCEvalDataset(ImageVideoBaseDataset):\n    \"\"\"For MSRVTT-MC test task\"\"\"\n    media_type = \"video\"\n\n    def __init__(self, ann_file, transform, num_frames=4,\n                 video_reader_type=\"decord\", sample_type=\"rand\", num_tries=1):\n        super(VidTxtRetMCEvalDataset, self).__init__()\n        self.anno_list = load_anno(ann_file)\n        self.transform = transform\n        # video args\n        self.num_frames = num_frames\n        self.video_reader_type = video_reader_type\n        self.video_reader = VIDEO_READER_FUNCS[video_reader_type]\n        self.sample_type = sample_type\n        self.num_tries = num_tries\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def __getitem__(self, index):\n        ann = self.anno_list[index]\n        image, index = self.load_and_transform_media_data(index)\n        caption = [pre_text(e) for e in ann[\"caption\"]]  # len=5\n        answer = ann[\"answer\"]\n        return image, caption, answer, ann", "\n\nclass VidTxtRetMCEvalDataset(ImageVideoBaseDataset):\n    \"\"\"For MSRVTT-MC test task\"\"\"\n    media_type = \"video\"\n\n    def __init__(self, ann_file, transform, num_frames=4,\n                 video_reader_type=\"decord\", sample_type=\"rand\", num_tries=1):\n        super(VidTxtRetMCEvalDataset, self).__init__()\n        self.anno_list = load_anno(ann_file)\n        self.transform = transform\n        # video args\n        self.num_frames = num_frames\n        self.video_reader_type = video_reader_type\n        self.video_reader = VIDEO_READER_FUNCS[video_reader_type]\n        self.sample_type = sample_type\n        self.num_tries = num_tries\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def __getitem__(self, index):\n        ann = self.anno_list[index]\n        image, index = self.load_and_transform_media_data(index)\n        caption = [pre_text(e) for e in ann[\"caption\"]]  # len=5\n        answer = ann[\"answer\"]\n        return image, caption, answer, ann", ""]}
{"filename": "dataset/dataloader.py", "chunked_list": ["import torch\nimport torch.distributed as dist\nfrom utils.distributed import get_rank, is_dist_avail_and_initialized, is_main_process\nimport random\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass MetaLoader(object):\n    \"\"\" wraps multiple data loader \"\"\"\n    def __init__(self, name2loader):\n        \"\"\"Iterates over multiple dataloaders, it ensures all processes\n        work on data from the same dataloader. This loader will end when\n        the shorter dataloader raises StopIteration exception.\n\n        loaders: Dict, {name: dataloader}\n        \"\"\"\n        self.name2loader = name2loader\n        self.name2iter = {name: iter(l) for name, l in name2loader.items()}\n        name2index = {name: idx for idx, (name, l) in enumerate(name2loader.items())}\n        index2name = {v: k for k, v in name2index.items()}\n\n        iter_order = []\n        for n, l in name2loader.items():\n            iter_order.extend([name2index[n]]*len(l))\n\n        random.shuffle(iter_order)\n        iter_order = torch.Tensor(iter_order).to(torch.device(\"cuda\")).to(torch.uint8)\n\n        # sync\n        if is_dist_avail_and_initialized():\n            # make sure all processes have the same order so that\n            # each step they will have data from the same loader\n            dist.broadcast(iter_order, src=0)\n        self.iter_order = [index2name[int(e.item())] for e in iter_order.cpu()]\n\n        logger.info(str(self))\n\n    def __str__(self):\n        output = [f\"MetaLoader has {len(self.name2loader)} dataloaders, {len(self)} batches in total\"]\n        for idx, (name, loader) in enumerate(self.name2loader.items()):\n            output.append(\n                f\"dataloader index={idx} name={name}, batch-size={loader.batch_size} length(#batches)={len(loader)} \"\n            )\n        return \"\\n\".join(output)\n\n    def __len__(self):\n        return len(self.iter_order)\n\n    def __iter__(self):\n        \"\"\" this iterator will run indefinitely \"\"\"\n        for name in self.iter_order:\n            _iter = self.name2iter[name]\n            batch = next(_iter)\n            yield name, batch", "\nclass MetaLoader(object):\n    \"\"\" wraps multiple data loader \"\"\"\n    def __init__(self, name2loader):\n        \"\"\"Iterates over multiple dataloaders, it ensures all processes\n        work on data from the same dataloader. This loader will end when\n        the shorter dataloader raises StopIteration exception.\n\n        loaders: Dict, {name: dataloader}\n        \"\"\"\n        self.name2loader = name2loader\n        self.name2iter = {name: iter(l) for name, l in name2loader.items()}\n        name2index = {name: idx for idx, (name, l) in enumerate(name2loader.items())}\n        index2name = {v: k for k, v in name2index.items()}\n\n        iter_order = []\n        for n, l in name2loader.items():\n            iter_order.extend([name2index[n]]*len(l))\n\n        random.shuffle(iter_order)\n        iter_order = torch.Tensor(iter_order).to(torch.device(\"cuda\")).to(torch.uint8)\n\n        # sync\n        if is_dist_avail_and_initialized():\n            # make sure all processes have the same order so that\n            # each step they will have data from the same loader\n            dist.broadcast(iter_order, src=0)\n        self.iter_order = [index2name[int(e.item())] for e in iter_order.cpu()]\n\n        logger.info(str(self))\n\n    def __str__(self):\n        output = [f\"MetaLoader has {len(self.name2loader)} dataloaders, {len(self)} batches in total\"]\n        for idx, (name, loader) in enumerate(self.name2loader.items()):\n            output.append(\n                f\"dataloader index={idx} name={name}, batch-size={loader.batch_size} length(#batches)={len(loader)} \"\n            )\n        return \"\\n\".join(output)\n\n    def __len__(self):\n        return len(self.iter_order)\n\n    def __iter__(self):\n        \"\"\" this iterator will run indefinitely \"\"\"\n        for name in self.iter_order:\n            _iter = self.name2iter[name]\n            batch = next(_iter)\n            yield name, batch", ""]}
{"filename": "dataset/__init__.py", "chunked_list": ["import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\n\nfrom dataset.caption_dataset import (\n    ImgTxtRetTrainDataset, ImgTxtRetEvalDataset,\n    VidTxtRetTrainDataset, VidTxtRetEvalDataset,\n    VidTxtRetMCEvalDataset\n)", "    VidTxtRetMCEvalDataset\n)\nfrom dataset.qa_dataset import ImageQADataset, VideoQADataset\n\nfrom dataset.dataloader import MetaLoader\n\n\ndef get_media_type(dataset_config):\n    if len(dataset_config) == 3 and dataset_config[2] == \"video\":\n        return \"video\"\n    else:\n        return \"image\"", "\n\ndef create_dataset(dataset_type, config):\n    if config.vit_type == \"deit\":\n        mean = (0.485, 0.456, 0.406)\n        std = (0.229, 0.224, 0.225)\n    elif config.vit_type in [\"beit\", \"vit\", \"vit32\"]:\n        mean = (0.5, 0.5, 0.5)  # for all beit model except IN1K finetuning\n        std = (0.5, 0.5, 0.5)\n    else:\n        raise ValueError(f\"ViT model {config.vit_type} not supported\")\n\n    normalize = transforms.Normalize(mean, std)\n\n    # loaded images and videos are torch.Tensor of torch.uint8 format,\n    # ordered as (T, 1 or 3, H, W) where T=1 for image\n    type_transform = transforms.Lambda(lambda x: x.float().div(255.))\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(\n            config.image_res, scale=(0.5, 1.0),\n            interpolation=InterpolationMode.BICUBIC),\n        transforms.RandomHorizontalFlip(),\n        type_transform,\n        normalize,\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize(\n            (config.image_res, config.image_res),\n            interpolation=InterpolationMode.BICUBIC),\n        type_transform,\n        normalize,\n    ])\n\n    video_only_dataset_kwargs_train = dict(\n        video_reader_type=config.video_input.reader,\n        sample_type=config.video_input.sample_type,\n        num_frames=config.video_input.num_frames,\n        num_tries=3,  # false tolerance\n    )\n    video_only_dataset_kwargs_eval = dict(\n        video_reader_type=config.video_input.reader,\n        sample_type=config.video_input.sample_type_test,\n        num_frames=config.video_input.num_frames_test,\n        num_tries=3,  # false tolerance\n        # num_tries=1,  # we want to have predictions for all videos\n    )\n\n    if dataset_type in [\"ret_train\", \"ret_eval\"]:  # for didemo and activitynet captions\n        is_paragraph_retrieval = config.get(\"is_paragraph_retrieval\", False)\n        video_only_dataset_kwargs_eval[\"is_paragraph_retrieval\"] = is_paragraph_retrieval\n        video_only_dataset_kwargs_train[\"is_paragraph_retrieval\"] = is_paragraph_retrieval\n\n    if dataset_type in [\"pt_train\", \"ret_train\"]:\n        # convert to list of lists\n        train_files = [config.train_file] \\\n            if isinstance(config.train_file[0], str) else config.train_file\n        train_media_types = sorted(list({get_media_type(e) for e in train_files}))\n        if dataset_type == \"ret_train\":\n            assert len(train_media_types) == 1, \\\n                f\"retrieval downstream should only have one media type, got {train_media_types}\"\n\n        train_datasets = []\n        for m in train_media_types:\n            dataset_cls = ImgTxtRetTrainDataset if m == \"image\" else VidTxtRetTrainDataset\n            # dataset of the same media_type will be mixed in a single Dataset object\n            _train_files = [e for e in train_files if get_media_type(e) == m]\n            dataset_kwargs = dict(\n                ann_file=_train_files, transform=train_transform,\n                has_multi_vision_gt=config.get(\"has_multi_vision_gt\", False)  # true for ssv2 ret\n            )\n            if m == \"video\":\n                dataset_kwargs.update(video_only_dataset_kwargs_train)\n            train_datasets.append(dataset_cls(**dataset_kwargs))\n        return train_datasets\n\n    elif dataset_type in [\"pt_eval\", \"ret_eval\"]:\n        test_datasets = []\n        test_dataset_names = []\n        # multiple test datasets, all separate\n        for name, data_cfg in config.test_file.items():\n            media_type = get_media_type(data_cfg)\n            test_dataset_cls = ImgTxtRetEvalDataset if media_type == \"image\" else VidTxtRetEvalDataset\n            test_dataset_names.append(name)\n            dataset_kwargs = dict(\n                ann_file=[data_cfg], transform=test_transform,\n                has_multi_vision_gt=config.get(\"has_multi_vision_gt\", False)  # true for ssv2 ret\n            )\n            if media_type == \"video\":\n                dataset_kwargs.update(video_only_dataset_kwargs_eval)\n            test_datasets.append(test_dataset_cls(**dataset_kwargs))\n        return test_datasets, test_dataset_names\n\n    elif dataset_type == \"qa_train\":\n        media_type = get_media_type(config.train_file[0])  # assuming single train media type\n        dataset_cls = ImageQADataset if media_type == \"image\" else VideoQADataset\n        dataset_kwargs = dict(\n            ann_file=config.train_file, transform=train_transform, eos=config.eos, mode=\"train\")\n        if media_type == \"video\":\n            dataset_kwargs.update(video_only_dataset_kwargs_train)\n        train_dataset = dataset_cls(**dataset_kwargs)\n        return train_dataset\n\n    elif dataset_type == \"qa_eval\":\n        test_datasets = []\n        test_dataset_names = []\n        # multiple test datasets, all separate\n        for name, data_cfg in config.test_file.items():\n            media_type = get_media_type(data_cfg)\n            test_dataset_cls = ImageQADataset if media_type == \"image\" else VideoQADataset\n            test_dataset_names.append(name)\n            dataset_kwargs = dict(\n                ann_file=[data_cfg], transform=test_transform,\n                eos=config.eos, mode=\"eval\", answer_list=config.answer_list\n            )\n            if media_type == \"video\":\n                dataset_kwargs.update(video_only_dataset_kwargs_eval)\n            test_datasets.append(test_dataset_cls(**dataset_kwargs))\n        return test_datasets, test_dataset_names\n\n    elif dataset_type == \"mc_test\":\n        dataset_kwargs = dict(\n            ann_file=[config.test_file.mc_test], transform=test_transform\n        )\n        dataset_kwargs.update(video_only_dataset_kwargs_eval)\n        return VidTxtRetMCEvalDataset(**dataset_kwargs)", "\n\ndef vqa_collate_fn(batch):\n    image_list, question_list, answer_list, weight_list, n = [], [], [], [], []\n    for image, question, answer, weights in batch:\n        image_list.append(image)\n        question_list.append(question)\n        weight_list += weights\n        answer_list += answer\n        n.append(len(answer))\n    return torch.stack(image_list, dim=0), \\\n        question_list, answer_list, torch.Tensor(weight_list), n", "\n\ndef create_sampler(datasets, shuffles, num_tasks, global_rank):\n    samplers = []\n    for dataset, shuffle in zip(datasets, shuffles):\n        sampler = torch.utils.data.DistributedSampler(\n            dataset, num_replicas=num_tasks, rank=global_rank, shuffle=shuffle)\n        samplers.append(sampler)\n    return samplers\n", "\n\ndef create_loader(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n    loaders = []\n    for dataset, sampler, bs, n_worker, is_train, collate_fn in \\\n            zip(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n        if is_train:\n            shuffle = (sampler is None)\n            drop_last = True\n        else:\n            shuffle = False\n            drop_last = False\n        loader = DataLoader(\n            dataset,\n            batch_size=bs,\n            num_workers=n_worker,\n            pin_memory=True,\n            sampler=sampler,\n            shuffle=shuffle,\n            collate_fn=collate_fn,\n            drop_last=drop_last,\n            persistent_workers=True\n        )\n        loaders.append(loader)\n    return loaders", "\n\ndef iterate_dataloaders(dataloaders):\n    \"\"\"Alternatively generate data from multiple dataloaders,\n    since we use `zip` to concat multiple dataloaders,\n    the loop will end when the smaller dataloader runs out.\n\n    Args:\n        dataloaders List(DataLoader): can be a single or multiple dataloaders\n    \"\"\"\n    for data_tuples in zip(*dataloaders):\n        for idx, data in enumerate(data_tuples):\n            yield dataloaders[idx].dataset.media_type, data", ""]}
{"filename": "dataset/utils.py", "chunked_list": ["from utils.distributed import is_main_process, get_rank, get_world_size\nimport logging\nimport torch.distributed as dist\nimport torch\nimport os\nimport json\nimport re\nimport numpy as np\nfrom os.path import join\nfrom tqdm import trange", "from os.path import join\nfrom tqdm import trange\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom torchvision.transforms import PILToTensor\nImageFile.LOAD_TRUNCATED_IMAGES = True\nImage.MAX_IMAGE_PIXELS = None\n\n\ndef load_image_from_path(image_path):\n    image = Image.open(image_path).convert('RGB')  # PIL Image\n    image = PILToTensor()(image).unsqueeze(0)  # (1, C, H, W), torch.uint8\n    return image", "\ndef load_image_from_path(image_path):\n    image = Image.open(image_path).convert('RGB')  # PIL Image\n    image = PILToTensor()(image).unsqueeze(0)  # (1, C, H, W), torch.uint8\n    return image\n\n\ndef load_anno(ann_file_list):\n    \"\"\"[summary]\n\n    Args:\n        ann_file_list (List[List[str, str]] or List[str, str]):\n            the latter will be automatically converted to the former.\n            Each sublist contains [anno_path, image_root], (or [anno_path, video_root, 'video'])\n            which specifies the data type, video or image\n\n    Returns:\n        List(dict): each dict is {\n            image: str or List[str],  # image_path,\n            caption: str or List[str]  # caption text string\n        }\n    \"\"\"\n    if isinstance(ann_file_list[0], str):\n        ann_file_list = [ann_file_list]\n\n    ann = []\n    for d in ann_file_list:\n        data_root = d[1]\n        fp = d[0]\n        is_video = len(d) == 3 and d[2] == \"video\"\n        cur_ann = json.load(open(fp, \"r\"))\n        iterator = trange(len(cur_ann), desc=f\"Loading {fp}\") \\\n            if is_main_process() else range(len(cur_ann))\n        for idx in iterator:\n            key = \"video\" if is_video else \"image\"\n            # unified to have the same key for data path\n            if isinstance(cur_ann[idx][key], str):\n                cur_ann[idx][\"image\"] = join(data_root, cur_ann[idx][key])\n            else:  # list\n                cur_ann[idx][\"image\"] = [join(data_root, e) for e in cur_ann[idx][key]]\n        ann += cur_ann\n    return ann", "\n\ndef pre_text(text, max_l=None):\n    text = re.sub(r\"([,.'!?\\\"()*#:;~])\", '', text.lower())\n    text = text.replace('-', ' ').replace('/', ' ').replace('<person>', 'person')\n\n    text = re.sub(r\"\\s{2,}\", ' ', text)\n    text = text.rstrip('\\n').strip(' ')\n\n    if max_l:  # truncate\n        words = text.split(' ')\n        if len(words) > max_l:\n            text = ' '.join(words[:max_l])\n    return text", "\n\nlogger = logging.getLogger(__name__)\n\n\ndef collect_result(result, result_dir, filename, is_json=True, is_list=True):\n    if is_json:\n        result_file = os.path.join(\n            result_dir, '%s_rank%d.json' % (filename, get_rank()))\n        final_result_file = os.path.join(result_dir, '%s.json' % filename)\n        json.dump(result, open(result_file, 'w'))\n    else:\n        result_file = os.path.join(\n            result_dir, '%s_rank%d.pth' % (filename, get_rank()))\n        final_result_file = os.path.join(result_dir, '%s.pth' % filename)\n        torch.save(result, result_file)\n\n    dist.barrier()\n\n    result = None\n    if is_main_process():\n        # combine results from all processes\n        if is_list:\n            result = []\n        else:\n            result = {}\n        for rank in range(get_world_size()):\n            if is_json:\n                result_file = os.path.join(\n                    result_dir, '%s_rank%d.json' % (filename, rank))\n                res = json.load(open(result_file, 'r'))\n            else:\n                result_file = os.path.join(\n                    result_dir, '%s_rank%d.pth' % (filename, rank))\n                res = torch.load(result_file)\n            if is_list:\n                result += res\n            else:\n                result.update(res)\n\n    return result", "\n\ndef sync_save_result(result, result_dir, filename, is_json=True, is_list=True):\n    \"\"\"gather results from multiple GPUs\"\"\"\n    if is_json:\n        result_file = os.path.join(\n            result_dir, \"dist_res\", '%s_rank%d.json' % (filename, get_rank()))\n        final_result_file = os.path.join(result_dir, '%s.json' % filename)\n        os.makedirs(os.path.dirname(result_file), exist_ok=True)\n        json.dump(result, open(result_file, 'w'))\n    else:\n        result_file = os.path.join(\n            result_dir, \"dist_res\", '%s_rank%d.pth' % (filename, get_rank()))\n        os.makedirs(os.path.dirname(result_file), exist_ok=True)\n        final_result_file = os.path.join(result_dir, '%s.pth' % filename)\n        torch.save(result, result_file)\n\n    dist.barrier()\n\n    if is_main_process():\n        # combine results from all processes\n        if is_list:\n            result = []\n        else:\n            result = {}\n        for rank in range(get_world_size()):\n            if is_json:\n                result_file = os.path.join(\n                    result_dir, \"dist_res\", '%s_rank%d.json' % (filename, rank))\n                res = json.load(open(result_file, 'r'))\n            else:\n                result_file = os.path.join(\n                    result_dir, \"dist_res\", '%s_rank%d.pth' % (filename, rank))\n                res = torch.load(result_file)\n            if is_list:\n                result += res\n            else:\n                result.update(res)\n        if is_json:\n            json.dump(result, open(final_result_file, 'w'))\n        else:\n            torch.save(result, final_result_file)\n\n        logger.info('result file saved to %s' % final_result_file)\n    dist.barrier()\n    return final_result_file, result", "\n\ndef pad_sequences_1d(sequences, dtype=torch.long, device=torch.device(\"cpu\"), fixed_length=None):\n    \"\"\" Pad a single-nested list or a sequence of n-d array (torch.tensor or np.ndarray)\n    into a (n+1)-d array, only allow the first dim has variable lengths.\n    Args:\n        sequences: list(n-d tensor or list)\n        dtype: np.dtype or torch.dtype\n        device:\n        fixed_length: pad all seq in sequences to fixed length. All seq should have a length <= fixed_length.\n            return will be of shape [len(sequences), fixed_length, ...]\n    Returns:\n        padded_seqs: ((n+1)-d tensor) padded with zeros\n        mask: (2d tensor) of the same shape as the first two dims of padded_seqs,\n              1 indicate valid, 0 otherwise\n    Examples:\n        >>> test_data_list = [[1,2,3], [1,2], [3,4,7,9]]\n        >>> pad_sequences_1d(test_data_list, dtype=torch.long)\n        >>> test_data_3d = [torch.randn(2,3,4), torch.randn(4,3,4), torch.randn(1,3,4)]\n        >>> pad_sequences_1d(test_data_3d, dtype=torch.float)\n        >>> test_data_list = [[1,2,3], [1,2], [3,4,7,9]]\n        >>> pad_sequences_1d(test_data_list, dtype=np.float32)\n        >>> test_data_3d = [np.random.randn(2,3,4), np.random.randn(4,3,4), np.random.randn(1,3,4)]\n        >>> pad_sequences_1d(test_data_3d, dtype=np.float32)\n    \"\"\"\n    if isinstance(sequences[0], list):\n        if \"torch\" in str(dtype):\n            sequences = [torch.tensor(s, dtype=dtype, device=device) for s in sequences]\n        else:\n            sequences = [np.asarray(s, dtype=dtype) for s in sequences]\n\n    extra_dims = sequences[0].shape[1:]  # the extra dims should be the same for all elements\n    lengths = [len(seq) for seq in sequences]\n    if fixed_length is not None:\n        max_length = fixed_length\n    else:\n        max_length = max(lengths)\n    if isinstance(sequences[0], torch.Tensor):\n        assert \"torch\" in str(dtype), \"dtype and input type does not match\"\n        padded_seqs = torch.zeros((len(sequences), max_length) + extra_dims, dtype=dtype, device=device)\n        mask = torch.zeros((len(sequences), max_length), dtype=torch.float32, device=device)\n    else:  # np\n        assert \"numpy\" in str(dtype), \"dtype and input type does not match\"\n        padded_seqs = np.zeros((len(sequences), max_length) + extra_dims, dtype=dtype)\n        mask = np.zeros((len(sequences), max_length), dtype=np.float32)\n\n    for idx, seq in enumerate(sequences):\n        end = lengths[idx]\n        padded_seqs[idx, :end] = seq\n        mask[idx, :end] = 1\n    return padded_seqs, mask  # , lengths", "\n\n"]}
{"filename": "dataset/base_dataset.py", "chunked_list": ["from torch.utils.data import Dataset\nfrom dataset.utils import load_image_from_path\nimport random\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ImageVideoBaseDataset(Dataset):\n    \"\"\"Base class that implements the image and video loading methods\"\"\"\n    media_type = \"video\"\n\n    def __init__(self):\n        assert self.media_type in [\"image\", \"video\"]\n        self.anno_list = None  # list(dict), each dict contains {\"image\": str, # image or video path}\n        self.transform = None\n        self.video_reader = None\n        self.num_tries = None\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def load_and_transform_media_data(self, index):\n        if self.media_type == \"image\":\n            return self.load_and_transform_media_data_image(index)\n        else:\n            return self.load_and_transform_media_data_video(index)\n\n    def load_and_transform_media_data_image(self, index):\n        ann = self.anno_list[index]\n        data_path = ann[\"image\"]\n        image = load_image_from_path(data_path)\n        image = self.transform(image)\n        return image, index\n\n    def load_and_transform_media_data_video(self, index):\n        for i in range(self.num_tries):\n            ann = self.anno_list[index]\n            data_path = ann[\"image\"]\n            try:\n                max_num_frames = self.max_num_frames \\\n                    if hasattr(self, \"max_num_frames\") else -1\n                frames, frame_indices, video_duration = self.video_reader(\n                    data_path, self.num_frames, self.sample_type,\n                    max_num_frames=max_num_frames\n                )\n            except Exception as e:\n                index = random.randint(0, len(self) - 1)\n                logger.warning(\n                    f\"Caught exception {e} when loading video {data_path}, \"\n                    f\"randomly sample a new video as replacement\")\n                continue\n\n            frames = self.transform(frames)\n            return frames, index\n        else:\n            raise RuntimeError(\n                f\"Failed to fetch video after {self.num_tries} tries. \"\n                f\"This might indicate that you have many corrupted videos.\"\n            )", "class ImageVideoBaseDataset(Dataset):\n    \"\"\"Base class that implements the image and video loading methods\"\"\"\n    media_type = \"video\"\n\n    def __init__(self):\n        assert self.media_type in [\"image\", \"video\"]\n        self.anno_list = None  # list(dict), each dict contains {\"image\": str, # image or video path}\n        self.transform = None\n        self.video_reader = None\n        self.num_tries = None\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def load_and_transform_media_data(self, index):\n        if self.media_type == \"image\":\n            return self.load_and_transform_media_data_image(index)\n        else:\n            return self.load_and_transform_media_data_video(index)\n\n    def load_and_transform_media_data_image(self, index):\n        ann = self.anno_list[index]\n        data_path = ann[\"image\"]\n        image = load_image_from_path(data_path)\n        image = self.transform(image)\n        return image, index\n\n    def load_and_transform_media_data_video(self, index):\n        for i in range(self.num_tries):\n            ann = self.anno_list[index]\n            data_path = ann[\"image\"]\n            try:\n                max_num_frames = self.max_num_frames \\\n                    if hasattr(self, \"max_num_frames\") else -1\n                frames, frame_indices, video_duration = self.video_reader(\n                    data_path, self.num_frames, self.sample_type,\n                    max_num_frames=max_num_frames\n                )\n            except Exception as e:\n                index = random.randint(0, len(self) - 1)\n                logger.warning(\n                    f\"Caught exception {e} when loading video {data_path}, \"\n                    f\"randomly sample a new video as replacement\")\n                continue\n\n            frames = self.transform(frames)\n            return frames, index\n        else:\n            raise RuntimeError(\n                f\"Failed to fetch video after {self.num_tries} tries. \"\n                f\"This might indicate that you have many corrupted videos.\"\n            )", ""]}
{"filename": "dataset/qa_dataset.py", "chunked_list": ["import json\nfrom dataset.base_dataset import ImageVideoBaseDataset\nfrom dataset.utils import pre_text, load_anno\nfrom dataset.video_utils import VIDEO_READER_FUNCS\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ImageQADataset(ImageVideoBaseDataset):\n    media_type = \"image\"\n\n    def __init__(self, ann_file, transform, eos=\"[SEP]\", mode=\"train\", answer_list=None):\n        super(ImageQADataset, self).__init__()\n        assert mode in [\"train\", \"eval\"]\n        self.mode = mode\n        self.transform = transform\n        self.eos = eos\n\n        self.anno_list = load_anno(ann_file)\n\n        if mode == \"eval\":\n            self.answer_list = json.load(open(answer_list, \"r\"))\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def get_answers_with_weights(self, raw_answers):\n        if isinstance(raw_answers, str):\n            raw_answers = [raw_answers]\n        answer_weight = {}\n        for answer in raw_answers:\n            if answer in answer_weight.keys():\n                answer_weight[answer] += 1/len(raw_answers)\n            else:\n                answer_weight[answer] = 1/len(raw_answers)\n\n        answers = list(answer_weight.keys())\n        weights = [answer_weight[a] for a in answers]\n        answers = [answer + \" \" + self.eos for answer in answers]\n        return answers, weights\n\n    def __getitem__(self, index):\n        ann = self.anno_list[index]\n        image, index = self.load_and_transform_media_data(index)\n\n        question = pre_text(ann[\"question\"])\n        if self.mode == \"train\":\n            answers, weights = self.get_answers_with_weights(ann[\"answer\"])\n            return image, question, answers, weights\n        else:  # self.mode == \"eval\":\n            question_id = ann[\"question_id\"]\n            return image, question, question_id", "\nclass ImageQADataset(ImageVideoBaseDataset):\n    media_type = \"image\"\n\n    def __init__(self, ann_file, transform, eos=\"[SEP]\", mode=\"train\", answer_list=None):\n        super(ImageQADataset, self).__init__()\n        assert mode in [\"train\", \"eval\"]\n        self.mode = mode\n        self.transform = transform\n        self.eos = eos\n\n        self.anno_list = load_anno(ann_file)\n\n        if mode == \"eval\":\n            self.answer_list = json.load(open(answer_list, \"r\"))\n\n    def __len__(self):\n        return len(self.anno_list)\n\n    def get_answers_with_weights(self, raw_answers):\n        if isinstance(raw_answers, str):\n            raw_answers = [raw_answers]\n        answer_weight = {}\n        for answer in raw_answers:\n            if answer in answer_weight.keys():\n                answer_weight[answer] += 1/len(raw_answers)\n            else:\n                answer_weight[answer] = 1/len(raw_answers)\n\n        answers = list(answer_weight.keys())\n        weights = [answer_weight[a] for a in answers]\n        answers = [answer + \" \" + self.eos for answer in answers]\n        return answers, weights\n\n    def __getitem__(self, index):\n        ann = self.anno_list[index]\n        image, index = self.load_and_transform_media_data(index)\n\n        question = pre_text(ann[\"question\"])\n        if self.mode == \"train\":\n            answers, weights = self.get_answers_with_weights(ann[\"answer\"])\n            return image, question, answers, weights\n        else:  # self.mode == \"eval\":\n            question_id = ann[\"question_id\"]\n            return image, question, question_id", "\n\nclass VideoQADataset(ImageQADataset):\n    media_type = \"video\"\n\n    def __init__(\n            self, ann_file, transform, eos=\"[SEP]\", mode=\"train\", answer_list=None,\n            num_frames=4, video_reader_type=\"decord\", sample_type=\"rand\", num_tries=1\n    ):\n        super(VideoQADataset, self).__init__(\n            ann_file, transform, eos, mode, answer_list)\n        self.num_frames = num_frames\n        self.video_reader_type = video_reader_type\n        self.video_reader = VIDEO_READER_FUNCS[video_reader_type]\n        self.sample_type = sample_type\n        self.num_tries = num_tries", ""]}
{"filename": "dataset/video_utils.py", "chunked_list": ["\"\"\"\nModified from https://github.com/m-bain/frozen-in-time/blob/22a91d78405ec6032fdf521ae1ff5573358e632f/base/base_dataset.py\n\"\"\"\nimport random\nimport av\nimport decord\nimport torch\nimport numpy as np\nimport math\ndecord.bridge.set_bridge(\"torch\")", "import math\ndecord.bridge.set_bridge(\"torch\")\n\n\ndef pts_to_secs(pts: int, time_base: float, start_pts: int) -> float:\n    \"\"\"\n    Converts a present time with the given time base and start_pts offset to seconds.\n\n    Returns:\n        time_in_seconds (float): The corresponding time in seconds.\n\n    https://github.com/facebookresearch/pytorchvideo/blob/main/pytorchvideo/data/utils.py#L54-L64\n    \"\"\"\n    if pts == math.inf:\n        return math.inf\n\n    return int(pts - start_pts) * time_base", "\n\ndef get_pyav_video_duration(video_reader):\n    video_stream = video_reader.streams.video[0]\n    video_duration = pts_to_secs(\n        video_stream.duration,\n        video_stream.time_base,\n        video_stream.start_time\n    )\n    return float(video_duration)", "\n\ndef get_frame_indices_by_fps():\n    pass\n\n\ndef get_frame_indices(num_frames, vlen, sample='rand', fix_start=None, input_fps=1, max_num_frames=-1):\n    if sample in [\"rand\", \"middle\"]:\n        acc_samples = min(num_frames, vlen)\n        # split the video into `acc_samples` intervals, and sample from each interval.\n        intervals = np.linspace(start=0, stop=vlen, num=acc_samples + 1).astype(int)\n        ranges = []\n        for idx, interv in enumerate(intervals[:-1]):\n            ranges.append((interv, intervals[idx + 1] - 1))\n        if sample == 'rand':\n            try:\n                frame_indices = [random.choice(range(x[0], x[1])) for x in ranges]\n            except:\n                frame_indices = np.random.permutation(vlen)[:acc_samples]\n                frame_indices.sort()\n                frame_indices = list(frame_indices)\n        elif fix_start is not None:\n            frame_indices = [x[0] + fix_start for x in ranges]\n        elif sample == 'middle':\n            frame_indices = [(x[0] + x[1]) // 2 for x in ranges]\n        else:\n            raise NotImplementedError\n\n        if len(frame_indices) < num_frames:  # padded with last frame\n            padded_frame_indices = [frame_indices[-1]] * num_frames\n            padded_frame_indices[:len(frame_indices)] = frame_indices\n            frame_indices = padded_frame_indices\n    elif \"fps\" in sample:  # fps0.5, sequentially sample frames at 0.5 fps\n        output_fps = float(sample[3:])\n        duration = float(vlen) / input_fps\n        delta = 1 / output_fps  # gap between frames, this is also the clip length each frame represents\n        frame_seconds = np.arange(0 + delta / 2, duration + delta / 2, delta)\n        frame_indices = np.around(frame_seconds * input_fps).astype(int)\n        frame_indices = [e for e in frame_indices if e < vlen]\n        if max_num_frames > 0 and len(frame_indices) > max_num_frames:\n            frame_indices = frame_indices[:max_num_frames]\n            # frame_indices = np.linspace(0 + delta / 2, duration + delta / 2, endpoint=False, num=max_num_frames)\n    else:\n        raise ValueError\n    return frame_indices", "\n\ndef read_frames_av(video_path, num_frames, sample='rand', fix_start=None, max_num_frames=-1):\n    reader = av.open(video_path)\n    frames = [torch.from_numpy(f.to_rgb().to_ndarray()) for f in reader.decode(video=0)]\n    vlen = len(frames)\n    duration = get_pyav_video_duration(reader)\n    fps = vlen / float(duration)\n    frame_indices = get_frame_indices(\n        num_frames, vlen, sample=sample, fix_start=fix_start,\n        input_fps=fps, max_num_frames=max_num_frames\n    )\n    frames = torch.stack([frames[idx] for idx in frame_indices])  # (T, H, W, C), torch.uint8\n    frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W), torch.uint8\n    return frames, frame_indices, duration", "\n\ndef read_frames_decord(video_path, num_frames, sample='rand', fix_start=None, max_num_frames=-1):\n    video_reader = decord.VideoReader(video_path, num_threads=1)\n    vlen = len(video_reader)\n    fps = video_reader.get_avg_fps()\n    duration = vlen / float(fps)\n    frame_indices = get_frame_indices(\n        num_frames, vlen, sample=sample, fix_start=fix_start,\n        input_fps=fps, max_num_frames=max_num_frames\n    )\n    frames = video_reader.get_batch(frame_indices)  # (T, H, W, C), torch.uint8\n    frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W), torch.uint8\n    return frames, frame_indices, duration", "\n\nVIDEO_READER_FUNCS = {\n    'av': read_frames_av,\n    'decord': read_frames_decord\n}\n"]}
{"filename": "models/sparse_xbert.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model. \"\"\"\n\nimport math", "\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint", "from torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n    add_start_docstrings,", "    ModelOutput,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,", "    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,", "from transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom models.sparse_config import BertConfig\n\nimport transformers", "\nimport transformers\ntransformers.logging.set_verbosity_error()\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"BertConfig\"\n_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n\nBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [", "\nBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"bert-base-uncased\",\n    \"bert-large-uncased\",\n    \"bert-base-cased\",\n    \"bert-large-cased\",\n    \"bert-base-multilingual-uncased\",\n    \"bert-base-multilingual-cased\",\n    \"bert-base-chinese\",\n    \"bert-base-german-cased\",", "    \"bert-base-chinese\",\n    \"bert-base-german-cased\",\n    \"bert-large-uncased-whole-word-masking\",\n    \"bert-large-cased-whole-word-masking\",\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n    \"bert-base-cased-finetuned-mrpc\",\n    \"bert-base-german-dbmdz-cased\",\n    \"bert-base-german-dbmdz-uncased\",\n    \"cl-tohoku/bert-base-japanese\",", "    \"bert-base-german-dbmdz-uncased\",\n    \"cl-tohoku/bert-base-japanese\",\n    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n    \"cl-tohoku/bert-base-japanese-char\",\n    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n    \"TurkuNLP/bert-base-finnish-cased-v1\",\n    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n    \"wietsedv/bert-base-dutch-cased\",\n    # See all BERT models at https://huggingface.co/models?filter=bert\n]", "    # See all BERT models at https://huggingface.co/models?filter=bert\n]\n\n\n@dataclass\nclass BertModelOutputWithPastAndCrossAttentions(BaseModelOutputWithPastAndCrossAttentions):\n    token_idx: Optional[Tuple[torch.LongTensor]] = None\n\n\n@dataclass\nclass BertModelOutputWithPoolingAndCrossAttentions(BaseModelOutputWithPoolingAndCrossAttentions):\n    token_idx: Optional[Tuple[torch.LongTensor]] = None", "\n@dataclass\nclass BertModelOutputWithPoolingAndCrossAttentions(BaseModelOutputWithPoolingAndCrossAttentions):\n    token_idx: Optional[Tuple[torch.LongTensor]] = None\n\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\"/\")\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(\n            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\",\n                  \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n            for n in name\n        ):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        try:\n            assert (\n                pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(\n            config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(\n            config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\")\n\n        self.config = config\n\n    def forward(\n        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:,\n                                             past_key_values_length: seq_length + past_key_values_length]\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings", "\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(\n            config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(\n                2 * config.max_position_embeddings - 1, self.attention_head_size)\n        self.save_attention = False\n\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[\n            :-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(\n                self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(\n                self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(\n            query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(\n                distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(\n                dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\n                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + \\\n                    relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / \\\n            math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[\n            :-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        # added `attention_scores` to return tuple\n        outputs = (context_layer, attention_probs, attention_scores) if output_attentions else (\n            context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs", "\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False):\n        super().__init__()\n        self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - \\\n            len(heads)\n        self.self.all_head_size = self.self.attention_head_size * \\\n            self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        # add attentions if we output them\n        outputs = (attention_output,) + self_outputs[1:]\n        return outputs  # (context_layer, attention_probs, attention_scores, past_key_value,)", "\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states", "\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num, token_keep_rate=1.0):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n\n        self.has_cross_attention = (layer_num >= config.fusion_layer)\n        if self.has_cross_attention:\n            self.layer_num = layer_num\n            self.crossattention = BertAttention(\n                config, is_cross_attention=True)\n\n            # sparse params\n            self.token_keep_rate = token_keep_rate\n            self.token_keep_strategy = config.token_keep_strategy\n            self.encoder_num_cls_tokens = 1  # multiple cls tokens\n\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n    \n    def sparsify(self, x, attn, mask=None):\n        x_cls, x_ = x[:, :self.encoder_num_cls_tokens], x[:, self.encoder_num_cls_tokens:]\n        assert 0 < self.token_keep_rate <= 1, \"Expected keep rate in range (0, 1]\"\n        left_tokens = math.ceil(self.token_keep_rate * x_.size(1))\n        if len(attn.shape) == 4:\n            attn = attn.mean(1)  # pool over attention heads\n\n        if self.token_keep_strategy == 'cls_attn':\n            cls_attn = attn[:, 0, self.encoder_num_cls_tokens:]\n            _, idx = torch.topk(cls_attn, left_tokens, dim=1)  # [B, left_tokens]\n\n        elif self.token_keep_strategy == 'avg_attn':\n            avg_attn = attn.mean(1)[:, self.encoder_num_cls_tokens:]\n            _, idx = torch.topk(avg_attn, left_tokens, dim=1)  # [B, left_tokens]\n\n        elif self.token_keep_strategy == 'random':\n            rand = torch.rand(x_.shape[:2], device=x_.device)\n            _, idx = torch.topk(rand, left_tokens, dim=1)  # [B, left_tokens]\n\n        else:\n            raise NotImplementedError(f\"Sparse strategy {self.token_keep_strategy} is not implemented\")\n\n        idx, _ = torch.sort(idx, dim=1)\n        index = idx.unsqueeze(-1).expand(-1, -1, x_.size(-1))  # [B, left_tokens, C]\n        outputs = torch.cat((x_cls, x_.gather(1, index)), dim=1).contiguous()\n        if mask is not None:\n            mask_cls, mask_ = mask[..., :self.encoder_num_cls_tokens], mask[..., self.encoder_num_cls_tokens:]\n            index = idx.unsqueeze(1).unsqueeze(1)  # [B, 1, 1, left_tokens]\n            mask = torch.cat((mask_cls, mask_.gather(-1, index)), dim=-1).contiguous()\n        return outputs, mask, idx\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:\n                                                  2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n        attention_output = self_attention_outputs[0]\n\n        outputs = self_attention_outputs[1:-1]\n        present_key_value = self_attention_outputs[-1]\n\n        if self.has_cross_attention:\n            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n            output_attentions = (output_attentions or self.token_keep_rate < 1)\n\n            if type(encoder_hidden_states) == list:\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states[(\n                        self.layer_num-self.config.fusion_layer) % len(encoder_hidden_states)],\n                    encoder_attention_mask[(\n                        self.layer_num-self.config.fusion_layer) % len(encoder_hidden_states)],\n                    output_attentions=output_attentions,\n                )\n                attention_output = cross_attention_outputs[0]\n                outputs = outputs + cross_attention_outputs[1:-1]\n\n            else:\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n                attention_output = cross_attention_outputs[0]\n\n                # add cross attentions if we output attention weights\n                outputs = outputs + cross_attention_outputs[1:-1]\n                \n                # node sparsification\n                if self.token_keep_rate < 1:\n                    encoder_hidden_states, encoder_attention_mask, token_keep_idx = self.sparsify(\n                        encoder_hidden_states, cross_attention_outputs[1], encoder_attention_mask)\n                    outputs = outputs + (encoder_hidden_states, encoder_attention_mask, token_keep_idx)\n\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output", "\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        # node sparsification\n        token_keep_rate = [1] * config.num_hidden_layers\n        for loc in config.token_drop_loc:\n            token_keep_rate[loc] = config.token_keep_rate\n\n        self.layer = nn.ModuleList([BertLayer(config, i, token_keep_rate[i])\n                                   for i in range(config.num_hidden_layers)])\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        output_token_idx=False,\n        return_dict=True,\n        mode='multi_modal',\n        normalize_attention=True  \n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions else None\n        all_token_idx = () if output_token_idx else None\n\n        next_decoder_cache = () if use_cache else None\n\n        if mode == 'text':\n            start_layer = 0\n            output_layer = self.config.fusion_layer\n\n        elif mode == 'fusion':\n            start_layer = self.config.fusion_layer\n            output_layer = self.config.num_hidden_layers\n\n        elif mode == 'multi_modal':\n            start_layer = 0\n            output_layer = self.config.num_hidden_layers\n\n        for i in range(start_layer, output_layer):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n                        \"`use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n            hidden_states = layer_outputs[0]\n            # update visual sequence\n            if mode == 'fusion' and layer_module.token_keep_rate < 1:\n                encoder_hidden_states, encoder_attention_mask, token_idx = layer_outputs[-4:-1]\n\n                if output_token_idx:\n                    all_token_idx = all_token_idx + (token_idx,)\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                # whether to output normalized attention, \n                # note for unnormalized attention, there is a mask added\n                offset = int(normalize_attention)\n                all_self_attentions = all_self_attentions + (layer_outputs[2-offset], )\n                if hasattr(layer_module, \"crossattention\"):\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[4-offset], )\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BertModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n            token_idx=all_token_idx\n        )", "\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output", "\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states", "\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(\n            config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states", "\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores", "\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score", "\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score", "\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(\n                mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()", "\n\n@dataclass\nclass BertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of :class:`~transformers.BertForPreTraining`.\n    Args:\n        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n            (classification) loss.\n        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n            before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n            sequence_length, sequence_length)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    prediction_logits: torch.FloatTensor = None\n    seq_relationship_logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None", "\n\nBERT_START_DOCSTRING = r\"\"\"\n    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)\n    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n    general usage and behavior.\n    Parameters:", "    general usage and behavior.\n    Parameters:\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n            weights.\n\"\"\"\n\nBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:", "BERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n            Indices of input sequence tokens in the vocabulary.\n            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n            `What are input IDs? <../glossary.html#input-ids>`__\n        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:", "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n            1]``:\n            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.", "            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.\n            `What are token type IDs? <../glossary.html#token-type-ids>`_\n        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n            config.max_position_embeddings - 1]``.\n            `What are position IDs? <../glossary.html#position-ids>`_\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n            - 1 indicates the head is **not masked**,", "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n            vectors than the model's internal embedding lookup matrix.\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.", "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n", "\n\n@add_start_docstrings(\n    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n    BERT_START_DOCSTRING,\n)\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = seq_ids[None, None, :].repeat(\n                    batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n\n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - \\\n                        causal_mask.shape[1]\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones(\n                                (batch_size, seq_length,\n                                 prefix_seq_len), device=device, dtype=causal_mask.dtype\n                            ),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )\n\n                extended_attention_mask = causal_mask[:, None,\n                                                      :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        output_token_idx=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n            device = input_ids.device\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = inputs_embeds.device\n        elif encoder_embeds is not None:\n            input_shape = encoder_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = encoder_embeds.device\n        else:\n            raise ValueError(\n                \"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)), device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape,\n                                                                                 device, is_decoder)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size(\n                )\n            else:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (\n                encoder_batch_size, encoder_sequence_length)\n\n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [\n                    self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask)\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(\n            head_mask, self.config.num_hidden_layers)\n\n        if encoder_embeds is None:\n            embedding_output = self.embeddings(\n                input_ids=input_ids,\n                position_ids=position_ids,\n                token_type_ids=token_type_ids,\n                inputs_embeds=inputs_embeds,\n                past_key_values_length=past_key_values_length,\n            )\n        else:\n            embedding_output = encoder_embeds\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_token_idx=output_token_idx,\n            return_dict=return_dict,\n            mode=mode,\n            normalize_attention=normalize_attention,\n\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(\n            sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BertModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n            token_idx=encoder_outputs.token_idx,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n    sentence prediction (classification)` head.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(\n                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(\n                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n)\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=True,\n        reduction='mean',\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores[:, :-1, :].contiguous()\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:,\n                                                          :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(reduction=reduction)\n            lm_loss = loss_fct(\n                shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(shifted_prediction_scores,\n                          dim=1)*soft_labels, dim=-1)\n            loss_distill = (loss_distill * (labels != -100)).sum(1)\n            lm_loss = (1-alpha)*lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past,\n            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx)\n                               for past_state in layer_past),)\n        return reordered_past", "\n\n@dataclass\nclass MaskedLMOutputWithDistill(MaskedLMOutput):\n    loss_aux: Optional[torch.FloatTensor] = None\n    loss_distill: Optional[torch.FloatTensor] = None\n\n\n@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def tie_aux_decoder_weights(self, module, aux_modules):\n        \"\"\"Tie decoder weights of all `aux_modules` to `module`, (not bias)\"\"\"\n        for m in aux_modules:\n            m.predictions.decoder.weight = module.predictions.decoder.weight\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_embeds=encoder_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        masked_lm_loss_aux = 0.\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(prediction_scores, dim=1)\n                          * soft_labels, dim=-1)\n            loss_distill = loss_distill[labels != -100].mean()\n            masked_lm_loss = (1-alpha)*masked_lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        # changed from MaskedLMOutput to MaskedLMOutputWithDistill\n        return MaskedLMOutputWithDistill(\n            loss=masked_lm_loss,\n            loss_aux=masked_lm_loss_aux,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        effective_batch_size = input_shape[0]\n\n        #  add a dummy token\n        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n        attention_mask = torch.cat(\n            [attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}", "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def tie_aux_decoder_weights(self, module, aux_modules):\n        \"\"\"Tie decoder weights of all `aux_modules` to `module`, (not bias)\"\"\"\n        for m in aux_modules:\n            m.predictions.decoder.weight = module.predictions.decoder.weight\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_embeds=encoder_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        masked_lm_loss_aux = 0.\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(prediction_scores, dim=1)\n                          * soft_labels, dim=-1)\n            loss_distill = loss_distill[labels != -100].mean()\n            masked_lm_loss = (1-alpha)*masked_lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        # changed from MaskedLMOutput to MaskedLMOutputWithDistill\n        return MaskedLMOutputWithDistill(\n            loss=masked_lm_loss,\n            loss_aux=masked_lm_loss_aux,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        effective_batch_size = input_shape[0]\n\n        #  add a dummy token\n        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n        attention_mask = torch.cat(\n            [attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n            >>> logits = outputs.logits\n            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n        \"\"\"\n\n        if \"next_sentence_label\" in kwargs:\n            warnings.warn(\n                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n                FutureWarning,\n            )\n            labels = kwargs.pop(\"next_sentence_label\")\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        seq_relationship_scores = self.cls(pooled_output)\n\n        next_sentence_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            next_sentence_loss = loss_fct(\n                seq_relationship_scores.view(-1, 2), labels.view(-1))\n\n        if not return_dict:\n            output = (seq_relationship_scores,) + outputs[2:]\n            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n\n        return NextSentencePredictorOutput(\n            loss=next_sentence_loss,\n            logits=seq_relationship_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)\n                                   ) if input_ids is not None else None\n        attention_mask = attention_mask.view(\n            -1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(\n            -1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)\n                                         ) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2),\n                               inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)\n                                   ) if input_ids is not None else None\n        attention_mask = attention_mask.view(\n            -1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(\n            -1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)\n                                         ) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2),\n                               inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(\n                        loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(\n                        loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ""]}
{"filename": "models/model_vqa.py", "chunked_list": ["from models.xbert import BertConfig, BertLMHeadModel\nfrom models.model_retrieval_base import SingularityRetrievalBase\nfrom models.utils import tile\nfrom einops import rearrange\n\nimport torch\nimport torch.nn.functional as F\nimport logging\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass Singularity(SingularityRetrievalBase):\n    def __init__(self, config=None, tokenizer=None):\n        super(Singularity, self).__init__(config=config, tokenizer=tokenizer, pretrain=False)\n\n        # delete extra/unnecessary modules inherited from SingularityRetrievalBase\n        extra_attributes = [\"vision_proj\", \"text_proj\", \"temp\", \"itm_head\"]\n        for attr in extra_attributes:\n            delattr(self, attr)\n        # unset BEIT pooler\n        if \"beit\" in config.vit_type:\n            self.vision_encoder.pooler = None\n\n        self.text_decoder = self.build_text_decoder()\n\n    def build_text_decoder(self):\n        logger.info(f\"Build text_decoder {self.config.text_decoder}\")\n        decoder_config = BertConfig.from_json_file(self.config.bert_config)\n        decoder_config.fusion_layer = 0\n        decoder_config.num_hidden_layers = 3  # 12 - configs/config_bert.fusion_layer\n        text_decoder, loading_info = BertLMHeadModel.from_pretrained(\n            self.config.text_decoder, config=decoder_config, output_loading_info=True)\n        if self.config.debug:\n            for k, v in loading_info.items():\n                logger.debug(f\"loading_info[{k}]: {v}\")\n        logger.info(f\"Build text_decoder {self.config.text_decoder}, done!\")\n        return text_decoder\n\n    def forward(self, image, question, answer=None, k=None, weights=None, train=True):\n        \"\"\"\n        Args:\n        k: number of answers for each question\n        weights: weight for each answer\n        \"\"\"\n        if not train and self.config.eval_frame_ensemble != \"concat\":  # default\n            return self.forward_ensemble(image, question, answer, k)\n\n        image_embeds, _ = self.encode_image(image)\n        image_atts = torch.ones(\n            image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n\n        if train:\n            answer_targets = answer.input_ids.masked_fill(\n                answer.input_ids == self.tokenizer.pad_token_id, -100)\n\n            question_output = self.text_encoder(\n                question.input_ids,\n                attention_mask=question.attention_mask,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True\n            )\n\n            question_states = []\n            question_atts = []\n            for b, n in enumerate(k):\n                question_states += [question_output.last_hidden_state[b]]*n\n                question_atts += [question.attention_mask[b]]*n\n            question_states = torch.stack(question_states, 0)\n            question_atts = torch.stack(question_atts, 0)\n\n            answer_output = self.text_decoder(\n                answer.input_ids,\n                attention_mask=answer.attention_mask,\n                encoder_hidden_states=question_states,\n                encoder_attention_mask=question_atts,\n                labels=answer_targets,\n                return_dict=True,\n                reduction='none',\n            )\n            loss = weights * answer_output.loss\n            loss = loss.sum()/image.size(0)\n\n            return loss\n\n        else:\n            question_output = self.text_encoder(\n                question.input_ids,\n                attention_mask=question.attention_mask,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True\n            )\n            topk_ids, topk_probs = self.rank_answer(\n                question_output.last_hidden_state, question.attention_mask,\n                answer.input_ids, answer.attention_mask, k\n            )  # (bsz, 128), (bsz, 128)\n            return topk_ids, topk_probs\n\n    def rank_answer(self, question_states, question_atts, answer_ids, answer_atts, k):\n        \"\"\"\n        question_states: (bsz, Lq, d)\n        answer_ids: answer input id after tokenization, (#answers, La)\n        \"\"\"\n        num_ques = question_states.size(0)\n        start_ids = answer_ids[0, 0].repeat(num_ques, 1)  # bos token\n\n        start_output = self.text_decoder(\n            start_ids,\n            encoder_hidden_states=question_states,\n            encoder_attention_mask=question_atts,\n            return_dict=True,\n            reduction='none'\n        )\n        logits = start_output.logits[:, 0, :]  # first token's logit\n\n        # topk_probs: top-k probability\n        # topk_ids: [num_question, k]\n        answer_first_token = answer_ids[:, 1]\n        prob_first_token = F.softmax(logits, dim=1).index_select(\n            dim=1, index=answer_first_token)\n        topk_probs, topk_ids = prob_first_token.topk(k, dim=1)\n\n        # answer input: [num_question*k, answer_len]\n        input_ids = []\n        input_atts = []\n        for b, topk_id in enumerate(topk_ids):\n            input_ids.append(answer_ids.index_select(dim=0, index=topk_id))\n            input_atts.append(answer_atts.index_select(dim=0, index=topk_id))\n        input_ids = torch.cat(input_ids, dim=0)\n        input_atts = torch.cat(input_atts, dim=0)\n\n        targets_ids = input_ids.masked_fill(\n            input_ids == self.tokenizer.pad_token_id, -100)\n\n        # repeat encoder's output for top-k answers\n        question_states = tile(question_states, 0, k)\n        question_atts = tile(question_atts, 0, k)\n\n        output = self.text_decoder(\n            input_ids,\n            attention_mask=input_atts,\n            encoder_hidden_states=question_states,\n            encoder_attention_mask=question_atts,\n            labels=targets_ids,\n            return_dict=True,\n            reduction='none'\n        )\n\n        answer_loss = output.loss\n        answer_loss = answer_loss.view(input_ids.size(0), -1)\n\n        # topk_prob: first token probability\n        topk_probs = topk_probs.view(-1, 1)\n        log_probs = torch.cat([topk_probs.log(), -answer_loss], dim=1)\n\n        # re-calculate log probabilities for the answer sequences using chain rule\n        log_probs_sum = log_probs.sum(1)\n        log_probs_sum = log_probs_sum.view(num_ques, k)\n\n        topk_probs = F.softmax(log_probs_sum, dim=-1)\n        # get top-k after re-ranking\n        topk_probs, rerank_id = topk_probs.topk(k, dim=1)\n        topk_ids = torch.gather(topk_ids, 1, rerank_id)\n\n        return topk_ids, topk_probs\n\n    def forward_ensemble(self, image, question, answer=None, k=None):\n        \"\"\"\n        Args:\n        k: number of answers for each question\n        weights: weight for each answer\n        \"\"\"\n        # assert self.config.video_input.num_frames == 1, \"only support single-frame\"\n        bsz = image.shape[0]\n        num_frames = self.config.video_input.num_frames\n        assert image.shape[1] % num_frames == 0, \"video_input.num_frames_test must be multiples of video_input.num_frames\"\n        assert self.config.eval_frame_ensemble in [\"mean\", \"max\", \"lse\"]\n\n        if getattr(self.config, 'eval_sparse_sampling', False):\n            image = rearrange(image, 'b (n k) c h w -> (b k) n c h w', n=num_frames)\n        else:\n            image = rearrange(image, 'b (k n) c h w -> (b k) n c h w', n=num_frames)\n            \n        image_embeds, _ = self.encode_image(image)   # (bsz, #frm, L, d)\n        image_embeds = rearrange(image_embeds, '(b k) n d -> b k n d', b=bsz)\n        # image_embeds, _ = self._encode_image(image)   # (bsz, #frm, L, d)\n        num_clips = image_embeds.shape[1]\n\n        image_atts = torch.ones(\n            (image_embeds.shape[0], image_embeds.shape[2]), dtype=torch.long).to(image.device)\n\n        question_states_list = []\n        for idx in range(num_clips):\n            question_output = self.text_encoder(\n                question.input_ids,\n                attention_mask=question.attention_mask,\n                encoder_hidden_states=image_embeds[:, idx],\n                encoder_attention_mask=image_atts,\n                return_dict=True\n            )\n            question_states_list.append(question_output.last_hidden_state)\n        topk_ids, topk_probs = self.rank_answer_ensemble(\n            question_states_list, question.attention_mask,\n            answer.input_ids, answer.attention_mask, k\n        )  # (bsz, 128), (bsz, 128)\n        return topk_ids, topk_probs\n\n    def rank_answer_ensemble(self, question_states, question_atts, answer_ids, answer_atts, k):\n        \"\"\"\n        question_states: list( (bsz, Lq, d), )\n        answer_ids: answer input id after tokenization, (#answers, La)\n        \"\"\"\n        num_ques = question_states[0].size(0)\n        start_ids = answer_ids[0, 0].repeat(num_ques, 1)  # bos token\n\n        logits_list = []\n        for q_state in question_states:\n            start_output = self.text_decoder(\n                start_ids,\n                encoder_hidden_states=q_state,\n                encoder_attention_mask=question_atts,\n                return_dict=True,\n                reduction='none'\n            )\n            logits = start_output.logits[:, 0, :]  # first token's logit\n            logits_list.append(F.softmax(logits, dim=1))\n        logits_all = torch.stack(logits_list)  # (num_clips, #vocab)\n        if self.config.eval_frame_ensemble == \"mean\":\n            logits = logits_all.mean(0)\n        elif self.config.eval_frame_ensemble == \"max\":\n            logits = logits_all.max(0)[0]\n        elif self.config.eval_frame_ensemble == \"lse\":  # LogSumExp\n            logits = torch.logsumexp(logits_all, dim=0)\n        else:\n            raise ValueError(\"config.eval_frame_ensemble must in [mean, max, lse] when #clip > 1.\")\n\n        # topk_probs: top-k probability\n        # topk_ids: [num_question, k]\n        answer_first_token = answer_ids[:, 1]\n        prob_first_token = F.softmax(logits, dim=1).index_select(\n            dim=1, index=answer_first_token)\n        topk_probs, topk_ids = prob_first_token.topk(k, dim=1)\n\n        # answer input: [num_question*k, answer_len]\n        input_ids = []\n        input_atts = []\n        for b, topk_id in enumerate(topk_ids):\n            input_ids.append(answer_ids.index_select(dim=0, index=topk_id))\n            input_atts.append(answer_atts.index_select(dim=0, index=topk_id))\n        input_ids = torch.cat(input_ids, dim=0)\n        input_atts = torch.cat(input_atts, dim=0)\n\n        targets_ids = input_ids.masked_fill(\n            input_ids == self.tokenizer.pad_token_id, -100)\n\n        log_topk_probs = topk_probs.view(-1, 1).log()\n        logits_list = []\n        question_atts = tile(question_atts, 0, k)\n        for q_state in question_states:\n            # repeat encoder's output for top-k answers\n            q_state = tile(q_state, 0, k)\n\n            output = self.text_decoder(\n                input_ids,\n                attention_mask=input_atts,\n                encoder_hidden_states=q_state,\n                encoder_attention_mask=question_atts,\n                labels=targets_ids,\n                return_dict=True,\n                reduction='none'\n            )\n\n            answer_loss = output.loss\n            answer_loss = answer_loss.view(input_ids.size(0), -1)\n\n            # topk_prob: first token probability\n\n            log_probs = torch.cat([log_topk_probs, -answer_loss], dim=1)\n\n            # re-calculate log probabilities for the answer sequences using chain rule\n            log_probs_sum = log_probs.sum(1)\n            log_probs_sum = log_probs_sum.view(num_ques, k)\n\n            # logits = F.softmax(log_probs_sum, dim=-1)\n            logits = log_probs_sum\n            logits_list.append(logits)\n\n        # ensemble\n        logits_all = torch.stack(logits_list)  # (num_clips, #question, k)\n        if self.config.eval_frame_ensemble == \"mean\":\n            logits = logits_all.mean(0)\n        elif self.config.eval_frame_ensemble == \"max\":\n            logits = logits_all.max(0)[0]\n        elif self.config.eval_frame_ensemble == \"lse\":  # LogSumExp\n            logits = torch.logsumexp(logits_all, dim=0)\n        else:\n            raise ValueError(\"config.eval_frame_ensemble must in [mean, max, lse] when #clip > 1.\")\n        topk_probs = F.softmax(logits, dim=1)\n\n        # get top-k after re-ranking\n        topk_probs, rerank_id = topk_probs.topk(k, dim=1)\n        topk_ids = torch.gather(topk_ids, 1, rerank_id)\n\n        return topk_ids, topk_probs", "\n"]}
{"filename": "models/sparse_xbeit.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 Google AI, Ross Wightman, The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch BEiT model. \"\"\"\n\n\nimport collections.abc", "\nimport collections.abc\nimport math\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport zCurve\nimport hilbert\n\nimport torch", "\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom einops import rearrange, repeat\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, MaskedLMOutput, SequenceClassifierOutput", "from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, MaskedLMOutput, SequenceClassifierOutput\nfrom transformers.modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import logging\nfrom models.sparse_config import BeitConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"BeitConfig\"", "\n_CONFIG_FOR_DOC = \"BeitConfig\"\n_CHECKPOINT_FOR_DOC = \"microsoft/beit-base-patch16-224\"\n\nBEIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"microsoft/beit-base-patch16-224\",\n    # See all BEiT models at https://huggingface.co/models?filter=beit\n]\n\n", "\n\n@dataclass\nclass BeitModelOutputWithPooling(BaseModelOutputWithPooling):\n    \"\"\"\n    Class for outputs of :class:`~transformers.BeitModel`.\n\n    Args:\n        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n            Average of the last layer hidden states of the patch tokens (excluding the `[CLS]` token) if\n            `config.use_mean_pooling` is set to True. If set to False, then the final hidden state of the `[CLS]` token\n            will be returned.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n            sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n    token_idx: Optional[Tuple[torch.LongTensor]] = None", "\n\n@dataclass\nclass BeitModelOutput(BaseModelOutput):\n    token_idx: Optional[Tuple[torch.LongTensor]] = None\n\n\n# Inspired by\n# https://github.com/rwightman/pytorch-image-models/blob/b9bd960a032c75ca6b808ddeed76bee5f3ed4972/timm/models/layers/helpers.py\n# From PyTorch internals\ndef to_2tuple(x):\n    if isinstance(x, collections.abc.Iterable):\n        return x\n    return (x, x)", "# https://github.com/rwightman/pytorch-image-models/blob/b9bd960a032c75ca6b808ddeed76bee5f3ed4972/timm/models/layers/helpers.py\n# From PyTorch internals\ndef to_2tuple(x):\n    if isinstance(x, collections.abc.Iterable):\n        return x\n    return (x, x)\n\n\n# Based on https://github.com/rwightman/pytorch-image-models/blob/a2727c1bf78ba0d7b5727f5f95e37fb7f8866b1f/timm/models/layers/drop.py\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n    argument.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output", "# Based on https://github.com/rwightman/pytorch-image-models/blob/a2727c1bf78ba0d7b5727f5f95e37fb7f8866b1f/timm/models/layers/drop.py\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n    argument.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output", "\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)", "\n\n# Based on timm implementation, which can be found here:\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\nclass BeitEmbeddings(nn.Module):\n    \"\"\"\n    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n        if config.use_mask_token:\n            self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n        else:\n            self.mask_token = None\n        self.patch_embeddings = PatchEmbeddings(\n            image_size=config.image_size,\n            patch_size=config.patch_size,\n            num_channels=config.num_channels,\n            embed_dim=config.hidden_size,\n        )\n        num_patches = self.patch_embeddings.num_patches\n        if config.use_absolute_position_embeddings:\n            self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n        else:\n            self.position_embeddings = None\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, pixel_values, bool_masked_pos=None):\n\n        if pixel_values.ndim == 5:  # video input=\n            embeddings = self.patch_embeddings(pixel_values.flatten(0, 1))\n            embeddings = rearrange(embeddings, '(b m) n d -> b (m n) d', m=pixel_values.shape[1])\n        else:  # image input\n            embeddings = self.patch_embeddings(pixel_values)\n            \n        batch_size, seq_len, _ = embeddings.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        if bool_masked_pos is not None:\n            mask_tokens = self.mask_token.expand(batch_size, seq_len, -1)\n            # replace the masked visual tokens by mask_tokens\n            w = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n            embeddings = embeddings * (1 - w) + mask_tokens * w\n\n        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n        if self.position_embeddings is not None:\n            embeddings = embeddings + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n\n        return embeddings", "\n\n# Based on timm implementation, which can be found here:\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\nclass PatchEmbeddings(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    \"\"\"\n\n    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n        super().__init__()\n        image_size = to_2tuple(image_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n        patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.patch_shape = patch_shape\n\n        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, pixel_values):\n        batch_size, num_channels, height, width = pixel_values.shape\n        # FIXME look at relaxing size constraints\n        if height != self.image_size[0] or width != self.image_size[1]:\n            raise ValueError(\n                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n            )\n        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n\n        return x", "\n\nclass BeitSelfAttention(nn.Module):\n    def __init__(self, config, window_size=None):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n                f\"heads {config.num_attention_heads}.\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        # sparse params\n        self.random_attn = config.sparse_random_attn\n        self.local_attn = config.sparse_local_attn\n        self.block_size = config.attn_block_size\n        self.num_cls_tokens = config.num_cls_tokens\n        if self.local_attn is not None and self.random_attn is not None:\n            self.num_kv_blocks = self.local_attn + self.random_attn\n\n        if window_size:\n            self.relative_position_bias = BeitRelativePositionBias3D(config, window_size=window_size)\n        else:\n            self.relative_position_bias = None\n    \n    def split_heads(self, x):\n        return rearrange(x, 'b n (h d) -> b h n d', h=self.num_attention_heads)\n    \n    def join_heads(self, x):\n        return rearrange(x, 'b h n d -> b n (h d)')\n    \n    def blockify(self, x):\n        assert x.dim() == 4, f\"Unsupported input shape {x.shape}\"\n        seq_len = x.shape[2]\n        if seq_len % self.block_size > 0:  # seq_len not divisible by block_size, zero pad\n            pad_len = self.block_size - seq_len % self.block_size\n            x = nn.functional.pad(x, (0, 0, 0, pad_len))\n        else:\n            pad_len = 0\n        x = rearrange(x, 'b h (m n) d -> b h m n d', n=self.block_size)\n        return x, pad_len\n    \n    def dense_attention(self, q, k, v, head_mask=None, relative_position_bias=None, q_idx=None, k_idx=None):\n        # q, k, v: (bsz, num_heads, seq_len, dims)\n        assert k.shape[2] == v.shape[2], \"Key and value shapes mismatch\"\n        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n        sim = sim / math.sqrt(self.attention_head_size)\n\n        # Add relative position bias if present.\n        if self.relative_position_bias is not None:\n            if q_idx is not None and q_idx.ndim == 2:\n                assert k_idx is not None and len(q_idx) == len(k_idx)\n                bias = torch.stack([\n                    self.relative_position_bias(from_idx=q_idx_, to_idx=k_idx_)\n                    for q_idx_, k_idx_ in zip(q_idx, k_idx)\n                ])\n            else:\n                bias = self.relative_position_bias(from_idx=q_idx, to_idx=k_idx).unsqueeze(0)\n            sim = sim + bias\n\n        # Add shared relative position bias if provided.\n        if relative_position_bias is not None:\n            sim = sim + relative_position_bias\n\n        # Normalize the attention scores to probabilities.\n        attn = sim.softmax(dim=-1)\n        attn = self.dropout(attn)\n        if head_mask is not None:\n            attn = attn * head_mask\n\n        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n        return out, attn\n    \n    def _sparse_attn_relative_position_bias(self, q_idx, pad_q, attn_idx, group_len):\n        q_idx_blk = nn.functional.pad(q_idx, (0, pad_q)).view(-1, self.block_size)\n        attn_idx_flt = rearrange(q_idx_blk[attn_idx], 'm n j -> m (n j)')  # (seq_len, num_kv_blocks * group_len)\n        cls_idx = torch.arange(self.num_cls_tokens, device=q_idx.device)\n        cls_idx = repeat(cls_idx, 'n -> m n', m=len(attn_idx_flt))\n        attn_idx_flt = torch.cat((cls_idx, attn_idx_flt), dim=1)\n        attn_idx_flt = repeat(attn_idx_flt, 'm n -> (m i) n', i=group_len)\n        if pad_q > 0:\n            attn_idx_flt = attn_idx_flt[:-pad_q]\n        bias_flt = self.relative_position_bias(from_idx=q_idx, to_idx=attn_idx_flt)\n        if pad_q > 0:\n            bias_flt = nn.functional.pad(bias_flt, (0, 0, 0, pad_q))\n        return rearrange(bias_flt, 'h (m i) n -> h m i n', i=group_len)  # num_heads, seq_len, group_len, (num_kv_blocks * group_len + num_cls_tokens)\n    \n    def sparse_attention(self, q, k, v, head_mask=None, relative_position_bias=None, q_idx=None, mimic_full=False):\n        assert self.local_attn == 0 or self.local_attn % 2 == 1, \"Even local window size not supported\"\n        assert k.shape[2] == v.shape[2], \"Key and value shapes mismatch\"\n\n        \n        if not mimic_full:\n            cls_k, k = k[..., :self.num_cls_tokens, :], k[..., self.num_cls_tokens:, :]  # cls_k: (bsz, num_heads, num_cls_tokens, dims)\n            cls_v, v = v[..., :self.num_cls_tokens, :], v[..., self.num_cls_tokens:, :]\n\n        # pad token sequence to multiples of block_size\n        if mimic_full:\n            bsz, num_heads, seq_len, dims = q.shape\n        else:\n            q, pad_q = self.blockify(q)  # q: (bsz, num_heads, seq_len, group_len, dims)\n            k, pad_k = self.blockify(k)\n            v, pad_v = self.blockify(v)\n            bsz, num_heads, seq_len, group_len, dims = q.shape\n\n            # global attention\n            cls_sim = torch.einsum('b h n i d, b h j d -> b h n i j', q, cls_k)  # (bsz, num_heads, seq_len, group_len, num_cls_tokens)\n\n        if mimic_full:\n            sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n            sim = sim / math.sqrt(self.attention_head_size)\n            sim = sim + self.relative_position_bias(from_idx=q_idx).unsqueeze(0)\n        \n        else:\n            # initialize empty sim matrix\n            sim = torch.empty((bsz, num_heads, seq_len, self.num_kv_blocks, group_len, group_len), device=q.device)\n            attn_idx = torch.zeros((seq_len, self.num_kv_blocks), dtype=torch.int64, device=q.device)\n\n            # local window attention\n            cnt = 0\n            if self.local_attn > 0:\n                num_rolls = self.local_attn // 2\n                for r in range(-num_rolls, num_rolls + 1):\n                    sim[..., cnt, :, :] = torch.einsum('b h n i d, b h n j d -> b h n i j', q, k.roll(-r, dims=2))\n                    attn_idx[:, cnt] = torch.arange(seq_len, device=q.device).roll(r)\n                    cnt += 1\n            \n            # random attention\n            if self.random_attn > 0:\n                # generate random attention pattern\n                rand = torch.rand((seq_len, seq_len), device=q.device)\n                if self.local_attn > 0:\n                    # avoid overlap with local attention\n                    for r in range(-num_rolls, num_rolls + 1):\n                        tgt_idx = list(i % seq_len for i in range(r, seq_len + r))\n                        rand[range(seq_len), tgt_idx] = 0\n                _, idx = rand.topk(self.random_attn, dim=-1)  # seq_len, random_attn\n                idx, _ = torch.sort(idx, dim=1)\n                attn_idx[:, cnt:] = idx\n\n                idx_ = repeat(idx, 'n m -> b h n m i d', b=bsz, h=num_heads, i=group_len, d=dims)\n\n                for r in range(self.random_attn):\n                    sim[..., cnt, :, :] = torch.einsum('b h n i d, b h n j d -> b h n i j', q, k.gather(2, idx_[..., r, :, :]))\n                    cnt += 1\n\n            sim = rearrange(sim, 'b h m n i j -> b h m i (n j)')  # (bsz, num_heads, seq_len, group_len, num_kv_blocks * group_len)\n            sim = torch.cat((cls_sim, sim), -1)\n            sim = sim / math.sqrt(self.attention_head_size)\n\n            # Add relative position bias if present.\n            # NOTE: we assume q and k (excluding cls) use same token indexing, for relative position embedding\n            if self.relative_position_bias is not None:\n                assert q_idx is not None, \"query index required for relative position bias\"\n                if q_idx.ndim == 2:\n                    # different indices for each sample\n                    bias = torch.stack([\n                        self._sparse_attn_relative_position_bias(q_idx_, pad_q, attn_idx, group_len)\n                        for q_idx_ in q_idx\n                    ])\n                else:\n                    bias = self._sparse_attn_relative_position_bias(q_idx, pad_q, attn_idx, group_len).unsqueeze(0)\n                sim = sim + bias\n\n        # Add shared relative position bias if provided.\n        if relative_position_bias is not None:\n            raise NotImplementedError\n            sim = sim + relative_position_bias\n\n        attn = sim.softmax(dim=-1)\n        attn = self.dropout(attn)\n        if head_mask is not None:\n            attn = attn * head_mask\n\n        # block attention\n        if mimic_full:\n            out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n\n        else:\n            out = torch.empty((bsz, num_heads, seq_len, group_len, dims), device=q.device)\n            for m in range(seq_len):\n                v_row = torch.index_select(v, 2, attn_idx[m])\n                v_row = rearrange(v_row, 'b h n j d -> b h (n j) d')  # (bsz, num_heads, num_kv_blocks * group_len, dims)\n                v_row = torch.cat((cls_v, v_row), 2)\n                out[..., m, :, :] = torch.einsum('b h i j, b h j d -> b h i d', attn[..., m, :, :], v_row)\n            out = rearrange(out, 'b h n i d -> b h (n i) d')\n            if pad_q > 0:\n                out = out[..., :-pad_q, :]\n\n        return out, attn\n        \n    def forward(self, hidden_states, head_mask=None, output_attentions=False, relative_position_bias=None, token_idx=None):\n        # compute qkv\n        q = self.split_heads(self.query(hidden_states))\n        k = self.split_heads(self.key(hidden_states))\n        v = self.split_heads(self.value(hidden_states))\n        \n        # combine local token_idx with cls tokens\n        # NOTE: assume token_idx starts from 0\n        cls_q_idx = torch.arange(self.num_cls_tokens, device=q.device)\n        if token_idx is not None:\n            if token_idx.ndim == 2:\n                cls_q_idx = repeat(cls_q_idx, 'n -> b n', b=q.shape[0])\n            all_token_idx = torch.cat((cls_q_idx, token_idx + self.num_cls_tokens), dim=-1)\n        else:\n            all_token_idx = None\n\n        if self.random_attn is None:\n            outputs, attention_probs = self.dense_attention(q, k, v, head_mask=head_mask,\n                                                            relative_position_bias=relative_position_bias,\n                                                            q_idx=all_token_idx,\n                                                            k_idx=all_token_idx)\n            cls_attention_probs = attention_probs[..., :self.num_cls_tokens, :]\n\n        else:\n            cls_q, q = q[..., :self.num_cls_tokens, :], q[..., self.num_cls_tokens:, :]\n\n            # dense global attention (num_cls_tokens, seq_len)\n            cls_outputs, cls_attention_probs = self.dense_attention(cls_q, k, v, head_mask=head_mask,\n                                                                    relative_position_bias=relative_position_bias,\n                                                                    q_idx=cls_q_idx,\n                                                                    k_idx=all_token_idx)\n\n            # sparse local attention (local_seq_len, seq_len)\n            if token_idx is None:\n                token_idx = torch.arange(q.shape[-2], device=q.device)\n            outputs, attention_probs = self.sparse_attention(q, k, v, head_mask=head_mask,\n                                                             relative_position_bias=relative_position_bias,\n                                                             q_idx=token_idx + self.num_cls_tokens)\n\n            outputs = torch.cat((cls_outputs, outputs), dim=2)\n        \n        outputs = self.join_heads(outputs)\n\n        outputs = (outputs, cls_attention_probs) if output_attentions else (outputs,)\n\n        return outputs", "\n\nclass BeitSelfOutput(nn.Module):\n    \"\"\"\n    The residual connection is defined in BeitLayer instead of here (as is the case with other models), due to the\n    layernorm applied before each block.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, gamma=None):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        return hidden_states", "\n\nclass BeitAttention(nn.Module):\n    def __init__(self, config, window_size=None):\n        super().__init__()\n        self.attention = BeitSelfAttention(config, window_size=window_size)\n        self.output = BeitSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.attention.query = prune_linear_layer(self.attention.query, index)\n        self.attention.key = prune_linear_layer(self.attention.key, index)\n        self.attention.value = prune_linear_layer(self.attention.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, hidden_states, head_mask=None, output_attentions=False, relative_position_bias=None, token_idx=None):\n        self_outputs = self.attention(hidden_states, head_mask, output_attentions, relative_position_bias, token_idx)\n\n        attention_output = self.output(self_outputs[0], hidden_states)\n\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs", "\n\nclass BeitIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n\n        return hidden_states", "\n\nclass BeitOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        return hidden_states", "\n\nclass BeitLayer(nn.Module):\n    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n\n    def __init__(self, config, window_size=None, drop_path_rate=0.0, \n                 token_keep_rate=1.0):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BeitAttention(config, window_size=window_size)\n        self.intermediate = BeitIntermediate(config)\n        self.output = BeitOutput(config)\n        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        # sparse params\n        self.token_keep_rate = token_keep_rate\n        self.token_keep_strategy = config.token_keep_strategy\n        self.num_cls_tokens = config.num_cls_tokens\n\n        init_values = config.layer_scale_init_value\n        if init_values > 0:\n            self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n            self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n        else:\n            self.lambda_1, self.lambda_2 = None, None\n    \n    def sparsify(self, x, attn):\n        x_cls, x_ = x[:, :self.num_cls_tokens], x[:, self.num_cls_tokens:]\n        assert 0 < self.token_keep_rate <= 1, \"Expected keep rate in range (0, 1]\"\n        left_tokens = math.ceil(self.token_keep_rate * x_.size(1))\n\n        if self.token_keep_strategy == 'cls_attn':\n            if len(attn.shape) == 4:\n                attn = attn.mean(1)  # pool over attention heads\n            cls_attn = attn[:, 0, self.num_cls_tokens:]\n            _, idx = torch.topk(cls_attn, left_tokens, dim=1)  # [B, left_tokens]\n\n        elif self.token_keep_strategy == 'random':\n            rand = torch.rand(x_.shape[:2], device=x_.device)\n            _, idx = torch.topk(rand, left_tokens, dim=1)  # [B, left_tokens]\n\n        else:\n            raise NotImplementedError(f\"Sparse strategy {self.token_keep_strategy} is not implemented\")\n\n        idx, _ = torch.sort(idx, dim=1)\n        index = idx.unsqueeze(-1).expand(-1, -1, x_.size(-1))  # [B, left_tokens, C]\n        outputs = torch.cat((x_cls, x_.gather(1, index)), dim=1).contiguous()\n        return outputs, idx\n\n    def forward(self, hidden_states, head_mask=None, output_attentions=False, relative_position_bias=None, token_idx=None):\n        self_attention_outputs = self.attention(\n            self.layernorm_before(hidden_states),  # in BEiT, layernorm is applied before self-attention\n            head_mask,\n            output_attentions=(output_attentions or self.token_keep_rate < 1),\n            relative_position_bias=relative_position_bias,\n            token_idx=token_idx\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        # apply lambda_1 if present\n        if self.lambda_1 is not None:\n            attention_output = self.lambda_1 * attention_output\n\n        # first residual connection\n        hidden_states = self.drop_path(attention_output) + hidden_states\n\n        # in BEiT, layernorm is also applied after self-attention\n        layer_output = self.layernorm_after(hidden_states)\n\n        layer_output = self.intermediate(layer_output)\n        layer_output = self.output(layer_output)\n\n        if self.lambda_2 is not None:\n            layer_output = self.lambda_2 * layer_output\n\n        # second residual connection\n        layer_output = self.drop_path(layer_output) + hidden_states\n\n        # node sparsification\n        if self.token_keep_rate < 1:\n            layer_output, token_keep_idx = self.sparsify(layer_output, outputs[0])\n            if token_idx is not None:\n                if token_idx.ndim == 1:\n                    token_idx = repeat(token_idx, 'n -> b n', b=len(token_keep_idx))\n                token_keep_idx = token_idx.gather(1, token_keep_idx)\n            outputs = outputs + (token_keep_idx,)\n\n        outputs = (layer_output,) + outputs\n\n        return outputs", "\n\nclass BeitRelativePositionBias(nn.Module):\n    def __init__(self, config, window_size):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, config.num_attention_heads)\n        )  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(\n            size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype\n        )\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index, persistent=False)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1\n        )  # Wh*Ww,Wh*Ww,nH\n\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww", "\n\nclass BeitRelativePositionBias3D(nn.Module):\n    \"\"\"\n    3D relative position bias\n    \"\"\"\n    def __init__(self, config, window_size, num_cls_tokens=1):\n        super().__init__()\n        self.window_size = window_size\n        self.num_cls_tokens = num_cls_tokens\n        \n        relative_size = [w * 2 - 1 for w in window_size]\n        self.num_relative_distance = np.prod(relative_size) + 2 * num_cls_tokens + num_cls_tokens ** 2\n\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, config.num_attention_heads)\n        )\n\n        # get pair-wise relative position index for each token inside the window\n        coords_range = [torch.arange(w) for w in window_size]\n        coords_flatten = torch.stack(torch.meshgrid(coords_range)).flatten(1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n        \n        for i, w in enumerate(window_size):\n            relative_coords[:, :, i] += w - 1  # shift to start from 0\n        \n        for i, r in enumerate(relative_size[1:]):\n            relative_coords[:, :, :i + 1] *= r\n\n        self.seq_len = np.prod(window_size) + num_cls_tokens\n        relative_position_index = torch.zeros((self.seq_len, self.seq_len), dtype=relative_coords.dtype)\n        relative_position_index[num_cls_tokens:, num_cls_tokens:] = relative_coords.sum(-1)\n        \n        start = np.prod(relative_size)\n        cls2loc = torch.arange(num_cls_tokens).unsqueeze(1) + start\n        relative_position_index[:num_cls_tokens, num_cls_tokens:] = cls2loc\n        start += num_cls_tokens\n\n        loc2cls = torch.arange(num_cls_tokens).unsqueeze(0) + start\n        relative_position_index[num_cls_tokens:, :num_cls_tokens] = loc2cls\n        start += num_cls_tokens\n\n        cls2cls = torch.arange(num_cls_tokens ** 2).view(num_cls_tokens, num_cls_tokens) + start\n        relative_position_index[:num_cls_tokens, :num_cls_tokens] = cls2cls\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self, from_idx=None, to_idx=None):\n        \"\"\"\n        from_idx: indices of query tokens (1-dim)\n        to_idx: indices of key/value tokens (1-dim, or 2-dim w/ one row per query)\n        \"\"\"\n        attn_idx = self.relative_position_index\n\n        # query indices\n        if from_idx is not None:\n            attn_idx = attn_idx[from_idx]\n\n        # key indices\n        if to_idx is not None:\n            assert to_idx.ndim in (1, 2), \"to_idx must be 1- or 2-dimensional tensors\"\n            if to_idx.ndim == 1:\n                attn_idx = attn_idx[:, to_idx]\n            else:\n                attn_idx = attn_idx.gather(1, to_idx)\n\n        rows, cols = attn_idx.shape\n        relative_position_bias = self.relative_position_bias_table[attn_idx.flatten()]\n        relative_position_bias = rearrange(relative_position_bias, '(i j) h -> h i j', i=rows, j=cols)\n        return relative_position_bias.contiguous()", "\n\nclass BeitEncoder(nn.Module):\n    def __init__(self, config, window_size=None):\n        super().__init__()\n        self.config = config\n        if config.use_shared_relative_position_bias:\n            self.relative_position_bias = BeitRelativePositionBias3D(config, window_size=window_size)\n        else:\n            self.relative_position_bias = None\n\n        self._register_token_order(window_size)\n\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n\n        # node sparsification\n        token_keep_rate = [1] * config.num_hidden_layers\n        for loc in config.token_drop_loc:\n            token_keep_rate[loc] = config.token_keep_rate\n        \n        self.layer = nn.ModuleList(\n            [\n                BeitLayer(\n                    config,\n                    window_size=window_size if config.use_relative_position_bias else None,\n                    drop_path_rate=dpr[i], token_keep_rate=token_keep_rate[i]\n                )\n                for i in range(config.num_hidden_layers)\n            ]\n        )\n\n        self.gradient_checkpointing = False\n    \n    def _register_token_order(self, shape):\n        if self.config.token_3d_order == 'none':\n            order = None\n        elif self.config.token_3d_order == 'zcurve':\n            nbits = max(shape).bit_length()\n            coords = list(np.ndindex(*shape))\n            order = zCurve.par_interlace(coords, len(shape), nbits)\n            order = torch.tensor(np.argsort(order))\n        elif self.config.token_3d_order == 'hilbert':\n            nbits = max(shape).bit_length()\n            coords = list(np.ndindex(*shape))\n            order = hilbert.encode(np.stack(coords), len(shape), nbits)\n            order = torch.tensor(np.argsort(order))\n        else:\n            raise NotImplementedError(f\"Token ordering {self.config.token_3d_order} not supported\")\n\n        if order is not None:\n            self.register_buffer('token_order', order, persistent=False)\n        else:\n            self.token_order = None\n\n    def forward(\n        self,\n        hidden_states,\n        head_mask=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        output_token_idx=False,\n        return_dict=True,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_token_idx = () if output_token_idx else None\n\n        token_idx = self.token_order\n        if token_idx is not None:\n            cls_states, local_states = hidden_states[:, :self.config.num_cls_tokens], hidden_states[:, self.config.num_cls_tokens:]\n            local_states = torch.index_select(local_states, dim=1, index=token_idx)\n            hidden_states = torch.cat((cls_states, local_states), 1)\n\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    layer_head_mask,\n                )\n            else:\n                relative_position_bias = (\n                    self.relative_position_bias() if self.relative_position_bias is not None else None\n                )\n                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions, relative_position_bias, token_idx)\n\n            hidden_states = layer_outputs[0]\n\n            if layer_module.token_keep_rate < 1:\n                token_idx = layer_outputs[-1]\n\n                if output_token_idx:\n                    all_token_idx = all_token_idx + (token_idx,)\n\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n        return BeitModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            token_idx=all_token_idx\n        )", "\n\nclass BeitPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BeitConfig\n    base_model_prefix = \"beit\"\n    supports_gradient_checkpointing = True\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, BeitEncoder):\n            module.gradient_checkpointing = value", "\n\nBEIT_START_DOCSTRING = r\"\"\"\n    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use\n    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n    behavior.\n\n    Parameters:\n        config (:class:`~transformers.BeitConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the", "        config (:class:`~transformers.BeitConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n            weights.\n\"\"\"\n\nBEIT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        pixel_values (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_channels, height, width)`):\n            Pixel values. Pixel values can be obtained using :class:`~transformers.BeitFeatureExtractor`. See", "        pixel_values (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_channels, height, width)`):\n            Pixel values. Pixel values can be obtained using :class:`~transformers.BeitFeatureExtractor`. See\n            :meth:`transformers.BeitFeatureExtractor.__call__` for details.\n\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n", "            - 0 indicates the head is **masked**.\n\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.", "        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Beit Model transformer outputting raw hidden-states without any specific head on top.\",\n    BEIT_START_DOCSTRING,\n)\nclass BeitModel(BeitPreTrainedModel):\n    def __init__(self, config, add_pooling_layer=True, num_frames=None):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BeitEmbeddings(config)\n        self.window_size = self.embeddings.patch_embeddings.patch_shape\n        if num_frames is not None:\n            self.window_size = (num_frames,) + self.window_size\n        self.encoder = BeitEncoder(config, window_size=self.window_size)\n\n        self.layernorm = (\n            nn.Identity() if config.use_mean_pooling else nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        )\n        self.pooler = BeitPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.patch_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=BeitModelOutputWithPooling, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        bool_masked_pos=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        output_token_idx=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitModel\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n            >>> model = BeitModel.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if pixel_values is None:\n            raise ValueError(\"You have to specify pixel_values\")\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(pixel_values, bool_masked_pos)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_token_idx=output_token_idx,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        sequence_output = self.layernorm(sequence_output)\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BeitModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            token_idx=encoder_outputs.token_idx,\n        )", ")\nclass BeitModel(BeitPreTrainedModel):\n    def __init__(self, config, add_pooling_layer=True, num_frames=None):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BeitEmbeddings(config)\n        self.window_size = self.embeddings.patch_embeddings.patch_shape\n        if num_frames is not None:\n            self.window_size = (num_frames,) + self.window_size\n        self.encoder = BeitEncoder(config, window_size=self.window_size)\n\n        self.layernorm = (\n            nn.Identity() if config.use_mean_pooling else nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        )\n        self.pooler = BeitPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.patch_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=BeitModelOutputWithPooling, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        bool_masked_pos=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        output_token_idx=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitModel\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n            >>> model = BeitModel.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if pixel_values is None:\n            raise ValueError(\"You have to specify pixel_values\")\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(pixel_values, bool_masked_pos)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_token_idx=output_token_idx,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        sequence_output = self.layernorm(sequence_output)\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BeitModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            token_idx=encoder_outputs.token_idx,\n        )", "\n\nclass BeitPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layernorm = (\n            nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) if config.use_mean_pooling else None\n        )\n\n    def forward(self, hidden_states):\n        if self.layernorm is not None:\n            # Mean pool the final hidden states of the patch tokens\n            patch_tokens = hidden_states[:, 1:, :]\n            pooled_output = self.layernorm(patch_tokens.mean(1))\n        else:\n            # Pool by simply taking the final hidden state of the [CLS] token\n            pooled_output = hidden_states[:, 0]\n\n        return pooled_output", "\n\n@add_start_docstrings(\n    \"Beit Model transformer with a 'language' modeling head on top (to predict visual tokens).\",\n    BEIT_START_DOCSTRING,\n)\nclass BeitForMaskedImageModeling(BeitPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.beit = BeitModel(config, add_pooling_layer=False)\n\n        # Classifier head\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        bool_masked_pos=None,\n        head_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        bool_masked_pos (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, num_patches)`):\n            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the image classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitForMaskedImageModeling\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224-pt22k')\n            >>> model = BeitForMaskedImageModeling.from_pretrained('microsoft/beit-base-patch16-224-pt22k')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.beit(\n            pixel_values,\n            bool_masked_pos=bool_masked_pos,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        sequence_output = self.layernorm(sequence_output)\n        prediction_scores = self.lm_head(sequence_output[:, 1:])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(prediction_scores[bool_masked_pos], labels)\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final\n    hidden states of the patch tokens) e.g. for ImageNet.\n    \"\"\",\n    BEIT_START_DOCSTRING,\n)\nclass BeitForImageClassification(BeitPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.beit = BeitModel(config, add_pooling_layer=True)\n\n        # Classifier head\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        head_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the image classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitForImageClassification\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224')\n            >>> model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> logits = outputs.logits\n            >>> # model predicts one of the 1000 ImageNet classes\n            >>> predicted_class_idx = logits.argmax(-1).item()\n            >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.beit(\n            pixel_values,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs.pooler_output if return_dict else outputs[1]\n\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BeitForImageClassification(BeitPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.beit = BeitModel(config, add_pooling_layer=True)\n\n        # Classifier head\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        head_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the image classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitForImageClassification\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224')\n            >>> model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> logits = outputs.logits\n            >>> # model predicts one of the 1000 ImageNet classes\n            >>> predicted_class_idx = logits.argmax(-1).item()\n            >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.beit(\n            pixel_values,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs.pooler_output if return_dict else outputs[1]\n\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\nclass BeitConvModule(nn.Module):\n    \"\"\"\n    A convolutional block that bundles conv/norm/activation layers. This block simplifies the usage of convolution\n    layers, which are commonly used with a norm layer (e.g., BatchNorm) and activation layer (e.g., ReLU).\n\n    Based on OpenMMLab's implementation, found in https://github.com/open-mmlab/mmsegmentation.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=False, dilation=1):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n            bias=bias,\n            dilation=dilation,\n        )\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.activation = nn.ReLU()\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        output = self.activation(output)\n\n        return output", "\n\nclass BeitPyramidPoolingModule(nn.ModuleList):\n    \"\"\"\n    Pyramid Pooling Module (PPM) used in PSPNet.\n\n    Args:\n        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n            Module.\n        in_channels (int): Input channels.\n        channels (int): Channels after modules, before conv_seg.\n        align_corners (bool): align_corners argument of F.interpolate.\n\n    Based on OpenMMLab's implementation, found in https://github.com/open-mmlab/mmsegmentation.\n    \"\"\"\n\n    def __init__(self, pool_scales, in_channels, channels, align_corners):\n        super().__init__()\n        self.pool_scales = pool_scales\n        self.align_corners = align_corners\n        self.in_channels = in_channels\n        self.channels = channels\n        for pool_scale in pool_scales:\n            self.append(\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(pool_scale),\n                    BeitConvModule(self.in_channels, self.channels, kernel_size=1),\n                )\n            )\n\n    def forward(self, x):\n        ppm_outs = []\n        for ppm in self:\n            ppm_out = ppm(x)\n            upsampled_ppm_out = nn.functional.interpolate(\n                ppm_out, size=x.size()[2:], mode=\"bilinear\", align_corners=self.align_corners\n            )\n            ppm_outs.append(upsampled_ppm_out)\n        return ppm_outs", "\n\nclass BeitUperHead(nn.Module):\n    \"\"\"\n    Unified Perceptual Parsing for Scene Understanding. This head is the implementation of `UPerNet\n    <https://arxiv.org/abs/1807.10221>`_.\n\n    Based on OpenMMLab's implementation, found in https://github.com/open-mmlab/mmsegmentation.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        self.pool_scales = config.pool_scales  # e.g. (1, 2, 3, 6)\n        self.in_channels = [config.hidden_size] * 4  # e.g. [768, 768, 768, 768]\n        self.channels = config.hidden_size\n        self.align_corners = False\n        self.classifier = nn.Conv2d(self.channels, config.num_labels, kernel_size=1)\n\n        # PSP Module\n        self.psp_modules = BeitPyramidPoolingModule(\n            self.pool_scales,\n            self.in_channels[-1],\n            self.channels,\n            align_corners=self.align_corners,\n        )\n        self.bottleneck = BeitConvModule(\n            self.in_channels[-1] + len(self.pool_scales) * self.channels,\n            self.channels,\n            kernel_size=3,\n            padding=1,\n        )\n        # FPN Module\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        for in_channels in self.in_channels[:-1]:  # skip the top layer\n            l_conv = BeitConvModule(in_channels, self.channels, kernel_size=1)\n            fpn_conv = BeitConvModule(self.channels, self.channels, kernel_size=3, padding=1)\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        self.fpn_bottleneck = BeitConvModule(\n            len(self.in_channels) * self.channels,\n            self.channels,\n            kernel_size=3,\n            padding=1,\n        )\n\n    def psp_forward(self, inputs):\n        x = inputs[-1]\n        psp_outs = [x]\n        psp_outs.extend(self.psp_modules(x))\n        psp_outs = torch.cat(psp_outs, dim=1)\n        output = self.bottleneck(psp_outs)\n\n        return output\n\n    def forward(self, encoder_hidden_states):\n        # build laterals\n        laterals = [lateral_conv(encoder_hidden_states[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n\n        laterals.append(self.psp_forward(encoder_hidden_states))\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] = laterals[i - 1] + nn.functional.interpolate(\n                laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners\n            )\n\n        # build outputs\n        fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]\n        # append psp feature\n        fpn_outs.append(laterals[-1])\n\n        for i in range(used_backbone_levels - 1, 0, -1):\n            fpn_outs[i] = nn.functional.interpolate(\n                fpn_outs[i], size=fpn_outs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners\n            )\n        fpn_outs = torch.cat(fpn_outs, dim=1)\n        output = self.fpn_bottleneck(fpn_outs)\n        output = self.classifier(output)\n\n        return output", "\n\nclass BeitFCNHead(nn.Module):\n    \"\"\"\n    Fully Convolution Networks for Semantic Segmentation. This head is implemented of `FCNNet\n    <https://arxiv.org/abs/1411.4038>`_.\n\n    Args:\n        config (BeitConfig): Configuration.\n        in_channels\n        kernel_size (int): The kernel size for convs in the head. Default: 3.\n        dilation (int): The dilation rate for convs in the head. Default: 1.\n\n\n    Based on OpenMMLab's implementation, found in https://github.com/open-mmlab/mmsegmentation.\n    \"\"\"\n\n    def __init__(self, config, in_index=2, kernel_size=3, dilation=1):\n        super().__init__()\n        self.in_channels = config.hidden_size\n        self.channels = config.auxiliary_channels\n        self.num_convs = config.auxiliary_num_convs\n        self.concat_input = config.auxiliary_concat_input\n        self.in_index = in_index\n\n        conv_padding = (kernel_size // 2) * dilation\n        convs = []\n        convs.append(\n            BeitConvModule(\n                self.in_channels, self.channels, kernel_size=kernel_size, padding=conv_padding, dilation=dilation\n            )\n        )\n        for i in range(self.num_convs - 1):\n            convs.append(\n                BeitConvModule(\n                    self.channels, self.channels, kernel_size=kernel_size, padding=conv_padding, dilation=dilation\n                )\n            )\n        if self.num_convs == 0:\n            self.convs = nn.Identity()\n        else:\n            self.convs = nn.Sequential(*convs)\n        if self.concat_input:\n            self.conv_cat = BeitConvModule(\n                self.in_channels + self.channels, self.channels, kernel_size=kernel_size, padding=kernel_size // 2\n            )\n\n        self.classifier = nn.Conv2d(self.channels, config.num_labels, kernel_size=1)\n\n    def forward(self, encoder_hidden_states):\n        # just take the relevant feature maps\n        hidden_states = encoder_hidden_states[self.in_index]\n        output = self.convs(hidden_states)\n        if self.concat_input:\n            output = self.conv_cat(torch.cat([hidden_states, output], dim=1))\n        output = self.classifier(output)\n        return output", "\n\n@add_start_docstrings(\n    \"\"\"\n    Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k, CityScapes.\n    \"\"\",\n    BEIT_START_DOCSTRING,\n)\nclass BeitForSemanticSegmentation(BeitPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.beit = BeitModel(config, add_pooling_layer=False)\n\n        # FPNs\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n            nn.BatchNorm2d(config.hidden_size),\n            nn.GELU(),\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n        )\n        self.fpn2 = nn.Sequential(\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n        )\n        self.fpn3 = nn.Identity()\n        self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Semantic segmentation head(s)\n        self.decode_head = BeitUperHead(config)\n        self.auxiliary_head = BeitFCNHead(config) if config.use_auxiliary_head else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def compute_loss(self, logits, auxiliary_logits, labels):\n        # upsample logits to the images' original size\n        upsampled_logits = nn.functional.interpolate(\n            logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n        if auxiliary_logits is not None:\n            upsampled_auxiliary_logits = nn.functional.interpolate(\n                auxiliary_logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n            )\n        # compute weighted loss\n        loss_fct = CrossEntropyLoss(ignore_index=self.config.semantic_loss_ignore_index)\n        main_loss = loss_fct(upsampled_logits, labels)\n        auxiliary_loss = loss_fct(upsampled_auxiliary_logits, labels)\n        loss = main_loss + self.config.auxiliary_loss_weight * auxiliary_loss\n\n        return loss\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        head_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, height, width)`, `optional`):\n            Ground truth semantic segmentation maps for computing the loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels > 1`, a classification loss is computed\n            (Cross-Entropy).\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n            >>> model = BeitForSemanticSegmentation.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> # logits are of shape (batch_size, num_labels, height/4, width/4)\n            >>> logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n\n        outputs = self.beit(\n            pixel_values,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=True,  # we need the intermediate hidden states\n            return_dict=return_dict,\n        )\n\n        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[2]\n\n        # only keep certain features, and reshape\n        # note that we do +1 as the encoder_hidden_states also includes the initial embeddings\n        features = [feature for idx, feature in enumerate(encoder_hidden_states) if idx + 1 in self.config.out_indices]\n        batch_size = pixel_values.shape[0]\n        patch_resolution = self.config.image_size // self.config.patch_size\n        features = [\n            x[:, 1:, :].permute(0, 2, 1).reshape(batch_size, -1, patch_resolution, patch_resolution) for x in features\n        ]\n\n        # apply FPNs\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        logits = self.decode_head(features)\n        auxiliary_logits = None\n        if self.auxiliary_head is not None:\n            auxiliary_logits = self.auxiliary_head(features)\n\n        loss = None\n        if labels is not None:\n            if self.config.num_labels == 1:\n                raise ValueError(\"The number of labels should be greater than one\")\n            else:\n                loss = self.compute_loss(logits, auxiliary_logits, labels)\n\n        if not return_dict:\n            if output_hidden_states:\n                output = (logits,) + outputs[2:]\n            else:\n                output = (logits,) + outputs[3:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states if output_hidden_states else None,\n            attentions=outputs.attentions,\n        )", "class BeitForSemanticSegmentation(BeitPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.beit = BeitModel(config, add_pooling_layer=False)\n\n        # FPNs\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n            nn.BatchNorm2d(config.hidden_size),\n            nn.GELU(),\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n        )\n        self.fpn2 = nn.Sequential(\n            nn.ConvTranspose2d(config.hidden_size, config.hidden_size, kernel_size=2, stride=2),\n        )\n        self.fpn3 = nn.Identity()\n        self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Semantic segmentation head(s)\n        self.decode_head = BeitUperHead(config)\n        self.auxiliary_head = BeitFCNHead(config) if config.use_auxiliary_head else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def compute_loss(self, logits, auxiliary_logits, labels):\n        # upsample logits to the images' original size\n        upsampled_logits = nn.functional.interpolate(\n            logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n        if auxiliary_logits is not None:\n            upsampled_auxiliary_logits = nn.functional.interpolate(\n                auxiliary_logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n            )\n        # compute weighted loss\n        loss_fct = CrossEntropyLoss(ignore_index=self.config.semantic_loss_ignore_index)\n        main_loss = loss_fct(upsampled_logits, labels)\n        auxiliary_loss = loss_fct(upsampled_auxiliary_logits, labels)\n        loss = main_loss + self.config.auxiliary_loss_weight * auxiliary_loss\n\n        return loss\n\n    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        pixel_values=None,\n        head_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, height, width)`, `optional`):\n            Ground truth semantic segmentation maps for computing the loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels > 1`, a classification loss is computed\n            (Cross-Entropy).\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation\n            >>> from PIL import Image\n            >>> import requests\n\n            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n            >>> image = Image.open(requests.get(url, stream=True).raw)\n\n            >>> feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n            >>> model = BeitForSemanticSegmentation.from_pretrained('microsoft/beit-base-finetuned-ade-640-640')\n\n            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> # logits are of shape (batch_size, num_labels, height/4, width/4)\n            >>> logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n\n        outputs = self.beit(\n            pixel_values,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=True,  # we need the intermediate hidden states\n            return_dict=return_dict,\n        )\n\n        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[2]\n\n        # only keep certain features, and reshape\n        # note that we do +1 as the encoder_hidden_states also includes the initial embeddings\n        features = [feature for idx, feature in enumerate(encoder_hidden_states) if idx + 1 in self.config.out_indices]\n        batch_size = pixel_values.shape[0]\n        patch_resolution = self.config.image_size // self.config.patch_size\n        features = [\n            x[:, 1:, :].permute(0, 2, 1).reshape(batch_size, -1, patch_resolution, patch_resolution) for x in features\n        ]\n\n        # apply FPNs\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        logits = self.decode_head(features)\n        auxiliary_logits = None\n        if self.auxiliary_head is not None:\n            auxiliary_logits = self.auxiliary_head(features)\n\n        loss = None\n        if labels is not None:\n            if self.config.num_labels == 1:\n                raise ValueError(\"The number of labels should be greater than one\")\n            else:\n                loss = self.compute_loss(logits, auxiliary_logits, labels)\n\n        if not return_dict:\n            if output_hidden_states:\n                output = (logits,) + outputs[2:]\n            else:\n                output = (logits,) + outputs[3:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states if output_hidden_states else None,\n            attentions=outputs.attentions,\n        )"]}
{"filename": "models/model_pretrain.py", "chunked_list": ["\"\"\"\n * Copyright (c) 2021, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\nfrom models.model_retrieval_base import SingularityRetrievalBase\n\nimport torch\n", "import torch\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass Singularity(SingularityRetrievalBase):\n    def __init__(self, config=None, tokenizer=None):\n        super(Singularity, self).__init__(\n            config=config, tokenizer=tokenizer, pretrain=True)\n        self.mlm_prob = config.mlm_prob\n\n    def get_mlm_loss(self, text, image_embeds, image_atts):\n        input_ids = text.input_ids.clone()\n        labels = input_ids.clone()\n        probability_matrix = torch.full(labels.shape, self.mlm_prob)\n        input_ids, labels = self.mask(\n            input_ids, self.text_encoder.config.vocab_size, input_ids.device,\n            targets=labels, probability_matrix=probability_matrix\n        )        \n\n        intermediate_mlm_output = self.text_encoder.bert(\n            input_ids,\n            attention_mask=text.attention_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            mode=\"text\"\n        )\n\n        text_embeds = intermediate_mlm_output.last_hidden_state\n\n        mlm_output = self.text_encoder(\n            encoder_embeds=text_embeds,\n            attention_mask=text.attention_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n            soft_labels=None,\n            mode=\"fusion\"\n        )\n        return mlm_output.loss\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            # We only compute loss on masked tokens\n            targets[~masked_indices] = -100\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(\n            input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(\n            input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(\n            vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids", ""]}
{"filename": "models/__init__.py", "chunked_list": [""]}
{"filename": "models/utils.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom scipy import interpolate\nimport numpy as np\nimport logging\nfrom einops import rearrange, repeat\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\ndef _init_transformer_weights(module, initializer_range=0.02):\n    \"\"\"Initialize the weights. Copied from transformers ViT/Bert model init\"\"\"\n    if isinstance(module, (nn.Linear, nn.Conv2d)):\n        # Slightly different from the TF version which uses truncated_normal for initialization\n        # cf https://github.com/pytorch/pytorch/pull/5617\n        module.weight.data.normal_(mean=0.0, std=initializer_range)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=initializer_range)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    elif isinstance(module, nn.LayerNorm):\n        module.bias.data.zero_()\n        module.weight.data.fill_(1.0)", "\n\ndef interpolate_pos_embed(pos_embed_old, pos_embed_new, num_patches_new):\n    \"\"\"\n    Args:\n        pos_embed_old: (1, L_old, d), pre-trained\n        pos_embed_new: (1, L_new, d), newly initialized, to be replaced by interpolated weights\n        num_patches_new:\n    \"\"\"\n    # interpolate position embedding\n    embedding_size = pos_embed_old.shape[-1]\n    num_extra_tokens = pos_embed_new.shape[-2] - num_patches_new\n    # height (== width) for the checkpoint position embedding\n    orig_size = int((pos_embed_old.shape[-2] - num_extra_tokens) ** 0.5)\n    # height (== width) for the new position embedding\n    new_size = int(num_patches_new ** 0.5)\n\n    if orig_size != new_size:\n        # class_token and dist_token are kept unchanged\n        # the extra tokens seems always at the beginning of the position embedding\n        extra_tokens = pos_embed_old[:, :num_extra_tokens]\n        # only the position tokens are interpolated\n        pos_tokens = pos_embed_old[:, num_extra_tokens:]\n        pos_tokens = pos_tokens.reshape(\n            -1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n        pos_tokens = torch.nn.functional.interpolate(\n            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n        interpolated_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n        logger.info(f\"reshape position embedding from {orig_size}**2 to {new_size}**2\")\n        return interpolated_pos_embed\n    else:\n        return pos_embed_old", "\n\ndef interpolate_pos_relative_bias_beit(state_dict_old, state_dict_new, patch_shape_new):\n    \"\"\"\n    Args:\n        state_dict_old: loaded state dict\n        state_dict_new: state dict for model with new image size\n        patch_shape_new: new model patch_shape\n    ref: https://github.com/microsoft/unilm/blob/master/beit/run_class_finetuning.py\n    \"\"\"\n    all_keys = list(state_dict_old.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict_old.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict_old[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            dst_num_pos, _ = state_dict_new[key].size()\n            dst_patch_shape = patch_shape_new\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                # logger.info(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n                #     key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                # logger.info(\"Original positions = %s\" % str(x))\n                # logger.info(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict_old[key] = new_rel_pos_bias\n    return state_dict_old", "\n\ndef interpolate_pos_relative_bias_beit_3d(state_dict_old, state_dict_new, patch_shape_new, src_t_size=1):\n    \"\"\"\n    Args:\n        state_dict_old: loaded state dict\n        state_dict_new: state dict for model with new image size\n        patch_shape_new: new model patch_shape\n    ref: https://github.com/microsoft/unilm/blob/master/beit/run_class_finetuning.py\n    \"\"\"\n    all_keys = list(state_dict_old.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict_old.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            src_num_pos, num_attn_heads = state_dict_old[key].size()\n            dst_num_pos, _ = state_dict_new[key].size()\n            if src_num_pos == dst_num_pos:\n                continue\n\n            num_extra_tokens = dst_num_pos - np.prod([w * 2 - 1 for w in patch_shape_new])\n                \n            src_s_size = int((src_num_pos - num_extra_tokens) / src_t_size)\n            src_size = int(src_s_size ** 0.5)\n            dst_size = patch_shape_new[-1] * 2 - 1\n\n            if src_size != dst_size:\n                # Spatial interpolation\n                logger.info(f\"Position interpolate for {key} from {src_size}x{src_size} to {dst_size}x{dst_size}\")\n                \n                rel_pos_bias = state_dict_old[key]\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                # logger.info(\"Original positions = %s\" % str(x))\n                # logger.info(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict_old[key] = new_rel_pos_bias\n\n            dst_t_size = patch_shape_new[0] * 2 - 1\n            if src_t_size != dst_t_size:\n                # Temporal interpolation\n                logger.info(f\"Inflating {key} from {src_t_size}x{src_size}x{src_size} to {dst_t_size}x{dst_size}x{dst_size}\")\n                \n                rel_pos_bias = state_dict_old[key]\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n                \n                if src_t_size == 1:\n                    rel_pos_bias = repeat(rel_pos_bias, 's d -> (t s) d', t=dst_t_size)\n                else:\n                    rel_pos_bias = rearrange(rel_pos_bias, '(t s) d -> s d t', t=src_t_size)\n                    rel_pos_bias = F.interpolate(rel_pos_bias, dst_t_size, mode='nearest')\n                    rel_pos_bias = rearrange(rel_pos_bias, 's d t -> (t s) d')\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict_old[key] = new_rel_pos_bias\n\n    return state_dict_old", "\n\ndef tile(x, dim, n_tile):\n    init_dim = x.size(dim)\n    repeat_idx = [1] * x.dim()\n    repeat_idx[dim] = n_tile\n    x = x.repeat(*repeat_idx)\n    order_index = torch.LongTensor(np.concatenate(\n        [init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n    return torch.index_select(x, dim, order_index.to(x.device))", "\n\ndef mask_logits(target, mask):\n    return target * mask + (1 - mask) * (-1e10)\n\n"]}
{"filename": "models/sparse_config.py", "chunked_list": ["# coding=utf-8\n# Copyright Microsoft Research and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import OrderedDict\nfrom typing import Mapping\n", "from typing import Mapping\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.onnx import OnnxConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {", "\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/config.json\",\n    \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/config.json\",\n    \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/config.json\",\n    \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/config.json\",\n    \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json\",\n    \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\",\n    \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/config.json\",\n    \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/config.json\",", "    \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/config.json\",\n    \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/config.json\",\n    \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json\",\n    \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/config.json\",\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n    \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/config.json\",\n    \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/config.json\",\n    \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/config.json\",\n    \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/config.json\",\n    \"cl-tohoku/bert-base-japanese\": \"https://huggingface.co/cl-tohoku/bert-base-japanese/resolve/main/config.json\",", "    \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/config.json\",\n    \"cl-tohoku/bert-base-japanese\": \"https://huggingface.co/cl-tohoku/bert-base-japanese/resolve/main/config.json\",\n    \"cl-tohoku/bert-base-japanese-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json\",\n    \"cl-tohoku/bert-base-japanese-char\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char/resolve/main/config.json\",\n    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\": \"https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking/resolve/main/config.json\",\n    \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json\",\n    \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/config.json\",\n    \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/config.json\",\n    # See all BERT models at https://huggingface.co/models?filter=bert\n}", "    # See all BERT models at https://huggingface.co/models?filter=bert\n}\n\n\nclass BertConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`BertModel`] or a\n    [`TFBertModel`]. It is used to instantiate a BERT model according to the specified arguments,\n    defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration\n    to that of the BERT [bert-base-uncased](https://huggingface.co/bert-base-uncased) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n    outputs. Read the documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 30522):\n            Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`BertModel`] or\n            [`TFBertModel`].\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string,\n            `\"gelu\"`, `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 512):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        type_vocab_size (`int`, *optional*, defaults to 2):\n            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or\n            [`TFBertModel`].\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`,\n            `\"relative_key_query\"`. For positional embeddings use `\"absolute\"`. For more information on\n            `\"relative_key\"`, please refer to [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155). For more information on `\"relative_key_query\"`, please refer to\n            *Method 4* in [Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        classifier_dropout (`float`, *optional*):\n            The dropout ratio for the classification head.\n\n    Examples:\n\n    ```python\n    >>> from transformers import BertModel, BertConfig\n\n    >>> # Initializing a BERT bert-base-uncased style configuration\n    >>> configuration = BertConfig()\n\n    >>> # Initializing a model from the bert-base-uncased style configuration\n    >>> model = BertModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"bert\"\n\n    def __init__(\n        self,\n        vocab_size=30522,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        max_position_embeddings=512,\n        type_vocab_size=2,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        classifier_dropout=None,\n        token_keep_rate=1,\n        token_keep_strategy='cls_attn',\n        token_drop_loc=[9],\n        **kwargs\n    ):\n        super().__init__(pad_token_id=pad_token_id, **kwargs)\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.classifier_dropout = classifier_dropout\n        self.token_keep_rate = token_keep_rate\n        self.token_keep_strategy = token_keep_strategy\n        self.token_drop_loc = token_drop_loc", "\n\nclass BertOnnxConfig(OnnxConfig):\n    @property\n    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n        return OrderedDict(\n            [\n                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n                (\"token_type_ids\", {0: \"batch\", 1: \"sequence\"}),\n            ]\n        )", "\n\nBEIT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/beit-base-patch16-224-in22k\": \"https://huggingface.co/microsoft/beit-base-patch16-224-in22k/resolve/main/config.json\",\n    # See all BEiT models at https://huggingface.co/models?filter=beit\n}\n\n\nclass BeitConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`BeitModel`]. It is used to\n    instantiate an BEiT model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the BEiT\n    [microsoft/beit-base-patch16-224-in22k](https://huggingface.co/microsoft/beit-base-patch16-224-in22k)\n    architecture.\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 8092):\n            Vocabulary size of the BEiT model. Defines the number of different image tokens that can be used during\n            pre-training.\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string,\n            `\"gelu\"`, `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        image_size (`int`, *optional*, defaults to `224`):\n            The size (resolution) of each image.\n        patch_size (`int`, *optional*, defaults to `16`):\n            The size (resolution) of each patch.\n        num_channels (`int`, *optional*, defaults to `3`):\n            The number of input channels.\n        use_mask_token (`bool`, *optional*, defaults to `False`):\n            Whether to use a mask token for masked image modeling.\n        use_absolute_position_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to use BERT-style absolute position embeddings.\n        use_relative_position_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use T5-style relative position embeddings in the self-attention layers.\n        use_shared_relative_position_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use the same relative position embeddings across all self-attention layers of the Transformer.\n        layer_scale_init_value (`float`, *optional*, defaults to 0.1):\n            Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable layer scale.\n        drop_path_rate (`float`, *optional*, defaults to 0.1):\n            Stochastic depth rate per sample (when applied in the main path of residual layers).\n        use_mean_pooling (`bool`, *optional*, defaults to `True`):\n            Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the\n            CLS token, before applying the classification head.\n        out_indices (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`):\n            Indices of the feature maps to use for semantic segmentation.\n        pool_scales (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`):\n            Pooling scales used in Pooling Pyramid Module applied on the last feature map.\n        use_auxiliary_head (`bool`, *optional*, defaults to `True`):\n            Whether to use an auxiliary head during training.\n        auxiliary_loss_weight (`float`, *optional*, defaults to 0.4):\n            Weight of the cross-entropy loss of the auxiliary head.\n        auxiliary_channels (`int`, *optional*, defaults to 256):\n            Number of channels to use in the auxiliary head.\n        auxiliary_num_convs (`int`, *optional*, defaults to 1):\n            Number of convolutional layers to use in the auxiliary head.\n        auxiliary_concat_input (`bool`, *optional*, defaults to `False`):\n            Whether to concatenate the output of the auxiliary head with the input before the classification layer.\n        semantic_loss_ignore_index (`int`, *optional*, defaults to 255):\n            The index that is ignored by the loss function of the semantic segmentation model.\n\n    Example:\n\n    ```python\n    >>> from transformers import BeitModel, BeitConfig\n\n    >>> # Initializing a BEiT beit-base-patch16-224-in22k style configuration\n    >>> configuration = BeitConfig()\n\n    >>> # Initializing a model from the beit-base-patch16-224-in22k style configuration\n    >>> model = BeitModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"beit\"\n\n    def __init__(\n        self,\n        vocab_size=8192,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.0,\n        attention_probs_dropout_prob=0.0,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        is_encoder_decoder=False,\n        image_size=224,\n        patch_size=16,\n        num_channels=3,\n        use_mask_token=False,\n        use_absolute_position_embeddings=False,\n        use_relative_position_bias=False,\n        use_shared_relative_position_bias=False,\n        layer_scale_init_value=0.1,\n        drop_path_rate=0.1,\n        use_mean_pooling=True,\n        out_indices=[3, 5, 7, 11],\n        pool_scales=[1, 2, 3, 6],\n        use_auxiliary_head=True,\n        auxiliary_loss_weight=0.4,\n        auxiliary_channels=256,\n        auxiliary_num_convs=1,\n        auxiliary_concat_input=False,\n        semantic_loss_ignore_index=255,\n        token_keep_rate=1,\n        token_keep_strategy='cls_attn',\n        token_drop_loc=[3, 6, 9],\n        sparse_random_attn=None,\n        sparse_local_attn=1,\n        attn_block_size=1,\n        num_cls_tokens=1,\n        token_3d_order='none',\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_channels = num_channels\n        self.use_mask_token = use_mask_token\n        self.use_absolute_position_embeddings = use_absolute_position_embeddings\n        self.use_relative_position_bias = use_relative_position_bias\n        self.use_shared_relative_position_bias = use_shared_relative_position_bias\n        self.layer_scale_init_value = layer_scale_init_value\n        self.drop_path_rate = drop_path_rate\n        self.use_mean_pooling = use_mean_pooling\n        # decode head attributes (semantic segmentation)\n        self.out_indices = out_indices\n        self.pool_scales = pool_scales\n        # auxiliary head attributes (semantic segmentation)\n        self.use_auxiliary_head = use_auxiliary_head\n        self.auxiliary_loss_weight = auxiliary_loss_weight\n        self.auxiliary_channels = auxiliary_channels\n        self.auxiliary_num_convs = auxiliary_num_convs\n        self.auxiliary_concat_input = auxiliary_concat_input\n        self.semantic_loss_ignore_index = semantic_loss_ignore_index\n\n        # node sparsification\n        self.token_keep_rate = token_keep_rate\n        self.token_keep_strategy = token_keep_strategy\n        self.token_drop_loc = token_drop_loc\n        # edge sparsification\n        self.sparse_random_attn = sparse_random_attn\n        self.sparse_local_attn = sparse_local_attn\n        self.attn_block_size = attn_block_size\n        self.num_cls_tokens = num_cls_tokens\n        # token order\n        self.token_3d_order = token_3d_order", "class BeitConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`BeitModel`]. It is used to\n    instantiate an BEiT model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the BEiT\n    [microsoft/beit-base-patch16-224-in22k](https://huggingface.co/microsoft/beit-base-patch16-224-in22k)\n    architecture.\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 8092):\n            Vocabulary size of the BEiT model. Defines the number of different image tokens that can be used during\n            pre-training.\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string,\n            `\"gelu\"`, `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        image_size (`int`, *optional*, defaults to `224`):\n            The size (resolution) of each image.\n        patch_size (`int`, *optional*, defaults to `16`):\n            The size (resolution) of each patch.\n        num_channels (`int`, *optional*, defaults to `3`):\n            The number of input channels.\n        use_mask_token (`bool`, *optional*, defaults to `False`):\n            Whether to use a mask token for masked image modeling.\n        use_absolute_position_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to use BERT-style absolute position embeddings.\n        use_relative_position_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use T5-style relative position embeddings in the self-attention layers.\n        use_shared_relative_position_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use the same relative position embeddings across all self-attention layers of the Transformer.\n        layer_scale_init_value (`float`, *optional*, defaults to 0.1):\n            Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable layer scale.\n        drop_path_rate (`float`, *optional*, defaults to 0.1):\n            Stochastic depth rate per sample (when applied in the main path of residual layers).\n        use_mean_pooling (`bool`, *optional*, defaults to `True`):\n            Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the\n            CLS token, before applying the classification head.\n        out_indices (`List[int]`, *optional*, defaults to `[3, 5, 7, 11]`):\n            Indices of the feature maps to use for semantic segmentation.\n        pool_scales (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`):\n            Pooling scales used in Pooling Pyramid Module applied on the last feature map.\n        use_auxiliary_head (`bool`, *optional*, defaults to `True`):\n            Whether to use an auxiliary head during training.\n        auxiliary_loss_weight (`float`, *optional*, defaults to 0.4):\n            Weight of the cross-entropy loss of the auxiliary head.\n        auxiliary_channels (`int`, *optional*, defaults to 256):\n            Number of channels to use in the auxiliary head.\n        auxiliary_num_convs (`int`, *optional*, defaults to 1):\n            Number of convolutional layers to use in the auxiliary head.\n        auxiliary_concat_input (`bool`, *optional*, defaults to `False`):\n            Whether to concatenate the output of the auxiliary head with the input before the classification layer.\n        semantic_loss_ignore_index (`int`, *optional*, defaults to 255):\n            The index that is ignored by the loss function of the semantic segmentation model.\n\n    Example:\n\n    ```python\n    >>> from transformers import BeitModel, BeitConfig\n\n    >>> # Initializing a BEiT beit-base-patch16-224-in22k style configuration\n    >>> configuration = BeitConfig()\n\n    >>> # Initializing a model from the beit-base-patch16-224-in22k style configuration\n    >>> model = BeitModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"beit\"\n\n    def __init__(\n        self,\n        vocab_size=8192,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.0,\n        attention_probs_dropout_prob=0.0,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        is_encoder_decoder=False,\n        image_size=224,\n        patch_size=16,\n        num_channels=3,\n        use_mask_token=False,\n        use_absolute_position_embeddings=False,\n        use_relative_position_bias=False,\n        use_shared_relative_position_bias=False,\n        layer_scale_init_value=0.1,\n        drop_path_rate=0.1,\n        use_mean_pooling=True,\n        out_indices=[3, 5, 7, 11],\n        pool_scales=[1, 2, 3, 6],\n        use_auxiliary_head=True,\n        auxiliary_loss_weight=0.4,\n        auxiliary_channels=256,\n        auxiliary_num_convs=1,\n        auxiliary_concat_input=False,\n        semantic_loss_ignore_index=255,\n        token_keep_rate=1,\n        token_keep_strategy='cls_attn',\n        token_drop_loc=[3, 6, 9],\n        sparse_random_attn=None,\n        sparse_local_attn=1,\n        attn_block_size=1,\n        num_cls_tokens=1,\n        token_3d_order='none',\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_channels = num_channels\n        self.use_mask_token = use_mask_token\n        self.use_absolute_position_embeddings = use_absolute_position_embeddings\n        self.use_relative_position_bias = use_relative_position_bias\n        self.use_shared_relative_position_bias = use_shared_relative_position_bias\n        self.layer_scale_init_value = layer_scale_init_value\n        self.drop_path_rate = drop_path_rate\n        self.use_mean_pooling = use_mean_pooling\n        # decode head attributes (semantic segmentation)\n        self.out_indices = out_indices\n        self.pool_scales = pool_scales\n        # auxiliary head attributes (semantic segmentation)\n        self.use_auxiliary_head = use_auxiliary_head\n        self.auxiliary_loss_weight = auxiliary_loss_weight\n        self.auxiliary_channels = auxiliary_channels\n        self.auxiliary_num_convs = auxiliary_num_convs\n        self.auxiliary_concat_input = auxiliary_concat_input\n        self.semantic_loss_ignore_index = semantic_loss_ignore_index\n\n        # node sparsification\n        self.token_keep_rate = token_keep_rate\n        self.token_keep_strategy = token_keep_strategy\n        self.token_drop_loc = token_drop_loc\n        # edge sparsification\n        self.sparse_random_attn = sparse_random_attn\n        self.sparse_local_attn = sparse_local_attn\n        self.attn_block_size = attn_block_size\n        self.num_cls_tokens = num_cls_tokens\n        # token order\n        self.token_3d_order = token_3d_order", "\n"]}
{"filename": "models/xbert.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model. \"\"\"\n\nimport math", "\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint", "from torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n    add_start_docstrings,", "    ModelOutput,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,", "    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,", "from transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.models.bert.configuration_bert import BertConfig\n\nimport transformers", "\nimport transformers\ntransformers.logging.set_verbosity_error()\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"BertConfig\"\n_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n\nBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [", "\nBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"bert-base-uncased\",\n    \"bert-large-uncased\",\n    \"bert-base-cased\",\n    \"bert-large-cased\",\n    \"bert-base-multilingual-uncased\",\n    \"bert-base-multilingual-cased\",\n    \"bert-base-chinese\",\n    \"bert-base-german-cased\",", "    \"bert-base-chinese\",\n    \"bert-base-german-cased\",\n    \"bert-large-uncased-whole-word-masking\",\n    \"bert-large-cased-whole-word-masking\",\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n    \"bert-base-cased-finetuned-mrpc\",\n    \"bert-base-german-dbmdz-cased\",\n    \"bert-base-german-dbmdz-uncased\",\n    \"cl-tohoku/bert-base-japanese\",", "    \"bert-base-german-dbmdz-uncased\",\n    \"cl-tohoku/bert-base-japanese\",\n    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n    \"cl-tohoku/bert-base-japanese-char\",\n    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n    \"TurkuNLP/bert-base-finnish-cased-v1\",\n    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n    \"wietsedv/bert-base-dutch-cased\",\n    # See all BERT models at https://huggingface.co/models?filter=bert\n]", "    # See all BERT models at https://huggingface.co/models?filter=bert\n]\n\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\"/\")\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(\n            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\",\n                  \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n            for n in name\n        ):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        try:\n            assert (\n                pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(\n            config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(\n            config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\")\n\n        self.config = config\n\n    def forward(\n        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:,\n                                             past_key_values_length: seq_length + past_key_values_length]\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings", "\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(\n            config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(\n                2 * config.max_position_embeddings - 1, self.attention_head_size)\n        self.save_attention = False\n\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[\n            :-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(\n                self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(\n                self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(\n            query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(\n                distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(\n                dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\n                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + \\\n                    relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / \\\n            math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[\n            :-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        # added `attention_scores` to return tuple\n        outputs = (context_layer, attention_probs, attention_scores) if output_attentions else (\n            context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs", "\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False):\n        super().__init__()\n        self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - \\\n            len(heads)\n        self.self.all_head_size = self.self.attention_head_size * \\\n            self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        # add attentions if we output them\n        outputs = (attention_output,) + self_outputs[1:]\n        return outputs  # (context_layer, attention_probs, attention_scores, past_key_value,)", "\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states", "\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n\n        self.has_cross_attention = (layer_num >= config.fusion_layer)\n        if self.has_cross_attention:\n            self.layer_num = layer_num\n            self.crossattention = BertAttention(\n                config, is_cross_attention=True)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:\n                                                  2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n        attention_output = self_attention_outputs[0]\n\n        outputs = self_attention_outputs[1:-1]\n        present_key_value = self_attention_outputs[-1]\n\n        if self.has_cross_attention:\n            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n\n            if type(encoder_hidden_states) == list:\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states[(\n                        self.layer_num-self.config.fusion_layer) % len(encoder_hidden_states)],\n                    encoder_attention_mask[(\n                        self.layer_num-self.config.fusion_layer) % len(encoder_hidden_states)],\n                    output_attentions=output_attentions,\n                )\n                attention_output = cross_attention_outputs[0]\n                outputs = outputs + cross_attention_outputs[1:-1]\n\n            else:\n                cross_attention_outputs = self.crossattention(\n                    attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n                attention_output = cross_attention_outputs[0]\n                # add cross attentions if we output attention weights\n                outputs = outputs + cross_attention_outputs[1:-1]\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output", "\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config, i)\n                                   for i in range(config.num_hidden_layers)])\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n        mode='multi_modal',\n        normalize_attention=True  \n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions else None\n\n        next_decoder_cache = () if use_cache else None\n\n        if mode == 'text':\n            start_layer = 0\n            output_layer = self.config.fusion_layer\n\n        elif mode == 'fusion':\n            start_layer = self.config.fusion_layer\n            output_layer = self.config.num_hidden_layers\n\n        elif mode == 'multi_modal':\n            start_layer = 0\n            output_layer = self.config.num_hidden_layers\n\n        for i in range(start_layer, output_layer):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n                        \"`use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )  # (context_layer, attention_probs, attention_scores, past_key_value,)\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                # whether to output normalized attention, \n                # note for unnormalized attention, there is a mask added\n                offset = int(normalize_attention)\n                all_self_attentions = all_self_attentions + (layer_outputs[2-offset], )\n                if hasattr(layer_module, \"crossattention\"):\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[4-offset], )\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )", "\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output", "\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states", "\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(\n            config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states", "\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores", "\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score", "\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score", "\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(\n                mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()", "\n\n@dataclass\nclass BertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of :class:`~transformers.BertForPreTraining`.\n    Args:\n        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n            (classification) loss.\n        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n            before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n            sequence_length, sequence_length)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    prediction_logits: torch.FloatTensor = None\n    seq_relationship_logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None", "\n\nBERT_START_DOCSTRING = r\"\"\"\n    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)\n    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n    general usage and behavior.\n    Parameters:", "    general usage and behavior.\n    Parameters:\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n            weights.\n\"\"\"\n\nBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:", "BERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n            Indices of input sequence tokens in the vocabulary.\n            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n            `What are input IDs? <../glossary.html#input-ids>`__\n        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:", "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n            1]``:\n            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.", "            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.\n            `What are token type IDs? <../glossary.html#token-type-ids>`_\n        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n            config.max_position_embeddings - 1]``.\n            `What are position IDs? <../glossary.html#position-ids>`_\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n            - 1 indicates the head is **not masked**,", "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n            vectors than the model's internal embedding lookup matrix.\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.", "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n", "\n\n@add_start_docstrings(\n    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n    BERT_START_DOCSTRING,\n)\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = seq_ids[None, None, :].repeat(\n                    batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n\n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - \\\n                        causal_mask.shape[1]\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones(\n                                (batch_size, seq_length,\n                                 prefix_seq_len), device=device, dtype=causal_mask.dtype\n                            ),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )\n\n                extended_attention_mask = causal_mask[:, None,\n                                                      :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n            device = input_ids.device\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = inputs_embeds.device\n        elif encoder_embeds is not None:\n            input_shape = encoder_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n            device = encoder_embeds.device\n        else:\n            raise ValueError(\n                \"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)), device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape,\n                                                                                 device, is_decoder)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size(\n                )\n            else:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (\n                encoder_batch_size, encoder_sequence_length)\n\n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [\n                    self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask)\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(\n            head_mask, self.config.num_hidden_layers)\n\n        if encoder_embeds is None:\n            embedding_output = self.embeddings(\n                input_ids=input_ids,\n                position_ids=position_ids,\n                token_type_ids=token_type_ids,\n                inputs_embeds=inputs_embeds,\n                past_key_values_length=past_key_values_length,\n            )\n        else:\n            embedding_output = encoder_embeds\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            mode=mode,\n            normalize_attention=normalize_attention,\n\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(\n            sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n    sentence prediction (classification)` head.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(\n                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(\n                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n)\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=True,\n        reduction='mean',\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores[:, :-1, :].contiguous()\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:,\n                                                          :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(reduction=reduction)\n            lm_loss = loss_fct(\n                shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(shifted_prediction_scores,\n                          dim=1)*soft_labels, dim=-1)\n            loss_distill = (loss_distill * (labels != -100)).sum(1)\n            lm_loss = (1-alpha)*lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past,\n            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx)\n                               for past_state in layer_past),)\n        return reordered_past", "\n\n@dataclass\nclass MaskedLMOutputWithDistill(MaskedLMOutput):\n    loss_aux: Optional[torch.FloatTensor] = None\n    loss_distill: Optional[torch.FloatTensor] = None\n\n\n@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def tie_aux_decoder_weights(self, module, aux_modules):\n        \"\"\"Tie decoder weights of all `aux_modules` to `module`, (not bias)\"\"\"\n        for m in aux_modules:\n            m.predictions.decoder.weight = module.predictions.decoder.weight\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_embeds=encoder_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        masked_lm_loss_aux = 0.\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(prediction_scores, dim=1)\n                          * soft_labels, dim=-1)\n            loss_distill = loss_distill[labels != -100].mean()\n            masked_lm_loss = (1-alpha)*masked_lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        # changed from MaskedLMOutput to MaskedLMOutputWithDistill\n        return MaskedLMOutputWithDistill(\n            loss=masked_lm_loss,\n            loss_aux=masked_lm_loss_aux,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        effective_batch_size = input_shape[0]\n\n        #  add a dummy token\n        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n        attention_mask = torch.cat(\n            [attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}", "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def tie_aux_decoder_weights(self, module, aux_modules):\n        \"\"\"Tie decoder weights of all `aux_modules` to `module`, (not bias)\"\"\"\n        for m in aux_modules:\n            m.predictions.decoder.weight = module.predictions.decoder.weight\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n        mode='multi_modal',\n        normalize_attention=True,\n        soft_labels=None,\n        alpha=0,\n        return_logits=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_embeds=encoder_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n            mode=mode,\n            normalize_attention=normalize_attention\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        masked_lm_loss_aux = 0.\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if soft_labels is not None:\n            loss_distill = - \\\n                torch.sum(F.log_softmax(prediction_scores, dim=1)\n                          * soft_labels, dim=-1)\n            loss_distill = loss_distill[labels != -100].mean()\n            masked_lm_loss = (1-alpha)*masked_lm_loss + alpha*loss_distill\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        # changed from MaskedLMOutput to MaskedLMOutputWithDistill\n        return MaskedLMOutputWithDistill(\n            loss=masked_lm_loss,\n            loss_aux=masked_lm_loss_aux,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        effective_batch_size = input_shape[0]\n\n        #  add a dummy token\n        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n        attention_mask = torch.cat(\n            [attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n            >>> logits = outputs.logits\n            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n        \"\"\"\n\n        if \"next_sentence_label\" in kwargs:\n            warnings.warn(\n                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n                FutureWarning,\n            )\n            labels = kwargs.pop(\"next_sentence_label\")\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        seq_relationship_scores = self.cls(pooled_output)\n\n        next_sentence_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            next_sentence_loss = loss_fct(\n                seq_relationship_scores.view(-1, 2), labels.view(-1))\n\n        if not return_dict:\n            output = (seq_relationship_scores,) + outputs[2:]\n            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n\n        return NextSentencePredictorOutput(\n            loss=next_sentence_loss,\n            logits=seq_relationship_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)\n                                   ) if input_ids is not None else None\n        attention_mask = attention_mask.view(\n            -1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(\n            -1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)\n                                         ) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2),\n                               inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)\n                                   ) if input_ids is not None else None\n        attention_mask = attention_mask.view(\n            -1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(\n            -1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)\n                                         ) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2),\n                               inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(\n                        loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(\n                        loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(\n                    logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ""]}
{"filename": "models/tokenization_bert.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes for Bert.\"\"\"\n\n\nimport collections", "\nimport collections\nimport os\nimport unicodedata\nfrom typing import List, Optional, Tuple\n\nfrom transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\nfrom transformers.utils import logging\n\n", "\n\nlogger = logging.get_logger(__name__)\n\nVOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\",\n        \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt\",", "        \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\",\n        \"bert-large-uncased\": \"https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt\",\n        \"bert-base-cased\": \"https://huggingface.co/bert-base-cased/resolve/main/vocab.txt\",\n        \"bert-large-cased\": \"https://huggingface.co/bert-large-cased/resolve/main/vocab.txt\",\n        \"bert-base-multilingual-uncased\": \"https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt\",\n        \"bert-base-multilingual-cased\": \"https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt\",\n        \"bert-base-chinese\": \"https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\",\n        \"bert-base-german-cased\": \"https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt\",\n        \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt\",\n        \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/vocab.txt\",", "        \"bert-large-uncased-whole-word-masking\": \"https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt\",\n        \"bert-large-cased-whole-word-masking\": \"https://huggingface.co/bert-large-cased-whole-word-masking/resolve/main/vocab.txt\",\n        \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt\",\n        \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt\",\n        \"bert-base-cased-finetuned-mrpc\": \"https://huggingface.co/bert-base-cased-finetuned-mrpc/resolve/main/vocab.txt\",\n        \"bert-base-german-dbmdz-cased\": \"https://huggingface.co/bert-base-german-dbmdz-cased/resolve/main/vocab.txt\",\n        \"bert-base-german-dbmdz-uncased\": \"https://huggingface.co/bert-base-german-dbmdz-uncased/resolve/main/vocab.txt\",\n        \"TurkuNLP/bert-base-finnish-cased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt\",\n        \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/vocab.txt\",\n        \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/vocab.txt\",", "        \"TurkuNLP/bert-base-finnish-uncased-v1\": \"https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/resolve/main/vocab.txt\",\n        \"wietsedv/bert-base-dutch-cased\": \"https://huggingface.co/wietsedv/bert-base-dutch-cased/resolve/main/vocab.txt\",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"bert-base-uncased\": 512,\n    \"bert-large-uncased\": 512,\n    \"bert-base-cased\": 512,\n    \"bert-large-cased\": 512,", "    \"bert-base-cased\": 512,\n    \"bert-large-cased\": 512,\n    \"bert-base-multilingual-uncased\": 512,\n    \"bert-base-multilingual-cased\": 512,\n    \"bert-base-chinese\": 512,\n    \"bert-base-german-cased\": 512,\n    \"bert-large-uncased-whole-word-masking\": 512,\n    \"bert-large-cased-whole-word-masking\": 512,\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\": 512,\n    \"bert-large-cased-whole-word-masking-finetuned-squad\": 512,", "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": 512,\n    \"bert-large-cased-whole-word-masking-finetuned-squad\": 512,\n    \"bert-base-cased-finetuned-mrpc\": 512,\n    \"bert-base-german-dbmdz-cased\": 512,\n    \"bert-base-german-dbmdz-uncased\": 512,\n    \"TurkuNLP/bert-base-finnish-cased-v1\": 512,\n    \"TurkuNLP/bert-base-finnish-uncased-v1\": 512,\n    \"wietsedv/bert-base-dutch-cased\": 512,\n}\n", "}\n\nPRETRAINED_INIT_CONFIGURATION = {\n    \"bert-base-uncased\": {\"do_lower_case\": True},\n    \"bert-large-uncased\": {\"do_lower_case\": True},\n    \"bert-base-cased\": {\"do_lower_case\": False},\n    \"bert-large-cased\": {\"do_lower_case\": False},\n    \"bert-base-multilingual-uncased\": {\"do_lower_case\": True},\n    \"bert-base-multilingual-cased\": {\"do_lower_case\": False},\n    \"bert-base-chinese\": {\"do_lower_case\": False},", "    \"bert-base-multilingual-cased\": {\"do_lower_case\": False},\n    \"bert-base-chinese\": {\"do_lower_case\": False},\n    \"bert-base-german-cased\": {\"do_lower_case\": False},\n    \"bert-large-uncased-whole-word-masking\": {\"do_lower_case\": True},\n    \"bert-large-cased-whole-word-masking\": {\"do_lower_case\": False},\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": True},\n    \"bert-large-cased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": False},\n    \"bert-base-cased-finetuned-mrpc\": {\"do_lower_case\": False},\n    \"bert-base-german-dbmdz-cased\": {\"do_lower_case\": False},\n    \"bert-base-german-dbmdz-uncased\": {\"do_lower_case\": True},", "    \"bert-base-german-dbmdz-cased\": {\"do_lower_case\": False},\n    \"bert-base-german-dbmdz-uncased\": {\"do_lower_case\": True},\n    \"TurkuNLP/bert-base-finnish-cased-v1\": {\"do_lower_case\": False},\n    \"TurkuNLP/bert-base-finnish-uncased-v1\": {\"do_lower_case\": True},\n    \"wietsedv/bert-base-dutch-cased\": {\"do_lower_case\": False},\n}\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\"\\n\")\n        vocab[token] = index\n    return vocab", "def load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\"\\n\")\n        vocab[token] = index\n    return vocab\n", "\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n", "\n\nclass BertTokenizer(PreTrainedTokenizer):\n    r\"\"\"\n    Construct a BERT tokenizer. Based on WordPiece.\n    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n    Users should refer to this superclass for more information regarding those methods.\n    Args:\n        vocab_file (:obj:`str`):\n            File containing the vocabulary.\n        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to lowercase the input when tokenizing.\n        do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to do basic tokenization before WordPiece.\n        never_split (:obj:`Iterable`, `optional`):\n            Collection of tokens which will never be split during tokenization. Only has an effect when\n            :obj:`do_basic_tokenize=True`\n        unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead.\n        sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n            sequence classification or for a text and a question for question answering. It is also used as the last\n            token of a sequence built with special tokens.\n        pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n            The token used for padding, for example when batching sequences of different lengths.\n        cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n            The classifier token which is used when doing sequence classification (classification of the whole sequence\n            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n        mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n            The token used for masking values. This is the token used when training this model with masked language\n            modeling. This is the token which the model will try to predict.\n        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to tokenize Chinese characters.\n            This should likely be deactivated for Japanese (see this `issue\n            <https://github.com/huggingface/transformers/issues/328>`__).\n        strip_accents: (:obj:`bool`, `optional`):\n            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n            value for :obj:`lowercase` (as in the original BERT).\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(\n        self,\n        vocab_file,\n        do_lower_case=True,\n        do_basic_tokenize=True,\n        never_split=None,\n        unk_token=\"[UNK]\",\n        sep_token=\"[SEP]\",\n        pad_token=\"[PAD]\",\n        cls_token=\"[CLS]\",\n        mask_token=\"[MASK]\",\n        tokenize_chinese_chars=True,\n        strip_accents=None,\n        **kwargs\n    ):\n        super().__init__(\n            do_lower_case=do_lower_case,\n            do_basic_tokenize=do_basic_tokenize,\n            never_split=never_split,\n            unk_token=unk_token,\n            sep_token=sep_token,\n            pad_token=pad_token,\n            cls_token=cls_token,\n            mask_token=mask_token,\n            tokenize_chinese_chars=tokenize_chinese_chars,\n            strip_accents=strip_accents,\n            **kwargs,\n        )\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n                    vocab_file)\n            )\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=do_lower_case,\n                never_split=never_split,\n                tokenize_chinese_chars=tokenize_chinese_chars,\n                strip_accents=strip_accents,\n            )\n        self.wordpiece_tokenizer = WordpieceTokenizer(\n            vocab=self.vocab, unk_token=self.unk_token)\n\n    @property\n    def do_lower_case(self):\n        return self.basic_tokenizer.do_lower_case\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n    def get_vocab(self):\n        return dict(self.vocab, **self.added_tokens_encoder)\n\n    def _tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n\n                # If the token is part of the never_split set\n                if token in self.basic_tokenizer.never_split:\n                    split_tokens.append(token)\n                else:\n                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n        return self.vocab.get(token, self.vocab.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n        adding special tokens. A BERT sequence has the following format:\n        - single sequence: ``[CLS] X ``\n        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n        Returns:\n            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n    ) -> List[int]:\n        \"\"\"\n        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` method.\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Whether or not the token list is already formatted with special tokens for the model.\n        Returns:\n            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(\n                    \"You should not supply a second sequence if the provided sequence of \"\n                    \"ids is already formatted with special tokens for the model.\"\n                )\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -> List[int]:\n        \"\"\"\n        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n        pair mask has the following format:\n        ::\n            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n            | first sequence    | second sequence |\n        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs.\n            token_ids_1 (:obj:`List[int]`, `optional`):\n                Optional second list of IDs for sequence pairs.\n        Returns:\n            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n            sequence(s).\n        \"\"\"\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n        index = 0\n        if os.path.isdir(save_directory):\n            vocab_file = os.path.join(\n                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") +\n                VOCAB_FILES_NAMES[\"vocab_file\"]\n            )\n        else:\n            vocab_file = (filename_prefix +\n                          \"-\" if filename_prefix else \"\") + save_directory\n        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(\n                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n                        \" Please check that the vocabulary is not corrupted!\".format(\n                            vocab_file)\n                    )\n                    index = token_index\n                writer.write(token + \"\\n\")\n                index += 1\n        return (vocab_file,)", "\n\nclass BasicTokenizer(object):\n    \"\"\"\n    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n    Args:\n        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to lowercase the input when tokenizing.\n        never_split (:obj:`Iterable`, `optional`):\n            Collection of tokens which will never be split during tokenization. Only has an effect when\n            :obj:`do_basic_tokenize=True`\n        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to tokenize Chinese characters.\n            This should likely be deactivated for Japanese (see this `issue\n            <https://github.com/huggingface/transformers/issues/328>`__).\n        strip_accents: (:obj:`bool`, `optional`):\n            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n            value for :obj:`lowercase` (as in the original BERT).\n    \"\"\"\n\n    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True, strip_accents=None):\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = set(never_split)\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n        self.strip_accents = strip_accents\n\n    def tokenize(self, text, never_split=None):\n        \"\"\"\n        Basic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n        WordPieceTokenizer.\n        Args:\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.\n        \"\"\"\n        # union() returns a new set by concatenating the two sets.\n        never_split = self.never_split.union(\n            set(never_split)) if never_split else self.never_split\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if token not in never_split:\n                if self.do_lower_case:\n                    token = token.lower()\n                    if self.strip_accents is not False:\n                        token = self._run_strip_accents(token)\n                elif self.strip_accents:\n                    token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token, never_split))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text, never_split=None):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if (\n            (cp >= 0x4E00 and cp <= 0x9FFF)\n            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n            or (cp >= 0xF900 and cp <= 0xFAFF)\n            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n        ):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xFFFD or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)", "\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenization.\"\"\"\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"\n        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n        tokenization using the given vocabulary.\n        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens", ""]}
{"filename": "models/model_retrieval.py", "chunked_list": ["from models.model_retrieval_base import SingularityRetrievalBase\n\n\nclass Singularity(SingularityRetrievalBase):\n    def __init__(self, config=None, tokenizer=None):\n        super(Singularity, self).__init__(\n            config=config, tokenizer=tokenizer, pretrain=False\n        )\n", ""]}
{"filename": "models/model_retrieval_base.py", "chunked_list": ["from models.utils import (\n    interpolate_pos_embed,\n    interpolate_pos_relative_bias_beit,\n    interpolate_pos_relative_bias_beit_3d,\n    _init_transformer_weights\n)\nfrom omegaconf import OmegaConf\nfrom transformers import ViTModel, ViTConfig\nfrom models.sparse_config import BertConfig, BeitConfig\nfrom models.sparse_xbeit import BeitModel", "from models.sparse_config import BertConfig, BeitConfig\nfrom models.sparse_xbeit import BeitModel\nfrom models.sparse_xbert import BertModel, BertForMaskedLM, BertEncoder\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport logging\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass SingularityRetrievalBase(nn.Module):\n    \"\"\"Common utils shared by pretraining and downstream retrieval\"\"\"\n    def __init__(self, config=None, tokenizer=None, pretrain=True, **kwargs):\n        super().__init__()\n        self.config = config\n        self.tokenizer = tokenizer\n        self.embed_dim = config.embed_dim\n        self.vision_width = 768\n        self.text_width = 768\n        self.pretrain = pretrain\n\n        self.vision_encoder, self.vision_layernorm = self.build_vision_encoder()\n        self.text_encoder = self.build_text_encoder()\n\n        self.vision_proj = nn.Linear(self.vision_width, self.embed_dim)\n        self.text_proj = nn.Linear(self.text_width, self.embed_dim)\n\n        self.temp = nn.Parameter(torch.ones([]) * config.temp)\n        self.itm_head = nn.Linear(self.text_width, 2)\n\n    def forward(self, image, text, idx):\n        # ================= Dual Encoder ITC loss ================ #\n        self.clip_contrastive_temperature()\n\n        image_embeds, pooled_image_embeds = self.encode_image(image)\n        text_embeds, pooled_text_embeds = self.encode_text(text)\n\n        loss_ita, sim_i2t, sim_t2i = self.get_contrastive_loss(\n            pooled_image_embeds, pooled_text_embeds, idx)\n\n        # ================= Multi-Modal Encoder ITM loss ================ #\n        image_atts = torch.ones(\n            image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n        loss_itm, ce_cls_embeds_pos = self.get_itm_loss(\n            sim_i2t, sim_t2i, text_embeds, text.attention_mask,\n            image_embeds, image_atts, idx=idx)\n\n        return_dict = dict(\n            loss_ita=loss_ita * self.config.loss_weight.itc,\n            loss_itm=loss_itm * self.config.loss_weight.itm\n        )\n\n        # ================= Multi-Modal Encoder MLM loss ======================== #\n        if self.pretrain:\n            loss_mlm = self.get_mlm_loss(text, image_embeds, image_atts)\n            return_dict.update(loss_mlm=loss_mlm * self.config.loss_weight.mlm)\n\n        return return_dict\n\n    def build_text_encoder(self):\n        logger.info(f\"Build text_encoder {self.config.text_encoder}\")\n\n        bert_config = BertConfig.from_json_file(self.config.bert_config)\n\n        # Override params for sparse vision encoder\n        model_args = getattr(self.config, 'text_encoder_args', {})\n        if model_args:\n            model_args = OmegaConf.to_object(model_args)\n            bert_config.update(model_args)\n\n        if self.pretrain:\n            text_encoder, loading_info = BertForMaskedLM.from_pretrained(\n                self.config.text_encoder, config=bert_config,\n                output_loading_info=True\n            )\n        else:\n            text_encoder, loading_info = BertModel.from_pretrained(\n                self.config.text_encoder, config=bert_config,\n                add_pooling_layer=False, output_loading_info=True\n            )\n        if self.config.debug:\n            for k, v in loading_info.items():\n                logger.debug(f\"loading_info[{k}]: {v}\")\n        logger.info(f\"Build text_encoder {self.config.text_encoder}, done!\")\n        return text_encoder\n\n    def build_vision_encoder(self):\n        # if self.config.vit_type in [\"beit\", \"deit\", \"vit\", \"vit32\"]:\n        if self.config.vit_type in [\"beit\"]:\n            vision_encoder = self.build_huggingface_vit_with_image_size(\n                self.config.vit_name_or_pretrained_path, self.config.image_res,)\n        else:\n            raise ValueError(f\"Unknown vit type {self.config.vit_type}\")\n\n        # add layernorm for normalizing BEiT outputs hidden states\n        vision_layernorm = None\n        if self.config.vit_type == \"beit\":\n            vision_layernorm = nn.LayerNorm(self.vision_width, eps=1e-12)\n        return vision_encoder, vision_layernorm\n\n    # @classmethod\n    # def build_huggingface_vit_with_image_size(cls, model_card: str, image_size: int):\n    def build_huggingface_vit_with_image_size(self, model_card: str, image_size: int):\n        \"\"\"Build a vit model from huggingface hub, also interpolate pos_embed when needed.\n\n        Args:\n            model_card: name in huggingface hub, e.g., `facebook/deit-base-patch16-224`\n            image_size: new image size, may be different from pre-training image_size of `model_card`\n\n        ref: https://github.com/huggingface/transformers/issues/12167#issuecomment-861356232\n        \"\"\"\n        is_beit = \"beit\" in model_card\n        if \"beit\" in model_card:\n            model_cls, config_cls = BeitModel, BeitConfig\n        elif \"deit\" in model_card or \"vit\" in model_card:\n            # the deit model we use is loaded in vit arch,\n            # see https://huggingface.co/facebook/deit-base-patch16-224#how-to-use\n            model_cls, config_cls = ViTModel, ViTConfig\n        else:\n            raise ValueError(f\"Unexpected model_card: {model_card}\")\n\n        logger.info(f\"Loading vit pre-trained weights from huggingface {model_card}.\")\n        # BEiT uses average pooled tokens instead of [CLS] used by other models\n        tmp_model = model_cls.from_pretrained(model_card, add_pooling_layer=is_beit)\n        state_dict = tmp_model.state_dict()\n        del tmp_model\n\n        logger.info(f\"Init new model with new image size {image_size}, and load weights.\")\n\n        # Override params for sparse vision encoder\n        model_args = getattr(self.config, 'vision_encoder_args', {})\n        if model_args:\n            model_args = OmegaConf.to_object(model_args)\n        model_config = config_cls.from_pretrained(model_card, image_size=image_size, **model_args)\n        model = model_cls(config=model_config, add_pooling_layer=is_beit, num_frames=self.config.video_input.num_frames)\n        if is_beit:\n            # interpolate relative pos bias\n            state_dict = interpolate_pos_relative_bias_beit_3d(\n                state_dict_old=state_dict,\n                state_dict_new=model.state_dict(),\n                patch_shape_new=model.window_size\n            )\n        else:\n            # interpolate pos_embed and load weights to new model\n            state_dict[\"embeddings.position_embeddings\"] = interpolate_pos_embed(\n                pos_embed_old=state_dict[\"embeddings.position_embeddings\"],\n                pos_embed_new=model.embeddings.position_embeddings,\n                num_patches_new=model.embeddings.patch_embeddings.num_patches\n            )\n        msg = model.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        return model\n\n    def get_text_encoder(self):\n        \"\"\"get text encoder, used for text and cross-modal encoding\"\"\"\n        encoder = self.text_encoder\n        return encoder.bert if hasattr(encoder, \"bert\") else encoder\n\n    def encode_image(self, video, output_token_idx=False, output_attentions=False):\n        video_embeds = self.vision_encoder(video, output_token_idx=output_token_idx, output_attentions=output_attentions)  # (bsz, seq_len, d)\n        if self.vision_layernorm is not None:  # only for BEiT mean-pooling\n            video_embeds.last_hidden_state = self.vision_layernorm(video_embeds.last_hidden_state)\n\n        if output_token_idx:\n            token_idx = video_embeds.token_idx\n        \n        if output_attentions:\n            attentions = video_embeds.attentions\n            \n        if self.config.vit_type == \"beit\":\n            pooled_video_embeds = video_embeds.pooler_output  # (bsz*num_frms, d)\n            video_embeds = video_embeds.last_hidden_state  # (bsz*num_frms, L, d)\n        else:\n            video_embeds = video_embeds.last_hidden_state\n            pooled_video_embeds = video_embeds[:, 0]\n        \n        outputs = (video_embeds, pooled_video_embeds)\n\n        if output_token_idx:\n            outputs += (token_idx,)\n        \n        if output_attentions:\n            outputs += (attentions,)\n\n        return outputs\n\n    def _encode_image(self, image):\n        bsz, num_frms, c, h, w = image.shape  # `num_frms` could be changing for image (=1) or video (e.g., =4)\n        image = image.view(bsz*num_frms, c, h, w)\n        image_embeds = self.vision_encoder(image)\n        if self.vision_layernorm is not None:  # only for BEiT mean-pooling\n            image_embeds.last_hidden_state = self.vision_layernorm(image_embeds.last_hidden_state)\n\n        if self.config.vit_type == \"beit\":\n            pooled_image_embeds = image_embeds.pooler_output  # (bsz*num_frms, d)\n            image_embeds = image_embeds.last_hidden_state  # (bsz*num_frms, L, d)\n        else:\n            image_embeds = image_embeds.last_hidden_state\n            pooled_image_embeds = image_embeds[:, 0]\n\n        image_embeds = image_embeds.view(bsz, num_frms, -1, self.vision_width)  # (bsz, num_frms, L, d)\n        pooled_image_embeds = pooled_image_embeds.view(bsz, num_frms, self.vision_width) \\\n            if pooled_image_embeds is not None else None  # (bsz, num_frms, d)\n        return image_embeds, pooled_image_embeds\n\n    def encode_text(self, text):\n        # text\n        text_output = self.get_text_encoder()(\n            text.input_ids,\n            attention_mask=text.attention_mask,\n            return_dict=True,\n            mode='text'\n        )\n        text_embeds = text_output.last_hidden_state\n        pooled_text_embeds = text_embeds[:, 0]\n        return text_embeds, pooled_text_embeds\n\n    @torch.no_grad()\n    def clip_contrastive_temperature(self, min_val=0.001, max_val=0.5):\n        \"\"\"Seems only used during pre-training\"\"\"\n        self.temp.clamp_(min_val, max_val)\n\n    @torch.no_grad()\n    def get_mask(self, sim, idx=None, normalize=False):\n        \"\"\"\n        sim: (N, N)\n        idx: (N, )\n        normalize: bool, make row sum equal to 1\n        \"\"\"\n        if idx is not None:\n            idx = idx.view(-1, 1)\n            mask = torch.eq(idx, idx.T).to(sim.dtype)\n            if normalize:  \n                mask = mask / mask.sum(1, keepdim=True)\n        else:\n            mask = torch.zeros_like(sim)\n            mask.fill_diagonal_(1)\n        return mask  # `1` mark valid/matched location\n\n    def get_contrastive_loss(self, pooled_image_embeds, pooled_text_embeds, idx=None):\n        sim_i2t, sim_t2i = self.get_sim(\n            pooled_image_embeds, pooled_text_embeds, t=self.temp)\n\n        with torch.no_grad():\n            sim_i2t_targets = self.get_mask(sim_i2t, idx=idx, normalize=True)\n            sim_t2i_targets = sim_i2t_targets\n\n        loss_i2t = -torch.sum(\n            F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean()\n        loss_t2i = -torch.sum(\n            F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean()\n\n        loss_ita = (loss_i2t + loss_t2i) / 2\n        return loss_ita, sim_i2t, sim_t2i\n\n    def get_sim(self, pooled_image_embeds, pooled_text_embeds, t=1):\n        \"\"\"\n        Args:\n            pooled_image_embeds: (bsz, num_frms, d)\n            pooled_text_embeds: (bsz, d)\n            t: temperature\n        \"\"\"\n        image_proj = self.vision_proj\n        text_proj = self.text_proj\n\n        image_feat = F.normalize(image_proj(pooled_image_embeds), dim=-1)\n        text_feat = F.normalize(text_proj(pooled_text_embeds), dim=-1)\n\n        if image_feat.ndim == 3:\n            sim_i2t = torch.einsum(\"mld,nd->mln\", image_feat, text_feat).mean(1) / t  # (N, N)\n        else:\n            sim_i2t = torch.einsum(\"md,nd ->mn\", image_feat, text_feat) / t  # (N, N)\n        sim_t2i = sim_i2t.T\n        return sim_i2t, sim_t2i\n\n    def get_itm_loss(\n            self, sim_i2t, sim_t2i, text_embeds, text_atts, image_embeds, image_atts, idx=None):\n        \"\"\"\n        sim_i2t, sim_t2i: (N, N)\n        text_embeds, text_atts, image_embeds, image_atts: (N, *)\n        idx: (N, )\n        \"\"\"\n        bsz = len(sim_i2t)\n        text_encoder = self.get_text_encoder()\n\n        with torch.no_grad():\n            weights_i2t = F.softmax(sim_i2t+1e-4, dim=1)  # (N, N)\n            weights_t2i = F.softmax(sim_t2i+1e-4, dim=1)\n\n            mask = self.get_mask(sim_i2t, idx=idx).bool()\n            weights_i2t.masked_fill_(mask, 0)\n            weights_t2i.masked_fill_(mask, 0)\n\n        # select a negative image for each text\n        if self.config.itm_hard_neg:\n            img_neg_indices = torch.multinomial(weights_t2i, 1).squeeze()\n        else:\n            img_neg_indices = self.get_rand_indices(mask, 1).squeeze()            \n\n        image_embeds_neg = image_embeds[img_neg_indices]\n\n        # select a negative text for each image\n        if self.config.itm_hard_neg:\n            txt_neg_indices = torch.multinomial(weights_i2t, 1).squeeze()\n        else:\n            txt_neg_indices = self.get_rand_indices(mask, 1).squeeze()\n\n        text_embeds_neg = text_embeds[txt_neg_indices]\n        text_atts_neg = text_atts[txt_neg_indices]  # (N, L, d)\n\n        # embedding on local gpu\n        _text_embeds = text_embeds  \n        _text_atts = text_atts\n        _image_embeds = image_embeds\n        _image_atts = image_atts\n        # concat embeddings\n        text_embeds_all = torch.cat([_text_embeds, _text_embeds, text_embeds_neg], dim=0)\n        text_atts_all = torch.cat([_text_atts, _text_atts, text_atts_neg], dim=0)\n        image_embeds_all = torch.cat([_image_embeds, image_embeds_neg, _image_embeds], dim=0)\n        image_atts_all = torch.cat([_image_atts, _image_atts, _image_atts], dim=0)\n        output = text_encoder(\n            encoder_embeds=text_embeds_all,\n            attention_mask=text_atts_all,\n            encoder_hidden_states=image_embeds_all,\n            encoder_attention_mask=image_atts_all,\n            return_dict=True,\n            mode='fusion',\n        )\n\n        itm_embeds = output.last_hidden_state[:, 0]  # pos (N, d) + neg (2N, d)\n\n        loss_itm = self._get_itm_loss(itm_embeds, enc=self.itm_head)\n        itm_embeds_pos = itm_embeds[:bsz]  # (N, d)\n\n        return loss_itm, itm_embeds_pos\n\n    def _get_itm_loss(self, itm_embeds, enc):\n        \"\"\"\n        itm_embeds: (3*N, D)\n        enc: nn.Module that projects cls_embeds\n        \"\"\"\n        itm_scores = enc(itm_embeds)  # (3*N, 2)\n        bs = itm_scores.size(0) // 3\n        itm_labels = itm_scores.new_ones(3*bs, dtype=torch.long)\n        itm_labels[bs:] = 0\n        loss_itm = F.cross_entropy(itm_scores, itm_labels)\n        return loss_itm\n\n    def get_rand_indices(self, mask, k):\n        \"\"\"\n        Args:\n            mask: (N, L) 0 indicates the positions that we can sample, 1 otherwise\n            k: #indices to sample at each row\n        Returns:\n            (N, k) indices\n        \"\"\"\n        mask = mask.float()\n        mask = mask - 10000 * mask\n        mask += torch.randn_like(mask)\n        _, indices = torch.sort(mask, dim=1, descending=True)\n        indices = indices[:, :k].contiguous()  \n        return indices", ""]}
{"filename": "tasks/shared_utils.py", "chunked_list": ["import torch\nimport copy\nfrom models.utils import (\n    interpolate_pos_embed, \n    interpolate_pos_relative_bias_beit, \n    interpolate_pos_relative_bias_beit_3d\n)\nfrom models.tokenization_bert import BertTokenizer\n\nfrom utils.scheduler import create_scheduler", "\nfrom utils.scheduler import create_scheduler\nfrom utils.optimizer import create_optimizer\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_model(config, model_cls, has_decoder=False, pretrain=False, find_unused_parameters=False):\n    logger.info(\"Creating model\")\n    config = copy.deepcopy(config)\n\n    tokenizer = BertTokenizer.from_pretrained(config.text_encoder)\n    model = model_cls(config=config, tokenizer=tokenizer)\n\n    model = model.to(torch.device(config.device))\n    model_without_ddp = model\n    if config.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[config.gpu],\n            find_unused_parameters=find_unused_parameters  # `False` for image-only task\n        )\n\n    optimizer = create_optimizer(config.optimizer, model)\n    scheduler = create_scheduler(config.scheduler, optimizer)\n    scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n\n    start_epoch = 0\n    global_step = 0\n    if config.pretrained_path:\n        logger.info(f\"Loading checkpoint from {config.pretrained_path}\")\n        checkpoint = torch.load(config.pretrained_path, map_location=\"cpu\")\n        state_dict = checkpoint[\"model\"]\n\n        if config.evaluate:\n            # fix for zero-shot evaluation\n            for key in list(state_dict.keys()):\n                if \"bert\" in key:\n                    encoder_key = key.replace(\"bert.\", \"\")\n                    state_dict[encoder_key] = state_dict[key]\n                    if not has_decoder:\n                        del state_dict[key]\n        elif config.resume:\n            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            scheduler.load_state_dict(checkpoint[\"scheduler\"])\n            scaler.load_state_dict(checkpoint[\"scaler\"])\n            start_epoch = checkpoint[\"epoch\"] + 1\n            global_step = checkpoint[\"global_step\"]\n        # elif not pretrain:  # downstream init from pretrained ckpt\n        else:  # init from pretrained ckpt\n            # reshape positional embeddings\n            is_beit = \"beit\" in config.vit_type\n            if is_beit:\n                # interpolate relative pos bias\n                if 'config' in checkpoint:\n                    src_t_size = checkpoint['config']['video_input']['num_frames'] * 2 - 1\n                else:\n                    src_t_size = 1\n                state_dict = interpolate_pos_relative_bias_beit_3d(\n                    state_dict_old=state_dict,\n                    state_dict_new=model_without_ddp.state_dict(),\n                    patch_shape_new=model_without_ddp.vision_encoder.window_size,\n                    src_t_size=src_t_size\n                )\n            else:\n                # interpolate pos_embed\n                state_dict[\"vision_encoder.embeddings.position_embeddings\"] = \\\n                    interpolate_pos_embed(\n                        pos_embed_old=state_dict[\"vision_encoder.embeddings.position_embeddings\"],\n                        pos_embed_new=model_without_ddp.vision_encoder.embeddings.position_embeddings,\n                        num_patches_new=model_without_ddp.vision_encoder.embeddings.patch_embeddings.num_patches\n                    )\n            if not pretrain:  # downstream init from pretrained ckpt\n                for key in list(state_dict.keys()):\n                    if \"bert\" in key:\n                        encoder_key = key.replace(\"bert.\", \"\")\n                        state_dict[encoder_key] = state_dict[key]\n                        if not has_decoder:\n                            del state_dict[key]\n\n                    # init text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n                    # only for generation tasks like VQA\n                    if has_decoder and \"text_encoder\" in key:\n                        if \"layer\" in key:\n                            encoder_keys = key.split(\".\")\n                            layer_num = int(encoder_keys[4])\n                            if layer_num < 9:  # configs/config_bert.fusion_layer\n                                del state_dict[key]\n                                continue\n                            else:\n                                decoder_layer_num = (layer_num-9)\n                                encoder_keys[4] = str(decoder_layer_num)\n                                encoder_key = \".\".join(encoder_keys)\n                        else:\n                            encoder_key = key\n                        decoder_key = encoder_key.replace(\"text_encoder\", \"text_decoder\")\n                        state_dict[decoder_key] = state_dict[key]\n                        del state_dict[key]\n\n        msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"Loaded checkpoint from {config.pretrained_path}\")\n    else:\n        logger.warning(\"No pretrained checkpoint provided, training from scratch\")\n\n    return model, model_without_ddp, optimizer, scheduler, scaler, tokenizer, start_epoch, global_step", "\ndef setup_model(config, model_cls, has_decoder=False, pretrain=False, find_unused_parameters=False):\n    logger.info(\"Creating model\")\n    config = copy.deepcopy(config)\n\n    tokenizer = BertTokenizer.from_pretrained(config.text_encoder)\n    model = model_cls(config=config, tokenizer=tokenizer)\n\n    model = model.to(torch.device(config.device))\n    model_without_ddp = model\n    if config.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[config.gpu],\n            find_unused_parameters=find_unused_parameters  # `False` for image-only task\n        )\n\n    optimizer = create_optimizer(config.optimizer, model)\n    scheduler = create_scheduler(config.scheduler, optimizer)\n    scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n\n    start_epoch = 0\n    global_step = 0\n    if config.pretrained_path:\n        logger.info(f\"Loading checkpoint from {config.pretrained_path}\")\n        checkpoint = torch.load(config.pretrained_path, map_location=\"cpu\")\n        state_dict = checkpoint[\"model\"]\n\n        if config.evaluate:\n            # fix for zero-shot evaluation\n            for key in list(state_dict.keys()):\n                if \"bert\" in key:\n                    encoder_key = key.replace(\"bert.\", \"\")\n                    state_dict[encoder_key] = state_dict[key]\n                    if not has_decoder:\n                        del state_dict[key]\n        elif config.resume:\n            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n            scheduler.load_state_dict(checkpoint[\"scheduler\"])\n            scaler.load_state_dict(checkpoint[\"scaler\"])\n            start_epoch = checkpoint[\"epoch\"] + 1\n            global_step = checkpoint[\"global_step\"]\n        # elif not pretrain:  # downstream init from pretrained ckpt\n        else:  # init from pretrained ckpt\n            # reshape positional embeddings\n            is_beit = \"beit\" in config.vit_type\n            if is_beit:\n                # interpolate relative pos bias\n                if 'config' in checkpoint:\n                    src_t_size = checkpoint['config']['video_input']['num_frames'] * 2 - 1\n                else:\n                    src_t_size = 1\n                state_dict = interpolate_pos_relative_bias_beit_3d(\n                    state_dict_old=state_dict,\n                    state_dict_new=model_without_ddp.state_dict(),\n                    patch_shape_new=model_without_ddp.vision_encoder.window_size,\n                    src_t_size=src_t_size\n                )\n            else:\n                # interpolate pos_embed\n                state_dict[\"vision_encoder.embeddings.position_embeddings\"] = \\\n                    interpolate_pos_embed(\n                        pos_embed_old=state_dict[\"vision_encoder.embeddings.position_embeddings\"],\n                        pos_embed_new=model_without_ddp.vision_encoder.embeddings.position_embeddings,\n                        num_patches_new=model_without_ddp.vision_encoder.embeddings.patch_embeddings.num_patches\n                    )\n            if not pretrain:  # downstream init from pretrained ckpt\n                for key in list(state_dict.keys()):\n                    if \"bert\" in key:\n                        encoder_key = key.replace(\"bert.\", \"\")\n                        state_dict[encoder_key] = state_dict[key]\n                        if not has_decoder:\n                            del state_dict[key]\n\n                    # init text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n                    # only for generation tasks like VQA\n                    if has_decoder and \"text_encoder\" in key:\n                        if \"layer\" in key:\n                            encoder_keys = key.split(\".\")\n                            layer_num = int(encoder_keys[4])\n                            if layer_num < 9:  # configs/config_bert.fusion_layer\n                                del state_dict[key]\n                                continue\n                            else:\n                                decoder_layer_num = (layer_num-9)\n                                encoder_keys[4] = str(decoder_layer_num)\n                                encoder_key = \".\".join(encoder_keys)\n                        else:\n                            encoder_key = key\n                        decoder_key = encoder_key.replace(\"text_encoder\", \"text_decoder\")\n                        state_dict[decoder_key] = state_dict[key]\n                        del state_dict[key]\n\n        msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n        logger.info(msg)\n        logger.info(f\"Loaded checkpoint from {config.pretrained_path}\")\n    else:\n        logger.warning(\"No pretrained checkpoint provided, training from scratch\")\n\n    return model, model_without_ddp, optimizer, scheduler, scaler, tokenizer, start_epoch, global_step"]}
{"filename": "tasks/pretrain.py", "chunked_list": ["import pandas as pd\nimport time\nimport datetime\nimport wandb\nfrom os.path import join\nimport logging\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist", "import torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom models.model_pretrain import Singularity\n\nfrom utils.logger import log_dict_to_wandb, setup_wandb\nfrom utils.config_utils import setup_main\nfrom utils.basic_utils import MetricLogger, SmoothedValue, setup_seed, remove_files_if_exist\nfrom utils.distributed import get_rank, get_world_size, is_main_process\nfrom dataset import create_dataset, create_sampler, create_loader, MetaLoader", "from utils.distributed import get_rank, get_world_size, is_main_process\nfrom dataset import create_dataset, create_sampler, create_loader, MetaLoader\nfrom tasks.retrieval_utils import evaluation_wrapper\nfrom tasks.shared_utils import setup_model\n\nlogger = logging.getLogger(__name__)\n\n    \ndef train(model, train_loaders, optimizer, tokenizer, epoch, global_step,\n          device, scheduler, scaler, config):\n    model.train()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", SmoothedValue(window=100, fmt=\"{value:.6f}\"))\n    metric_logger.add_meter(\"temperature\", SmoothedValue(window=100, fmt=\"{value:.4f}\"))\n    loss_names = [\"loss_mlm\", \"loss_ita\", \"loss_itm\"]\n    media_types = [loader.dataset.media_type for loader in train_loaders]\n    for name in loss_names:\n        for m in media_types:\n            metric_logger.add_meter(f\"{m}-{name}\", SmoothedValue(window=100, fmt=\"{value:.4f}\"))\n\n    header = f\"Train Epoch: [{epoch}]\"\n    log_freq = config.log_freq\n\n    if config.distributed:\n        for d in train_loaders:\n            d.sampler.set_epoch(epoch)\n    train_loader = MetaLoader(name2loader=dict(list(zip(media_types, train_loaders))))\n\n    model_without_ddp = model.module if config.distributed else model\n    iterator = metric_logger.log_every(train_loader, log_freq, header)\n    for i, (media_type,  (image, text, idx)) in enumerate(iterator):\n        image = image.to(device, non_blocking=True)\n        idx = idx.to(device, non_blocking=True)\n        text_input = tokenizer(\n            text, padding=\"max_length\", truncation=True,\n            max_length=config.max_txt_l[media_type], return_tensors=\"pt\"\n        ).to(device)  # change from \"longest\" to \"max_length\"\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            loss_dict = model(image, text_input, idx=idx)\n            loss = sum(loss_dict.values())\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        if config.optimizer.max_grad_norm > 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), config.optimizer.max_grad_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        # logging\n        for name in loss_names:\n            value = loss_dict[name]\n            value = value if isinstance(value, float) else value.item()\n            metric_logger.update(**{f\"{media_type}-{name}\": value})\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        metric_logger.update(temperature=model_without_ddp.temp.item())\n\n        if is_main_process() and config.wandb.enable \\\n                and global_step % log_freq == 0:\n            logs = metric_logger.get_global_avg_dict()\n            log_dict_to_wandb(logs, step=global_step, prefix=\"train/\")\n\n        global_step += 1\n\n        if config.debug and global_step % (2 * log_freq + 3) == 0:\n            logger.info(\"debug mode, break training loop\")\n            break\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    logger.info(f\"Averaged stats: {metric_logger.global_avg()}\")\n    return global_step", "def train(model, train_loaders, optimizer, tokenizer, epoch, global_step,\n          device, scheduler, scaler, config):\n    model.train()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", SmoothedValue(window=100, fmt=\"{value:.6f}\"))\n    metric_logger.add_meter(\"temperature\", SmoothedValue(window=100, fmt=\"{value:.4f}\"))\n    loss_names = [\"loss_mlm\", \"loss_ita\", \"loss_itm\"]\n    media_types = [loader.dataset.media_type for loader in train_loaders]\n    for name in loss_names:\n        for m in media_types:\n            metric_logger.add_meter(f\"{m}-{name}\", SmoothedValue(window=100, fmt=\"{value:.4f}\"))\n\n    header = f\"Train Epoch: [{epoch}]\"\n    log_freq = config.log_freq\n\n    if config.distributed:\n        for d in train_loaders:\n            d.sampler.set_epoch(epoch)\n    train_loader = MetaLoader(name2loader=dict(list(zip(media_types, train_loaders))))\n\n    model_without_ddp = model.module if config.distributed else model\n    iterator = metric_logger.log_every(train_loader, log_freq, header)\n    for i, (media_type,  (image, text, idx)) in enumerate(iterator):\n        image = image.to(device, non_blocking=True)\n        idx = idx.to(device, non_blocking=True)\n        text_input = tokenizer(\n            text, padding=\"max_length\", truncation=True,\n            max_length=config.max_txt_l[media_type], return_tensors=\"pt\"\n        ).to(device)  # change from \"longest\" to \"max_length\"\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            loss_dict = model(image, text_input, idx=idx)\n            loss = sum(loss_dict.values())\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        if config.optimizer.max_grad_norm > 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), config.optimizer.max_grad_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        # logging\n        for name in loss_names:\n            value = loss_dict[name]\n            value = value if isinstance(value, float) else value.item()\n            metric_logger.update(**{f\"{media_type}-{name}\": value})\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        metric_logger.update(temperature=model_without_ddp.temp.item())\n\n        if is_main_process() and config.wandb.enable \\\n                and global_step % log_freq == 0:\n            logs = metric_logger.get_global_avg_dict()\n            log_dict_to_wandb(logs, step=global_step, prefix=\"train/\")\n\n        global_step += 1\n\n        if config.debug and global_step % (2 * log_freq + 3) == 0:\n            logger.info(\"debug mode, break training loop\")\n            break\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    logger.info(f\"Averaged stats: {metric_logger.global_avg()}\")\n    return global_step", "\n\ndef setup_dataloaders(config, mode=\"pt\"):\n    # train datasets, create a list of data loaders\n    logger.info(f\"Creating dataset for {mode}\")\n    train_datasets = create_dataset(f\"{mode}_train\", config)\n    media_types = [d.media_type for d in train_datasets]\n\n    if config.distributed:\n        num_tasks = get_world_size()\n        global_rank = get_rank()\n        samplers = create_sampler(\n            train_datasets, [True] * len(media_types), num_tasks, global_rank)\n    else:\n        samplers = [None] * len(media_types)\n\n    train_loaders = create_loader(\n        train_datasets, samplers,\n        batch_size=[config.batch_size[k] for k in media_types],\n        num_workers=[config.num_workers] * len(media_types),\n        is_trains=[True] * len(media_types),\n        collate_fns=[None] * len(media_types),\n    )  # [0]\n\n    # test datasets, a mapping from dataset name to data loader\n    test_datasets, test_dataset_names = create_dataset(f\"{mode}_eval\", config)\n    test_loaders = create_loader(\n        test_datasets, [None] * len(test_datasets),\n        batch_size=[config.batch_size_test[d.media_type] for d in test_datasets],\n        num_workers=[config.num_workers] * len(test_datasets),\n        is_trains=[False] * len(test_datasets),\n        collate_fns=[None] * len(test_datasets)\n    )\n    test_name2loaders = {k: v for k, v in zip(test_dataset_names, test_loaders)}\n    return train_loaders, test_name2loaders, media_types", "\n\ndef main(config):\n    if is_main_process() and config.wandb.enable:\n        run = setup_wandb(config)\n\n    logger.info(f\"config: \\n{config}\")\n    logger.info(f\"train_file: {config.train_file}\")\n\n    setup_seed(config.seed + get_rank())\n    device = torch.device(config.device)\n\n    train_loaders, test_name2loaders, train_media_types = setup_dataloaders(config, mode=\"pt\")\n    num_steps_per_epoch = sum(len(d) for d in train_loaders)\n    config.scheduler.num_training_steps = num_steps_per_epoch * config.scheduler.epochs\n    config.scheduler.num_warmup_steps = num_steps_per_epoch * config.scheduler.warmup_epochs\n    # set cudnn.benchmark=True only when input size is fixed\n    # https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/3\n    cudnn.benchmark = len(train_media_types) == 1\n\n    model, model_without_ddp, optimizer, scheduler, scaler, \\\n        tokenizer, start_epoch, global_step = setup_model(\n            config,\n            model_cls=Singularity,\n            has_decoder=False,\n            pretrain=True,\n            find_unused_parameters=True\n        )\n    if is_main_process() and config.wandb.enable:\n        wandb.watch(model)\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(start_epoch, config.scheduler.epochs):\n        global_step = train(\n            model, train_loaders, optimizer, tokenizer, epoch, global_step,\n            device, scheduler, scaler, config\n        )\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            eval_res = {}\n            for test_name, test_loader in test_name2loaders.items():\n                res = evaluation_wrapper(\n                    model_without_ddp, test_loader, tokenizer, device, config, prefix=test_name)\n                eval_res.update(res)\n\n        if is_main_process():\n            if config.wandb.enable:\n                for p, v in eval_res.items():\n                    log_dict_to_wandb(v, step=global_step, prefix=p)\n\n            eval_res = pd.DataFrame(eval_res)\n            logger.info(f\"Epoch {epoch}\")\n            logger.info(f\"\\n{eval_res.transpose()}\")\n\n            save_obj = {\n                \"model\": model_without_ddp.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"scaler\": scaler.state_dict(),\n                \"config\": config,\n                \"epoch\": epoch,\n                \"global_step\": global_step,\n            }\n            torch.save(save_obj, join(config.output_dir, f\"ckpt_{epoch:02d}.pth\"))\n            keep_last_n = 2  # only keep the last n checkpoints to save storage.\n            remove_files_if_exist(\n                [join(config.output_dir, f\"ckpt_{e:02d}.pth\")\n                 for e in range(0, epoch-keep_last_n)]\n            )\n            eval_file = \"eval_res_best.json\"\n            eval_res.to_json(join(config.output_dir, eval_file))\n\n        dist.barrier()\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info(f\"Training time {total_time_str}\")\n    logger.info(f\"Checkpoints and Logs saved at {config.output_dir}\")\n\n    if is_main_process() and config.wandb.enable:\n        run.finish()", "\n\nif __name__ == \"__main__\":\n    cfg = setup_main()\n    main(cfg)\n"]}
{"filename": "tasks/retrieval_utils.py", "chunked_list": ["import os\nimport time\nimport datetime\nimport logging\nimport numpy as np\n\nimport torch\nimport torch.distributed as dist\nfrom einops import rearrange\n", "from einops import rearrange\n\nfrom utils.basic_utils import MetricLogger\nfrom utils.distributed import get_rank, get_world_size\nfrom utils.eval import reorder_frames\nfrom utils.visualization import save_token_map\n\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\ndef extract_text_feats(texts, max_txt_l, tokenizer, model, device):\n    num_text = len(texts)\n    text_bs = 256\n    text_feats = []\n    text_atts = []\n\n    for i in range(0, num_text, text_bs):\n        text = texts[i: min(num_text, i+text_bs)]\n        text_input = tokenizer(\n            text, padding=\"max_length\",\n            truncation=True, max_length=max_txt_l,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        text_feat = model.encode_text(text_input)[0]\n        text_feats.append(text_feat)\n        text_atts.append(text_input.attention_mask)\n\n    text_feats = torch.cat(text_feats, dim=0)\n    text_atts = torch.cat(text_atts, dim=0)\n    return text_feats, text_atts", "\n\ndef extract_vision_feats(data_loader, model, device, config):\n    image_feats_all = []\n    pooled_image_feats_all = []\n    token_idx_all = []\n\n    # frame sampling\n    eval_sparse_sampling = getattr(config, 'eval_sparse_sampling', False)\n    eval_frame_order = getattr(config, 'eval_frame_order', 'none')\n\n    # compute token index\n    eval_save_token_map = getattr(config, 'eval_save_token_map', False)\n    eval_attn_dist = getattr(config, 'eval_attention_distance', False)\n    # output_token_idx = eval_save_token_map or eval_attn_dist\n    eval_save_cross_token_map = getattr(config, 'eval_save_cross_token_map', False)  # should return token idx\n    output_token_idx = eval_save_token_map or eval_attn_dist or eval_save_cross_token_map\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"extracting image feats\"\n    iterator = metric_logger.log_every(data_loader, 100, header)\n    for image, img_id in iterator:\n        image = image.to(device, non_blocking=True)\n\n        if config.eval_frame_ensemble == \"concat\":  # default\n            image_feat, pooled_image_feat = model.encode_image(image)   # (bsz, #frm*L, d), (bsz, #frm, d)\n            image_feat = image_feat.unsqueeze(1)  # (bsz, 1, #frm*L, d)\n        else:\n            bsz = image.shape[0]\n            assert image.shape[1] % config.video_input.num_frames == 0, \"video_input.num_frames_test must be multiples of video_input.num_frames\"\n            assert config.eval_frame_ensemble in [\"mean\", \"max\", \"lse\"]\n\n            if eval_sparse_sampling:\n                image = rearrange(image, 'b (n k) c h w -> (b k) n c h w', n=config.video_input.num_frames)\n            else:\n                image = rearrange(image, 'b (k n) c h w -> (b k) n c h w', n=config.video_input.num_frames)\n\n            image = reorder_frames(image, eval_frame_order)\n\n            # if eval_save_token_map:\n            #     image_feat, pooled_image_feat, token_idx = model.encode_image(image, output_token_idx=True)   # (bsz, #frm, L, d), (bsz, #frm, d)\n            # elif compute_attn_dist:\n            #     image_feat, pooled_image_feat, token_idx = model.encode_image(image, output_attentions=True)   # (bsz, #frm, L, d), (bsz, #frm, d)\n            # else:\n            #     image_feat, pooled_image_feat = model.encode_image(image)   # (bsz, #frm, L, d), (bsz, #frm, d)\n            \n            image_feat, pooled_image_feat, *outputs = model.encode_image(\n                image, \n                output_token_idx=output_token_idx, \n                output_attentions=eval_attn_dist\n            )\n            if output_token_idx:\n                token_idx = outputs[0]\n            if eval_attn_dist:\n                attentions = outputs[-1]\n                # breakpoint()\n\n            image_feat = rearrange(image_feat, '(b k) n d -> b k n d', b=bsz)\n            if pooled_image_feat.ndim == 3:\n                pooled_image_feat = rearrange(pooled_image_feat, '(b k) n d -> b (k n) d', b=bsz)\n            else:\n                pooled_image_feat = rearrange(pooled_image_feat, '(b k) d -> b k d', b=bsz)\n            \n            if eval_save_token_map:\n                save_token_map(img_id, image, token_idx, \n                               thw_shape=model.vision_encoder.window_size, \n                               save_path=os.path.join(config.output_dir, 'token_map'))\n                save_token_map(img_id, image, token_idx, \n                               thw_shape=model.vision_encoder.window_size, layer_id=(),\n                               save_path=os.path.join(config.output_dir, 'raw_video'))\n\n        if config.eval_offload:\n            image_feats_all.append(image_feat.cpu())\n            pooled_image_feats_all.append(pooled_image_feat.cpu())\n            if eval_save_cross_token_map and len(token_idx) > 0:\n                token_idx_all.append(token_idx[-1].cpu())\n        else:\n            image_feats_all.append(image_feat)\n            pooled_image_feats_all.append(pooled_image_feat)\n            if eval_save_cross_token_map and len(token_idx) > 0:\n                token_idx_all.append(token_idx[-1])\n    image_feats_all = torch.cat(image_feats_all, dim=0)\n    pooled_image_feats_all = torch.cat(pooled_image_feats_all, dim=0)\n\n    if eval_save_cross_token_map:\n        if len(token_idx_all) > 0:\n            token_idx_all = torch.cat(token_idx_all, dim=0)\n        else:\n            token_idx_all = None\n        return image_feats_all, pooled_image_feats_all, token_idx_all\n    return image_feats_all, pooled_image_feats_all", "\n\n@torch.no_grad()\ndef evaluation_wrapper(model, data_loader, tokenizer, device, config, prefix=\"\"):\n    with torch.cuda.amp.autocast(enabled=config.fp16):\n        eval_func = cross_encoder_evaluation if config.eval_x_only else evaluation\n        i2t_x, t2i_x, i2t_emb, t2i_emb = eval_func(model, data_loader, tokenizer, device, config)\n    score_pairs = [\n        (prefix + \"/\", i2t_x, t2i_x),\n        (prefix + \"_emb/\", i2t_emb, t2i_emb),\n    ]\n    res = dict()\n    for name, i2t, t2i in score_pairs:\n        if i2t is not None:\n            txt2img_ids = data_loader.dataset.txt2img\n            img2txt_ids = data_loader.dataset.img2txt\n            res[name] = itm_eval(i2t, t2i, txt2img_ids, img2txt_ids)\n    return res", "\n\n@torch.no_grad()\ndef cross_encoder_evaluation(model, data_loader, tokenizer, device, config):\n    model.eval()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"Evaluation:\"\n    dtype = torch.half if config.fp16 else torch.float\n    media_type = data_loader.dataset.media_type\n    logger.info(f\"Start evaluation for media_type={media_type}\")\n\n    logger.info(\"Computing dual encoder features...\")\n    start_time = time.time()\n\n    # the stats of the raw eval set, useful when having distrators\n    raw_n_image = data_loader.dataset.raw_n_image\n    raw_n_text = data_loader.dataset.raw_n_text\n\n    # this computes all features in each GPU\n    texts = data_loader.dataset.text\n    max_txt_l = config.max_txt_l\n    if not isinstance(max_txt_l, int):\n        max_txt_l = max_txt_l[media_type]\n    text_feats, text_atts = extract_text_feats(\n        texts, max_txt_l, tokenizer, model, device)\n    image_feats, pooled_image_feats = extract_vision_feats(data_loader, model, device, config)\n    logger.info(\"Finished feature extraction\")\n    logger.info(\"Computing ITC scores [dot-product]\")\n    _pooled_image_feats = pooled_image_feats.to(device, non_blocking=True) \\\n        if config.eval_offload else pooled_image_feats\n    i2t_scores, t2i_scores = model.get_sim(_pooled_image_feats, text_feats[:, 0])\n    logger.info(\"Computing ITC scores [dot-product], done!\")\n\n    i2t_scores_x = torch.full(\n        (raw_n_image, len(texts)), -100.0).to(device, non_blocking=True).to(dtype)\n\n    # computes only part of the scores at each GPU, gather at the end\n    logger.info(\"Rerank dual-encoder results with cross-encoder...\")\n    num_tasks = get_world_size()\n    rank = get_rank()\n    # only uses the part associated with the raw eval set\n    # compute image2text #\n    step = raw_n_image // num_tasks + 1\n    start = rank * step\n    end = min(raw_n_image, start+step)\n\n    text_encoder = model.get_text_encoder()\n    iterator = metric_logger.log_every(i2t_scores[start:end], 50, header)\n    logger.info(f\"i2t_scores.shape {i2t_scores[start:end].shape}\")\n    inner_bsz = 1024\n    for i, _ in enumerate(iterator):\n        for inner_st in range(0, len(texts), inner_bsz):\n            inner_ed = min(inner_st+inner_bsz, len(texts))\n            cur_bsz = inner_ed - inner_st\n            encoder_output = image_feats[start+i].to(device, non_blocking=True) \\\n                if config.eval_offload else image_feats[start+i]\n            encoder_output = encoder_output.repeat(cur_bsz, 1, 1)\n            encoder_att = torch.ones(encoder_output.size()[\n                                    :-1], dtype=torch.long).to(device, non_blocking=True)\n            output = text_encoder(\n                encoder_embeds=text_feats[inner_st:inner_ed],\n                attention_mask=text_atts[inner_st:inner_ed],\n                encoder_hidden_states=encoder_output,\n                encoder_attention_mask=encoder_att,\n                return_dict=True,\n                mode=\"fusion\"\n            )\n\n            itm_embeds = output.last_hidden_state[:, 0]  # [CLS]\n\n            score = model.itm_head(itm_embeds)[:, 1]\n            i2t_scores_x[start+i, inner_st:inner_ed] = score\n    \n    if config.distributed:\n        # gether across GPUs\n        dist.barrier()\n        dist.all_reduce(i2t_scores_x, op=dist.ReduceOp.SUM)\n\n    # compute text2image\n    t2i_scores_x = i2t_scores_x.T\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info(f\"Evaluation time {total_time_str}\")\n\n    return i2t_scores_x.cpu().numpy(), t2i_scores_x.cpu().numpy(), \\\n        i2t_scores.cpu().numpy(), t2i_scores.cpu().numpy()", "\n\n@torch.no_grad()\ndef evaluation(model, data_loader, tokenizer, device, config):\n    model.eval()\n    \n    eval_save_cross_token_map = getattr(config, 'eval_save_cross_token_map', False)\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"Evaluation:\"\n    dtype = torch.half if config.fp16 else torch.float\n    media_type = data_loader.dataset.media_type\n    logger.info(f\"Start evaluation for media_type={media_type}\")\n\n    logger.info(\"Computing dual encoder features...\")\n    start_time = time.time()\n\n    # this computes all features in each GPU\n    texts = data_loader.dataset.text\n    max_txt_l = config.max_txt_l\n    if not isinstance(max_txt_l, int):\n        max_txt_l = max_txt_l[media_type]\n    text_feats, text_atts = extract_text_feats(\n        texts, max_txt_l, tokenizer, model, device)  # (bsz, Lt, d), (bsz, Lt)\n    \n    if eval_save_cross_token_map:\n        image_feats, pooled_image_feats, vis_token_idx = extract_vision_feats(\n            data_loader, model, device, config)  # (bsz, 1, #frm*Li, d) or (bsz, #frm, Li, d), (bsz, #frm, d)\n    else:\n        image_feats, pooled_image_feats = extract_vision_feats(\n            data_loader, model, device, config)  # (bsz, 1, #frm*Li, d) or (bsz, #frm, Li, d), (bsz, #frm, d)\n\n    logger.info(\"Finished feature extraction\")\n    logger.info(\"Computing ITC scores [dot-product]\")\n    _pooled_image_feats = pooled_image_feats.to(device, non_blocking=True) \\\n        if config.eval_offload else pooled_image_feats\n    i2t_scores, t2i_scores = model.get_sim(_pooled_image_feats, text_feats[:, 0])\n    logger.info(\"Computing ITC scores [dot-product], done!\")\n\n    num_images = len(data_loader.dataset.image)\n    i2t_scores_x = torch.full(\n        (num_images, len(texts)), -100.0).to(device, dtype, non_blocking=True)\n\n    # computes only part of the scores at each GPU, gather at the end\n    logger.info(\"Rerank dual-encoder results with cross-encoder...\")\n    num_tasks = get_world_size()\n    rank = get_rank()\n    # only uses the part associated with the raw eval set\n    # compute image2text #\n    step = num_images // num_tasks + 1\n    start = rank * step\n    end = min(num_images, start+step)\n\n    text_encoder = model.get_text_encoder()\n    iterator = metric_logger.log_every(i2t_scores[start:end], 100, header)\n    logger.info(f\"i2t_scores.shape {i2t_scores[start:end].shape}\")\n    n_clip_per_video = image_feats.shape[1]  # generate score for each clip, and aggregate all clip scores for a video\n    logger.info(f\"n_clip_per_video={n_clip_per_video}, with eval_frame_ensemble={config.eval_frame_ensemble}\")\n    for i, sims in enumerate(iterator):\n        k = min(len(sims), config.k_test)\n        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n\n        clip_scores = []\n        for clip_idx in range(n_clip_per_video):\n            encoder_output = image_feats[start+i, clip_idx].to(device, non_blocking=True) \\\n                if config.eval_offload else image_feats[start+i, clip_idx]  # (#frm*Li, d)\n            encoder_output = encoder_output.repeat(k, 1, 1)   # (k=128, #frm*Li, d)\n            encoder_att = torch.ones(\n                encoder_output.size()[:-1], dtype=torch.long\n            ).to(device, non_blocking=True)\n            output = text_encoder(\n                encoder_embeds=text_feats[topk_idx],\n                attention_mask=text_atts[topk_idx],\n                encoder_hidden_states=encoder_output,\n                encoder_attention_mask=encoder_att,\n                output_token_idx=eval_save_cross_token_map,\n                return_dict=True,\n                mode=\"fusion\"\n            )\n            \n            if eval_save_cross_token_map:\n                if (start + i) in topk_idx:\n                    loc = topk_idx.tolist().index(start + i)\n                    # token_idx = [x[loc:loc+1] for x in output.token_idx]\n                    if vis_token_idx is not None:\n                        token_idx = [vis_token_idx[start + i][x[loc:loc+1]] for x in output.token_idx]\n                    else:\n                        token_idx = [x[loc:loc+1] for x in output.token_idx]\n                    image, img_id = data_loader.dataset[start + i]\n                    save_token_map([img_id], image.unsqueeze(0), token_idx, \n                        thw_shape=model.vision_encoder.window_size, \n                        save_path=os.path.join(config.output_dir, 'cross_token_map'),\n                        bg_val=0.2, layer_id=(0,), texts=[texts[start + i]])\n\n            itm_embeds = output.last_hidden_state[:, 0]\n\n            score = model.itm_head(itm_embeds)[:, 1]\n            clip_scores.append(score)\n\n        if len(clip_scores) == 1:\n            score = clip_scores[0]\n        else:\n            assert config.eval_frame_ensemble in [\"mean\", \"max\", \"lse\"]\n            clip_scores = torch.stack(clip_scores)  # (#clips, k)\n            if config.eval_frame_ensemble == \"mean\":\n                score = clip_scores.mean(0)\n            elif config.eval_frame_ensemble == \"max\":\n                score = clip_scores.max(0)[0]\n            elif config.eval_frame_ensemble == \"lse\":  # LogSumExp\n                score = torch.logsumexp(clip_scores, dim=0)\n            else:\n                raise ValueError(\"config.eval_frame_ensemble must in [mean, max, lse] when #clip > 1.\")\n\n        i2t_scores_x[start+i, topk_idx] = score\n    \n    # compute text2image #\n    num_text = len(data_loader.dataset.text)\n    t2i_scores_x = torch.full(\n        (num_text, len(data_loader.dataset.image)), -100.0).to(device, dtype, non_blocking=True)\n\n    step = num_text // num_tasks + 1\n    start = rank*step\n    end = min(num_text, start+step)\n\n    iterator = metric_logger.log_every(t2i_scores[start:end], 100, header)\n    logger.info(f\"t2i_scores.shape {t2i_scores[start:end].shape}\")\n    n_clip_per_video = image_feats.shape[1]  # generate score for each clip, and aggregate all clip scores for a video\n    for i, sims in enumerate(iterator):\n        k = min(len(sims), config.k_test)\n        topk_sim, topk_idx = sims.topk(k=k, dim=0)\n\n        clip_scores = []\n        for clip_idx in range(n_clip_per_video):\n            encoder_output = image_feats[topk_idx, clip_idx].to(device, non_blocking=True) \\\n                if config.eval_offload else image_feats[topk_idx, clip_idx]\n            encoder_att = torch.ones(\n                encoder_output.size()[:-1], dtype=torch.long\n            ).to(device, non_blocking=True)\n\n            output = text_encoder(\n                encoder_embeds=text_feats[start+i].repeat(k, 1, 1),\n                attention_mask=text_atts[start+i].repeat(k, 1),\n                encoder_hidden_states=encoder_output,\n                encoder_attention_mask=encoder_att,\n                return_dict=True,\n                mode=\"fusion\"\n            )\n\n            itm_embeds = output.last_hidden_state[:, 0]\n\n            score = model.itm_head(itm_embeds)[:, 1]\n            clip_scores.append(score)\n\n        if len(clip_scores) == 1:\n            score = clip_scores[0]\n        else:\n            assert config.eval_frame_ensemble in [\"mean\", \"max\", \"lse\"]\n            clip_scores = torch.stack(clip_scores)  # (#clips, k)\n            if config.eval_frame_ensemble == \"mean\":\n                score = clip_scores.mean(0)\n            elif config.eval_frame_ensemble == \"max\":\n                score = clip_scores.max(0)[0]\n            elif config.eval_frame_ensemble == \"lse\":  # LogSumExp\n                score = torch.logsumexp(clip_scores, dim=0)\n            else:\n                raise ValueError(\"config.eval_frame_ensemble must in [mean, max, lse] when #clip > 1.\")\n\n        t2i_scores_x[start+i, topk_idx] = score\n\n    if config.distributed:\n        # gether across GPUs\n        dist.barrier()\n        dist.all_reduce(i2t_scores_x, op=dist.ReduceOp.SUM)\n        dist.all_reduce(t2i_scores_x, op=dist.ReduceOp.SUM)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info(f\"Evaluation time {total_time_str}\")\n\n    return i2t_scores_x.cpu().numpy(), t2i_scores_x.cpu().numpy(), \\\n        i2t_scores.cpu().numpy(), i2t_scores.T.cpu().numpy()", "\n\n@torch.no_grad()\ndef itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n    # Images->Text\n    ranks = np.zeros(scores_i2t.shape[0])\n    for index, score in enumerate(scores_i2t):\n        inds = np.argsort(score)[::-1]\n        # Score\n        gt_txt_ids = img2txt[index]\n        if isinstance(gt_txt_ids, int):\n            ranks[index] = np.where(inds == gt_txt_ids)[0][0]\n        else:\n            rank = 1e20\n            for i in gt_txt_ids:\n                tmp = np.where(inds == i)[0][0]\n                if tmp < rank:\n                    rank = tmp\n            ranks[index] = rank\n\n    # Compute metrics\n    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n\n    # Text->Images\n    ranks = np.zeros(scores_t2i.shape[0])\n\n    for index, score in enumerate(scores_t2i):\n        inds = np.argsort(score)[::-1]\n        gt_img_ids = txt2img[index]\n        if isinstance(gt_img_ids, int):\n            ranks[index] = np.where(inds == gt_img_ids)[0][0]\n        else:  # list, used in the case each caption has multiple GT images\n            # Score\n            rank = 1e20\n            for i in gt_img_ids:\n                tmp = np.where(inds == i)[0][0]\n                if tmp < rank:\n                    rank = tmp\n            ranks[index] = rank\n\n    # Compute metrics\n    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n\n    tr_mean = (tr1 + tr5 + tr10) / 3\n    ir_mean = (ir1 + ir5 + ir10) / 3\n    r_mean = (tr_mean + ir_mean) / 2\n\n    eval_result = {\"txt_r1\": tr1,\n                   \"txt_r5\": tr5,\n                   \"txt_r10\": tr10,\n                   \"txt_r_mean\": tr_mean,\n                   \"img_r1\": ir1,\n                   \"img_r5\": ir5,\n                   \"img_r10\": ir10,\n                   \"img_r_mean\": ir_mean,\n                   \"r_mean\": r_mean}\n    eval_result = {k: round(v, 2) for k, v in eval_result.items()}\n    return eval_result", ""]}
{"filename": "tasks/vqa.py", "chunked_list": ["import time\nimport datetime\nimport logging\nimport wandb\nimport os\nfrom os.path import join\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist", "import torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom models.model_vqa import Singularity\n\nfrom utils.logger import log_dict_to_wandb, setup_wandb\nfrom utils.config_utils import setup_main\nfrom utils.basic_utils import MetricLogger, SmoothedValue, setup_seed, save_json\nfrom utils.distributed import get_rank, get_world_size, is_main_process\nfrom dataset.utils import sync_save_result", "from utils.distributed import get_rank, get_world_size, is_main_process\nfrom dataset.utils import sync_save_result\nfrom dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\nfrom tasks.vqa_utils import eval_qa_acc\nfrom tasks.shared_utils import setup_model\nfrom shutil import copyfile\nfrom omegaconf import OmegaConf\nimport copy\n\nfrom utils.eval import reorder_frames", "\nfrom utils.eval import reorder_frames\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef train(model, train_loader, optimizer, tokenizer, epoch, global_step,\n          device, scheduler, scaler, config):\n    model.train()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", SmoothedValue(window=1, fmt=\"{value:.6f}\"))\n    metric_logger.add_meter(\"loss\", SmoothedValue(window=1, fmt=\"{value:.4f}\"))\n\n    header = \"Train Epoch: [{}]\".format(epoch)\n    log_freq = config.log_freq\n\n    if config.distributed:\n        train_loader.sampler.set_epoch(epoch)\n\n    iterator = metric_logger.log_every(train_loader, log_freq, header)\n    for i, data in enumerate(iterator):\n        image, question, answer, weights, n = data\n        image = image.to(device, non_blocking=True)\n        weights = weights.to(device, non_blocking=True)\n        question_input = tokenizer(\n            question, padding=\"max_length\", truncation=True,\n            max_length=config.max_q_len, return_tensors=\"pt\"\n        ).to(device)\n        answer_input = tokenizer(\n            answer, padding=\"max_length\", truncation=True,\n            max_length=config.max_a_len, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            loss = model(\n                image, question_input, answer_input,\n                train=True, k=n, weights=weights\n            )\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        if config.optimizer.max_grad_norm > 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), config.optimizer.max_grad_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        # logging\n        metric_logger.update(loss=loss.item())\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n        if is_main_process() and config.wandb.enable \\\n                and global_step % log_freq == 0:\n            logs = metric_logger.get_global_avg_dict()\n            log_dict_to_wandb(logs, step=global_step, prefix=\"train/\")\n\n        global_step += 1\n\n        if config.debug and (i+1) % 5 == 0:\n            break\n\n    metric_logger.synchronize_between_processes()\n    logger.info(f\"Averaged train stats: {metric_logger.global_avg()}\")\n    return global_step", "\n\n@torch.no_grad()\ndef evaluation(model, data_loader, tokenizer, device, config):\n    model.eval()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"[evaluation] Generating answers:\"\n    log_freq = config.log_freq // 2\n\n    result = []\n\n    raw_answer_list = data_loader.dataset.answer_list\n    answer_list = [a + \" \" + config.eos for a in raw_answer_list]\n    answer_input = tokenizer(\n        answer_list, padding=\"max_length\", truncation=True,\n        max_length=config.max_a_len, return_tensors=\"pt\"\n    ).to(device)\n\n    logger.info(\"Start generating results.\")\n\n    iterator = metric_logger.log_every(data_loader, log_freq, header)\n    for n, data in enumerate(iterator):\n        image, question, question_id = data\n        image = image.to(device, non_blocking=True)\n\n        eval_frame_order = getattr(config, 'eval_frame_order', 'none')\n        image = reorder_frames(image, eval_frame_order)\n\n        question_input = tokenizer(\n            question, padding=\"max_length\", truncation=True,\n            max_length=config.max_q_len, return_tensors=\"pt\"\n        ).to(device)\n\n        topk_ids, topk_probs = model(\n            image, question_input, answer_input, train=False, k=config.k_test)\n\n        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n            ques_id = int(ques_id.item()) if not isinstance(ques_id, str) else ques_id\n            _, pred = topk_prob.max(dim=0)\n            result.append({\n                \"question_id\": ques_id, \n                \"answer\": raw_answer_list[topk_id[pred]]\n            })\n\n    return result", "\n\ndef setup_dataloaders(config, dataset_name=\"vqa\"):\n    logger.info(f\"Creating {dataset_name} QA datasets\")\n    train_dataset = create_dataset(\"qa_train\", config)\n    test_datasets, test_dataset_names = create_dataset(\"qa_eval\", config)\n    all_datasets = [train_dataset] + test_datasets\n\n    if config.distributed:\n        num_tasks = get_world_size()\n        global_rank = get_rank()\n        samplers = create_sampler(\n            all_datasets,\n            shuffles=[True] + [False] * len(test_datasets),\n            num_tasks=num_tasks,\n            global_rank=global_rank\n        )\n    else:\n        samplers = [None] * len(all_datasets)\n\n    train_bsz = config.batch_size[train_dataset.media_type]\n    test_bsz_list = [config.batch_size_test[d.media_type] for d in test_datasets]\n    loaders = create_loader(\n        all_datasets, samplers,\n        batch_size=[train_bsz] + test_bsz_list,\n        num_workers=[config.num_workers, ] * len(all_datasets),\n        is_trains=[True] + [False] * len(test_datasets),\n        collate_fns=[vqa_collate_fn] + [None] * len(test_datasets)\n    )\n    train_loader, test_loaders = loaders[0], loaders[1:]\n    test_name2loaders = {k: v for k, v in zip(test_dataset_names, test_loaders)}\n    return train_loader, test_name2loaders", "\n\ndef main(config):\n    if is_main_process() and config.wandb.enable:\n        run = setup_wandb(config)\n\n    logger.info(f\"config: \\n{config}\")\n    logger.info(f\"train_file: {config.train_file}\")\n\n    setup_seed(config.seed + get_rank())\n    device = torch.device(config.device)\n    cudnn.benchmark = True\n\n    train_loader, test_name2loaders = setup_dataloaders(config)\n    config.scheduler.num_training_steps = len(train_loader) * config.scheduler.epochs\n    config.scheduler.num_warmup_steps = len(train_loader) * config.scheduler.warmup_epochs\n\n    model, model_without_ddp, optimizer, scheduler, scaler, \\\n        tokenizer, start_epoch, global_step = setup_model(\n            config,\n            model_cls=Singularity,\n            has_decoder=True,\n            pretrain=False,\n            find_unused_parameters=True\n        )\n    if is_main_process() and config.wandb.enable:\n        wandb.watch(model)\n\n    best = 0\n    best_epoch = 0\n    has_gt = config.dataset_name != \"vqa\" or config.test_types[0] == \"minival\"\n\n    logger.info(\"Start \" + \"evaluation\" if config.evaluate else \"training\")\n    start_time = time.time()\n    for epoch in range(start_epoch, config.scheduler.epochs):\n        if not config.evaluate:\n            global_step = train(\n                model, train_loader, optimizer, tokenizer, epoch, global_step,\n                device, scheduler, scaler, config\n            )\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            eval_res = {}\n            pred_name2file = {}\n            for test_name, test_loader in test_name2loaders.items():\n                if test_name not in config.test_types:\n                    logger.info(f\"Skip eval {test_name} split. All test_types {config.test_types}\")\n                    continue\n                logger.info(f\"Evaluating {test_name} split...\")\n                qa_result = evaluation(model, test_loader, tokenizer, device, config)\n\n                # sync/gather results and eval\n                pred_file, gathered_result = sync_save_result(\n                    qa_result, config.result_dir, f\"{test_name}_latest\")\n                pred_name2file[test_name] = pred_file\n                if is_main_process() and has_gt:\n                    eval_res[test_name] = eval_qa_acc(\n                        test_loader.dataset.anno_list, gathered_result,\n                        is_vqa=config.dataset_name == \"vqa\"\n                    )\n\n        if is_main_process():\n            if len(eval_res) > 0:\n                logger.info(f\"eval_res {eval_res}\")\n\n                if config.wandb.enable:\n                    for name, acc_dict in eval_res.items():\n                        log_dict_to_wandb(acc_dict, step=global_step, prefix=f\"{name}/\")\n\n            if not config.evaluate and has_gt and eval_res[config.stop_key][\"overall\"] > best:\n                save_obj = {\n                    \"model\": model_without_ddp.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                    \"scheduler\": scheduler.state_dict(),\n                    \"scaler\": scaler.state_dict(),\n                    \"config\": config,\n                    \"epoch\": epoch,\n                    \"global_step\": global_step,\n                }\n                for name, pred_file in pred_name2file.items():\n                    copyfile(pred_file, pred_file.replace(\"latest\", \"best\"))\n                save_json(eval_res, join(config.output_dir, \"eval_res_best.json\"), save_pretty=True)\n                torch.save(save_obj, join(config.output_dir, \"ckpt_best.pth\"))\n                best = eval_res[config.stop_key][\"overall\"]\n                best_epoch = epoch\n            if config.evaluate:\n                save_json(eval_res, join(config.output_dir, \"eval_res_best.json\"), save_pretty=True)\n\n                for name, pred_file in pred_name2file.items():\n                    copyfile(pred_file, pred_file.replace(\"latest\", \"eval\"))\n                    if has_gt:\n                        save_path = join(config.output_dir, f\"{name}_acc_eval.json\")\n                        save_json(eval_res[name], save_path, save_pretty=True)\n\n        if config.evaluate or config.debug:\n            break\n\n        dist.barrier()\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info(f\"Training time {total_time_str}\")\n    logger.info(f\"best epoch {best_epoch}\")\n    logger.info(f\"Checkpoints and Logs saved at {config.output_dir}\")\n\n    if is_main_process() and config.wandb.enable:\n        run.finish()", "\n\ndef eval_after_training(train_config):\n    # general config for all\n    train_config.wandb.enable = False\n    train_config.evaluate = True\n    train_config.pretrained_path = join(train_config.output_dir, \"ckpt_best.pth\")\n\n    eval_config = copy.deepcopy(train_config)\n    eval_config.test_types = [\"val\", \"test\"] if eval_config.dataset_name != \"vqa\" else [\"minival\"]\n    eval_config.output_dir = join(\n        eval_config.output_dir, f\"eval_after_training\")\n    eval_config.result_dir = eval_config.output_dir\n    if is_main_process():\n        os.makedirs(eval_config.output_dir, exist_ok=False)\n        OmegaConf.save(eval_config, open(join(eval_config.output_dir, 'config.yaml'), 'w'))\n    logger.info(f\"===========> START eval_after_training [{eval_config.test_types}]\")\n    main(eval_config)", "\n\nif __name__ == \"__main__\":\n    cfg = setup_main()\n    cfg.result_dir = cfg.output_dir\n    main(cfg)\n    if not cfg.evaluate:\n        eval_after_training(cfg)\n", ""]}
{"filename": "tasks/vqa_utils.py", "chunked_list": ["# coding=utf-8\n\n__author__ = 'aagrawal'\n\n# This code is based on the code written by Tsung-Yi Lin for MSCOCO Python API available at the following link: \n# (https://github.com/tylin/coco-caption/blob/master/pycocoevalcap/eval.py).\nimport sys\nimport re\nimport json\nimport logging", "import json\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass VQAEval:\n    \"\"\"Modified from https://github.com/GT-Vision-Lab/VQA/blob/master/PythonEvaluationTools/vqaEvaluation/vqaEval.py\n\n    gt_data_or_path:\n        - List(dict): each dict is {\n            question_id: int,\n            answer: List(str),\n            question_type: str,\n            answer_type: str,\n            ...\n            }\n        - str: path to file, use json.load() to load it as List(dict)\n    pred_data_or_path:\n        - List(dict): each dict is {\n            question_id: int,\n            answer: str\n        }\n        - str: path to file, use json.load() to load it as List(dict)\n    n: #valid float point\n    \"\"\"\n\n    def __init__(self, gt_data_or_path, n=2):\n        self.n = n\n        self.accuracy = {}\n        self.evalQA = {}\n        self.evalQuesType = {}\n        self.evalAnsType = {}\n        self.contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\",\n                             \"couldnt\": \"couldn't\",\n                             \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\",\n                             \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\",\n                             \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\",\n                             \"hed\": \"he'd\", \"hed've\": \"he'd've\",\n                             \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\",\n                             \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\",\n                             \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\",\n                             \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n                             \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\",\n                             \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n                             \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\",\n                             \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n                             \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\",\n                             \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\",\n                             \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\",\n                             \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\",\n                             \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\",\n                             \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\",\n                             \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\",\n                             \"someone'dve\": \"someone'd've\",\n                             \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\",\n                             \"somethingd've\": \"something'd've\",\n                             \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\",\n                             \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n                             \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\",\n                             \"theyd've\": \"they'd've\",\n                             \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\",\n                             \"twas\": \"'twas\", \"wasnt\": \"wasn't\",\n                             \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\",\n                             \"whatll\": \"what'll\", \"whatre\": \"what're\",\n                             \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\",\n                             \"wheres\": \"where's\", \"whereve\": \"where've\",\n                             \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\",\n                             \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n                             \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\",\n                             \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n                             \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\",\n                             \"yall'd've\": \"y'all'd've\",\n                             \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\",\n                             \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\",\n                             \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n        self.manualMap = {'none': '0',\n                          'zero': '0',\n                          'one': '1',\n                          'two': '2',\n                          'three': '3',\n                          'four': '4',\n                          'five': '5',\n                          'six': '6',\n                          'seven': '7',\n                          'eight': '8',\n                          'nine': '9',\n                          'ten': '10'\n                          }\n        self.articles = ['a',\n                         'an',\n                         'the'\n                         ]\n\n        self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n        self.commaStrip = re.compile(\"(\\d)(,)(\\d)\")\n        self.punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n                      '(', ')', '=', '+', '\\\\', '_', '-',\n                      '>', '<', '@', '`', ',', '?', '!']\n\n        self.gt_qid2data, self.gt_qid2processed_ans \\\n            = self.prepare_gt_data(gt_data_or_path)\n\n    def load_data_or_path(self, data_or_path):\n        if isinstance(data_or_path, str):\n            with open(data_or_path, \"r\") as f:\n                return json.load(f)\n        else:\n            return data_or_path\n\n    def prepare_gt_data(self, gt_data_or_path):\n        gt_data = self.load_data_or_path(gt_data_or_path)\n        gt_qid2processed_ans = {}\n        for d in gt_data:\n            gtAnswers = [ans for ans in d['answer']]\n            if len(set(gtAnswers)) > 1:\n                gtAnswers = [self.processPunctuation(e) for e in gtAnswers]\n            gt_qid2processed_ans[d[\"question_id\"]] = gtAnswers\n        gt_qid2data = {e[\"question_id\"]: e for e in gt_data}\n        return gt_qid2data, gt_qid2processed_ans\n\n    def init_entry(self):\n        print(\"Cleaned entries for computing new inputs\")\n        self.accuracy = {}\n        self.evalQA = {}\n        self.evalQuesType = {}\n        self.evalAnsType = {}\n\n    def evaluate(self, pred_data_or_path):\n        self.init_entry()\n        pred_data = self.load_data_or_path(pred_data_or_path)\n        res = {e[\"question_id\"]: e for e in pred_data}\n        gts = self.gt_qid2data\n        quesIds = list(gts.keys())\n        assert len(res) == len(gts), \\\n            f\"expect the same length, but got len(res) {len(res)} == len(gts) {len(gts)}\"\n\n        # =================================================\n        # Compute accuracy\n        # =================================================\n        accQA = []\n        accQuesType = {}\n        accAnsType = {}\n        print(\"computing accuracy\")\n        step = 0\n        for quesId in quesIds:\n            resAns = res[quesId]['answer']\n            resAns = resAns.replace('\\n', ' ')\n            resAns = resAns.replace('\\t', ' ')\n            resAns = resAns.strip()\n            resAns = self.processPunctuation(resAns)\n            resAns = self.processDigitArticle(resAns)\n            gtAcc = []\n            gtAnswers = self.gt_qid2processed_ans[quesId]\n            # In order to be consistent with \u2018human accuracies\u2019, machine accuracies are averaged over all 10 choose 9 sets of human annotators.\n            for idx in range(len(gtAnswers)):\n                otherGTAns = gtAnswers[:idx] + gtAnswers[idx + 1:]\n                matchingAns = [e for e in otherGTAns if e == resAns]\n                acc = min(1, float(len(matchingAns)) / 3)\n                gtAcc.append(acc)\n            quesType = gts[quesId]['question_type']\n            ansType = gts[quesId]['answer_type']\n            avgGTAcc = float(sum(gtAcc)) / len(gtAcc)\n            accQA.append(avgGTAcc)\n            if quesType not in accQuesType:\n                accQuesType[quesType] = []\n            accQuesType[quesType].append(avgGTAcc)\n            if ansType not in accAnsType:\n                accAnsType[ansType] = []\n            accAnsType[ansType].append(avgGTAcc)\n            self.setEvalQA(quesId, avgGTAcc)\n            self.setEvalQuesType(quesId, quesType, avgGTAcc)\n            self.setEvalAnsType(quesId, ansType, avgGTAcc)\n            if step % 100 == 0:\n                self.updateProgress(step / float(len(quesIds)))\n            step = step + 1\n\n        self.setAccuracy(accQA, accQuesType, accAnsType)\n        print(\"Done computing accuracy\")\n\n    def processPunctuation(self, inText):\n        outText = inText\n        for p in self.punct:\n            if (p + ' ' in inText or ' ' + p in inText) or (re.search(self.commaStrip, inText) != None):\n                outText = outText.replace(p, '')\n            else:\n                outText = outText.replace(p, ' ')\n        outText = self.periodStrip.sub(\"\",\n                                       outText,\n                                       re.UNICODE)\n        return outText\n\n    def processDigitArticle(self, inText):\n        outText = []\n        tempText = inText.lower().split()\n        for word in tempText:\n            word = self.manualMap.setdefault(word, word)\n            if word not in self.articles:\n                outText.append(word)\n            else:\n                pass\n        for wordId, word in enumerate(outText):\n            if word in self.contractions:\n                outText[wordId] = self.contractions[word]\n        outText = ' '.join(outText)\n        return outText\n\n    def setAccuracy(self, accQA, accQuesType, accAnsType):\n        self.accuracy['overall'] = round(100 * float(sum(accQA)) / len(accQA), self.n)\n        self.accuracy['perQuestionType'] = {\n            quesType: round(100 * float(sum(accQuesType[quesType])) / len(accQuesType[quesType]), self.n) for quesType\n            in accQuesType}\n        self.accuracy['perAnswerType'] = {\n            ansType: round(100 * float(sum(accAnsType[ansType])) / len(accAnsType[ansType]), self.n) for ansType in\n            accAnsType}\n        # replace \"yes/no\" as \"yes-no\" to be compatible with wandb, since \"/\" is a reserved char\n        # self.accuracy['perAnswerType']['yes-no'] = self.accuracy['perAnswerType'].pop('yes/no')\n\n    def setEvalQA(self, quesId, acc):\n        self.evalQA[quesId] = round(100 * acc, self.n)\n\n    def setEvalQuesType(self, quesId, quesType, acc):\n        if quesType not in self.evalQuesType:\n            self.evalQuesType[quesType] = {}\n        self.evalQuesType[quesType][quesId] = round(100 * acc, self.n)\n\n    def setEvalAnsType(self, quesId, ansType, acc):\n        if ansType not in self.evalAnsType:\n            self.evalAnsType[ansType] = {}\n        self.evalAnsType[ansType][quesId] = round(100 * acc, self.n)\n\n    def updateProgress(self, progress):\n        barLength = 20\n        status = \"\"\n        if isinstance(progress, int):\n            progress = float(progress)\n        if not isinstance(progress, float):\n            progress = 0\n            status = \"error: progress var must be float\\r\\n\"\n        if progress < 0:\n            progress = 0\n            status = \"Halt...\\r\\n\"\n        if progress >= 1:\n            progress = 1\n            status = \"Done...\\r\\n\"\n        block = int(round(barLength * progress))\n        text = \"\\rFinshed Percent: [{0}] {1}% {2}\".format(\"#\" * block + \"-\" * (barLength - block), int(progress * 100),\n                                                          status)\n        sys.stdout.write(text)\n        sys.stdout.flush()", "\n\ndef eval_simple_qa_acc(gt_data_or_path, pred_data_or_path):\n    \"\"\"For open-ended QA that has a single answer,\n    accuracy is computed based on exact match\"\"\"\n    if isinstance(gt_data_or_path, str):\n        gt_data_or_path = json.load(open(gt_data_or_path, \"r\"))\n    if isinstance(pred_data_or_path, str):\n        pred_data_or_path = json.load(open(pred_data_or_path, \"r\"))\n    true_or_false_list = []\n    gt_qid2answer = {d[\"question_id\"]: d[\"answer\"] for d in gt_data_or_path}\n    pred_qid2answer = {d[\"question_id\"]: d[\"answer\"] for d in pred_data_or_path}\n    missing_qids = []\n    for qid, answer in gt_qid2answer.items():\n        if qid in pred_qid2answer:\n            true_or_false_list.append(int(pred_qid2answer[qid] == answer))\n        else:  # if not exists automatically False\n            true_or_false_list.append(0)\n            missing_qids.append(qid)\n    logger.info(f\"Missing predictions for {len(missing_qids)} questions, example[:3] {missing_qids[:3]}\")\n    acc = round(100. * sum(true_or_false_list) / len(true_or_false_list), 2)\n    return acc", "\n\ndef eval_qa_acc(gt_data_or_path, pred_data_or_path, is_vqa=False):\n    if is_vqa:\n        vqa_evaluator = VQAEval(gt_data_or_path=gt_data_or_path)\n        if isinstance(pred_data_or_path, str):\n            pred_data_or_path = json.load(open(pred_data_or_path, \"r\"))\n        vqa_evaluator.evaluate(pred_data_or_path)\n        vqa_acc = vqa_evaluator.accuracy[\"perAnswerType\"]  # dict\n        vqa_acc[\"overall\"] = vqa_evaluator.accuracy[\"overall\"]\n        return vqa_acc\n    else:\n        acc = eval_simple_qa_acc(gt_data_or_path, pred_data_or_path)\n        return dict(overall=acc)", ""]}
{"filename": "tasks/retrieval.py", "chunked_list": ["import pandas as pd\nimport time\nimport datetime\nimport logging\nimport wandb\nimport os\nfrom os.path import join\n\nimport torch\nimport torch.backends.cudnn as cudnn", "import torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom models.model_retrieval import Singularity\nfrom tasks.pretrain import setup_dataloaders\n\nfrom utils.logger import log_dict_to_wandb, setup_wandb\nfrom utils.config_utils import setup_main\nfrom utils.basic_utils import MetricLogger, SmoothedValue, setup_seed", "from utils.config_utils import setup_main\nfrom utils.basic_utils import MetricLogger, SmoothedValue, setup_seed\nfrom utils.distributed import get_rank, is_main_process\nfrom dataset import MetaLoader\nfrom tasks.retrieval_utils import evaluation_wrapper\nfrom tasks.shared_utils import setup_model\nfrom omegaconf import OmegaConf\nimport copy\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\ndef train(model, train_loaders, optimizer, tokenizer, epoch, global_step,\n          device, scheduler, scaler, config):\n    model.train()\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", SmoothedValue(window=1, fmt=\"{value:.6f}\"))\n    metric_logger.add_meter(\"temperature\", SmoothedValue(window=1, fmt=\"{value:.4f}\"))\n    loss_names = [\"loss_ita\", \"loss_itm\"]\n    media_types = [loader.dataset.media_type for loader in train_loaders]\n    for name in loss_names:\n        for m in media_types:\n            metric_logger.add_meter(f\"{m}-{name}\", SmoothedValue(window=1, fmt=\"{value:.4f}\"))\n\n    header = f\"Train Epoch: [{epoch}]\"\n    log_freq = config.log_freq\n\n    if config.distributed:\n        for d in train_loaders:\n            d.sampler.set_epoch(epoch)\n    train_loader = MetaLoader(name2loader=dict(list(zip(media_types, train_loaders))))\n\n    model_without_ddp = model.module if config.distributed else model\n    iterator = metric_logger.log_every(train_loader, log_freq, header)\n    for i, (media_type, (image, text, idx)) in enumerate(iterator):\n        image = image.to(device, non_blocking=True)\n        idx = idx.to(device, non_blocking=True)\n        text_input = tokenizer(\n            text, padding=\"max_length\", truncation=True,\n            max_length=config.max_txt_l, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            loss_dict = model(image, text_input, idx=idx)\n            loss = sum(loss_dict.values())\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        if config.optimizer.max_grad_norm > 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), config.optimizer.max_grad_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        # logging\n        for name in loss_names:\n            value = loss_dict[name]\n            value = value if isinstance(value, float) else value.item()\n            metric_logger.update(**{f\"{media_type}-{name}\": value})\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        metric_logger.update(temperature=model_without_ddp.temp.item())\n\n        if is_main_process() and config.wandb.enable \\\n                and global_step % log_freq == 0:\n            logs = metric_logger.get_global_avg_dict()\n            log_dict_to_wandb(logs, step=global_step, prefix=\"train/\")\n\n        global_step += 1\n\n        if config.debug and (i+1) % 5 == 0:\n            break\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    logger.info(f\"Averaged train stats: {metric_logger.global_avg()}\")\n    return global_step", "\n\ndef main(config):\n    if is_main_process() and config.wandb.enable:\n        run = setup_wandb(config)\n\n    logger.info(f\"config: \\n{config}\")\n    logger.info(f\"train_file: {config.train_file}\")\n\n    setup_seed(config.seed + get_rank())\n    device = torch.device(config.device)\n    cudnn.benchmark = True\n\n    train_loaders, test_name2loaders, train_media_types = setup_dataloaders(config, mode=\"ret\")\n    num_steps_per_epoch = sum(len(d) for d in train_loaders)\n    config.scheduler.num_training_steps = num_steps_per_epoch * config.scheduler.epochs\n    config.scheduler.num_warmup_steps = num_steps_per_epoch * config.scheduler.warmup_epochs\n\n    model, model_without_ddp, optimizer, scheduler, scaler, \\\n        tokenizer, start_epoch, global_step = setup_model(\n            config,\n            model_cls=Singularity,\n            has_decoder=False,\n            pretrain=False,\n            find_unused_parameters=True\n        )\n    if is_main_process() and config.wandb.enable:\n        wandb.watch(model)\n\n    best = 0\n    best_epoch = 0\n\n    logger.info(\"Start \" + \"evaluation\" if config.evaluate else \"training\")\n    start_time = time.time()\n    for epoch in range(start_epoch, config.scheduler.epochs):\n        if not config.evaluate:\n            global_step = train(\n                model, train_loaders, optimizer, tokenizer, epoch, global_step,\n                device, scheduler, scaler, config\n            )\n\n        with torch.cuda.amp.autocast(enabled=config.fp16):\n            eval_res = {}\n            for test_name, test_loader in test_name2loaders.items():\n                if test_name not in config.test_types:\n                    logger.info(f\"Skip eval {test_name} split. All test_types {config.test_types}\")\n                    continue\n                res = evaluation_wrapper(\n                    model_without_ddp, test_loader, tokenizer, device, config, prefix=test_name)\n                eval_res.update(res)\n\n        if is_main_process():\n            if config.wandb.enable:\n                for p, v in eval_res.items():\n                    log_dict_to_wandb(v, step=global_step, prefix=p)\n\n            if config.stop_key is not None and config.stop_key in eval_res:\n                cur_r_mean = eval_res[config.stop_key][\"r_mean\"]\n            else:  # None\n                cur_r_mean = best + 1  # save the last as the best\n            eval_res = pd.DataFrame(eval_res)\n            logger.info(f\"Epoch {epoch}\")\n            logger.info(f\"\\n{eval_res.transpose()}\")\n            \n            eval_res.to_json(join(config.output_dir, \"eval_res_latest.json\"))\n\n            if not config.evaluate and cur_r_mean > best:\n                save_obj = {\n                    \"model\": model_without_ddp.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                    \"scheduler\": scheduler.state_dict(),\n                    \"scaler\": scaler.state_dict(),\n                    \"config\": config,\n                    \"epoch\": epoch,\n                    \"global_step\": global_step,\n                }\n                eval_file = \"eval_res_best.json\"\n                eval_res.to_json(join(config.output_dir, eval_file))\n                torch.save(save_obj, join(config.output_dir, \"ckpt_best.pth\"))\n                best = cur_r_mean\n                best_epoch = epoch\n            if config.evaluate:\n                eval_file = \"eval_res.json\"\n                eval_res.to_json(join(config.output_dir, eval_file))\n\n        if config.evaluate or config.debug:\n            break\n\n        dist.barrier()\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info(f\"Training time {total_time_str}\")\n    logger.info(f\"best epoch {best_epoch} [config.stop_key {config.stop_key}]\")\n    logger.info(f\"Checkpoints and Logs saved at {config.output_dir}\")\n\n    if is_main_process() and config.wandb.enable:\n        run.finish()", "\n\ndef eval_after_training(train_config):\n    # general config for all\n    train_config.wandb.enable = False\n    train_config.evaluate = True\n    train_config.pretrained_path = join(train_config.output_dir, \"ckpt_best.pth\")\n\n    eval_config = copy.deepcopy(train_config)\n    eval_config.test_types = list(eval_config.test_file.keys())\n    eval_config.output_dir = join(\n        eval_config.output_dir, f\"eval_after_training\")\n    eval_config.result_dir = eval_config.output_dir\n    if is_main_process():\n        os.makedirs(eval_config.output_dir, exist_ok=False)\n        OmegaConf.save(eval_config, open(join(eval_config.output_dir, 'config.yaml'), 'w'))\n    logger.info(f\"===========> START eval_after_training [{eval_config.test_types}]\")\n    main(eval_config)", "\n\nif __name__ == \"__main__\":\n    cfg = setup_main()\n    main(cfg)\n    if not cfg.evaluate:\n        eval_after_training(cfg)\n"]}
