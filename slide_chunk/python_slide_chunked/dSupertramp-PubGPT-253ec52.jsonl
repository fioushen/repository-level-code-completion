{"filename": "pubgpt/vllm_test.py", "chunked_list": ["from vllm import LLM, SamplingParams\n\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)", "]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n\nllm = LLM(model=\"facebook/opt-125m\")\n\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")", "\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"]}
{"filename": "pubgpt/app.py", "chunked_list": ["import streamlit as st\n\nfrom online_parser.article_parser import (\n    parse_article,\n    extract_genes_and_diseases,\n    extract_mesh_terms,\n    extract_other_terms,\n)\nfrom pdf_parser.utils import read_pdf, extract_pdf_content, split_pdf_content\n", "from pdf_parser.utils import read_pdf, extract_pdf_content, split_pdf_content\n\n\n## PDF PARSER\n# from pdf_parser.openai import create_embeddings_openai, retriever_openai\n# from pdf_parser.cohere import create_embeddings_cohere, retriever_cohere\nfrom pdf_parser.starcoder import create_embeddings, retriever\n\n\n## LLM", "\n## LLM\n# from llm.openai import get_associations, summarize\n# from llm_parser.cohere import get_associations, summarize\n# from llm_parser.starcoder import get_associations, summarize\nfrom llm_parser.falcon import get_associations, summarize\n\n\nst.set_page_config(page_title=\"PubGPT\", initial_sidebar_state=\"auto\")\n", "st.set_page_config(page_title=\"PubGPT\", initial_sidebar_state=\"auto\")\n\nhide_streamlit_style = \"\"\"\n            <style>\n            footer {visibility: hidden;}          \n            </style>\n            \"\"\"\nst.markdown(hide_streamlit_style, unsafe_allow_html=True)\n\nst.title(\"PubGPT \ud83d\udc89\ud83d\udcc4\")", "\nst.title(\"PubGPT \ud83d\udc89\ud83d\udcc4\")\n\n\n@st.cache_data\ndef convert_df(df):\n    return df.to_csv().encode(\"utf-8\")\n\n\ndef online_parser():\n    st.markdown(\"\"\"## Online parser\"\"\")\n    pubmed_id = st.text_input(\"PubMed ID\", \"32819603\")\n    parse_paper = st.button(\"Parse paper\")\n    if parse_paper:\n        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n        st.write(f\"Paper ID: {paper_id}\")\n        st.write(f\"Title: {title}\")\n        st.write(f\"Abstract: {abstract}\")\n        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n        mesh_terms = extract_mesh_terms(pubmed_id=pubmed_id)\n        other_terms = extract_other_terms(pubmed_id=pubmed_id)\n        first_row = st.columns(2)\n        first_row[0].markdown(\"### Genes\")\n        first_row[0].dataframe(gene_df)\n        st.download_button(\n            label=\"Download genes as CSV\",\n            data=convert_df(df=gene_df),\n            file_name=f\"{pubmed_id}_genes.csv\",\n        )\n        first_row[1].markdown(\"### Diseases\")\n        first_row[1].dataframe(disease_df)\n        st.download_button(\n            label=\"Download diseases as CSV\",\n            data=convert_df(df=disease_df),\n            file_name=f\"{pubmed_id}_diseases.csv\",\n        )\n\n        second_row = st.columns(2)\n        if mesh_terms is not None:\n            second_row[0].markdown(\"### MeSH terms\")\n            second_row[0].dataframe(mesh_terms)\n            st.download_button(\n                label=\"Download MeSH terms as CSV\",\n                data=convert_df(df=mesh_terms),\n                file_name=f\"{pubmed_id}_mesh_terms.csv\",\n            )\n\n        if other_terms is not None:\n            second_row[1].markdown(\"### Other terms\")\n            second_row[1].dataframe(other_terms)\n            st.download_button(\n                label=\"Download other terms as CSV\",\n                data=convert_df(df=other_terms),\n                file_name=f\"{pubmed_id}_other_terms.csv\",\n            )\n    extract_associations = st.button(\"Extract associations between genes and diseases\")\n    st.warning(\"May produce incorrect informations\", icon=\"\u26a0\ufe0f\")\n    if extract_associations:\n        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n        result_cohere = get_associations(\n            document=document, pubmed_id=pubmed_id, pairs=pairs\n        )\n        st.write(result_cohere)", "\ndef online_parser():\n    st.markdown(\"\"\"## Online parser\"\"\")\n    pubmed_id = st.text_input(\"PubMed ID\", \"32819603\")\n    parse_paper = st.button(\"Parse paper\")\n    if parse_paper:\n        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n        st.write(f\"Paper ID: {paper_id}\")\n        st.write(f\"Title: {title}\")\n        st.write(f\"Abstract: {abstract}\")\n        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n        mesh_terms = extract_mesh_terms(pubmed_id=pubmed_id)\n        other_terms = extract_other_terms(pubmed_id=pubmed_id)\n        first_row = st.columns(2)\n        first_row[0].markdown(\"### Genes\")\n        first_row[0].dataframe(gene_df)\n        st.download_button(\n            label=\"Download genes as CSV\",\n            data=convert_df(df=gene_df),\n            file_name=f\"{pubmed_id}_genes.csv\",\n        )\n        first_row[1].markdown(\"### Diseases\")\n        first_row[1].dataframe(disease_df)\n        st.download_button(\n            label=\"Download diseases as CSV\",\n            data=convert_df(df=disease_df),\n            file_name=f\"{pubmed_id}_diseases.csv\",\n        )\n\n        second_row = st.columns(2)\n        if mesh_terms is not None:\n            second_row[0].markdown(\"### MeSH terms\")\n            second_row[0].dataframe(mesh_terms)\n            st.download_button(\n                label=\"Download MeSH terms as CSV\",\n                data=convert_df(df=mesh_terms),\n                file_name=f\"{pubmed_id}_mesh_terms.csv\",\n            )\n\n        if other_terms is not None:\n            second_row[1].markdown(\"### Other terms\")\n            second_row[1].dataframe(other_terms)\n            st.download_button(\n                label=\"Download other terms as CSV\",\n                data=convert_df(df=other_terms),\n                file_name=f\"{pubmed_id}_other_terms.csv\",\n            )\n    extract_associations = st.button(\"Extract associations between genes and diseases\")\n    st.warning(\"May produce incorrect informations\", icon=\"\u26a0\ufe0f\")\n    if extract_associations:\n        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n        result_cohere = get_associations(\n            document=document, pubmed_id=pubmed_id, pairs=pairs\n        )\n        st.write(result_cohere)", "\n\ndef local_parser():\n    st.markdown(\"\"\"## Local parser\"\"\")\n    uploaded_file = st.file_uploader(\"Choose a file\")\n    if uploaded_file:\n        pdf = read_pdf(uploaded_file)\n        pdf_content = extract_pdf_content(pdf=pdf)\n        splitted_text_from_pdf = split_pdf_content(\n            pdf_content=pdf_content, chunk_size=1000, chunk_overlap=200\n        )\n        embeddings = create_embeddings(splitted_text_from_pdf=splitted_text_from_pdf)\n        query = st.text_input(\n            \"Insert query here:\",\n            \"Es: Is BRCA1 associated with breast cancer?\",\n        )\n        if st.button(\"Query document\"):\n            st.write(retriever(query=query, embeddings=embeddings))", "\n\nif __name__ == \"__main__\":\n    online_parser()\n    local_parser()\n"]}
{"filename": "pubgpt/local.py", "chunked_list": ["from pdf_parser.utils import (\n    read_pdf,\n    extract_pdf_content,\n    split_pdf_content,\n)\n\n# from pdf_parser.openai import create_embeddings_openai, retriever_openai\nfrom pdf_parser.cohere import create_embeddings_cohere, retriever_cohere\n\n\nif __name__ == \"__main__\":\n    pdf_path = \"input/Breast cancer genes: beyond BRCA1 and BRCA2.pdf\"\n    query = \"Is BRCA1 associated with breast cancer?\"\n    pdf = read_pdf(pdf_path)\n    pdf_content = extract_pdf_content(pdf=pdf)\n    splitted_text_from_pdf = split_pdf_content(\n        pdf_content=pdf_content, chunk_size=1000, chunk_overlap=200\n    )\n    ##########################################################\n\n    # embeddings = create_embeddings_openai(splitted_text_from_pdf=splitted_text_from_pdf)\n    # print(retriever_openai(query=query, embeddings=embeddings))\n    embeddings = create_embeddings_cohere(splitted_text_from_pdf=splitted_text_from_pdf)\n    print(retriever_cohere(query=query, embeddings=embeddings))", "\n\nif __name__ == \"__main__\":\n    pdf_path = \"input/Breast cancer genes: beyond BRCA1 and BRCA2.pdf\"\n    query = \"Is BRCA1 associated with breast cancer?\"\n    pdf = read_pdf(pdf_path)\n    pdf_content = extract_pdf_content(pdf=pdf)\n    splitted_text_from_pdf = split_pdf_content(\n        pdf_content=pdf_content, chunk_size=1000, chunk_overlap=200\n    )\n    ##########################################################\n\n    # embeddings = create_embeddings_openai(splitted_text_from_pdf=splitted_text_from_pdf)\n    # print(retriever_openai(query=query, embeddings=embeddings))\n    embeddings = create_embeddings_cohere(splitted_text_from_pdf=splitted_text_from_pdf)\n    print(retriever_cohere(query=query, embeddings=embeddings))", ""]}
{"filename": "pubgpt/__init__.py", "chunked_list": [""]}
{"filename": "pubgpt/online.py", "chunked_list": ["from online_parser.article_parser import (\n    parse_article,\n    extract_genes_and_diseases,\n    extract_mesh_terms,\n    extract_other_terms,\n)\n\n\n# from llm.openai import get_associations, summarize\n", "# from llm.openai import get_associations, summarize\n\n# from llm_parser.cohere import get_associations, summarize\n\n# from llm_parser.starcoder import get_associations, summarize\n\n# from llm_parser.falcon import get_associations, summarize\n\nfrom llm_parser.llama2 import get_associations, summarize\n", "from llm_parser.llama2 import get_associations, summarize\n\n\nif __name__ == \"__main__\":\n    pubmed_id = \"32819603\"\n    paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n    gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n    mesh_terms = extract_mesh_terms(pubmed_id=pubmed_id)\n    other_terms = extract_other_terms(pubmed_id=pubmed_id)\n    #####################################################\n    associations = get_associations(document=document, pubmed_id=pubmed_id, pairs=pairs)\n    digest = summarize(document=document, pubmed_id=pubmed_id)", ""]}
{"filename": "pubgpt/utils/__init__.py", "chunked_list": [""]}
{"filename": "pubgpt/utils/utils.py", "chunked_list": ["import pathlib\n\n\ndef create_id_folder(pubmed_id: str) -> None:\n    pathlib.Path(f\"output/{pubmed_id}\").mkdir(parents=True, exist_ok=True)\n"]}
{"filename": "pubgpt/llm_parser/llama2.py", "chunked_list": ["from typing import List, Tuple\nfrom dotenv import load_dotenv\nimport requests\nimport os\n\n\nload_dotenv()\n\n\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Llama-2 LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = (\n        \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\"\n    )\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    print(response)\n    print(response.text)\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Llama-2 LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = (\n        \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\"\n    )\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    print(response)\n    print(response.text)\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\n\ndef summarize(document: str, pubmed_id: str) -> str:\n    \"\"\"\n    Summarize the paper.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        str: Digest\n    \"\"\"\n    prompt = f\"\"\"\nSummarize this text, trying to keep all relevant informations:\n{document.strip()}\n    \"\"\"\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b\"\n    response = requests.post(\n        api_url,\n        headers=headers,\n        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n        timeout=60,\n    )\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/llama2_digest.txt\", \"w\") as f:\n        f.write(result)\n    return result", ""]}
{"filename": "pubgpt/llm_parser/falcon.py", "chunked_list": ["from typing import List, Tuple\nfrom dotenv import load_dotenv\nimport requests\nimport os\n\n\nload_dotenv()\n\n\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Falcon LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = (\n        \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n    )\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    result = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/falcon_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Falcon LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = (\n        \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n    )\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    result = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/falcon_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\n\ndef summarize(document: str, pubmed_id: str) -> str:\n    \"\"\"\n    Summarize the paper.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        str: Digest\n    \"\"\"\n    prompt = f\"\"\"\nSummarize this text, trying to keep all relevant informations:\n{document.strip()}\n    \"\"\"\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = (\n        \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n    )\n    response = requests.post(\n        api_url,\n        headers=headers,\n        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n        timeout=60,\n    )\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/falcon_digest.txt\", \"w\") as f:\n        f.write(result)\n    return result", ""]}
{"filename": "pubgpt/llm_parser/openai.py", "chunked_list": ["from typing import List, Tuple\nfrom dotenv import load_dotenv\nimport openai\nimport os\n\n\nload_dotenv()\n\n\ndef get_associations(\n    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n) -> str:\n    \"\"\"\n    Get associations using OpenAI LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    temperature, frequency_penalty, presence_penalty, max_tokens, top_p, engine = (\n        0,\n        0,\n        0,\n        500,\n        1,\n        \"gpt-3.5-turbo\",\n    )\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    response = openai.Completion.create(\n        engine=engine,\n        prompt=prompt,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n    )[\"choices\"]\n    with open(f\"output/{pubmed_id}/openai_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(response)\n    return response", "\ndef get_associations(\n    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n) -> str:\n    \"\"\"\n    Get associations using OpenAI LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    temperature, frequency_penalty, presence_penalty, max_tokens, top_p, engine = (\n        0,\n        0,\n        0,\n        500,\n        1,\n        \"gpt-3.5-turbo\",\n    )\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    response = openai.Completion.create(\n        engine=engine,\n        prompt=prompt,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n    )[\"choices\"]\n    with open(f\"output/{pubmed_id}/openai_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(response)\n    return response", ""]}
{"filename": "pubgpt/llm_parser/__init__.py", "chunked_list": [""]}
{"filename": "pubgpt/llm_parser/cohere.py", "chunked_list": ["from typing import List, Tuple\nfrom dotenv import load_dotenv\nimport cohere\nimport os\n\n\nload_dotenv()\n\n\ndef get_associations(\n    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n) -> str:\n    \"\"\"\n    Get associations using Cohere LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    temperature, max_tokens = (0, 500)\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n    response = (\n        co.generate(\n            model=\"command-xlarge-nightly\",\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n        .generations[0]\n        .text\n    )\n    with open(f\"output/{pubmed_id}/cohere_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(response)\n    return response", "\ndef get_associations(\n    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n) -> str:\n    \"\"\"\n    Get associations using Cohere LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    temperature, max_tokens = (0, 500)\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n    response = (\n        co.generate(\n            model=\"command-xlarge-nightly\",\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n        .generations[0]\n        .text\n    )\n    with open(f\"output/{pubmed_id}/cohere_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(response)\n    return response", "\n\ndef summarize(document: str, pubmed_id: str) -> str:\n    prompt = f\"\"\"\nSummarize this text, trying to keep all relevant informations:\n{document.strip()}\n    \"\"\"\n    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n    temperature, max_tokens = (0, 500)\n    response = (\n        co.generate(\n            model=\"command-xlarge-nightly\",\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n        .generations[0]\n        .text\n    )\n    with open(f\"output/{pubmed_id}/cohere_digest.txt\", \"w\") as f:\n        f.write(response)\n    return response", ""]}
{"filename": "pubgpt/llm_parser/starcoder.py", "chunked_list": ["from typing import List, Tuple\nfrom dotenv import load_dotenv\nimport requests\nimport os\n\n\nload_dotenv()\n\n\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Starcoder LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\ndef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n    \"\"\"\n    Get associations using Starcoder LLM.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\n    Returns:\n        str: Response\n    \"\"\"\n    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n    pre_prompt: list = []\n    for index, item in enumerate(pairs, 1):\n        pre_prompt.append(\n            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n        )\n    pre_prompt = \"\\n\".join(pre_prompt)\n    prompt = f\"\"\"\n    According to this abstract:\\n\n{document.strip()}\\n\nCan you tell me if:\\n\n{pre_prompt.strip()}\\n\nAs result, provide me only CSV with:\n- Boolean result (only 'Yes' or 'No')\n- The entire part before the sentence \"is associated with\"\n- The entire part after the sentence \"is associated with\"\nFor instance:\n'Yes,X,Y'\nAlso, remove the numbers list (like 1)) from the CSV\n    \"\"\".strip()\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n    response = requests.post(\n        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n    )\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n        f.write(\"result,gene,disease\")\n        f.write(result)\n    return result", "\n\ndef summarize(document: str, pubmed_id: str) -> str:\n    \"\"\"\n    Summarize the paper.\n\n    Args:\n        document (str): Text (abstract or full text)\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        str: Digest\n    \"\"\"\n    prompt = f\"\"\"\nSummarize this text, trying to keep all relevant informations:\n{document.strip()}\n    \"\"\"\n    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n    api_url: str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n    response = requests.post(\n        api_url,\n        headers=headers,\n        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n        timeout=60,\n    )\n    result: str = response.json()[0][\"generated_text\"]\n    with open(f\"output/{pubmed_id}/starcoder_digest.txt\", \"w\") as f:\n        f.write(result)\n    return result", ""]}
{"filename": "pubgpt/pdf_parser/openai.py", "chunked_list": ["from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import OpenAI\nfrom typing import List, Any\nfrom dotenv import load_dotenv\n\nload_dotenv()\n", "load_dotenv()\n\n\ndef create_embeddings_openai(splitted_text_from_pdf: List) -> Any:\n    \"\"\"\n    Create embeddings from chunks for OpenAI.\n\n    Args:\n        splitted_text_from_pdf (List): List of chunks\n\n    Returns:\n        Any: Embeddings\n    \"\"\"\n    embeddings = OpenAIEmbeddings()\n    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n    return documents", "\n\ndef create_opeanai_chain(query: str, embeddings: Any) -> None:\n    \"\"\"\n    Create chain for OpenAI.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n    \"\"\"\n    chain = load_qa_chain(llm=OpenAI(), chain_type=\"stuff\")\n    docs = embeddings.similarity_search(query)\n    chain.run(input_documents=docs, question=query)", "\n\ndef retriever_openai(query: str, embeddings: Any) -> str:\n    \"\"\"\n    Create retriever for OpenAI.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n\n    Returns:\n        str: Result of retriever\n    \"\"\"\n    retriever = embeddings.as_retriever(search_type=\"similarity\")\n    result = RetrievalQA.from_chain_type(\n        llm=OpenAI,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n    )\n    return result(query)[\"result\"]", ""]}
{"filename": "pubgpt/pdf_parser/__init__.py", "chunked_list": [""]}
{"filename": "pubgpt/pdf_parser/utils.py", "chunked_list": ["from PyPDF2 import PdfReader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom typing import List\n\n\ndef read_pdf(pdf_path: str) -> None:\n    \"\"\"\n    Read a local pdf.\n\n    Args:\n        pdf_path (str): PDF path\n\n    Returns:\n        PdfReader: PdfReader\n    \"\"\"\n    doc_reader = PdfReader(pdf_path)\n    return doc_reader", "\n\ndef extract_pdf_content(pdf: PdfReader) -> str:\n    \"\"\"\n    Extract the content of the PDF.\n\n    Args:\n        pdf (PdfReader): PDF as PdfReader\n\n    Returns:\n        str: Raw content of the PDF\n    \"\"\"\n    raw_text: str = \"\"\n    for index, page in enumerate(pdf.pages):\n        text = page.extract_text()\n        if text:\n            raw_text += text\n    return raw_text", "\n\ndef split_pdf_content(pdf_content: str, chunk_size: int, chunk_overlap: int) -> List:\n    \"\"\"\n    Split the content of the PDF into chunks.\n\n    Args:\n        pdf_content (str): Raw content of the PDF\n        chunk_size (int): Dimension of each chunk\n        chunk_overlap (int): Dimension of the overlap for each chunk\n\n    Returns:\n        List: List of chunks\n    \"\"\"\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n    )\n    return text_splitter.split_text(pdf_content)", ""]}
{"filename": "pubgpt/pdf_parser/cohere.py", "chunked_list": ["from langchain.embeddings import CohereEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import Cohere\nfrom typing import List, Any\nfrom dotenv import load_dotenv\n\nload_dotenv()\n", "load_dotenv()\n\n\ndef create_embeddings(splitted_text_from_pdf: List) -> Any:\n    \"\"\"\n    Create embeddings from chunks for Cohere.\n\n    Args:\n        splitted_text_from_pdf (List): List of chunks\n\n    Returns:\n        Any: Embeddings\n    \"\"\"\n    embeddings = CohereEmbeddings()\n    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n    return documents", "\n\ndef create_chain(query: str, embeddings: Any) -> None:\n    \"\"\"\n    Create chain for Cohere.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n    \"\"\"\n    chain = load_qa_chain(llm=Cohere(), chain_type=\"stuff\")\n    docs = embeddings.similarity_search(query)\n    chain.run(input_documents=docs, question=query)", "\n\ndef retriver(query: str, embeddings: Any) -> str:\n    \"\"\"\n    Create retriever for Cohere.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n\n    Returns:\n        str: Result of retriever\n    \"\"\"\n    retriever = embeddings.as_retriever(search_type=\"similarity\")\n    result = RetrievalQA.from_chain_type(\n        llm=Cohere(),\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n    )\n    return result(query)[\"result\"]", ""]}
{"filename": "pubgpt/pdf_parser/starcoder.py", "chunked_list": ["from langchain.embeddings import HuggingFaceHubEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import HuggingFaceHub\nfrom typing import List, Any\nfrom dotenv import load_dotenv\nimport os\n\n", "\n\nload_dotenv()\n\n\ndef create_embeddings(splitted_text_from_pdf: List) -> Any:\n    \"\"\"\n    Create embeddings from chunks for Starcoder.\n\n    Args:\n        splitted_text_from_pdf (List): List of chunks\n\n    Returns:\n        Any: Embeddings\n    \"\"\"\n    embeddings = HuggingFaceHubEmbeddings(\n        repo_id=\"sentence-transformers/all-mpnet-base-v2\",\n        huggingfacehug_api_key=os.getenv(\"HUGGINFACE_API_KEY\"),\n    )\n    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n    return documents", "\n\ndef create_chain(query: str, embeddings: Any) -> None:\n    \"\"\"\n    Create chain for Starcoder.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n    \"\"\"\n    chain = load_qa_chain(\n        llm=HuggingFaceHub(\n            repo_id=\"bigcode/starcoder\",\n            huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n        ),\n        chain_type=\"stuff\",\n    )\n    docs = embeddings.similarity_search(query)\n    chain.run(input_documents=docs, question=query)", "\n\ndef retriever(query: str, embeddings: Any) -> str:\n    \"\"\"\n    Create retriever for Cohere.\n\n    Args:\n        query (str): Query\n        embeddings (Any): Embeddings\n\n    Returns:\n        str: Result of retriever\n    \"\"\"\n    retriever = embeddings.as_retriever(search_type=\"similarity\")\n    result = RetrievalQA.from_chain_type(\n        llm=HuggingFaceHub(\n            repo_id=\"bigcode/starcoder\",\n            huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n        ),\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n    )\n    return result(query)[\"result\"]", ""]}
{"filename": "pubgpt/online_parser/__init__.py", "chunked_list": [""]}
{"filename": "pubgpt/online_parser/article_parser.py", "chunked_list": ["from typing import Tuple\nfrom Bio import Entrez, Medline\nimport pandas as pd\nimport numpy as np\nimport requests\nimport time\nfrom xml.etree import ElementTree\nfrom utils.utils import create_id_folder\n\n\ndef search_on_pubmed(pubmed_id: str) -> list:\n    \"\"\"\n    Search on PubMed using a PubMed ID.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        list: List of records of the article\n    \"\"\"\n    Entrez.email = \"random@example.com\"\n    handle = Entrez.esearch(\n        db=\"pubmed\", sort=\"relevance\", retmax=1, retmode=\"text\", term=pubmed_id\n    )\n    result = Entrez.read(handle)\n    ids = result[\"IdList\"]\n    handle = Entrez.efetch(\n        db=\"pubmed\", sort=\"relevance\", retmode=\"text\", rettype=\"medline\", id=ids\n    )\n    records = Medline.parse(handle)\n    return records", "\n\ndef search_on_pubmed(pubmed_id: str) -> list:\n    \"\"\"\n    Search on PubMed using a PubMed ID.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        list: List of records of the article\n    \"\"\"\n    Entrez.email = \"random@example.com\"\n    handle = Entrez.esearch(\n        db=\"pubmed\", sort=\"relevance\", retmax=1, retmode=\"text\", term=pubmed_id\n    )\n    result = Entrez.read(handle)\n    ids = result[\"IdList\"]\n    handle = Entrez.efetch(\n        db=\"pubmed\", sort=\"relevance\", retmode=\"text\", rettype=\"medline\", id=ids\n    )\n    records = Medline.parse(handle)\n    return records", "\n\ndef parse_article(pubmed_id: str) -> Tuple[str, str, str, str]:\n    \"\"\"\n    Parse the article.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        Tuple[str, str, str, str]: PubMed ID, title, abstract, title+abstract\n    \"\"\"\n    create_id_folder(pubmed_id=pubmed_id)\n    for record in search_on_pubmed(pubmed_id=pubmed_id):\n        title = record.get(\"TI\", \"\")\n        abstract = record.get(\"AB\", \"\")\n        pubmed_id = record.get(\"PMID\", \"\")\n    document = title + \" \" + abstract\n    with open(f\"output/{pubmed_id}/document.txt\", \"w\") as f:\n        f.write(document)\n    return pubmed_id, title, abstract, document", "\n\ndef extract_mesh_terms(pubmed_id: str) -> pd.DataFrame:\n    \"\"\"\n    Extract MeSH terms from article.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        pd.DataFrame: DataFrame with MeSH terms\n    \"\"\"\n    create_id_folder(pubmed_id=pubmed_id)\n    for record in search_on_pubmed(pubmed_id=pubmed_id):\n        mesh_terms = record.get(\"MH\", \"\")\n    df = pd.DataFrame(data=zip(mesh_terms), columns=[\"element\"])\n    df.to_csv(f\"output/{pubmed_id}/mesh_terms.csv\", encoding=\"utf-8\", index=False)", "\n\ndef extract_other_terms(pubmed_id: str) -> pd.DataFrame:\n    \"\"\"\n    Extract other terms from article.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        pd.DataFrame: DataFrame with other terms\n    \"\"\"\n    create_id_folder(pubmed_id=pubmed_id)\n    for record in search_on_pubmed(pubmed_id=pubmed_id):\n        other_terms = record.get(\"OT\", \"\")\n    df = pd.DataFrame(data=zip(other_terms), columns=[\"element\"])\n    df.to_csv(f\"output/{pubmed_id}/other_terms.csv\", encoding=\"utf-8\", index=False)", "\n\ndef extract_genes_and_diseases(pubmed_id: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Extract genes and disease from article.\n\n    Args:\n        pubmed_id (str): PubMed ID\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: DataFrame with genes and DataFrame with diseases\n    \"\"\"\n    create_id_folder(pubmed_id=pubmed_id)\n    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=gene,disease\"\n    response = requests.get(url)\n    time.sleep(0.5)\n    doc = ElementTree.fromstring(response.content)\n    tree = ElementTree.ElementTree(doc)\n    root = tree.getroot()\n    doc = root[3]\n    passage = doc[1:]\n    text_list, element_list, identifier_list, type_list = [], [], [], []\n    for i in passage:\n        for text in i.iterfind(\"text\"):\n            text = text.text\n            text_list.append(text)\n        for annotation in i.iterfind(\"annotation\"):\n            for text in annotation.iterfind(\"text\"):\n                element = text.text\n                element_list.append(element)\n            infos = annotation.findall(\"infon\")\n            try:\n                identifier = infos[0].text\n            except Exception:\n                identifier = \"\"\n            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n            try:\n                typex = infos[1].text\n            except Exception:\n                typex = infos[0].text\n            type_list.append(typex)\n    text_list = [i.strip() for i in text_list]\n    element_list = [i.strip() for i in element_list]\n    identifier_list = [i.strip() for i in identifier_list]\n    type_list = [i.strip() for i in type_list]\n    df = pd.DataFrame(\n        data=zip(element_list, identifier_list, type_list),\n        columns=[\"element\", \"identifier\", \"type\"],\n    )\n    df.identifier = df.identifier.replace(\"Disease\", np.nan)\n    df = df.drop_duplicates(\"identifier\")\n    disease_df = df[df.type == \"Disease\"]\n    gene_df = df[df.type == \"Gene\"]\n    disease_df = disease_df.drop(\"type\", axis=1)\n    gene_df = gene_df.drop(\"type\", axis=1)\n    genes_pairs = list(\n        gene_df[[\"element\", \"identifier\"]].itertuples(index=False, name=None)\n    )\n    disease_pairs = list(\n        disease_df[[\"element\", \"identifier\"]].itertuples(index=False, name=None)\n    )\n    pairs = list(set([(i, j) for i in genes_pairs for j in disease_pairs]))\n    gene_df.to_csv(f\"output/{pubmed_id}/genes.csv\", encoding=\"utf-8\", index=False)\n    disease_df.to_csv(f\"output/{pubmed_id}/diseases.csv\", encoding=\"utf-8\", index=False)\n    return gene_df, disease_df, pairs", "\n\ndef extract_chemicals(pubmed_id: str):\n    create_id_folder(pubmed_id=pubmed_id)\n    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=chemical\"\n    response = requests.get(url)\n    time.sleep(0.5)\n    doc = ElementTree.fromstring(response.content)\n    tree = ElementTree.ElementTree(doc)\n    root = tree.getroot()\n    doc = root[3]\n    passage = doc[1:]\n    text_list, element_list, identifier_list, type_list = [], [], [], []\n    for i in passage:\n        for text in i.iterfind(\"text\"):\n            text = text.text\n            text_list.append(text)\n        for annotation in i.iterfind(\"annotation\"):\n            for text in annotation.iterfind(\"text\"):\n                element = text.text\n                element_list.append(element)\n            infos = annotation.findall(\"infon\")\n            try:\n                identifier = infos[0].text\n            except Exception:\n                identifier = \"\"\n            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n            try:\n                typex = infos[1].text\n            except Exception:\n                typex = infos[0].text\n            type_list.append(typex)\n    text_list = [i.strip() for i in text_list]\n    element_list = [i.strip() for i in element_list]\n    identifier_list = [i.strip() for i in identifier_list]\n    type_list = [i.strip() for i in type_list]\n    df = pd.DataFrame(\n        data=zip(element_list, identifier_list, type_list),\n        columns=[\"element\", \"identifier\", \"type\"],\n    )\n    df[\"element\"] = df[\"element\"].astype(str)\n    df[\"identifier\"] = df[\"identifier\"].astype(str)\n    df[\"type\"] = df[\"type\"].astype(str)\n    df = df.drop_duplicates(\"identifier\")\n    df.to_csv(f\"output/{pubmed_id}/chemicals.csv\", sep=\",\", index=False)\n    return df", "\n\ndef extract_mutations(pubmed_id: str):\n    create_id_folder(pubmed_id=pubmed_id)\n    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=mutation\"\n    response = requests.get(url)\n    time.sleep(0.5)\n    doc = ElementTree.fromstring(response.content)\n    tree = ElementTree.ElementTree(doc)\n    root = tree.getroot()\n    doc = root[3]\n    passage = doc[1:]\n    text_list, element_list, identifier_list, type_list = [], [], [], []\n    for i in passage:\n        for text in i.iterfind(\"text\"):\n            text = text.text\n            text_list.append(text)\n        for annotation in i.iterfind(\"annotation\"):\n            for text in annotation.iterfind(\"text\"):\n                element = text.text\n                element_list.append(element)\n            infos = annotation.findall(\"infon\")\n            try:\n                identifier = infos[0].text\n            except Exception:\n                identifier = \"\"\n            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n            try:\n                typex = infos[1].text\n            except Exception:\n                typex = infos[0].text\n            type_list.append(typex)\n    text_list = [i.strip() for i in text_list]\n    element_list = [i.strip() for i in element_list]\n    identifier_list = [i.strip() for i in identifier_list]\n    type_list = [i.strip() for i in type_list]\n    df = pd.DataFrame(\n        data=zip(element_list, identifier_list, type_list),\n        columns=[\"element\", \"identifier\", \"type\"],\n    )\n    df[\"element\"] = df[\"element\"].astype(str)\n    df[\"identifier\"] = df[\"identifier\"].astype(str)\n    df[\"type\"] = df[\"type\"].astype(str)\n    df = df.drop_duplicates(\"identifier\")\n    df.to_csv(f\"output/{pubmed_id}/mutation.csv\", sep=\",\", index=False)\n    return df", "\n\ndef extract_species(pubmed_id: str):\n    create_id_folder(pubmed_id=pubmed_id)\n    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=species\"\n    response = requests.get(url)\n    time.sleep(0.5)\n    doc = ElementTree.fromstring(response.content)\n    tree = ElementTree.ElementTree(doc)\n    root = tree.getroot()\n    doc = root[3]\n    passage = doc[1:]\n    text_list, element_list, identifier_list, type_list = [], [], [], []\n    for i in passage:\n        for text in i.iterfind(\"text\"):\n            text = text.text\n            text_list.append(text)\n        for annotation in i.iterfind(\"annotation\"):\n            for text in annotation.iterfind(\"text\"):\n                element = text.text\n                element_list.append(element)\n            infos = annotation.findall(\"infon\")\n            try:\n                identifier = infos[0].text\n            except Exception:\n                identifier = \"\"\n            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n            try:\n                typex = infos[1].text\n            except Exception:\n                typex = infos[0].text\n            type_list.append(typex)\n    text_list = [i.strip() for i in text_list]\n    element_list = [i.strip() for i in element_list]\n    identifier_list = [i.strip() for i in identifier_list]\n    type_list = [i.strip() for i in type_list]\n    df = pd.DataFrame(\n        data=zip(element_list, identifier_list, type_list),\n        columns=[\"element\", \"identifier\", \"type\"],\n    )\n    df[\"element\"] = df[\"element\"].astype(str)\n    df[\"identifier\"] = df[\"identifier\"].astype(str)\n    df[\"type\"] = df[\"type\"].astype(str)\n    df = df.drop_duplicates(\"identifier\")\n    df.to_csv(f\"output/{pubmed_id}/species.csv\", sep=\",\", index=False)\n    return df", ""]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"PubGPT tests\"\"\"\n"]}
{"filename": "tests/llm_parser/test_falcon.py", "chunked_list": ["import unittest\n\nfrom pubgpt.llm_parser.falcon import get_associations\n\n\nclass TestGetAssociations(unittest.TestCase):\n    def test_get_associations(self):\n        document = \"This is a sample abstract.\"\n        pubmed_id = \"123456\"\n        pairs = [(\"GeneA\", \"DiseaseX\"), (\"GeneB\", \"DiseaseY\")]\n\n        result = get_associations(document, pubmed_id, pairs)\n\n        self.assertEqual(result, \"Yes,GeneA,DiseaseX\\nYes,GeneB,DiseaseY\")", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "tests/llm_parser/__init__.py", "chunked_list": ["\"\"\"PubGPT tests\"\"\"\n"]}
{"filename": "tests/online_parser/__init__.py", "chunked_list": [""]}
{"filename": "tests/online_parser/test_article_parser.py", "chunked_list": [""]}
