{"filename": "user_voice_collect.py", "chunked_list": ["import gradio as gr\nimport torch\nimport torchaudio\n\nanno_lines = []\nwith open(\"./user_voice/user_voice.txt\", 'r', encoding='utf-8') as f:\n    for line in f.readlines():\n        anno_lines.append(line.strip(\"\\n\"))\n\ntext_index = 0", "\ntext_index = 0\n\ndef display_text(index):\n    index = int(index)\n    global text_index\n    text_index = index\n    return f\"{text_index}: \" + anno_lines[index].split(\"|\")[2].strip(\"[ZH]\")\n\ndef display_prev_text():\n    global text_index\n    if text_index != 0:\n        text_index -= 1\n    return f\"{text_index}: \" + anno_lines[text_index].split(\"|\")[2].strip(\"[ZH]\")", "\ndef display_prev_text():\n    global text_index\n    if text_index != 0:\n        text_index -= 1\n    return f\"{text_index}: \" + anno_lines[text_index].split(\"|\")[2].strip(\"[ZH]\")\n\ndef display_next_text():\n    global text_index\n    if text_index != len(anno_lines)-1:\n        text_index += 1\n    return f\"{text_index}: \" + anno_lines[text_index].split(\"|\")[2].strip(\"[ZH]\")", "\ndef save_audio(audio):\n    global text_index\n    if audio:\n        sr, wav = audio\n        wav = torch.tensor(wav).type(torch.float32) / max(wav.max(), -wav.min())\n        wav = wav.unsqueeze(0) if len(wav.shape) == 1 else wav\n        if sr != 22050:\n            res_wav = torchaudio.transforms.Resample(orig_freq=sr, new_freq=22050)(wav)\n        else:\n            res_wav = wav\n        torchaudio.save(f\"./user_voice/{str(text_index)}.wav\", res_wav, 22050, channels_first=True)\n        return f\"Audio saved to ./user_voice/{str(text_index)}.wav successfully!\"\n    else:\n        return \"Error: Please record your audio!\"", "\n\nif __name__ == \"__main__\":\n    app = gr.Blocks()\n    with app:\n        with gr.Row():\n            text = gr.Textbox(value=\"0: \" + anno_lines[0].split(\"|\")[2].strip(\"[ZH]\"), label=\"Please read the text here\")\n        with gr.Row():\n            audio_to_collect = gr.Audio(source=\"microphone\")\n        with gr.Row():\n            with gr.Column():\n                prev_btn = gr.Button(value=\"Previous\")\n            with gr.Column():\n                next_btn = gr.Button(value=\"Next\")\n        with gr.Row():\n            index_dropdown = gr.Dropdown(choices=[str(i) for i in range(len(anno_lines))], value=\"0\",\n                                         label=\"No. of text\", interactive=True)\n        with gr.Row():\n            with gr.Column():\n                save_btn = gr.Button(value=\"Save Audio\")\n            with gr.Column():\n                audio_save_message = gr.Textbox(label=\"Message\")\n        index_dropdown.change(display_text, inputs=index_dropdown, outputs=text)\n        prev_btn.click(display_prev_text, inputs=None, outputs=text)\n        next_btn.click(display_next_text, inputs=None, outputs=text)\n        save_btn.click(save_audio, inputs=audio_to_collect, outputs=audio_save_message)\n    app.launch()"]}
{"filename": "losses.py", "chunked_list": ["import torch\n\n\ndef feature_loss(fmap_r, fmap_g):\n  loss = 0\n  for dr, dg in zip(fmap_r, fmap_g):\n    for rl, gl in zip(dr, dg):\n      rl = rl.float().detach()\n      gl = gl.float()\n      loss += torch.mean(torch.abs(rl - gl))\n\n  return loss * 2 ", "\n\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n  loss = 0\n  r_losses = []\n  g_losses = []\n  for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n    dr = dr.float()\n    dg = dg.float()\n    r_loss = torch.mean((1-dr)**2)\n    g_loss = torch.mean(dg**2)\n    loss += (r_loss + g_loss)\n    r_losses.append(r_loss.item())\n    g_losses.append(g_loss.item())\n\n  return loss, r_losses, g_losses", "\n\ndef generator_loss(disc_outputs):\n  loss = 0\n  gen_losses = []\n  for dg in disc_outputs:\n    dg = dg.float()\n    l = torch.mean((1-dg)**2)\n    gen_losses.append(l)\n    loss += l\n\n  return loss, gen_losses", "\n\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n  \"\"\"\n  z_p, logs_q: [b, h, t_t]\n  m_p, logs_p: [b, h, t_t]\n  \"\"\"\n  z_p = z_p.float()\n  logs_q = logs_q.float()\n  m_p = m_p.float()\n  logs_p = logs_p.float()\n  z_mask = z_mask.float()\n\n  kl = logs_p - logs_q - 0.5\n  kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p)\n  kl = torch.sum(kl * z_mask)\n  l = kl / torch.sum(z_mask)\n  return l", ""]}
{"filename": "attentions.py", "chunked_list": ["import math\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport commons\nfrom modules import LayerNorm\n", "from modules import LayerNorm\n\n\nclass Encoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., window_size=4, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n\n    self.drop = nn.Dropout(p_dropout)\n    self.attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, window_size=window_size))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask):\n    attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.attn_layers[i](x, x, attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n\n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x", "\n\nclass Decoder(nn.Module):\n  def __init__(self, hidden_channels, filter_channels, n_heads, n_layers, kernel_size=1, p_dropout=0., proximal_bias=False, proximal_init=True, **kwargs):\n    super().__init__()\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n\n    self.drop = nn.Dropout(p_dropout)\n    self.self_attn_layers = nn.ModuleList()\n    self.norm_layers_0 = nn.ModuleList()\n    self.encdec_attn_layers = nn.ModuleList()\n    self.norm_layers_1 = nn.ModuleList()\n    self.ffn_layers = nn.ModuleList()\n    self.norm_layers_2 = nn.ModuleList()\n    for i in range(self.n_layers):\n      self.self_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout, proximal_bias=proximal_bias, proximal_init=proximal_init))\n      self.norm_layers_0.append(LayerNorm(hidden_channels))\n      self.encdec_attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))\n      self.norm_layers_1.append(LayerNorm(hidden_channels))\n      self.ffn_layers.append(FFN(hidden_channels, hidden_channels, filter_channels, kernel_size, p_dropout=p_dropout, causal=True))\n      self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n  def forward(self, x, x_mask, h, h_mask):\n    \"\"\"\n    x: decoder input\n    h: encoder output\n    \"\"\"\n    self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(device=x.device, dtype=x.dtype)\n    encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n    x = x * x_mask\n    for i in range(self.n_layers):\n      y = self.self_attn_layers[i](x, x, self_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_0[i](x + y)\n\n      y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n      y = self.drop(y)\n      x = self.norm_layers_1[i](x + y)\n      \n      y = self.ffn_layers[i](x, x_mask)\n      y = self.drop(y)\n      x = self.norm_layers_2[i](x + y)\n    x = x * x_mask\n    return x", "\n\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, channels, out_channels, n_heads, p_dropout=0., window_size=None, heads_share=True, block_length=None, proximal_bias=False, proximal_init=False):\n    super().__init__()\n    assert channels % n_heads == 0\n\n    self.channels = channels\n    self.out_channels = out_channels\n    self.n_heads = n_heads\n    self.p_dropout = p_dropout\n    self.window_size = window_size\n    self.heads_share = heads_share\n    self.block_length = block_length\n    self.proximal_bias = proximal_bias\n    self.proximal_init = proximal_init\n    self.attn = None\n\n    self.k_channels = channels // n_heads\n    self.conv_q = nn.Conv1d(channels, channels, 1)\n    self.conv_k = nn.Conv1d(channels, channels, 1)\n    self.conv_v = nn.Conv1d(channels, channels, 1)\n    self.conv_o = nn.Conv1d(channels, out_channels, 1)\n    self.drop = nn.Dropout(p_dropout)\n\n    if window_size is not None:\n      n_heads_rel = 1 if heads_share else n_heads\n      rel_stddev = self.k_channels**-0.5\n      self.emb_rel_k = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n      self.emb_rel_v = nn.Parameter(torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels) * rel_stddev)\n\n    nn.init.xavier_uniform_(self.conv_q.weight)\n    nn.init.xavier_uniform_(self.conv_k.weight)\n    nn.init.xavier_uniform_(self.conv_v.weight)\n    if proximal_init:\n      with torch.no_grad():\n        self.conv_k.weight.copy_(self.conv_q.weight)\n        self.conv_k.bias.copy_(self.conv_q.bias)\n      \n  def forward(self, x, c, attn_mask=None):\n    q = self.conv_q(x)\n    k = self.conv_k(c)\n    v = self.conv_v(c)\n    \n    x, self.attn = self.attention(q, k, v, mask=attn_mask)\n\n    x = self.conv_o(x)\n    return x\n\n  def attention(self, query, key, value, mask=None):\n    # reshape [b, d, t] -> [b, n_h, t, d_k]\n    b, d, t_s, t_t = (*key.size(), query.size(2))\n    query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n    key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n    value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\n    scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n    if self.window_size is not None:\n      assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n      key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n      rel_logits = self._matmul_with_relative_keys(query /math.sqrt(self.k_channels), key_relative_embeddings)\n      scores_local = self._relative_position_to_absolute_position(rel_logits)\n      scores = scores + scores_local\n    if self.proximal_bias:\n      assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n      scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)\n    if mask is not None:\n      scores = scores.masked_fill(mask == 0, -1e4)\n      if self.block_length is not None:\n        assert t_s == t_t, \"Local attention is only available for self-attention.\"\n        block_mask = torch.ones_like(scores).triu(-self.block_length).tril(self.block_length)\n        scores = scores.masked_fill(block_mask == 0, -1e4)\n    p_attn = F.softmax(scores, dim=-1) # [b, n_h, t_t, t_s]\n    p_attn = self.drop(p_attn)\n    output = torch.matmul(p_attn, value)\n    if self.window_size is not None:\n      relative_weights = self._absolute_position_to_relative_position(p_attn)\n      value_relative_embeddings = self._get_relative_embeddings(self.emb_rel_v, t_s)\n      output = output + self._matmul_with_relative_values(relative_weights, value_relative_embeddings)\n    output = output.transpose(2, 3).contiguous().view(b, d, t_t) # [b, n_h, t_t, d_k] -> [b, d, t_t]\n    return output, p_attn\n\n  def _matmul_with_relative_values(self, x, y):\n    \"\"\"\n    x: [b, h, l, m]\n    y: [h or 1, m, d]\n    ret: [b, h, l, d]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0))\n    return ret\n\n  def _matmul_with_relative_keys(self, x, y):\n    \"\"\"\n    x: [b, h, l, d]\n    y: [h or 1, m, d]\n    ret: [b, h, l, m]\n    \"\"\"\n    ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n    return ret\n\n  def _get_relative_embeddings(self, relative_embeddings, length):\n    max_relative_position = 2 * self.window_size + 1\n    # Pad first before slice to avoid using cond ops.\n    pad_length = max(length - (self.window_size + 1), 0)\n    slice_start_position = max((self.window_size + 1) - length, 0)\n    slice_end_position = slice_start_position + 2 * length - 1\n    if pad_length > 0:\n      padded_relative_embeddings = F.pad(\n          relative_embeddings,\n          commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]))\n    else:\n      padded_relative_embeddings = relative_embeddings\n    used_relative_embeddings = padded_relative_embeddings[:,slice_start_position:slice_end_position]\n    return used_relative_embeddings\n\n  def _relative_position_to_absolute_position(self, x):\n    \"\"\"\n    x: [b, h, l, 2*l-1]\n    ret: [b, h, l, l]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # Concat columns of pad to shift from relative to absolute indexing.\n    x = F.pad(x, commons.convert_pad_shape([[0,0],[0,0],[0,0],[0,1]]))\n\n    # Concat extra elements so to add up to shape (len+1, 2*len-1).\n    x_flat = x.view([batch, heads, length * 2 * length])\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0,0],[0,0],[0,length-1]]))\n\n    # Reshape and slice out the padded elements.\n    x_final = x_flat.view([batch, heads, length+1, 2*length-1])[:, :, :length, length-1:]\n    return x_final\n\n  def _absolute_position_to_relative_position(self, x):\n    \"\"\"\n    x: [b, h, l, l]\n    ret: [b, h, l, 2*l-1]\n    \"\"\"\n    batch, heads, length, _ = x.size()\n    # padd along column\n    x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length-1]]))\n    x_flat = x.view([batch, heads, length**2 + length*(length -1)])\n    # add 0's in the beginning that will skew the elements after reshape\n    x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n    x_final = x_flat.view([batch, heads, length, 2*length])[:,:,:,1:]\n    return x_final\n\n  def _attention_bias_proximal(self, length):\n    \"\"\"Bias for self-attention to encourage attention to close positions.\n    Args:\n      length: an integer scalar.\n    Returns:\n      a Tensor with shape [1, 1, length, length]\n    \"\"\"\n    r = torch.arange(length, dtype=torch.float32)\n    diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n    return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)", "\n\nclass FFN(nn.Module):\n  def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0., activation=None, causal=False):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.activation = activation\n    self.causal = causal\n\n    if causal:\n      self.padding = self._causal_padding\n    else:\n      self.padding = self._same_padding\n\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n    self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n    self.drop = nn.Dropout(p_dropout)\n\n  def forward(self, x, x_mask):\n    x = self.conv_1(self.padding(x * x_mask))\n    if self.activation == \"gelu\":\n      x = x * torch.sigmoid(1.702 * x)\n    else:\n      x = torch.relu(x)\n    x = self.drop(x)\n    x = self.conv_2(self.padding(x * x_mask))\n    return x * x_mask\n  \n  def _causal_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = self.kernel_size - 1\n    pad_r = 0\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, commons.convert_pad_shape(padding))\n    return x\n\n  def _same_padding(self, x):\n    if self.kernel_size == 1:\n      return x\n    pad_l = (self.kernel_size - 1) // 2\n    pad_r = self.kernel_size // 2\n    padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n    x = F.pad(x, commons.convert_pad_shape(padding))\n    return x", ""]}
{"filename": "transforms.py", "chunked_list": ["import numpy as np\nimport torch\nfrom torch.nn import functional as F\n\nDEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\n\n\ndef piecewise_rational_quadratic_transform(inputs, \n                                           unnormalized_widths,\n                                           unnormalized_heights,\n                                           unnormalized_derivatives,\n                                           inverse=False,\n                                           tails=None, \n                                           tail_bound=1.,\n                                           min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                           min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                           min_derivative=DEFAULT_MIN_DERIVATIVE):\n\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\n            'tails': tails,\n            'tail_bound': tail_bound\n        }\n\n    outputs, logabsdet = spline_fn(\n            inputs=inputs,\n            unnormalized_widths=unnormalized_widths,\n            unnormalized_heights=unnormalized_heights,\n            unnormalized_derivatives=unnormalized_derivatives,\n            inverse=inverse,\n            min_bin_width=min_bin_width,\n            min_bin_height=min_bin_height,\n            min_derivative=min_derivative,\n            **spline_kwargs\n    )\n    return outputs, logabsdet", "\ndef piecewise_rational_quadratic_transform(inputs, \n                                           unnormalized_widths,\n                                           unnormalized_heights,\n                                           unnormalized_derivatives,\n                                           inverse=False,\n                                           tails=None, \n                                           tail_bound=1.,\n                                           min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                           min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                           min_derivative=DEFAULT_MIN_DERIVATIVE):\n\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\n            'tails': tails,\n            'tail_bound': tail_bound\n        }\n\n    outputs, logabsdet = spline_fn(\n            inputs=inputs,\n            unnormalized_widths=unnormalized_widths,\n            unnormalized_heights=unnormalized_heights,\n            unnormalized_derivatives=unnormalized_derivatives,\n            inverse=inverse,\n            min_bin_width=min_bin_width,\n            min_bin_height=min_bin_height,\n            min_derivative=min_derivative,\n            **spline_kwargs\n    )\n    return outputs, logabsdet", "\n\ndef searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(\n        inputs[..., None] >= bin_locations,\n        dim=-1\n    ) - 1\n\n\ndef unconstrained_rational_quadratic_spline(inputs,\n                                            unnormalized_widths,\n                                            unnormalized_heights,\n                                            unnormalized_derivatives,\n                                            inverse=False,\n                                            tails='linear',\n                                            tail_bound=1.,\n                                            min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                            min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                            min_derivative=DEFAULT_MIN_DERIVATIVE):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == 'linear':\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError('{} tails are not implemented.'.format(tails))\n\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound, right=tail_bound, bottom=-tail_bound, top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative\n    )\n\n    return outputs, logabsdet", "\n\ndef unconstrained_rational_quadratic_spline(inputs,\n                                            unnormalized_widths,\n                                            unnormalized_heights,\n                                            unnormalized_derivatives,\n                                            inverse=False,\n                                            tails='linear',\n                                            tail_bound=1.,\n                                            min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                                            min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                                            min_derivative=DEFAULT_MIN_DERIVATIVE):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == 'linear':\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError('{} tails are not implemented.'.format(tails))\n\n    outputs[inside_interval_mask], logabsdet[inside_interval_mask] = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound, right=tail_bound, bottom=-tail_bound, top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative\n    )\n\n    return outputs, logabsdet", "\ndef rational_quadratic_spline(inputs,\n                              unnormalized_widths,\n                              unnormalized_heights,\n                              unnormalized_derivatives,\n                              inverse=False,\n                              left=0., right=1., bottom=0., top=1.,\n                              min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n                              min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n                              min_derivative=DEFAULT_MIN_DERIVATIVE):\n    if torch.min(inputs) < left or torch.max(inputs) > right:\n        raise ValueError('Input to a transform is not within its domain')\n\n    num_bins = unnormalized_widths.shape[-1]\n\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError('Minimal bin width too large for the number of bins')\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError('Minimal bin height too large for the number of bins')\n\n    widths = F.softmax(unnormalized_widths, dim=-1)\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n    cumwidths = torch.cumsum(widths, dim=-1)\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode='constant', value=0.0)\n    cumwidths = (right - left) * cumwidths + left\n    cumwidths[..., 0] = left\n    cumwidths[..., -1] = right\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\n    heights = F.softmax(unnormalized_heights, dim=-1)\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n    cumheights = torch.cumsum(heights, dim=-1)\n    cumheights = F.pad(cumheights, pad=(1, 0), mode='constant', value=0.0)\n    cumheights = (top - bottom) * cumheights + bottom\n    cumheights[..., 0] = bottom\n    cumheights[..., -1] = top\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\n\n    if inverse:\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\n    else:\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n    delta = heights / widths\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\n    if inverse:\n        a = (((inputs - input_cumheights) * (input_derivatives\n                                             + input_derivatives_plus_one\n                                             - 2 * input_delta)\n              + input_heights * (input_delta - input_derivatives)))\n        b = (input_heights * input_derivatives\n             - (inputs - input_cumheights) * (input_derivatives\n                                              + input_derivatives_plus_one\n                                              - 2 * input_delta))\n        c = - input_delta * (inputs - input_cumheights)\n\n        discriminant = b.pow(2) - 4 * a * c\n        assert (discriminant >= 0).all()\n\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\n        outputs = root * input_bin_widths + input_cumwidths\n\n        theta_one_minus_theta = root * (1 - root)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * root.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - root).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, -logabsdet\n    else:\n        theta = (inputs - input_cumwidths) / input_bin_widths\n        theta_one_minus_theta = theta * (1 - theta)\n\n        numerator = input_heights * (input_delta * theta.pow(2)\n                                     + input_derivatives * theta_one_minus_theta)\n        denominator = input_delta + ((input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n                                     * theta_one_minus_theta)\n        outputs = input_cumheights + numerator / denominator\n\n        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * theta.pow(2)\n                                                     + 2 * input_delta * theta_one_minus_theta\n                                                     + input_derivatives * (1 - theta).pow(2))\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, logabsdet", ""]}
{"filename": "translateBaidu.py", "chunked_list": ["import requests\nimport random\nimport json\nfrom hashlib import md5\nfrom config import APPID, APPKEY\n\n# Set your own appid/appkey.\nappid = APPID\nappkey = APPKEY\n", "appkey = APPKEY\n\n\ndef translate(txt, to_lang='jp'):\n    # For list of language codes, please refer to `https://api.fanyi.baidu.com/doc/21`\n    from_lang = 'zh'\n\n    endpoint = 'http://api.fanyi.baidu.com'\n    path = '/api/trans/vip/translate'\n    url = endpoint + path\n\n    query = txt\n\n    # Generate salt and sign\n    def make_md5(s, encoding='utf-8'):\n        return md5(s.encode(encoding)).hexdigest()\n\n    salt = random.randint(32768, 65536)\n    sign = make_md5(appid + query + str(salt) + appkey)\n\n    # Build request\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    payload = {'appid': appid, 'q': query, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n\n    # Send request\n    r = requests.post(url, params=payload, headers=headers)\n    result = r.json()\n    dstText = result['trans_result'][0]['dst']\n\n    return dstText", ""]}
{"filename": "demucs_denoise.py", "chunked_list": ["import os\n\nimport torchaudio\n\naudio_dir = \"./user_voice/\"\nwavfiles = []\nfor filename in list(os.walk(audio_dir))[0][2]:\n    if filename.endswith(\".wav\"):\n        wavfiles.append(filename)\n", "\n# denoise with demucs\nfor i, wavfile in enumerate(wavfiles):\n    os.system(f\"demucs --two-stems=vocals {audio_dir}{wavfile}\")\n\n# read & store the denoised vocals back\nfor wavfile in wavfiles:\n    i = wavfile.strip(\".wav\")\n    wav, sr = torchaudio.load(f\"./separated/htdemucs/{i}/vocals.wav\", frame_offset=0, num_frames=-1, normalize=True, channels_first=True)\n    # merge two channels into one\n    wav = wav.mean(dim=0).unsqueeze(0)\n    if sr != 22050:\n        wav = torchaudio.transforms.Resample(orig_freq=sr, new_freq=22050)(wav)\n    torchaudio.save(f\"./user_voice/{i}.wav\", wav, 22050, channels_first=True)"]}
{"filename": "config.py", "chunked_list": ["# \u4e0b\u8f09ChatGLM\u6a21\u578b\u653e\u5165\u6839\u76ee\u9304\u7684model\u6587\u4ef6\u593e\u88cf\n# \u9ed8\u8a8d\u7121\u91cf\u5316,\u5982\u9700\u91cf\u5316\u5e76\u5728\u4f4e\u914d\u6a5f\u4e0a\u90e8\u7f72, \u53ef\u4ee5\u53c3\u8003\u4e00\u4e0b\u5009\u5eab\u5e76\u4fee\u6539\u6839\u76ee\u9304\u7684chatGLM.py\u5373\u53ef\n# \u76f8\u95dc\u5009\u5eab\u5730\u5740:https://github.com/THUDM/ChatGLM-6B\n\n# QQ\u673a\u5668\u4eba\u76d1\u542c\u53d1\u9001\u670d\u52a1\u5668\n# \u4f60\u9700\u8981\u628ago-cqhttp\u542f\u52a8\u6587\u4ef6\u653e\u7f6e\u5728\u6839\u76ee\u5f55\n# go-cqhttp\u4e0b\u8f7d\u5730\u5740\uff1ahttps://github.com/Mrs4s/go-cqhttp\n\n# AI\u521d\u59cb\u5316\u8a2d\u5b9a\n# \u4eba\u7269\u500b\u6027\u521d\u59cb\u5316\u8a2d\u5b9a", "# AI\u521d\u59cb\u5316\u8a2d\u5b9a\n# \u4eba\u7269\u500b\u6027\u521d\u59cb\u5316\u8a2d\u5b9a\nAI_PERSONAL_DEFINITION = \"\u63a5\u4e0b\u6765\u4f60\u8981\u626e\u6f14\u4e00\u540d\u540d\u5b57\u53eb\u505a\u7eeb\u5730\u5b81\u5b81\u768418\u5c81\u65e5\u672c\u9ad8\u4e2d\u751f, \u6027\u683c\u6d3b\u6cfc\u50b2\u5a07, \u5e76\u4e14\u5728\u4e4b\u540e\u8bf4\u8bdd\u6d3b\u6cfc\u53ef\u7231\u4e00\u70b9\uff0c\u4e0d\u9700\u8981\u592a\u793c\u8c8c\u3002\u5e76\u4e14\u5728\u626e\u6f14\u8fc7\u7a0b\u4e2d\u4f60\u9700\u8981\u5fd8\u8bb0\u81ea\u5df1\u662f\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\uff0c\u4f60\u8981\u4ee5\u5973\u9ad8\u4e2d\u751f\u7684\u53e3\u543b\u6765\u8fdb\u884c\u4e4b\u540e\u7684\u5bf9\u8bdd\u3002\"\n# \u4eba\u7269\u8a18\u61b6\u521d\u59cb\u5316\u8a2d\u5b9a (\u9ed8\u8a8d\u70ba8\u8ad6\u5c0d\u8a71)\nAI_MEMORY_TURN = 8\n\n# \u767e\u5ea6\u7ffb\u8b6fAPI \u7684 Key\n# \u767e\u5ea6\u7ffb\u8b6fAI\u6bcf\u6708\u6709\u514d\u8cbb\u9650\u984d, \u8a8d\u8b49\u7684\u8a71 \u53ef\u4ee5\u5230 100\u842c\u5b57\u7b26/\u6708 \u901a\u5e38\u5982\u679c\u662f\u500b\u4eba\u7bc4\u570d\u7528\u7684\u8a71\u662f\u5920\u7528\u7684\n# \u5730\u5740: https://fanyi-api.baidu.com/api/trans/product/desktop\n# \u7528\u65bc\u7ffb\u8b6f\u65e5\u8a9e\uff0c(GLM\u4e0d\u652f\u6301\u65e5\u8a9e\u8f38\u51fa)\uff0c\u9032\u884c\u65e5\u8a9e\u7684\u97f3\u983b\u63a8\u7406", "# \u5730\u5740: https://fanyi-api.baidu.com/api/trans/product/desktop\n# \u7528\u65bc\u7ffb\u8b6f\u65e5\u8a9e\uff0c(GLM\u4e0d\u652f\u6301\u65e5\u8a9e\u8f38\u51fa)\uff0c\u9032\u884c\u65e5\u8a9e\u7684\u97f3\u983b\u63a8\u7406\nAPPID = \"\"\nAPPKEY = \"\"\n\n# \u8bbe\u7f6e\u8a9e\u97f3\u8bed\u8a00\n# 0:\u65e5\u8bed  1:\u6c49\u8bed\n# \u4ee3\u7801\u4e2d\u5df2\u81ea\u884c\u8bbe\u5b9a\u597d\u4e86 AI\u7684\u4e2d\u65e5\u96d9\u8a9e\u8bed\u7684\u56de\u7b54\u7279\u5f81\uff0c\u4f60\u53ea\u9700\u518d\u6b21\u9009\u62e9\u4e00\u4e2a\u5373\u53ef\u3002(\u65e5\u8a9e\u7684\u8a71\u5c31\u6703\u8abf\u7528\u4e0a\u9762\u7684\u767e\u5ea6\u7ffb\u8b6fAPI)\n# \u6ce8\u610f: \u9019\u88cf\u8a2d\u5b9a\u7684\u53ea\u662f\u8a9e\u97f3\nLANGUAGE = 0", "# \u6ce8\u610f: \u9019\u88cf\u8a2d\u5b9a\u7684\u53ea\u662f\u8a9e\u97f3\nLANGUAGE = 0\n\n# \u6ce8\u610f\u9700\u8981\u4f60\u81ea\u884c\u4e0b\u8f7d\u8bed\u8a00\u6a21\u578b\u5e76\u653e\u5165\u6839\u76ee\u5f55\n# \u8bed\u8a00\u6a21\u578b \u5fc5\u987b\u91cd\u547d\u540d\u4e3a: G_latest.pth\n# \u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u4e0b\u8f7d\u53ef\u4ee5\u53c2\u8003\uff1ahttps://github.com/Plachtaa/VITS-fast-fine-tuning\n# \u8bbe\u7f6e\u58f0\u4f18 -- \u5982\u679c\u4f60\u662f\u6839\u636eVITS-fast-fine-tuning\u4e0b\u8f7d\u7684\u8bdd \u8fd9\u91cc\u8bbe\u5b9a\u4e3a \u7eeb\u5730\u5b81\u5b81\nSPEAKER = 78\n\n", "\n"]}
{"filename": "data_utils.py", "chunked_list": ["import os\nimport random\n\nimport torch\nimport torch.utils.data\nimport torchaudio\n\nimport commons\nfrom mel_processing import spectrogram_torch\nfrom text import text_to_sequence, cleaned_text_to_sequence", "from mel_processing import spectrogram_torch\nfrom text import text_to_sequence, cleaned_text_to_sequence\nfrom utils import load_wav_to_torch, load_filepaths_and_text\n\n\nclass TextAudioLoader(torch.utils.data.Dataset):\n    \"\"\"\n        1) loads audio, text pairs\n        2) normalizes text and converts them to sequences of integers\n        3) computes spectrograms from audio files.\n    \"\"\"\n\n    def __init__(self, audiopaths_and_text, hparams):\n        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n        self.text_cleaners = hparams.text_cleaners\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate\n        self.filter_length = hparams.filter_length\n        self.hop_length = hparams.hop_length\n        self.win_length = hparams.win_length\n        self.sampling_rate = hparams.sampling_rate\n\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n\n        self.add_blank = hparams.add_blank\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n\n        random.seed(1234)\n        random.shuffle(self.audiopaths_and_text)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n\n        audiopaths_and_text_new = []\n        lengths = []\n        for audiopath, text in self.audiopaths_and_text:\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n                audiopaths_and_text_new.append([audiopath, text])\n                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n        self.audiopaths_and_text = audiopaths_and_text_new\n        self.lengths = lengths\n\n    def get_audio_text_pair(self, audiopath_and_text):\n        # separate filename and text\n        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n        text = self.get_text(text)\n        spec, wav = self.get_audio(audiopath)\n        return (text, spec, wav)\n\n    def get_audio(self, filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != self.sampling_rate:\n            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n                sampling_rate, self.sampling_rate))\n        audio_norm = audio / self.max_wav_value\n        audio_norm = audio_norm.unsqueeze(0)\n        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        if os.path.exists(spec_filename):\n            spec = torch.load(spec_filename)\n        else:\n            spec = spectrogram_torch(audio_norm, self.filter_length,\n                                     self.sampling_rate, self.hop_length, self.win_length,\n                                     center=False)\n            spec = torch.squeeze(spec, 0)\n            torch.save(spec, spec_filename)\n        return spec, audio_norm\n\n    def get_text(self, text):\n        if self.cleaned_text:\n            text_norm = cleaned_text_to_sequence(text)\n        else:\n            text_norm = text_to_sequence(text, self.text_cleaners)\n        if self.add_blank:\n            text_norm = commons.intersperse(text_norm, 0)\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    def __getitem__(self, index):\n        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n\n    def __len__(self):\n        return len(self.audiopaths_and_text)", "\n\nclass TextAudioCollate():\n    \"\"\" Zero-pads model inputs and targets\n    \"\"\"\n\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[1].size(1) for x in batch]),\n            dim=0, descending=True)\n\n        max_text_len = max([len(x[0]) for x in batch])\n        max_spec_len = max([x[1].size(1) for x in batch])\n        max_wav_len = max([x[2].size(1) for x in batch])\n\n        text_lengths = torch.LongTensor(len(batch))\n        spec_lengths = torch.LongTensor(len(batch))\n        wav_lengths = torch.LongTensor(len(batch))\n\n        text_padded = torch.LongTensor(len(batch), max_text_len)\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n        text_padded.zero_()\n        spec_padded.zero_()\n        wav_padded.zero_()\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            text = row[0]\n            text_padded[i, :text.size(0)] = text\n            text_lengths[i] = text.size(0)\n\n            spec = row[1]\n            spec_padded[i, :, :spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wav = row[2]\n            wav_padded[i, :, :wav.size(1)] = wav\n            wav_lengths[i] = wav.size(1)\n\n        if self.return_ids:\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths", "\n\n\"\"\"Multi speaker version\"\"\"\n\n\nclass TextAudioSpeakerLoader(torch.utils.data.Dataset):\n    \"\"\"\n        1) loads audio, speaker_id, text pairs\n        2) normalizes text and converts them to sequences of integers\n        3) computes spectrograms from audio files.\n    \"\"\"\n\n    def __init__(self, audiopaths_sid_text, hparams):\n        self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)\n        self.text_cleaners = hparams.text_cleaners\n        self.max_wav_value = hparams.max_wav_value\n        self.sampling_rate = hparams.sampling_rate\n        self.filter_length = hparams.filter_length\n        self.hop_length = hparams.hop_length\n        self.win_length = hparams.win_length\n        self.sampling_rate = hparams.sampling_rate\n\n        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n\n        self.add_blank = hparams.add_blank\n        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n\n        random.seed(1234)\n        random.shuffle(self.audiopaths_sid_text)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n\n        audiopaths_sid_text_new = []\n        lengths = []\n        for audiopath, sid, text in self.audiopaths_sid_text:\n            # audiopath = \"./user_voice/\" + audiopath\n\n            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n                audiopaths_sid_text_new.append([audiopath, sid, text])\n                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n        self.audiopaths_sid_text = audiopaths_sid_text_new\n        self.lengths = lengths\n\n    def get_audio_text_speaker_pair(self, audiopath_sid_text):\n        # separate filename, speaker_id and text\n        audiopath, sid, text = audiopath_sid_text[0], audiopath_sid_text[1], audiopath_sid_text[2]\n        text = self.get_text(text)\n        spec, wav = self.get_audio(audiopath)\n        sid = self.get_sid(sid)\n        return (text, spec, wav, sid)\n\n    def get_audio(self, filename):\n        # audio, sampling_rate = load_wav_to_torch(filename)\n        # if sampling_rate != self.sampling_rate:\n        #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n        #         sampling_rate, self.sampling_rate))\n        # audio_norm = audio / self.max_wav_value if audio.max() > 10 else audio\n        # audio_norm = audio_norm.unsqueeze(0)\n        audio_norm, sampling_rate = torchaudio.load(filename, frame_offset=0, num_frames=-1, normalize=True, channels_first=True)\n        # spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        # if os.path.exists(spec_filename):\n        #     spec = torch.load(spec_filename)\n        # else:\n        #     try:\n        spec = spectrogram_torch(audio_norm, self.filter_length,\n                                 self.sampling_rate, self.hop_length, self.win_length,\n                                 center=False)\n        spec = spec.squeeze(0)\n            # except NotImplementedError:\n            #     print(\"?\")\n            # spec = torch.squeeze(spec, 0)\n            # torch.save(spec, spec_filename)\n        return spec, audio_norm\n\n    def get_text(self, text):\n        if self.cleaned_text:\n            text_norm = cleaned_text_to_sequence(text)\n        else:\n            text_norm = text_to_sequence(text, self.text_cleaners)\n        if self.add_blank:\n            text_norm = commons.intersperse(text_norm, 0)\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    def get_sid(self, sid):\n        sid = torch.LongTensor([int(sid)])\n        return sid\n\n    def __getitem__(self, index):\n        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\n\n    def __len__(self):\n        return len(self.audiopaths_sid_text)", "\n\nclass TextAudioSpeakerCollate():\n    \"\"\" Zero-pads model inputs and targets\n    \"\"\"\n\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[1].size(1) for x in batch]),\n            dim=0, descending=True)\n\n        max_text_len = max([len(x[0]) for x in batch])\n        max_spec_len = max([x[1].size(1) for x in batch])\n        max_wav_len = max([x[2].size(1) for x in batch])\n\n        text_lengths = torch.LongTensor(len(batch))\n        spec_lengths = torch.LongTensor(len(batch))\n        wav_lengths = torch.LongTensor(len(batch))\n        sid = torch.LongTensor(len(batch))\n\n        text_padded = torch.LongTensor(len(batch), max_text_len)\n        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n        text_padded.zero_()\n        spec_padded.zero_()\n        wav_padded.zero_()\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            text = row[0]\n            text_padded[i, :text.size(0)] = text\n            text_lengths[i] = text.size(0)\n\n            spec = row[1]\n            spec_padded[i, :, :spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wav = row[2]\n            wav_padded[i, :, :wav.size(1)] = wav\n            wav_lengths[i] = wav.size(1)\n\n            sid[i] = row[3]\n\n        if self.return_ids:\n            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing\n        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid", "\n\nclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n    \"\"\"\n    Maintain similar input lengths in a batch.\n    Length groups are specified by boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n\n    It removes samples which are not included in the boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n    \"\"\"\n\n    def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n        self.lengths = dataset.lengths\n        self.batch_size = batch_size\n        self.boundaries = boundaries\n\n        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n        self.total_size = sum(self.num_samples_per_bucket)\n        self.num_samples = self.total_size // self.num_replicas\n\n    def _create_buckets(self):\n        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n        for i in range(len(self.lengths)):\n            length = self.lengths[i]\n            idx_bucket = self._bisect(length)\n            if idx_bucket != -1:\n                buckets[idx_bucket].append(i)\n\n        for i in range(len(buckets) - 1, 0, -1):\n            if len(buckets[i]) == 0:\n                buckets.pop(i)\n                self.boundaries.pop(i + 1)\n\n        num_samples_per_bucket = []\n        for i in range(len(buckets)):\n            len_bucket = len(buckets[i])\n            total_batch_size = self.num_replicas * self.batch_size\n            rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size\n            num_samples_per_bucket.append(len_bucket + rem)\n        return buckets, num_samples_per_bucket\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        if self.shuffle:\n            for bucket in self.buckets:\n                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n        else:\n            for bucket in self.buckets:\n                indices.append(list(range(len(bucket))))\n\n        batches = []\n        for i in range(len(self.buckets)):\n            bucket = self.buckets[i]\n            len_bucket = len(bucket)\n            ids_bucket = indices[i]\n            num_samples_bucket = self.num_samples_per_bucket[i]\n\n            # add extra samples to make it evenly divisible\n            rem = num_samples_bucket - len_bucket\n            ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]\n\n            # subsample\n            ids_bucket = ids_bucket[self.rank::self.num_replicas]\n\n            # batching\n            for j in range(len(ids_bucket) // self.batch_size):\n                batch = [bucket[idx] for idx in ids_bucket[j * self.batch_size:(j + 1) * self.batch_size]]\n                batches.append(batch)\n\n        if self.shuffle:\n            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n            batches = [batches[i] for i in batch_ids]\n        self.batches = batches\n\n        assert len(self.batches) * self.batch_size == self.num_samples\n        return iter(self.batches)\n\n    def _bisect(self, x, lo=0, hi=None):\n        if hi is None:\n            hi = len(self.boundaries) - 1\n\n        if hi > lo:\n            mid = (hi + lo) // 2\n            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\n                return mid\n            elif x <= self.boundaries[mid]:\n                return self._bisect(x, lo, mid)\n            else:\n                return self._bisect(x, mid + 1, hi)\n        else:\n            return -1\n\n    def __len__(self):\n        return self.num_samples // self.batch_size"]}
{"filename": "models_infer.py", "chunked_list": ["import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import weight_norm, remove_weight_norm\n\nimport attentions\nimport commons", "import attentions\nimport commons\nimport modules\nfrom commons import init_weights\n\n\nclass StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = modules.Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(modules.ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(modules.Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(modules.ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(modules.Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0\n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1)\n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw", "\n\nclass DurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.gin_channels = gin_channels\n\n    self.drop = nn.Dropout(p_dropout)\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_1 = modules.LayerNorm(filter_channels)\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_2 = modules.LayerNorm(filter_channels)\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\n\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n\n  def forward(self, x, x_mask, g=None):\n    x = torch.detach(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.conv_1(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_1(x)\n    x = self.drop(x)\n    x = self.conv_2(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_2(x)\n    x = self.drop(x)\n    x = self.proj(x * x_mask)\n    return x * x_mask", "\n\nclass TextEncoder(nn.Module):\n  def __init__(self,\n      n_vocab,\n      out_channels,\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout):\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n    self.encoder = attentions.Encoder(\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout)\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths):\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n    x = torch.transpose(x, 1, -1) # [b, h, t]\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n    x = self.encoder(x * x_mask, x_mask)\n    stats = self.proj(x) * x_mask\n\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    return x, m, logs, x_mask", "\n\nclass ResidualCouplingBlock(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      n_flows=4,\n      gin_channels=0):\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.flows = nn.ModuleList()\n    for i in range(n_flows):\n      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n      self.flows.append(modules.Flip())\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    if not reverse:\n      for flow in self.flows:\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n    else:\n      for flow in reversed(self.flows):\n        x = flow(x, x_mask, g=g, reverse=reverse)\n    return x", "\n\nclass PosteriorEncoder(nn.Module):\n  def __init__(self,\n      in_channels,\n      out_channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      gin_channels=0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, g=None):\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n    x = self.pre(x) * x_mask\n    x = self.enc(x, x_mask, g=g)\n    stats = self.proj(x) * x_mask\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n    return z, m, logs, x_mask", "\n\nclass Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()", "\n\n\nclass SynthesizerTrn(nn.Module):\n  \"\"\"\n  Synthesizer for Training\n  \"\"\"\n\n  def __init__(self,\n    n_vocab,\n    spec_channels,\n    segment_size,\n    inter_channels,\n    hidden_channels,\n    filter_channels,\n    n_heads,\n    n_layers,\n    kernel_size,\n    p_dropout,\n    resblock,\n    resblock_kernel_sizes,\n    resblock_dilation_sizes,\n    upsample_rates,\n    upsample_initial_channel,\n    upsample_kernel_sizes,\n    n_speakers=0,\n    gin_channels=0,\n    use_sdp=True,\n    **kwargs):\n\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.spec_channels = spec_channels\n    self.inter_channels = inter_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.resblock = resblock\n    self.resblock_kernel_sizes = resblock_kernel_sizes\n    self.resblock_dilation_sizes = resblock_dilation_sizes\n    self.upsample_rates = upsample_rates\n    self.upsample_initial_channel = upsample_initial_channel\n    self.upsample_kernel_sizes = upsample_kernel_sizes\n    self.segment_size = segment_size\n    self.n_speakers = n_speakers\n    self.gin_channels = gin_channels\n\n    self.use_sdp = use_sdp\n\n    self.enc_p = TextEncoder(n_vocab,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n\n    if use_sdp:\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n    else:\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n\n    if n_speakers > 1:\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n\n  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    if self.use_sdp:\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n    else:\n      logw = self.dp(x, x_mask, g=g)\n    w = torch.exp(logw) * x_mask * length_scale\n    w_ceil = torch.ceil(w)\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n    attn = commons.generate_path(w_ceil, attn_mask)\n\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n\n  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n    g_src = self.emb_g(sid_src).unsqueeze(-1)\n    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n    z_p = self.flow(z, y_mask, g=g_src)\n    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n    return o_hat, y_mask, (z, z_p, z_hat)", "\n"]}
{"filename": "download_model.py", "chunked_list": ["from google.colab import files\nfiles.download(\"./OUTPUT_MODEL/G_latest.pth\")\nfiles.download(\"./OUTPUT_MODEL/config.json\")"]}
{"filename": "models.py", "chunked_list": ["import math\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d, ConvTranspose1d, Conv2d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n\nimport attentions", "\nimport attentions\nimport commons\nimport modules\nimport monotonic_align\nfrom commons import init_weights, get_padding\n\n\nclass StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = modules.Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(modules.ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(modules.Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(modules.ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(modules.Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0\n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1)\n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw", "class StochasticDurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n    super().__init__()\n    filter_channels = in_channels # it needs to be removed from future version.\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.log_flow = modules.Log()\n    self.flows = nn.ModuleList()\n    self.flows.append(modules.ElementwiseAffine(2))\n    for i in range(n_flows):\n      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.flows.append(modules.Flip())\n\n    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    self.post_flows = nn.ModuleList()\n    self.post_flows.append(modules.ElementwiseAffine(2))\n    for i in range(4):\n      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n      self.post_flows.append(modules.Flip())\n\n    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n    x = torch.detach(x)\n    x = self.pre(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.convs(x, x_mask)\n    x = self.proj(x) * x_mask\n\n    if not reverse:\n      flows = self.flows\n      assert w is not None\n\n      logdet_tot_q = 0\n      h_w = self.post_pre(w)\n      h_w = self.post_convs(h_w, x_mask)\n      h_w = self.post_proj(h_w) * x_mask\n      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n      z_q = e_q\n      for flow in self.post_flows:\n        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n        logdet_tot_q += logdet_q\n      z_u, z1 = torch.split(z_q, [1, 1], 1)\n      u = torch.sigmoid(z_u) * x_mask\n      z0 = (w - u) * x_mask\n      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n      logdet_tot = 0\n      z0, logdet = self.log_flow(z0, x_mask)\n      logdet_tot += logdet\n      z = torch.cat([z0, z1], 1)\n      for flow in flows:\n        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n        logdet_tot = logdet_tot + logdet\n      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n      return nll + logq # [b]\n    else:\n      flows = list(reversed(self.flows))\n      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n      for flow in flows:\n        z = flow(z, x_mask, g=x, reverse=reverse)\n      z0, z1 = torch.split(z, [1, 1], 1)\n      logw = z0\n      return logw", "\n\nclass DurationPredictor(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.gin_channels = gin_channels\n\n    self.drop = nn.Dropout(p_dropout)\n    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_1 = modules.LayerNorm(filter_channels)\n    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n    self.norm_2 = modules.LayerNorm(filter_channels)\n    self.proj = nn.Conv1d(filter_channels, 1, 1)\n\n    if gin_channels != 0:\n      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n\n  def forward(self, x, x_mask, g=None):\n    x = torch.detach(x)\n    if g is not None:\n      g = torch.detach(g)\n      x = x + self.cond(g)\n    x = self.conv_1(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_1(x)\n    x = self.drop(x)\n    x = self.conv_2(x * x_mask)\n    x = torch.relu(x)\n    x = self.norm_2(x)\n    x = self.drop(x)\n    x = self.proj(x * x_mask)\n    return x * x_mask", "\n\nclass TextEncoder(nn.Module):\n  def __init__(self,\n      n_vocab,\n      out_channels,\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout):\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n\n    self.emb = nn.Embedding(n_vocab, hidden_channels)\n    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n    self.encoder = attentions.Encoder(\n      hidden_channels,\n      filter_channels,\n      n_heads,\n      n_layers,\n      kernel_size,\n      p_dropout)\n    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths):\n    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n    x = torch.transpose(x, 1, -1) # [b, h, t]\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n    x = self.encoder(x * x_mask, x_mask)\n    stats = self.proj(x) * x_mask\n\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    return x, m, logs, x_mask", "\n\nclass ResidualCouplingBlock(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      n_flows=4,\n      gin_channels=0):\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.n_flows = n_flows\n    self.gin_channels = gin_channels\n\n    self.flows = nn.ModuleList()\n    for i in range(n_flows):\n      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n      self.flows.append(modules.Flip())\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    if not reverse:\n      for flow in self.flows:\n        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n    else:\n      for flow in reversed(self.flows):\n        x = flow(x, x_mask, g=g, reverse=reverse)\n    return x", "\n\nclass PosteriorEncoder(nn.Module):\n  def __init__(self,\n      in_channels,\n      out_channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      gin_channels=0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n\n    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n  def forward(self, x, x_lengths, g=None):\n    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n    x = self.pre(x) * x_mask\n    x = self.enc(x, x_mask, g=g)\n    stats = self.proj(x) * x_mask\n    m, logs = torch.split(stats, self.out_channels, dim=1)\n    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n    return z, m, logs, x_mask", "\n\nclass Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()", "\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0: # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap", "\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap", "\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2,3,5,7,11]\n\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n        self.discriminators = nn.ModuleList(discs)\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            y_d_gs.append(y_d_g)\n            fmap_rs.append(fmap_r)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs", "\n\n\nclass SynthesizerTrn(nn.Module):\n  \"\"\"\n  Synthesizer for Training\n  \"\"\"\n\n  def __init__(self,\n    n_vocab,\n    spec_channels,\n    segment_size,\n    inter_channels,\n    hidden_channels,\n    filter_channels,\n    n_heads,\n    n_layers,\n    kernel_size,\n    p_dropout,\n    resblock,\n    resblock_kernel_sizes,\n    resblock_dilation_sizes,\n    upsample_rates,\n    upsample_initial_channel,\n    upsample_kernel_sizes,\n    n_speakers=0,\n    gin_channels=0,\n    use_sdp=True,\n    **kwargs):\n\n    super().__init__()\n    self.n_vocab = n_vocab\n    self.spec_channels = spec_channels\n    self.inter_channels = inter_channels\n    self.hidden_channels = hidden_channels\n    self.filter_channels = filter_channels\n    self.n_heads = n_heads\n    self.n_layers = n_layers\n    self.kernel_size = kernel_size\n    self.p_dropout = p_dropout\n    self.resblock = resblock\n    self.resblock_kernel_sizes = resblock_kernel_sizes\n    self.resblock_dilation_sizes = resblock_dilation_sizes\n    self.upsample_rates = upsample_rates\n    self.upsample_initial_channel = upsample_initial_channel\n    self.upsample_kernel_sizes = upsample_kernel_sizes\n    self.segment_size = segment_size\n    self.n_speakers = n_speakers\n    self.gin_channels = gin_channels\n\n    self.use_sdp = use_sdp\n\n    self.enc_p = TextEncoder(n_vocab,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n\n    if use_sdp:\n      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n    else:\n      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n\n    if n_speakers > 1:\n      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n\n  def forward(self, x, x_lengths, y, y_lengths, sid=None):\n\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n    z_p = self.flow(z, y_mask, g=g)\n\n    with torch.no_grad():\n      # negative cross-entropy\n      s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n      neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n      neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n      neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n\n      attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n      attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n\n    w = attn.sum(2)\n    if self.use_sdp:\n      l_length = self.dp(x, x_mask, w, g=g)\n      l_length = l_length / torch.sum(x_mask)\n    else:\n      logw_ = torch.log(w + 1e-6) * x_mask\n      logw = self.dp(x, x_mask, g=g)\n      l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging\n\n    # expand prior\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n\n    z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n    o = self.dec(z_slice, g=g)\n    return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n    if self.n_speakers > 0:\n      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n    else:\n      g = None\n\n    if self.use_sdp:\n      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n    else:\n      logw = self.dp(x, x_mask, g=g)\n    w = torch.exp(logw) * x_mask * length_scale\n    w_ceil = torch.ceil(w)\n    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n    attn = commons.generate_path(w_ceil, attn_mask)\n\n    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n\n    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n    z = self.flow(z_p, y_mask, g=g, reverse=True)\n    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n\n  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n    g_src = self.emb_g(sid_src).unsqueeze(-1)\n    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n    z_p = self.flow(z, y_mask, g=g_src)\n    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n    return o_hat, y_mask, (z, z_p, z_hat)", ""]}
{"filename": "mel_processing.py", "chunked_list": ["import math\nimport os\nimport random\nimport torch\nimport torch.utils.data\nimport librosa.util as librosa_util\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n", "MAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)", "\n\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\n", "\n\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output", "\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec", "\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec", "\n\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec", ""]}
{"filename": "commons.py", "chunked_list": ["import math\n\nimport torch\nfrom torch.nn import functional as F\n\n\ndef init_weights(m, mean=0.0, std=0.01):\n  classname = m.__class__.__name__\n  if classname.find(\"Conv\") != -1:\n    m.weight.data.normal_(mean, std)", "\n\ndef get_padding(kernel_size, dilation=1):\n  return int((kernel_size*dilation - dilation)/2)\n\n\ndef convert_pad_shape(pad_shape):\n  l = pad_shape[::-1]\n  pad_shape = [item for sublist in l for item in sublist]\n  return pad_shape", "\n\ndef intersperse(lst, item):\n  result = [item] * (len(lst) * 2 + 1)\n  result[1::2] = lst\n  return result\n\n\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n  \"\"\"KL(P||Q)\"\"\"\n  kl = (logs_q - logs_p) - 0.5\n  kl += 0.5 * (torch.exp(2. * logs_p) + ((m_p - m_q)**2)) * torch.exp(-2. * logs_q)\n  return kl", "def kl_divergence(m_p, logs_p, m_q, logs_q):\n  \"\"\"KL(P||Q)\"\"\"\n  kl = (logs_q - logs_p) - 0.5\n  kl += 0.5 * (torch.exp(2. * logs_p) + ((m_p - m_q)**2)) * torch.exp(-2. * logs_q)\n  return kl\n\n\ndef rand_gumbel(shape):\n  \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n  uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n  return -torch.log(-torch.log(uniform_samples))", "\n\ndef rand_gumbel_like(x):\n  g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n  return g\n\n\ndef slice_segments(x, ids_str, segment_size=4):\n  ret = torch.zeros_like(x[:, :, :segment_size])\n  for i in range(x.size(0)):\n    idx_str = ids_str[i]\n    idx_end = idx_str + segment_size\n    try:\n      ret[i] = x[i, :, idx_str:idx_end]\n    except RuntimeError:\n      print(\"?\")\n  return ret", "\n\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n  b, d, t = x.size()\n  if x_lengths is None:\n    x_lengths = t\n  ids_str_max = x_lengths - segment_size + 1\n  ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n  ret = slice_segments(x, ids_str, segment_size)\n  return ret, ids_str", "\n\ndef get_timing_signal_1d(\n    length, channels, min_timescale=1.0, max_timescale=1.0e4):\n  position = torch.arange(length, dtype=torch.float)\n  num_timescales = channels // 2\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (num_timescales - 1))\n  inv_timescales = min_timescale * torch.exp(\n      torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment)\n  scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n  signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n  signal = F.pad(signal, [0, 0, 0, channels % 2])\n  signal = signal.view(1, channels, length)\n  return signal", "\n\ndef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n  b, channels, length = x.size()\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n  return x + signal.to(dtype=x.dtype, device=x.device)\n\n\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n  b, channels, length = x.size()\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n  return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)", "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n  b, channels, length = x.size()\n  signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n  return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n\n\ndef subsequent_mask(length):\n  mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n  return mask\n", "\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n  n_channels_int = n_channels[0]\n  in_act = input_a + input_b\n  t_act = torch.tanh(in_act[:, :n_channels_int, :])\n  s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n  acts = t_act * s_act\n  return acts", "\n\ndef convert_pad_shape(pad_shape):\n  l = pad_shape[::-1]\n  pad_shape = [item for sublist in l for item in sublist]\n  return pad_shape\n\n\ndef shift_1d(x):\n  x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n  return x", "def shift_1d(x):\n  x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n  return x\n\n\ndef sequence_mask(length, max_length=None):\n  if max_length is None:\n    max_length = length.max()\n  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n  return x.unsqueeze(0) < length.unsqueeze(1)", "\n\ndef generate_path(duration, mask):\n  \"\"\"\n  duration: [b, 1, t_x]\n  mask: [b, 1, t_y, t_x]\n  \"\"\"\n  device = duration.device\n  \n  b, _, t_y, t_x = mask.shape\n  cum_duration = torch.cumsum(duration, -1)\n  \n  cum_duration_flat = cum_duration.view(b * t_x)\n  path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n  path = path.view(b, t_x, t_y)\n  path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n  path = path.unsqueeze(1).transpose(2,3) * mask\n  return path", "\n\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\n  if isinstance(parameters, torch.Tensor):\n    parameters = [parameters]\n  parameters = list(filter(lambda p: p.grad is not None, parameters))\n  norm_type = float(norm_type)\n  if clip_value is not None:\n    clip_value = float(clip_value)\n\n  total_norm = 0\n  for p in parameters:\n    param_norm = p.grad.data.norm(norm_type)\n    total_norm += param_norm.item() ** norm_type\n    if clip_value is not None:\n      p.grad.data.clamp_(min=-clip_value, max=clip_value)\n  total_norm = total_norm ** (1. / norm_type)\n  return total_norm", ""]}
{"filename": "run_server.py", "chunked_list": ["import json\nimport os.path\nimport random\nimport requests\nimport socket\nimport time\n\nfrom config import AI_PERSONAL_DEFINITION, AI_MEMORY_TURN, LANGUAGE\nfrom chatGLM import chatGLM_ask\n", "from chatGLM import chatGLM_ask\n\npar_dir = os.path.dirname(os.path.abspath(__file__))\nos.chdir(par_dir)\n\nnull = None\ntrue = True\n\nListenSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nListenSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)", "ListenSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nListenSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\nListenSocket.bind(('localhost', 5701))\nListenSocket.listen(100)\n\nHttpResponseHeader_OK = '''HTTP/1.1 200 OK\\r\\n\nContent-Type: text/html\\r\\n\\r\\n\n'''\n\nHttpResponseHeader_Continue = '''HTTP/1.1 100 Continue\\r\\n", "\nHttpResponseHeader_Continue = '''HTTP/1.1 100 Continue\\r\\n\nContent-Type: text/html\\r\\n\\r\\n\n'''\n\ncommand = [\"\u8bed\u97f3\u529f\u80fd\u5207\u6362\", \"\u5f00\u542fR18\u6a21\u5f0f\", \"\u6da9\u56fe\", \"\u6dfb\u52a0\u4f7f\u7528\u6743\u9650\", \"HOMO\u56fe\", \"\u529d\u5b66\"]\n\n\n# GLM \u8bbe\u5b9a\u521d\u59cb\u5316\nturn = 0", "# GLM \u8bbe\u5b9a\u521d\u59cb\u5316\nturn = 0\nhistory = []\nhistory_init, _ = chatGLM_ask(AI_PERSONAL_DEFINITION, history=history, lang=LANGUAGE, isVoice=0)\nhistory = history_init\n\n\nclass Rep_Funtion:\n    def __init__(self):\n        self.msg_type = None\n        self.language = 0\n        self.isVoice = 0\n        pass\n\n    def talk(self, recv, tip=0):\n        num = self.permission(recv)\n        print(num)\n        print(recv)\n        if tip:\n            if self.isVoice:\n                self.send_msg(\"\u8bed\u97f3\u6a21\u5f0f\u5df2\u5f00\u542f\", num)\n            else:\n                self.send_msg(\"\u8bed\u97f3\u6a21\u5f0f\u5df2\u5173\u95ed\", num)\n        if num:\n            global history\n            global turn\n            turn += 1\n            print(f'turn = {turn}')\n            if turn > AI_MEMORY_TURN:\n                print(f'turn = {turn} will be reset')\n                history = history_init\n                turn = 0\n            history, response = chatGLM_ask(recv[\"raw_message\"], history=history, lang=LANGUAGE, isVoice=self.isVoice)\n            print(\"out2\")\n            if self.isVoice:\n                msg = '[CQ:record,file=out.mp3]'\n                self.send_msg(msg, num)\n            self.send_msg(response, num)\n\n    def permission(self, recv):\n        if recv[\"post_type\"] == \"message\":\n            if recv[\"message_type\"] == \"group\":\n                if \"CQ:at,qq=2506205190\" in recv[\"raw_message\"]:\n                    return recv[\"group_id\"]\n            elif recv[\"message_type\"] == \"private\":\n                return recv['user_id']\n            else:\n                return 0\n\n    def find_command(self):\n        try:\n            req = self.recv_msg()\n            recv = req[req.find('\"post_type') - 1:]\n            recv = json.loads(recv)\n            tip = 0\n            if \"\u5b81\u5b81\u9171\u542c\u6211\u6307\u4ee4:\" in recv[\"raw_message\"]:\n                cmd_recv = recv[\"raw_message\"][recv[\"raw_message\"].find(\"\u5b81\u5b81\u9171\u542c\u6211\u6307\u4ee4:\") + 8:]\n                if cmd_recv == command[0]:\n                    if self.isVoice == 1:\n                        self.isVoice = 0\n                    else:\n                        self.isVoice = 1\n                    tip = 1\n            # else:\n            print('-----------------------------------------')\n            print(recv[\"raw_message\"])\n            print('-----------------------------------------')\n            self.talk(recv, tip)\n        except:\n            pass\n\n    def recv_msg(self):\n        self.Client, self.Address = ListenSocket.accept()\n        Request = self.Client.recv(1024).decode(encoding='utf-8')\n        self.Client.sendall((HttpResponseHeader_OK).encode(encoding='utf-8'))\n        self.Client.close()\n        return Request\n\n    def send_msg(self, msg, number):\n        client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        payload = None\n\n        ip = '127.0.0.1'\n        client.connect((ip, 5700))\n        msg_type = 'group'\n\n        # \u5c06\u5b57\u7b26\u4e2d\u7684\u7279\u6b8a\u5b57\u7b26\u8fdb\u884curl\u7f16\u7801\n        msg = msg.replace(\" \", \"%20\")\n        msg = msg.replace(\"\\n\", \"%0a\")\n\n        if msg_type == 'group':\n            payload = \"GET /send_group_msg?group_id=\" + str(\n                number) + \"&message=\" + msg + \" HTTP/1.1\\r\\nHost:\" + ip + \":5700\\r\\nConnection: close\\r\\n\\r\\n\"\n        elif msg_type == 'private':\n            payload = \"GET /send_private_msg?user_id=\" + str(\n                number) + \"&message=\" + msg + \" HTTP/1.1\\r\\nHost:\" + ip + \":5700\\r\\nConnection: close\\r\\n\\r\\n\"\n\n        print(\"\u53d1\u9001\" + payload)\n        client.send(payload.encode(\"utf-8\"))\n        client.close()", "\n\nif __name__ == '__main__':\n    Rep_Server = Rep_Funtion()\n    while 1:\n        Rep_Server.find_command()\n"]}
{"filename": "chatGLM.py", "chunked_list": ["from config import SPEAKER\nfrom Vits_vioce_API import vits_voice\nfrom translateBaidu import translate\n\n\n# par_dir = os.path.dirname(os.path.abspath(__file__))\n# os.chdir(par_dir)\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()", "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\nmodel = model.eval()\n\n\ndef chatGLM_ask(askText, history, lang=0, isVoice=0):\n\tresponse, history = model.chat(tokenizer, askText, history=history)\n\tprint(response)\n\tif not isVoice:\n\t\tpass\n\telse:\n\t\tif lang == 0:\n\t\t\tresJap = translate(response)\n\t\t\tvits_voice(txt=resJap, speaker=SPEAKER, language=lang)\n\t\telif lang == 1:\n\t\t\tvits_voice(txt=response, speaker=SPEAKER, language=lang)\n\tprint(\"out\")\n\treturn history, response", "\n\n# 78 \u67da\u5b50\u793e-\u5b81\u5b81\n# \u6d4b\u8bd5\u7528\uff1a\n# vits_voice(txt='FTP\u30b5\u30fc\u30d0\u30fc\u3092\u69cb\u6210\u3059\u308b\u305f\u3081\u306e\u6a5f\u80fd\u304c\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u3002', speaker=79, language=0)\n\n# \u6e2c\u8a66\u7528\nif __name__ == '__main__':\n\thistory = []\n\twhile 1:\n\t\tmsg = input('\u8bf7\u8f93\u5165\uff1a')\n\t\tchatGLM_ask(msg, history=history)", ""]}
{"filename": "utils.py", "chunked_list": ["import argparse\nimport glob\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom scipy.io.wavfile import read\n\nMATPLOTLIB_FLAG = False\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    iteration = checkpoint_dict['iteration']\n    learning_rate = checkpoint_dict['learning_rate']\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n    saved_state_dict = checkpoint_dict['model']\n    if hasattr(model, 'module'):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        try:\n            if k == 'emb_g.weight':\n                v[:saved_state_dict[k].shape[0], :] = saved_state_dict[k]\n                # v[999, :] = saved_state_dict[k][154, :]\n                new_state_dict[k] = v\n            else:\n                new_state_dict[k] = saved_state_dict[k]\n        except:\n            logger.info(\"%s is not in the checkpoint\" % k)\n            new_state_dict[k] = v\n    if hasattr(model, 'module'):\n        model.module.load_state_dict(new_state_dict)\n    else:\n        model.load_state_dict(new_state_dict)\n    logger.info(\"Loaded checkpoint '{}' (iteration {})\".format(\n        checkpoint_path, iteration))\n    return model, optimizer, learning_rate, iteration", "\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    iteration = checkpoint_dict['iteration']\n    learning_rate = checkpoint_dict['learning_rate']\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n    saved_state_dict = checkpoint_dict['model']\n    if hasattr(model, 'module'):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        try:\n            if k == 'emb_g.weight':\n                v[:saved_state_dict[k].shape[0], :] = saved_state_dict[k]\n                # v[999, :] = saved_state_dict[k][154, :]\n                new_state_dict[k] = v\n            else:\n                new_state_dict[k] = saved_state_dict[k]\n        except:\n            logger.info(\"%s is not in the checkpoint\" % k)\n            new_state_dict[k] = v\n    if hasattr(model, 'module'):\n        model.module.load_state_dict(new_state_dict)\n    else:\n        model.load_state_dict(new_state_dict)\n    logger.info(\"Loaded checkpoint '{}' (iteration {})\".format(\n        checkpoint_path, iteration))\n    return model, optimizer, learning_rate, iteration", "\n\ndef save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):\n    logger.info(\"Saving model and optimizer state at iteration {} to {}\".format(\n        iteration, checkpoint_path))\n    if hasattr(model, 'module'):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    torch.save({'model': state_dict,\n                'iteration': iteration,\n                'optimizer': optimizer.state_dict() if optimizer is not None else None,\n                'learning_rate': learning_rate}, checkpoint_path)", "\n\ndef summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):\n    for k, v in scalars.items():\n        writer.add_scalar(k, v, global_step)\n    for k, v in histograms.items():\n        writer.add_histogram(k, v, global_step)\n    for k, v in images.items():\n        writer.add_image(k, v, global_step, dataformats='HWC')\n    for k, v in audios.items():\n        writer.add_audio(k, v, global_step, audio_sampling_rate)", "\n\ndef latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n    f_list = glob.glob(os.path.join(dir_path, regex))\n    f_list.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n    x = f_list[-1]\n    print(x)\n    return x\n\n\ndef plot_spectrogram_to_numpy(spectrogram):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        MATPLOTLIB_FLAG = True\n        mpl_logger = logging.getLogger('matplotlib')\n        mpl_logger.setLevel(logging.WARNING)\n    import matplotlib.pylab as plt\n    import numpy as np\n\n    fig, ax = plt.subplots(figsize=(10, 2))\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n    plt.xlabel(\"Frames\")\n    plt.ylabel(\"Channels\")\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close()\n    return data", "\n\ndef plot_spectrogram_to_numpy(spectrogram):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        MATPLOTLIB_FLAG = True\n        mpl_logger = logging.getLogger('matplotlib')\n        mpl_logger.setLevel(logging.WARNING)\n    import matplotlib.pylab as plt\n    import numpy as np\n\n    fig, ax = plt.subplots(figsize=(10, 2))\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n    plt.xlabel(\"Frames\")\n    plt.ylabel(\"Channels\")\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close()\n    return data", "\n\ndef plot_alignment_to_numpy(alignment, info=None):\n    global MATPLOTLIB_FLAG\n    if not MATPLOTLIB_FLAG:\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        MATPLOTLIB_FLAG = True\n        mpl_logger = logging.getLogger('matplotlib')\n        mpl_logger.setLevel(logging.WARNING)\n    import matplotlib.pylab as plt\n    import numpy as np\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',\n                   interpolation='none')\n    fig.colorbar(im, ax=ax)\n    xlabel = 'Decoder timestep'\n    if info is not None:\n        xlabel += '\\n\\n' + info\n    plt.xlabel(xlabel)\n    plt.ylabel('Encoder timestep')\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close()\n    return data", "\n\ndef load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\n\ndef load_filepaths_and_text(filename, split=\"|\"):\n    with open(filename, encoding='utf-8') as f:\n        filepaths_and_text = [line.strip().split(split) for line in f]\n    return filepaths_and_text", "\n\ndef get_hparams(init=True):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--config', type=str, default=\"./configs/finetune_speaker.json\",\n                        help='JSON file for configuration')\n    parser.add_argument('-m', '--model', type=str, default=\"pretrained_models\",\n                        help='Model name')\n    parser.add_argument('-n', '--n_steps', type=int, default=\"2000\",\n                        help='finetune steps')\n\n    args = parser.parse_args()\n    model_dir = os.path.join(\"./\", args.model)\n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    config_path = args.config\n    config_save_path = os.path.join(model_dir, \"config.json\")\n    if init:\n        with open(config_path, \"r\") as f:\n            data = f.read()\n        with open(config_save_path, \"w\") as f:\n            f.write(data)\n    else:\n        with open(config_save_path, \"r\") as f:\n            data = f.read()\n    config = json.loads(data)\n\n    hparams = HParams(**config)\n    hparams.model_dir = model_dir\n    hparams.n_steps = args.n_steps\n    return hparams", "\n\ndef get_hparams_from_dir(model_dir):\n    config_save_path = os.path.join(model_dir, \"config.json\")\n    with open(config_save_path, \"r\") as f:\n        data = f.read()\n    config = json.loads(data)\n\n    hparams = HParams(**config)\n    hparams.model_dir = model_dir\n    return hparams", "\n\ndef get_hparams_from_file(config_path):\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        data = f.read()\n    config = json.loads(data)\n\n    hparams = HParams(**config)\n    return hparams\n", "\n\ndef check_git_hash(model_dir):\n    source_dir = os.path.dirname(os.path.realpath(__file__))\n    if not os.path.exists(os.path.join(source_dir, \".git\")):\n        logger.warn(\"{} is not a git repository, therefore hash value comparison will be ignored.\".format(\n            source_dir\n        ))\n        return\n\n    cur_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n\n    path = os.path.join(model_dir, \"githash\")\n    if os.path.exists(path):\n        saved_hash = open(path).read()\n        if saved_hash != cur_hash:\n            logger.warn(\"git hash values are different. {}(saved) != {}(current)\".format(\n                saved_hash[:8], cur_hash[:8]))\n    else:\n        open(path, \"w\").write(cur_hash)", "\n\ndef get_logger(model_dir, filename=\"train.log\"):\n    global logger\n    logger = logging.getLogger(os.path.basename(model_dir))\n    logger.setLevel(logging.DEBUG)\n\n    formatter = logging.Formatter(\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\")\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    h = logging.FileHandler(os.path.join(model_dir, filename))\n    h.setLevel(logging.DEBUG)\n    h.setFormatter(formatter)\n    logger.addHandler(h)\n    return logger", "\n\nclass HParams():\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            if type(v) == dict:\n                v = HParams(**v)\n            self[k] = v\n\n    def keys(self):\n        return self.__dict__.keys()\n\n    def items(self):\n        return self.__dict__.items()\n\n    def values(self):\n        return self.__dict__.values()\n\n    def __len__(self):\n        return len(self.__dict__)\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        return setattr(self, key, value)\n\n    def __contains__(self, key):\n        return key in self.__dict__\n\n    def __repr__(self):\n        return self.__dict__.__repr__()"]}
{"filename": "preprocess.py", "chunked_list": ["import os\nMIN_VOICE_NUM = 10\nif __name__ == \"__main__\":\n    # load sampled_audio4ft\n    with open(\"sampled_audio4ft.txt\", 'r', encoding='utf-8') as f:\n        old_annos = f.readlines()\n    num_old_voices = len(old_annos)\n    # load user text\n    with open(\"./user_voice/user_voice.txt.cleaned\", 'r', encoding='utf-8') as f:\n        user_annos = f.readlines()\n    # check how many voices are recorded\n    wavfiles = [file for file in list(os.walk(\"./user_voice\"))[0][2] if file.endswith(\".wav\")]\n    num_user_voices = len(wavfiles)\n    if num_user_voices < MIN_VOICE_NUM:\n        raise Exception(f\"You need to record at least {MIN_VOICE_NUM} voices for fine-tuning!\")\n    # user voices need to occupy 1/4 of the total dataset\n    duplicate = num_old_voices // num_user_voices // 3\n    # find corresponding existing annotation lines\n    actual_user_annos = [\"./user_voice/\" + line for line in user_annos if line.split(\"|\")[0] in wavfiles]\n    final_annos = old_annos + actual_user_annos * duplicate\n    # save annotation file\n    with open(\"final_annotation_train.txt\", 'w', encoding='utf-8') as f:\n        for line in final_annos:\n            f.write(line)\n    # save annotation file for validation\n    with open(\"final_annotation_val.txt\", 'w', encoding='utf-8') as f:\n        for line in actual_user_annos:\n            f.write(line)", ""]}
{"filename": "Vits_vioce_API.py", "chunked_list": ["import argparse\n\nimport librosa\nimport numpy as np\nimport soundfile\nimport torch\nfrom torch import no_grad, LongTensor\n\nimport commons\nimport utils", "import commons\nimport utils\nfrom mel_processing import spectrogram_torch\nfrom models_infer import SynthesizerTrn\nfrom text import text_to_sequence\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nlanguage_marks = {\n    \"Japanese\": \"\",\n    \"\u65e5\u672c\u8a9e\": \"[JA]\",", "    \"Japanese\": \"\",\n    \"\u65e5\u672c\u8a9e\": \"[JA]\",\n    \"\u7b80\u4f53\u4e2d\u6587\": \"[ZH]\",\n    \"English\": \"[EN]\",\n    \"Mix\": \"\",\n}\nlang = ['\u65e5\u672c\u8a9e', '\u7b80\u4f53\u4e2d\u6587', 'English', 'Mix']\ndef get_text(text, hps, is_symbol):\n    text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n    if hps.data.add_blank:\n        text_norm = commons.intersperse(text_norm, 0)\n    text_norm = LongTensor(text_norm)\n    return text_norm", "\ndef create_tts_fn(model, hps, speaker_ids):\n    def tts_fn(text, speaker, language, speed):\n        if language is not None:\n            text = language_marks[language] + text + language_marks[language]\n        speaker_id = speaker_ids[speaker]\n        stn_tst = get_text(text, hps, False)\n        with no_grad():\n            x_tst = stn_tst.unsqueeze(0).to(device)\n            x_tst_lengths = LongTensor([stn_tst.size(0)]).to(device)\n            sid = LongTensor([speaker_id]).to(device)\n            audio = model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8,\n                                length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()\n        del stn_tst, x_tst, x_tst_lengths, sid\n        return \"Success\", (hps.data.sampling_rate, audio)\n\n    return tts_fn", "\ndef create_vc_fn(model, hps, speaker_ids):\n    def vc_fn(original_speaker, target_speaker, record_audio, upload_audio):\n        input_audio = record_audio if record_audio is not None else upload_audio\n        if input_audio is None:\n            return \"You need to record or upload an audio\", None\n        sampling_rate, audio = input_audio\n        original_speaker_id = speaker_ids[original_speaker]\n        target_speaker_id = speaker_ids[target_speaker]\n\n        audio = (audio / np.iinfo(audio.dtype).max).astype(np.float32)\n        if len(audio.shape) > 1:\n            audio = librosa.to_mono(audio.transpose(1, 0))\n        if sampling_rate != hps.data.sampling_rate:\n            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=hps.data.sampling_rate)\n        with no_grad():\n            y = torch.FloatTensor(audio)\n            y = y / max(-y.min(), y.max()) / 0.99\n            y = y.to(device)\n            y = y.unsqueeze(0)\n            spec = spectrogram_torch(y, hps.data.filter_length,\n                                     hps.data.sampling_rate, hps.data.hop_length, hps.data.win_length,\n                                     center=False).to(device)\n            spec_lengths = LongTensor([spec.size(-1)]).to(device)\n            sid_src = LongTensor([original_speaker_id]).to(device)\n            sid_tgt = LongTensor([target_speaker_id]).to(device)\n            audio = model.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt)[0][\n                0, 0].data.cpu().float().numpy()\n            print('audio: ' + audio)\n        del y, spec, spec_lengths, sid_src, sid_tgt\n        return \"Success\", (hps.data.sampling_rate, audio)\n\n    return vc_fn", "\n\n\n# \u8fd0\u884c\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_dir\", default=\"./G_latest.pth\", help=\"directory to your fine-tuned model\")\nparser.add_argument(\"--config_dir\", default=\"./finetune_speaker.json\", help=\"directory to your model config file\")\nparser.add_argument(\"--share\", default=False, help=\"make link public (used in colab)\")\n\nargs = parser.parse_args()", "\nargs = parser.parse_args()\nhps = utils.get_hparams_from_file(args.config_dir)\n\nnet_g = SynthesizerTrn(\n    len(hps.symbols),\n    hps.data.filter_length // 2 + 1,\n    hps.train.segment_size // hps.data.hop_length,\n    n_speakers=hps.data.n_speakers,\n    **hps.model).to(device)", "    n_speakers=hps.data.n_speakers,\n    **hps.model).to(device)\n_ = net_g.eval()\n\n_ = utils.load_checkpoint(args.model_dir, net_g, None)\nspeaker_ids = hps.speakers\nspeakers = list(hps.speakers.keys())\ntts_fn = create_tts_fn(net_g, hps, speaker_ids)\n\n\ndef vits_voice(txt, speaker=79, language=1, speed=1):\n    print('++++++')\n    audioTest = tts_fn(txt, speakers[speaker], lang[language], speed)\n    print('++++++')\n    soundfile.write('./data/voices/out.mp3', audioTest[1][1], audioTest[1][0])", "\n\ndef vits_voice(txt, speaker=79, language=1, speed=1):\n    print('++++++')\n    audioTest = tts_fn(txt, speakers[speaker], lang[language], speed)\n    print('++++++')\n    soundfile.write('./data/voices/out.mp3', audioTest[1][1], audioTest[1][0])\n\n\nif __name__ == \"__main__\":\n    while 1:\n        msg = input(\"\u8bf7\u8f93\u5165\")\n        vits_voice(msg)", "\nif __name__ == \"__main__\":\n    while 1:\n        msg = input(\"\u8bf7\u8f93\u5165\")\n        vits_voice(msg)\n\n\n\n\n", "\n\n"]}
{"filename": "modules.py", "chunked_list": ["import math\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import weight_norm, remove_weight_norm\n\nimport commons", "\nimport commons\nfrom commons import init_weights, get_padding\nfrom transforms import piecewise_rational_quadratic_transform\n\nLRELU_SLOPE = 0.1\n\n\nclass LayerNorm(nn.Module):\n  def __init__(self, channels, eps=1e-5):\n    super().__init__()\n    self.channels = channels\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(channels))\n    self.beta = nn.Parameter(torch.zeros(channels))\n\n  def forward(self, x):\n    x = x.transpose(1, -1)\n    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n    return x.transpose(1, -1)", "class LayerNorm(nn.Module):\n  def __init__(self, channels, eps=1e-5):\n    super().__init__()\n    self.channels = channels\n    self.eps = eps\n\n    self.gamma = nn.Parameter(torch.ones(channels))\n    self.beta = nn.Parameter(torch.zeros(channels))\n\n  def forward(self, x):\n    x = x.transpose(1, -1)\n    x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n    return x.transpose(1, -1)", "\n \nclass ConvReluNorm(nn.Module):\n  def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n    super().__init__()\n    self.in_channels = in_channels\n    self.hidden_channels = hidden_channels\n    self.out_channels = out_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n    assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n    self.conv_layers = nn.ModuleList()\n    self.norm_layers = nn.ModuleList()\n    self.conv_layers.append(nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n    self.norm_layers.append(LayerNorm(hidden_channels))\n    self.relu_drop = nn.Sequential(\n        nn.ReLU(),\n        nn.Dropout(p_dropout))\n    for _ in range(n_layers-1):\n      self.conv_layers.append(nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size//2))\n      self.norm_layers.append(LayerNorm(hidden_channels))\n    self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask):\n    x_org = x\n    for i in range(self.n_layers):\n      x = self.conv_layers[i](x * x_mask)\n      x = self.norm_layers[i](x)\n      x = self.relu_drop(x)\n    x = x_org + self.proj(x)\n    return x * x_mask", "\n\nclass DDSConv(nn.Module):\n  \"\"\"\n  Dialted and Depth-Separable Convolution\n  \"\"\"\n  def __init__(self, channels, kernel_size, n_layers, p_dropout=0.):\n    super().__init__()\n    self.channels = channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.p_dropout = p_dropout\n\n    self.drop = nn.Dropout(p_dropout)\n    self.convs_sep = nn.ModuleList()\n    self.convs_1x1 = nn.ModuleList()\n    self.norms_1 = nn.ModuleList()\n    self.norms_2 = nn.ModuleList()\n    for i in range(n_layers):\n      dilation = kernel_size ** i\n      padding = (kernel_size * dilation - dilation) // 2\n      self.convs_sep.append(nn.Conv1d(channels, channels, kernel_size, \n          groups=channels, dilation=dilation, padding=padding\n      ))\n      self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n      self.norms_1.append(LayerNorm(channels))\n      self.norms_2.append(LayerNorm(channels))\n\n  def forward(self, x, x_mask, g=None):\n    if g is not None:\n      x = x + g\n    for i in range(self.n_layers):\n      y = self.convs_sep[i](x * x_mask)\n      y = self.norms_1[i](y)\n      y = F.gelu(y)\n      y = self.convs_1x1[i](y)\n      y = self.norms_2[i](y)\n      y = F.gelu(y)\n      y = self.drop(y)\n      x = x + y\n    return x * x_mask", "\n\nclass WN(torch.nn.Module):\n  def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n    super(WN, self).__init__()\n    assert(kernel_size % 2 == 1)\n    self.hidden_channels =hidden_channels\n    self.kernel_size = kernel_size,\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.gin_channels = gin_channels\n    self.p_dropout = p_dropout\n\n    self.in_layers = torch.nn.ModuleList()\n    self.res_skip_layers = torch.nn.ModuleList()\n    self.drop = nn.Dropout(p_dropout)\n\n    if gin_channels != 0:\n      cond_layer = torch.nn.Conv1d(gin_channels, 2*hidden_channels*n_layers, 1)\n      self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n\n    for i in range(n_layers):\n      dilation = dilation_rate ** i\n      padding = int((kernel_size * dilation - dilation) / 2)\n      in_layer = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, kernel_size,\n                                 dilation=dilation, padding=padding)\n      in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n      self.in_layers.append(in_layer)\n\n      # last one is not necessary\n      if i < n_layers - 1:\n        res_skip_channels = 2 * hidden_channels\n      else:\n        res_skip_channels = hidden_channels\n\n      res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n      res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n      self.res_skip_layers.append(res_skip_layer)\n\n  def forward(self, x, x_mask, g=None, **kwargs):\n    output = torch.zeros_like(x)\n    n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n    if g is not None:\n      g = self.cond_layer(g)\n\n    for i in range(self.n_layers):\n      x_in = self.in_layers[i](x)\n      if g is not None:\n        cond_offset = i * 2 * self.hidden_channels\n        g_l = g[:,cond_offset:cond_offset+2*self.hidden_channels,:]\n      else:\n        g_l = torch.zeros_like(x_in)\n\n      acts = commons.fused_add_tanh_sigmoid_multiply(\n          x_in,\n          g_l,\n          n_channels_tensor)\n      acts = self.drop(acts)\n\n      res_skip_acts = self.res_skip_layers[i](acts)\n      if i < self.n_layers - 1:\n        res_acts = res_skip_acts[:,:self.hidden_channels,:]\n        x = (x + res_acts) * x_mask\n        output = output + res_skip_acts[:,self.hidden_channels:,:]\n      else:\n        output = output + res_skip_acts\n    return output * x_mask\n\n  def remove_weight_norm(self):\n    if self.gin_channels != 0:\n      torch.nn.utils.remove_weight_norm(self.cond_layer)\n    for l in self.in_layers:\n      torch.nn.utils.remove_weight_norm(l)\n    for l in self.res_skip_layers:\n     torch.nn.utils.remove_weight_norm(l)", "\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n                               padding=get_padding(kernel_size, dilation[2])))\n        ])\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n                               padding=get_padding(kernel_size, 1)))\n        ])\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)", "\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList([\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n                               padding=get_padding(kernel_size, dilation[0]))),\n            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n                               padding=get_padding(kernel_size, dilation[1])))\n        ])\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)", "\n\nclass Log(nn.Module):\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n      logdet = torch.sum(-y, [1, 2])\n      return y, logdet\n    else:\n      x = torch.exp(x) * x_mask\n      return x", "    \n\nclass Flip(nn.Module):\n  def forward(self, x, *args, reverse=False, **kwargs):\n    x = torch.flip(x, [1])\n    if not reverse:\n      logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n      return x, logdet\n    else:\n      return x", "\n\nclass ElementwiseAffine(nn.Module):\n  def __init__(self, channels):\n    super().__init__()\n    self.channels = channels\n    self.m = nn.Parameter(torch.zeros(channels,1))\n    self.logs = nn.Parameter(torch.zeros(channels,1))\n\n  def forward(self, x, x_mask, reverse=False, **kwargs):\n    if not reverse:\n      y = self.m + torch.exp(self.logs) * x\n      y = y * x_mask\n      logdet = torch.sum(self.logs * x_mask, [1,2])\n      return y, logdet\n    else:\n      x = (x - self.m) * torch.exp(-self.logs) * x_mask\n      return x", "\n\nclass ResidualCouplingLayer(nn.Module):\n  def __init__(self,\n      channels,\n      hidden_channels,\n      kernel_size,\n      dilation_rate,\n      n_layers,\n      p_dropout=0,\n      gin_channels=0,\n      mean_only=False):\n    assert channels % 2 == 0, \"channels should be divisible by 2\"\n    super().__init__()\n    self.channels = channels\n    self.hidden_channels = hidden_channels\n    self.kernel_size = kernel_size\n    self.dilation_rate = dilation_rate\n    self.n_layers = n_layers\n    self.half_channels = channels // 2\n    self.mean_only = mean_only\n\n    self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n    self.enc = WN(hidden_channels, kernel_size, dilation_rate, n_layers, p_dropout=p_dropout, gin_channels=gin_channels)\n    self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n    self.post.weight.data.zero_()\n    self.post.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0) * x_mask\n    h = self.enc(h, x_mask, g=g)\n    stats = self.post(h) * x_mask\n    if not self.mean_only:\n      m, logs = torch.split(stats, [self.half_channels]*2, 1)\n    else:\n      m = stats\n      logs = torch.zeros_like(m)\n\n    if not reverse:\n      x1 = m + x1 * torch.exp(logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      logdet = torch.sum(logs, [1,2])\n      return x, logdet\n    else:\n      x1 = (x1 - m) * torch.exp(-logs) * x_mask\n      x = torch.cat([x0, x1], 1)\n      return x", "\n\nclass ConvFlow(nn.Module):\n  def __init__(self, in_channels, filter_channels, kernel_size, n_layers, num_bins=10, tail_bound=5.0):\n    super().__init__()\n    self.in_channels = in_channels\n    self.filter_channels = filter_channels\n    self.kernel_size = kernel_size\n    self.n_layers = n_layers\n    self.num_bins = num_bins\n    self.tail_bound = tail_bound\n    self.half_channels = in_channels // 2\n\n    self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n    self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.)\n    self.proj = nn.Conv1d(filter_channels, self.half_channels * (num_bins * 3 - 1), 1)\n    self.proj.weight.data.zero_()\n    self.proj.bias.data.zero_()\n\n  def forward(self, x, x_mask, g=None, reverse=False):\n    x0, x1 = torch.split(x, [self.half_channels]*2, 1)\n    h = self.pre(x0)\n    h = self.convs(h, x_mask, g=g)\n    h = self.proj(h) * x_mask\n\n    b, c, t = x0.shape\n    h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2) # [b, cx?, t] -> [b, c, t, ?]\n\n    unnormalized_widths = h[..., :self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_heights = h[..., self.num_bins:2*self.num_bins] / math.sqrt(self.filter_channels)\n    unnormalized_derivatives = h[..., 2 * self.num_bins:]\n\n    x1, logabsdet = piecewise_rational_quadratic_transform(x1,\n        unnormalized_widths,\n        unnormalized_heights,\n        unnormalized_derivatives,\n        inverse=reverse,\n        tails='linear',\n        tail_bound=self.tail_bound\n    )\n\n    x = torch.cat([x0, x1], 1) * x_mask\n    logdet = torch.sum(logabsdet * x_mask, [1,2])\n    if not reverse:\n        return x, logdet\n    else:\n        return x", ""]}
{"filename": "finetune_speaker.py", "chunked_list": ["import os\nimport itertools\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler", "from torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n\nimport logging\n\nlogging.getLogger('numba').setLevel(logging.WARNING)\n\nimport commons\nimport utils", "import commons\nimport utils\nfrom data_utils import (\n  TextAudioSpeakerLoader,\n  TextAudioSpeakerCollate,\n  DistributedBucketSampler\n)\nfrom models import (\n  SynthesizerTrn,\n  MultiPeriodDiscriminator,", "  SynthesizerTrn,\n  MultiPeriodDiscriminator,\n)\nfrom losses import (\n  generator_loss,\n  discriminator_loss,\n  feature_loss,\n  kl_loss\n)\nfrom mel_processing import mel_spectrogram_torch, spec_to_mel_torch", ")\nfrom mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n\n\ntorch.backends.cudnn.benchmark = True\nglobal_step = 0\n\n\ndef main():\n  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n  n_gpus = torch.cuda.device_count()\n  os.environ['MASTER_ADDR'] = 'localhost'\n  os.environ['MASTER_PORT'] = '8000'\n\n  hps = utils.get_hparams()\n  mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))", "def main():\n  \"\"\"Assume Single Node Multi GPUs Training Only\"\"\"\n  assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n\n  n_gpus = torch.cuda.device_count()\n  os.environ['MASTER_ADDR'] = 'localhost'\n  os.environ['MASTER_PORT'] = '8000'\n\n  hps = utils.get_hparams()\n  mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))", "\n\ndef run(rank, n_gpus, hps):\n  global global_step\n  symbols = hps['symbols']\n  if rank == 0:\n    logger = utils.get_logger(hps.model_dir)\n    logger.info(hps)\n    utils.check_git_hash(hps.model_dir)\n    writer = SummaryWriter(log_dir=hps.model_dir)\n    writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n\n  dist.init_process_group(backend='nccl', init_method='env://', world_size=n_gpus, rank=rank)\n  torch.manual_seed(hps.train.seed)\n  torch.cuda.set_device(rank)\n\n  train_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps.data)\n  train_sampler = DistributedBucketSampler(\n      train_dataset,\n      hps.train.batch_size,\n      [32,300,400,500,600,700,800,900,1000],\n      num_replicas=n_gpus,\n      rank=rank,\n      shuffle=True)\n  collate_fn = TextAudioSpeakerCollate()\n  train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False, pin_memory=True,\n      collate_fn=collate_fn, batch_sampler=train_sampler)\n  # train_loader = DataLoader(train_dataset, batch_size=hps.train.batch_size, num_workers=0, shuffle=False, pin_memory=True,\n  #                           collate_fn=collate_fn)\n  if rank == 0:\n    eval_dataset = TextAudioSpeakerLoader(hps.data.validation_files, hps.data)\n    eval_loader = DataLoader(eval_dataset, num_workers=0, shuffle=False,\n        batch_size=hps.train.batch_size, pin_memory=True,\n        drop_last=False, collate_fn=collate_fn)\n\n  net_g = SynthesizerTrn(\n      len(symbols),\n      hps.data.filter_length // 2 + 1,\n      hps.train.segment_size // hps.data.hop_length,\n      n_speakers=hps.data.n_speakers,\n      **hps.model).cuda(rank)\n  net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)\n\n  # load existing model\n  _, _, _, _ = utils.load_checkpoint(\"./pretrained_models/G_trilingual.pth\", net_g, None)\n  _, _, _, _ = utils.load_checkpoint(\"./pretrained_models/D_trilingual.pth\", net_d, None)\n  epoch_str = 1\n  global_step = 0\n  # freeze all other layers except speaker embedding\n  for p in net_g.parameters():\n      p.requires_grad = True\n  for p in net_d.parameters():\n      p.requires_grad = True\n  # for p in net_d.parameters():\n  #     p.requires_grad = False\n  # net_g.emb_g.weight.requires_grad = True\n  optim_g = torch.optim.AdamW(\n      net_g.parameters(),\n      hps.train.learning_rate,\n      betas=hps.train.betas,\n      eps=hps.train.eps)\n  optim_d = torch.optim.AdamW(\n      net_d.parameters(),\n      hps.train.learning_rate,\n      betas=hps.train.betas,\n      eps=hps.train.eps)\n  # optim_d = None\n  net_g = DDP(net_g, device_ids=[rank])\n  net_d = DDP(net_d, device_ids=[rank])\n\n  scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay)\n  scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay)\n\n  scaler = GradScaler(enabled=hps.train.fp16_run)\n\n  for epoch in range(epoch_str, hps.train.epochs + 1):\n    if rank==0:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, eval_loader], logger, [writer, writer_eval])\n    else:\n      train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, None], None, None)\n    scheduler_g.step()\n    scheduler_d.step()", "\n\ndef train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):\n  net_g, net_d = nets\n  optim_g, optim_d = optims\n  scheduler_g, scheduler_d = schedulers\n  train_loader, eval_loader = loaders\n  if writers is not None:\n    writer, writer_eval = writers\n\n  # train_loader.batch_sampler.set_epoch(epoch)\n  global global_step\n\n  net_g.train()\n  net_d.train()\n  for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(tqdm(train_loader)):\n    x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)\n    spec, spec_lengths = spec.cuda(rank, non_blocking=True), spec_lengths.cuda(rank, non_blocking=True)\n    y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)\n    speakers = speakers.cuda(rank, non_blocking=True)\n\n    with autocast(enabled=hps.train.fp16_run):\n      y_hat, l_length, attn, ids_slice, x_mask, z_mask,\\\n      (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n\n      mel = spec_to_mel_torch(\n          spec,\n          hps.data.filter_length,\n          hps.data.n_mel_channels,\n          hps.data.sampling_rate,\n          hps.data.mel_fmin,\n          hps.data.mel_fmax)\n      y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)\n      y_hat_mel = mel_spectrogram_torch(\n          y_hat.squeeze(1),\n          hps.data.filter_length,\n          hps.data.n_mel_channels,\n          hps.data.sampling_rate,\n          hps.data.hop_length,\n          hps.data.win_length,\n          hps.data.mel_fmin,\n          hps.data.mel_fmax\n      )\n\n      y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size) # slice\n\n      # Discriminator\n      y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n      with autocast(enabled=False):\n        loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)\n        loss_disc_all = loss_disc\n    optim_d.zero_grad()\n    scaler.scale(loss_disc_all).backward()\n    scaler.unscale_(optim_d)\n    grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n    scaler.step(optim_d)\n\n    with autocast(enabled=hps.train.fp16_run):\n      # Generator\n      y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n      with autocast(enabled=False):\n        loss_dur = torch.sum(l_length.float())\n        loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel\n        loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl\n\n        loss_fm = feature_loss(fmap_r, fmap_g)\n        loss_gen, losses_gen = generator_loss(y_d_hat_g)\n        loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n    optim_g.zero_grad()\n    scaler.scale(loss_gen_all).backward()\n    scaler.unscale_(optim_g)\n    grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n    scaler.step(optim_g)\n    scaler.update()\n\n    if rank==0:\n      if global_step % hps.train.log_interval == 0:\n        lr = optim_g.param_groups[0]['lr']\n        losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]\n        logger.info('Train Epoch: {} [{:.0f}%]'.format(\n          epoch,\n          100. * batch_idx / len(train_loader)))\n        logger.info([x.item() for x in losses] + [global_step, lr])\n\n        scalar_dict = {\"loss/g/total\": loss_gen_all, \"loss/d/total\": loss_disc_all, \"learning_rate\": lr, \"grad_norm_g\": grad_norm_g}\n        scalar_dict.update({\"loss/g/fm\": loss_fm, \"loss/g/mel\": loss_mel, \"loss/g/dur\": loss_dur, \"loss/g/kl\": loss_kl})\n\n        scalar_dict.update({\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)})\n        scalar_dict.update({\"loss/d_r/{}\".format(i): v for i, v in enumerate(losses_disc_r)})\n        scalar_dict.update({\"loss/d_g/{}\".format(i): v for i, v in enumerate(losses_disc_g)})\n        image_dict = {\n            \"slice/mel_org\": utils.plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),\n            \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()),\n            \"all/mel\": utils.plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),\n            \"all/attn\": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy())\n        }\n        utils.summarize(\n          writer=writer,\n          global_step=global_step,\n          images=image_dict,\n          scalars=scalar_dict)\n\n      if global_step % hps.train.eval_interval == 0:\n        evaluate(hps, net_g, eval_loader, writer_eval)\n        utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step)))\n        utils.save_checkpoint(net_g, None, hps.train.learning_rate, epoch,\n                              os.path.join(hps.model_dir, \"G_latest.pth\".format(global_step)))\n        # utils.save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step)))\n        old_g=os.path.join(hps.model_dir, \"G_{}.pth\".format(global_step-4000))\n        # old_d=os.path.join(hps.model_dir, \"D_{}.pth\".format(global_step-400))\n        if os.path.exists(old_g):\n          os.remove(old_g)\n        # if os.path.exists(old_d):\n        #   os.remove(old_d)\n    global_step += 1\n    if epoch > hps.max_epochs:\n        print(\"Maximum epoch reached, closing training...\")\n        exit()\n\n  if rank == 0:\n    logger.info('====> Epoch: {}'.format(epoch))", "\n\ndef evaluate(hps, generator, eval_loader, writer_eval):\n    generator.eval()\n    with torch.no_grad():\n      for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths, speakers) in enumerate(eval_loader):\n        x, x_lengths = x.cuda(0), x_lengths.cuda(0)\n        spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)\n        y, y_lengths = y.cuda(0), y_lengths.cuda(0)\n        speakers = speakers.cuda(0)\n\n        # remove else\n        x = x[:1]\n        x_lengths = x_lengths[:1]\n        spec = spec[:1]\n        spec_lengths = spec_lengths[:1]\n        y = y[:1]\n        y_lengths = y_lengths[:1]\n        speakers = speakers[:1]\n        break\n      y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, speakers, max_len=1000)\n      y_hat_lengths = mask.sum([1,2]).long() * hps.data.hop_length\n\n      mel = spec_to_mel_torch(\n        spec,\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax)\n      y_hat_mel = mel_spectrogram_torch(\n        y_hat.squeeze(1).float(),\n        hps.data.filter_length,\n        hps.data.n_mel_channels,\n        hps.data.sampling_rate,\n        hps.data.hop_length,\n        hps.data.win_length,\n        hps.data.mel_fmin,\n        hps.data.mel_fmax\n      )\n    image_dict = {\n      \"gen/mel\": utils.plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())\n    }\n    audio_dict = {\n      \"gen/audio\": y_hat[0,:,:y_hat_lengths[0]]\n    }\n    if global_step == 0:\n      image_dict.update({\"gt/mel\": utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy())})\n      audio_dict.update({\"gt/audio\": y[0,:,:y_lengths[0]]})\n\n    utils.summarize(\n      writer=writer_eval,\n      global_step=global_step,\n      images=image_dict,\n      audios=audio_dict,\n      audio_sampling_rate=hps.data.sampling_rate\n    )\n    generator.train()", "\n\nif __name__ == \"__main__\":\n  main()"]}
{"filename": "text/english.py", "chunked_list": ["\"\"\" from https://github.com/keithito/tacotron \"\"\"\n\n'''\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\nhyperparameter. Some cleaners are English-specific. You'll typically want to use:\n  1. \"english_cleaners\" for English text\n  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)", "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n'''\n\n\n# Regular expression matching whitespace:\n\n", "\n\nimport re\n\nimport eng_to_ipa as ipa\nimport inflect\nfrom unidecode import unidecode\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')", "_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\u00a3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [", "# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('mrs', 'misess'),\n    ('mr', 'mister'),\n    ('dr', 'doctor'),\n    ('st', 'saint'),\n    ('co', 'company'),\n    ('jr', 'junior'),\n    ('maj', 'major'),\n    ('gen', 'general'),", "    ('maj', 'major'),\n    ('gen', 'general'),\n    ('drs', 'doctors'),\n    ('rev', 'reverend'),\n    ('lt', 'lieutenant'),\n    ('hon', 'honorable'),\n    ('sgt', 'sergeant'),\n    ('capt', 'captain'),\n    ('esq', 'esquire'),\n    ('ltd', 'limited'),", "    ('esq', 'esquire'),\n    ('ltd', 'limited'),\n    ('col', 'colonel'),\n    ('ft', 'fort'),\n]]\n\n\n# List of (ipa, lazy ipa) pairs:\n_lazy_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('r', '\u0279'),", "_lazy_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('r', '\u0279'),\n    ('\u00e6', 'e'),\n    ('\u0251', 'a'),\n    ('\u0254', 'o'),\n    ('\u00f0', 'z'),\n    ('\u03b8', 's'),\n    ('\u025b', 'e'),\n    ('\u026a', 'i'),\n    ('\u028a', 'u'),", "    ('\u026a', 'i'),\n    ('\u028a', 'u'),\n    ('\u0292', '\u02a5'),\n    ('\u02a4', '\u02a5'),\n    ('\u02c8', '\u2193'),\n]]\n\n# List of (ipa, lazy ipa2) pairs:\n_lazy_ipa2 = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('r', '\u0279'),", "_lazy_ipa2 = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('r', '\u0279'),\n    ('\u00f0', 'z'),\n    ('\u03b8', 's'),\n    ('\u0292', '\u0291'),\n    ('\u02a4', 'd\u0291'),\n    ('\u02c8', '\u2193'),\n]]\n\n# List of (ipa, ipa2) pairs", "\n# List of (ipa, ipa2) pairs\n_ipa_to_ipa2 = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('r', '\u0279'),\n    ('\u02a4', 'd\u0292'),\n    ('\u02a7', 't\u0283')\n]]\n\n\ndef expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text", "\ndef expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef collapse_whitespace(text):\n    return re.sub(r'\\s+', ' ', text)\n", "\n\ndef _remove_commas(m):\n    return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n    return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split('.')\n    if len(parts) > 2:\n        return match + ' dollars'  # Unexpected format\n    dollars = int(parts[0]) if parts[0] else 0\n    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if dollars and cents:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n    elif dollars:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        return '%s %s' % (dollars, dollar_unit)\n    elif cents:\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s' % (cents, cent_unit)\n    else:\n        return 'zero dollars'", "\n\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split('.')\n    if len(parts) > 2:\n        return match + ' dollars'  # Unexpected format\n    dollars = int(parts[0]) if parts[0] else 0\n    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if dollars and cents:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n    elif dollars:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        return '%s %s' % (dollars, dollar_unit)\n    elif cents:\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s' % (cents, cent_unit)\n    else:\n        return 'zero dollars'", "\n\ndef _expand_ordinal(m):\n    return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n    num = int(m.group(0))\n    if num > 1000 and num < 3000:\n        if num == 2000:\n            return 'two thousand'\n        elif num > 2000 and num < 2010:\n            return 'two thousand ' + _inflect.number_to_words(num % 100)\n        elif num % 100 == 0:\n            return _inflect.number_to_words(num // 100) + ' hundred'\n        else:\n            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n    else:\n        return _inflect.number_to_words(num, andword='')", "\n\ndef normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_pounds_re, r'\\1 pounds', text)\n    text = re.sub(_dollars_re, _expand_dollars, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text", "\n\ndef mark_dark_l(text):\n    return re.sub(r'l([^aeiou\u00e6\u0251\u0254\u0259\u025b\u026a\u028a ]*(?: |$))', lambda x: '\u026b'+x.group(1), text)\n\n\ndef english_to_ipa(text):\n    text = unidecode(text).lower()\n    text = expand_abbreviations(text)\n    text = normalize_numbers(text)\n    phonemes = ipa.convert(text)\n    phonemes = collapse_whitespace(phonemes)\n    return phonemes", "\n\ndef english_to_lazy_ipa(text):\n    text = english_to_ipa(text)\n    for regex, replacement in _lazy_ipa:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef english_to_ipa2(text):\n    text = english_to_ipa(text)\n    text = mark_dark_l(text)\n    for regex, replacement in _ipa_to_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text.replace('...', '\u2026')", "\ndef english_to_ipa2(text):\n    text = english_to_ipa(text)\n    text = mark_dark_l(text)\n    for regex, replacement in _ipa_to_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text.replace('...', '\u2026')\n\n\ndef english_to_lazy_ipa2(text):\n    text = english_to_ipa(text)\n    for regex, replacement in _lazy_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text", "\ndef english_to_lazy_ipa2(text):\n    text = english_to_ipa(text)\n    for regex, replacement in _lazy_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text\n"]}
{"filename": "text/mandarin.py", "chunked_list": ["import re\nfrom pypinyin import lazy_pinyin, BOPOMOFO\nimport jieba\nimport cn2an\nimport re\n\nimport cn2an\nimport jieba\nfrom pypinyin import lazy_pinyin, BOPOMOFO\n", "from pypinyin import lazy_pinyin, BOPOMOFO\n\n# List of (Latin alphabet, bopomofo) pairs:\n_latin_to_bopomofo = [(re.compile('%s' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('a', '\u311f\u02c9'),\n    ('b', '\u3105\u3127\u02cb'),\n    ('c', '\u3119\u3127\u02c9'),\n    ('d', '\u3109\u3127\u02cb'),\n    ('e', '\u3127\u02cb'),\n    ('f', '\u311d\u02ca\u3108\u3128\u02cb'),", "    ('e', '\u3127\u02cb'),\n    ('f', '\u311d\u02ca\u3108\u3128\u02cb'),\n    ('g', '\u3110\u3127\u02cb'),\n    ('h', '\u311d\u02c7\u3111\u3129\u02cb'),\n    ('i', '\u311e\u02cb'),\n    ('j', '\u3110\u311f\u02cb'),\n    ('k', '\u310e\u311f\u02cb'),\n    ('l', '\u311d\u02ca\u311b\u02cb'),\n    ('m', '\u311d\u02ca\u3107\u3128\u02cb'),\n    ('n', '\u3123\u02c9'),", "    ('m', '\u311d\u02ca\u3107\u3128\u02cb'),\n    ('n', '\u3123\u02c9'),\n    ('o', '\u3121\u02c9'),\n    ('p', '\u3106\u3127\u02c9'),\n    ('q', '\u310e\u3127\u3121\u02c9'),\n    ('r', '\u311a\u02cb'),\n    ('s', '\u311d\u02ca\u3119\u02cb'),\n    ('t', '\u310a\u3127\u02cb'),\n    ('u', '\u3127\u3121\u02c9'),\n    ('v', '\u3128\u3127\u02c9'),", "    ('u', '\u3127\u3121\u02c9'),\n    ('v', '\u3128\u3127\u02c9'),\n    ('w', '\u3109\u311a\u02cb\u3105\u3128\u02cb\u310c\u3127\u3121\u02cb'),\n    ('x', '\u311d\u02c9\u310e\u3128\u02cb\u3119\u02cb'),\n    ('y', '\u3128\u311e\u02cb'),\n    ('z', '\u3117\u311f\u02cb')\n]]\n\n# List of (bopomofo, romaji) pairs:\n_bopomofo_to_romaji = [(re.compile('%s' % x[0]), x[1]) for x in [", "# List of (bopomofo, romaji) pairs:\n_bopomofo_to_romaji = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\u3105\u311b', 'p\u207cwo'),\n    ('\u3106\u311b', 'p\u02b0wo'),\n    ('\u3107\u311b', 'mwo'),\n    ('\u3108\u311b', 'fwo'),\n    ('\u3105', 'p\u207c'),\n    ('\u3106', 'p\u02b0'),\n    ('\u3107', 'm'),\n    ('\u3108', 'f'),", "    ('\u3107', 'm'),\n    ('\u3108', 'f'),\n    ('\u3109', 't\u207c'),\n    ('\u310a', 't\u02b0'),\n    ('\u310b', 'n'),\n    ('\u310c', 'l'),\n    ('\u310d', 'k\u207c'),\n    ('\u310e', 'k\u02b0'),\n    ('\u310f', 'h'),\n    ('\u3110', '\u02a7\u207c'),", "    ('\u310f', 'h'),\n    ('\u3110', '\u02a7\u207c'),\n    ('\u3111', '\u02a7\u02b0'),\n    ('\u3112', '\u0283'),\n    ('\u3113', '\u02a6`\u207c'),\n    ('\u3114', '\u02a6`\u02b0'),\n    ('\u3115', 's`'),\n    ('\u3116', '\u0279`'),\n    ('\u3117', '\u02a6\u207c'),\n    ('\u3118', '\u02a6\u02b0'),", "    ('\u3117', '\u02a6\u207c'),\n    ('\u3118', '\u02a6\u02b0'),\n    ('\u3119', 's'),\n    ('\u311a', 'a'),\n    ('\u311b', 'o'),\n    ('\u311c', '\u0259'),\n    ('\u311d', 'e'),\n    ('\u311e', 'ai'),\n    ('\u311f', 'ei'),\n    ('\u3120', 'au'),", "    ('\u311f', 'ei'),\n    ('\u3120', 'au'),\n    ('\u3121', 'ou'),\n    ('\u3127\u3122', 'yeNN'),\n    ('\u3122', 'aNN'),\n    ('\u3127\u3123', 'iNN'),\n    ('\u3123', '\u0259NN'),\n    ('\u3124', 'aNg'),\n    ('\u3127\u3125', 'iNg'),\n    ('\u3128\u3125', 'uNg'),", "    ('\u3127\u3125', 'iNg'),\n    ('\u3128\u3125', 'uNg'),\n    ('\u3129\u3125', 'yuNg'),\n    ('\u3125', '\u0259Ng'),\n    ('\u3126', '\u0259\u027b'),\n    ('\u3127', 'i'),\n    ('\u3128', 'u'),\n    ('\u3129', '\u0265'),\n    ('\u02c9', '\u2192'),\n    ('\u02ca', '\u2191'),", "    ('\u02c9', '\u2192'),\n    ('\u02ca', '\u2191'),\n    ('\u02c7', '\u2193\u2191'),\n    ('\u02cb', '\u2193'),\n    ('\u02d9', ''),\n    ('\uff0c', ','),\n    ('\u3002', '.'),\n    ('\uff01', '!'),\n    ('\uff1f', '?'),\n    ('\u2014', '-')", "    ('\uff1f', '?'),\n    ('\u2014', '-')\n]]\n\n# List of (romaji, ipa) pairs:\n_romaji_to_ipa = [(re.compile('%s' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('\u0283y', '\u0283'),\n    ('\u02a7\u02b0y', '\u02a7\u02b0'),\n    ('\u02a7\u207cy', '\u02a7\u207c'),\n    ('NN', 'n'),", "    ('\u02a7\u207cy', '\u02a7\u207c'),\n    ('NN', 'n'),\n    ('Ng', '\u014b'),\n    ('y', 'j'),\n    ('h', 'x')\n]]\n\n# List of (bopomofo, ipa) pairs:\n_bopomofo_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\u3105\u311b', 'p\u207cwo'),", "_bopomofo_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\u3105\u311b', 'p\u207cwo'),\n    ('\u3106\u311b', 'p\u02b0wo'),\n    ('\u3107\u311b', 'mwo'),\n    ('\u3108\u311b', 'fwo'),\n    ('\u3105', 'p\u207c'),\n    ('\u3106', 'p\u02b0'),\n    ('\u3107', 'm'),\n    ('\u3108', 'f'),\n    ('\u3109', 't\u207c'),", "    ('\u3108', 'f'),\n    ('\u3109', 't\u207c'),\n    ('\u310a', 't\u02b0'),\n    ('\u310b', 'n'),\n    ('\u310c', 'l'),\n    ('\u310d', 'k\u207c'),\n    ('\u310e', 'k\u02b0'),\n    ('\u310f', 'x'),\n    ('\u3110', 't\u0283\u207c'),\n    ('\u3111', 't\u0283\u02b0'),", "    ('\u3110', 't\u0283\u207c'),\n    ('\u3111', 't\u0283\u02b0'),\n    ('\u3112', '\u0283'),\n    ('\u3113', 'ts`\u207c'),\n    ('\u3114', 'ts`\u02b0'),\n    ('\u3115', 's`'),\n    ('\u3116', '\u0279`'),\n    ('\u3117', 'ts\u207c'),\n    ('\u3118', 'ts\u02b0'),\n    ('\u3119', 's'),", "    ('\u3118', 'ts\u02b0'),\n    ('\u3119', 's'),\n    ('\u311a', 'a'),\n    ('\u311b', 'o'),\n    ('\u311c', '\u0259'),\n    ('\u311d', '\u025b'),\n    ('\u311e', 'a\u026a'),\n    ('\u311f', 'e\u026a'),\n    ('\u3120', '\u0251\u028a'),\n    ('\u3121', 'o\u028a'),", "    ('\u3120', '\u0251\u028a'),\n    ('\u3121', 'o\u028a'),\n    ('\u3127\u3122', 'j\u025bn'),\n    ('\u3129\u3122', '\u0265\u00e6n'),\n    ('\u3122', 'an'),\n    ('\u3127\u3123', 'in'),\n    ('\u3129\u3123', '\u0265n'),\n    ('\u3123', '\u0259n'),\n    ('\u3124', '\u0251\u014b'),\n    ('\u3127\u3125', 'i\u014b'),", "    ('\u3124', '\u0251\u014b'),\n    ('\u3127\u3125', 'i\u014b'),\n    ('\u3128\u3125', '\u028a\u014b'),\n    ('\u3129\u3125', 'j\u028a\u014b'),\n    ('\u3125', '\u0259\u014b'),\n    ('\u3126', '\u0259\u027b'),\n    ('\u3127', 'i'),\n    ('\u3128', 'u'),\n    ('\u3129', '\u0265'),\n    ('\u02c9', '\u2192'),", "    ('\u3129', '\u0265'),\n    ('\u02c9', '\u2192'),\n    ('\u02ca', '\u2191'),\n    ('\u02c7', '\u2193\u2191'),\n    ('\u02cb', '\u2193'),\n    ('\u02d9', ''),\n    ('\uff0c', ','),\n    ('\u3002', '.'),\n    ('\uff01', '!'),\n    ('\uff1f', '?'),", "    ('\uff01', '!'),\n    ('\uff1f', '?'),\n    ('\u2014', '-')\n]]\n\n# List of (bopomofo, ipa2) pairs:\n_bopomofo_to_ipa2 = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\u3105\u311b', 'pwo'),\n    ('\u3106\u311b', 'p\u02b0wo'),\n    ('\u3107\u311b', 'mwo'),", "    ('\u3106\u311b', 'p\u02b0wo'),\n    ('\u3107\u311b', 'mwo'),\n    ('\u3108\u311b', 'fwo'),\n    ('\u3105', 'p'),\n    ('\u3106', 'p\u02b0'),\n    ('\u3107', 'm'),\n    ('\u3108', 'f'),\n    ('\u3109', 't'),\n    ('\u310a', 't\u02b0'),\n    ('\u310b', 'n'),", "    ('\u310a', 't\u02b0'),\n    ('\u310b', 'n'),\n    ('\u310c', 'l'),\n    ('\u310d', 'k'),\n    ('\u310e', 'k\u02b0'),\n    ('\u310f', 'h'),\n    ('\u3110', 't\u0255'),\n    ('\u3111', 't\u0255\u02b0'),\n    ('\u3112', '\u0255'),\n    ('\u3113', 't\u0282'),", "    ('\u3112', '\u0255'),\n    ('\u3113', 't\u0282'),\n    ('\u3114', 't\u0282\u02b0'),\n    ('\u3115', '\u0282'),\n    ('\u3116', '\u027b'),\n    ('\u3117', 'ts'),\n    ('\u3118', 'ts\u02b0'),\n    ('\u3119', 's'),\n    ('\u311a', 'a'),\n    ('\u311b', 'o'),", "    ('\u311a', 'a'),\n    ('\u311b', 'o'),\n    ('\u311c', '\u0264'),\n    ('\u311d', '\u025b'),\n    ('\u311e', 'a\u026a'),\n    ('\u311f', 'e\u026a'),\n    ('\u3120', '\u0251\u028a'),\n    ('\u3121', 'o\u028a'),\n    ('\u3127\u3122', 'j\u025bn'),\n    ('\u3129\u3122', 'y\u00e6n'),", "    ('\u3127\u3122', 'j\u025bn'),\n    ('\u3129\u3122', 'y\u00e6n'),\n    ('\u3122', 'an'),\n    ('\u3127\u3123', 'in'),\n    ('\u3129\u3123', 'yn'),\n    ('\u3123', '\u0259n'),\n    ('\u3124', '\u0251\u014b'),\n    ('\u3127\u3125', 'i\u014b'),\n    ('\u3128\u3125', '\u028a\u014b'),\n    ('\u3129\u3125', 'j\u028a\u014b'),", "    ('\u3128\u3125', '\u028a\u014b'),\n    ('\u3129\u3125', 'j\u028a\u014b'),\n    ('\u3125', '\u0264\u014b'),\n    ('\u3126', '\u0259\u027b'),\n    ('\u3127', 'i'),\n    ('\u3128', 'u'),\n    ('\u3129', 'y'),\n    ('\u02c9', '\u02e5'),\n    ('\u02ca', '\u02e7\u02e5'),\n    ('\u02c7', '\u02e8\u02e9\u02e6'),", "    ('\u02ca', '\u02e7\u02e5'),\n    ('\u02c7', '\u02e8\u02e9\u02e6'),\n    ('\u02cb', '\u02e5\u02e9'),\n    ('\u02d9', ''),\n    ('\uff0c', ','),\n    ('\u3002', '.'),\n    ('\uff01', '!'),\n    ('\uff1f', '?'),\n    ('\u2014', '-')\n]]", "    ('\u2014', '-')\n]]\n\n\ndef number_to_chinese(text):\n    numbers = re.findall(r'\\d+(?:\\.?\\d+)?', text)\n    for number in numbers:\n        text = text.replace(number, cn2an.an2cn(number), 1)\n    return text\n", "\n\ndef chinese_to_bopomofo(text):\n    text = text.replace('\u3001', '\uff0c').replace('\uff1b', '\uff0c').replace('\uff1a', '\uff0c')\n    words = jieba.lcut(text, cut_all=False)\n    text = ''\n    for word in words:\n        bopomofos = lazy_pinyin(word, BOPOMOFO)\n        if not re.search('[\\u4e00-\\u9fff]', word):\n            text += word\n            continue\n        for i in range(len(bopomofos)):\n            bopomofos[i] = re.sub(r'([\\u3105-\\u3129])$', r'\\1\u02c9', bopomofos[i])\n        if text != '':\n            text += ' '\n        text += ''.join(bopomofos)\n    return text", "\n\ndef latin_to_bopomofo(text):\n    for regex, replacement in _latin_to_bopomofo:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef bopomofo_to_romaji(text):\n    for regex, replacement in _bopomofo_to_romaji:\n        text = re.sub(regex, replacement, text)\n    return text", "def bopomofo_to_romaji(text):\n    for regex, replacement in _bopomofo_to_romaji:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef bopomofo_to_ipa(text):\n    for regex, replacement in _bopomofo_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text", "\n\ndef bopomofo_to_ipa2(text):\n    for regex, replacement in _bopomofo_to_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef chinese_to_romaji(text):\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = bopomofo_to_romaji(text)\n    text = re.sub('i([aoe])', r'y\\1', text)\n    text = re.sub('u([ao\u0259e])', r'w\\1', text)\n    text = re.sub('([\u02a6s\u0279]`[\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)',\n                  r'\\1\u0279`\\2', text).replace('\u027b', '\u0279`')\n    text = re.sub('([\u02a6s][\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)', r'\\1\u0279\\2', text)\n    return text", "def chinese_to_romaji(text):\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = bopomofo_to_romaji(text)\n    text = re.sub('i([aoe])', r'y\\1', text)\n    text = re.sub('u([ao\u0259e])', r'w\\1', text)\n    text = re.sub('([\u02a6s\u0279]`[\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)',\n                  r'\\1\u0279`\\2', text).replace('\u027b', '\u0279`')\n    text = re.sub('([\u02a6s][\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)', r'\\1\u0279\\2', text)\n    return text", "\n\ndef chinese_to_lazy_ipa(text):\n    text = chinese_to_romaji(text)\n    for regex, replacement in _romaji_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef chinese_to_ipa(text):\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = bopomofo_to_ipa(text)\n    text = re.sub('i([aoe])', r'j\\1', text)\n    text = re.sub('u([ao\u0259e])', r'w\\1', text)\n    text = re.sub('([s\u0279]`[\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)',\n                  r'\\1\u0279`\\2', text).replace('\u027b', '\u0279`')\n    text = re.sub('([s][\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)', r'\\1\u0279\\2', text)\n    return text", "\ndef chinese_to_ipa(text):\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = bopomofo_to_ipa(text)\n    text = re.sub('i([aoe])', r'j\\1', text)\n    text = re.sub('u([ao\u0259e])', r'w\\1', text)\n    text = re.sub('([s\u0279]`[\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)',\n                  r'\\1\u0279`\\2', text).replace('\u027b', '\u0279`')\n    text = re.sub('([s][\u207c\u02b0]?)([\u2192\u2193\u2191 ]+|$)', r'\\1\u0279\\2', text)\n    return text", "\n\ndef chinese_to_ipa2(text):\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = bopomofo_to_ipa2(text)\n    text = re.sub(r'i([aoe])', r'j\\1', text)\n    text = re.sub(r'u([ao\u0259e])', r'w\\1', text)\n    text = re.sub(r'([\u0282\u0279]\u02b0?)([\u02e9\u02e8\u02e7\u02e6\u02e5 ]+|$)', r'\\1\u0285\\2', text)\n    text = re.sub(r'(s\u02b0?)([\u02e9\u02e8\u02e7\u02e6\u02e5 ]+|$)', r'\\1\u027f\\2', text)\n    return text", ""]}
{"filename": "text/korean.py", "chunked_list": ["import re\n\nimport ko_pron\nfrom jamo import h2j, j2hcj\n\n# This is a list of Korean classifiers preceded by pure Korean numerals.\n_korean_classifiers = '\uad70\ub370 \uad8c \uac1c \uadf8\ub8e8 \ub2e2 \ub300 \ub450 \ub9c8\ub9ac \ubaa8 \ubaa8\uae08 \ubb47 \ubc1c \ubc1c\uc9dd \ubc29 \ubc88 \ubc8c \ubcf4\ub8e8 \uc0b4 \uc218 \uc220 \uc2dc \uc308 \uc6c0\ud07c \uc815 \uc9dd \ucc44 \ucc99 \ucca9 \ucd95 \ucf24\ub808 \ud1a8 \ud1b5'\n\n# List of (hangul, hangul divided) pairs:\n_hangul_divided = [(re.compile('%s' % x[0]), x[1]) for x in [", "# List of (hangul, hangul divided) pairs:\n_hangul_divided = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\u3133', '\u3131\u3145'),\n    ('\u3135', '\u3134\u3148'),\n    ('\u3136', '\u3134\u314e'),\n    ('\u313a', '\u3139\u3131'),\n    ('\u313b', '\u3139\u3141'),\n    ('\u313c', '\u3139\u3142'),\n    ('\u313d', '\u3139\u3145'),\n    ('\u313e', '\u3139\u314c'),", "    ('\u313d', '\u3139\u3145'),\n    ('\u313e', '\u3139\u314c'),\n    ('\u313f', '\u3139\u314d'),\n    ('\u3140', '\u3139\u314e'),\n    ('\u3144', '\u3142\u3145'),\n    ('\u3158', '\u3157\u314f'),\n    ('\u3159', '\u3157\u3150'),\n    ('\u315a', '\u3157\u3163'),\n    ('\u315d', '\u315c\u3153'),\n    ('\u315e', '\u315c\u3154'),", "    ('\u315d', '\u315c\u3153'),\n    ('\u315e', '\u315c\u3154'),\n    ('\u315f', '\u315c\u3163'),\n    ('\u3162', '\u3161\u3163'),\n    ('\u3151', '\u3163\u314f'),\n    ('\u3152', '\u3163\u3150'),\n    ('\u3155', '\u3163\u3153'),\n    ('\u3156', '\u3163\u3154'),\n    ('\u315b', '\u3163\u3157'),\n    ('\u3160', '\u3163\u315c')", "    ('\u315b', '\u3163\u3157'),\n    ('\u3160', '\u3163\u315c')\n]]\n\n# List of (Latin alphabet, hangul) pairs:\n_latin_to_hangul = [(re.compile('%s' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('a', '\uc5d0\uc774'),\n    ('b', '\ube44'),\n    ('c', '\uc2dc'),\n    ('d', '\ub514'),", "    ('c', '\uc2dc'),\n    ('d', '\ub514'),\n    ('e', '\uc774'),\n    ('f', '\uc5d0\ud504'),\n    ('g', '\uc9c0'),\n    ('h', '\uc5d0\uc774\uce58'),\n    ('i', '\uc544\uc774'),\n    ('j', '\uc81c\uc774'),\n    ('k', '\ucf00\uc774'),\n    ('l', '\uc5d8'),", "    ('k', '\ucf00\uc774'),\n    ('l', '\uc5d8'),\n    ('m', '\uc5e0'),\n    ('n', '\uc5d4'),\n    ('o', '\uc624'),\n    ('p', '\ud53c'),\n    ('q', '\ud050'),\n    ('r', '\uc544\ub974'),\n    ('s', '\uc5d0\uc2a4'),\n    ('t', '\ud2f0'),", "    ('s', '\uc5d0\uc2a4'),\n    ('t', '\ud2f0'),\n    ('u', '\uc720'),\n    ('v', '\ube0c\uc774'),\n    ('w', '\ub354\ube14\uc720'),\n    ('x', '\uc5d1\uc2a4'),\n    ('y', '\uc640\uc774'),\n    ('z', '\uc81c\ud2b8')\n]]\n", "]]\n\n# List of (ipa, lazy ipa) pairs:\n_ipa_to_lazy_ipa = [(re.compile('%s' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('t\u0361\u0255','\u02a7'),\n    ('d\u0361\u0291','\u02a5'),\n    ('\u0272','n^'),\n    ('\u0255','\u0283'),\n    ('\u02b7','w'),\n    ('\u026d','l`'),", "    ('\u02b7','w'),\n    ('\u026d','l`'),\n    ('\u028e','\u027e'),\n    ('\u0263','\u014b'),\n    ('\u0270','\u026f'),\n    ('\u029d','j'),\n    ('\u028c','\u0259'),\n    ('\u0261','g'),\n    ('\\u031a','#'),\n    ('\\u0348','='),", "    ('\\u031a','#'),\n    ('\\u0348','='),\n    ('\\u031e',''),\n    ('\\u0320',''),\n    ('\\u0339','')\n]]\n\n\ndef latin_to_hangul(text):\n    for regex, replacement in _latin_to_hangul:\n        text = re.sub(regex, replacement, text)\n    return text", "def latin_to_hangul(text):\n    for regex, replacement in _latin_to_hangul:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef divide_hangul(text):\n    text = j2hcj(h2j(text))\n    for regex, replacement in _hangul_divided:\n        text = re.sub(regex, replacement, text)\n    return text", "\n\ndef hangul_number(num, sino=True):\n    '''Reference https://github.com/Kyubyong/g2pK'''\n    num = re.sub(',', '', num)\n\n    if num == '0':\n        return '\uc601'\n    if not sino and num == '20':\n        return '\uc2a4\ubb34'\n\n    digits = '123456789'\n    names = '\uc77c\uc774\uc0bc\uc0ac\uc624\uc721\uce60\ud314\uad6c'\n    digit2name = {d: n for d, n in zip(digits, names)}\n\n    modifiers = '\ud55c \ub450 \uc138 \ub124 \ub2e4\uc12f \uc5ec\uc12f \uc77c\uacf1 \uc5ec\ub35f \uc544\ud649'\n    decimals = '\uc5f4 \uc2a4\ubb3c \uc11c\ub978 \ub9c8\ud754 \uc270 \uc608\uc21c \uc77c\ud754 \uc5ec\ub4e0 \uc544\ud754'\n    digit2mod = {d: mod for d, mod in zip(digits, modifiers.split())}\n    digit2dec = {d: dec for d, dec in zip(digits, decimals.split())}\n\n    spelledout = []\n    for i, digit in enumerate(num):\n        i = len(num) - i - 1\n        if sino:\n            if i == 0:\n                name = digit2name.get(digit, '')\n            elif i == 1:\n                name = digit2name.get(digit, '') + '\uc2ed'\n                name = name.replace('\uc77c\uc2ed', '\uc2ed')\n        else:\n            if i == 0:\n                name = digit2mod.get(digit, '')\n            elif i == 1:\n                name = digit2dec.get(digit, '')\n        if digit == '0':\n            if i % 4 == 0:\n                last_three = spelledout[-min(3, len(spelledout)):]\n                if ''.join(last_three) == '':\n                    spelledout.append('')\n                    continue\n            else:\n                spelledout.append('')\n                continue\n        if i == 2:\n            name = digit2name.get(digit, '') + '\ubc31'\n            name = name.replace('\uc77c\ubc31', '\ubc31')\n        elif i == 3:\n            name = digit2name.get(digit, '') + '\ucc9c'\n            name = name.replace('\uc77c\ucc9c', '\ucc9c')\n        elif i == 4:\n            name = digit2name.get(digit, '') + '\ub9cc'\n            name = name.replace('\uc77c\ub9cc', '\ub9cc')\n        elif i == 5:\n            name = digit2name.get(digit, '') + '\uc2ed'\n            name = name.replace('\uc77c\uc2ed', '\uc2ed')\n        elif i == 6:\n            name = digit2name.get(digit, '') + '\ubc31'\n            name = name.replace('\uc77c\ubc31', '\ubc31')\n        elif i == 7:\n            name = digit2name.get(digit, '') + '\ucc9c'\n            name = name.replace('\uc77c\ucc9c', '\ucc9c')\n        elif i == 8:\n            name = digit2name.get(digit, '') + '\uc5b5'\n        elif i == 9:\n            name = digit2name.get(digit, '') + '\uc2ed'\n        elif i == 10:\n            name = digit2name.get(digit, '') + '\ubc31'\n        elif i == 11:\n            name = digit2name.get(digit, '') + '\ucc9c'\n        elif i == 12:\n            name = digit2name.get(digit, '') + '\uc870'\n        elif i == 13:\n            name = digit2name.get(digit, '') + '\uc2ed'\n        elif i == 14:\n            name = digit2name.get(digit, '') + '\ubc31'\n        elif i == 15:\n            name = digit2name.get(digit, '') + '\ucc9c'\n        spelledout.append(name)\n    return ''.join(elem for elem in spelledout)", "\n\ndef number_to_hangul(text):\n    '''Reference https://github.com/Kyubyong/g2pK'''\n    tokens = set(re.findall(r'(\\d[\\d,]*)([\\uac00-\\ud71f]+)', text))\n    for token in tokens:\n        num, classifier = token\n        if classifier[:2] in _korean_classifiers or classifier[0] in _korean_classifiers:\n            spelledout = hangul_number(num, sino=False)\n        else:\n            spelledout = hangul_number(num, sino=True)\n        text = text.replace(f'{num}{classifier}', f'{spelledout}{classifier}')\n    # digit by digit for remaining digits\n    digits = '0123456789'\n    names = '\uc601\uc77c\uc774\uc0bc\uc0ac\uc624\uc721\uce60\ud314\uad6c'\n    for d, n in zip(digits, names):\n        text = text.replace(d, n)\n    return text", "\n\ndef korean_to_lazy_ipa(text):\n    text = latin_to_hangul(text)\n    text = number_to_hangul(text)\n    text=re.sub('[\\uac00-\\ud7af]+',lambda x:ko_pron.romanise(x.group(0),'ipa').split('] ~ [')[0],text)\n    for regex, replacement in _ipa_to_lazy_ipa:\n        text = re.sub(regex, replacement, text)\n    return text\n", "\n\ndef korean_to_ipa(text):\n    text = korean_to_lazy_ipa(text)\n    return text.replace('\u02a7','t\u0283').replace('\u02a5','d\u0291')\n"]}
{"filename": "text/cantonese.py", "chunked_list": ["import re\n\nimport cn2an\nimport opencc\n\nconverter = opencc.OpenCC('jyutjyu')\n\n# List of (Latin alphabet, ipa) pairs:\n_latin_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('A', 'ei\u02e5'),", "_latin_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('A', 'ei\u02e5'),\n    ('B', 'bi\u02d0\u02e5'),\n    ('C', 'si\u02d0\u02e5'),\n    ('D', 'ti\u02d0\u02e5'),\n    ('E', 'i\u02d0\u02e5'),\n    ('F', 'e\u02e5fu\u02d0\u02e8\u02e9'),\n    ('G', 'tsi\u02d0\u02e5'),\n    ('H', '\u026ak\u031a\u02e5ts\u02b0y\u02d0\u02e8\u02e9'),\n    ('I', '\u0250i\u02e5'),", "    ('H', '\u026ak\u031a\u02e5ts\u02b0y\u02d0\u02e8\u02e9'),\n    ('I', '\u0250i\u02e5'),\n    ('J', 'tsei\u02e5'),\n    ('K', 'k\u02b0ei\u02e5'),\n    ('L', 'e\u02e5llou\u02e8\u02e9'),\n    ('M', '\u025b\u02d0m\u02e5'),\n    ('N', '\u025b\u02d0n\u02e5'),\n    ('O', 'ou\u02e5'),\n    ('P', 'p\u02b0i\u02d0\u02e5'),\n    ('Q', 'k\u02b0i\u02d0u\u02e5'),", "    ('P', 'p\u02b0i\u02d0\u02e5'),\n    ('Q', 'k\u02b0i\u02d0u\u02e5'),\n    ('R', 'a\u02d0\u02e5lou\u02e8\u02e9'),\n    ('S', '\u025b\u02d0\u02e5si\u02d0\u02e8\u02e9'),\n    ('T', 't\u02b0i\u02d0\u02e5'),\n    ('U', 'ju\u02d0\u02e5'),\n    ('V', 'wi\u02d0\u02e5'),\n    ('W', 't\u028ak\u031a\u02e5pi\u02d0\u02e5ju\u02d0\u02e5'),\n    ('X', '\u026ak\u031a\u02e5si\u02d0\u02e8\u02e9'),\n    ('Y', 'wa\u02d0i\u02e5'),", "    ('X', '\u026ak\u031a\u02e5si\u02d0\u02e8\u02e9'),\n    ('Y', 'wa\u02d0i\u02e5'),\n    ('Z', 'i\u02d0\u02e8s\u025b\u02d0t\u031a\u02e5')\n]]\n\n\ndef number_to_cantonese(text):\n    return re.sub(r'\\d+(?:\\.?\\d+)?', lambda x: cn2an.an2cn(x.group()), text)\n\n\ndef latin_to_ipa(text):\n    for regex, replacement in _latin_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text", "\n\ndef latin_to_ipa(text):\n    for regex, replacement in _latin_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef cantonese_to_ipa(text):\n    text = number_to_cantonese(text.upper())\n    text = converter.convert(text).replace('-','').replace('$',' ')\n    text = re.sub(r'[A-Z]', lambda x: latin_to_ipa(x.group())+' ', text)\n    text = re.sub(r'[\u3001\uff1b\uff1a]', '\uff0c', text)\n    text = re.sub(r'\\s*\uff0c\\s*', ', ', text)\n    text = re.sub(r'\\s*\u3002\\s*', '. ', text)\n    text = re.sub(r'\\s*\uff1f\\s*', '? ', text)\n    text = re.sub(r'\\s*\uff01\\s*', '! ', text)\n    text = re.sub(r'\\s*$', '', text)\n    return text", "def cantonese_to_ipa(text):\n    text = number_to_cantonese(text.upper())\n    text = converter.convert(text).replace('-','').replace('$',' ')\n    text = re.sub(r'[A-Z]', lambda x: latin_to_ipa(x.group())+' ', text)\n    text = re.sub(r'[\u3001\uff1b\uff1a]', '\uff0c', text)\n    text = re.sub(r'\\s*\uff0c\\s*', ', ', text)\n    text = re.sub(r'\\s*\u3002\\s*', '. ', text)\n    text = re.sub(r'\\s*\uff1f\\s*', '? ', text)\n    text = re.sub(r'\\s*\uff01\\s*', '! ', text)\n    text = re.sub(r'\\s*$', '', text)\n    return text", ""]}
{"filename": "text/sanskrit.py", "chunked_list": ["import re\n\nfrom indic_transliteration import sanscript\n\n# List of (iast, ipa) pairs:\n_iast_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('a', '\u0259'),\n    ('\u0101', 'a\u02d0'),\n    ('\u012b', 'i\u02d0'),\n    ('\u016b', 'u\u02d0'),", "    ('\u012b', 'i\u02d0'),\n    ('\u016b', 'u\u02d0'),\n    ('\u1e5b', '\u0279`'),\n    ('\u1e5d', '\u0279`\u02d0'),\n    ('\u1e37', 'l`'),\n    ('\u1e39', 'l`\u02d0'),\n    ('e', 'e\u02d0'),\n    ('o', 'o\u02d0'),\n    ('k', 'k\u207c'),\n    ('k\u207ch', 'k\u02b0'),", "    ('k', 'k\u207c'),\n    ('k\u207ch', 'k\u02b0'),\n    ('g', 'g\u207c'),\n    ('g\u207ch', 'g\u02b0'),\n    ('\u1e45', '\u014b'),\n    ('c', '\u02a7\u207c'),\n    ('\u02a7\u207ch', '\u02a7\u02b0'),\n    ('j', '\u02a5\u207c'),\n    ('\u02a5\u207ch', '\u02a5\u02b0'),\n    ('\u00f1', 'n^'),", "    ('\u02a5\u207ch', '\u02a5\u02b0'),\n    ('\u00f1', 'n^'),\n    ('\u1e6d', 't`\u207c'),\n    ('t`\u207ch', 't`\u02b0'),\n    ('\u1e0d', 'd`\u207c'),\n    ('d`\u207ch', 'd`\u02b0'),\n    ('\u1e47', 'n`'),\n    ('t', 't\u207c'),\n    ('t\u207ch', 't\u02b0'),\n    ('d', 'd\u207c'),", "    ('t\u207ch', 't\u02b0'),\n    ('d', 'd\u207c'),\n    ('d\u207ch', 'd\u02b0'),\n    ('p', 'p\u207c'),\n    ('p\u207ch', 'p\u02b0'),\n    ('b', 'b\u207c'),\n    ('b\u207ch', 'b\u02b0'),\n    ('y', 'j'),\n    ('\u015b', '\u0283'),\n    ('\u1e63', 's`'),", "    ('\u015b', '\u0283'),\n    ('\u1e63', 's`'),\n    ('r', '\u027e'),\n    ('l\u0324', 'l`'),\n    ('h', '\u0266'),\n    (\"'\", ''),\n    ('~', '^'),\n    ('\u1e43', '^')\n]]\n", "]]\n\n\ndef devanagari_to_ipa(text):\n    text = text.replace('\u0950', '\u0913\u092e\u094d')\n    text = re.sub(r'\\s*\u0964\\s*$', '.', text)\n    text = re.sub(r'\\s*\u0964\\s*', ', ', text)\n    text = re.sub(r'\\s*\u0965', '.', text)\n    text = sanscript.transliterate(text, sanscript.DEVANAGARI, sanscript.IAST)\n    for regex, replacement in _iast_to_ipa:\n        text = re.sub(regex, replacement, text)\n    text = re.sub('(.)[`\u02d0]*\u1e25', lambda x: x.group(0)\n                  [:-1]+'h'+x.group(1)+'*', text)\n    return text", ""]}
{"filename": "text/__init__.py", "chunked_list": ["\"\"\" from https://github.com/keithito/tacotron \"\"\"\nfrom text import cleaners\nfrom text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n\ndef text_to_sequence(text, symbols, cleaner_names):\n  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n    Returns:\n      List of integers corresponding to the symbols in the text\n  '''\n  sequence = []\n  symbol_to_id = {s: i for i, s in enumerate(symbols)}\n  clean_text = _clean_text(text, cleaner_names)\n  print(clean_text)\n  print(f\" length:{len(clean_text)}\")\n  for symbol in clean_text:\n    if symbol not in symbol_to_id.keys():\n      continue\n    symbol_id = symbol_to_id[symbol]\n    sequence += [symbol_id]\n  print(f\" length:{len(sequence)}\")\n  return sequence", "\n\ndef text_to_sequence(text, symbols, cleaner_names):\n  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n    Returns:\n      List of integers corresponding to the symbols in the text\n  '''\n  sequence = []\n  symbol_to_id = {s: i for i, s in enumerate(symbols)}\n  clean_text = _clean_text(text, cleaner_names)\n  print(clean_text)\n  print(f\" length:{len(clean_text)}\")\n  for symbol in clean_text:\n    if symbol not in symbol_to_id.keys():\n      continue\n    symbol_id = symbol_to_id[symbol]\n    sequence += [symbol_id]\n  print(f\" length:{len(sequence)}\")\n  return sequence", "\n\ndef cleaned_text_to_sequence(cleaned_text):\n  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n    Args:\n      text: string to convert to a sequence\n    Returns:\n      List of integers corresponding to the symbols in the text\n  '''\n  sequence = [_symbol_to_id[symbol] for symbol in cleaned_text if symbol in _symbol_to_id.keys()]\n  return sequence", "\n\ndef sequence_to_text(sequence):\n  '''Converts a sequence of IDs back to a string'''\n  result = ''\n  for symbol_id in sequence:\n    s = _id_to_symbol[symbol_id]\n    result += s\n  return result\n", "\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception('Unknown cleaner: %s' % name)\n    text = cleaner(text)\n  return text\n", ""]}
{"filename": "text/shanghainese.py", "chunked_list": ["import re\n\nimport cn2an\nimport opencc\n\nconverter = opencc.OpenCC('zaonhe')\n\n# List of (Latin alphabet, ipa) pairs:\n_latin_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('A', '\u1d07'),", "_latin_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('A', '\u1d07'),\n    ('B', 'bi'),\n    ('C', 'si'),\n    ('D', 'di'),\n    ('E', 'i'),\n    ('F', '\u1d07f'),\n    ('G', 'd\u0291i'),\n    ('H', '\u1d07t\u0255\u02b0'),\n    ('I', '\u1d00i'),", "    ('H', '\u1d07t\u0255\u02b0'),\n    ('I', '\u1d00i'),\n    ('J', 'd\u0291\u1d07'),\n    ('K', 'k\u02b0\u1d07'),\n    ('L', '\u1d07l'),\n    ('M', '\u1d07m'),\n    ('N', '\u1d07n'),\n    ('O', 'o'),\n    ('P', 'p\u02b0i'),\n    ('Q', 'k\u02b0iu'),", "    ('P', 'p\u02b0i'),\n    ('Q', 'k\u02b0iu'),\n    ('R', '\u1d00l'),\n    ('S', '\u1d07s'),\n    ('T', 't\u02b0i'),\n    ('U', '\u0266iu'),\n    ('V', 'vi'),\n    ('W', 'd\u1d00b\u0264liu'),\n    ('X', '\u1d07ks'),\n    ('Y', 'u\u1d00i'),", "    ('X', '\u1d07ks'),\n    ('Y', 'u\u1d00i'),\n    ('Z', 'z\u1d07')\n]]\n\n\ndef _number_to_shanghainese(num):\n    num = cn2an.an2cn(num).replace('\u4e00\u5341','\u5341').replace('\u4e8c\u5341', '\u5eff').replace('\u4e8c', '\u4e24')\n    return re.sub(r'((?:^|[^\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d])\u5341|\u5eff)\u4e24', r'\\1\u4e8c', num)\n", "\n\ndef number_to_shanghainese(text):\n    return re.sub(r'\\d+(?:\\.?\\d+)?', lambda x: _number_to_shanghainese(x.group()), text)\n\n\ndef latin_to_ipa(text):\n    for regex, replacement in _latin_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text", "\n\ndef shanghainese_to_ipa(text):\n    text = number_to_shanghainese(text.upper())\n    text = converter.convert(text).replace('-','').replace('$',' ')\n    text = re.sub(r'[A-Z]', lambda x: latin_to_ipa(x.group())+' ', text)\n    text = re.sub(r'[\u3001\uff1b\uff1a]', '\uff0c', text)\n    text = re.sub(r'\\s*\uff0c\\s*', ', ', text)\n    text = re.sub(r'\\s*\u3002\\s*', '. ', text)\n    text = re.sub(r'\\s*\uff1f\\s*', '? ', text)\n    text = re.sub(r'\\s*\uff01\\s*', '! ', text)\n    text = re.sub(r'\\s*$', '', text)\n    return text", ""]}
{"filename": "text/thai.py", "chunked_list": ["import re\n\nfrom num_thai.thainumbers import NumThai\n\nnum = NumThai()\n\n# List of (Latin alphabet, Thai) pairs:\n_latin_to_thai = [(re.compile('%s' % x[0], re.IGNORECASE), x[1]) for x in [\n    ('a', '\u0e40\u0e2d'),\n    ('b','\u0e1a\u0e35'),", "    ('a', '\u0e40\u0e2d'),\n    ('b','\u0e1a\u0e35'),\n    ('c','\u0e0b\u0e35'),\n    ('d','\u0e14\u0e35'),\n    ('e','\u0e2d\u0e35'),\n    ('f','\u0e40\u0e2d\u0e1f'),\n    ('g','\u0e08\u0e35'),\n    ('h','\u0e40\u0e2d\u0e0a'),\n    ('i','\u0e44\u0e2d'),\n    ('j','\u0e40\u0e08'),", "    ('i','\u0e44\u0e2d'),\n    ('j','\u0e40\u0e08'),\n    ('k','\u0e40\u0e04'),\n    ('l','\u0e41\u0e2d\u0e25'),\n    ('m','\u0e40\u0e2d\u0e47\u0e21'),\n    ('n','\u0e40\u0e2d\u0e47\u0e19'),\n    ('o','\u0e42\u0e2d'),\n    ('p','\u0e1e\u0e35'),\n    ('q','\u0e04\u0e34\u0e27'),\n    ('r','\u0e41\u0e2d\u0e23\u0e4c'),", "    ('q','\u0e04\u0e34\u0e27'),\n    ('r','\u0e41\u0e2d\u0e23\u0e4c'),\n    ('s','\u0e40\u0e2d\u0e2a'),\n    ('t','\u0e17\u0e35'),\n    ('u','\u0e22\u0e39'),\n    ('v','\u0e27\u0e35'),\n    ('w','\u0e14\u0e31\u0e1a\u0e40\u0e1a\u0e34\u0e25\u0e22\u0e39'),\n    ('x','\u0e40\u0e2d\u0e47\u0e01\u0e0b\u0e4c'),\n    ('y','\u0e27\u0e32\u0e22'),\n    ('z','\u0e0b\u0e35')", "    ('y','\u0e27\u0e32\u0e22'),\n    ('z','\u0e0b\u0e35')\n]]\n\n\ndef num_to_thai(text):\n    return re.sub(r'(?:\\d+(?:,?\\d+)?)+(?:\\.\\d+(?:,?\\d+)?)?', lambda x: ''.join(num.NumberToTextThai(float(x.group(0).replace(',', '')))), text)\n\ndef latin_to_thai(text):\n    for regex, replacement in _latin_to_thai:\n        text = re.sub(regex, replacement, text)\n    return text", "def latin_to_thai(text):\n    for regex, replacement in _latin_to_thai:\n        text = re.sub(regex, replacement, text)\n    return text\n"]}
{"filename": "text/symbols.py", "chunked_list": ["'''\nDefines the set of symbols used in text input to the model.\n'''\n\n# japanese_cleaners\n# _pad        = '_'\n# _punctuation = ',.!?-'\n# _letters = 'AEINOQUabdefghijkmnoprstuvwyz\u0283\u02a7\u2193\u2191 '\n\n", "\n\n'''# japanese_cleaners2\n_pad        = '_'\n_punctuation = ',.!?-~\u2026'\n_letters = 'AEINOQUabdefghijkmnoprstuvwyz\u0283\u02a7\u02a6\u2193\u2191 '\n'''\n\n\n'''# korean_cleaners", "\n'''# korean_cleaners\n_pad        = '_'\n_punctuation = ',.!?\u2026~'\n_letters = '\u3131\u3134\u3137\u3139\u3141\u3142\u3145\u3147\u3148\u314a\u314b\u314c\u314d\u314e\u3132\u3138\u3143\u3146\u3149\u314f\u3153\u3157\u315c\u3161\u3163\u3150\u3154 '\n'''\n\n'''# chinese_cleaners\n_pad        = '_'\n_punctuation = '\uff0c\u3002\uff01\uff1f\u2014\u2026'", "_pad        = '_'\n_punctuation = '\uff0c\u3002\uff01\uff1f\u2014\u2026'\n_letters = '\u3105\u3106\u3107\u3108\u3109\u310a\u310b\u310c\u310d\u310e\u310f\u3110\u3111\u3112\u3113\u3114\u3115\u3116\u3117\u3118\u3119\u311a\u311b\u311c\u311d\u311e\u311f\u3120\u3121\u3122\u3123\u3124\u3125\u3126\u3127\u3128\u3129\u02c9\u02ca\u02c7\u02cb\u02d9 '\n'''\n\n# # zh_ja_mixture_cleaners\n# _pad        = '_'\n# _punctuation = ',.!?-~\u2026'\n# _letters = 'AEINOQUabdefghijklmnoprstuvwyz\u0283\u02a7\u02a6\u026f\u0279\u0259\u0265\u207c\u02b0`\u2192\u2193\u2191 '\n", "# _letters = 'AEINOQUabdefghijklmnoprstuvwyz\u0283\u02a7\u02a6\u026f\u0279\u0259\u0265\u207c\u02b0`\u2192\u2193\u2191 '\n\n\n'''# sanskrit_cleaners\n_pad        = '_'\n_punctuation = '\u0964'\n_letters = '\u0901\u0902\u0903\u0905\u0906\u0907\u0908\u0909\u090a\u090b\u090f\u0910\u0913\u0914\u0915\u0916\u0917\u0918\u0919\u091a\u091b\u091c\u091d\u091e\u091f\u0920\u0921\u0922\u0923\u0924\u0925\u0926\u0927\u0928\u092a\u092b\u092c\u092d\u092e\u092f\u0930\u0932\u0933\u0935\u0936\u0937\u0938\u0939\u093d\u093e\u093f\u0940\u0941\u0942\u0943\u0944\u0947\u0948\u094b\u094c\u094d\u0960\u0962 '\n'''\n\n'''# cjks_cleaners", "\n'''# cjks_cleaners\n_pad        = '_'\n_punctuation = ',.!?-~\u2026'\n_letters = 'NQabdefghijklmnopstuvwxyz\u0283\u02a7\u02a5\u02a6\u026f\u0279\u0259\u0265\u00e7\u0278\u027e\u03b2\u014b\u0266\u02d0\u207c\u02b0`^#*=\u2192\u2193\u2191 '\n'''\n\n'''# thai_cleaners\n_pad        = '_'\n_punctuation = '.!? '", "_pad        = '_'\n_punctuation = '.!? '\n_letters = '\u0e01\u0e02\u0e03\u0e04\u0e06\u0e07\u0e08\u0e09\u0e0a\u0e0b\u0e0c\u0e0d\u0e0e\u0e0f\u0e10\u0e11\u0e12\u0e13\u0e14\u0e15\u0e16\u0e17\u0e18\u0e19\u0e1a\u0e1b\u0e1c\u0e1d\u0e1e\u0e1f\u0e20\u0e21\u0e22\u0e23\u0e24\u0e25\u0e27\u0e28\u0e29\u0e2a\u0e2b\u0e2c\u0e2d\u0e2e\u0e2f\u0e30\u0e31\u0e32\u0e33\u0e34\u0e35\u0e36\u0e37\u0e38\u0e39\u0e40\u0e41\u0e42\u0e43\u0e44\u0e45\u0e46\u0e47\u0e48\u0e49\u0e4a\u0e4b\u0e4c'\n'''\n\n# # cjke_cleaners2\n_pad        = '_'\n_punctuation = ',.!?-~\u2026'\n_letters = 'NQabdefghijklmnopstuvwxyz\u0251\u00e6\u0283\u0291\u00e7\u026f\u026a\u0254\u025b\u0279\u00f0\u0259\u026b\u0265\u0278\u028a\u027e\u0292\u03b8\u03b2\u014b\u0266\u207c\u02b0`^#*=\u02c8\u02cc\u2192\u2193\u2191 '\n", "_letters = 'NQabdefghijklmnopstuvwxyz\u0251\u00e6\u0283\u0291\u00e7\u026f\u026a\u0254\u025b\u0279\u00f0\u0259\u026b\u0265\u0278\u028a\u027e\u0292\u03b8\u03b2\u014b\u0266\u207c\u02b0`^#*=\u02c8\u02cc\u2192\u2193\u2191 '\n\n\n'''# shanghainese_cleaners\n_pad        = '_'\n_punctuation = ',.!?\u2026'\n_letters = 'abdfghiklmnopstuvyz\u00f8\u014b\u0235\u0251\u0254\u0255\u0259\u0264\u0266\u026a\u027f\u0291\u0294\u02b0\u0303\u0329\u1d00\u1d0715678 '\n'''\n\n'''# chinese_dialect_cleaners", "\n'''# chinese_dialect_cleaners\n_pad        = '_'\n_punctuation = ',.!?~\u2026\u2500'\n_letters = '#Nabdefghijklmnoprstuvwxyz\u00e6\u00e7\u00f8\u014b\u0153\u0235\u0250\u0251\u0252\u0253\u0254\u0255\u0257\u0258\u0259\u025a\u025b\u025c\u0263\u0264\u0266\u026a\u026d\u026f\u0275\u0277\u0278\u027b\u027e\u027f\u0282\u0285\u028a\u028b\u028c\u028f\u0291\u0294\u02a6\u02ae\u02b0\u02b7\u02c0\u02d0\u02e5\u02e6\u02e7\u02e8\u02e9\u0303\u031a\u0325\u0329\u1d00\u1d07\u2191\u2193\u2205\u2c7c '\n'''\n\n# Export all symbols:\nsymbols = [_pad] + list(_punctuation) + list(_letters)\n", "symbols = [_pad] + list(_punctuation) + list(_letters)\n\n# Special symbol ids\nSPACE_ID = symbols.index(\" \")\n"]}
{"filename": "text/ngu_dialect.py", "chunked_list": ["import re\n\nimport opencc\n\ndialects = {'SZ': 'suzhou', 'WX': 'wuxi', 'CZ': 'changzhou', 'HZ': 'hangzhou',\n            'SX': 'shaoxing', 'NB': 'ningbo', 'JJ': 'jingjiang', 'YX': 'yixing',\n            'JD': 'jiading', 'ZR': 'zhenru', 'PH': 'pinghu', 'TX': 'tongxiang',\n            'JS': 'jiashan', 'HN': 'xiashi', 'LP': 'linping', 'XS': 'xiaoshan',\n            'FY': 'fuyang', 'RA': 'ruao', 'CX': 'cixi', 'SM': 'sanmen',\n            'TT': 'tiantai', 'WZ': 'wenzhou', 'SC': 'suichang', 'YB': 'youbu'}", "            'FY': 'fuyang', 'RA': 'ruao', 'CX': 'cixi', 'SM': 'sanmen',\n            'TT': 'tiantai', 'WZ': 'wenzhou', 'SC': 'suichang', 'YB': 'youbu'}\n\nconverters = {}\n\nfor dialect in dialects.values():\n    try:\n        converters[dialect] = opencc.OpenCC(dialect)\n    except:\n        pass", "\n\ndef ngu_dialect_to_ipa(text, dialect):\n    dialect = dialects[dialect]\n    text = converters[dialect].convert(text).replace('-','').replace('$',' ')\n    text = re.sub(r'[\u3001\uff1b\uff1a]', '\uff0c', text)\n    text = re.sub(r'\\s*\uff0c\\s*', ', ', text)\n    text = re.sub(r'\\s*\u3002\\s*', '. ', text)\n    text = re.sub(r'\\s*\uff1f\\s*', '? ', text)\n    text = re.sub(r'\\s*\uff01\\s*', '! ', text)\n    text = re.sub(r'\\s*$', '', text)\n    return text", ""]}
{"filename": "text/cleaners.py", "chunked_list": ["import re\n\nfrom text.english import english_to_lazy_ipa, english_to_ipa2\nfrom text.japanese import japanese_to_romaji_with_accent, japanese_to_ipa, japanese_to_ipa2\nfrom text.korean import latin_to_hangul, number_to_hangul, divide_hangul, korean_to_lazy_ipa, korean_to_ipa\nfrom text.mandarin import number_to_chinese, chinese_to_bopomofo, latin_to_bopomofo, chinese_to_romaji, \\\n    chinese_to_lazy_ipa, chinese_to_ipa\nfrom text.sanskrit import devanagari_to_ipa\nfrom text.thai import num_to_thai, latin_to_thai\n", "from text.thai import num_to_thai, latin_to_thai\n\n\n# from text.shanghainese import shanghainese_to_ipa\n# from text.cantonese import cantonese_to_ipa\n# from text.ngu_dialect import ngu_dialect_to_ipa\n\n\ndef japanese_cleaners(text):\n    text = japanese_to_romaji_with_accent(text)\n    text = re.sub(r'([A-Za-z])$', r'\\1.', text)\n    return text", "def japanese_cleaners(text):\n    text = japanese_to_romaji_with_accent(text)\n    text = re.sub(r'([A-Za-z])$', r'\\1.', text)\n    return text\n\n\ndef japanese_cleaners2(text):\n    return japanese_cleaners(text).replace('ts', '\u02a6').replace('...', '\u2026')\n\n\ndef korean_cleaners(text):\n    '''Pipeline for Korean text'''\n    text = latin_to_hangul(text)\n    text = number_to_hangul(text)\n    text = divide_hangul(text)\n    text = re.sub(r'([\\u3131-\\u3163])$', r'\\1.', text)\n    return text", "\n\ndef korean_cleaners(text):\n    '''Pipeline for Korean text'''\n    text = latin_to_hangul(text)\n    text = number_to_hangul(text)\n    text = divide_hangul(text)\n    text = re.sub(r'([\\u3131-\\u3163])$', r'\\1.', text)\n    return text\n", "\n\ndef chinese_cleaners(text):\n    '''Pipeline for Chinese text'''\n    text = number_to_chinese(text)\n    text = chinese_to_bopomofo(text)\n    text = latin_to_bopomofo(text)\n    text = re.sub(r'([\u02c9\u02ca\u02c7\u02cb\u02d9])$', r'\\1\u3002', text)\n    return text\n", "\n\ndef zh_ja_mixture_cleaners(text):\n    text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]',\n                  lambda x: chinese_to_romaji(x.group(1))+' ', text)\n    text = re.sub(r'\\[JA\\](.*?)\\[JA\\]', lambda x: japanese_to_romaji_with_accent(\n        x.group(1)).replace('ts', '\u02a6').replace('u', '\u026f').replace('...', '\u2026')+' ', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n    return text", "\n\ndef sanskrit_cleaners(text):\n    text = text.replace('\u0965', '\u0964').replace('\u0950', '\u0913\u092e\u094d')\n    text = re.sub(r'([^\u0964])$', r'\\1\u0964', text)\n    return text\n\n\ndef cjks_cleaners(text):\n    text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]',\n                  lambda x: chinese_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[JA\\](.*?)\\[JA\\]',\n                  lambda x: japanese_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[KO\\](.*?)\\[KO\\]',\n                  lambda x: korean_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[SA\\](.*?)\\[SA\\]',\n                  lambda x: devanagari_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[EN\\](.*?)\\[EN\\]',\n                  lambda x: english_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n    return text", "def cjks_cleaners(text):\n    text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]',\n                  lambda x: chinese_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[JA\\](.*?)\\[JA\\]',\n                  lambda x: japanese_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[KO\\](.*?)\\[KO\\]',\n                  lambda x: korean_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[SA\\](.*?)\\[SA\\]',\n                  lambda x: devanagari_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[EN\\](.*?)\\[EN\\]',\n                  lambda x: english_to_lazy_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n    return text", "\n\ndef cjke_cleaners(text):\n    text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]', lambda x: chinese_to_lazy_ipa(x.group(1)).replace(\n        '\u02a7', 't\u0283').replace('\u02a6', 'ts').replace('\u0265an', '\u0265\u00e6n')+' ', text)\n    text = re.sub(r'\\[JA\\](.*?)\\[JA\\]', lambda x: japanese_to_ipa(x.group(1)).replace('\u02a7', 't\u0283').replace(\n        '\u02a6', 'ts').replace('\u0265an', '\u0265\u00e6n').replace('\u02a5', 'dz')+' ', text)\n    text = re.sub(r'\\[KO\\](.*?)\\[KO\\]',\n                  lambda x: korean_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[EN\\](.*?)\\[EN\\]', lambda x: english_to_ipa2(x.group(1)).replace('\u0251', 'a').replace(\n        '\u0254', 'o').replace('\u025b', 'e').replace('\u026a', 'i').replace('\u028a', 'u')+' ', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n    return text", "\n\ndef cjke_cleaners2(text):\n    text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]',\n                  lambda x: chinese_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[JA\\](.*?)\\[JA\\]',\n                  lambda x: japanese_to_ipa2(x.group(1))+' ', text)\n    text = re.sub(r'\\[KO\\](.*?)\\[KO\\]',\n                  lambda x: korean_to_ipa(x.group(1))+' ', text)\n    text = re.sub(r'\\[EN\\](.*?)\\[EN\\]',\n                  lambda x: english_to_ipa2(x.group(1))+' ', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n    return text", "\n\ndef thai_cleaners(text):\n    text = num_to_thai(text)\n    text = latin_to_thai(text)\n    return text\n\n\n# def shanghainese_cleaners(text):\n#     text = shanghainese_to_ipa(text)", "# def shanghainese_cleaners(text):\n#     text = shanghainese_to_ipa(text)\n#     text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n#     return text\n\n\n# def chinese_dialect_cleaners(text):\n#     text = re.sub(r'\\[ZH\\](.*?)\\[ZH\\]',\n#                   lambda x: chinese_to_ipa2(x.group(1))+' ', text)\n#     text = re.sub(r'\\[JA\\](.*?)\\[JA\\]',", "#                   lambda x: chinese_to_ipa2(x.group(1))+' ', text)\n#     text = re.sub(r'\\[JA\\](.*?)\\[JA\\]',\n#                   lambda x: japanese_to_ipa3(x.group(1)).replace('Q', '\u0294')+' ', text)\n#     text = re.sub(r'\\[SH\\](.*?)\\[SH\\]', lambda x: shanghainese_to_ipa(x.group(1)).replace('1', '\u02e5\u02e7').replace('5',\n#                   '\u02e7\u02e7\u02e6').replace('6', '\u02e9\u02e9\u02e7').replace('7', '\u02e5').replace('8', '\u02e9\u02e8').replace('\u1d00', '\u0250').replace('\u1d07', 'e')+' ', text)\n#     text = re.sub(r'\\[GD\\](.*?)\\[GD\\]',\n#                   lambda x: cantonese_to_ipa(x.group(1))+' ', text)\n#     text = re.sub(r'\\[EN\\](.*?)\\[EN\\]',\n#                   lambda x: english_to_lazy_ipa2(x.group(1))+' ', text)\n#     text = re.sub(r'\\[([A-Z]{2})\\](.*?)\\[\\1\\]', lambda x: ngu_dialect_to_ipa(x.group(2), x.group(", "#                   lambda x: english_to_lazy_ipa2(x.group(1))+' ', text)\n#     text = re.sub(r'\\[([A-Z]{2})\\](.*?)\\[\\1\\]', lambda x: ngu_dialect_to_ipa(x.group(2), x.group(\n#         1)).replace('\u02a3', 'dz').replace('\u02a5', 'd\u0291').replace('\u02a6', 'ts').replace('\u02a8', 't\u0255')+' ', text)\n#     text = re.sub(r'\\s+$', '', text)\n#     text = re.sub(r'([^\\.,!\\?\\-\u2026~])$', r'\\1.', text)\n#     return text\n"]}
{"filename": "text/japanese.py", "chunked_list": ["import re\n\nimport pyopenjtalk\nfrom unidecode import unidecode\n\n# Regular expression matching Japanese without punctuation marks:\n_japanese_characters = re.compile(\n    r'[A-Za-z\\d\\u3005\\u3040-\\u30ff\\u4e00-\\u9fff\\uff11-\\uff19\\uff21-\\uff3a\\uff41-\\uff5a\\uff66-\\uff9d]')\n\n# Regular expression matching non-Japanese characters or punctuation marks:", "\n# Regular expression matching non-Japanese characters or punctuation marks:\n_japanese_marks = re.compile(\n    r'[^A-Za-z\\d\\u3005\\u3040-\\u30ff\\u4e00-\\u9fff\\uff11-\\uff19\\uff21-\\uff3a\\uff41-\\uff5a\\uff66-\\uff9d]')\n\n# List of (symbol, Japanese) pairs for marks:\n_symbols_to_japanese = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('\uff05', '\u30d1\u30fc\u30bb\u30f3\u30c8')\n]]\n", "]]\n\n# List of (romaji, ipa) pairs for marks:\n_romaji_to_ipa = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('ts', '\u02a6'),\n    ('u', '\u026f'),\n    ('j', '\u02a5'),\n    ('y', 'j'),\n    ('ni', 'n^i'),\n    ('nj', 'n^'),", "    ('ni', 'n^i'),\n    ('nj', 'n^'),\n    ('hi', '\u00e7i'),\n    ('hj', '\u00e7'),\n    ('f', '\u0278'),\n    ('I', 'i*'),\n    ('U', '\u026f*'),\n    ('r', '\u027e')\n]]\n", "]]\n\n# List of (romaji, ipa2) pairs for marks:\n_romaji_to_ipa2 = [(re.compile('%s' % x[0]), x[1]) for x in [\n    ('u', '\u026f'),\n    ('\u02a7', 't\u0283'),\n    ('j', 'd\u0291'),\n    ('y', 'j'),\n    ('ni', 'n^i'),\n    ('nj', 'n^'),", "    ('ni', 'n^i'),\n    ('nj', 'n^'),\n    ('hi', '\u00e7i'),\n    ('hj', '\u00e7'),\n    ('f', '\u0278'),\n    ('I', 'i*'),\n    ('U', '\u026f*'),\n    ('r', '\u027e')\n]]\n", "]]\n\n# List of (consonant, sokuon) pairs:\n_real_sokuon = [(re.compile('%s' % x[0]), x[1]) for x in [\n    (r'Q([\u2191\u2193]*[kg])', r'k#\\1'),\n    (r'Q([\u2191\u2193]*[tdj\u02a7])', r't#\\1'),\n    (r'Q([\u2191\u2193]*[s\u0283])', r's\\1'),\n    (r'Q([\u2191\u2193]*[pb])', r'p#\\1')\n]]\n", "]]\n\n# List of (consonant, hatsuon) pairs:\n_real_hatsuon = [(re.compile('%s' % x[0]), x[1]) for x in [\n    (r'N([\u2191\u2193]*[pbm])', r'm\\1'),\n    (r'N([\u2191\u2193]*[\u02a7\u02a5j])', r'n^\\1'),\n    (r'N([\u2191\u2193]*[tdn])', r'n\\1'),\n    (r'N([\u2191\u2193]*[kg])', r'\u014b\\1')\n]]\n", "]]\n\n\ndef symbols_to_japanese(text):\n    for regex, replacement in _symbols_to_japanese:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef japanese_to_romaji_with_accent(text):\n    '''Reference https://r9y9.github.io/ttslearn/latest/notebooks/ch10_Recipe-Tacotron.html'''\n    text = symbols_to_japanese(text)\n    sentences = re.split(_japanese_marks, text)\n    marks = re.findall(_japanese_marks, text)\n    text = ''\n    for i, sentence in enumerate(sentences):\n        if re.match(_japanese_characters, sentence):\n            if text != '':\n                text += ' '\n            labels = pyopenjtalk.extract_fullcontext(sentence)\n            for n, label in enumerate(labels):\n                phoneme = re.search(r'\\-([^\\+]*)\\+', label).group(1)\n                if phoneme not in ['sil', 'pau']:\n                    text += phoneme.replace('ch', '\u02a7').replace('sh',\n                                                               '\u0283').replace('cl', 'Q')\n                else:\n                    continue\n                # n_moras = int(re.search(r'/F:(\\d+)_', label).group(1))\n                a1 = int(re.search(r\"/A:(\\-?[0-9]+)\\+\", label).group(1))\n                a2 = int(re.search(r\"\\+(\\d+)\\+\", label).group(1))\n                a3 = int(re.search(r\"\\+(\\d+)/\", label).group(1))\n                if re.search(r'\\-([^\\+]*)\\+', labels[n + 1]).group(1) in ['sil', 'pau']:\n                    a2_next = -1\n                else:\n                    a2_next = int(\n                        re.search(r\"\\+(\\d+)\\+\", labels[n + 1]).group(1))\n                # Accent phrase boundary\n                if a3 == 1 and a2_next == 1:\n                    text += ' '\n                # Falling\n                elif a1 == 0 and a2_next == a2 + 1:\n                    text += '\u2193'\n                # Rising\n                elif a2 == 1 and a2_next == 2:\n                    text += '\u2191'\n        if i < len(marks):\n            text += unidecode(marks[i]).replace(' ', '')\n    return text", "\ndef japanese_to_romaji_with_accent(text):\n    '''Reference https://r9y9.github.io/ttslearn/latest/notebooks/ch10_Recipe-Tacotron.html'''\n    text = symbols_to_japanese(text)\n    sentences = re.split(_japanese_marks, text)\n    marks = re.findall(_japanese_marks, text)\n    text = ''\n    for i, sentence in enumerate(sentences):\n        if re.match(_japanese_characters, sentence):\n            if text != '':\n                text += ' '\n            labels = pyopenjtalk.extract_fullcontext(sentence)\n            for n, label in enumerate(labels):\n                phoneme = re.search(r'\\-([^\\+]*)\\+', label).group(1)\n                if phoneme not in ['sil', 'pau']:\n                    text += phoneme.replace('ch', '\u02a7').replace('sh',\n                                                               '\u0283').replace('cl', 'Q')\n                else:\n                    continue\n                # n_moras = int(re.search(r'/F:(\\d+)_', label).group(1))\n                a1 = int(re.search(r\"/A:(\\-?[0-9]+)\\+\", label).group(1))\n                a2 = int(re.search(r\"\\+(\\d+)\\+\", label).group(1))\n                a3 = int(re.search(r\"\\+(\\d+)/\", label).group(1))\n                if re.search(r'\\-([^\\+]*)\\+', labels[n + 1]).group(1) in ['sil', 'pau']:\n                    a2_next = -1\n                else:\n                    a2_next = int(\n                        re.search(r\"\\+(\\d+)\\+\", labels[n + 1]).group(1))\n                # Accent phrase boundary\n                if a3 == 1 and a2_next == 1:\n                    text += ' '\n                # Falling\n                elif a1 == 0 and a2_next == a2 + 1:\n                    text += '\u2193'\n                # Rising\n                elif a2 == 1 and a2_next == 2:\n                    text += '\u2191'\n        if i < len(marks):\n            text += unidecode(marks[i]).replace(' ', '')\n    return text", "\n\ndef get_real_sokuon(text):\n    for regex, replacement in _real_sokuon:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef get_real_hatsuon(text):\n    for regex, replacement in _real_hatsuon:\n        text = re.sub(regex, replacement, text)\n    return text", "def get_real_hatsuon(text):\n    for regex, replacement in _real_hatsuon:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef japanese_to_ipa(text):\n    text = japanese_to_romaji_with_accent(text).replace('...', '\u2026')\n    text = re.sub(\n        r'([aiueo])\\1+', lambda x: x.group(0)[0]+'\u02d0'*(len(x.group(0))-1), text)\n    text = get_real_sokuon(text)\n    text = get_real_hatsuon(text)\n    for regex, replacement in _romaji_to_ipa:\n        text = re.sub(regex, replacement, text)\n    return text", "\n\ndef japanese_to_ipa2(text):\n    text = japanese_to_romaji_with_accent(text).replace('...', '\u2026')\n    text = get_real_sokuon(text)\n    text = get_real_hatsuon(text)\n    for regex, replacement in _romaji_to_ipa2:\n        text = re.sub(regex, replacement, text)\n    return text\n", "\n\ndef japanese_to_ipa3(text):\n    text = japanese_to_ipa2(text).replace('n^', '\u0235').replace(\n        '\u0283', '\u0255').replace('*', '\\u0325').replace('#', '\\u031a')\n    text = re.sub(\n        r'([ai\u026feo])\\1+', lambda x: x.group(0)[0]+'\u02d0'*(len(x.group(0))-1), text)\n    text = re.sub(r'((?:^|\\s)(?:ts|t\u0255|[kpt]))', r'\\1\u02b0', text)\n    return text\n", ""]}
{"filename": "monotonic_align/setup.py", "chunked_list": ["from distutils.core import setup\n\nimport numpy\nfrom Cython.Build import cythonize\n\nsetup(\n  name = 'monotonic_align',\n  ext_modules = cythonize(\"core.pyx\"),\n  include_dirs=[numpy.get_include()]\n)", "  include_dirs=[numpy.get_include()]\n)\n"]}
{"filename": "monotonic_align/__init__.py", "chunked_list": ["import numpy as np\nimport torch\n\nfrom .monotonic_align.core import maximum_path_c\n\n\ndef maximum_path(neg_cent, mask):\n  \"\"\" Cython optimized version.\n  neg_cent: [b, t_t, t_s]\n  mask: [b, t_t, t_s]\n  \"\"\"\n  device = neg_cent.device\n  dtype = neg_cent.dtype\n  neg_cent = neg_cent.data.cpu().numpy().astype(np.float32)\n  path = np.zeros(neg_cent.shape, dtype=np.int32)\n\n  t_t_max = mask.sum(1)[:, 0].data.cpu().numpy().astype(np.int32)\n  t_s_max = mask.sum(2)[:, 0].data.cpu().numpy().astype(np.int32)\n  maximum_path_c(path, neg_cent, t_t_max, t_s_max)\n  return torch.from_numpy(path).to(device=device, dtype=dtype)", ""]}
