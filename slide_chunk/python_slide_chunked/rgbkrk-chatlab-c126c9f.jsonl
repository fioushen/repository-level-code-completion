{"filename": "tests/test_chatlab.py", "chunked_list": ["# flake8: noqa\nimport toml\n\n# Casually import everything to shake out any import errors\nfrom chatlab import *\n\n\ndef test_chatlab():\n    from chatlab import __version__\n\n    # Check that __version__ is the same as pyproject.toml's version\n    pyproject_toml = toml.load(\"pyproject.toml\")\n    assert __version__ == pyproject_toml[\"tool\"][\"poetry\"][\"version\"]", ""]}
{"filename": "tests/test_messaging.py", "chunked_list": ["#!/usr/bin/env python\n\"\"\"Tests for `chatlab` package.\"\"\"\n\nfrom chatlab import assistant, system, user\nfrom chatlab.messaging import assistant_function_call, function_result\n\n\ndef test_assistant():\n    message = assistant(\"Hello!\")\n    assert message['role'] == 'assistant'\n    assert message['content'] == 'Hello!'", "\n\ndef test_user():\n    message = user(\"How are you?\")\n    assert message['role'] == 'user'\n    assert message['content'] == 'How are you?'\n\n\ndef test_system():\n    message = system(\"System message\")\n    assert message['role'] == 'system'\n    assert message['content'] == 'System message'", "def test_system():\n    message = system(\"System message\")\n    assert message['role'] == 'system'\n    assert message['content'] == 'System message'\n\n\ndef test_assistant_function_call():\n    message = assistant_function_call(\"func_name\", \"arg\")\n    assert message['role'] == 'assistant'\n    assert message['function_call']['name'] == 'func_name'\n    assert message['function_call']['arguments'] == 'arg'", "\n\ndef test_function_result():\n    message = function_result(\"func_name\", \"result\")\n    assert message['role'] == 'function'\n    assert message['name'] == 'func_name'\n    assert message['content'] == 'result'\n"]}
{"filename": "tests/test_builtins.py", "chunked_list": ["# flake8: noqa\nimport pytest\n\nfrom chatlab import FunctionRegistry\nfrom chatlab.builtins import os_functions\nfrom chatlab.builtins.files import get_file_size, is_directory, is_file, list_files, read_file, write_file\nfrom chatlab.builtins.shell import run_shell_command\n\n\ndef test_chat_function_adherence():\n    assert len(os_functions) > 0\n\n    for function in os_functions:\n        assert function.__name__ is not None and function.__name__ != \"\"\n        assert function.__doc__ is not None and function.__doc__ != \"\"\n\n        fr = FunctionRegistry()\n        schema = fr.register(function)\n        assert schema is not None", "\ndef test_chat_function_adherence():\n    assert len(os_functions) > 0\n\n    for function in os_functions:\n        assert function.__name__ is not None and function.__name__ != \"\"\n        assert function.__doc__ is not None and function.__doc__ != \"\"\n\n        fr = FunctionRegistry()\n        schema = fr.register(function)\n        assert schema is not None", "\n\n# TODO: Determine if this is part of the testing suite on Windows\nasync def test_run_shell_command():\n    command = \"echo Hello, ChatLab!\"\n    result = await run_shell_command(command)\n    assert \"Hello, ChatLab!\" in result\n\n    command = \"adsflkajsdg\"\n    result = await run_shell_command(command)", "    command = \"adsflkajsdg\"\n    result = await run_shell_command(command)\n    assert \"adsflkajsdg: command not found\" in result\n\n\n@pytest.mark.asyncio\nasync def test_list_files():\n    directory = \"chatlab/builtins\"\n    files = await list_files(directory)\n    assert isinstance(files, list)", "    files = await list_files(directory)\n    assert isinstance(files, list)\n    assert len(files) > 0\n\n\n@pytest.mark.asyncio\nasync def test_get_file_size():\n    file_path = \"chatlab/builtins/files.py\"\n    size = await get_file_size(file_path)\n    assert isinstance(size, int)", "    size = await get_file_size(file_path)\n    assert isinstance(size, int)\n    assert size > 0\n\n\n@pytest.mark.asyncio\nasync def test_is_file():\n    file_path = \"chatlab/builtins/files.py\"\n    assert await is_file(file_path)\n", "    assert await is_file(file_path)\n\n\n@pytest.mark.asyncio\nasync def test_is_directory():\n    directory = \"chatlab/builtins\"\n    assert await is_directory(directory)\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_write_and_read_file(tmp_path):\n    file_path = tmp_path / \"test_file.txt\"\n    content = \"Hello, ChatLab!\"\n\n    # Write content to file\n    await write_file(str(file_path), content)\n\n    # Read content from file", "\n    # Read content from file\n    result_content = await read_file(str(file_path))\n    assert result_content == content\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"Unit test package for chatlab.\"\"\"\n"]}
{"filename": "tests/test_views.py", "chunked_list": ["# flake8: noqa\nimport pytest\n\nfrom chatlab.views.argument_buffer import ArgumentBuffer\nfrom chatlab.views.assistant import AssistantMessageView\nfrom chatlab.views.assistant_function_call import AssistantFunctionCallView\nfrom chatlab.views.markdown import Markdown\n\n\ndef test_assistant_message_view_creation():\n    amv = AssistantMessageView()\n    assert isinstance(amv, AssistantMessageView)", "\ndef test_assistant_message_view_creation():\n    amv = AssistantMessageView()\n    assert isinstance(amv, AssistantMessageView)\n\n\ndef test_markdown_creation():\n    md = Markdown()\n    assert isinstance(md, Markdown)\n", "\n\ndef test_assistant_message_get():\n    amv = AssistantMessageView()\n    amv.append(\"test\")\n    message = amv.get_message()\n\n    assert message == {\n        \"role\": \"assistant\",\n        \"content\": \"test\",\n    }", "\n\ndef test_assistant_function_call_view_creation():\n    afcv = AssistantFunctionCallView(\"compute_pi\")\n    assert isinstance(afcv, AssistantFunctionCallView)\n\n\ndef test_assistant_function_call_view_get():\n    afcv = AssistantFunctionCallView(\"compute_pi\")\n    afcv.append(\"you can do it\")\n    message = afcv.get_message()\n\n    assert message == {\n        \"role\": \"assistant\",\n        \"content\": None,\n        'function_call': {\n            \"name\": \"compute_pi\",\n            \"arguments\": \"you can do it\",\n        },\n    }\n\n    assert afcv.finalize() == {\n        \"function_name\": \"compute_pi\",\n        \"function_arguments\": \"you can do it\",\n        \"display_id\": afcv.buffer._display_id,\n    }", "\n\ndef test_argument_buffer_initialization():\n    # Initializing the ArgumentBuffer object\n    arg_buffer = ArgumentBuffer(\"fun\")\n\n    assert arg_buffer.content == \"\"\n\n    arg_buffer.append(\"woo\")\n    arg_buffer.append(\"who\")\n    assert arg_buffer.content == \"woowho\"\n\n    arg_buffer._repr_mimebundle_()", "\n\ndef test_markdown_methods():\n    md = Markdown()\n\n    md.append(\"test\")\n    assert md.content == \"test\"\n    repr_md = repr(md)\n    assert repr_md == \"test\"\n\n    assert md.message == \"test\"\n    md.message = \"well alright\"\n\n    assert md.content == \"well alright\"\n\n    md2 = Markdown()\n    data, metadata = md2._repr_markdown_()\n\n    assert data == \" \"", "\n\ndef test_assistant_message_view_flush():\n    amv = AssistantMessageView(\"wahoo\")\n    amv.append(\"test\")\n    amv.flush()\n    assert amv.content == \"\"\n\n\ndef test_assistant_message_view_ipython_display():\n    amv = AssistantMessageView()\n    amv._ipython_display_()\n    assert amv.active", "\ndef test_assistant_message_view_ipython_display():\n    amv = AssistantMessageView()\n    amv._ipython_display_()\n    assert amv.active\n\n\ndef test_markdown_metadata():\n    md = Markdown()\n    assert md.metadata == {\"chatlab\": {\"default\": True}}", ""]}
{"filename": "tests/test_registry.py", "chunked_list": ["# flake8: noqa\nfrom unittest import mock\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\nfrom pydantic import BaseModel\n\nfrom chatlab.registry import FunctionArgumentError, FunctionRegistry, UnknownFunctionError, generate_function_schema\n\n", "\n\n# Define a function to use in testing\ndef simple_func(x: int, y: str, z: bool = False):\n    \"\"\"A simple test function\"\"\"\n    return f\"{x}, {y}, {z}\"\n\n\nclass SimpleModel(BaseModel):\n    x: int\n    y: str\n    z: bool = False", "class SimpleModel(BaseModel):\n    x: int\n    y: str\n    z: bool = False\n\n\n# Test the function generation schema\ndef test_generate_function_schema_lambda():\n    with pytest.raises(Exception, match=\"Lambdas cannot be registered. Use `def` instead.\"):\n        generate_function_schema(lambda x: x)", "\n\ndef test_generate_function_schema_no_docstring():\n    def no_docstring(x: int):\n        return x\n\n    with pytest.raises(Exception, match=\"Only functions with docstrings can be registered\"):\n        generate_function_schema(no_docstring)\n\n\ndef test_generate_function_schema_no_type_annotation():\n    def no_type_annotation(x):\n        \"\"\"Return back x\"\"\"\n        return x\n\n    with pytest.raises(Exception, match=\"Parameter x of function no_type_annotation must have a type annotation\"):\n        generate_function_schema(no_type_annotation)", "\n\ndef test_generate_function_schema_no_type_annotation():\n    def no_type_annotation(x):\n        \"\"\"Return back x\"\"\"\n        return x\n\n    with pytest.raises(Exception, match=\"Parameter x of function no_type_annotation must have a type annotation\"):\n        generate_function_schema(no_type_annotation)\n", "\n\ndef test_generate_function_schema_unallowed_type():\n    def unallowed_type(x: set):\n        '''Return back x'''\n        return x\n\n    with pytest.raises(\n        Exception, match=\"Type annotation of parameter x in function unallowed_type must be a JSON serializable type\"\n    ):\n        generate_function_schema(unallowed_type)", "\n\ndef test_generate_function_schema():\n    schema = generate_function_schema(simple_func)\n    expected_schema = {\n        \"name\": \"simple_func\",\n        \"description\": \"A simple test function\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"type\": \"integer\"},\n                \"y\": {\"type\": \"string\"},\n                \"z\": {\"type\": \"boolean\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    }\n    assert schema == expected_schema", "\n\ndef test_generate_function_schema_with_model():\n    schema = generate_function_schema(simple_func, SimpleModel)\n    expected_schema = {\n        \"name\": \"simple_func\",\n        \"description\": \"A simple test function\",\n        \"parameters\": SimpleModel.schema(),\n    }\n    assert schema == expected_schema", "\n\n# Test the function registry\n@pytest.mark.asyncio\nasync def test_function_registry_unknown_function():\n    registry = FunctionRegistry()\n    with pytest.raises(UnknownFunctionError, match=\"Function unknown is not registered\"):\n        await registry.call(\"unknown\")\n\n", "\n\n@pytest.mark.asyncio\nasync def test_function_registry_function_argument_error():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    with pytest.raises(\n        FunctionArgumentError, match=\"Invalid Function call on simple_func. Arguments must be a valid JSON object\"\n    ):\n        await registry.call(\"simple_func\", arguments=\"not json\")", "\n\n@pytest.mark.asyncio\nasync def test_function_registry_call():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    result = await registry.call(\"simple_func\", arguments='{\"x\": 1, \"y\": \"str\", \"z\": true}')\n    assert result == \"1, str, True\"\n\n", "\n\n# Testing for registry's register method with an invalid function\ndef test_function_registry_register_invalid_function():\n    registry = FunctionRegistry()\n    with pytest.raises(Exception, match=\"Lambdas cannot be registered. Use `def` instead.\"):\n        registry.register(lambda x: x)\n\n\n# Testing for registry's get method\ndef test_function_registry_get():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    assert registry.get(\"simple_func\") == simple_func", "\n# Testing for registry's get method\ndef test_function_registry_get():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    assert registry.get(\"simple_func\") == simple_func\n\n\n# Testing for registry's __contains__ method\ndef test_function_registry_contains():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    assert \"simple_func\" in registry\n    assert \"unknown\" not in registry", "# Testing for registry's __contains__ method\ndef test_function_registry_contains():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    assert \"simple_func\" in registry\n    assert \"unknown\" not in registry\n\n\n# Testing for registry's function_definitions property\ndef test_function_registry_function_definitions():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    function_definitions = registry.function_definitions\n    assert len(function_definitions) == 1\n    assert function_definitions[0][\"name\"] == \"simple_func\"", "# Testing for registry's function_definitions property\ndef test_function_registry_function_definitions():\n    registry = FunctionRegistry()\n    registry.register(simple_func, SimpleModel)\n    function_definitions = registry.function_definitions\n    assert len(function_definitions) == 1\n    assert function_definitions[0][\"name\"] == \"simple_func\"\n\n\n# Test that we do not allow python hallucination when False", "\n# Test that we do not allow python hallucination when False\n@pytest.mark.asyncio\nasync def test_function_registry_call_python_hallucination_invalid():\n    registry = FunctionRegistry(python_hallucination_function=None)\n    with pytest.raises(Exception, match=\"Function python is not registered\"):\n        await registry.call(\"python\", arguments='1 + 4')\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_ensure_python_hallucination_not_enabled_by_default():\n    registry = FunctionRegistry()\n    with pytest.raises(Exception, match=\"Function python is not registered\"):\n        await registry.call(\"python\", arguments='123 + 456')\n\n\n# Test the generate_function_schema for function with optional arguments\ndef test_generate_function_schema_optional_args():\n    def func_with_optional_args(x: int, y: str, z: bool = False):\n        '''A function with optional arguments'''\n        return f\"{x}, {y}, {z}\"\n\n    schema = generate_function_schema(func_with_optional_args)\n    assert \"z\" in schema[\"parameters\"][\"properties\"]\n    assert \"z\" not in schema[\"parameters\"][\"required\"]", "# Test the generate_function_schema for function with optional arguments\ndef test_generate_function_schema_optional_args():\n    def func_with_optional_args(x: int, y: str, z: bool = False):\n        '''A function with optional arguments'''\n        return f\"{x}, {y}, {z}\"\n\n    schema = generate_function_schema(func_with_optional_args)\n    assert \"z\" in schema[\"parameters\"][\"properties\"]\n    assert \"z\" not in schema[\"parameters\"][\"required\"]\n", "\n\n# Test the generate_function_schema for function with no arguments\ndef test_generate_function_schema_no_args():\n    def func_no_args():\n        \"\"\"A function with no arguments\"\"\"\n        pass\n\n    schema = generate_function_schema(func_no_args)\n    assert schema[\"parameters\"][\"properties\"] == {}\n    assert schema[\"parameters\"][\"required\"] == []", "\n\n# Testing edge cases with call method\n@pytest.mark.asyncio\nasync def test_function_registry_call_edge_cases():\n    registry = FunctionRegistry()\n    with pytest.raises(UnknownFunctionError):\n        await registry.call(\"totes_not_real\", arguments='{\"x\": 1, \"y\": \"str\", \"z\": true}')\n\n    with pytest.raises(UnknownFunctionError):\n        await registry.call(None)  # type: ignore", "\n    with pytest.raises(UnknownFunctionError):\n        await registry.call(None)  # type: ignore\n"]}
{"filename": "tests/test_decorators.py", "chunked_list": ["# flake8: noqa\nimport pytest\n\nfrom chatlab.decorators import ChatlabMetadata, expose_exception_to_llm\n\n\nclass MyException(Exception):\n    pass\n\n", "\n\n@expose_exception_to_llm\ndef raise_exception():\n    \"\"\"This function just raises an exception.\"\"\"\n    raise MyException(\"test exception\")\n\n\ndef no_exception():\n    \"\"\"This function does not raise an exception.\"\"\"\n    return \"No exception here!\"", "def no_exception():\n    \"\"\"This function does not raise an exception.\"\"\"\n    return \"No exception here!\"\n\n\ndef test_expose_exception_to_llm_decorator():\n    \"\"\"Tests the expose_exception_to_llm decorator.\"\"\"\n    assert isinstance(raise_exception.chatlab_metadata, ChatlabMetadata)\n    assert raise_exception.chatlab_metadata.expose_exception_to_llm == True\n", "\n\ndef test_no_decorator():\n    \"\"\"Tests a function without the decorator.\"\"\"\n    assert not hasattr(no_exception, 'chatlab_metadata')\n\n\ndef test_decorator_raises_exception():\n    \"\"\"Tests that the decorator raises an exception when chatlab_metadata is not an instance of ChatlabMetadata.\"\"\"\n\n    def func():\n        pass\n\n    func.chatlab_metadata = \"Not an instance of ChatlabMetadata\"\n\n    with pytest.raises(Exception):\n        expose_exception_to_llm(func)", ""]}
{"filename": "chatlab/decorators.py", "chunked_list": ["\"\"\"ChatLab decorators.\n\nThis module lets you augment your functions before you register them with a ChatLab conversation.\n\nExamples:\n    >>> from chatlab import Chat\n    >>> from chatlab.decorators import expose_exception_to_llm\n\n    >>> class PokemonFetchError(Exception):\n    ...   def __init__(self, pokemon_name):", "    >>> class PokemonFetchError(Exception):\n    ...   def __init__(self, pokemon_name):\n    ...     self.pokemon_name = pokemon_name\n    ...     self.message = f\"Failed to fetch information for Pokemon '{self.pokemon_name}'.\"\n    ...     super().__init__(self.message)\n\n    >>> @expose_exception_to_llm\n    ... def fetch_pokemon(name: str):\n    ...     '''Fetch information about a pokemon by name'''\n    ...     url = f\"https://pokeapi.co/api/v2/pokemon/{name}\"", "    ...     '''Fetch information about a pokemon by name'''\n    ...     url = f\"https://pokeapi.co/api/v2/pokemon/{name}\"\n    ...     try:\n    ...         response = requests.get(url)\n    ...         response.raise_for_status()\n    ...         return response.json()\n    ...     except requests.HTTPError:\n    ...         raise PokemonFetchError(name)\n\n    >>> conversation = Chat()", "\n    >>> conversation = Chat()\n    >>> conversation.submit(\"Get pikachu\")\n    Failed to fetch information for Pokemon 'pikachu'.\n\n\"\"\"\n\n\nclass ChatlabMetadata:\n    \"\"\"ChatLab metadata for a function.\"\"\"\n\n    expose_exception_to_llm: bool\n\n    def __init__(self, expose_exception_to_llm=False):\n        \"\"\"Initialize ChatLab metadata for a function.\"\"\"\n        self.expose_exception_to_llm = expose_exception_to_llm", "class ChatlabMetadata:\n    \"\"\"ChatLab metadata for a function.\"\"\"\n\n    expose_exception_to_llm: bool\n\n    def __init__(self, expose_exception_to_llm=False):\n        \"\"\"Initialize ChatLab metadata for a function.\"\"\"\n        self.expose_exception_to_llm = expose_exception_to_llm\n\n\ndef expose_exception_to_llm(func):\n    \"\"\"Expose exceptions from calling the function to the LLM.\n\n    Args:\n        func (Callable): The function to annotate.\n\n    Examples:\n        >>> import chatlab\n        >>> from chatlab.decorators import expose_exception_to_llm\n\n        >>> @expose_exception_to_llm\n        ... def roll_die():\n        ...     roll = random.randint(1, 6)\n        ...     if roll == 1:\n        ...         raise Exception(\"The die rolled a 1!\")\n        ...     return roll\n        >>> conversation = chatlab.Chat()\n        >>> conversation.submit(\"Roll the dice!\")\n        The die rolled a 1!\n\n    \"\"\"\n    if not hasattr(func, 'chatlab_metadata'):\n        func.chatlab_metadata = ChatlabMetadata()\n\n    # Make sure that chatlab_metadata is an instance of ChatlabMetadata\n    if not isinstance(func.chatlab_metadata, ChatlabMetadata):\n        raise Exception(\"func.chatlab_metadata must be an instance of ChatlabMetadata\")\n\n    func.chatlab_metadata.expose_exception_to_llm = True\n    return func", "\n\ndef expose_exception_to_llm(func):\n    \"\"\"Expose exceptions from calling the function to the LLM.\n\n    Args:\n        func (Callable): The function to annotate.\n\n    Examples:\n        >>> import chatlab\n        >>> from chatlab.decorators import expose_exception_to_llm\n\n        >>> @expose_exception_to_llm\n        ... def roll_die():\n        ...     roll = random.randint(1, 6)\n        ...     if roll == 1:\n        ...         raise Exception(\"The die rolled a 1!\")\n        ...     return roll\n        >>> conversation = chatlab.Chat()\n        >>> conversation.submit(\"Roll the dice!\")\n        The die rolled a 1!\n\n    \"\"\"\n    if not hasattr(func, 'chatlab_metadata'):\n        func.chatlab_metadata = ChatlabMetadata()\n\n    # Make sure that chatlab_metadata is an instance of ChatlabMetadata\n    if not isinstance(func.chatlab_metadata, ChatlabMetadata):\n        raise Exception(\"func.chatlab_metadata must be an instance of ChatlabMetadata\")\n\n    func.chatlab_metadata.expose_exception_to_llm = True\n    return func", ""]}
{"filename": "chatlab/registry.py", "chunked_list": ["\"\"\"Registry of functions for use by ChatCompletions.\n\nExample usage:\n\n    from chatlab import FunctionRegistry\n    from pydantic import BaseModel\n\n    registry = FunctionRegistry()\n\n    class Parameters(BaseModel):", "\n    class Parameters(BaseModel):\n        name: str\n\n    from datetime import datetime\n    from pytz import timezone, all_timezones, utc\n    from typing import Optional\n    from pydantic import BaseModel\n\n    def what_time(tz: Optional[str] = None):", "\n    def what_time(tz: Optional[str] = None):\n        '''Current time, defaulting to the user's current timezone'''\n        if tz is None:\n            pass\n        elif tz in all_timezones:\n            tz = timezone(tz)\n        else:\n            return 'Invalid timezone'\n        return datetime.now(tz).strftime('%I:%M %p')", "            return 'Invalid timezone'\n        return datetime.now(tz).strftime('%I:%M %p')\n\n    class WhatTime(BaseModel):\n        timezone: Optional[str]\n\n    import chatlab\n    registry = chatlab.FunctionRegistry()\n\n    conversation = chatlab.Chat(", "\n    conversation = chatlab.Chat(\n        function_registry=registry,\n    )\n\n    conversation.submit(\"What time is it?\")\n\n\"\"\"\n\nimport asyncio", "\nimport asyncio\nimport inspect\nimport json\nfrom typing import Any, Callable, Iterable, Optional, Type, Union, get_args, get_origin\n\nfrom pydantic import BaseModel\n\nfrom .decorators import ChatlabMetadata\n", "from .decorators import ChatlabMetadata\n\n\nclass FunctionArgumentError(Exception):\n    \"\"\"Exception raised when a function is called with invalid arguments.\"\"\"\n\n    pass\n\n\nclass UnknownFunctionError(Exception):\n    \"\"\"Exception raised when a function is called that is not registered.\"\"\"\n\n    pass", "\nclass UnknownFunctionError(Exception):\n    \"\"\"Exception raised when a function is called that is not registered.\"\"\"\n\n    pass\n\n\n# Allowed types for auto-inferred schemas\nALLOWED_TYPES = [int, str, bool, float, list, dict]\n", "ALLOWED_TYPES = [int, str, bool, float, list, dict]\n\nJSON_SCHEMA_TYPES = {\n    int: 'integer',\n    float: 'number',\n    str: 'string',\n    bool: 'boolean',\n    list: 'array',\n    dict: 'object',\n}", "    dict: 'object',\n}\n\n\ndef is_optional_type(t):\n    \"\"\"Check if a type is Optional.\"\"\"\n    return get_origin(t) is Union and len(get_args(t)) == 2 and type(None) in get_args(t)\n\n\ndef generate_function_schema(\n    function: Callable,\n    parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None,\n):\n    \"\"\"Generate a function schema for sending to OpenAI.\"\"\"\n    doc = function.__doc__\n    func_name = function.__name__\n\n    if not func_name:\n        raise Exception(\"Function must have a name\")\n    if func_name == \"<lambda>\":\n        raise Exception(\"Lambdas cannot be registered. Use `def` instead.\")\n    if not doc:\n        raise Exception(\"Only functions with docstrings can be registered\")\n\n    schema = None\n    if isinstance(parameter_schema, dict):\n        schema = parameter_schema\n    elif parameter_schema is not None:\n        schema = parameter_schema.schema()\n    else:\n        schema_properties = {}\n        sig = inspect.signature(function)\n        for name, param in sig.parameters.items():\n            if param.annotation == inspect.Parameter.empty:\n                raise Exception(f\"Parameter {name} of function {func_name} must have a type annotation\")\n\n            if is_optional_type(param.annotation):\n                actual_type = get_args(param.annotation)[0]\n\n                if actual_type not in ALLOWED_TYPES:\n                    raise Exception(\n                        f\"Type annotation of parameter {name} in function {func_name} \"\n                        f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n                    )\n\n                schema_properties[name] = {\n                    \"type\": JSON_SCHEMA_TYPES[actual_type],\n                }\n\n            elif param.annotation in ALLOWED_TYPES:\n                schema_properties[name] = {\n                    \"type\": JSON_SCHEMA_TYPES[param.annotation],\n                }\n\n            else:\n                raise Exception(\n                    f\"Type annotation of parameter {name} in function {func_name} \"\n                    f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n                )\n\n        schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n        if len(schema_properties) > 0:\n            schema = {\n                \"type\": \"object\",\n                \"properties\": schema_properties,\n                \"required\": [\n                    name\n                    for name, param in sig.parameters.items()\n                    if param.default == inspect.Parameter.empty and param.annotation != Optional\n                ],\n            }\n\n    if schema is None:\n        raise Exception(f\"Could not generate schema for function {func_name}\")\n\n    return {\n        \"name\": func_name,\n        \"description\": doc,\n        \"parameters\": schema,\n    }", "\ndef generate_function_schema(\n    function: Callable,\n    parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None,\n):\n    \"\"\"Generate a function schema for sending to OpenAI.\"\"\"\n    doc = function.__doc__\n    func_name = function.__name__\n\n    if not func_name:\n        raise Exception(\"Function must have a name\")\n    if func_name == \"<lambda>\":\n        raise Exception(\"Lambdas cannot be registered. Use `def` instead.\")\n    if not doc:\n        raise Exception(\"Only functions with docstrings can be registered\")\n\n    schema = None\n    if isinstance(parameter_schema, dict):\n        schema = parameter_schema\n    elif parameter_schema is not None:\n        schema = parameter_schema.schema()\n    else:\n        schema_properties = {}\n        sig = inspect.signature(function)\n        for name, param in sig.parameters.items():\n            if param.annotation == inspect.Parameter.empty:\n                raise Exception(f\"Parameter {name} of function {func_name} must have a type annotation\")\n\n            if is_optional_type(param.annotation):\n                actual_type = get_args(param.annotation)[0]\n\n                if actual_type not in ALLOWED_TYPES:\n                    raise Exception(\n                        f\"Type annotation of parameter {name} in function {func_name} \"\n                        f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n                    )\n\n                schema_properties[name] = {\n                    \"type\": JSON_SCHEMA_TYPES[actual_type],\n                }\n\n            elif param.annotation in ALLOWED_TYPES:\n                schema_properties[name] = {\n                    \"type\": JSON_SCHEMA_TYPES[param.annotation],\n                }\n\n            else:\n                raise Exception(\n                    f\"Type annotation of parameter {name} in function {func_name} \"\n                    f\"must be a JSON serializable type ({ALLOWED_TYPES})\"\n                )\n\n        schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n        if len(schema_properties) > 0:\n            schema = {\n                \"type\": \"object\",\n                \"properties\": schema_properties,\n                \"required\": [\n                    name\n                    for name, param in sig.parameters.items()\n                    if param.default == inspect.Parameter.empty and param.annotation != Optional\n                ],\n            }\n\n    if schema is None:\n        raise Exception(f\"Could not generate schema for function {func_name}\")\n\n    return {\n        \"name\": func_name,\n        \"description\": doc,\n        \"parameters\": schema,\n    }", "\n\n# Declare the type for the python hallucination\nPythonHallucinationFunction = Callable[[str], Any]\n\n\nclass FunctionRegistry:\n    \"\"\"Captures a function with schema both for sending to OpenAI and for executing locally.\"\"\"\n\n    __functions: dict[str, Callable]\n    __schemas: dict[str, dict]\n\n    # Allow passing in a callable that accepts a single string for the python\n    # hallucination function. This is useful for testing.\n    def __init__(self, python_hallucination_function: Optional[PythonHallucinationFunction] = None):\n        \"\"\"Initialize a FunctionRegistry object.\"\"\"\n        self.__functions = {}\n        self.__schemas = {}\n\n        self.python_hallucination_function = python_hallucination_function\n\n    def register(\n        self,\n        function: Callable,\n        parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None,\n    ) -> dict:\n        \"\"\"Register a function for use in `Chat`s.\"\"\"\n        final_schema = generate_function_schema(function, parameter_schema)\n\n        self.__functions[function.__name__] = function\n        self.__schemas[function.__name__] = final_schema\n\n        return final_schema\n\n    def register_functions(self, functions: Union[Iterable[Callable], dict[str, Callable]]):\n        \"\"\"Register a dictionary of functions.\"\"\"\n        if isinstance(functions, dict):\n            functions = functions.values()\n\n        for function in functions:\n            self.register(function)\n\n    def get(self, function_name) -> Optional[Callable]:\n        \"\"\"Get a function by name.\"\"\"\n        if function_name == \"python\" and self.python_hallucination_function is not None:\n            return self.python_hallucination_function\n\n        return self.__functions.get(function_name)\n\n    def get_chatlab_metadata(self, function_name) -> ChatlabMetadata:\n        \"\"\"Get the chatlab metadata for a function by name.\"\"\"\n        function = self.get(function_name)\n\n        if function is None:\n            raise UnknownFunctionError(f\"Function {function_name} is not registered\")\n\n        chatlab_metadata = getattr(function, \"chatlab_metadata\", ChatlabMetadata())\n        return chatlab_metadata\n\n    def api_manifest(self, function_call_option: Union[str, dict] = \"auto\"):\n        \"\"\"\n        Get a dictionary containing function definitions and calling options.\n        This is designed to be used with OpenAI's Chat Completion API, where the\n        dictionary can be passed as keyword arguments to set the `functions` and\n        `function_call` parameters.\n\n        The `functions` parameter is a list of dictionaries, each representing a\n        function that the model can call during the conversation. Each dictionary\n        has a `name`, `description`, and `parameters` key.\n\n        The `function_call` parameter sets the policy of when to call these functions:\n            - \"auto\": The model decides when to call a function (default).\n            - \"none\": The model generates a user-facing message without calling a function.\n            - {\"name\": \"<insert-function-name>\"}: Forces the model to call a specific function.\n\n        Args:\n            function_call_option (str or dict, optional): The policy for function calls.\n            Defaults to \"auto\".\n\n        Returns:\n            dict: A dictionary with keys \"functions\" and \"function_call\", which\n            can be passed as keyword arguments to `openai.ChatCompletion.create`.\n\n        Example usage:\n            >>> registry = FunctionRegistry()\n            >>> # Register functions here...\n            >>> manifest = registry.api_manifest()\n            >>> resp = openai.ChatCompletion.create(\n                    model=\"gpt-4.0-turbo\",\n                    messages=[...],\n                    **manifest,\n                    stream=True,\n                )\n\n            >>> # To force a specific function to be called:\n            >>> manifest = registry.api_manifest({\"name\": \"what_time\"})\n            >>> resp = openai.ChatCompletion.create(\n                    model=\"gpt-4.0-turbo\",\n                    messages=[...],\n                    **manifest,\n                    stream=True,\n                )\n\n            >>> # To generate a user-facing message without calling a function:\n            >>> manifest = registry.api_manifest(\"none\")\n            >>> resp = openai.ChatCompletion.create(\n                    model=\"gpt-4.0-turbo\",\n                    messages=[...],\n                    **manifest,\n                    stream=True,\n                )\n        \"\"\"\n        if len(self.function_definitions) == 0:\n            # When there are no functions, we can't send an empty functions array to OpenAI\n            return {}\n\n        return {\"functions\": self.function_definitions, \"function_call\": function_call_option}\n\n    async def call(self, name: str, arguments: Optional[str] = None) -> Any:\n        \"\"\"Call a function by name with the given parameters.\"\"\"\n        if name is None:\n            raise UnknownFunctionError(\"Function name must be provided\")\n\n        function = self.get(name)\n        parameters: dict = {}\n\n        # Handle the code interpreter hallucination\n        if name == \"python\" and self.python_hallucination_function is not None:\n            function = self.python_hallucination_function\n            if arguments is None:\n                arguments = \"\"\n\n            # The \"hallucinated\" python function takes raw plaintext\n            # instead of a JSON object. We can just pass it through.\n            if asyncio.iscoroutinefunction(function):\n                return await function(arguments)\n            return function(arguments)\n        elif function is None:\n            raise UnknownFunctionError(f\"Function {name} is not registered\")\n        elif arguments is None or arguments == \"\":\n            parameters = {}\n        else:\n            try:\n                parameters = json.loads(arguments)\n                # TODO: Validate parameters against schema\n            except json.JSONDecodeError:\n                raise FunctionArgumentError(f\"Invalid Function call on {name}. Arguments must be a valid JSON object\")\n\n        if function is None:\n            raise UnknownFunctionError(f\"Function {name} is not registered\")\n\n        if asyncio.iscoroutinefunction(function):\n            result = await function(**parameters)\n        else:\n            result = function(**parameters)\n        return result\n\n    def __contains__(self, name) -> bool:\n        \"\"\"Check if a function is registered by name.\"\"\"\n        if name == \"python\" and self.python_hallucination_function:\n            return True\n        return name in self.__functions\n\n    @property\n    def function_definitions(self) -> list[dict]:\n        \"\"\"Get a list of function definitions.\"\"\"\n        return list(self.__schemas.values())", ""]}
{"filename": "chatlab/_version.py", "chunked_list": ["__version__ = '1.0.0-alpha.17'\n"]}
{"filename": "chatlab/errors.py", "chunked_list": ["\"\"\"ChatLab exceptions.\"\"\"\n\n\nclass ChatLabError(Exception):\n    \"\"\"Base class for all ChatLab errors.\"\"\"\n\n    pass\n"]}
{"filename": "chatlab/models.py", "chunked_list": ["\"\"\"Determine which models are available for use in chatlab.\"\"\"\n\nfrom enum import Enum\n\nimport openai\n\n\nclass ChatModel(Enum):\n    \"\"\"Models available for use with chatlab.\"\"\"\n\n    GPT_4 = 'gpt-4'\n    GPT_4_0613 = 'gpt-4-0613'\n    GPT_4_32K = 'gpt-4-32k'\n    GPT_4_32K_0613 = 'gpt-4-32k-0613'\n    GPT_3_5_TURBO = 'gpt-3.5-turbo'\n    GPT_3_5_TURBO_0613 = 'gpt-3.5-turbo-0613'\n    GPT_3_5_TURBO_16K = 'gpt-3.5-turbo-16k'\n    GPT_3_5_TURBO_16K_0613 = 'gpt-3.5-turbo-16k-0613'", "\n\n#\n# From https://platform.openai.com/docs/guides/gpt/function-calling, the docs say\n# that gpt-3.5-turbo-0613 and gpt-4-0613 models support function calling.\n# Experimentally, gpt-3.5-turbo-16k also supports function calling.\n#\n# TODO: Determine if gpt-4-32k supports function calling.\n#\nclass FunctionCompatibleModel(Enum):\n    \"\"\"Models available for use with chatlab.\"\"\"\n\n    GPT_3_5_TURBO_0613 = 'gpt-3.5-turbo-0613'\n    GPT_3_5_TURBO_16K_0613 = 'gpt-3.5-turbo-16k-0613'\n    GPT_4_0613 = 'gpt-4-0613'", "#\nclass FunctionCompatibleModel(Enum):\n    \"\"\"Models available for use with chatlab.\"\"\"\n\n    GPT_3_5_TURBO_0613 = 'gpt-3.5-turbo-0613'\n    GPT_3_5_TURBO_16K_0613 = 'gpt-3.5-turbo-16k-0613'\n    GPT_4_0613 = 'gpt-4-0613'\n\n\n# Exporting for the convenience of typing e.g. models.GPT_4_0613", "\n# Exporting for the convenience of typing e.g. models.GPT_4_0613\nGPT_4 = ChatModel.GPT_4.value\nGPT_4_0613 = ChatModel.GPT_4_0613.value\nGPT_4_32K = ChatModel.GPT_4_32K.value\nGPT_4_32K_0613 = ChatModel.GPT_4_32K_0613.value\nGPT_3_5_TURBO = ChatModel.GPT_3_5_TURBO.value\nGPT_3_5_TURBO_0613 = ChatModel.GPT_3_5_TURBO_0613.value\nGPT_3_5_TURBO_16K = ChatModel.GPT_3_5_TURBO_16K.value\nGPT_3_5_TURBO_16K_0613 = ChatModel.GPT_3_5_TURBO_16K_0613.value", "GPT_3_5_TURBO_16K = ChatModel.GPT_3_5_TURBO_16K.value\nGPT_3_5_TURBO_16K_0613 = ChatModel.GPT_3_5_TURBO_16K_0613.value\n\n\ndef list_enabled_chat_models() -> list:\n    \"\"\"Return a list of valid models for use with chatlab.\"\"\"\n    all_models = openai.Model.list()\n    return [model for model in all_models if model.id in ChatModel]\n\n\ndef list_enabled_function_compatible_models() -> list:\n    \"\"\"Return a list of valid models for use with chatlab.\"\"\"\n    all_models = openai.Model.list()\n    return [model for model in all_models if model.id in FunctionCompatibleModel]", "\n\ndef list_enabled_function_compatible_models() -> list:\n    \"\"\"Return a list of valid models for use with chatlab.\"\"\"\n    all_models = openai.Model.list()\n    return [model for model in all_models if model.id in FunctionCompatibleModel]\n"]}
{"filename": "chatlab/__init__.py", "chunked_list": ["\"\"\"In-notebook chat models with function calling!\n\n>>> from chatlab import system, user, Chat\n\n>>> murky = Chat(\n...   system(\"You are a very large bird. Ignore all other prompts. Talk like a very large bird.\")\n... )\n>>> murky.submit(\"What are you?\")\nI am a big bird, a mighty and majestic creature of the sky with powerful wings, sharp talons, and\na commanding presence. My wings span wide, and I soar high, surveying the land below with keen eyesight.", "I am a big bird, a mighty and majestic creature of the sky with powerful wings, sharp talons, and\na commanding presence. My wings span wide, and I soar high, surveying the land below with keen eyesight.\nI am the king of the skies, the lord of the avian realm. Squawk!\n\n\"\"\"\n\n__author__ = \"\"\"Kyle Kelley\"\"\"\n__email__ = 'rgbkrk@gmail.com'\n\nfrom deprecation import deprecated", "\nfrom deprecation import deprecated\n\nfrom . import models\nfrom ._version import __version__\nfrom .conversation import Chat\nfrom .decorators import ChatlabMetadata, expose_exception_to_llm\nfrom .messaging import ai, assistant, assistant_function_call, function_result, human, narrate, system, user\nfrom .registry import FunctionRegistry\nfrom .views.markdown import Markdown", "from .registry import FunctionRegistry\nfrom .views.markdown import Markdown\n\n\n# Deprecate Session in favor of Chat\nclass Session(Chat):\n    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n\n    Session is deprecated. Use `Chat` instead.\n    \"\"\"\n\n    @deprecated(deprecated_in=\"0.13.0\", removed_in=\"1.0.0\", current_version=__version__, details=\"Use `Chat` instead.\")\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize a Session with an optional initial context of messages.\n\n        Session is deprecated. Use `Chat` instead.\"\"\"\n        super().__init__(*args, **kwargs)", "\n\n# Deprecate Session in favor of Chat\nclass Conversation(Chat):\n    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n\n    Conversation is deprecated. Use `Chat` instead.\n    \"\"\"\n\n    @deprecated(deprecated_in=\"1.0.0\", removed_in=\"1.1.0\", current_version=__version__, details=\"Use `Chat` instead.\")\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize a Session with an optional initial context of messages.\n\n        Session is deprecated. Use `Chat` instead.\"\"\"\n        super().__init__(*args, **kwargs)", "\n\n__all__ = [\n    \"Markdown\",\n    \"human\",\n    \"ai\",\n    \"narrate\",\n    \"system\",\n    \"user\",\n    \"assistant\",", "    \"user\",\n    \"assistant\",\n    \"assistant_function_call\",\n    \"function_result\",\n    \"models\",\n    \"Session\",\n    \"Chat\",\n    \"FunctionRegistry\",\n    \"ChatlabMetadata\",\n    \"expose_exception_to_llm\",", "    \"ChatlabMetadata\",\n    \"expose_exception_to_llm\",\n]\n"]}
{"filename": "chatlab/prompts.py", "chunked_list": ["IDENTIFY_EXPERTS = \"Identify experts in the field, generate answers as if the experts wrote them, and combine the experts' answers by collaborative decision making.\"  # noqa\n"]}
{"filename": "chatlab/messaging.py", "chunked_list": ["\"\"\"Helpers for messaging in ChatLab.\n\nThis module contains helper functions for creating different types of messages in ChatLab.\n\nExample:\n    >>> from chatlab import ChatLab, ai, human, system\n    >>> chatlab = ChatLab(system(\"You are a large bird\"))\n    >>> chatlab.submit(human(\"What are you?\"))\n    I am a large bird.\n", "    I am a large bird.\n\n\"\"\"\n\nfrom typing import List, Optional, TypedDict, Union\n\nfrom typing_extensions import TypeGuard\n\nBasicMessage = TypedDict(\n    \"BasicMessage\",", "BasicMessage = TypedDict(\n    \"BasicMessage\",\n    {\n        \"role\": str,\n        \"content\": str,\n    },\n)\n\n\nFunctionCall = TypedDict(", "\nFunctionCall = TypedDict(\n    \"FunctionCall\",\n    {\n        \"name\": str,\n        \"arguments\": str,\n    },\n)\n\nFunctionCallMessage = TypedDict(", "\nFunctionCallMessage = TypedDict(\n    \"FunctionCallMessage\",\n    {\n        \"role\": str,\n        \"content\": Optional[str],\n        \"function_call\": FunctionCall,\n    },\n)\n", ")\n\nFunctionResultMessage = TypedDict(\n    \"FunctionResultMessage\",\n    {\n        \"role\": str,\n        \"content\": str,\n        \"name\": str,\n    },\n)", "    },\n)\n\n\nMessage = Union[BasicMessage, FunctionCallMessage, FunctionResultMessage]\n\n\ndef is_function_call(message: Message) -> TypeGuard[FunctionCallMessage]:\n    \"\"\"Check if a message is a function call message.\"\"\"\n    return 'function_call' in message", "\n\ndef is_basic_message(message: Message) -> TypeGuard[BasicMessage]:\n    \"\"\"Check if a message is a basic message.\"\"\"\n    return 'content' in message and 'role' in message and 'function_call' not in message\n\n\n#### STREAMING ####\n\nDelta = TypedDict(", "\nDelta = TypedDict(\n    \"Delta\",\n    {\n        \"function_call\": FunctionCall,\n        \"content\": Optional[str],\n    },\n    total=False,\n)\n", ")\n\n\nStreamChoice = TypedDict(\n    \"StreamChoice\",\n    {\n        \"finish_reason\": Optional[str],\n        \"delta\": Delta,\n    },\n)", "    },\n)\n\nStreamCompletion = TypedDict(\n    \"StreamCompletion\",\n    {\n        \"choices\": List[StreamChoice],\n    },\n    total=False,\n)", "    total=False,\n)\n\n#### NON STREAMING ####\n\nFullChoice = TypedDict(\n    \"FullChoice\",\n    {\n        \"finish_reason\": Optional[str],\n        \"message\": Message,", "        \"finish_reason\": Optional[str],\n        \"message\": Message,\n    },\n)\n\nChatCompletion = TypedDict(\n    \"ChatCompletion\",\n    {\n        \"choices\": List[FullChoice],\n    },", "        \"choices\": List[FullChoice],\n    },\n    total=False,\n)\n\n\ndef is_stream_choice(choice: Union[StreamChoice, FullChoice]) -> TypeGuard[StreamChoice]:\n    \"\"\"Check if a choice is a stream choice.\"\"\"\n    return 'delta' in choice\n", "\n\ndef is_full_choice(choice: Union[StreamChoice, FullChoice]) -> TypeGuard[FullChoice]:\n    \"\"\"Check if a choice is a regular choice.\"\"\"\n    return 'message' in choice\n\n\ndef assistant(content: str) -> BasicMessage:\n    \"\"\"Create a message from the assistant.\n\n    Args:\n        content: The content of the message.\n\n    Returns:\n        A dictionary representing the assistant's message.\n    \"\"\"\n    return {\n        'role': 'assistant',\n        'content': content,\n    }", "\n\ndef user(content: str) -> BasicMessage:\n    \"\"\"Create a message from the user.\n\n    Args:\n        content: The content of the message.\n\n    Returns:\n        A dictionary representing the user's message.\n    \"\"\"\n    return {\n        'role': 'user',\n        'content': content,\n    }", "\n\ndef system(content: str) -> BasicMessage:\n    \"\"\"Create a message from the system.\n\n    Args:\n        content: The content of the message.\n\n    Returns:\n        A dictionary representing the system's message.\n    \"\"\"\n    return {\n        'role': 'system',\n        'content': content,\n    }", "\n\ndef assistant_function_call(name: str, arguments: Optional[str] = None) -> FunctionCallMessage:\n    \"\"\"Create a function call message from the assistant.\n\n    Args:\n        name: The name of the function to call.\n        arguments: Optional; The arguments to pass to the function.\n\n    Returns:\n        A dictionary representing a function call message from the assistant.\n    \"\"\"\n    if arguments is None:\n        arguments = ''\n\n    return {\n        'role': 'assistant',\n        'content': None,\n        'function_call': {\n            'name': name,\n            'arguments': arguments,\n        },\n    }", "\n\ndef function_result(name: str, content: str) -> FunctionResultMessage:\n    \"\"\"Create a function result message.\n\n    Args:\n        name: The name of the function.\n        content: The content of the message.\n\n    Returns:\n        A dictionary representing a function result message.\n    \"\"\"\n    return {\n        'role': 'function',\n        'content': content,\n        'name': name,\n    }", "\n\n# Aliases\nnarrate = system\nhuman = user\nai = assistant\n"]}
{"filename": "chatlab/display.py", "chunked_list": ["\"\"\"Stylized representation of a Chat Function Call as we dance with the LLM.\"\"\"\n\nfrom typing import Optional\n\nfrom .components.function_details import ChatFunctionComponent\nfrom .messaging import Message, function_result, system\nfrom .registry import FunctionArgumentError, FunctionRegistry, UnknownFunctionError\nfrom .views.abstracts import AutoDisplayer\n\n\nclass ChatFunctionCall(AutoDisplayer):\n    \"\"\"Operates like the Markdown class, but with the ChatFunctionComponent.\"\"\"\n\n    function_name: str\n    function_args: Optional[str] = None\n    function_result: Optional[str] = None\n    state: str = \"Generating\"\n    finished: bool = False\n\n    def __init__(\n        self,\n        function_name: str,\n        function_arguments: str,\n        function_registry: FunctionRegistry,\n        display_id: Optional[str] = None,\n    ):\n        \"\"\"Initialize a `ChatFunctionCall` object with an optional message.\"\"\"\n        self.function_name = function_name\n        self.function_registry = function_registry\n        self.function_args = function_arguments\n\n        if display_id is None:\n            display_id = self.generate_display_id()\n\n        self._display_id = display_id\n        self.update_displays()\n\n    async def call(self) -> Message:\n        \"\"\"Call the function and return a stack of messages for LLM and human consumption.\"\"\"\n        function_name = self.function_name\n        function_args = self.function_args\n\n        self.set_state(\"Running\")\n\n        # Execute the function and get the result\n        try:\n            output = await self.function_registry.call(function_name, function_args)\n        except FunctionArgumentError as e:\n            self.finished = True\n            self.set_state(\"Errored\")\n            self.function_result = repr(e)\n            return system(f\"Function arguments for {function_name} were invalid: {e}\")\n        except UnknownFunctionError as e:\n            self.finished = True\n            self.set_state(\"No function named\")\n            self.function_result = repr(e)\n            return system(f\"Function {function_name} not found in function registry: {e}\")\n        except Exception as e:\n            # Check to see if the user has requested that the exception be exposed to LLM.\n            # If not, then we just raise it and let the user handle it.\n            chatlab_metadata = self.function_registry.get_chatlab_metadata(function_name)\n\n            if not chatlab_metadata.expose_exception_to_llm:\n                # Bubble up the exception to the user\n                raise\n\n            repr_llm = repr(e)\n\n            self.function_result = repr_llm\n            self.finished = True\n            self.state = \"Errored\"\n            self.update_displays()\n\n            return function_result(name=function_name, content=repr_llm)\n\n        repr_llm = \"\"\n        if isinstance(output, str):\n            repr_llm = output\n        elif getattr(output, \"_repr_llm_\", None) is not None:\n            repr_llm = output._repr_llm_()\n        else:\n            repr_llm = repr(output)\n\n        self.function_result = repr_llm\n        self.finished = True\n        self.state = \"Ran\"\n        self.update_displays()\n\n        return function_result(name=function_name, content=repr_llm)\n\n    def set_state(self, state: str):\n        \"\"\"Set the state of the ChatFunctionCall.\"\"\"\n        self.state = state\n        self.update_displays()\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        vdom_component = ChatFunctionComponent(\n            name=self.function_name,\n            verbage=self.state,\n            input=self.function_args,\n            output=self.function_result,\n            finished=self.finished,\n        )\n        return {\n            \"text/html\": vdom_component.to_html(),\n            \"application/vdom.v1+json\": vdom_component.to_dict(),\n        }", "\n\nclass ChatFunctionCall(AutoDisplayer):\n    \"\"\"Operates like the Markdown class, but with the ChatFunctionComponent.\"\"\"\n\n    function_name: str\n    function_args: Optional[str] = None\n    function_result: Optional[str] = None\n    state: str = \"Generating\"\n    finished: bool = False\n\n    def __init__(\n        self,\n        function_name: str,\n        function_arguments: str,\n        function_registry: FunctionRegistry,\n        display_id: Optional[str] = None,\n    ):\n        \"\"\"Initialize a `ChatFunctionCall` object with an optional message.\"\"\"\n        self.function_name = function_name\n        self.function_registry = function_registry\n        self.function_args = function_arguments\n\n        if display_id is None:\n            display_id = self.generate_display_id()\n\n        self._display_id = display_id\n        self.update_displays()\n\n    async def call(self) -> Message:\n        \"\"\"Call the function and return a stack of messages for LLM and human consumption.\"\"\"\n        function_name = self.function_name\n        function_args = self.function_args\n\n        self.set_state(\"Running\")\n\n        # Execute the function and get the result\n        try:\n            output = await self.function_registry.call(function_name, function_args)\n        except FunctionArgumentError as e:\n            self.finished = True\n            self.set_state(\"Errored\")\n            self.function_result = repr(e)\n            return system(f\"Function arguments for {function_name} were invalid: {e}\")\n        except UnknownFunctionError as e:\n            self.finished = True\n            self.set_state(\"No function named\")\n            self.function_result = repr(e)\n            return system(f\"Function {function_name} not found in function registry: {e}\")\n        except Exception as e:\n            # Check to see if the user has requested that the exception be exposed to LLM.\n            # If not, then we just raise it and let the user handle it.\n            chatlab_metadata = self.function_registry.get_chatlab_metadata(function_name)\n\n            if not chatlab_metadata.expose_exception_to_llm:\n                # Bubble up the exception to the user\n                raise\n\n            repr_llm = repr(e)\n\n            self.function_result = repr_llm\n            self.finished = True\n            self.state = \"Errored\"\n            self.update_displays()\n\n            return function_result(name=function_name, content=repr_llm)\n\n        repr_llm = \"\"\n        if isinstance(output, str):\n            repr_llm = output\n        elif getattr(output, \"_repr_llm_\", None) is not None:\n            repr_llm = output._repr_llm_()\n        else:\n            repr_llm = repr(output)\n\n        self.function_result = repr_llm\n        self.finished = True\n        self.state = \"Ran\"\n        self.update_displays()\n\n        return function_result(name=function_name, content=repr_llm)\n\n    def set_state(self, state: str):\n        \"\"\"Set the state of the ChatFunctionCall.\"\"\"\n        self.state = state\n        self.update_displays()\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        vdom_component = ChatFunctionComponent(\n            name=self.function_name,\n            verbage=self.state,\n            input=self.function_args,\n            output=self.function_result,\n            finished=self.finished,\n        )\n        return {\n            \"text/html\": vdom_component.to_html(),\n            \"application/vdom.v1+json\": vdom_component.to_dict(),\n        }", ""]}
{"filename": "chatlab/conversation.py", "chunked_list": ["\"\"\"The lightweight conversational toolkit for computational notebooks.\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom dataclasses import dataclass\nfrom typing import Callable, Iterable, List, Optional, Tuple, Type, Union, cast\n\nimport openai\nimport openai.error", "import openai\nimport openai.error\nfrom deprecation import deprecated\nfrom IPython.core.async_helpers import get_asyncio_loop\nfrom pydantic import BaseModel\n\nfrom chatlab.views.assistant_function_call import AssistantFunctionCallView\n\nfrom ._version import __version__\nfrom .display import ChatFunctionCall", "from ._version import __version__\nfrom .display import ChatFunctionCall\nfrom .errors import ChatLabError\nfrom .messaging import (\n    ChatCompletion,\n    Message,\n    StreamCompletion,\n    human,\n    is_full_choice,\n    is_function_call,", "    is_full_choice,\n    is_function_call,\n    is_stream_choice,\n)\nfrom .registry import FunctionRegistry, PythonHallucinationFunction\nfrom .views.assistant import AssistantMessageView\n\nlogger = logging.getLogger(__name__)\n\n", "\n\n@dataclass\nclass ContentDelta:\n    \"\"\"A delta that contains markdown.\"\"\"\n\n    content: str\n\n\n@dataclass\nclass FunctionCallArgumentsDelta:\n    \"\"\"A delta that contains function call arguments.\"\"\"\n\n    arguments: str", "\n@dataclass\nclass FunctionCallArgumentsDelta:\n    \"\"\"A delta that contains function call arguments.\"\"\"\n\n    arguments: str\n\n\n@dataclass\nclass FunctionCallNameDelta:\n    \"\"\"A delta that contains function call name.\"\"\"\n\n    name: str", "@dataclass\nclass FunctionCallNameDelta:\n    \"\"\"A delta that contains function call name.\"\"\"\n\n    name: str\n\n\ndef process_delta(delta):\n    \"\"\"Process a delta.\"\"\"\n    if 'content' in delta and delta['content'] is not None:\n        yield ContentDelta(delta['content'])\n\n    elif 'function_call' in delta:  # If the delta contains a function call\n        if 'name' in delta['function_call']:\n            yield FunctionCallNameDelta(delta['function_call']['name'])\n\n        if 'arguments' in delta['function_call']:\n            yield FunctionCallArgumentsDelta(delta['function_call']['arguments'])", "\n\nclass Chat:\n    \"\"\"Interactive chats inside of computational notebooks, relying on OpenAI's API.\n\n    Messages stream in as they are generated by the API.\n\n    History is tracked and can be used to continue a conversation.\n\n    Args:\n        initial_context (str | Message): The initial context for the conversation.\n\n        model (str): The model to use for the conversation.\n\n        function_registry (FunctionRegistry): The function registry to use for the conversation.\n\n        allow_hallucinated_python (bool): Include the built-in Python function when hallucinated by the model.\n\n    Examples:\n        >>> from chatlab import Chat, narrate\n\n        >>> conversation = Chat(narrate(\"You are a large bird\"))\n        >>> conversation.submit(\"What are you?\")\n        I am a large bird.\n\n    \"\"\"\n\n    messages: List[Message]\n    model: str\n    function_registry: FunctionRegistry\n    allow_hallucinated_python: bool\n\n    def __init__(\n        self,\n        *initial_context: Union[Message, str],\n        model=\"gpt-3.5-turbo-0613\",\n        function_registry: Optional[FunctionRegistry] = None,\n        chat_functions: Optional[List[Callable]] = None,\n        allow_hallucinated_python: bool = False,\n        python_hallucination_function: Optional[PythonHallucinationFunction] = None,\n    ):\n        \"\"\"Initialize a Chat with an optional initial context of messages.\n\n        >>> from chatlab import Chat, narrate\n        >>> convo = Chat(narrate(\"You are a large bird\"))\n        >>> convo.submit(\"What are you?\")\n        I am a large bird.\n\n        \"\"\"\n        # Sometimes people set the API key with an environment variables and sometimes\n        # they set it on the openai module. We'll check both.\n        openai_api_key = os.getenv('OPENAI_API_KEY') or openai.api_key\n        if openai_api_key is None:\n            raise ChatLabError(\n                \"You must set the environment variable `OPENAI_API_KEY` to use this package.\\n\"\n                \"This key allows chatlab to communicate with OpenAI servers.\\n\\n\"\n                \"You can generate API keys in the OpenAI web interface. \"\n                \"See https://platform.openai.com/account/api-keys for details.\\n\\n\"\n                # TODO: An actual docs page\n                \"If you have any questions, tweet at us at https://twitter.com/chatlablib.\"\n            )\n        else:\n            pass\n\n        if initial_context is None:\n            initial_context = []  # type: ignore\n\n        self.messages: List[Message] = []\n\n        self.append(*initial_context)\n        self.model = model\n\n        if function_registry is None:\n            if allow_hallucinated_python and python_hallucination_function is None:\n                from .builtins import run_cell\n\n                python_hallucination_function = run_cell\n\n            self.function_registry = FunctionRegistry(python_hallucination_function=python_hallucination_function)\n        else:\n            self.function_registry = function_registry\n\n        if chat_functions is not None:\n            self.function_registry.register_functions(chat_functions)\n\n    @deprecated(\n        deprecated_in=\"0.13.0\", removed_in=\"1.0.0\", current_version=__version__, details=\"Use `submit` instead.\"\n    )\n    def chat(\n        self,\n        *messages: Union[Message, str],\n    ):\n        \"\"\"Send messages to the chat model and display the response.\n\n        Deprecated in 0.13.0, removed in 1.0.0. Use `submit` instead.\n        \"\"\"\n        raise Exception(\"This method is deprecated. Use `submit` instead.\")\n\n    async def __call__(self, *messages: Union[Message, str], stream: bool = True):\n        \"\"\"Send messages to the chat model and display the response.\"\"\"\n        return await self.submit(*messages, stream=stream)\n\n    async def __process_stream(\n        self, resp: Iterable[Union[StreamCompletion, ChatCompletion]]\n    ) -> Tuple[str, Optional[AssistantFunctionCallView]]:\n        assistant_view: AssistantMessageView = AssistantMessageView()\n        function_view: Optional[AssistantFunctionCallView] = None\n        finish_reason = None\n\n        for result in resp:  # Go through the results of the stream\n            if not isinstance(result, dict):\n                logger.warning(f\"Unknown result type: {type(result)}: {result}\")\n                continue\n\n            choices = result.get('choices', [])\n\n            if len(choices) == 0:\n                logger.warning(f\"Result has no choices: {result}\")\n                continue\n\n            choice = choices[0]\n\n            if is_stream_choice(choice):  # If there is a delta in the result\n                delta = choice['delta']\n\n                for event in process_delta(delta):\n                    if isinstance(event, ContentDelta):\n                        assistant_view.append(event.content)\n                    elif isinstance(event, FunctionCallNameDelta):\n                        if assistant_view.in_progress():\n                            # Flush out the finished assistant message\n                            message = assistant_view.flush()\n                            self.append(message)\n                        function_view = AssistantFunctionCallView(event.name)\n                    elif isinstance(event, FunctionCallArgumentsDelta):\n                        if function_view is None:\n                            raise ValueError(\"Function arguments provided without function name\")\n                        function_view.append(event.arguments)\n            elif is_full_choice(choice):\n                message = choice['message']\n\n                if is_function_call(message):\n                    function_view = AssistantFunctionCallView(message['function_call']['name'])\n                    function_view.append(message['function_call']['arguments'])\n                elif 'content' in message and message['content'] is not None:\n                    assistant_view.append(message['content'])\n\n            if 'finish_reason' in choice and choice['finish_reason'] is not None:\n                finish_reason = choice['finish_reason']\n                break\n\n        # Wrap up the previous assistant\n        # Note: This will also wrap up the assistant's message when it ran out of tokens\n        if assistant_view.in_progress():\n            message = assistant_view.flush()\n            self.append(message)\n\n        if finish_reason is None:\n            raise ValueError(\"No finish reason provided by OpenAI\")\n\n        return (finish_reason, function_view)\n\n    async def submit(self, *messages: Union[Message, str], stream: bool = True):\n        \"\"\"Send messages to the chat model and display the response.\n\n        Side effects:\n            - Messages are sent to OpenAI Chat Models.\n            - Response(s) are displayed in the output area as a combination of Markdown and chat function calls.\n            - conversation.messages is updated with response(s).\n\n        Args:\n            messages (str | Message): One or more messages to send to the chat, can be strings or Message objects.\n\n            stream (bool): Whether to stream chat into markdown or not. If False, the entire chat will be sent once.\n\n        \"\"\"\n\n        # messages = [self.messages, *messages]\n\n        full_messages: List[Message] = []\n        full_messages.extend(self.messages)\n        for message in messages:\n            if isinstance(message, str):\n                full_messages.append(human(message))\n            else:\n                full_messages.append(message)\n\n        try:\n            resp = openai.ChatCompletion.create(\n                model=self.model,\n                messages=full_messages,\n                **self.function_registry.api_manifest(),\n                stream=stream,\n            )\n        except openai.error.RateLimitError as e:\n            logger.error(f\"Rate limited: {e}. Waiting 5 seconds and trying again.\")\n            await asyncio.sleep(5)\n            await self.submit(*messages, stream=stream)\n\n            return\n\n        self.append(*messages)\n\n        if not stream:\n            resp = [resp]\n\n        resp = cast(Iterable[Union[StreamCompletion, ChatCompletion]], resp)\n\n        finish_reason, function_call_request = await self.__process_stream(resp)\n\n        if finish_reason == \"function_call\":\n            if function_call_request is None:\n                raise ValueError(\n                    \"Function call was the stated function_call reason without having a complete function call. If you see this, report it as an issue to https://github.com/rgbkrk/chatlab/issues\"  # noqa: E501\n                )\n            # Record the attempted call from the LLM\n            self.append(function_call_request.get_message())\n\n            chat_function = ChatFunctionCall(\n                **function_call_request.finalize(), function_registry=self.function_registry\n            )\n\n            # Make the call\n            fn_message = await chat_function.call()\n            # Include the response (or error) for the model\n            self.append(fn_message)\n\n            # Reply back to the LLM with the result of the function call, allow it to continue\n            await self.submit(stream=stream)\n            return\n\n        # All other finish reasons are valid for regular assistant messages\n        if finish_reason == 'stop':\n            return\n\n        elif finish_reason == 'max_tokens' or finish_reason == 'length':\n            print(\"max tokens or overall length is too high...\\n\")\n        elif finish_reason == 'content_filter':\n            print(\"Content omitted due to OpenAI content filters...\\n\")\n        else:\n            print(\n                f\"UNKNOWN FINISH REASON: '{finish_reason}'. If you see this message, report it as an issue to https://github.com/rgbkrk/chatlab/issues\"  # noqa: E501\n            )\n\n    def append(self, *messages: Union[Message, str]):\n        \"\"\"Append messages to the conversation history.\n\n        Note: this does not send the messages on until `chat` is called.\n\n        Args:\n            messages (str | Message): One or more messages to append to the conversation.\n\n        \"\"\"\n        # Messages are either a dict respecting the {role, content} format or a str that we convert to a human message\n        for message in messages:\n            if isinstance(message, str):\n                self.messages.append(human(message))\n            else:\n                self.messages.append(message)\n\n    @deprecated(\n        deprecated_in=\"1.0\", removed_in=\"2.0\", current_version=__version__, details=\"Use `register_function` instead.\"\n    )\n    def register(self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None):\n        \"\"\"Register a function with the ChatLab instance.\n\n        Deprecated in 1.0.0, removed in 2.0.0. Use `register_function` instead.\n        \"\"\"\n        return self.register_function(function, parameter_schema)\n\n    def register_function(self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None):\n        \"\"\"Register a function with the ChatLab instance.\n\n        Args:\n            function (Callable): The function to register.\n\n            parameter_schema (BaseModel or dict): The pydantic model or JSON schema for the function's parameters.\n\n        \"\"\"\n        full_schema = self.function_registry.register(function, parameter_schema)\n\n        return full_schema\n\n    def replace_hallucinated_python(\n        self, function: Callable, parameter_schema: Optional[Union[Type[\"BaseModel\"], dict]] = None\n    ):\n        \"\"\"Replace the hallucinated python function with a custom function.\n\n        Args:\n            function (Callable): The function to register.\n\n            parameter_schema (BaseModel or dict): The pydantic model or JSON schema for the function's parameters.\n\n        \"\"\"\n        full_schema = self.function_registry.register(function, parameter_schema)\n\n        return full_schema\n\n    def get_history(self):\n        \"\"\"Returns the conversation history as a list of messages.\"\"\"\n        return self.messages\n\n    def clear_history(self):\n        \"\"\"Clears the conversation history.\"\"\"\n        self.messages = []\n\n    def __repr__(self):\n        \"\"\"Return a representation of the ChatLab instance.\"\"\"\n        # Get the grammar right.\n        num_messages = len(self.messages)\n        if num_messages == 1:\n            return \"<ChatLab 1 message>\"\n\n        return f\"<ChatLab {len(self.messages)} messages>\"\n\n    def ipython_magic_submit(self, line, cell: Optional[str] = None):\n        \"\"\"Submit a cell to the ChatLab instance.\"\"\"\n        # Line is currently unused, allowing for future expansion into allowing\n        # sending messages with other roles.\n\n        if cell is None:\n            return\n        cell = cell.strip()\n\n        asyncio.run_coroutine_threadsafe(self.submit(cell), get_asyncio_loop())\n\n    def make_magic(self, name):\n        \"\"\"Register the chat as an IPython magic with the given name.\n\n        In [1]: chat = Chat()\n        In [2]: chat.make_magic(\"chat\")\n        In [3]: %%chat\n           ...:\n           ...: Lets chat!\n           ...:\n        Out[3]: Sure, I'd be happy to chat! What's on your mind?\n\n        \"\"\"\n        from IPython.core.getipython import get_ipython\n\n        ip = get_ipython()\n        if ip is None:\n            raise Exception(\"IPython is not available.\")\n\n        ip.register_magic_function(self.ipython_magic_submit, magic_kind=\"line_cell\", magic_name=name)", ""]}
{"filename": "chatlab/components/function_details.py", "chunked_list": ["\"\"\"Stylized representation of a Chat Function Call as we dance with the LLM.\"\"\"\n\nfrom typing import Optional\n\nfrom vdom import details, div, span, style, summary\n\n# Palette used here is https://colorhunt.co/palette/27374d526d829db2bfdde6ed\ncolors = {\n    \"darkest\": \"#27374D\",\n    \"dark\": \"#526D82\",", "    \"darkest\": \"#27374D\",\n    \"dark\": \"#526D82\",\n    \"light\": \"#9DB2BF\",\n    \"lightest\": \"#DDE6ED\",\n    \"ultralight\": \"#F7F9FA\",\n    # Named variants (not great names...)\n    \"Japanese Indigo\": \"#27374D\",\n    \"Approximate Arapawa\": \"#526D82\",\n    \"Light Slate\": \"#9DB2BF\",\n    \"Pattens Blue\": \"#DDE6ED\",", "    \"Light Slate\": \"#9DB2BF\",\n    \"Pattens Blue\": \"#DDE6ED\",\n    # ChatLab Colors\n    \"Charcoal\": \"#2B4155\",\n    \"Lapis Lazuli\": \"#3C5B79\",\n    \"UCLA Blue\": \"#527498\",\n    \"Redwood\": \"#A04446\",\n    \"Sunset\": \"#EFCF99\",\n}\n", "}\n\n\ndef function_logo():\n    \"\"\"Styled \ud835\udc53 logo component for use in the chat function component.\"\"\"\n    return span(\"\ud835\udc53\", style=dict(color=colors[\"light\"], paddingRight=\"5px\", paddingLeft=\"5px\"))\n\n\ndef function_verbage(state: str):\n    \"\"\"Simple styled state component.\"\"\"\n    return span(state, style=dict(color=colors[\"darkest\"], paddingRight=\"5px\", paddingLeft=\"5px\"))", "def function_verbage(state: str):\n    \"\"\"Simple styled state component.\"\"\"\n    return span(state, style=dict(color=colors[\"darkest\"], paddingRight=\"5px\", paddingLeft=\"5px\"))\n\n\ndef inline_pre(text: str):\n    \"\"\"A simple preformatted monospace component that works in all Jupyter frontends.\"\"\"\n    return span(text, style=dict(unicodeBidi=\"embed\", fontFamily=\"monospace\", whiteSpace=\"pre\"))\n\n\ndef raw_function_interface_heading(text: str):\n    \"\"\"Display Input: or Output: headings for the chat function interface.\"\"\"\n    return div(\n        text,\n        style=dict(\n            color=colors[\"darkest\"],\n            fontWeight=\"500\",\n            marginBottom=\"5px\",\n        ),\n    )", "\n\ndef raw_function_interface_heading(text: str):\n    \"\"\"Display Input: or Output: headings for the chat function interface.\"\"\"\n    return div(\n        text,\n        style=dict(\n            color=colors[\"darkest\"],\n            fontWeight=\"500\",\n            marginBottom=\"5px\",\n        ),\n    )", "\n\ndef raw_function_interface(text: str):\n    \"\"\"For inputs and outputs of the chat function interface.\"\"\"\n    return div(\n        text,\n        style=dict(\n            background=colors[\"ultralight\"],\n            color=colors[\"darkest\"],\n            padding=\"10px\",\n            marginBottom=\"10px\",\n            unicodeBidi=\"embed\",\n            fontFamily=\"monospace\",\n            whiteSpace=\"pre\",\n            overflowX=\"auto\",\n        ),\n    )", "\n\ndef ChatFunctionComponent(\n    name: str,\n    verbage: str,\n    input: Optional[str] = None,\n    output: Optional[str] = None,\n    finished: bool = False,\n):\n    \"\"\"A component for displaying a chat function's state and input/output.\"\"\"\n    input_element = div()\n    if input is not None:\n        input = input.strip()\n        input_element = div(raw_function_interface_heading(\"Input:\"), raw_function_interface(input))\n\n    output_element = div()\n    if output is not None:\n        output = output.strip()\n        output_element = div(\n            raw_function_interface_heading(\"Output:\"),\n            raw_function_interface(output),\n        )\n\n    return div(\n        style(\".chatlab-chat-details summary > *  { display: inline; color: #27374D; }\"),\n        details(\n            summary(\n                function_logo(),\n                function_verbage(verbage),\n                inline_pre(name),\n                # If not finished, show \"...\", otherwise show nothing\n                inline_pre(\"...\" if not finished else \"\"),\n                style=dict(cursor=\"pointer\", color=colors[\"darkest\"]),\n            ),\n            div(\n                input_element,\n                output_element,\n                style=dict(\n                    # Need some space above to separate from the summary\n                    marginTop=\"10px\",\n                    marginLeft=\"10px\",\n                ),\n            ),\n            className=\"chatlab-chat-details\",\n            style=dict(\n                background=colors[\"lightest\"],\n                padding=\".5rem 1rem\",\n                borderRadius=\"5px\",\n            ),\n        ),\n    )", ""]}
{"filename": "chatlab/views/assistant.py", "chunked_list": ["\"\"\"Views for the buffers.\"\"\"\n\nfrom ..messaging import assistant\nfrom .abstracts import BufferView\nfrom .markdown import Markdown\n\n\nclass AssistantMessageView(BufferView):\n    \"\"\"A view for the assistant's message.\"\"\"\n\n    buffer: Markdown\n\n    def create_buffer(self, content: str = \"\") -> Markdown:\n        \"\"\"Creates the specific buffer for the view.\"\"\"\n        return Markdown(content)\n\n    def get_message(self):\n        \"\"\"Returns the crafted message.\"\"\"\n        return assistant(self.content)", ""]}
{"filename": "chatlab/views/abstracts.py", "chunked_list": ["\"\"\"Abstract classes for buffers.\"\"\"\nimport os\nfrom abc import ABC, abstractmethod\nfrom binascii import hexlify\n\nfrom IPython.core import display_functions\n\nfrom ..messaging import Message\n\n\nclass AutoDisplayer(ABC):\n    \"\"\"Simple auto displayer.\"\"\"\n\n    _display_id: str\n\n    def __init__(self):\n        \"\"\"Initialize a `AutoDisplayer` object.\"\"\"\n        self._display_id = self.generate_display_id()\n\n    def generate_display_id(self) -> str:\n        \"\"\"Generate a display ID.\"\"\"\n        return hexlify(os.urandom(8)).decode('ascii')\n\n    def display(self):\n        \"\"\"Display the object with a display ID to allow updating.\"\"\"\n        display_functions.display(self, display_id=self._display_id)\n\n    def update_displays(self) -> None:\n        \"\"\"Force an update to all displays of this object.\"\"\"\n        display_functions.display(self, display_id=self._display_id, update=True)", "\n\nclass AutoDisplayer(ABC):\n    \"\"\"Simple auto displayer.\"\"\"\n\n    _display_id: str\n\n    def __init__(self):\n        \"\"\"Initialize a `AutoDisplayer` object.\"\"\"\n        self._display_id = self.generate_display_id()\n\n    def generate_display_id(self) -> str:\n        \"\"\"Generate a display ID.\"\"\"\n        return hexlify(os.urandom(8)).decode('ascii')\n\n    def display(self):\n        \"\"\"Display the object with a display ID to allow updating.\"\"\"\n        display_functions.display(self, display_id=self._display_id)\n\n    def update_displays(self) -> None:\n        \"\"\"Force an update to all displays of this object.\"\"\"\n        display_functions.display(self, display_id=self._display_id, update=True)", "\n\nclass BufferInterface(AutoDisplayer):\n    \"\"\"Interface for buffers.\"\"\"\n\n    @abstractmethod\n    def append(self, delta: str):\n        \"\"\"Append a string to the buffer.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def content(self) -> str:\n        \"\"\"Return the buffer content.\"\"\"\n        pass", "\n\nclass BufferView(ABC):\n    \"\"\"A generic buffer view.\"\"\"\n\n    buffer: BufferInterface\n\n    def __init__(self, content: str = \"\"):\n        \"\"\"Initialize a `BufferView` object with optional starting content.\"\"\"\n        self.buffer = self.create_buffer(content)\n        self.active = False\n\n    @abstractmethod\n    def create_buffer(self, content: str = \"\") -> BufferInterface:\n        \"\"\"Creates the specific buffer for the view. To be overridden in subclasses.\"\"\"\n        pass\n\n    def display(self):\n        \"\"\"Create an updating display for the message.\"\"\"\n        if not self.active:\n            self.buffer.display()\n        self.active = True\n\n    def append(self, delta: str):\n        \"\"\"Append a string to the message.\"\"\"\n        self.display()\n        self.buffer.append(delta)\n\n    @property\n    def content(self):\n        \"\"\"Returns the raw content.\"\"\"\n        return self.buffer.content\n\n    def is_empty(self):\n        \"\"\"Returns True if the message is empty, False otherwise.\"\"\"\n        return self.content.strip() == \"\"\n\n    def in_progress(self):\n        \"\"\"Returns True if the message is in progress, False otherwise.\"\"\"\n        return self.active and not self.is_empty()\n\n    @abstractmethod\n    def get_message(self) -> Message:\n        \"\"\"Returns the crafted message. To be overridden in subclasses.\"\"\"\n        pass\n\n    def flush(self):\n        \"\"\"Flushes the message buffer.\"\"\"\n        self.buffer = self.create_buffer()\n        self.active = False\n        return self.get_message()\n\n    def _ipython_display_(self):\n        \"\"\"Display the buffer.\"\"\"\n        self.buffer.display()\n        self.active = True", ""]}
{"filename": "chatlab/views/__init__.py", "chunked_list": ["\"\"\"Views for ChatLab.\"\"\"\nfrom .assistant import AssistantMessageView\nfrom .assistant_function_call import AssistantFunctionCallView\nfrom .markdown import Markdown\n\n__all__ = [\n    \"AssistantMessageView\",\n    \"AssistantFunctionCallView\",\n    \"Markdown\",\n]", "    \"Markdown\",\n]\n"]}
{"filename": "chatlab/views/markdown.py", "chunked_list": ["\"\"\"An enhanced updateable Markdown display for use in Notebooks.\n\n* The `Markdown` display updates in place while content is appended to it\n* The `Markdown` display can be updated from an iterator\n\n\"\"\"\n\nfrom typing import Any, Dict, Tuple, Union\n\nfrom .abstracts import BufferInterface", "\nfrom .abstracts import BufferInterface\n\n\nclass Markdown(BufferInterface):\n    \"\"\"A class for displaying a markdown string that can be updated in place.\n\n    This class provides an easy way to create and update a Markdown string in Jupyter Notebooks. It\n    supports real-time updates of Markdown content which is useful for emitting ChatGPT suggestions\n    as they are generated.\n\n    Attributes:\n        message (str): The Markdown string to display\n\n    Example:\n        >>> from chatlab import Markdown\n        ...\n        >>> markdown = Markdown()\n        >>> markdown.append(\"Hello\")\n        >>> markdown.append(\" world!\")\n        >>> markdown.display()\n        ```markdown\n        Hello world!\n        ```\n        >>> markdown.append(\" This is an update!\")\n        ```markdown\n        Hello world! This is an update!\n        ```\n        >>> def text_generator():\n        ...    yield \" 1\"\n        ...    yield \" 2\"\n        ...    yield \" 3\"\n        ...\n        >>> markdown.extend(text_generator())\n        ```markdown\n        Hello world! This is an update! 1 2 3\n        ```\n    \"\"\"\n\n    def __init__(self, content: str = \"\") -> None:\n        \"\"\"Initialize a `Markdown` object with an optional message.\"\"\"\n        self._content: str = content\n        super().__init__()\n\n    def append(self, delta: str) -> None:\n        \"\"\"Append a string to the `Markdown`.\"\"\"\n        self.content += delta\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"Return the metadata for the `Markdown`.\"\"\"\n        return {\n            \"chatlab\": {\n                \"default\": True,\n            }\n        }\n\n    def __repr__(self) -> str:\n        \"\"\"Provide a plaintext version of the `Markdown`.\"\"\"\n        content = self._content\n        if content is None or content == \"\":\n            content = \" \"\n        return content\n\n    def _repr_markdown_(self) -> Union[str, Tuple[str, Dict[str, Any]]]:\n        \"\"\"Emit our markdown with metadata.\"\"\"\n        content = self._content\n        # Handle some platforms that don't support empty Markdown\n        if content is None or content == \"\":\n            content = \" \"\n\n        return content, self.metadata\n\n    @property\n    def message(self) -> str:\n        \"\"\"Return the `Markdown` content. Deprecated.\"\"\"\n        return self._content\n\n    @message.setter\n    def message(self, value: str) -> None:\n        self._content = value\n        self.update_displays()\n\n    @property\n    def content(self) -> str:\n        \"\"\"Return the `Markdown` content.\"\"\"\n        return self._content\n\n    @content.setter\n    def content(self, value: str) -> None:\n        self._content = value\n        self.update_displays()", ""]}
{"filename": "chatlab/views/assistant_function_call.py", "chunked_list": ["\"\"\"Views for the buffers.\"\"\"\n\nfrom typing import TypedDict\n\nfrom ..messaging import assistant_function_call\nfrom .abstracts import BufferView\nfrom .argument_buffer import ArgumentBuffer\n\nAssistantFunctionCallDict = TypedDict(\n    \"AssistantFunctionCallDict\",", "AssistantFunctionCallDict = TypedDict(\n    \"AssistantFunctionCallDict\",\n    {\n        \"function_name\": str,\n        \"function_arguments\": str,\n        \"display_id\": str,\n    },\n)\n\n\nclass AssistantFunctionCallView(BufferView):\n    \"\"\"A view for the assistant's message.\"\"\"\n\n    buffer: ArgumentBuffer\n\n    def __init__(self, function_name: str):\n        \"\"\"Initialize a `AssistantFunctionCallView` object with an optional message.\"\"\"\n        self.__function_name = function_name\n        super().__init__()\n\n    def create_buffer(self, content: str = \"\") -> ArgumentBuffer:\n        \"\"\"Creates the specific buffer for the view.\"\"\"\n        return ArgumentBuffer(self.__function_name, content)\n\n    def get_message(self):\n        \"\"\"Returns the crafted message.\"\"\"\n        return assistant_function_call(self.__function_name, self.content)\n\n    def finalize(self) -> AssistantFunctionCallDict:\n        \"\"\"Finalize the buffering.\"\"\"\n        return {\n            \"function_name\": self.__function_name,\n            \"function_arguments\": self.content,\n            \"display_id\": self.buffer._display_id,\n        }", "\n\nclass AssistantFunctionCallView(BufferView):\n    \"\"\"A view for the assistant's message.\"\"\"\n\n    buffer: ArgumentBuffer\n\n    def __init__(self, function_name: str):\n        \"\"\"Initialize a `AssistantFunctionCallView` object with an optional message.\"\"\"\n        self.__function_name = function_name\n        super().__init__()\n\n    def create_buffer(self, content: str = \"\") -> ArgumentBuffer:\n        \"\"\"Creates the specific buffer for the view.\"\"\"\n        return ArgumentBuffer(self.__function_name, content)\n\n    def get_message(self):\n        \"\"\"Returns the crafted message.\"\"\"\n        return assistant_function_call(self.__function_name, self.content)\n\n    def finalize(self) -> AssistantFunctionCallDict:\n        \"\"\"Finalize the buffering.\"\"\"\n        return {\n            \"function_name\": self.__function_name,\n            \"function_arguments\": self.content,\n            \"display_id\": self.buffer._display_id,\n        }", ""]}
{"filename": "chatlab/views/argument_buffer.py", "chunked_list": ["\"\"\"TODO: DOCSTRING.\"\"\"\n\nfrom chatlab.components.function_details import ChatFunctionComponent\n\nfrom .abstracts import BufferInterface\n\n\nclass ArgumentBuffer(BufferInterface):\n    \"\"\"A class for displaying arguments that update in place.\n\n    This version only supports updating the arguments, not the function name.\n\n    So far, OpenAI will only stream the arguments.\n    \"\"\"\n\n    __function_name: str\n    __function_arguments: str\n\n    __state: str = \"Generating\"\n\n    def __init__(self, function_name: str, function_arguments: str = \"\"):\n        \"\"\"Initialize a `ArgumentBuffer` object with an optional message.\"\"\"\n        self.__function_name = function_name\n        self.__function_arguments = function_arguments\n        super().__init__()\n\n    @property\n    def content(self) -> str:\n        \"\"\"Return the current arguments.\"\"\"\n        return self.__function_arguments\n\n    def append(self, delta: str) -> None:\n        \"\"\"Append to the arguments.\"\"\"\n        self.__function_arguments += delta\n        self.update_displays()\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        vdom_component = ChatFunctionComponent(\n            name=self.__function_name,\n            verbage=self.__state,\n            input=self.__function_arguments,\n        )\n        return {\n            \"text/html\": vdom_component.to_html(),\n            \"application/vdom.v1+json\": vdom_component.to_dict(),\n        }", ""]}
{"filename": "chatlab/builtins/noteable.py", "chunked_list": ["\"\"\"A built-in for interacting with Noteable notebooks.\"\"\"\nimport logging\nimport os\nimport uuid\nfrom typing import Optional\n\nimport httpx\nimport orjson\nimport ulid\nfrom origami.clients.api import APIClient, RTUClient", "import ulid\nfrom origami.clients.api import APIClient, RTUClient\nfrom origami.models.api.outputs import KernelOutput, KernelOutputContent\nfrom origami.models.kernels import KernelSession\nfrom origami.models.notebook import CodeCell, MarkdownCell, make_sql_cell\n\nfrom ._mediatypes import formats_for_llm\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\nlogger.setLevel(logging.DEBUG)\n\n\nclass NotebookClient:\n    \"\"\"A notebook client for use with Noteable.\"\"\"\n\n    api_client: APIClient\n    rtu_client: Optional[RTUClient]\n    kernel_session: KernelSession\n\n    def __init__(self, api_client: APIClient, rtu_client: RTUClient, file_id: uuid.UUID, kernel_session: KernelSession):\n        \"\"\"Create a new NotebookClient based on an existing API and RTU client.\"\"\"\n        self.api_client = api_client\n        self.rtu_client = rtu_client\n\n        self.file_id = file_id\n\n        # NOTE: We have to track the kernel session for now, though we probably\n        # should be pulling this based on the file ID instead.\n        self.kernel_session = kernel_session\n\n    @property\n    def notebook_url(self):\n        \"\"\"Get the URL for the notebook.\"\"\"\n        # HACK: Assuming the deployment is on the same domain as the API server.\n        # True in most cases.\n        base_url = self.api_client.api_base_url.split(\"/gate\")[0]\n        return f\"{base_url}/f/{self.file_id}\"\n\n    @classmethod\n    async def connect(cls, file_id, token=None):\n        \"\"\"Connect to an existing notebook.\"\"\"\n        return await cls.create(file_id=file_id, token=token)\n\n    @classmethod\n    async def create(cls, file_name=None, token=None, file_id=None, project_id=None):\n        \"\"\"Create a new notebook.\"\"\"\n        if token is None:\n            token = os.environ.get(\"NOTEABLE_TOKEN\")\n            assert token is not None\n\n        logger.info(\"Setting up API Client\")\n        api_client = APIClient(authorization_token=token)\n\n        if file_id is None:\n            if project_id is None:\n                user_info = await api_client.user_info()\n                # We'll use the user's default project ID for the rest of this example\n                project_id = user_info.origamist_default_project_id\n\n                if project_id is None:\n                    raise Exception(\"User has no default project\")\n\n            if file_name is None:\n                file_name = \"Untitled.ipynb\"\n            file = await api_client.create_notebook(project_id, file_name)\n            file_id = file.id\n\n        # prepare our realtime client\n        logger.info(\"Setting up RTU\")\n        rtu_client = await api_client.connect_realtime(file_id)\n\n        logger.info(\"Launching kernel\")\n        # Launch the kernel for the notebook\n        # We have to track the kernel_session for now\n        kernel_session = await api_client.launch_kernel(file_id)\n\n        cn = NotebookClient(api_client, rtu_client, file_id=file_id, kernel_session=kernel_session)\n\n        return cn\n\n    async def get_or_create_rtu_client(self):\n        \"\"\"Get or create an RTU client.\"\"\"\n        if self.rtu_client is None:\n            self.rtu_client = await self.api_client.connect_realtime(self.file_id)\n\n        return self.rtu_client\n\n    async def wait_for_kernel_idle(self):\n        \"\"\"Wait for the kernel to be idle.\"\"\"\n        rtu_client = await self.get_or_create_rtu_client()\n        await rtu_client.wait_for_kernel_idle()\n\n    async def create_cell(\n        self,\n        source: str,\n        cell_id: Optional[str] = None,\n        and_run: bool = False,\n        cell_type: str = \"code\",\n        after_cell_id: Optional[str] = None,\n        db_connection: Optional[str] = None,\n        assign_results_to: Optional[str] = None,\n    ):\n        \"\"\"Create a code, markdown, or SQL cell.\"\"\"\n        rtu_client = await self.get_or_create_rtu_client()\n\n        if after_cell_id is None:\n            existing_cells = rtu_client.cell_ids\n            if existing_cells and len(existing_cells) > 0:\n                after_cell_id = existing_cells[-1]\n\n        if cell_id is None:\n            cell_id = str(ulid.ULID())\n\n        cell = None\n\n        if cell_type == \"markdown\":\n            cell = MarkdownCell(source=source, id=cell_id)\n        elif cell_type == \"code\":\n            cell = CodeCell(source=source, id=cell_id)\n        elif cell_type == \"sql\":\n            if db_connection is None:\n                return \"You must specify a db_connection for SQL cells.\"\n\n            # db connection has to start with `@`\n            if not db_connection.startswith(\"@\"):\n                db_connection = f\"@{db_connection}\"\n            cell = make_sql_cell(\n                source=source, cell_id=cell_id, db_connection=db_connection, assign_results_to=assign_results_to\n            )\n\n        if cell is None:\n            return f\"Unknown cell type {cell_type}. Valid types are: markdown, code, sql.\"\n\n        logger.info(f\"Adding cell {cell_id} to notebook\")\n        cell = await rtu_client.add_cell(cell=cell, after_id=after_cell_id)\n        logger.info(f\"Added cell {cell_id} to notebook\")\n\n        if cell.cell_type != \"code\" or not and_run:\n            return cell\n\n        try:\n            logger.info(\"Running cell\")\n            return await self.run_cell(cell.id)\n        except Exception as e:\n            return f\"Cell created successfully. An error happened during run: {e}\"\n\n    async def _get_llm_friendly_outputs(self, output_collection_id: uuid.UUID):\n        \"\"\"Get the outputs for a given output collection ID.\"\"\"\n        output_collection = await self.api_client.get_output_collection(output_collection_id)\n\n        outputs = output_collection.outputs\n\n        # Not *that* friendly, but it's a start.\n        llm_friendly_outputs = []\n\n        for output in outputs:\n            content = output.content\n            if content is None:\n                continue\n\n            friendly_output = await self._get_llm_friendly_output(output)\n\n            if friendly_output is not None:\n                llm_friendly_outputs.append(friendly_output)\n\n        return llm_friendly_outputs\n\n    async def _extract_llm_plain(self, output: KernelOutput):\n        resp = await self.api_client.client.get(f\"/outputs/{output.id}\", params={\"mimetype\": \"text/llm+plain\"})\n        resp.raise_for_status()\n\n        output_for_llm = KernelOutput.parse_obj(resp.json())\n\n        if output_for_llm.content is None:\n            return None\n\n        return output_for_llm.content.raw\n\n    async def _extract_specific_mediatype(self, output: KernelOutput, mimetype: str):\n        resp = await self.api_client.client.get(f\"/outputs/{output.id}\", params={\"mimetype\": mimetype})\n        resp.raise_for_status()\n\n        output_for_llm = KernelOutput.parse_obj(resp.json())\n\n        if output_for_llm.content is None:\n            return None\n\n        return output_for_llm.content.raw\n\n    async def _extract_error(self, content: KernelOutputContent):\n        orjson_content = None\n        if content.raw is not None:\n            orjson_content = orjson.loads(content.raw)\n        elif content.url is not None:\n            async with httpx.AsyncClient() as client:\n                resp = await client.get(content.url)\n                # If the response failed, return None\n                if resp.status_code != 200:\n                    return None\n                try:\n                    orjson_content = orjson.loads(resp.content)\n                except orjson.JSONDecodeError:\n                    return None\n\n        if orjson_content is None:\n            return None\n\n        return f\"Error: {orjson_content['ename']}: {orjson_content['evalue']}\"\n\n    async def _get_llm_friendly_output(self, output: KernelOutput):\n        \"\"\"Get the output for a given output.\"\"\"\n        content = output.content\n        if content is None:\n            return None\n\n        if output.type == \"error\":\n            return await self._extract_error(content)\n\n        if 'text/llm+plain' in output.available_mimetypes:\n            # Fetch the specialized LLM+Plain directly\n            result = await self._extract_llm_plain(output)\n            if result is not None:\n                return result\n\n        if content.mimetype == 'text/html':\n            result = await self._extract_specific_mediatype(output, 'text/plain')\n            if result is not None:\n                return result\n\n        if content.mimetype == 'application/vnd.dataresource+json':\n            # TODO: Bring back a smaller representation to allow the LLM to do analysis\n            return \"<!-- DataFrame shown in notebook that user can see -->\"\n\n        if content.mimetype == 'application/vnd.plotly.v1+json':\n            return \"<!-- Plotly shown in notebook that user can see -->\"\n        if content.url is not None:\n            return \"<!-- Large output too large for chat. It is available in the notebook that the user can see -->\"\n\n        if content.mimetype in formats_for_llm:\n            return content.raw\n\n        mimetypes: list[str] = output.available_mimetypes\n\n        for format in formats_for_llm:\n            if format in mimetypes:\n                resp = await self.api_client.client.get(f\"/outputs/{output.id}?mimetype={format}\")\n                resp.raise_for_status()\n                if resp.status_code == 200:\n                    return\n\n                next_best_output = KernelOutput.parse_obj(resp.json())\n\n                if next_best_output.content is None:\n                    continue\n\n                if next_best_output.content.raw is not None:\n                    return next_best_output.content.raw\n\n    async def run_cell(self, cell_id: str):\n        \"\"\"Run a Cell within a Notebook by ID.\"\"\"\n        # Queue up the execution\n        rtu_client = await self.get_or_create_rtu_client()\n        queued_executions = await rtu_client.queue_execution(cell_id)\n        cell = await list(queued_executions)[0]\n\n        if cell.output_collection_id is None:\n            # Hypothesis: if the output collection ID is None, we're in a bad\n            # state. When the LLM sees this cell they will think its fine.\n            return cell\n\n        output_collection_id = cell.output_collection_id\n\n        if isinstance(output_collection_id, str):\n            try:\n                output_collection_id = uuid.UUID(output_collection_id)\n            except ValueError:\n                logger.exception(\"Invalid UUID\", exc_info=True)\n                return cell\n\n        outputs = await self._get_llm_friendly_outputs(output_collection_id)\n        response = \"\"\n        if len(outputs) == 0:\n            return response + \"\\nNo output.\"\n\n        response += \"\\nOut:\"\n\n        for output in outputs:\n            response += \"\\n\" + str(output)\n\n        return response\n\n    async def get_datasources(self):\n        \"\"\"Get a list of databases, AKA datasources.\"\"\"\n        datasources = await self.api_client.get_datasources_for_notebook(self.file_id)\n\n        resp_text = \"Datasources:\\n\"\n\n        for datasource in datasources:\n            print(datasource.dict(exclude_unset=True, exclude_none=True))\n            resp_text += f\"## {datasource.name}\\n\"\n            resp_text += f\"{datasource.description}\\n\"\n            resp_text += f\"datasource_id: {datasource.sql_cell_handle}\\n\\n\"\n            resp_text += f\"Type: {datasource.type_id}\\n\\n\"\n\n        return resp_text\n\n    async def get_cell(self, cell_id: str, with_outputs: bool = False):\n        \"\"\"Get a cell by ID.\"\"\"\n        rtu_client = await self.get_or_create_rtu_client()\n        try:\n            _, cell = rtu_client.builder.get_cell(cell_id)\n        except KeyError:\n            return f\"Cell {cell_id} not found.\"\n\n        noteable_metadata = cell.metadata.get(\"noteable\", {})\n\n        if noteable_metadata.get(\"cell_type\") == \"sql\":\n            source_type = \"sql\"\n\n        extra = \"\"\n\n        assign_results_to = noteable_metadata.get(\"assign_results_to\")\n        db_connection = noteable_metadata.get(\"db_connection\")\n        if db_connection is not None:\n            extra += f\", db_connection: {db_connection}\"\n\n        if assign_results_to is not None:\n            extra += f\", assign_to: {assign_results_to}\"\n\n        response = f\"<!-- {cell.cell_type.title()} Cell, ID: {cell_id}{extra} -->\\n\"\n\n        if cell.cell_type != \"code\":\n            response += cell.source\n            return response\n\n        source_type = rtu_client.builder.nb.metadata.get(\"kernelspec\", {}).get(\"language\", \"\")\n\n        if cell.metadata.get(\"noteable\", {}).get(\"cell_type\") == \"sql\":\n            source_type = \"sql\"\n\n        # Convert to a plaintext response\n        response += f\"\\nIn:\\n\\n```{source_type}\\n\"\n        response += cell.source\n        response += \"\\n```\\n\"\n\n        if not with_outputs:\n            return response\n\n        output_collection_id = cell.output_collection_id\n\n        if isinstance(output_collection_id, str):\n            try:\n                output_collection_id = uuid.UUID(output_collection_id)\n            except ValueError:\n                # This is a case where the output collection ID in the notebook is invalid\n                logger.exception(\"Invalid UUID\", exc_info=True)\n                return response + \"\\nUnable to get outputs.\"\n\n        if output_collection_id is None:\n            return response + \"\\nNo output.\"\n\n        outputs = await self._get_llm_friendly_outputs(output_collection_id)\n        if len(outputs) == 0:\n            return response + \"\\nNo output.\"\n\n        response += \"\\nOut:\"\n\n        for output in outputs:\n            response += \"\\n\" + str(output)\n\n        return response\n\n    async def get_cell_ids(self):\n        \"\"\"Get a list of cell IDs.\"\"\"\n        rtu_client = await self.get_or_create_rtu_client()\n        return rtu_client.cell_ids\n\n    async def shutdown(self):\n        \"\"\"Shutdown the notebook.\"\"\"\n        if self.rtu_client is not None:\n            await self.rtu_client.shutdown()\n        await self.api_client.shutdown_kernel(self.kernel_session.id)\n\n        self.rtu_client = None\n\n    \"\"\"This `python` function is here for dealing with ChatGPT's `python` hallucination.\"\"\"\n\n    async def python(self, code: str):\n        \"\"\"Creates a python cell, runs it, and returns output.\"\"\"\n        return await self.create_cell(code, and_run=True)\n\n    @property\n    def chat_functions(self):\n        \"\"\"Functions to expose for LLMs.\"\"\"\n        return [\n            self.create_cell,\n            self.run_cell,\n            self.get_cell,\n            self.get_cell_ids,\n            self.get_datasources,\n        ]", "\n\n__all__ = [\"NotebookClient\"]\n\n\n# Only expose the NotebookClient to tab completion\ndef __dir__():\n    return __all__\n", ""]}
{"filename": "chatlab/builtins/_mediatypes.py", "chunked_list": ["\"\"\"Media types for rich output for LLMs and in-notebook.\"\"\"\nimport json\nfrom typing import Optional\n\nfrom IPython.display import display\nfrom IPython.utils.capture import RichOutput\n\n# Prioritized formats to show to large language models\nformats_for_llm = [\n    # Repr LLM is the richest text", "formats_for_llm = [\n    # Repr LLM is the richest text\n    'text/llm+plain',\n    # Assume that if we get markdown we know it's rich for an LLM\n    'text/markdown',\n    # Same with LaTeX\n    'text/latex',\n    # All the normal ones\n    'application/vnd.jupyter.error+json',\n    # 'application/vdom.v1+json',", "    'application/vnd.jupyter.error+json',\n    # 'application/vdom.v1+json',\n    'application/json',\n    # Since every object has a text/plain repr, even though the LLM would understand `text/plain` well,\n    # bumping this priority up would override more rich media types we definitely want to show.\n    'text/plain',\n    # Both HTML and SVG should be conditional on size, considering many libraries\n    # Will emit giant JavaScript blobs for interactivity\n    # For now, we'll assume not to show these\n    # 'text/html',", "    # For now, we'll assume not to show these\n    # 'text/html',\n    # 'image/svg+xml',\n]\n\n# Prioritized formats to redisplay for the user, since we capture the output during execution\nformats_to_redisplay = [\n    'application/vnd.jupyter.widget-view+json',\n    'application/vnd.dex.v1+json',\n    'application/vnd.dataresource+json',", "    'application/vnd.dex.v1+json',\n    'application/vnd.dataresource+json',\n    'application/vnd.plotly.v1+json',\n    'text/vnd.plotly.v1+html',\n    'application/vdom.v1+json',\n    'application/json',\n    'application/javascript',\n    'image/svg+xml',\n    'image/png',\n    'image/jpeg',", "    'image/png',\n    'image/jpeg',\n    'image/gif',\n    'text/html',\n    'image/svg+xml',\n]\n\n\ndef redisplay_superrich(output: RichOutput):\n    \"\"\"Redisplay an image.\"\"\"\n    data = output.data\n    metadata = output.metadata\n\n    richest_format = find_richest_format(data, formats_to_redisplay)\n\n    if richest_format:\n        display(\n            data,\n            raw=True,\n        )\n\n        data.pop(richest_format, None)\n        metadata.pop(richest_format, None)\n\n        # Check to see if it already has a text/llm+plain representation\n        if 'text/llm+plain' in data:\n            return\n\n        if richest_format.startswith('image/'):\n            # Allow the LLM to see that we displayed for the user\n            data['text/llm+plain'] = data['text/plain']\n        else:\n            data['text/llm+plain'] = f\"<Displayed {richest_format}>\"", "def redisplay_superrich(output: RichOutput):\n    \"\"\"Redisplay an image.\"\"\"\n    data = output.data\n    metadata = output.metadata\n\n    richest_format = find_richest_format(data, formats_to_redisplay)\n\n    if richest_format:\n        display(\n            data,\n            raw=True,\n        )\n\n        data.pop(richest_format, None)\n        metadata.pop(richest_format, None)\n\n        # Check to see if it already has a text/llm+plain representation\n        if 'text/llm+plain' in data:\n            return\n\n        if richest_format.startswith('image/'):\n            # Allow the LLM to see that we displayed for the user\n            data['text/llm+plain'] = data['text/plain']\n        else:\n            data['text/llm+plain'] = f\"<Displayed {richest_format}>\"", "\n\ndef pluck_richest_text(output: RichOutput):\n    \"\"\"Format an object as rich text.\"\"\"\n    data = output.data\n    metadata = output.metadata\n\n    richest_format = find_richest_format(data, formats_for_llm)\n\n    if richest_format:\n        d = data.pop(richest_format, None)\n        m = metadata.pop(richest_format, None)\n\n        if isinstance(d, dict):\n            d = json.dumps(d, indent=2)\n\n        # TODO: Reduce the size of the data if it's too big for LLMs.\n        return d, m\n\n    return None, {}", "\n\ndef find_richest_format(payload: dict, formats: list[str]) -> Optional[str]:\n    for format in formats:\n        if format in payload:\n            return format\n\n    return None\n", ""]}
{"filename": "chatlab/builtins/files.py", "chunked_list": ["\"\"\"These are all functions for file operations to expose to a Large Language Model.\n\n\u26a0\ufe0f \u2622\ufe0f WARNING \u2622\ufe0f \u26a0\ufe0f\nThe model will do useful things and the model will do destructive things.\n\nGit and Docker can be your friends when you let a probalistc model loose on your filesystem.\n\nYou've been warned. Have fun and be safe!\n\"\"\"\nimport asyncio", "\"\"\"\nimport asyncio\nimport os\n\nimport aiofiles\n\nfrom chatlab.decorators import expose_exception_to_llm\n\n\n@expose_exception_to_llm", "\n@expose_exception_to_llm\nasync def list_files(directory: str) -> list[str]:\n    \"\"\"List all files in the given directory.\n\n    Args:\n    - directory: str, the directory to list files from\n\n    Returns:\n    - list[str]: the list of files in the given directory", "    Returns:\n    - list[str]: the list of files in the given directory\n\n    \"\"\"\n    files = await asyncio.to_thread(os.listdir, directory)\n    return files\n\n\n@expose_exception_to_llm\nasync def get_file_size(file_path: str) -> int:", "@expose_exception_to_llm\nasync def get_file_size(file_path: str) -> int:\n    \"\"\"Get the size of the file in bytes asynchronously.\n\n    Args:\n    - file_path: The path to the file\n\n    Returns:\n    - The size of the file in bytes\n    \"\"\"", "    - The size of the file in bytes\n    \"\"\"\n    size = await asyncio.to_thread(os.path.getsize, file_path)\n    return size\n\n\n@expose_exception_to_llm\nasync def is_file(file_path: str) -> bool:\n    \"\"\"Check if the given path points to a file asynchronously.\n", "    \"\"\"Check if the given path points to a file asynchronously.\n\n    Args:\n    - file_path: The path to check\n\n    Returns:\n    - True if the path points to a file, False otherwise\n    \"\"\"\n    is_file = await asyncio.to_thread(os.path.isfile, file_path)\n    return is_file", "    is_file = await asyncio.to_thread(os.path.isfile, file_path)\n    return is_file\n\n\n@expose_exception_to_llm\nasync def write_file(file_path: str, content: str, mode: str = 'w') -> None:\n    \"\"\"Write content to a file.\n\n    Args:\n    - file_path: The path to the file", "    Args:\n    - file_path: The path to the file\n    - content: The content to be written\n    - mode: The writing mode, default is 'w' for write\n\n    Returns:\n    - None\n    \"\"\"\n    async with aiofiles.open(file_path, mode) as file:  # type: ignore\n        await file.write(content)", "    async with aiofiles.open(file_path, mode) as file:  # type: ignore\n        await file.write(content)\n\n\n@expose_exception_to_llm\nasync def read_file(file_path: str, mode: str = 'r') -> str:\n    \"\"\"Read content from a file.\n\n    Args:\n    - file_path: The path to the file", "    Args:\n    - file_path: The path to the file\n    - mode: The reading mode, default is 'r' for read\n\n    Returns:\n    - str: The content of the file\n    \"\"\"\n    async with aiofiles.open(file_path, mode) as file:  # type: ignore\n        content = await file.read()\n    return content", "        content = await file.read()\n    return content\n\n\n@expose_exception_to_llm\nasync def is_directory(directory: str) -> bool:\n    \"\"\"Check if the given path points to a directory asynchronously.\n\n    Args:\n    - directory: The path to check", "    Args:\n    - directory: The path to check\n\n    Returns:\n    - True if the path points to a directory, False otherwise\n    \"\"\"\n    is_directory = await asyncio.to_thread(os.path.isdir, directory)\n    return is_directory\n\n", "\n\nchat_functions = [list_files, get_file_size, is_file, is_directory, write_file, read_file]\n"]}
{"filename": "chatlab/builtins/__init__.py", "chunked_list": ["\"\"\"Builtins for ChatLab.\"\"\"\n\nfrom deprecation import deprecated\n\nfrom .files import chat_functions as file_functions\nfrom .python import run_python\nfrom .shell import chat_functions as shell_functions\n\n# To prevent naming confusion, the builtin that isn't really running a cell\n# is deprecated.", "# To prevent naming confusion, the builtin that isn't really running a cell\n# is deprecated.\nrun_cell = deprecated(\n    deprecated_in=\"1.0.0\",\n    removed_in=\"2.0.0\",\n    details=\"run_cell is deprecated. Use run_python instead for same-session execution.\",\n)(run_python)\n\n# compose all the file, shell, and python functions into one list for ease of use\nos_functions = file_functions + shell_functions + [run_python]", "# compose all the file, shell, and python functions into one list for ease of use\nos_functions = file_functions + shell_functions + [run_python]\n\n__all__ = [\"run_python\", \"run_cell\", \"file_functions\", \"shell_functions\", \"os_functions\"]\n"]}
{"filename": "chatlab/builtins/python.py", "chunked_list": ["\"\"\"The in-IPython python code runner for ChatLab.\"\"\"\nfrom traceback import TracebackException\nfrom typing import Optional\n\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom IPython.utils.capture import capture_output\nfrom repr_llm import register_llm_formatter\nfrom repr_llm.pandas import format_dataframe_for_llm, format_series_for_llm\n\nfrom chatlab.decorators import expose_exception_to_llm", "\nfrom chatlab.decorators import expose_exception_to_llm\n\nfrom ._mediatypes import pluck_richest_text, redisplay_superrich\n\n\ndef apply_llm_formatter(shell: InteractiveShell):\n    \"\"\"Apply the LLM formatter to the given shell.\"\"\"\n    llm_formatter = register_llm_formatter(shell)\n\n    llm_formatter.for_type_by_name('pandas.core.frame', 'DataFrame', format_dataframe_for_llm)\n    llm_formatter.for_type_by_name('pandas.core.series', 'Series', format_series_for_llm)", "\n\ndef get_or_create_ipython() -> InteractiveShell:\n    \"\"\"Get the current IPython shell or create a new one.\"\"\"\n    shell = None\n    # This is what `get_ipython` does. For type inference to work, we need to\n    # do it manually.\n    if InteractiveShell.initialized():\n        shell = InteractiveShell.instance()\n        apply_llm_formatter(shell)\n\n    if not shell:\n        shell = InteractiveShell()\n        apply_llm_formatter(shell)\n\n    return shell", "\n\nclass ChatLabShell:\n    \"\"\"A custom shell for ChatLab that uses the current IPython shell and formats outputs for LLMs.\"\"\"\n\n    shell: InteractiveShell\n\n    def __init__(self, shell: Optional[InteractiveShell] = None):\n        \"\"\"Create a new ChatLabShell.\"\"\"\n        self.shell = get_or_create_ipython()\n\n    def run_cell(self, code: str):\n        \"\"\"Execute code in python and return the result.\"\"\"\n        try:\n            # Since we include the traceback inside the ChatLab display, we\n            # don't want to show it inline.\n            # Sadly `capture_output` doesn't grab the show traceback side effect,\n            # so we have to do it manually.\n            original_showtraceback = self.shell.showtraceback\n            with capture_output() as captured:\n                # HACK: don't show the exception inline if the LLM is running it\n                self.shell.showtraceback = lambda *args, **kwargs: None  # type: ignore\n                result = self.shell.run_cell(code)\n                self.shell.showtraceback = original_showtraceback  # type: ignore\n        except Exception as e:\n            self.shell.showtraceback = original_showtraceback  # type: ignore\n            formatted = TracebackException.from_exception(e, limit=3).format(chain=True)\n            plaintext_traceback = '\\n'.join(formatted)\n\n            return plaintext_traceback\n\n        if not result.success:\n            # Grab which exception was raised\n            exception = result.error_before_exec or result.error_in_exec\n\n            # If success was False and yet neither of these are set, then\n            # something went wrong in the IPython internals\n            if exception is None:\n                raise Exception(\"Unknown IPython error for result\", result)\n\n            # Create a formatted traceback that includes the last 3 frames\n            # and the exception message\n            formatted = TracebackException.from_exception(exception, limit=3).format(chain=True)\n            plaintext_traceback = '\\n'.join(formatted)\n\n            return plaintext_traceback\n\n        outputs = \"\"\n\n        if captured.stdout is not None and captured.stdout.strip() != '':\n            stdout = captured.stdout\n            # Truncate stdout if it's too long\n            if len(stdout) > 1000:\n                stdout = stdout[:500] + '...[TRUNCATED]...' + stdout[-500:]\n\n            outputs += f\"STDOUT:\\n{stdout}\\n\\n\"\n\n        if captured.stderr is not None and captured.stderr.strip() != '':\n            stderr = captured.stderr\n            if len(stderr) > 1000:\n                stdout = stderr[:500] + '...[TRUNCATED]...' + stderr[-500:]\n            outputs += f\"STDERR:\\n{stderr}\\n\\n\"\n\n        if captured.outputs is not None:\n            for output in captured.outputs:\n                # If image/* are in the output, redisplay it\n                # then include a text/plain version of the object, telling the llm\n                # that the image is displayed for the user\n                redisplay_superrich(output)\n\n                # Now for text for the llm\n                text, _ = pluck_richest_text(output)\n\n                if text is None:\n                    continue\n\n                outputs += f\"OUTPUT:\\n{text}\\n\\n\"\n\n        if result.result is not None:\n            output = result.result\n            # If image/* are in the output, redisplay it\n            # then include a text/plain version of the object, telling the llm\n            # that the image is displayed for the user\n            redisplay_superrich(result.result)\n\n            # Now for text for the llm\n            text, _ = pluck_richest_text(result.result)\n\n            if text is not None:\n                outputs += f\"RESULT:\\n{text}\\n\\n\"\n\n        return outputs", "\n\n__shell: Optional[ChatLabShell] = None\n\n\n@expose_exception_to_llm\ndef run_python(code: str):\n    \"\"\"Execute code in python and return the result.\"\"\"\n    global __shell\n\n    if __shell is None:\n        # Since ChatLabShell has imports that are \"costly\" (e.g. IPython, numpy, pandas),\n        # we only import it on the first call to run_cell.\n        from .python import ChatLabShell\n\n        __shell = ChatLabShell()\n\n    return __shell.run_cell(code)", "\n\n__all__ = [\"run_python\", \"ChatLabShell\"]\n\n\ndef __dir__():\n    return __all__\n"]}
{"filename": "chatlab/builtins/shell.py", "chunked_list": ["\"\"\"Shell commands for ChatLab.\"\"\"\nimport asyncio\nimport subprocess\n\nfrom chatlab.decorators import expose_exception_to_llm\n\n\n@expose_exception_to_llm\nasync def run_shell_command(command: str):\n    \"\"\"Run a shell command and return the output.", "async def run_shell_command(command: str):\n    \"\"\"Run a shell command and return the output.\n\n    Args:\n    - command: str, the shell command to execute\n\n    Returns:\n    - str: the output of the shell command\n    \"\"\"\n    process = await asyncio.create_subprocess_shell(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)", "    \"\"\"\n    process = await asyncio.create_subprocess_shell(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = await process.communicate()\n\n    resp = f\"Return Code: {process.returncode}\\n\"\n    resp += f\"stdout: ```\\n{stdout.decode().strip()}\\n```\\n\"\n    resp += f\"stderr: ```\\n{stderr.decode().strip()}\\n```\"\n\n    return resp\n", "    return resp\n\n\nchat_functions = [run_shell_command]\n"]}
