{"filename": "server/tests/conftest.py", "chunked_list": ["import pytest\n\nfrom embedding_server.pb import embedding_pb2\n\n\n@pytest.fixture\ndef default_pb_parameters():\n    return embedding_pb2.NextTokenChooserParameters(\n        temperature=1.0,\n        repetition_penalty=1.0,\n        top_k=0,\n        top_p=1.0,\n        typical_p=1.0,\n        do_sample=False,\n    )", "\n\n@pytest.fixture\ndef default_pb_stop_parameters():\n    return embedding_pb2.StoppingCriteriaParameters(stop_sequences=[], max_new_tokens=10)\n"]}
{"filename": "server/tests/utils/test_hub.py", "chunked_list": ["import pytest\n\nfrom embedding_server.utils.hub import (\n    weight_hub_files,\n    download_weights,\n    weight_files,\n    EntryNotFoundError,\n    LocalEntryNotFoundError,\n    RevisionNotFoundError,\n)", "    RevisionNotFoundError,\n)\n\n\ndef test_weight_hub_files():\n    filenames = weight_hub_files(\"bigscience/bloom-560m\")\n    assert filenames == [\"model.safetensors\"]\n\n\ndef test_weight_hub_files_llm():\n    filenames = weight_hub_files(\"bigscience/bloom\")\n    assert filenames == [f\"model_{i:05d}-of-00072.safetensors\" for i in range(1, 73)]", "\ndef test_weight_hub_files_llm():\n    filenames = weight_hub_files(\"bigscience/bloom\")\n    assert filenames == [f\"model_{i:05d}-of-00072.safetensors\" for i in range(1, 73)]\n\n\ndef test_weight_hub_files_empty():\n    with pytest.raises(EntryNotFoundError):\n        weight_hub_files(\"bigscience/bloom\", extension=\".errors\")\n", "\n\ndef test_download_weights():\n    model_id = \"bigscience/bloom-560m\"\n    filenames = weight_hub_files(model_id)\n    files = download_weights(filenames, model_id)\n    local_files = weight_files(\"bigscience/bloom-560m\")\n    assert files == local_files\n\n\ndef test_weight_files_error():\n    with pytest.raises(RevisionNotFoundError):\n        weight_files(\"bigscience/bloom-560m\", revision=\"error\")\n    with pytest.raises(LocalEntryNotFoundError):\n        weight_files(\"bert-base-uncased\")", "\n\ndef test_weight_files_error():\n    with pytest.raises(RevisionNotFoundError):\n        weight_files(\"bigscience/bloom-560m\", revision=\"error\")\n    with pytest.raises(LocalEntryNotFoundError):\n        weight_files(\"bert-base-uncased\")\n"]}
{"filename": "server/tests/utils/test_watermark.py", "chunked_list": ["# test_watermark_logits_processor.py\n\nimport os\nimport numpy as np\nimport torch\nfrom embedding_server.utils.watermark import WatermarkLogitsProcessor\n\n\nGAMMA = os.getenv(\"WATERMARK_GAMMA\", 0.5)\nDELTA = os.getenv(\"WATERMARK_DELTA\", 2.0)", "GAMMA = os.getenv(\"WATERMARK_GAMMA\", 0.5)\nDELTA = os.getenv(\"WATERMARK_DELTA\", 2.0)\n\n\ndef test_seed_rng():\n    input_ids = [101, 2036, 3731, 102, 2003, 103]\n    processor = WatermarkLogitsProcessor()\n    processor._seed_rng(input_ids)\n    assert isinstance(processor.rng, torch.Generator)\n", "\n\ndef test_get_greenlist_ids():\n    input_ids = [101, 2036, 3731, 102, 2003, 103]\n    processor = WatermarkLogitsProcessor()\n    result = processor._get_greenlist_ids(input_ids, 10, torch.device(\"cpu\"))\n    assert max(result) <= 10\n    assert len(result) == int(10 * 0.5)\n\n\ndef test_calc_greenlist_mask():\n    processor = WatermarkLogitsProcessor()\n    scores = torch.tensor([[0.5, 0.3, 0.2, 0.8], [0.1, 0.2, 0.7, 0.9]])\n    greenlist_token_ids = torch.tensor([2, 3])\n    result = processor._calc_greenlist_mask(scores, greenlist_token_ids)\n    assert result.tolist() == [[False, False, False, False], [False, False, True, True]]\n    assert result.shape == scores.shape", "\n\ndef test_calc_greenlist_mask():\n    processor = WatermarkLogitsProcessor()\n    scores = torch.tensor([[0.5, 0.3, 0.2, 0.8], [0.1, 0.2, 0.7, 0.9]])\n    greenlist_token_ids = torch.tensor([2, 3])\n    result = processor._calc_greenlist_mask(scores, greenlist_token_ids)\n    assert result.tolist() == [[False, False, False, False], [False, False, True, True]]\n    assert result.shape == scores.shape\n", "\n\ndef test_bias_greenlist_logits():\n    processor = WatermarkLogitsProcessor()\n    scores = torch.tensor([[0.5, 0.3, 0.2, 0.8], [0.1, 0.2, 0.7, 0.9]])\n    green_tokens_mask = torch.tensor(\n        [[False, False, True, True], [False, False, False, True]]\n    )\n    greenlist_bias = 2.0\n    result = processor._bias_greenlist_logits(scores, green_tokens_mask, greenlist_bias)\n    assert np.allclose(result.tolist(), [[0.5, 0.3, 2.2, 2.8], [0.1, 0.2, 0.7, 2.9]])\n    assert result.shape == scores.shape", "\n\ndef test_call():\n    input_ids = [101, 2036, 3731, 102, 2003, 103]\n    processor = WatermarkLogitsProcessor()\n    scores = torch.tensor([[0.5, 0.3, 0.2, 0.8], [0.1, 0.2, 0.7, 0.9]])\n    result = processor(input_ids, scores)\n    assert result.shape == scores.shape\n", ""]}
{"filename": "server/tests/utils/test_convert.py", "chunked_list": ["from embedding_server.utils.hub import (\n    download_weights,\n    weight_hub_files,\n    weight_files,\n)\n\nfrom embedding_server.utils.convert import convert_files\n\n\ndef test_convert_files():\n    model_id = \"bigscience/bloom-560m\"\n    pt_filenames = weight_hub_files(model_id, extension=\".bin\")\n    local_pt_files = download_weights(pt_filenames, model_id)\n    local_st_files = [\n        p.parent / f\"{p.stem.lstrip('pytorch_')}.safetensors\" for p in local_pt_files\n    ]\n    convert_files(local_pt_files, local_st_files)\n\n    found_st_files = weight_files(model_id)\n\n    assert all([p in found_st_files for p in local_st_files])", "\ndef test_convert_files():\n    model_id = \"bigscience/bloom-560m\"\n    pt_filenames = weight_hub_files(model_id, extension=\".bin\")\n    local_pt_files = download_weights(pt_filenames, model_id)\n    local_st_files = [\n        p.parent / f\"{p.stem.lstrip('pytorch_')}.safetensors\" for p in local_pt_files\n    ]\n    convert_files(local_pt_files, local_st_files)\n\n    found_st_files = weight_files(model_id)\n\n    assert all([p in found_st_files for p in local_st_files])", ""]}
{"filename": "server/tests/utils/test_tokens.py", "chunked_list": ["from embedding_server.utils.tokens import (\n    StopSequenceCriteria,\n    StoppingCriteria,\n    FinishReason,\n)\n\n\ndef test_stop_sequence_criteria():\n    criteria = StopSequenceCriteria(\"/test;\")\n\n    assert not criteria(\"/\")\n    assert not criteria(\"/test\")\n    assert criteria(\"/test;\")\n    assert not criteria(\"/test; \")", "\n\ndef test_stop_sequence_criteria_escape():\n    criteria = StopSequenceCriteria(\"<|stop|>\")\n\n    assert not criteria(\"<\")\n    assert not criteria(\"<|stop\")\n    assert criteria(\"<|stop|>\")\n    assert not criteria(\"<|stop|> \")\n", "\n\ndef test_stopping_criteria():\n    criteria = StoppingCriteria(0, [StopSequenceCriteria(\"/test;\")], max_new_tokens=5)\n    assert criteria(65827, \"/test\") == (False, None)\n    assert criteria(30, \";\") == (True, FinishReason.FINISH_REASON_STOP_SEQUENCE)\n\n\ndef test_stopping_criteria_eos():\n    criteria = StoppingCriteria(0, [StopSequenceCriteria(\"/test;\")], max_new_tokens=5)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(0, \"\") == (True, FinishReason.FINISH_REASON_EOS_TOKEN)", "def test_stopping_criteria_eos():\n    criteria = StoppingCriteria(0, [StopSequenceCriteria(\"/test;\")], max_new_tokens=5)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(0, \"\") == (True, FinishReason.FINISH_REASON_EOS_TOKEN)\n\n\ndef test_stopping_criteria_max():\n    criteria = StoppingCriteria(0, [StopSequenceCriteria(\"/test;\")], max_new_tokens=5)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(1, \"\") == (False, None)\n    assert criteria(1, \"\") == (True, FinishReason.FINISH_REASON_LENGTH)", ""]}
{"filename": "server/tests/models/test_bloom.py", "chunked_list": ["import pytest\nimport torch\n\nfrom copy import copy\nfrom transformers import AutoTokenizer\n\nfrom embedding_server.pb import generate_pb2\nfrom embedding_server.models.causal_lm import EncoderBatch\nfrom embedding_server.models.bloom import BloomCausalLMBatch, BLOOM\n", "from embedding_server.models.bloom import BloomCausalLMBatch, BLOOM\n\n\n@pytest.fixture(scope=\"session\")\ndef default_bloom():\n    return BLOOM(\"bigscience/bloom-560m\")\n\n\n@pytest.fixture(scope=\"session\")\ndef bloom_560m_tokenizer():\n    return AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\", padding_side=\"left\")", "@pytest.fixture(scope=\"session\")\ndef bloom_560m_tokenizer():\n    return AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\", padding_side=\"left\")\n\n\n@pytest.fixture\ndef default_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"Test\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )", "\n\n@pytest.fixture\ndef default_pb_batch(default_pb_request):\n    return generate_pb2.Batch(id=0, requests=[default_pb_request], size=1)\n\n\n@pytest.fixture\ndef default_bloom_batch(default_pb_batch, bloom_560m_tokenizer):\n    return BloomCausalLMBatch.from_pb(\n        default_pb_batch, bloom_560m_tokenizer, torch.device(\"cpu\")\n    )", "def default_bloom_batch(default_pb_batch, bloom_560m_tokenizer):\n    return BloomCausalLMBatch.from_pb(\n        default_pb_batch, bloom_560m_tokenizer, torch.device(\"cpu\")\n    )\n\n\n@pytest.fixture\ndef default_multi_requests_bloom_batch(default_pb_request, bloom_560m_tokenizer):\n    req_0 = copy(default_pb_request)\n    req_0.id = 1\n    req_1 = default_pb_request\n    req_1.id = 2\n    req_1.stopping_parameters.max_new_tokens = 5\n\n    batch_pb = generate_pb2.Batch(id=0, requests=[req_0, req_1], size=2)\n    return BloomCausalLMBatch.from_pb(\n        batch_pb, bloom_560m_tokenizer, torch.device(\"cpu\")\n    )", "\n\ndef test_batch_from_pb(default_pb_batch, default_bloom_batch):\n    batch = default_bloom_batch\n\n    assert batch.batch_id == default_pb_batch.id\n    assert batch.requests == default_pb_batch.requests\n\n    assert len(batch.input_ids) == default_pb_batch.size\n    assert batch.input_ids[0][-1] == 10264\n    assert torch.all(batch.input_ids[0][:-1] == 3)\n\n    assert batch.attention_mask[0][0] == 1\n    assert torch.all(batch.attention_mask[0][1:] == 0)\n\n    assert batch.past_key_values is None\n\n    assert all(\n        [\n            torch.equal(input_ids, all_input_ids[:, 0])\n            for input_ids, all_input_ids in zip(batch.input_ids, batch.all_input_ids)\n        ]\n    )\n\n    assert batch.input_lengths == [1]\n\n    assert len(batch) == default_pb_batch.size\n    assert len(batch.next_token_choosers) == len(batch.stopping_criterias) == len(batch)\n\n    assert batch.max_input_length == batch.input_lengths[0]", "\n\ndef test_batch_concatenate_no_prefill(default_bloom_batch):\n    with pytest.raises(ValueError):\n        BloomCausalLMBatch.concatenate([default_bloom_batch, default_bloom_batch])\n\n\ndef test_causal_lm_batch_type(default_bloom):\n    assert default_bloom.batch_type == BloomCausalLMBatch\n", "\n\ndef test_causal_lm_generate_token(default_bloom, default_bloom_batch):\n    sequence_length = len(default_bloom_batch.all_input_ids[0])\n    generations, next_batch = default_bloom.generate_token(default_bloom_batch)\n\n    assert len(generations) == len(default_bloom_batch)\n    assert isinstance(next_batch, EncoderBatch)\n    assert not next_batch.keys_head_dim_last\n\n    assert len(next_batch.all_input_ids) == len(next_batch)\n    assert len(next_batch.all_input_ids[0]) == sequence_length + 1\n    assert len(next_batch.attention_mask[0]) == 11\n    assert torch.all(next_batch.all_input_ids[0][-2:] == 10264)\n    assert torch.all(next_batch.all_input_ids[0][:-2] == 3)\n\n    assert torch.all(next_batch.attention_mask[0][:2] == 1)\n    assert torch.all(next_batch.attention_mask[0][2:] == 0)\n\n    assert next_batch.input_ids.shape == (len(next_batch), 1)\n    assert next_batch.input_ids[0, 0] == 10264\n\n    assert next_batch.input_lengths == [2]\n    assert next_batch.max_input_length == next_batch.input_lengths[0]\n\n    assert next_batch.past_key_values is not None\n    assert all(\n        [p[0].shape == (16, 64, sequence_length) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[1].shape == (16, sequence_length, 64) for p in next_batch.past_key_values]\n    )\n    assert all([generation.generated_text is None for generation in generations])\n    assert all([len(generation.prefill_tokens) == 1 for generation in generations])\n    assert all([generation.token_id.item() == 10264 for generation in generations])\n    assert all([generation.token_text == \"Test\" for generation in generations])\n    assert generations[0].request_id == 0", "\n\ndef test_causal_lm_generate_token_completion(default_bloom, default_bloom_batch):\n    next_batch = default_bloom_batch\n    for _ in range(default_bloom_batch.stopping_criterias[0].max_new_tokens - 1):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(default_bloom_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert (\n        generations[0].generated_text.text == \"TestTestTestTestTestTestTestTestTestTest\"\n    )\n    assert generations[0].request_id == default_bloom_batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_bloom_batch.stopping_criterias[0].max_new_tokens\n    )", "\n\ndef test_causal_lm_generate_token_completion_multi(\n    default_bloom, default_multi_requests_bloom_batch\n):\n    next_batch = default_multi_requests_bloom_batch\n\n    for i in range(\n        default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens - 1\n    ):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(default_multi_requests_bloom_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert generations[1].generated_text.text == \"TestTestTestTestTest\"\n    assert (\n        generations[1].request_id == default_multi_requests_bloom_batch.requests[1].id\n    )\n    assert (\n        generations[1].generated_text.generated_tokens\n        == default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens\n    )\n    # Copy stopping_criterias before filtering\n    stopping_criterias = default_multi_requests_bloom_batch.stopping_criterias.copy()\n\n    next_batch = next_batch.filter([next_batch.requests[0]])\n\n    for _ in range(\n        stopping_criterias[0].max_new_tokens - stopping_criterias[1].max_new_tokens - 1\n    ):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert (\n        generations[0].generated_text.text == \"TestTestTestTestTestTestTestTestTestTest\"\n    )\n    assert (\n        generations[0].request_id == default_multi_requests_bloom_batch.requests[0].id\n    )\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_multi_requests_bloom_batch.stopping_criterias[0].max_new_tokens\n    )", "\n\ndef test_batch_concatenate(\n    default_bloom, default_bloom_batch, default_multi_requests_bloom_batch\n):\n    next_batch_0 = default_bloom_batch\n    _, next_batch_0 = default_bloom.generate_token(next_batch_0)\n    _, next_batch_0 = default_bloom.generate_token(next_batch_0)\n\n    next_batch_1 = default_multi_requests_bloom_batch\n    _, next_batch_1 = default_bloom.generate_token(next_batch_1)\n\n    # Clone past_key_values before concatenating to compare after,\n    # because they are removed from the concatenated batches\n    next_batch_0_past_key_values = [\n        (k.clone(), v.clone()) for (k, v) in next_batch_0.past_key_values\n    ]\n    next_batch_1_past_key_values = [\n        (k.clone(), v.clone()) for (k, v) in next_batch_1.past_key_values\n    ]\n\n    next_batch = BloomCausalLMBatch.concatenate([next_batch_0, next_batch_1])\n\n    assert torch.equal(next_batch.all_input_ids[0], next_batch_0.all_input_ids[0])\n    assert torch.equal(next_batch.all_input_ids[1], next_batch_1.all_input_ids[0])\n    assert torch.equal(next_batch.all_input_ids[2], next_batch_1.all_input_ids[1])\n\n    assert torch.all(\n        next_batch.attention_mask[0, : -next_batch.padding_right_offset] == 1\n    )\n    assert torch.all(\n        next_batch.attention_mask[1:, 1 : -next_batch.padding_right_offset] == 1\n    )\n    assert torch.all(next_batch.attention_mask[1:, 3:] == 0)\n\n    assert next_batch.batch_id == 0\n    assert torch.all(next_batch.input_ids == 10264)\n\n    assert next_batch.input_lengths == [3, 2, 2]\n    assert next_batch.max_input_length == 3\n\n    assert next_batch.requests[0] == next_batch_0.requests[0]\n    assert next_batch.requests[1:] == next_batch_1.requests\n\n    assert next_batch.next_token_choosers[0] == next_batch_0.next_token_choosers[0]\n    assert next_batch.next_token_choosers[1:] == next_batch_1.next_token_choosers\n\n    assert next_batch.stopping_criterias[0] == next_batch_0.stopping_criterias[0]\n    assert next_batch.stopping_criterias[1:] == next_batch_1.stopping_criterias\n\n    assert next_batch.past_key_values is not None\n    assert all([p[0].shape == (3, 16, 64, 2) for p in next_batch.past_key_values])\n    assert all([p[1].shape == (3, 16, 2, 64) for p in next_batch.past_key_values])\n\n    for i, past in enumerate(next_batch.past_key_values):\n        assert torch.equal(next_batch_0_past_key_values[i][0][:, :, -2:], past[0][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][0][:, :, -1:],\n            past[0][1:, :, :, -1].reshape(-1, 64, 1),\n        )\n\n        assert torch.equal(next_batch_0_past_key_values[i][1][:, -2:, :], past[1][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][1][:, -1:, :],\n            past[1][1:, :, -1, :].reshape(-1, 1, 64),\n        )\n\n    for _ in range(\n        default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens - 2\n    ):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 3\n    assert generations[2].generated_text.text == \"TestTestTestTestTest\"\n    assert (\n        generations[2].request_id == default_multi_requests_bloom_batch.requests[1].id\n    )\n    assert (\n        generations[2].generated_text.generated_tokens\n        == default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens\n    )\n\n    next_batch = next_batch.filter([next_batch.requests[0], next_batch.requests[1]])\n\n    for _ in range(\n        default_bloom_batch.stopping_criterias[0].max_new_tokens\n        - default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens\n        - 2\n    ):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert (\n        generations[0].generated_text.text == \"TestTestTestTestTestTestTestTestTestTest\"\n    )\n    assert generations[0].request_id == default_bloom_batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_bloom_batch.stopping_criterias[0].max_new_tokens\n    )\n\n    next_batch = next_batch.filter([next_batch.requests[1]])\n\n    for _ in range(\n        default_multi_requests_bloom_batch.stopping_criterias[0].max_new_tokens\n        - default_bloom_batch.stopping_criterias[0].max_new_tokens\n        - default_multi_requests_bloom_batch.stopping_criterias[1].max_new_tokens\n        - 4\n    ):\n        generations, next_batch = default_bloom.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_bloom.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert (\n        generations[0].generated_text.text == \"TestTestTestTestTestTestTestTestTestTest\"\n    )\n    assert (\n        generations[0].request_id == default_multi_requests_bloom_batch.requests[0].id\n    )\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_multi_requests_bloom_batch.stopping_criterias[0].max_new_tokens\n    )", ""]}
{"filename": "server/tests/models/test_seq2seq_lm.py", "chunked_list": ["import pytest\nimport torch\n\nfrom copy import copy\n\nfrom transformers import AutoTokenizer\n\nfrom embedding_server.pb import generate_pb2\nfrom embedding_server.models.seq2seq_lm import Seq2SeqLM, Seq2SeqLMBatch\n", "from embedding_server.models.seq2seq_lm import Seq2SeqLM, Seq2SeqLMBatch\n\n\n@pytest.fixture(scope=\"session\")\ndef mt0_small_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"bigscience/mt0-small\", padding_side=\"left\"\n    )\n    tokenizer.bos_token_id = 0\n    return tokenizer", "\n\n@pytest.fixture(scope=\"session\")\ndef default_seq2seq_lm():\n    return Seq2SeqLM(\"bigscience/mt0-small\")\n\n\n@pytest.fixture\ndef default_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"Test\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )", "def default_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"Test\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )\n\n", "\n\n@pytest.fixture\ndef default_pb_batch(default_pb_request):\n    return generate_pb2.Batch(id=0, requests=[default_pb_request], size=1)\n\n\n@pytest.fixture\ndef default_seq2seq_lm_batch(default_pb_batch, mt0_small_tokenizer):\n    return Seq2SeqLMBatch.from_pb(\n        default_pb_batch, mt0_small_tokenizer, torch.device(\"cpu\")\n    )", "def default_seq2seq_lm_batch(default_pb_batch, mt0_small_tokenizer):\n    return Seq2SeqLMBatch.from_pb(\n        default_pb_batch, mt0_small_tokenizer, torch.device(\"cpu\")\n    )\n\n\n@pytest.fixture\ndef default_multi_requests_seq2seq_lm_batch(default_pb_request, mt0_small_tokenizer):\n    req_0 = copy(default_pb_request)\n    req_0.id = 1\n    req_1 = default_pb_request\n    req_1.id = 2\n    req_1.stopping_parameters.max_new_tokens = 5\n\n    batch_pb = generate_pb2.Batch(id=0, requests=[req_0, req_1], size=2)\n    return Seq2SeqLMBatch.from_pb(batch_pb, mt0_small_tokenizer, torch.device(\"cpu\"))", "\n\ndef test_batch_from_pb(default_pb_batch, default_seq2seq_lm_batch):\n    batch = default_seq2seq_lm_batch\n    sequence_length = len(default_seq2seq_lm_batch.input_ids[0])\n\n    assert batch.batch_id == default_pb_batch.id\n    assert batch.requests == default_pb_batch.requests\n\n    assert batch.input_ids.shape == (default_pb_batch.size, sequence_length)\n    assert batch.input_ids[0][-2] == 4268\n    assert batch.input_ids[0][-1] == 1\n    assert torch.all(batch.input_ids[0][:-2] == 0)\n\n    assert torch.all(batch.attention_mask[0][-2:] == 1)\n    assert torch.all(batch.attention_mask[0][:-2] == 0)\n\n    assert len(batch.decoder_input_ids) == default_pb_batch.size\n    assert batch.decoder_attention_mask is None\n    assert batch.encoder_last_hidden_state is None\n\n    assert batch.past_key_values is None\n\n    assert batch.input_lengths == [2]\n    assert batch.decoder_input_lengths == [1]\n\n    assert len(batch) == default_pb_batch.size\n    assert len(batch.next_token_choosers) == len(batch.stopping_criterias) == len(batch)\n\n    assert batch.max_input_length == batch.input_lengths[0]\n    assert batch.max_decoder_input_length == batch.decoder_input_lengths[0]", "\n\ndef test_batch_concatenate_no_prefill(default_seq2seq_lm_batch):\n    with pytest.raises(ValueError):\n        Seq2SeqLMBatch.concatenate([default_seq2seq_lm_batch, default_seq2seq_lm_batch])\n\n\ndef test_seq2seq_lm_batch_type(default_seq2seq_lm):\n    assert default_seq2seq_lm.batch_type == Seq2SeqLMBatch\n", "\n\ndef test_seq2seq_lm_generate_token(default_seq2seq_lm, default_seq2seq_lm_batch):\n    sequence_length = len(default_seq2seq_lm_batch.input_ids[0])\n    generations, next_batch = default_seq2seq_lm.generate_token(\n        default_seq2seq_lm_batch\n    )\n\n    assert len(generations) == len(next_batch)\n    assert isinstance(next_batch, Seq2SeqLMBatch)\n\n    assert next_batch.input_ids is None\n    assert torch.equal(\n        next_batch.attention_mask, default_seq2seq_lm_batch.attention_mask\n    )\n    assert next_batch.input_lengths == default_seq2seq_lm_batch.input_lengths\n    assert next_batch.max_input_length == default_seq2seq_lm_batch.max_input_length\n    assert (\n        next_batch.next_token_choosers == default_seq2seq_lm_batch.next_token_choosers\n    )\n    assert next_batch.stopping_criterias == default_seq2seq_lm_batch.stopping_criterias\n\n    assert len(next_batch.decoder_input_ids) == len(next_batch)\n    assert next_batch.all_decoder_input_ids[0][0] == 0\n    assert next_batch.all_decoder_input_ids[0][1] == 259\n    assert next_batch.decoder_attention_mask is None\n    assert next_batch.encoder_last_hidden_state.shape == (1, sequence_length, 512)\n\n    assert next_batch.decoder_input_lengths == [2]\n    assert next_batch.max_decoder_input_length == 2\n\n    assert next_batch.past_key_values is not None\n    assert all(\n        [p[0].shape == (len(next_batch), 6, 1, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[1].shape == (len(next_batch), 6, 1, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [\n            p[2].shape == (len(next_batch), 6, sequence_length, 64)\n            for p in next_batch.past_key_values\n        ]\n    )\n    assert all(\n        [\n            p[3].shape == (len(next_batch), 6, sequence_length, 64)\n            for p in next_batch.past_key_values\n        ]\n    )\n    assert all([generation.generated_text is None for generation in generations])\n    assert all([len(generation.prefill_tokens) == 1 for generation in generations])\n    assert all([generation.token_id.item() == 259 for generation in generations])\n    assert all([generation.token_text == \" \" for generation in generations])\n    assert generations[0].request_id == 0", "\n\ndef test_seq2seq_lm_generate_token_completion(\n    default_seq2seq_lm, default_seq2seq_lm_batch\n):\n    next_batch = default_seq2seq_lm_batch\n    for _ in range(6):\n        generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \"a few weeks\"\n    assert generations[0].request_id == default_seq2seq_lm_batch.requests[0].id\n    assert generations[0].generated_text.generated_tokens == 7", "\n\ndef test_seq2seq_lm_generate_token_completion_multi(\n    default_seq2seq_lm, default_multi_requests_seq2seq_lm_batch\n):\n    next_batch = default_multi_requests_seq2seq_lm_batch\n\n    for i in range(4):\n        generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert generations[1].generated_text.text == \"a few \"\n    assert (\n        generations[1].request_id\n        == default_multi_requests_seq2seq_lm_batch.requests[1].id\n    )\n    assert generations[1].generated_text.generated_tokens == 5\n\n    next_batch = next_batch.filter([next_batch.requests[0]])\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \"a few weeks\"\n    assert (\n        generations[0].request_id\n        == default_multi_requests_seq2seq_lm_batch.requests[0].id\n    )\n    assert generations[0].generated_text.generated_tokens == 7", "\n\ndef test_batch_concatenate(\n    default_seq2seq_lm,\n    default_seq2seq_lm_batch,\n    default_multi_requests_seq2seq_lm_batch,\n):\n    next_batch_0 = default_seq2seq_lm_batch\n    _, next_batch_0 = default_seq2seq_lm.generate_token(next_batch_0)\n    _, next_batch_0 = default_seq2seq_lm.generate_token(next_batch_0)\n\n    next_batch_1 = default_multi_requests_seq2seq_lm_batch\n    _, next_batch_1 = default_seq2seq_lm.generate_token(next_batch_1)\n\n    # Copy hidden state because it is removed from the concatenated branches\n    next_batch_0_encoder_last_hidden_state = next_batch_0.encoder_last_hidden_state\n    next_batch_1_encoder_last_hidden_state = next_batch_1.encoder_last_hidden_state\n\n    # Clone past_key_values before concatenating to compare after,\n    # because they are removed from the concatenated batches\n    next_batch_0_past_key_values = [\n        [t.clone() for t in layer] for layer in next_batch_0.past_key_values\n    ]\n    next_batch_1_past_key_values = [\n        [t.clone() for t in layer] for layer in next_batch_1.past_key_values\n    ]\n\n    next_batch = Seq2SeqLMBatch.concatenate([next_batch_0, next_batch_1])\n\n    assert next_batch.batch_id == 0\n\n    assert torch.equal(\n        next_batch.decoder_input_ids[0], next_batch_0.decoder_input_ids[0]\n    )\n    assert next_batch.all_decoder_input_ids[1][0] == 0\n    assert next_batch.all_decoder_input_ids[2][0] == 0\n    assert torch.equal(\n        next_batch.decoder_input_ids[1:, -2:], next_batch_1.decoder_input_ids\n    )\n\n    assert torch.all(next_batch.decoder_attention_mask[0, :3] == 1)\n    assert torch.all(next_batch.decoder_attention_mask[0, 3:] == 0)\n    assert torch.all(next_batch.decoder_attention_mask[1:, 0] == 0)\n    assert torch.all(next_batch.decoder_attention_mask[1:, 1:3] == 1)\n\n    assert torch.equal(\n        next_batch.encoder_last_hidden_state[0],\n        next_batch_0_encoder_last_hidden_state[0, -2:],\n    )\n    assert torch.equal(\n        next_batch.encoder_last_hidden_state[1:],\n        next_batch_1_encoder_last_hidden_state[:, -2:],\n    )\n\n    assert next_batch.input_lengths == [2, 2, 2]\n    assert next_batch.decoder_input_lengths == [3, 2, 2]\n    assert next_batch.max_input_length == 2\n    assert next_batch.max_decoder_input_length == 3\n\n    assert next_batch.requests[0] == next_batch_0.requests[0]\n    assert next_batch.requests[1:] == next_batch_1.requests\n\n    assert next_batch.next_token_choosers[0] == next_batch_0.next_token_choosers[0]\n    assert next_batch.next_token_choosers[1:] == next_batch_1.next_token_choosers\n\n    assert next_batch.stopping_criterias[0] == next_batch_0.stopping_criterias[0]\n    assert next_batch.stopping_criterias[1:] == next_batch_1.stopping_criterias\n\n    assert next_batch.past_key_values is not None\n    assert all(\n        [p[0].shape == (len(next_batch), 6, 2, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[1].shape == (len(next_batch), 6, 2, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[2].shape == (len(next_batch), 6, 2, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[3].shape == (len(next_batch), 6, 2, 64) for p in next_batch.past_key_values]\n    )\n\n    for i, past in enumerate(next_batch.past_key_values):\n        assert torch.equal(next_batch_0_past_key_values[i][0][0, :, -2:, :], past[0][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][0][:, :, -1:, :], past[0][1:, :, -1:, :]\n        )\n\n        assert torch.equal(next_batch_0_past_key_values[i][1][0, :, -2:, :], past[1][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][1][:, :, -1:, :], past[1][1:, :, -1:, :]\n        )\n\n        assert torch.equal(next_batch_0_past_key_values[i][2][0, :, -2:, :], past[2][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][2][:, :, -2:, :], past[2][1:]\n        )\n\n        assert torch.equal(next_batch_0_past_key_values[i][3][0, :, -2:, :], past[3][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][3][:, :, -2:, :], past[3][1:]\n        )\n\n    for _ in range(3):\n        generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 3\n    assert generations[2].generated_text.text == \"a few \"\n    assert (\n        generations[2].request_id\n        == default_multi_requests_seq2seq_lm_batch.requests[1].id\n    )\n    assert generations[2].generated_text.generated_tokens == 5\n\n    next_batch = next_batch.filter([next_batch.requests[0], next_batch.requests[1]])\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert generations[0].generated_text.text == \"a few weeks\"\n    assert generations[0].request_id == default_seq2seq_lm_batch.requests[0].id\n    assert generations[0].generated_text.generated_tokens == 7\n\n    next_batch = next_batch.filter([next_batch.requests[1]])\n\n    generations, next_batch = default_seq2seq_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \"a few weeks\"\n    assert (\n        generations[0].request_id\n        == default_multi_requests_seq2seq_lm_batch.requests[0].id\n    )\n    assert generations[0].generated_text.generated_tokens == 7", ""]}
{"filename": "server/tests/models/test_santacoder.py", "chunked_list": ["import pytest\n\nfrom embedding_server.pb import generate_pb2\nfrom embedding_server.models.causal_lm import EncoderBatch\nfrom embedding_server.models.santacoder import SantaCoder\n\n\n@pytest.fixture(scope=\"session\")\ndef default_santacoder():\n    return SantaCoder(\"bigcode/santacoder\")", "def default_santacoder():\n    return SantaCoder(\"bigcode/santacoder\")\n\n\n@pytest.fixture\ndef default_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"def\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )", "\n\n@pytest.fixture\ndef default_pb_batch(default_pb_request):\n    return generate_pb2.Batch(id=0, requests=[default_pb_request], size=1)\n\n\n@pytest.fixture\ndef default_fim_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"<fim-prefix>def<fim-suffix>world<fim-middle>\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )", "def default_fim_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"<fim-prefix>def<fim-suffix>world<fim-middle>\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )\n\n", "\n\n@pytest.fixture\ndef default_fim_pb_batch(default_fim_pb_request):\n    return generate_pb2.Batch(id=0, requests=[default_fim_pb_request], size=1)\n\n\n@pytest.mark.skip\ndef test_santacoder_generate_token_completion(default_santacoder, default_pb_batch):\n    batch = EncoderBatch.from_pb(\n        default_pb_batch, default_santacoder.tokenizer, default_santacoder.device\n    )\n    next_batch = batch\n\n    for _ in range(batch.stopping_criterias[0].max_new_tokens - 1):\n        generations, next_batch = default_santacoder.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_santacoder.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \" test_get_all_users_with_\"\n    assert generations[0].request_id == batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == batch.stopping_criterias[0].max_new_tokens\n    )", "def test_santacoder_generate_token_completion(default_santacoder, default_pb_batch):\n    batch = EncoderBatch.from_pb(\n        default_pb_batch, default_santacoder.tokenizer, default_santacoder.device\n    )\n    next_batch = batch\n\n    for _ in range(batch.stopping_criterias[0].max_new_tokens - 1):\n        generations, next_batch = default_santacoder.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_santacoder.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \" test_get_all_users_with_\"\n    assert generations[0].request_id == batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == batch.stopping_criterias[0].max_new_tokens\n    )", "\n\n@pytest.mark.skip\ndef test_fim_santacoder_generate_token_completion(\n    default_santacoder, default_fim_pb_batch\n):\n    batch = EncoderBatch.from_pb(\n        default_fim_pb_batch, default_santacoder.tokenizer, default_santacoder.device\n    )\n    next_batch = batch\n\n    for _ in range(batch.stopping_criterias[0].max_new_tokens - 1):\n        generations, next_batch = default_santacoder.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_santacoder.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert (\n        generations[0].generated_text.text\n        == \"\"\"ineProperty(exports, \"__esModule\", { value\"\"\"\n    )\n    assert generations[0].request_id == batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == batch.stopping_criterias[0].max_new_tokens\n    )", ""]}
{"filename": "server/tests/models/test_model.py", "chunked_list": ["import pytest\nimport torch\n\nfrom transformers import AutoTokenizer\n\nfrom embedding_server.models import Model\n\n\ndef get_test_model():\n    class TestModel(Model):\n        def batch_type(self):\n            raise NotImplementedError\n\n        def generate_token(self, batch):\n            raise NotImplementedError\n\n    tokenizer = AutoTokenizer.from_pretrained(\"huggingface/llama-7b\")\n\n    model = TestModel(\n        torch.nn.Linear(1, 1), tokenizer, False, torch.float32, torch.device(\"cpu\")\n    )\n    return model", "def get_test_model():\n    class TestModel(Model):\n        def batch_type(self):\n            raise NotImplementedError\n\n        def generate_token(self, batch):\n            raise NotImplementedError\n\n    tokenizer = AutoTokenizer.from_pretrained(\"huggingface/llama-7b\")\n\n    model = TestModel(\n        torch.nn.Linear(1, 1), tokenizer, False, torch.float32, torch.device(\"cpu\")\n    )\n    return model", "\n\n@pytest.mark.private\ndef test_decode_streaming_english_spaces():\n    model = get_test_model()\n    truth = \"Hello here, this is a simple test\"\n    all_input_ids = [15043, 1244, 29892, 445, 338, 263, 2560, 1243]\n    assert (\n        all_input_ids == model.tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n    )\n\n    decoded_text = \"\"\n    offset = 0\n    token_offset = 0\n    for i in range(len(all_input_ids)):\n        text, offset, token_offset = model.decode_token(\n            all_input_ids[: i + 1], offset, token_offset\n        )\n        decoded_text += text\n\n    assert decoded_text == truth", "\n\n@pytest.mark.private\ndef test_decode_streaming_chinese_utf8():\n    model = get_test_model()\n    truth = \"\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5\"\n    all_input_ids = [\n        30672,\n        232,\n        193,\n        139,\n        233,\n        135,\n        162,\n        235,\n        179,\n        165,\n        30919,\n        30210,\n        234,\n        134,\n        176,\n        30993,\n    ]\n\n    decoded_text = \"\"\n    offset = 0\n    token_offset = 0\n    for i in range(len(all_input_ids)):\n        text, offset, token_offset = model.decode_token(\n            all_input_ids[: i + 1], offset, token_offset\n        )\n        decoded_text += text\n\n    assert decoded_text == truth", ""]}
{"filename": "server/tests/models/test_causal_lm.py", "chunked_list": ["import pytest\nimport torch\n\nfrom copy import copy\nfrom transformers import AutoTokenizer\n\nfrom embedding_server.pb import generate_pb2\nfrom embedding_server.models.causal_lm import CausalLM, EncoderBatch\n\n", "\n\n@pytest.fixture(scope=\"session\")\ndef default_causal_lm():\n    return CausalLM(\"gpt2\")\n\n\n@pytest.fixture(scope=\"session\")\ndef gpt2_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n    tokenizer.pad_token_id = 50256\n    return tokenizer", "def gpt2_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n    tokenizer.pad_token_id = 50256\n    return tokenizer\n\n\n@pytest.fixture\ndef default_pb_request(default_pb_parameters, default_pb_stop_parameters):\n    return generate_pb2.Request(\n        id=0,\n        inputs=\"Test\",\n        truncate=100,\n        parameters=default_pb_parameters,\n        stopping_parameters=default_pb_stop_parameters,\n    )", "\n\n@pytest.fixture\ndef default_pb_batch(default_pb_request):\n    return generate_pb2.Batch(id=0, requests=[default_pb_request], size=1)\n\n\n@pytest.fixture\ndef default_causal_lm_batch(default_pb_batch, gpt2_tokenizer):\n    return EncoderBatch.from_pb(default_pb_batch, gpt2_tokenizer, torch.device(\"cpu\"))", "def default_causal_lm_batch(default_pb_batch, gpt2_tokenizer):\n    return EncoderBatch.from_pb(default_pb_batch, gpt2_tokenizer, torch.device(\"cpu\"))\n\n\n@pytest.fixture\ndef default_multi_requests_causal_lm_batch(default_pb_request, gpt2_tokenizer):\n    req_0 = copy(default_pb_request)\n    req_0.id = 1\n    req_1 = default_pb_request\n    req_1.id = 2\n    req_1.stopping_parameters.max_new_tokens = 5\n\n    batch_pb = generate_pb2.Batch(id=1, requests=[req_0, req_1], size=2)\n    return EncoderBatch.from_pb(batch_pb, gpt2_tokenizer, torch.device(\"cpu\"))", "\n\ndef test_batch_from_pb(default_pb_batch, default_causal_lm_batch):\n    batch = default_causal_lm_batch\n\n    assert batch.batch_id == default_pb_batch.id\n    assert batch.requests == default_pb_batch.requests\n\n    assert len(batch.input_ids) == default_pb_batch.size\n    assert batch.input_ids[0][-1] == 14402\n    assert torch.all(batch.input_ids[0][:-1] == 50256)\n\n    assert batch.attention_mask[0, 0] == 1\n    assert torch.all(batch.attention_mask[0, 1:] == 0)\n\n    assert batch.past_key_values is None\n\n    assert all(\n        [\n            torch.equal(input_ids, all_input_ids[:, 0])\n            for input_ids, all_input_ids in zip(batch.input_ids, batch.all_input_ids)\n        ]\n    )\n\n    assert batch.input_lengths == [1]\n\n    assert len(batch) == default_pb_batch.size\n    assert len(batch.next_token_choosers) == len(batch.stopping_criterias) == len(batch)\n\n    assert batch.max_input_length == batch.input_lengths[0]", "\n\ndef test_batch_concatenate_no_prefill(default_causal_lm_batch):\n    with pytest.raises(ValueError):\n        EncoderBatch.concatenate([default_causal_lm_batch, default_causal_lm_batch])\n\n\ndef test_causal_lm_batch_type(default_causal_lm):\n    assert default_causal_lm.batch_type == EncoderBatch\n", "\n\ndef test_causal_lm_generate_token(default_causal_lm, default_causal_lm_batch):\n    sequence_length = len(default_causal_lm_batch.all_input_ids[0])\n    generations, next_batch = default_causal_lm.generate_token(default_causal_lm_batch)\n\n    assert len(generations) == len(next_batch)\n    assert isinstance(next_batch, EncoderBatch)\n\n    assert len(next_batch.all_input_ids) == len(next_batch)\n    assert len(next_batch.all_input_ids[0]) == sequence_length + 1\n    assert len(next_batch.attention_mask[0]) == 11\n    assert next_batch.all_input_ids[0][-1] == 13\n    assert next_batch.all_input_ids[0][-2] == 14402\n    assert torch.all(next_batch.all_input_ids[0][:-2] == 50256)\n\n    assert torch.all(next_batch.attention_mask[0][0:2] == 1)\n    assert torch.all(next_batch.attention_mask[0][2:] == 0)\n\n    assert next_batch.input_ids.shape == (len(next_batch), 1)\n    assert next_batch.input_ids[0, 0] == 13\n\n    assert next_batch.input_lengths == [2]\n    assert next_batch.max_input_length == next_batch.input_lengths[0]\n\n    assert next_batch.past_key_values is not None\n    assert all(\n        [p[0].shape == (1, 12, sequence_length, 64) for p in next_batch.past_key_values]\n    )\n    assert all(\n        [p[1].shape == (1, 12, sequence_length, 64) for p in next_batch.past_key_values]\n    )\n    assert all([generation.generated_text is None for generation in generations])\n    assert all([len(generation.prefill_tokens) == 1 for generation in generations])\n    assert all([generation.token_id.item() == 13 for generation in generations])\n    assert all([generation.token_text == \".\" for generation in generations])\n    assert generations[0].request_id == 0", "\n\ndef test_causal_lm_generate_token_completion(\n    default_causal_lm, default_causal_lm_batch\n):\n    next_batch = default_causal_lm_batch\n    for _ in range(default_causal_lm_batch.stopping_criterias[0].max_new_tokens - 1):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \".java:784) at net.minecraft.\"\n    assert generations[0].request_id == default_causal_lm_batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_causal_lm_batch.stopping_criterias[0].max_new_tokens\n    )", "\n\ndef test_causal_lm_generate_token_completion_multi(\n    default_causal_lm, default_multi_requests_causal_lm_batch\n):\n    next_batch = default_multi_requests_causal_lm_batch\n\n    for i in range(\n        default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens - 1\n    ):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert generations[1].generated_text.text == \".java:784)\"\n    assert (\n        generations[1].request_id\n        == default_multi_requests_causal_lm_batch.requests[1].id\n    )\n    assert (\n        generations[1].generated_text.generated_tokens\n        == default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens\n    )\n    # Copy stopping_criterias before filtering\n    stopping_criterias = (\n        default_multi_requests_causal_lm_batch.stopping_criterias.copy()\n    )\n\n    next_batch = next_batch.filter([next_batch.requests[0]])\n\n    for _ in range(\n        stopping_criterias[0].max_new_tokens - stopping_criterias[1].max_new_tokens - 1\n    ):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \".java:784) at net.minecraft.\"\n    assert (\n        generations[0].request_id\n        == default_multi_requests_causal_lm_batch.requests[0].id\n    )\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_multi_requests_causal_lm_batch.stopping_criterias[0].max_new_tokens\n    )", "\n\ndef test_batch_concatenate(\n    default_causal_lm, default_causal_lm_batch, default_multi_requests_causal_lm_batch\n):\n    next_batch_0 = default_causal_lm_batch\n    _, next_batch_0 = default_causal_lm.generate_token(next_batch_0)\n    _, next_batch_0 = default_causal_lm.generate_token(next_batch_0)\n\n    next_batch_1 = default_multi_requests_causal_lm_batch\n    _, next_batch_1 = default_causal_lm.generate_token(next_batch_1)\n\n    # Clone past_key_values before concatenating to compare after,\n    # because they are removed from the concatenated batches\n    next_batch_0_past_key_values = [\n        (k.clone(), v.clone()) for (k, v) in next_batch_0.past_key_values\n    ]\n    next_batch_1_past_key_values = [\n        (k.clone(), v.clone()) for (k, v) in next_batch_1.past_key_values\n    ]\n\n    next_batch = EncoderBatch.concatenate([next_batch_0, next_batch_1])\n\n    assert torch.equal(next_batch.all_input_ids[0], next_batch_0.all_input_ids[0])\n    assert torch.equal(next_batch.all_input_ids[1], next_batch_1.all_input_ids[0])\n    assert torch.equal(next_batch.all_input_ids[2], next_batch_1.all_input_ids[1])\n\n    assert torch.all(\n        next_batch.attention_mask[0, : -next_batch.padding_right_offset] == 1\n    )\n    assert torch.all(\n        next_batch.attention_mask[1:, 1 : -next_batch.padding_right_offset] == 1\n    )\n    assert torch.all(next_batch.attention_mask[1:, 3:] == 0)\n\n    assert next_batch.batch_id == 0\n    assert next_batch.input_ids[0, 0] == 12355\n    assert torch.all(next_batch.input_ids[1:] == 13)\n\n    assert next_batch.input_lengths == [3, 2, 2]\n    assert next_batch.max_input_length == 3\n\n    assert next_batch.requests[0] == next_batch_0.requests[0]\n    assert next_batch.requests[1:] == next_batch_1.requests\n\n    assert next_batch.next_token_choosers[0] == next_batch_0.next_token_choosers[0]\n    assert next_batch.next_token_choosers[1:] == next_batch_1.next_token_choosers\n\n    assert next_batch.stopping_criterias[0] == next_batch_0.stopping_criterias[0]\n    assert next_batch.stopping_criterias[1:] == next_batch_1.stopping_criterias\n\n    assert next_batch.past_key_values is not None\n    assert all([p[0].shape == (3, 12, 2, 64) for p in next_batch.past_key_values])\n    assert all([p[1].shape == (3, 12, 2, 64) for p in next_batch.past_key_values])\n\n    for i, past in enumerate(next_batch.past_key_values):\n        assert torch.equal(next_batch_0_past_key_values[i][0][0, :, -2:], past[0][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][0][:, :, -1:], past[0][1:, :, -1:, :]\n        )\n\n        assert torch.equal(next_batch_0_past_key_values[i][1][0, :, -2:], past[1][0])\n        assert torch.equal(\n            next_batch_1_past_key_values[i][1][:, :, -1:], past[1][1:, :, -1:, :]\n        )\n\n    for _ in range(\n        default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens - 2\n    ):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 3\n    assert generations[2].generated_text.text == \".java:784)\"\n    assert (\n        generations[2].request_id\n        == default_multi_requests_causal_lm_batch.requests[1].id\n    )\n    assert (\n        generations[2].generated_text.generated_tokens\n        == default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens\n    )\n\n    next_batch = next_batch.filter([next_batch.requests[0], next_batch.requests[1]])\n\n    for _ in range(\n        default_causal_lm_batch.stopping_criterias[0].max_new_tokens\n        - default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens\n        - 2\n    ):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is not None\n\n    assert len(generations) == 2\n    assert generations[0].generated_text.text == \".java:784) at net.minecraft.\"\n    assert generations[0].request_id == default_causal_lm_batch.requests[0].id\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_causal_lm_batch.stopping_criterias[0].max_new_tokens\n    )\n\n    next_batch = next_batch.filter([next_batch.requests[1]])\n\n    for _ in range(\n        default_multi_requests_causal_lm_batch.stopping_criterias[0].max_new_tokens\n        - default_causal_lm_batch.stopping_criterias[0].max_new_tokens\n        - default_multi_requests_causal_lm_batch.stopping_criterias[1].max_new_tokens\n        - 4\n    ):\n        generations, next_batch = default_causal_lm.generate_token(next_batch)\n        assert len(generations) == len(next_batch)\n\n    generations, next_batch = default_causal_lm.generate_token(next_batch)\n    assert next_batch is None\n\n    assert len(generations) == 1\n    assert generations[0].generated_text.text == \".java:784) at net.minecraft.\"\n    assert (\n        generations[0].request_id\n        == default_multi_requests_causal_lm_batch.requests[0].id\n    )\n    assert (\n        generations[0].generated_text.generated_tokens\n        == default_multi_requests_causal_lm_batch.stopping_criterias[0].max_new_tokens\n    )", ""]}
{"filename": "server/embedding_server/tracing.py", "chunked_list": ["import grpc\n\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.grpc._aio_server import (\n    OpenTelemetryAioServerInterceptor,\n)\nfrom opentelemetry.semconv.trace import SpanAttributes\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider", "from opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n)\n\n\nclass UDSOpenTelemetryAioServerInterceptor(OpenTelemetryAioServerInterceptor):\n    def __init__(self):\n        super().__init__(trace.get_tracer(__name__))\n\n    def _start_span(self, handler_call_details, context, set_status_on_exception=False):\n        \"\"\"\n        Rewrite _start_span method to support Unix Domain Socket gRPC contexts\n        \"\"\"\n\n        # standard attributes\n        attributes = {\n            SpanAttributes.RPC_SYSTEM: \"grpc\",\n            SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],\n        }\n\n        # if we have details about the call, split into service and method\n        if handler_call_details.method:\n            service, method = handler_call_details.method.lstrip(\"/\").split(\"/\", 1)\n            attributes.update(\n                {\n                    SpanAttributes.RPC_METHOD: method,\n                    SpanAttributes.RPC_SERVICE: service,\n                }\n            )\n\n        # add some attributes from the metadata\n        metadata = dict(context.invocation_metadata())\n        if \"user-agent\" in metadata:\n            attributes[\"rpc.user_agent\"] = metadata[\"user-agent\"]\n\n        # We use gRPC over a UNIX socket\n        attributes.update({SpanAttributes.NET_TRANSPORT: \"unix\"})\n\n        return self._tracer.start_as_current_span(\n            name=handler_call_details.method,\n            kind=trace.SpanKind.SERVER,\n            attributes=attributes,\n            set_status_on_exception=set_status_on_exception,\n        )", "\n\ndef setup_tracing(shard: int, otlp_endpoint: str):\n    resource = Resource.create(\n        attributes={\"service.name\": f\"text-generation-inference.server-{shard}\"}\n    )\n    span_exporter = OTLPSpanExporter(endpoint=otlp_endpoint, insecure=True)\n    span_processor = BatchSpanProcessor(span_exporter)\n\n    trace.set_tracer_provider(TracerProvider(resource=resource))\n    trace.get_tracer_provider().add_span_processor(span_processor)", ""]}
{"filename": "server/embedding_server/interceptor.py", "chunked_list": ["import grpc\n\nfrom google.rpc import status_pb2, code_pb2\nfrom grpc_status import rpc_status\nfrom grpc_interceptor.server import AsyncServerInterceptor\nfrom loguru import logger\nfrom typing import Callable, Any\n\n\nclass ExceptionInterceptor(AsyncServerInterceptor):\n    async def intercept(\n        self,\n        method: Callable,\n        request_or_iterator: Any,\n        context: grpc.ServicerContext,\n        method_name: str,\n    ) -> Any:\n        try:\n            response = method(request_or_iterator, context)\n            return await response\n        except Exception as err:\n            method_name = method_name.split(\"/\")[-1]\n            logger.exception(f\"Method {method_name} encountered an error.\")\n\n            await context.abort_with_status(\n                rpc_status.to_status(\n                    status_pb2.Status(code=code_pb2.INTERNAL, message=str(err))\n                )\n            )", "\nclass ExceptionInterceptor(AsyncServerInterceptor):\n    async def intercept(\n        self,\n        method: Callable,\n        request_or_iterator: Any,\n        context: grpc.ServicerContext,\n        method_name: str,\n    ) -> Any:\n        try:\n            response = method(request_or_iterator, context)\n            return await response\n        except Exception as err:\n            method_name = method_name.split(\"/\")[-1]\n            logger.exception(f\"Method {method_name} encountered an error.\")\n\n            await context.abort_with_status(\n                rpc_status.to_status(\n                    status_pb2.Status(code=code_pb2.INTERNAL, message=str(err))\n                )\n            )", ""]}
{"filename": "server/embedding_server/cache.py", "chunked_list": ["from typing import Dict, Optional, TypeVar\n\nfrom embedding_server.models.types import Batch\n\nB = TypeVar(\"B\", bound=Batch)\n\n\nclass Cache:\n    def __init__(self):\n        self.cache: Dict[int, B] = {}\n\n    def pop(self, batch_id: int) -> Optional[B]:\n        return self.cache.pop(batch_id, None)\n\n    def set(self, entry: B):\n        if entry is not None:\n            self.cache[entry.batch_id] = entry\n\n    def delete(self, batch_id: int):\n        batch = self.pop(batch_id)\n        if batch is not None:\n            del batch\n\n    def clear(self):\n        self.cache.clear()\n\n    def __len__(self):\n        return len(self.cache.keys())", ""]}
{"filename": "server/embedding_server/__init__.py", "chunked_list": [""]}
{"filename": "server/embedding_server/server.py", "chunked_list": ["import asyncio\nimport os\nimport torch\n\nfrom grpc import aio\nfrom loguru import logger\n\nfrom grpc_reflection.v1alpha import reflection\nfrom pathlib import Path\nfrom typing import List, Optional", "from pathlib import Path\nfrom typing import List, Optional\n\nfrom embedding_server.cache import Cache\nfrom embedding_server.interceptor import ExceptionInterceptor\nfrom embedding_server.models import Model, get_model\nfrom embedding_server.pb import embedding_pb2_grpc, embedding_pb2\nfrom embedding_server.tracing import UDSOpenTelemetryAioServerInterceptor\n\n\nclass EmbeddingService(embedding_pb2_grpc.EmbeddingServiceServicer):\n    def __init__(self, model: Model, cache: Cache, server_urls: List[str]):\n        self.cache = cache\n        self.model = model\n        self.server_urls = server_urls\n        # For some reason, inference_mode does not work well with GLOO which we use on CPU\n        if model.device.type == \"cuda\":\n            # Force inference mode for the lifetime of EmbeddingService\n            self._inference_mode_raii_guard = torch._C._InferenceMode(True)\n\n    async def Info(self, request, context):\n        return self.model.info\n\n    async def Health(self, request, context):\n        if self.model.device.type == \"cuda\":\n            torch.zeros((2, 2)).cuda()\n        return embedding_pb2.HealthResponse()\n\n    async def ServiceDiscovery(self, request, context):\n        return embedding_pb2.ServiceDiscoveryResponse(urls=self.server_urls)\n\n    async def ClearCache(self, request, context):\n        if request.HasField(\"id\"):\n            self.cache.delete(request.id)\n        else:\n            self.cache.clear()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return embedding_pb2.ClearCacheResponse()\n\n    async def Embed(self, request, context):\n        batch = self.model.batch_type.from_pb(\n            request.batch, self.model.tokenizer, self.model.device\n        )\n\n        executions = self.model.embed(batch)\n\n        return embedding_pb2.EmbedResponse(\n            embeddings=[e.to_pb() for e in executions],\n        )", "\n\nclass EmbeddingService(embedding_pb2_grpc.EmbeddingServiceServicer):\n    def __init__(self, model: Model, cache: Cache, server_urls: List[str]):\n        self.cache = cache\n        self.model = model\n        self.server_urls = server_urls\n        # For some reason, inference_mode does not work well with GLOO which we use on CPU\n        if model.device.type == \"cuda\":\n            # Force inference mode for the lifetime of EmbeddingService\n            self._inference_mode_raii_guard = torch._C._InferenceMode(True)\n\n    async def Info(self, request, context):\n        return self.model.info\n\n    async def Health(self, request, context):\n        if self.model.device.type == \"cuda\":\n            torch.zeros((2, 2)).cuda()\n        return embedding_pb2.HealthResponse()\n\n    async def ServiceDiscovery(self, request, context):\n        return embedding_pb2.ServiceDiscoveryResponse(urls=self.server_urls)\n\n    async def ClearCache(self, request, context):\n        if request.HasField(\"id\"):\n            self.cache.delete(request.id)\n        else:\n            self.cache.clear()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return embedding_pb2.ClearCacheResponse()\n\n    async def Embed(self, request, context):\n        batch = self.model.batch_type.from_pb(\n            request.batch, self.model.tokenizer, self.model.device\n        )\n\n        executions = self.model.embed(batch)\n\n        return embedding_pb2.EmbedResponse(\n            embeddings=[e.to_pb() for e in executions],\n        )", "\ndef serve(\n    model_id: str,\n    revision: Optional[str],\n    sharded: bool,\n    quantize: Optional[str],\n    uds_path: Path,\n):\n    async def serve_inner(\n        model_id: str,\n        revision: Optional[str],\n        sharded: bool = False,\n        quantize: Optional[str] = None,\n    ):\n        unix_socket_template = \"unix://{}-{}\"\n        if sharded:\n            server_urls = [\n                unix_socket_template.format(uds_path, rank)\n                for rank in range(int(os.environ[\"WORLD_SIZE\"]))\n            ]\n            local_url = server_urls[int(os.environ[\"RANK\"])]\n        else:\n            local_url = unix_socket_template.format(uds_path, 0)\n            server_urls = [local_url]\n\n        try:\n            model = get_model(model_id, revision, sharded, quantize)\n        except Exception:\n            logger.exception(\"Error when initializing model\")\n            raise\n\n        server = aio.server(\n            interceptors=[\n                ExceptionInterceptor(),\n                UDSOpenTelemetryAioServerInterceptor(),\n            ]\n        )\n        embedding_pb2_grpc.add_EmbeddingServiceServicer_to_server(\n            EmbeddingService(model, Cache(), server_urls), server\n        )\n        SERVICE_NAMES = (\n            embedding_pb2.DESCRIPTOR.services_by_name[\"EmbeddingService\"].full_name,\n            reflection.SERVICE_NAME,\n        )\n        reflection.enable_server_reflection(SERVICE_NAMES, server)\n        server.add_insecure_port(local_url)\n\n        await server.start()\n\n        logger.info(\"Server started at {}\".format(local_url))\n\n        try:\n            await server.wait_for_termination()\n        except KeyboardInterrupt:\n            logger.info(\"Signal received. Shutting down\")\n            await server.stop(0)\n\n    asyncio.run(serve_inner(model_id, revision, sharded, quantize))", ""]}
{"filename": "server/embedding_server/cli.py", "chunked_list": ["import os\nimport sys\nimport warnings\n\nimport typer\n\nfrom pathlib import Path\nfrom loguru import logger\nfrom typing import Optional\nfrom enum import Enum", "from typing import Optional\nfrom enum import Enum\n\n\napp = typer.Typer()\n\n\nclass Quantization(str, Enum):\n    bitsandbytes = \"bitsandbytes\"\n    gptq = \"gptq\"", "\n\n@app.command()\ndef serve(\n    model_id: str,\n    revision: Optional[str] = None,\n    sharded: bool = False,\n    quantize: Optional[Quantization] = None,\n    uds_path: Path = \"/tmp/embedding-server\",\n    logger_level: str = \"INFO\",\n    json_output: bool = False,\n    otlp_endpoint: Optional[str] = None,\n):\n    if sharded:\n        warnings.warn(\"Sharded mode is not supported yet\")\n        sharded = False\n\n    if sharded:\n        assert (\n            os.getenv(\"RANK\", None) is not None\n        ), \"RANK must be set when sharded is True\"\n        assert (\n            os.getenv(\"WORLD_SIZE\", None) is not None\n        ), \"WORLD_SIZE must be set when sharded is True\"\n        assert (\n            os.getenv(\"MASTER_ADDR\", None) is not None\n        ), \"MASTER_ADDR must be set when sharded is True\"\n        assert (\n            os.getenv(\"MASTER_PORT\", None) is not None\n        ), \"MASTER_PORT must be set when sharded is True\"\n\n    # Remove default handler\n    logger.remove()\n    logger.add(\n        sys.stdout,\n        format=\"{message}\",\n        filter=\"embedding_server\",\n        level=logger_level,\n        serialize=json_output,\n        backtrace=True,\n        diagnose=False,\n    )\n\n    # Import here after the logger is added to log potential import exceptions\n    from embedding_server import server\n    from embedding_server.tracing import setup_tracing\n\n    # Setup OpenTelemetry distributed tracing\n    if otlp_endpoint is not None:\n        setup_tracing(shard=os.getenv(\"RANK\", 0), otlp_endpoint=otlp_endpoint)\n\n    # Downgrade enum into str for easier management later on\n    quantize = None if quantize is None else quantize.value\n    server.serve(model_id, revision, sharded, quantize, uds_path)", "\n\n@app.command()\ndef download_weights(\n    model_id: str,\n    revision: Optional[str] = None,\n    extension: str = \".safetensors\",\n    auto_convert: bool = True,\n    logger_level: str = \"INFO\",\n    json_output: bool = False,\n):\n    # Remove default handler\n    logger.remove()\n    logger.add(\n        sys.stdout,\n        format=\"{message}\",\n        filter=\"embedding_server\",\n        level=logger_level,\n        serialize=json_output,\n        backtrace=True,\n        diagnose=False,\n    )\n\n    # Import here after the logger is added to log potential import exceptions\n    from embedding_server import utils\n\n    # Test if files were already download\n    try:\n        utils.weight_files(model_id, revision, extension)\n        logger.info(\"Files are already present on the host. \" \"Skipping download.\")\n        return\n    # Local files not found\n    except (utils.LocalEntryNotFoundError, FileNotFoundError):\n        pass\n\n    is_local_model = (Path(model_id).exists() and Path(model_id).is_dir()) or os.getenv(\n        \"WEIGHTS_CACHE_OVERRIDE\", None\n    ) is not None\n\n    if not is_local_model:\n        # Try to download weights from the hub\n        try:\n            filenames = utils.weight_hub_files(model_id, revision, extension)\n            utils.download_weights(filenames, model_id, revision)\n            # Successfully downloaded weights\n            return\n\n        # No weights found on the hub with this extension\n        except utils.EntryNotFoundError as e:\n            # Check if we want to automatically convert to safetensors or if we can use .bin weights instead\n            if not extension == \".safetensors\" or not auto_convert:\n                raise e\n\n    # Try to see if there are local pytorch weights\n    try:\n        # Get weights for a local model, a hub cached model and inside the WEIGHTS_CACHE_OVERRIDE\n        local_pt_files = utils.weight_files(model_id, revision, \".bin\")\n\n    # No local pytorch weights\n    except utils.LocalEntryNotFoundError:\n        if extension == \".safetensors\":\n            logger.warning(\n                f\"No safetensors weights found for model {model_id} at revision {revision}. \"\n                f\"Downloading PyTorch weights.\"\n            )\n\n        # Try to see if there are pytorch weights on the hub\n        pt_filenames = utils.weight_hub_files(model_id, revision, \".bin\")\n        # Download pytorch weights\n        local_pt_files = utils.download_weights(pt_filenames, model_id, revision)\n\n    if auto_convert:\n        logger.warning(\n            f\"No safetensors weights found for model {model_id} at revision {revision}. \"\n            f\"Converting PyTorch weights to safetensors.\"\n        )\n\n        # Safetensors final filenames\n        local_st_files = [\n            p.parent / f\"{p.stem.lstrip('pytorch_')}.safetensors\"\n            for p in local_pt_files\n        ]\n        # Convert pytorch weights to safetensors\n        utils.convert_files(local_pt_files, local_st_files)", "\n\nif __name__ == \"__main__\":\n    app()\n"]}
{"filename": "server/embedding_server/utils/layers.py", "chunked_list": ["import torch\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom typing import Optional\n\nHAS_BITS_AND_BYTES = True\ntry:\n    from bitsandbytes.nn import Linear8bitLt\nexcept ImportError as e:\n    HAS_BITS_AND_BYTES = False", "\n\nclass FastLinear(nn.Linear):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        device=None,\n        dtype=None,\n    ) -> None:\n        super(FastLinear, self).__init__(in_features, out_features, bias, device, dtype)\n        self.quantized = False\n        self.bnb_linear = None\n\n    def prepare_weights(self, quantize: Optional[str] = None):\n        if quantize == \"bitsandbytes\":\n            if not HAS_BITS_AND_BYTES:\n                raise ImportError(\n                    \"bitsandbytes is not available on your machine either because it is not installed \"\n                    \"or you don't have a GPU.\\n\"\n                    \"You can install it with `pip install bitsandbytes`.\"\n                )\n\n            self.quantized = True\n            self.bnb_linear = Linear8bitLt(\n                self.in_features,\n                self.out_features,\n                has_fp16_weights=False,\n                threshold=6.0,\n                bias=False,\n            )\n            # Copy data to bnb_linear\n            self.bnb_linear.weight.data = self.weight.data\n            if self.bias is not None:\n                self.bnb_linear.bias = nn.Parameter(self.bias)\n\n            # Delete reference to data\n            self.weight = None\n            self.bias = None\n        elif quantize == \"gptq\":\n            raise NotImplementedError(\"`gptq` is not implemented for now\")\n        elif quantize is None:\n            self.weight = nn.Parameter(self.weight.T)\n        else:\n            raise ValueError(f\"Unexpected quantize `{quantize}`\")\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if self.quantized:\n            return self.bnb_linear(input)\n        else:\n            if self.bias is not None:\n                return torch.addmm(self.bias, input, self.weight)\n            return torch.matmul(input, self.weight)", "\n\nclass TensorParallelColumnLinear(FastLinear):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        process_group: torch.distributed.ProcessGroup,\n        bias=True,\n        device=None,\n        dtype=None,\n    ):\n        self.process_group = process_group\n        self.tp_world_size = process_group.size()\n        assert out_features % self.tp_world_size == 0\n        out_features = out_features // self.tp_world_size\n\n        super().__init__(\n            in_features=in_features,\n            out_features=out_features,\n            bias=bias,\n            device=device,\n            dtype=dtype,\n        )", "\n\nclass TensorParallelRowLinear(FastLinear):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        process_group: torch.distributed.ProcessGroup,\n        reduce=True,\n        bias=True,\n        device=None,\n        dtype=None,\n    ):\n        self.process_group = process_group\n        self.tp_world_size = process_group.size()\n        self.reduce = reduce\n        assert in_features % self.tp_world_size == 0\n        in_features = in_features // self.tp_world_size\n\n        super().__init__(\n            in_features=in_features,\n            out_features=out_features,\n            bias=bias,\n            device=device,\n            dtype=dtype,\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        out = super(TensorParallelRowLinear, self).forward(input)\n        if self.reduce:\n            torch.distributed.all_reduce(out, group=self.process_group)\n\n        return out", "\n\nclass TensorParallelEmbedding(nn.Embedding):\n    def __init__(\n        self,\n        num_embeddings,\n        embedding_dim,\n        process_group: torch.distributed.ProcessGroup,\n        reduce=True,\n        padding_idx=None,\n        max_norm=None,\n        norm_type=2.0,\n        scale_grad_by_freq=False,\n        sparse=False,\n        _weight=None,\n        device=None,\n        dtype=None,\n    ):\n        self.reduce = reduce\n        self.process_group = process_group\n        self.tp_rank = process_group.rank()\n        self.tp_world_size = process_group.size()\n\n        self.original_num_embeddings = num_embeddings\n\n        assert num_embeddings % self.tp_world_size == 0\n        block_size = num_embeddings // self.tp_world_size\n        # inputs in `[min_id, max_id[` are handled by `self` to get embeddings\n        self.min_id = self.tp_rank * block_size\n        self.max_id = (self.tp_rank + 1) * block_size\n\n        # Additional entry that will map to zero\n        # Used for masking\n        self.null_idx = block_size\n\n        super().__init__(\n            block_size,\n            embedding_dim,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse,\n            _weight=_weight,\n            device=device,\n            dtype=dtype,\n        )\n\n    def add_null_idx(self):\n        \"\"\"Additional 0 entry used for masking\"\"\"\n        self.weight = nn.Parameter(F.pad(self.weight, (0, 0, 0, 1)))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        # default all out of bounds values to `self.null_idx` that will then be mapped to 0\n        # translate for [0, self.max_id - self.min_id[\n        input = torch.where(\n            (self.min_id > input) | (input >= self.max_id),\n            self.null_idx,\n            input - self.min_id,\n        )\n        out = super().forward(input)\n        if self.reduce:\n            torch.distributed.all_reduce(out, group=self.process_group)\n        return out", "\n\ntry:\n    import dropout_layer_norm\n\n    class FastLayerNorm(nn.LayerNorm):\n        def forward(self, hidden_states, residual=None):\n            if hidden_states.shape[-1] > 8192:\n                if residual is not None:\n                    hidden_states += residual\n                residual = hidden_states\n\n                return super(FastLayerNorm, self).forward(hidden_states), residual\n            else:\n                (\n                    normed_hidden_states,\n                    residual,\n                    *rest,\n                ) = dropout_layer_norm.dropout_add_ln_fwd(\n                    hidden_states,\n                    residual,\n                    self.weight,\n                    self.bias,\n                    None,\n                    None,\n                    None,\n                    None,\n                    0.0,\n                    self.eps,\n                    1.0,\n                    0,\n                    None,\n                    False,\n                    False,\n                )\n                if residual is None:\n                    residual = hidden_states\n\n                return normed_hidden_states, residual\n\nexcept ImportError:\n    pass", "\n\ntry:\n    from flash_attn.layers.rotary import RotaryEmbedding\n    import rotary_emb\n\n    class PositionRotaryEmbedding(RotaryEmbedding):\n        def _update_cos_sin_cache(self, dtype, device, seqlen):\n            # Reset the tables if the sequence length has changed,\n            # or if we're on a new device (possibly due to tracing for instance)\n            if (\n                seqlen > self._seq_len_cached\n                or self._cos_cached.device != device\n                or self._cos_cached.dtype != dtype\n            ):\n                self._seq_len_cached = seqlen\n                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n                # Don't do einsum, it converts fp32 to fp16\n                # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n                freqs = torch.outer(t, self.inv_freq.to(device=t.device))\n                self._cos_cached = torch.cos(freqs).to(dtype)\n                self._sin_cached = torch.sin(freqs).to(dtype)\n\n        def get_cos_sin(\n            self, position_ids: torch.Tensor, max_s: int, dtype: torch.dtype\n        ):\n            \"\"\"\n            Return cos and sin for the asked position ids\n            \"\"\"\n\n            self._update_cos_sin_cache(dtype, position_ids.device, max_s)\n\n            cos = torch.index_select(self._cos_cached, 0, position_ids)\n            sin = torch.index_select(self._sin_cached, 0, position_ids)\n            return cos.unsqueeze(1), sin.unsqueeze(1)\n\n        def forward(self, qkv: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n            rotary_dim = cos.shape[-1]\n            q1 = qkv[:, 0, :, :rotary_dim]\n            q2 = qkv[:, 0, :, rotary_dim : 2 * rotary_dim]\n            k1 = qkv[:, 1, :, :rotary_dim]\n            k2 = qkv[:, 1, :, rotary_dim : 2 * rotary_dim]\n\n            rotary_emb.apply_rotary(q1, q2, cos, sin, q1, q2, False)\n            rotary_emb.apply_rotary(k1, k2, cos, sin, k1, k2, False)\n            return qkv\n\nexcept ImportError:\n    pass", ""]}
{"filename": "server/embedding_server/utils/convert.py", "chunked_list": ["import concurrent\nimport time\nimport datetime\nimport torch\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom collections import defaultdict\nfrom datetime import timedelta\nfrom loguru import logger\nfrom pathlib import Path", "from loguru import logger\nfrom pathlib import Path\nfrom safetensors.torch import load_file, save_file\nfrom safetensors import safe_open\nfrom typing import Dict, List\n\n\ndef check_file_size(source_file: Path, target_file: Path):\n    \"\"\"\n    Check that two files are close in size\n    \"\"\"\n    source_file_size = source_file.stat().st_size\n    target_file_size = target_file.stat().st_size\n\n    if (source_file_size - target_file_size) / source_file_size > 0.01:\n        raise RuntimeError(\n            f\"\"\"The file size different is more than 1%:\n         - {source_file}: {source_file_size}\n         - {target_file}: {target_file_size}\n         \"\"\"\n        )", "\n\ndef remove_shared_pointers(tensors: Dict[str, torch.Tensor]):\n    \"\"\"\n    For a Dict of tensors, check if two or more tensors point to the same underlying memory and\n    remove them\n    \"\"\"\n    ptrs = defaultdict(list)\n    for k, v in tensors.items():\n        ptrs[v.data_ptr()].append(k)\n\n    # Iterate over all found memory addresses\n    for ptr, names in ptrs.items():\n        if len(names) > 1:\n            # Multiple tensors are point to the same memory\n            # Only keep the first tensor\n            for name in names[1:]:\n                tensors.pop(name)", "\n\ndef convert_file(pt_file: Path, sf_file: Path):\n    \"\"\"\n    Convert a pytorch file to a safetensors file\n    \"\"\"\n    logger.info(f\"Convert {pt_file} to {sf_file}.\")\n\n    pt_state = torch.load(pt_file, map_location=\"cpu\")\n    if \"state_dict\" in pt_state:\n        pt_state = pt_state[\"state_dict\"]\n\n    remove_shared_pointers(pt_state)\n\n    # Tensors need to be contiguous\n    pt_state = {k: v.contiguous() for k, v in pt_state.items()}\n\n    sf_file.parent.mkdir(parents=True, exist_ok=True)\n    save_file(pt_state, str(sf_file), metadata={\"format\": \"pt\"})\n\n    # Check that both files are close in size\n    check_file_size(pt_file, sf_file)\n\n    # Load safetensors state\n    for k in pt_state:\n        pt_tensor = pt_state[k]\n        with safe_open(sf_file, framework=\"pt\") as f:\n            sf_tensor = f.get_tensor(k)\n            if not torch.equal(pt_tensor, sf_tensor):\n                raise RuntimeError(f\"The output tensors do not match for key {k}\")", "\n\ndef convert_files(pt_files: List[Path], sf_files: List[Path]):\n    assert len(pt_files) == len(sf_files)\n\n    N = len(pt_files)\n    # We do this instead of using tqdm because we want to parse the logs with the launcher\n\n    for i, (pt_file, sf_file) in enumerate(zip(pt_files, sf_files)):\n        start = datetime.datetime.now()\n        convert_file(pt_file, sf_file)\n        elapsed = datetime.datetime.now() - start\n        logger.info(f\"Convert: [{i + 1}/{N}] -- Took: {elapsed}\")", ""]}
{"filename": "server/embedding_server/utils/__init__.py", "chunked_list": ["from embedding_server.utils.convert import convert_file, convert_files\nfrom embedding_server.utils.dist import initialize_torch_distributed\nfrom embedding_server.utils.hub import (\n    weight_files,\n    weight_hub_files,\n    download_weights,\n    EntryNotFoundError,\n    LocalEntryNotFoundError,\n    RevisionNotFoundError,\n)", "    RevisionNotFoundError,\n)\n\n__all__ = [\n    \"convert_file\",\n    \"convert_files\",\n    \"initialize_torch_distributed\",\n    \"weight_files\",\n    \"weight_hub_files\",\n    \"download_weights\",", "    \"weight_hub_files\",\n    \"download_weights\",\n    \"EntryNotFoundError\",\n    \"LocalEntryNotFoundError\",\n    \"RevisionNotFoundError\",\n]\n"]}
{"filename": "server/embedding_server/utils/dist.py", "chunked_list": ["import os\nimport torch\n\nfrom datetime import timedelta\n\n\ndef initialize_torch_distributed():\n    rank = int(os.getenv(\"RANK\", \"0\"))\n    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n\n    if torch.cuda.is_available():\n        from torch.distributed import ProcessGroupNCCL\n\n        # Set the device id.\n        assert world_size <= torch.cuda.device_count(), \"Each process is one gpu\"\n        device = rank % torch.cuda.device_count()\n        torch.cuda.set_device(device)\n        backend = \"nccl\"\n        options = ProcessGroupNCCL.Options()\n        options.is_high_priority_stream = True\n        options._timeout = timedelta(seconds=60)\n    else:\n        backend = \"gloo\"\n        options = None\n\n    # Call the init process.\n    torch.distributed.init_process_group(\n        backend=backend,\n        world_size=world_size,\n        rank=rank,\n        timeout=timedelta(seconds=60),\n        pg_options=options,\n    )\n\n    return torch.distributed.group.WORLD, rank, world_size", ""]}
{"filename": "server/embedding_server/utils/hub.py", "chunked_list": ["import time\nimport os\n\nfrom datetime import timedelta\nfrom loguru import logger\nfrom pathlib import Path\nfrom typing import Optional, List\n\nfrom huggingface_hub import HfApi, hf_hub_download\nfrom huggingface_hub.constants import HUGGINGFACE_HUB_CACHE", "from huggingface_hub import HfApi, hf_hub_download\nfrom huggingface_hub.constants import HUGGINGFACE_HUB_CACHE\nfrom huggingface_hub.utils import (\n    LocalEntryNotFoundError,\n    EntryNotFoundError,\n    RevisionNotFoundError,  # Import here to ease try/except in other part of the lib\n)\n\nWEIGHTS_CACHE_OVERRIDE = os.getenv(\"WEIGHTS_CACHE_OVERRIDE\", None)\n", "WEIGHTS_CACHE_OVERRIDE = os.getenv(\"WEIGHTS_CACHE_OVERRIDE\", None)\n\n\ndef weight_hub_files(\n    model_id: str, revision: Optional[str] = None, extension: str = \".safetensors\"\n) -> List[str]:\n    \"\"\"Get the weights filenames on the hub\"\"\"\n    api = HfApi()\n    info = api.model_info(model_id, revision=revision)\n    filenames = [s.rfilename for s in info.siblings if s.rfilename.endswith(extension)]\n\n    if not filenames:\n        raise EntryNotFoundError(\n            f\"No {extension} weights found for model {model_id} and revision {revision}.\",\n            None,\n        )\n\n    return filenames", "\n\ndef try_to_load_from_cache(\n    model_id: str, revision: Optional[str], filename: str\n) -> Optional[Path]:\n    \"\"\"Try to load a file from the Hugging Face cache\"\"\"\n    if revision is None:\n        revision = \"main\"\n\n    object_id = model_id.replace(\"/\", \"--\")\n    repo_cache = Path(HUGGINGFACE_HUB_CACHE) / f\"models--{object_id}\"\n\n    if not repo_cache.is_dir():\n        # No cache for this model\n        return None\n\n    refs_dir = repo_cache / \"refs\"\n    snapshots_dir = repo_cache / \"snapshots\"\n\n    # Resolve refs (for instance to convert main to the associated commit sha)\n    if refs_dir.is_dir():\n        revision_file = refs_dir / revision\n        if revision_file.exists():\n            with revision_file.open() as f:\n                revision = f.read()\n\n    # Check if revision folder exists\n    if not snapshots_dir.exists():\n        return None\n    cached_shas = os.listdir(snapshots_dir)\n    if revision not in cached_shas:\n        # No cache for this revision and we won't try to return a random revision\n        return None\n\n    # Check if file exists in cache\n    cached_file = snapshots_dir / revision / filename\n    return cached_file if cached_file.is_file() else None", "\n\ndef weight_files(\n    model_id: str, revision: Optional[str] = None, extension: str = \".safetensors\"\n) -> List[Path]:\n    \"\"\"Get the local files\"\"\"\n    # Local model\n    if Path(model_id).exists() and Path(model_id).is_dir():\n        local_files = list(Path(model_id).glob(f\"*{extension}\"))\n        if not local_files:\n            raise FileNotFoundError(\n                f\"No local weights found in {model_id} with extension {extension}\"\n            )\n        return local_files\n\n    try:\n        filenames = weight_hub_files(model_id, revision, extension)\n    except EntryNotFoundError as e:\n        if extension != \".safetensors\":\n            raise e\n        # Try to see if there are pytorch weights\n        pt_filenames = weight_hub_files(model_id, revision, extension=\".bin\")\n        # Change pytorch extension to safetensors extension\n        # It is possible that we have safetensors weights locally even though they are not on the\n        # hub if we converted weights locally without pushing them\n        filenames = [\n            f\"{Path(f).stem.lstrip('pytorch_')}.safetensors\" for f in pt_filenames\n        ]\n\n    if WEIGHTS_CACHE_OVERRIDE is not None:\n        files = []\n        for filename in filenames:\n            p = Path(WEIGHTS_CACHE_OVERRIDE) / filename\n            if not p.exists():\n                raise FileNotFoundError(\n                    f\"File {p} not found in {WEIGHTS_CACHE_OVERRIDE}.\"\n                )\n            files.append(p)\n        return files\n\n    files = []\n    for filename in filenames:\n        cache_file = try_to_load_from_cache(\n            model_id, revision=revision, filename=filename\n        )\n        if cache_file is None:\n            raise LocalEntryNotFoundError(\n                f\"File {filename} of model {model_id} not found in \"\n                f\"{os.getenv('HUGGINGFACE_HUB_CACHE', 'the local cache')}. \"\n                f\"Please run `embedding-server download-weights {model_id}` first.\"\n            )\n        files.append(cache_file)\n\n    return files", "\n\ndef download_weights(\n    filenames: List[str], model_id: str, revision: Optional[str] = None\n) -> List[Path]:\n    \"\"\"Download the safetensors files from the hub\"\"\"\n\n    def download_file(filename):\n        local_file = try_to_load_from_cache(model_id, revision, filename)\n        if local_file is not None:\n            logger.info(f\"File {filename} already present in cache.\")\n            return Path(local_file)\n\n        logger.info(f\"Download file: {filename}\")\n        start_time = time.time()\n        local_file = hf_hub_download(\n            filename=filename,\n            repo_id=model_id,\n            revision=revision,\n            local_files_only=False,\n        )\n        logger.info(\n            f\"Downloaded {local_file} in {timedelta(seconds=int(time.time() - start_time))}.\"\n        )\n        return Path(local_file)\n\n    # We do this instead of using tqdm because we want to parse the logs with the launcher\n    start_time = time.time()\n    files = []\n    for i, filename in enumerate(filenames):\n        file = download_file(filename)\n\n        elapsed = timedelta(seconds=int(time.time() - start_time))\n        remaining = len(filenames) - (i + 1)\n        eta = (elapsed / (i + 1)) * remaining if remaining > 0 else 0\n\n        logger.info(f\"Download: [{i + 1}/{len(filenames)}] -- ETA: {eta}\")\n        files.append(file)\n\n    return files", ""]}
{"filename": "server/embedding_server/utils/watermark.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n# available at https://arxiv.org/abs/2301.10226\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nimport torch", "\nimport torch\nfrom transformers import LogitsProcessor\nfrom typing import List, Union\n\nGAMMA = os.getenv(\"WATERMARK_GAMMA\", 0.5)\nDELTA = os.getenv(\"WATERMARK_DELTA\", 2.0)\n\n\nclass WatermarkLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        gamma: float = GAMMA,\n        delta: float = DELTA,\n        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\n        device: str = \"cpu\",\n    ):\n        # watermarking parameters\n        self.gamma = gamma\n        self.delta = delta\n        self.rng = torch.Generator(device=device)\n        self.hash_key = hash_key\n\n    def _seed_rng(self, input_ids: Union[List[int], torch.LongTensor]):\n        if isinstance(input_ids, list):\n            assert (\n                len(input_ids) >= 1\n            ), \"requires at least a 1 token prefix sequence to seed rng\"\n            prev_token = input_ids[-1]\n        else:\n            assert len(input_ids) == 1\n            input_ids = input_ids[0]\n            assert (\n                input_ids.shape[-1] >= 1\n            ), \"requires at least a 1 token prefix sequence to seed rng\"\n            prev_token = input_ids[-1].item()\n        self.rng.manual_seed(self.hash_key * prev_token)\n\n    def _get_greenlist_ids(\n        self,\n        input_ids: Union[List[int], torch.LongTensor],\n        max_value: int,\n        device: torch.device,\n    ) -> List[int]:\n        # seed the rng using the previous tokens/prefix\n        self._seed_rng(input_ids)\n\n        greenlist_size = int(max_value * self.gamma)\n        vocab_permutation = torch.randperm(max_value, device=device, generator=self.rng)\n        greenlist_ids = vocab_permutation[:greenlist_size]\n        return greenlist_ids\n\n    @staticmethod\n    def _calc_greenlist_mask(\n        scores: torch.FloatTensor, greenlist_token_ids\n    ) -> torch.BoolTensor:\n        green_tokens_mask = torch.zeros_like(scores)\n        green_tokens_mask[-1, greenlist_token_ids] = 1\n        final_mask = green_tokens_mask.bool()\n        return final_mask\n\n    @staticmethod\n    def _bias_greenlist_logits(\n        scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float\n    ) -> torch.Tensor:\n        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n        return scores\n\n    def __call__(\n        self, input_ids: Union[List[int], torch.LongTensor], scores: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        greenlist_ids = self._get_greenlist_ids(\n            input_ids, scores.shape[-1], scores.device\n        )\n        green_tokens_mask = self._calc_greenlist_mask(\n            scores=scores, greenlist_token_ids=greenlist_ids\n        )\n\n        scores = self._bias_greenlist_logits(\n            scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta\n        )\n        return scores", "\nclass WatermarkLogitsProcessor(LogitsProcessor):\n    def __init__(\n        self,\n        gamma: float = GAMMA,\n        delta: float = DELTA,\n        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\n        device: str = \"cpu\",\n    ):\n        # watermarking parameters\n        self.gamma = gamma\n        self.delta = delta\n        self.rng = torch.Generator(device=device)\n        self.hash_key = hash_key\n\n    def _seed_rng(self, input_ids: Union[List[int], torch.LongTensor]):\n        if isinstance(input_ids, list):\n            assert (\n                len(input_ids) >= 1\n            ), \"requires at least a 1 token prefix sequence to seed rng\"\n            prev_token = input_ids[-1]\n        else:\n            assert len(input_ids) == 1\n            input_ids = input_ids[0]\n            assert (\n                input_ids.shape[-1] >= 1\n            ), \"requires at least a 1 token prefix sequence to seed rng\"\n            prev_token = input_ids[-1].item()\n        self.rng.manual_seed(self.hash_key * prev_token)\n\n    def _get_greenlist_ids(\n        self,\n        input_ids: Union[List[int], torch.LongTensor],\n        max_value: int,\n        device: torch.device,\n    ) -> List[int]:\n        # seed the rng using the previous tokens/prefix\n        self._seed_rng(input_ids)\n\n        greenlist_size = int(max_value * self.gamma)\n        vocab_permutation = torch.randperm(max_value, device=device, generator=self.rng)\n        greenlist_ids = vocab_permutation[:greenlist_size]\n        return greenlist_ids\n\n    @staticmethod\n    def _calc_greenlist_mask(\n        scores: torch.FloatTensor, greenlist_token_ids\n    ) -> torch.BoolTensor:\n        green_tokens_mask = torch.zeros_like(scores)\n        green_tokens_mask[-1, greenlist_token_ids] = 1\n        final_mask = green_tokens_mask.bool()\n        return final_mask\n\n    @staticmethod\n    def _bias_greenlist_logits(\n        scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float\n    ) -> torch.Tensor:\n        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n        return scores\n\n    def __call__(\n        self, input_ids: Union[List[int], torch.LongTensor], scores: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        greenlist_ids = self._get_greenlist_ids(\n            input_ids, scores.shape[-1], scores.device\n        )\n        green_tokens_mask = self._calc_greenlist_mask(\n            scores=scores, greenlist_token_ids=greenlist_ids\n        )\n\n        scores = self._bias_greenlist_logits(\n            scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta\n        )\n        return scores", ""]}
{"filename": "server/embedding_server/models/sentence_transformer.py", "chunked_list": ["import torch\n\nfrom dataclasses import dataclass\nfrom opentelemetry import trace\nfrom transformers import PreTrainedTokenizerBase\nfrom sentence_transformers import SentenceTransformer\nfrom typing import Optional, Tuple, List, Type, Dict\n\nfrom embedding_server.models import Model\nfrom embedding_server.models.types import (", "from embedding_server.models import Model\nfrom embedding_server.models.types import (\n    Batch,\n    Execution,\n    Embedding,\n)\nfrom embedding_server.pb import embedding_pb2\n\nfrom huggingface_hub.constants import HUGGINGFACE_HUB_CACHE\n", "from huggingface_hub.constants import HUGGINGFACE_HUB_CACHE\n\ntracer = trace.get_tracer(__name__)\n\n\n@dataclass\nclass SentenceTransformerBatch(Batch):\n    batch_id: int\n    requests: List[embedding_pb2.Request]\n    requests_idx_mapping: Dict[int, int]\n\n    # Texts that will be embedded\n    input_texts: List[str]\n\n    def to_pb(self) -> embedding_pb2.Batch:\n        return embedding_pb2.Batch(\n            id=self.batch_id,\n            requests=self.requests,\n            size=len(self),\n        )\n\n    @classmethod\n    def from_pb(\n        cls,\n        pb: embedding_pb2.Batch,\n        tokenizer: PreTrainedTokenizerBase,\n        device: torch.device,\n    ) -> \"SentenceTransformerBatch\":\n        inputs = []\n        requests_idx_mapping = {}\n\n        # Parse batch\n        for i, r in enumerate(pb.requests):\n            requests_idx_mapping[r.id] = i\n            inputs.append(r.inputs)\n\n        return cls(\n            batch_id=pb.id,\n            requests=pb.requests,\n            requests_idx_mapping=requests_idx_mapping,\n            input_texts=inputs\n        )\n\n    def __len__(self):\n        return len(self.requests)", "\n\nclass SentenceTransformerModel(Model):\n    def __init__(\n        self,\n        model_id: str,\n        revision: Optional[str] = None,\n    ):\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n            dtype = torch.float16\n        else:\n            device = torch.device(\"cpu\")\n            dtype = torch.float32\n\n        model = SentenceTransformer(model_id, device=str(device), cache_folder=HUGGINGFACE_HUB_CACHE).to(dtype)\n\n        super(SentenceTransformerModel, self).__init__(\n            model=model,\n            tokenizer=model.tokenizer,\n            dtype=dtype,\n            device=device,\n        )\n\n    @property\n    def batch_type(self) -> Type[SentenceTransformerBatch]:\n        return SentenceTransformerBatch\n\n    def forward(\n        self, input_ids, attention_mask, position_ids, past_key_values: Optional = None\n    ) -> Tuple[torch.Tensor, List[Tuple[torch.Tensor, torch.Tensor]]]:\n        # Model Forward\n        outputs = self.model.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            use_cache=True,\n        )\n        return outputs.logits, outputs.past_key_values\n\n    @tracer.start_as_current_span(\"embed\")\n    def embed(self, batch: SentenceTransformerBatch) -> List[Execution]:\n        # use the native encode function from the generic sentence_transformers module\n        embeddings = self.model.encode(\n            sentences=batch.input_texts,\n            batch_size=len(batch.input_texts),\n            show_progress_bar=False,\n            convert_to_numpy=False,\n            convert_to_tensor=True,\n            normalize_embeddings=False,\n            device=str(self.device)\n        )\n        assert embeddings.shape[0] == len(batch.requests)\n        embeddings = embeddings.cpu()\n        dim = embeddings.shape[1]\n\n        out = [\n            Execution(\n                request_id=req.id,\n                embedding=Embedding(\n                    embedding=embeddings[i].tolist(),\n                    dim=dim\n                )\n            )\n            for i, req in enumerate(batch.requests)\n        ]\n\n        return out", ""]}
{"filename": "server/embedding_server/models/model.py", "chunked_list": ["import torch\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple, Optional, TypeVar, Type\nfrom transformers import PreTrainedTokenizerBase\nfrom sentence_transformers import SentenceTransformer\n\nfrom embedding_server.models.types import Batch, Embedding\nfrom embedding_server.pb.embedding_pb2 import InfoResponse\n", "from embedding_server.pb.embedding_pb2 import InfoResponse\n\nB = TypeVar(\"B\", bound=Batch)\n\n\nclass Model(ABC):\n    def __init__(\n        self,\n        model: SentenceTransformer,\n        tokenizer: PreTrainedTokenizerBase,\n        dtype: torch.dtype,\n        device: torch.device,\n        rank: int = 0,\n        world_size: int = 1,\n    ):\n        self.model = model.eval()\n        test_emb = self.model.encode([\"test\"], convert_to_numpy=False, convert_to_tensor=True)\n        self.dim = int(test_emb.shape[-1])\n        self.tokenizer = tokenizer\n        self.dtype = dtype\n        self.device = device\n        self.rank = rank\n        self.world_size = world_size\n        self.check_initialized()\n\n    @property\n    def info(self) -> InfoResponse:\n        return InfoResponse(\n            requires_padding=True,\n            dtype=str(self.dtype),\n            device_type=self.device.type,\n            dim=self.dim,\n        )\n\n    @property\n    @abstractmethod\n    def batch_type(self) -> Type[B]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def embed(self, batch: B) -> List[Embedding]:\n        raise NotImplementedError\n\n    def check_initialized(self):\n        uninitialized_parameters = []\n        for n, p in self.model.named_parameters():\n            if p.data.device == torch.device(\"meta\"):\n                uninitialized_parameters.append(n)\n        if uninitialized_parameters:\n            raise RuntimeError(\n                f\"found uninitialized parameters in model {self.__class__.__name__}: {uninitialized_parameters}\"\n            )", ""]}
{"filename": "server/embedding_server/models/types.py", "chunked_list": ["import torch\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nfrom transformers import PreTrainedTokenizerBase\n\nfrom embedding_server.pb import embedding_pb2\nfrom embedding_server.pb.embedding_pb2 import Embedding", "from embedding_server.pb import embedding_pb2\nfrom embedding_server.pb.embedding_pb2 import Embedding\n\n\nclass Batch(ABC):\n    @abstractmethod\n    def to_pb(self) -> embedding_pb2.Batch:\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def from_pb(\n        cls,\n        pb: embedding_pb2.Batch,\n        tokenizer: PreTrainedTokenizerBase,\n        device: torch.device,\n    ) -> \"Batch\":\n        raise NotImplementedError\n\n    @abstractmethod\n    def __len__(self):\n        raise NotImplementedError", "\n\n@dataclass\nclass Embedding:\n    embedding: List[float]\n    dim: int\n\n    def to_pb(self) -> embedding_pb2.Embedding:\n        return embedding_pb2.Embedding(\n            embedding=self.embedding,\n            dim=self.dim\n        )", "\n\n@dataclass\nclass Execution:\n    request_id: int\n    embedding: Optional[Embedding]\n\n    def to_pb(self) -> embedding_pb2.Execution:\n        return embedding_pb2.Execution(\n            request_id=self.request_id,\n            embedding=self.embedding.to_pb() if self.embedding is not None else None,\n        )", ""]}
{"filename": "server/embedding_server/models/__init__.py", "chunked_list": ["import torch\n\nfrom loguru import logger\nfrom transformers import AutoConfig\nfrom transformers.models.auto import modeling_auto\nfrom typing import Optional\n\nfrom embedding_server.models.model import Model\nfrom embedding_server.models.sentence_transformer import SentenceTransformerModel\n", "from embedding_server.models.sentence_transformer import SentenceTransformerModel\n\n__all__ = [\n    \"Model\",\n    \"get_model\",\n]\n\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True", "# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\ntorch.backends.cudnn.allow_tf32 = True\n\n# Disable gradients\ntorch.set_grad_enabled(False)\n\n\ndef get_model(\n    model_id: str, revision: Optional[str], sharded: bool, quantize: Optional[str]\n) -> Model:\n    if sharded:\n        raise ValueError(\"sharded is not supported yet\")\n\n    if quantize is not None:\n        raise ValueError(\"quantize is not supported yet\")\n\n    return SentenceTransformerModel(model_id, revision)", "\n\ndef get_model(\n    model_id: str, revision: Optional[str], sharded: bool, quantize: Optional[str]\n) -> Model:\n    if sharded:\n        raise ValueError(\"sharded is not supported yet\")\n\n    if quantize is not None:\n        raise ValueError(\"quantize is not supported yet\")\n\n    return SentenceTransformerModel(model_id, revision)", ""]}
{"filename": "clients/python/tests/test_types.py", "chunked_list": ["import pytest\n\nfrom embedding_server.types import EmbedRequest\nfrom embedding_server.errors import ValidationError\n\n\ndef test_request_validation():\n    EmbedRequest(inputs=\"test\")\n\n    with pytest.raises(ValidationError):\n        EmbedRequest(inputs=\"\")\n\n    EmbedRequest(inputs=\"test\")\n    EmbedRequest(inputs=\"test\")\n\n    with pytest.raises(ValidationError):\n        EmbedRequest(\n            inputs=\"test\", stream=True\n        )", ""]}
{"filename": "clients/python/tests/test_errors.py", "chunked_list": ["from embedding_server.errors import (\n    parse_error,\n    ExecutionError,\n    IncompleteGenerationError,\n    OverloadedError,\n    ValidationError,\n    BadRequestError,\n    ShardNotReadyError,\n    ShardTimeoutError,\n    NotFoundError,", "    ShardTimeoutError,\n    NotFoundError,\n    RateLimitExceededError,\n    UnknownError,\n)\n\n\ndef test_generation_error():\n    payload = {\"error_type\": \"execution\", \"error\": \"test\"}\n    assert isinstance(parse_error(400, payload), ExecutionError)", "\n\ndef test_incomplete_generation_error():\n    payload = {\"error_type\": \"incomplete_generation\", \"error\": \"test\"}\n    assert isinstance(parse_error(400, payload), IncompleteGenerationError)\n\n\ndef test_overloaded_error():\n    payload = {\"error_type\": \"overloaded\", \"error\": \"test\"}\n    assert isinstance(parse_error(400, payload), OverloadedError)", "\n\ndef test_validation_error():\n    payload = {\"error_type\": \"validation\", \"error\": \"test\"}\n    assert isinstance(parse_error(400, payload), ValidationError)\n\n\ndef test_bad_request_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(400, payload), BadRequestError)", "\n\ndef test_shard_not_ready_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(403, payload), ShardNotReadyError)\n    assert isinstance(parse_error(424, payload), ShardNotReadyError)\n\n\ndef test_shard_timeout_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(504, payload), ShardTimeoutError)", "def test_shard_timeout_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(504, payload), ShardTimeoutError)\n\n\ndef test_not_found_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(404, payload), NotFoundError)\n\n\ndef test_rate_limit_exceeded_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(429, payload), RateLimitExceededError)", "\n\ndef test_rate_limit_exceeded_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(429, payload), RateLimitExceededError)\n\n\ndef test_unknown_error():\n    payload = {\"error\": \"test\"}\n    assert isinstance(parse_error(500, payload), UnknownError)", ""]}
{"filename": "clients/python/tests/test_client.py", "chunked_list": ["import pytest\n\nfrom embedding_server import Client, AsyncClient\nfrom embedding_server.errors import NotFoundError, ValidationError\n\n\ndef test_embed(flan_t5_xxl_url, hf_headers):\n    client = Client(flan_t5_xxl_url, hf_headers)\n    response = client.embed(\"test\")\n\n    assert response.embedding is not None\n    assert len(response.embedding) == 1024\n    assert response.dim == 1024", "\n\ndef test_generate_not_found(fake_url, hf_headers):\n    client = Client(fake_url, hf_headers)\n    with pytest.raises(NotFoundError):\n        client.embed(\"test\")\n\n\n@pytest.mark.asyncio\nasync def test_generate_async(flan_t5_xxl_url, hf_headers):", "@pytest.mark.asyncio\nasync def test_generate_async(flan_t5_xxl_url, hf_headers):\n    client = AsyncClient(flan_t5_xxl_url, hf_headers)\n    response = await client.embed(\"test\")\n\n    assert response.embedding is not None\n    assert len(response.embedding) == 1024\n    assert response.dim == 1024\n\n", "\n\n@pytest.mark.asyncio\nasync def test_generate_async_not_found(fake_url, hf_headers):\n    client = AsyncClient(fake_url, hf_headers)\n    with pytest.raises(NotFoundError):\n        await client.embed(\"test\")\n"]}
{"filename": "clients/python/tests/conftest.py", "chunked_list": ["import pytest\n\nfrom embedding_server import __version__\nfrom huggingface_hub.utils import build_hf_headers\n\n\n@pytest.fixture\ndef flan_t5_xxl():\n    return \"google/flan-t5-xxl\"\n", "\n\n@pytest.fixture\ndef fake_model():\n    return \"fake/model\"\n\n\n@pytest.fixture\ndef unsupported_model():\n    return \"gpt2\"", "def unsupported_model():\n    return \"gpt2\"\n\n\n@pytest.fixture\ndef base_url():\n    return \"https://api-inference.huggingface.co/models\"\n\n\n@pytest.fixture\ndef bloom_url(base_url, bloom_model):\n    return f\"{base_url}/{bloom_model}\"", "\n@pytest.fixture\ndef bloom_url(base_url, bloom_model):\n    return f\"{base_url}/{bloom_model}\"\n\n\n@pytest.fixture\ndef flan_t5_xxl_url(base_url, flan_t5_xxl):\n    return f\"{base_url}/{flan_t5_xxl}\"\n", "\n\n@pytest.fixture\ndef fake_url(base_url, fake_model):\n    return f\"{base_url}/{fake_model}\"\n\n\n@pytest.fixture\ndef unsupported_url(base_url, unsupported_model):\n    return f\"{base_url}/{unsupported_model}\"", "def unsupported_url(base_url, unsupported_model):\n    return f\"{base_url}/{unsupported_model}\"\n\n\n@pytest.fixture(scope=\"session\")\ndef hf_headers():\n    return build_hf_headers(\n        library_name=\"text-generation-tests\", library_version=__version__\n    )\n", ""]}
{"filename": "clients/python/embedding_server/types.py", "chunked_list": ["from enum import Enum\nfrom pydantic import BaseModel, validator\nfrom typing import Optional, List\n\nfrom embedding_server.errors import ValidationError\n\n\nclass EmbedRequest(BaseModel):\n    # Prompt\n    inputs: str\n\n    @validator(\"inputs\")\n    def valid_input(cls, v):\n        if not v:\n            raise ValidationError(\"`inputs` cannot be empty\")\n        return v", "\n\n# `embed` return value\nclass EmbedResponse(BaseModel):\n    # embedding for the passed input\n    embedding: List[float]\n    # dimensions of the embedding\n    dim: int\n\n", "\n\n# `token_count` return value\nclass TokenCountResponse(BaseModel):\n    # number of tokens in the passed input\n    count: int\n\n\n# `info` return value\nclass InfoResponse(BaseModel):\n    # Model info\n    model_id: str\n    model_sha: Optional[str]\n    model_dtype: str\n    model_device_type: str\n    model_pipeline_tag: Optional[str]\n    model_dim: int\n\n    # Router Parameters\n    max_concurrent_requests: int\n    max_input_length: int\n    waiting_served_ratio: float\n    max_batch_total_tokens: int\n    validation_workers: int\n\n    # Router Info\n    version: str\n    sha: Optional[str]\n    docker_label: Optional[str]", "# `info` return value\nclass InfoResponse(BaseModel):\n    # Model info\n    model_id: str\n    model_sha: Optional[str]\n    model_dtype: str\n    model_device_type: str\n    model_pipeline_tag: Optional[str]\n    model_dim: int\n\n    # Router Parameters\n    max_concurrent_requests: int\n    max_input_length: int\n    waiting_served_ratio: float\n    max_batch_total_tokens: int\n    validation_workers: int\n\n    # Router Info\n    version: str\n    sha: Optional[str]\n    docker_label: Optional[str]", "\n\n# Inference API currently deployed model\nclass DeployedModel(BaseModel):\n    model_id: str\n    sha: str\n"]}
{"filename": "clients/python/embedding_server/errors.py", "chunked_list": ["from typing import Dict\n\n\n# Text Generation Inference Errors\nclass ValidationError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass ExecutionError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)", "\nclass ExecutionError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass OverloadedError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n", "\n\nclass IncompleteGenerationError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\n# API Inference Errors\nclass BadRequestError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)", "class BadRequestError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass ShardNotReadyError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass ShardTimeoutError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)", "\n\nclass ShardTimeoutError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass NotFoundError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)", "\n\nclass RateLimitExceededError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\nclass NotSupportedError(Exception):\n    def __init__(self, model_id: str):\n        message = (\n            f\"Model `{model_id}` is not available for inference with this client. \\n\"\n            \"Use `huggingface_hub.inference_api.InferenceApi` instead.\"\n        )\n        super(NotSupportedError, self).__init__(message)", "\n\n# Unknown error\nclass UnknownError(Exception):\n    def __init__(self, message: str):\n        super().__init__(message)\n\n\ndef parse_error(status_code: int, payload: Dict[str, str]) -> Exception:\n    \"\"\"\n    Parse error given an HTTP status code and a json payload\n\n    Args:\n        status_code (`int`):\n            HTTP status code\n        payload (`Dict[str, str]`):\n            Json payload\n\n    Returns:\n        Exception: parsed exception\n\n    \"\"\"\n    # Try to parse a Text Generation Inference error\n    message = payload[\"error\"]\n    if \"error_type\" in payload:\n        error_type = payload[\"error_type\"]\n        if error_type == \"execution\":\n            return ExecutionError(message)\n        if error_type == \"incomplete_generation\":\n            return IncompleteGenerationError(message)\n        if error_type == \"overloaded\":\n            return OverloadedError(message)\n        if error_type == \"validation\":\n            return ValidationError(message)\n\n    # Try to parse a APIInference error\n    if status_code == 400:\n        return BadRequestError(message)\n    if status_code == 403 or status_code == 424:\n        return ShardNotReadyError(message)\n    if status_code == 504:\n        return ShardTimeoutError(message)\n    if status_code == 404:\n        return NotFoundError(message)\n    if status_code == 429:\n        return RateLimitExceededError(message)\n\n    # Fallback to an unknown error\n    return UnknownError(message)", "def parse_error(status_code: int, payload: Dict[str, str]) -> Exception:\n    \"\"\"\n    Parse error given an HTTP status code and a json payload\n\n    Args:\n        status_code (`int`):\n            HTTP status code\n        payload (`Dict[str, str]`):\n            Json payload\n\n    Returns:\n        Exception: parsed exception\n\n    \"\"\"\n    # Try to parse a Text Generation Inference error\n    message = payload[\"error\"]\n    if \"error_type\" in payload:\n        error_type = payload[\"error_type\"]\n        if error_type == \"execution\":\n            return ExecutionError(message)\n        if error_type == \"incomplete_generation\":\n            return IncompleteGenerationError(message)\n        if error_type == \"overloaded\":\n            return OverloadedError(message)\n        if error_type == \"validation\":\n            return ValidationError(message)\n\n    # Try to parse a APIInference error\n    if status_code == 400:\n        return BadRequestError(message)\n    if status_code == 403 or status_code == 424:\n        return ShardNotReadyError(message)\n    if status_code == 504:\n        return ShardTimeoutError(message)\n    if status_code == 404:\n        return NotFoundError(message)\n    if status_code == 429:\n        return RateLimitExceededError(message)\n\n    # Fallback to an unknown error\n    return UnknownError(message)", ""]}
{"filename": "clients/python/embedding_server/client.py", "chunked_list": ["import json\nimport requests\n\nfrom aiohttp import ClientSession, ClientTimeout\nfrom pydantic import ValidationError\nfrom typing import Dict, Optional, List, AsyncIterator, Iterator\n\nfrom embedding_server.types import (\n    EmbedResponse,\n    EmbedRequest, InfoResponse, TokenCountResponse,", "    EmbedResponse,\n    EmbedRequest, InfoResponse, TokenCountResponse,\n)\nfrom embedding_server.errors import parse_error\n\n\nclass Client:\n    \"\"\"Client to make calls to a text-generation-inference instance\n\n     Example:\n\n     ```python\n     >>> from embedding_server import Client\n\n     >>> client = Client(\"https://api-inference.huggingface.co/models/bigscience/bloomz\")\n     >>> client.embed(\"Why is the sky blue?\").embedding\n     [0.1, 0.2, 0.3, ...]\n     ```\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        headers: Optional[Dict[str, str]] = None,\n        cookies: Optional[Dict[str, str]] = None,\n        timeout: int = 10,\n    ):\n        \"\"\"\n        Args:\n            base_url (`str`):\n                text-generation-inference instance base url\n            headers (`Optional[Dict[str, str]]`):\n                Additional headers\n            cookies (`Optional[Dict[str, str]]`):\n                Cookies to include in the requests\n            timeout (`int`):\n                Timeout in seconds\n        \"\"\"\n        self.base_url = base_url\n        self.headers = headers\n        self.cookies = cookies\n        self.timeout = timeout\n\n    def info(self) -> InfoResponse:\n        \"\"\"\n        Get the model info\n\n        Returns:\n            InfoResponse: model info\n        \"\"\"\n        resp = requests.get(\n            self.base_url + \"/info\",\n            headers=self.headers,\n            cookies=self.cookies,\n            timeout=self.timeout,\n        )\n        payload = resp.json()\n        if resp.status_code != 200:\n            raise parse_error(resp.status_code, payload)\n        return InfoResponse(**payload)\n\n    def embed(\n        self,\n        inputs: str,\n    ) -> EmbedResponse:\n        \"\"\"\n        Given a prompt, generate the following text\n\n        Args:\n            inputs (`str`):\n                Input text that will be embedded\n\n        Returns:\n            EmbedResponse: embedding for the text\n        \"\"\"\n        request = EmbedRequest(inputs=inputs)\n\n        resp = requests.post(\n            self.base_url + \"/embed\",\n            json=request.dict(),\n            headers=self.headers,\n            cookies=self.cookies,\n            timeout=self.timeout,\n        )\n        payload = resp.json()\n        if resp.status_code != 200:\n            raise parse_error(resp.status_code, payload)\n        return EmbedResponse(**payload)\n\n    def token_count(\n        self,\n        inputs: str,\n    ) -> TokenCountResponse:\n        \"\"\"\n        Given a prompt, generate the following text\n\n        Args:\n            inputs (`str`):\n                Input text that will be embedded\n\n        Returns:\n            EmbedResponse: embedding for the text\n        \"\"\"\n        request = EmbedRequest(inputs=inputs)\n\n        resp = requests.post(\n            self.base_url + \"/token-count\",\n            json=request.dict(),\n            headers=self.headers,\n            cookies=self.cookies,\n            timeout=self.timeout,\n        )\n        payload = resp.json()\n        if resp.status_code != 200:\n            raise parse_error(resp.status_code, payload)\n        return TokenCountResponse(**payload)", "\n\nclass AsyncClient:\n    \"\"\"Asynchronous Client to make calls to a text-generation-inference instance\n\n     Example:\n\n     ```python\n     >>> from embedding_server import AsyncClient\n\n     >>> client = AsyncClient(\"https://api-inference.huggingface.co/models/bigscience/bloomz\")\n     >>> response = await client.embed(\"Why is the sky blue?\")\n     >>> response.embedding\n     [0.1, 0.2, 0.3, ...]\n     ```\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        headers: Optional[Dict[str, str]] = None,\n        cookies: Optional[Dict[str, str]] = None,\n        timeout: int = 10,\n    ):\n        \"\"\"\n        Args:\n            base_url (`str`):\n                text-generation-inference instance base url\n            headers (`Optional[Dict[str, str]]`):\n                Additional headers\n            cookies (`Optional[Dict[str, str]]`):\n                Cookies to include in the requests\n            timeout (`int`):\n                Timeout in seconds\n        \"\"\"\n        self.base_url = base_url\n        self.headers = headers\n        self.cookies = cookies\n        self.timeout = ClientTimeout(timeout * 60)\n\n    async def info(self) -> InfoResponse:\n        \"\"\"\n        Get the model info\n\n        Returns:\n            InfoResponse: model info\n        \"\"\"\n        async with ClientSession(\n            headers=self.headers, cookies=self.cookies, timeout=self.timeout\n        ) as session:\n            async with session.get(self.base_url + \"/info\") as resp:\n                payload = await resp.json()\n\n                if resp.status != 200:\n                    raise parse_error(resp.status, payload)\n                return InfoResponse(**payload)\n\n    async def embed(\n        self,\n        inputs: str,\n    ) -> EmbedResponse:\n        \"\"\"\n        Given a prompt, generate the following text asynchronously\n\n        Args:\n            inputs (`str`):\n                Input text that will be embedded\n\n        Returns:\n            EmbedResponse: embedding for the text\n        \"\"\"\n        request = EmbedRequest(inputs=inputs)\n\n        async with ClientSession(\n            headers=self.headers, cookies=self.cookies, timeout=self.timeout\n        ) as session:\n            async with session.post(self.base_url, json=request.dict()) as resp:\n                payload = await resp.json()\n\n                if resp.status != 200:\n                    raise parse_error(resp.status, payload)\n                return EmbedResponse(**payload)\n\n    async def token_count(\n        self,\n        inputs: str,\n    ) -> TokenCountResponse:\n        \"\"\"\n        Given a prompt, generate the following text asynchronously\n\n        Args:\n            inputs (`str`):\n                Input text that will be embedded\n\n        Returns:\n            EmbedResponse: embedding for the text\n        \"\"\"\n        request = EmbedRequest(inputs=inputs)\n\n        async with ClientSession(\n            headers=self.headers, cookies=self.cookies, timeout=self.timeout\n        ) as session:\n            async with session.post(self.base_url + \"/token_count\", json=request.dict()) as resp:\n                payload = await resp.json()\n\n                if resp.status != 200:\n                    raise parse_error(resp.status, payload)\n                return TokenCountResponse(**payload)", ""]}
{"filename": "clients/python/embedding_server/__init__.py", "chunked_list": ["# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \"0.3.0\"\n\nfrom embedding_server.client import Client, AsyncClient\n", "from embedding_server.client import Client, AsyncClient\n"]}
{"filename": "integration-tests/conftest.py", "chunked_list": ["import sys\nimport subprocess\nimport contextlib\nimport pytest\nimport asyncio\nimport os\nimport docker\nimport json\nimport math\nimport time", "import math\nimport time\nimport random\n\nfrom docker.errors import NotFound\nfrom typing import Optional, List, Dict\nfrom syrupy.extensions.json import JSONSnapshotExtension\nfrom aiohttp import ClientConnectorError, ClientOSError, ServerDisconnectedError\n\nfrom text_generation import AsyncClient", "\nfrom text_generation import AsyncClient\nfrom text_generation.types import Response, Details, PrefillToken, Token, BestOfSequence\n\nDOCKER_IMAGE = os.getenv(\"DOCKER_IMAGE\", None)\nHUGGING_FACE_HUB_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\", None)\nDOCKER_VOLUME = os.getenv(\"DOCKER_VOLUME\", \"/data\")\n\n\nclass ResponseComparator(JSONSnapshotExtension):\n    def serialize(\n        self,\n        data,\n        *,\n        exclude=None,\n        matcher=None,\n    ):\n        if isinstance(data, List):\n            data = [d.dict() for d in data]\n\n        data = self._filter(\n            data=data, depth=0, path=(), exclude=exclude, matcher=matcher\n        )\n        return json.dumps(data, indent=2, ensure_ascii=False, sort_keys=False) + \"\\n\"\n\n    def matches(\n        self,\n        *,\n        serialized_data,\n        snapshot_data,\n    ) -> bool:\n        def convert_data(data):\n            data = json.loads(data)\n\n            if isinstance(data, Dict):\n                return Response(**data)\n            if isinstance(data, List):\n                return [Response(**d) for d in data]\n            raise NotImplementedError\n\n        def eq_token(token: Token, other: Token) -> bool:\n            return (\n                token.id == other.id\n                and token.text == other.text\n                and math.isclose(token.logprob, other.logprob, rel_tol=0.2)\n                and token.special == other.special\n            )\n\n        def eq_prefill_token(prefill_token: PrefillToken, other: PrefillToken) -> bool:\n            try:\n                return (\n                    prefill_token.id == other.id\n                    and prefill_token.text == other.text\n                    and (\n                        math.isclose(prefill_token.logprob, other.logprob, rel_tol=0.2)\n                        if prefill_token.logprob is not None\n                        else prefill_token.logprob == other.logprob\n                    )\n                )\n            except TypeError:\n                return False\n\n        def eq_best_of(details: BestOfSequence, other: BestOfSequence) -> bool:\n            return (\n                details.finish_reason == other.finish_reason\n                and details.generated_tokens == other.generated_tokens\n                and details.seed == other.seed\n                and len(details.prefill) == len(other.prefill)\n                and all(\n                    [\n                        eq_prefill_token(d, o)\n                        for d, o in zip(details.prefill, other.prefill)\n                    ]\n                )\n                and len(details.tokens) == len(other.tokens)\n                and all([eq_token(d, o) for d, o in zip(details.tokens, other.tokens)])\n            )\n\n        def eq_details(details: Details, other: Details) -> bool:\n            return (\n                details.finish_reason == other.finish_reason\n                and details.generated_tokens == other.generated_tokens\n                and details.seed == other.seed\n                and len(details.prefill) == len(other.prefill)\n                and all(\n                    [\n                        eq_prefill_token(d, o)\n                        for d, o in zip(details.prefill, other.prefill)\n                    ]\n                )\n                and len(details.tokens) == len(other.tokens)\n                and all([eq_token(d, o) for d, o in zip(details.tokens, other.tokens)])\n                and (\n                    len(details.best_of_sequences)\n                    if details.best_of_sequences is not None\n                    else 0\n                )\n                == (\n                    len(other.best_of_sequences)\n                    if other.best_of_sequences is not None\n                    else 0\n                )\n                and (\n                    all(\n                        [\n                            eq_best_of(d, o)\n                            for d, o in zip(\n                                details.best_of_sequences, other.best_of_sequences\n                            )\n                        ]\n                    )\n                    if details.best_of_sequences is not None\n                    else details.best_of_sequences == other.best_of_sequences\n                )\n            )\n\n        def eq_response(response: Response, other: Response) -> bool:\n            return response.generated_text == other.generated_text and eq_details(\n                response.details, other.details\n            )\n\n        serialized_data = convert_data(serialized_data)\n        snapshot_data = convert_data(snapshot_data)\n\n        if not isinstance(serialized_data, List):\n            serialized_data = [serialized_data]\n        if not isinstance(snapshot_data, List):\n            snapshot_data = [snapshot_data]\n\n        return len(snapshot_data) == len(serialized_data) and all(\n            [eq_response(r, o) for r, o in zip(serialized_data, snapshot_data)]\n        )", "\nclass ResponseComparator(JSONSnapshotExtension):\n    def serialize(\n        self,\n        data,\n        *,\n        exclude=None,\n        matcher=None,\n    ):\n        if isinstance(data, List):\n            data = [d.dict() for d in data]\n\n        data = self._filter(\n            data=data, depth=0, path=(), exclude=exclude, matcher=matcher\n        )\n        return json.dumps(data, indent=2, ensure_ascii=False, sort_keys=False) + \"\\n\"\n\n    def matches(\n        self,\n        *,\n        serialized_data,\n        snapshot_data,\n    ) -> bool:\n        def convert_data(data):\n            data = json.loads(data)\n\n            if isinstance(data, Dict):\n                return Response(**data)\n            if isinstance(data, List):\n                return [Response(**d) for d in data]\n            raise NotImplementedError\n\n        def eq_token(token: Token, other: Token) -> bool:\n            return (\n                token.id == other.id\n                and token.text == other.text\n                and math.isclose(token.logprob, other.logprob, rel_tol=0.2)\n                and token.special == other.special\n            )\n\n        def eq_prefill_token(prefill_token: PrefillToken, other: PrefillToken) -> bool:\n            try:\n                return (\n                    prefill_token.id == other.id\n                    and prefill_token.text == other.text\n                    and (\n                        math.isclose(prefill_token.logprob, other.logprob, rel_tol=0.2)\n                        if prefill_token.logprob is not None\n                        else prefill_token.logprob == other.logprob\n                    )\n                )\n            except TypeError:\n                return False\n\n        def eq_best_of(details: BestOfSequence, other: BestOfSequence) -> bool:\n            return (\n                details.finish_reason == other.finish_reason\n                and details.generated_tokens == other.generated_tokens\n                and details.seed == other.seed\n                and len(details.prefill) == len(other.prefill)\n                and all(\n                    [\n                        eq_prefill_token(d, o)\n                        for d, o in zip(details.prefill, other.prefill)\n                    ]\n                )\n                and len(details.tokens) == len(other.tokens)\n                and all([eq_token(d, o) for d, o in zip(details.tokens, other.tokens)])\n            )\n\n        def eq_details(details: Details, other: Details) -> bool:\n            return (\n                details.finish_reason == other.finish_reason\n                and details.generated_tokens == other.generated_tokens\n                and details.seed == other.seed\n                and len(details.prefill) == len(other.prefill)\n                and all(\n                    [\n                        eq_prefill_token(d, o)\n                        for d, o in zip(details.prefill, other.prefill)\n                    ]\n                )\n                and len(details.tokens) == len(other.tokens)\n                and all([eq_token(d, o) for d, o in zip(details.tokens, other.tokens)])\n                and (\n                    len(details.best_of_sequences)\n                    if details.best_of_sequences is not None\n                    else 0\n                )\n                == (\n                    len(other.best_of_sequences)\n                    if other.best_of_sequences is not None\n                    else 0\n                )\n                and (\n                    all(\n                        [\n                            eq_best_of(d, o)\n                            for d, o in zip(\n                                details.best_of_sequences, other.best_of_sequences\n                            )\n                        ]\n                    )\n                    if details.best_of_sequences is not None\n                    else details.best_of_sequences == other.best_of_sequences\n                )\n            )\n\n        def eq_response(response: Response, other: Response) -> bool:\n            return response.generated_text == other.generated_text and eq_details(\n                response.details, other.details\n            )\n\n        serialized_data = convert_data(serialized_data)\n        snapshot_data = convert_data(snapshot_data)\n\n        if not isinstance(serialized_data, List):\n            serialized_data = [serialized_data]\n        if not isinstance(snapshot_data, List):\n            snapshot_data = [snapshot_data]\n\n        return len(snapshot_data) == len(serialized_data) and all(\n            [eq_response(r, o) for r, o in zip(serialized_data, snapshot_data)]\n        )", "\n\nclass LauncherHandle:\n    def __init__(self, port: int):\n        self.client = AsyncClient(f\"http://localhost:{port}\")\n\n    def _inner_health(self):\n        raise NotImplementedError\n\n    async def health(self, timeout: int = 60):\n        assert timeout > 0\n        for _ in range(timeout):\n            if not self._inner_health():\n                raise RuntimeError(\"Launcher crashed\")\n\n            try:\n                await self.client.generate(\"test\")\n                return\n            except (ClientConnectorError, ClientOSError, ServerDisconnectedError) as e:\n                time.sleep(1)\n        raise RuntimeError(\"Health check failed\")", "\n\nclass ContainerLauncherHandle(LauncherHandle):\n    def __init__(self, docker_client, container_name, port: int):\n        super(ContainerLauncherHandle, self).__init__(port)\n        self.docker_client = docker_client\n        self.container_name = container_name\n\n    def _inner_health(self) -> bool:\n        container = self.docker_client.containers.get(self.container_name)\n        return container.status in [\"running\", \"created\"]", "\n\nclass ProcessLauncherHandle(LauncherHandle):\n    def __init__(self, process, port: int):\n        super(ProcessLauncherHandle, self).__init__(port)\n        self.process = process\n\n    def _inner_health(self) -> bool:\n        return self.process.poll() is None\n", "\n\n@pytest.fixture\ndef response_snapshot(snapshot):\n    return snapshot.use_extension(ResponseComparator)\n\n\n@pytest.fixture(scope=\"module\")\ndef event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n    loop.close()", "def event_loop():\n    loop = asyncio.get_event_loop()\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"module\")\ndef launcher(event_loop):\n    @contextlib.contextmanager\n    def local_launcher(\n        model_id: str, num_shard: Optional[int] = None, quantize: Optional[str] = None\n    ):\n        port = random.randint(8000, 10_000)\n        master_port = random.randint(10_000, 20_000)\n\n        shard_uds_path = (\n            f\"/tmp/tgi-tests-{model_id.split('/')[-1]}-{num_shard}-{quantize}-server\"\n        )\n\n        args = [\n            \"embedding-server-launcher\",\n            \"--model-id\",\n            model_id,\n            \"--port\",\n            str(port),\n            \"--master-port\",\n            str(master_port),\n            \"--shard-uds-path\",\n            shard_uds_path,\n        ]\n\n        if num_shard is not None:\n            args.extend([\"--num-shard\", str(num_shard)])\n        if quantize:\n            args.append(\"--quantize\")\n\n        with subprocess.Popen(\n            args, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        ) as process:\n            yield ProcessLauncherHandle(process, port)\n\n            process.terminate()\n            process.wait(60)\n\n            launcher_output = process.stdout.read().decode(\"utf-8\")\n            print(launcher_output, file=sys.stderr)\n\n            process.stdout.close()\n            process.stderr.close()\n\n    @contextlib.contextmanager\n    def docker_launcher(\n        model_id: str, num_shard: Optional[int] = None, quantize: Optional[str] = None\n    ):\n        port = random.randint(8000, 10_000)\n\n        args = [\"--model-id\", model_id, \"--env\"]\n\n        if num_shard is not None:\n            args.extend([\"--num-shard\", str(num_shard)])\n        if quantize:\n            args.append(\"--quantize\")\n\n        client = docker.from_env()\n\n        container_name = f\"tgi-tests-{model_id.split('/')[-1]}-{num_shard}-{quantize}\"\n\n        try:\n            container = client.containers.get(container_name)\n            container.stop()\n            container.wait()\n        except NotFound:\n            pass\n\n        gpu_count = num_shard if num_shard is not None else 1\n\n        env = {}\n        if HUGGING_FACE_HUB_TOKEN is not None:\n            env[\"HUGGING_FACE_HUB_TOKEN\"] = HUGGING_FACE_HUB_TOKEN\n\n        volumes = []\n        if DOCKER_VOLUME:\n            volumes = [f\"{DOCKER_VOLUME}:/data\"]\n\n        container = client.containers.run(\n            DOCKER_IMAGE,\n            command=args,\n            name=container_name,\n            environment=env,\n            auto_remove=False,\n            detach=True,\n            device_requests=[\n                docker.types.DeviceRequest(count=gpu_count, capabilities=[[\"gpu\"]])\n            ],\n            volumes=volumes,\n            ports={\"80/tcp\": port},\n        )\n\n        yield ContainerLauncherHandle(client, container.name, port)\n\n        try:\n            container.stop()\n            container.wait()\n        except NotFound:\n            pass\n\n        container_output = container.logs().decode(\"utf-8\")\n        print(container_output, file=sys.stderr)\n\n        container.remove()\n\n    if DOCKER_IMAGE is not None:\n        return docker_launcher\n    return local_launcher", "\n\n@pytest.fixture(scope=\"module\")\ndef generate_load():\n    async def generate_load_inner(\n        client: AsyncClient, prompt: str, max_new_tokens: int, n: int\n    ) -> List[Response]:\n        futures = [\n            client.generate(prompt, max_new_tokens=max_new_tokens) for _ in range(n)\n        ]\n\n        return await asyncio.gather(*futures)\n\n    return generate_load_inner", ""]}
{"filename": "integration-tests/models/test_bloom_560m_sharded.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef bloom_560m_sharded_handle(launcher):\n    with launcher(\"all-MiniLM-L6-v2\", num_shard=2) as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def bloom_560m_sharded(bloom_560m_sharded_handle):\n    await bloom_560m_sharded_handle.health(60)\n    return bloom_560m_sharded_handle.client\n\n\n@pytest.mark.asyncio\nasync def test_bloom_560m_sharded(bloom_560m_sharded, response_snapshot):\n    response = await bloom_560m_sharded.generate(", "async def test_bloom_560m_sharded(bloom_560m_sharded, response_snapshot):\n    response = await bloom_560m_sharded.generate(\n        \"Pour d\u00e9guster un ortolan, il faut tout d'abord\",\n        max_new_tokens=10,\n        top_p=0.9,\n        seed=0,\n    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot", "    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\nasync def test_bloom_560m_sharded_load(\n    bloom_560m_sharded, generate_load, response_snapshot\n):\n    responses = await generate_load(\n        bloom_560m_sharded,", "    responses = await generate_load(\n        bloom_560m_sharded,\n        \"Pour d\u00e9guster un ortolan, il faut tout d'abord\",\n        max_new_tokens=10,\n        n=4,\n    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n", "    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_flash_llama.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef flash_llama_handle(launcher):\n    with launcher(\"huggingface/llama-7b\", num_shard=2) as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def flash_llama(flash_llama_handle):\n    await flash_llama_handle.health(120)\n    return flash_llama_handle.client\n\n\n@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_llama(flash_llama, response_snapshot):", "@pytest.mark.private\nasync def test_flash_llama(flash_llama, response_snapshot):\n    response = await flash_llama.generate(\"Test request\", max_new_tokens=10)\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\n@pytest.mark.private", "@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_llama_all_params(flash_llama, response_snapshot):\n    response = await flash_llama.generate(\n        \"Test request\",\n        max_new_tokens=10,\n        repetition_penalty=1.2,\n        return_full_text=True,\n        stop_sequences=[\"test\"],\n        temperature=0.5,", "        stop_sequences=[\"test\"],\n        temperature=0.5,\n        top_p=0.9,\n        top_k=10,\n        truncate=5,\n        typical_p=0.9,\n        watermark=True,\n        seed=0,\n    )\n", "    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_llama_load(flash_llama, generate_load, response_snapshot):\n    responses = await generate_load(flash_llama, \"Test request\", max_new_tokens=10, n=4)", "async def test_flash_llama_load(flash_llama, generate_load, response_snapshot):\n    responses = await generate_load(flash_llama, \"Test request\", max_new_tokens=10, n=4)\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_flash_starcoder.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef flash_starcoder_handle(launcher):\n    with launcher(\"bigcode/starcoder\", num_shard=2) as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def flash_starcoder(flash_starcoder_handle):\n    await flash_starcoder_handle.health(240)\n    return flash_starcoder_handle.client\n\n\n@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_starcoder(flash_starcoder, response_snapshot):", "@pytest.mark.private\nasync def test_flash_starcoder(flash_starcoder, response_snapshot):\n    response = await flash_starcoder.generate(\"def print_hello\", max_new_tokens=10)\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\n@pytest.mark.private", "@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_starcoder_default_params(flash_starcoder, response_snapshot):\n    response = await flash_starcoder.generate(\n        \"def print_hello\", max_new_tokens=60, temperature=0.2, top_p=0.95, seed=0\n    )\n\n    assert response.details.generated_tokens == 12\n    assert response == response_snapshot\n", "    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\n@pytest.mark.private\nasync def test_flash_starcoder_load(flash_starcoder, generate_load, response_snapshot):\n    responses = await generate_load(\n        flash_starcoder, \"def print_hello\", max_new_tokens=10, n=4\n    )\n", "    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_flash_neox.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef flash_neox_handle(launcher):\n    with launcher(\"OpenAssistant/oasst-sft-1-pythia-12b\", num_shard=2) as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def flash_neox(flash_neox_handle):\n    await flash_neox_handle.health(240)\n    return flash_neox_handle.client\n\n\n@pytest.mark.asyncio\nasync def test_flash_neox(flash_neox, response_snapshot):\n    response = await flash_neox.generate(", "async def test_flash_neox(flash_neox, response_snapshot):\n    response = await flash_neox.generate(\n        \"<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\",\n        max_new_tokens=10,\n    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n", "\n\n@pytest.mark.asyncio\nasync def test_flash_neox_load(flash_neox, generate_load, response_snapshot):\n    responses = await generate_load(\n        flash_neox,\n        \"<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\",\n        max_new_tokens=10,\n        n=4,\n    )", "        n=4,\n    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_flash_santacoder.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef flash_santacoder_handle(launcher):\n    with launcher(\"bigcode/santacoder\") as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def flash_santacoder(flash_santacoder_handle):\n    await flash_santacoder_handle.health(240)\n    return flash_santacoder_handle.client\n\n\n@pytest.mark.asyncio\nasync def test_flash_santacoder(flash_santacoder, response_snapshot):\n    response = await flash_santacoder.generate(\"def print_hello\", max_new_tokens=10)", "async def test_flash_santacoder(flash_santacoder, response_snapshot):\n    response = await flash_santacoder.generate(\"def print_hello\", max_new_tokens=10)\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\nasync def test_flash_santacoder_load(\n    flash_santacoder, generate_load, response_snapshot", "async def test_flash_santacoder_load(\n    flash_santacoder, generate_load, response_snapshot\n):\n    responses = await generate_load(\n        flash_santacoder, \"def print_hello\", max_new_tokens=10, n=4\n    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n", "    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_mt0_base.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef mt0_base_handle(launcher):\n    with launcher(\"bigscience/mt0-base\") as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def mt0_base(mt0_base_handle):\n    await mt0_base_handle.health(60)\n    return mt0_base_handle.client\n\n\n@pytest.mark.asyncio\nasync def test_mt0_base(mt0_base, response_snapshot):\n    response = await mt0_base.generate(", "async def test_mt0_base(mt0_base, response_snapshot):\n    response = await mt0_base.generate(\n        \"Why is the sky blue?\",\n        max_new_tokens=10,\n        top_p=0.9,\n        seed=0,\n    )\n\n    assert response.details.generated_tokens == 5\n    assert response == response_snapshot", "    assert response.details.generated_tokens == 5\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\nasync def test_mt0_base_all_params(mt0_base, response_snapshot):\n    response = await mt0_base.generate(\n        \"Why is the sky blue?\",\n        max_new_tokens=10,\n        repetition_penalty=1.2,", "        max_new_tokens=10,\n        repetition_penalty=1.2,\n        return_full_text=True,\n        stop_sequences=[\"test\"],\n        temperature=0.5,\n        top_p=0.9,\n        top_k=10,\n        truncate=5,\n        typical_p=0.9,\n        watermark=True,", "        typical_p=0.9,\n        watermark=True,\n        seed=0,\n    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_mt0_base_load(mt0_base, generate_load, response_snapshot):\n    responses = await generate_load(\n        mt0_base,\n        \"Why is the sky blue?\",\n        max_new_tokens=10,\n        n=4,\n    )\n", "    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
{"filename": "integration-tests/models/test_bloom_560m.py", "chunked_list": ["import pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef bloom_560_handle(launcher):\n    with launcher(\"all-MiniLM-L6-v2\") as handle:\n        yield handle\n\n\n@pytest.fixture(scope=\"module\")", "\n@pytest.fixture(scope=\"module\")\nasync def bloom_560(bloom_560_handle):\n    await bloom_560_handle.health(60)\n    return bloom_560_handle.client\n\n\n@pytest.mark.asyncio\nasync def test_bloom_560m(bloom_560, response_snapshot):\n    response = await bloom_560.generate(", "async def test_bloom_560m(bloom_560, response_snapshot):\n    response = await bloom_560.generate(\n        \"Pour d\u00e9guster un ortolan, il faut tout d'abord\",\n        max_new_tokens=10,\n        top_p=0.9,\n        seed=0,\n    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot", "    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio\nasync def test_bloom_560m_all_params(bloom_560, response_snapshot):\n    response = await bloom_560.generate(\n        \"Pour d\u00e9guster un ortolan, il faut tout d'abord\",\n        max_new_tokens=10,\n        repetition_penalty=1.2,", "        max_new_tokens=10,\n        repetition_penalty=1.2,\n        return_full_text=True,\n        stop_sequences=[\"test\"],\n        temperature=0.5,\n        top_p=0.9,\n        top_k=10,\n        truncate=5,\n        typical_p=0.9,\n        watermark=True,", "        typical_p=0.9,\n        watermark=True,\n        seed=0,\n    )\n\n    assert response.details.generated_tokens == 10\n    assert response == response_snapshot\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_bloom_560m_load(bloom_560, generate_load, response_snapshot):\n    responses = await generate_load(\n        bloom_560,\n        \"Pour d\u00e9guster un ortolan, il faut tout d'abord\",\n        max_new_tokens=10,\n        n=4,\n    )\n", "    )\n\n    assert len(responses) == 4\n    assert all([r.generated_text == responses[0].generated_text for r in responses])\n\n    assert responses == response_snapshot\n"]}
