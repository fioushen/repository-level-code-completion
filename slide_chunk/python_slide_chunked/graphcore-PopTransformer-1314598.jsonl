{"filename": "poptransformer/sessions.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport popart\nfrom poptransformer.utils import REGISTRY\n\n\nclass Session:\n\n    logger = REGISTRY.get('logger')\n\n    def __init__(self, **kwargs):\n        self.logger.info(f'initializing {self.__class__.__name__}')\n        self.execution_cache_name = kwargs.get('execution_cache_name', None)\n        self.disable_outlining = kwargs.get('disable_outlining', False)\n        self.unstable_softmax = kwargs.get('unstable_softmax', False)\n        self.disable_matmul_multi_stage_reduce = kwargs.get('disable_matmul_multi_stage_reduce', False)\n        self.constant_folding_of_multiple_consumers = kwargs.get('constant_folding_of_multiple_consumers', False)\n        self.use_loop_candidate_creator = kwargs.get('use_loop_candidate_creator', True)\n        self.profile_name = kwargs.get('profile_name', None)\n        self.enable_pipeline = REGISTRY.get('enable_pipeline')\n        self.batch_per_step = REGISTRY.get('batch_per_step')\n\n    def compile(self, model):\n        self.model = model\n        self._build_data_flow()\n        self._build_device_info()\n        self._build_options()\n        self._build_inference_session()\n        self.logger.info('compiling')\n        self.session.prepareDevice()\n        self.session.weightsFromHost()\n        self._anchor_arrays = self.session.initAnchorArrays()\n        self.logger.info('compiled')\n\n    def run(self, input_dict):\n        stepio = popart.PyStepIO(input_dict, self.anchor_arrays)\n        self.session.run(stepio)\n\n    def _build_data_flow(self):\n        self.logger.info(f'building data flow, with batch_per_step: {self.batch_per_step}.')\n        self.data_flow = popart.DataFlow(self.batch_per_step, self.model.model_output)\n\n    def _build_device_info(self):\n        self.logger.info('building device info')\n        ipu_num = pow(2, math.ceil(math.log2(self.model.stage_num * self.model.num_replicas)))\n        self.device_info = popart.DeviceManager().acquireAvailableDevice(ipu_num)\n        self.logger.info(\n            f'acquired {ipu_num} ipu with {self.model.stage_num} stage, {self.model.num_replicas} replica')\n\n    def _build_options(self):\n        self.logger.info('building session options')\n        self.options = popart.SessionOptions()\n        self.options.enableOutlining = not self.disable_outlining\n        self.options.enableNonStableSoftmax = self.unstable_softmax\n        self.options.enableReplicatedGraphs = self.model.num_replicas != 1\n        self.options.replicatedGraphCount = self.model.num_replicas\n        self.options.enableConstantFoldingOfMultipleConsumers = self.constant_folding_of_multiple_consumers\n        self.options.useLoopCandidateCreator = self.use_loop_candidate_creator\n        self.options.enablePipelining = self.enable_pipeline\n\n        if self.disable_matmul_multi_stage_reduce:\n            self.options.matmulOptions = {\"enableMultiStageReduce\": \"false\"}\n        if self.model.stage_num != 1:\n            self.options.virtualGraphMode = popart.VirtualGraphMode.Manual\n            self.logger.info('setting virtual graph model to manual')\n        if self.execution_cache_name:\n            self.options.enableEngineCaching = True\n            self.options.cachePath = self.execution_cache_name\n            self.logger.info(f'saving execution cache in {self.execution_cache_name}')\n        if self.profile_name:\n            self.options.engineOptions = {\n                \"autoReport.all\": \"true\",\n                \"autoReport.directory\": f'profiles/{self.profile_name}',\n                \"autoReport.executionProfileProgramRunCount\":\"2\",\n                \"debug.allowOutOfMemory\": \"true\"\n            }\n            self.logger.info(f'saving profiles in profiles/{self.profile_name}')\n\n    def _build_inference_session(self):\n        self.logger.info('building inference session')\n        self.session = popart.InferenceSession(\n            fnModel=self.model.model_proto,\n            deviceInfo=self.device_info,\n            dataFlow=self.data_flow,\n            userOptions=self.options\n        )\n\n    @property\n    def anchor_arrays(self):\n        return self._anchor_arrays", "\n\nclass Session:\n\n    logger = REGISTRY.get('logger')\n\n    def __init__(self, **kwargs):\n        self.logger.info(f'initializing {self.__class__.__name__}')\n        self.execution_cache_name = kwargs.get('execution_cache_name', None)\n        self.disable_outlining = kwargs.get('disable_outlining', False)\n        self.unstable_softmax = kwargs.get('unstable_softmax', False)\n        self.disable_matmul_multi_stage_reduce = kwargs.get('disable_matmul_multi_stage_reduce', False)\n        self.constant_folding_of_multiple_consumers = kwargs.get('constant_folding_of_multiple_consumers', False)\n        self.use_loop_candidate_creator = kwargs.get('use_loop_candidate_creator', True)\n        self.profile_name = kwargs.get('profile_name', None)\n        self.enable_pipeline = REGISTRY.get('enable_pipeline')\n        self.batch_per_step = REGISTRY.get('batch_per_step')\n\n    def compile(self, model):\n        self.model = model\n        self._build_data_flow()\n        self._build_device_info()\n        self._build_options()\n        self._build_inference_session()\n        self.logger.info('compiling')\n        self.session.prepareDevice()\n        self.session.weightsFromHost()\n        self._anchor_arrays = self.session.initAnchorArrays()\n        self.logger.info('compiled')\n\n    def run(self, input_dict):\n        stepio = popart.PyStepIO(input_dict, self.anchor_arrays)\n        self.session.run(stepio)\n\n    def _build_data_flow(self):\n        self.logger.info(f'building data flow, with batch_per_step: {self.batch_per_step}.')\n        self.data_flow = popart.DataFlow(self.batch_per_step, self.model.model_output)\n\n    def _build_device_info(self):\n        self.logger.info('building device info')\n        ipu_num = pow(2, math.ceil(math.log2(self.model.stage_num * self.model.num_replicas)))\n        self.device_info = popart.DeviceManager().acquireAvailableDevice(ipu_num)\n        self.logger.info(\n            f'acquired {ipu_num} ipu with {self.model.stage_num} stage, {self.model.num_replicas} replica')\n\n    def _build_options(self):\n        self.logger.info('building session options')\n        self.options = popart.SessionOptions()\n        self.options.enableOutlining = not self.disable_outlining\n        self.options.enableNonStableSoftmax = self.unstable_softmax\n        self.options.enableReplicatedGraphs = self.model.num_replicas != 1\n        self.options.replicatedGraphCount = self.model.num_replicas\n        self.options.enableConstantFoldingOfMultipleConsumers = self.constant_folding_of_multiple_consumers\n        self.options.useLoopCandidateCreator = self.use_loop_candidate_creator\n        self.options.enablePipelining = self.enable_pipeline\n\n        if self.disable_matmul_multi_stage_reduce:\n            self.options.matmulOptions = {\"enableMultiStageReduce\": \"false\"}\n        if self.model.stage_num != 1:\n            self.options.virtualGraphMode = popart.VirtualGraphMode.Manual\n            self.logger.info('setting virtual graph model to manual')\n        if self.execution_cache_name:\n            self.options.enableEngineCaching = True\n            self.options.cachePath = self.execution_cache_name\n            self.logger.info(f'saving execution cache in {self.execution_cache_name}')\n        if self.profile_name:\n            self.options.engineOptions = {\n                \"autoReport.all\": \"true\",\n                \"autoReport.directory\": f'profiles/{self.profile_name}',\n                \"autoReport.executionProfileProgramRunCount\":\"2\",\n                \"debug.allowOutOfMemory\": \"true\"\n            }\n            self.logger.info(f'saving profiles in profiles/{self.profile_name}')\n\n    def _build_inference_session(self):\n        self.logger.info('building inference session')\n        self.session = popart.InferenceSession(\n            fnModel=self.model.model_proto,\n            deviceInfo=self.device_info,\n            dataFlow=self.data_flow,\n            userOptions=self.options\n        )\n\n    @property\n    def anchor_arrays(self):\n        return self._anchor_arrays", ""]}
{"filename": "poptransformer/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"]}
{"filename": "poptransformer/utils/options_scope.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .registry import REGISTRY\n\n\nclass OptionsScope:\n\n    def __init__(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n        self.amp = amp\n        self.partialtype = partialtype\n        self.serial_factor = serial_factor\n        self.serial_mode = serial_mode\n\n        self.default_amp = None\n        self.default_partialtype = None\n        self.default_serial_factor = None\n        self.default_serial_mode = 'none'\n\n    def __enter__(self):\n        if self.amp is not None:\n            self.default_amp = REGISTRY.get('amp')\n            REGISTRY.update('amp', self.amp)\n            REGISTRY.get('logger').debug(f'using amp: {self.amp}')\n        if self.partialtype is not None:\n            self.default_partialtype = REGISTRY.get('partialtype')\n            REGISTRY.update('partialtype', self.partialtype)\n            REGISTRY.get('logger').debug(f'using partialtype: {self.partialtype}')\n        if self.serial_factor is not None:\n            self.default_serial_factor = REGISTRY.get('serial_factor')\n            self.default_serial_mode = REGISTRY.get('serial_mode')\n            REGISTRY.update('serial_factor', self.serial_factor)\n            REGISTRY.update('serial_mode', self.serial_mode)\n            REGISTRY.get('logger').debug(f'using serialize: {self.serial_factor}, mode: {self.serial_mode}')\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.default_amp != self.amp:\n            REGISTRY.update('amp', self.default_amp)\n            REGISTRY.get('logger').debug(f'exit and using default_amp: {self.default_amp}')\n        if self.default_partialtype != self.partialtype:\n            REGISTRY.update('partialtype', self.default_partialtype)\n            REGISTRY.get('logger').debug(f'exit and using default_partialtype: {self.default_partialtype}')\n        if self.default_serial_factor != self.serial_factor:\n            REGISTRY.update('serial_factor', self.default_serial_factor)\n            REGISTRY.update('serial_mode', self.default_serial_mode)\n            REGISTRY.get('logger').debug(f'exit and using default_serial_factor: {self.default_serial_factor}')\n            REGISTRY.get('logger').debug(f'exit and using serial_mode: {self.serial_mode}')\n        return False", "class OptionsScope:\n\n    def __init__(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n        self.amp = amp\n        self.partialtype = partialtype\n        self.serial_factor = serial_factor\n        self.serial_mode = serial_mode\n\n        self.default_amp = None\n        self.default_partialtype = None\n        self.default_serial_factor = None\n        self.default_serial_mode = 'none'\n\n    def __enter__(self):\n        if self.amp is not None:\n            self.default_amp = REGISTRY.get('amp')\n            REGISTRY.update('amp', self.amp)\n            REGISTRY.get('logger').debug(f'using amp: {self.amp}')\n        if self.partialtype is not None:\n            self.default_partialtype = REGISTRY.get('partialtype')\n            REGISTRY.update('partialtype', self.partialtype)\n            REGISTRY.get('logger').debug(f'using partialtype: {self.partialtype}')\n        if self.serial_factor is not None:\n            self.default_serial_factor = REGISTRY.get('serial_factor')\n            self.default_serial_mode = REGISTRY.get('serial_mode')\n            REGISTRY.update('serial_factor', self.serial_factor)\n            REGISTRY.update('serial_mode', self.serial_mode)\n            REGISTRY.get('logger').debug(f'using serialize: {self.serial_factor}, mode: {self.serial_mode}')\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.default_amp != self.amp:\n            REGISTRY.update('amp', self.default_amp)\n            REGISTRY.get('logger').debug(f'exit and using default_amp: {self.default_amp}')\n        if self.default_partialtype != self.partialtype:\n            REGISTRY.update('partialtype', self.default_partialtype)\n            REGISTRY.get('logger').debug(f'exit and using default_partialtype: {self.default_partialtype}')\n        if self.default_serial_factor != self.serial_factor:\n            REGISTRY.update('serial_factor', self.default_serial_factor)\n            REGISTRY.update('serial_mode', self.default_serial_mode)\n            REGISTRY.get('logger').debug(f'exit and using default_serial_factor: {self.default_serial_factor}')\n            REGISTRY.get('logger').debug(f'exit and using serial_mode: {self.serial_mode}')\n        return False", ""]}
{"filename": "poptransformer/utils/registry.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport popart\n\n\nclass Registry:\n\n    def __init__(self):\n        self._registry = {}\n\n    def register(self, key, value):\n        if key in self._registry:\n            raise KeyError(f\"Duplicate key: {key}\")\n        self._registry[key] = value\n\n    def update(self, key, value):\n        try:\n            self._registry[key] = value\n        except KeyError:\n            raise KeyError(f\"key: {key} not found, please register first\")\n\n    def get(self, key):\n        try:\n            return self._registry[key]\n        except KeyError:\n            raise KeyError(f'key: {key} not found')", "class Registry:\n\n    def __init__(self):\n        self._registry = {}\n\n    def register(self, key, value):\n        if key in self._registry:\n            raise KeyError(f\"Duplicate key: {key}\")\n        self._registry[key] = value\n\n    def update(self, key, value):\n        try:\n            self._registry[key] = value\n        except KeyError:\n            raise KeyError(f\"key: {key} not found, please register first\")\n\n    def get(self, key):\n        try:\n            return self._registry[key]\n        except KeyError:\n            raise KeyError(f'key: {key} not found')", "\n\nREGISTRY = Registry()\nREGISTRY.register('main_graph', popart.Builder(opsets={'ai.onnx': 10, 'ai.graphcore': 1}))\nREGISTRY.register('serial_factor', None)\nREGISTRY.register('serial_mode', None)\nREGISTRY.register('partialtype', None)\nREGISTRY.register('amp', None)\nREGISTRY.register('state_dict', {})\n# main graph is to be built for certain, we build it here, move to other place if there be more comments", "REGISTRY.register('state_dict', {})\n# main graph is to be built for certain, we build it here, move to other place if there be more comments\n"]}
{"filename": "poptransformer/utils/tensor_type.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\n\nclass TensorType:\n\n    all_precision = ['fp32', 'fp16', 'int4', 'fp8', 'fp8_weight']\n    np_map = {'fp32': np.float32, 'fp16': np.float16, 'int4': np.float16, 'fp8': np.float16, 'fp8_weight': np.float16}\n    popart_map = {'fp32': 'FLOAT', 'fp16': 'FLOAT16', 'int4': 'FLOAT16', 'fp8': 'FLOAT16', 'fp8_weight': 'FLOAT16'}\n\n    def __init__(self, precision):\n        assert precision in self.all_precision\n        self.precision = precision\n\n    @property\n    def np_float_type(self):\n        return self.np_map[self.precision]\n\n    @property\n    def popart_float_type(self):\n        return self.popart_map[self.precision]", "class TensorType:\n\n    all_precision = ['fp32', 'fp16', 'int4', 'fp8', 'fp8_weight']\n    np_map = {'fp32': np.float32, 'fp16': np.float16, 'int4': np.float16, 'fp8': np.float16, 'fp8_weight': np.float16}\n    popart_map = {'fp32': 'FLOAT', 'fp16': 'FLOAT16', 'int4': 'FLOAT16', 'fp8': 'FLOAT16', 'fp8_weight': 'FLOAT16'}\n\n    def __init__(self, precision):\n        assert precision in self.all_precision\n        self.precision = precision\n\n    @property\n    def np_float_type(self):\n        return self.np_map[self.precision]\n\n    @property\n    def popart_float_type(self):\n        return self.popart_map[self.precision]", ""]}
{"filename": "poptransformer/utils/prepare.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport popart\nfrom hydra.utils import instantiate\nfrom .registry import REGISTRY\nfrom .tensor_type import TensorType", "from .registry import REGISTRY\nfrom .tensor_type import TensorType\n\n\ndef register_global_args(config):\n    global_args = instantiate(config.global_args)\n    for key, value in global_args.items():\n        REGISTRY.register(key, value)\n    tensor_type = TensorType(global_args.precision)\n    REGISTRY.register('tensor_type', tensor_type)", "\ndef register_config_logger(config):\n    popart.getLogger().setLevel(config.popart_log_level.upper())\n    logger = logging.getLogger('poptransformer')\n    logger.setLevel(config.log_level.upper())\n    REGISTRY.register('logger', logger)\n\ndef prepare_model_session(config):\n    register_global_args(config)\n    register_config_logger(config)\n    model = instantiate(config.model)\n    session = instantiate(config.session)\n    model.build_graph()\n    model.graph.saveInitializersExternally(\n        model.initializers,\n        'saved_initializer.onnx'\n    )\n    session.compile(model)\n    return session, model", ""]}
{"filename": "poptransformer/utils/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .registry import REGISTRY\nfrom .device_scope import DeviceScope\nfrom .options_scope import OptionsScope\nfrom .tensor_type import TensorType\nfrom .prepare import prepare_model_session", "from .tensor_type import TensorType\nfrom .prepare import prepare_model_session\nfrom .param_handler.param_handler import ParamHandler\nfrom .param_handler.tensor_parallel_strategy import shard, shard_fused_qkv, build_sharded_weight, repeat\n"]}
{"filename": "poptransformer/utils/device_scope.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom contextlib import ExitStack\n\n\nclass DeviceScope:\n    def __init__(self, graph, virtual_graph_id=None, pipeline_stage_id=None, enable_pipeline=False, outline_attr=None):\n        self.graph = graph\n        self.virtual_graph_id = virtual_graph_id\n        self.pipeline_stage_id = pipeline_stage_id\n        self.enable_pipeline = enable_pipeline\n        self.outline_attr = outline_attr\n\n    def __enter__(self):\n        self.stack = ExitStack()\n        if self.virtual_graph_id is not None:\n            self.stack.enter_context(self.graph.virtualGraph(self.virtual_graph_id))\n        if self.pipeline_stage_id is not None and self.enable_pipeline:\n            self.stack.enter_context(self.graph.pipelineStage(self.pipeline_stage_id))\n        if self.outline_attr is not None:\n            self.stack.enter_context(self.graph.outlineAttributes(self.outline_attr))\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.stack.close()\n        return False", "class DeviceScope:\n    def __init__(self, graph, virtual_graph_id=None, pipeline_stage_id=None, enable_pipeline=False, outline_attr=None):\n        self.graph = graph\n        self.virtual_graph_id = virtual_graph_id\n        self.pipeline_stage_id = pipeline_stage_id\n        self.enable_pipeline = enable_pipeline\n        self.outline_attr = outline_attr\n\n    def __enter__(self):\n        self.stack = ExitStack()\n        if self.virtual_graph_id is not None:\n            self.stack.enter_context(self.graph.virtualGraph(self.virtual_graph_id))\n        if self.pipeline_stage_id is not None and self.enable_pipeline:\n            self.stack.enter_context(self.graph.pipelineStage(self.pipeline_stage_id))\n        if self.outline_attr is not None:\n            self.stack.enter_context(self.graph.outlineAttributes(self.outline_attr))\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.stack.close()\n        return False", ""]}
{"filename": "poptransformer/utils/param_handler/param_handler.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ..registry import REGISTRY\nfrom .precision_strategy import PrecisionStrategyMap\nfrom .tensor_parallel_strategy import TPStragetgyMap\n\n\nclass ParamHandler:\n\n    def __init__(self, host_layer, tp_strategy_name='identity', **vs_setting):\n        self.host_layer = host_layer\n        self.tp_strategy = TPStragetgyMap[tp_strategy_name]\n        self.precision_strategy = PrecisionStrategyMap[self.precision]\n        self.vs_setting = vs_setting\n\n    @property\n    def num_replicas(self):\n        return REGISTRY.get('num_replicas')\n\n    @property\n    def precision(self):\n        return REGISTRY.get('precision')\n\n    def process_linear_weight(self, weight_np, weight_key):\n        weight_fn_tp = self.tp_strategy['weight_fn']\n        weight_fn_precision = self.precision_strategy['weight_fn']\n\n        weight_np = weight_fn_tp(weight_np, self.num_replicas, self.tp_strategy['weight_axis'])\n        weight_np = weight_fn_precision(\n            host_layer=self.host_layer,\n            weight_np=weight_np,\n            weight_key=weight_key,\n            weight_fn_tp=weight_fn_tp,\n            num_replicas=self.num_replicas,\n            weight_axis=self.tp_strategy['weight_axis'],\n            **self.vs_setting\n        )\n        return weight_np\n\n    def process_linear_bias(self, bias):\n        bias_fn_tp = self.tp_strategy['bias_fn']\n\n        bias = bias_fn_tp(bias, self.num_replicas, 0)\n        return bias\n\n    def matmul(self, graph, x, weight):\n        x, weight = self._prepare_matmul(graph, x, weight)\n        y = self._matmul(graph, x, weight)\n        y = self._post_process_matmul(graph, y)\n        return y\n\n    def _prepare_matmul(self, graph, x, weight):\n        prepare_fn_tp = self.tp_strategy['prepare_matmul']\n        prepare_fn_precision = self.precision_strategy['prepare_matmul']\n\n        x, weight = prepare_fn_tp(graph, x, weight)\n        x, weight = prepare_fn_precision(graph, x, weight)\n        return x, weight\n\n    def _post_process_matmul(self, graph, y):\n        prepare_fn_tp = self.tp_strategy['post_process_matmul']\n        prepare_fn_precision = self.precision_strategy['post_process_matmul']\n\n        y = prepare_fn_tp(graph, y)\n        y = prepare_fn_precision(graph, y)\n        return y\n\n    def _matmul(self, graph, x, weight):\n        matmul_fn = self.precision_strategy['matmul_fn']\n\n        y = matmul_fn(graph, x, weight)\n        return y", "\n\nclass ParamHandler:\n\n    def __init__(self, host_layer, tp_strategy_name='identity', **vs_setting):\n        self.host_layer = host_layer\n        self.tp_strategy = TPStragetgyMap[tp_strategy_name]\n        self.precision_strategy = PrecisionStrategyMap[self.precision]\n        self.vs_setting = vs_setting\n\n    @property\n    def num_replicas(self):\n        return REGISTRY.get('num_replicas')\n\n    @property\n    def precision(self):\n        return REGISTRY.get('precision')\n\n    def process_linear_weight(self, weight_np, weight_key):\n        weight_fn_tp = self.tp_strategy['weight_fn']\n        weight_fn_precision = self.precision_strategy['weight_fn']\n\n        weight_np = weight_fn_tp(weight_np, self.num_replicas, self.tp_strategy['weight_axis'])\n        weight_np = weight_fn_precision(\n            host_layer=self.host_layer,\n            weight_np=weight_np,\n            weight_key=weight_key,\n            weight_fn_tp=weight_fn_tp,\n            num_replicas=self.num_replicas,\n            weight_axis=self.tp_strategy['weight_axis'],\n            **self.vs_setting\n        )\n        return weight_np\n\n    def process_linear_bias(self, bias):\n        bias_fn_tp = self.tp_strategy['bias_fn']\n\n        bias = bias_fn_tp(bias, self.num_replicas, 0)\n        return bias\n\n    def matmul(self, graph, x, weight):\n        x, weight = self._prepare_matmul(graph, x, weight)\n        y = self._matmul(graph, x, weight)\n        y = self._post_process_matmul(graph, y)\n        return y\n\n    def _prepare_matmul(self, graph, x, weight):\n        prepare_fn_tp = self.tp_strategy['prepare_matmul']\n        prepare_fn_precision = self.precision_strategy['prepare_matmul']\n\n        x, weight = prepare_fn_tp(graph, x, weight)\n        x, weight = prepare_fn_precision(graph, x, weight)\n        return x, weight\n\n    def _post_process_matmul(self, graph, y):\n        prepare_fn_tp = self.tp_strategy['post_process_matmul']\n        prepare_fn_precision = self.precision_strategy['post_process_matmul']\n\n        y = prepare_fn_tp(graph, y)\n        y = prepare_fn_precision(graph, y)\n        return y\n\n    def _matmul(self, graph, x, weight):\n        matmul_fn = self.precision_strategy['matmul_fn']\n\n        y = matmul_fn(graph, x, weight)\n        return y", ""]}
{"filename": "poptransformer/utils/param_handler/precision_strategy.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom poprt.utils import convert_float_to_uint8\nfrom poptransformer import ops\nfrom ..registry import REGISTRY\nfrom .tensor_parallel_strategy import shard, repeat", "from ..registry import REGISTRY\nfrom .tensor_parallel_strategy import shard, repeat\n\n\ndef weight_fn_identity(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n    return weight_np\n\ndef weight_fn_int4(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n    if weight_np.dtype == np.int8:  # Embedding/LM are FP16 precision\n        if len(weight_np.shape) == 3:   # TP:[num_replicas, shape1, shape2]\n            weight_np = weight_np.transpose(0,2,1)\n        elif len(weight_np.shape) == 2:\n            weight_np = weight_np.transpose(1, 0)\n        else:\n            raise ValueError(f\"weight_np can only have rank 2 or 3, but got {len(weight_np.shape)}.\")\n        scale_key = weight_key + '_scale'\n        scale_np = host_layer.get_param_from_state_dict(scale_key, shape_list=(weight_np.shape[1],))\n        if num_replicas > 1 and len(weight_np.shape)==3:\n            if weight_axis == 0:\n                scale_np = repeat(scale_np, num_replicas, 0)\n            elif weight_axis in [-1, 1]:\n                scale_np = shard(scale_np, num_replicas, 0)\n            else:\n                raise ValueError(f\"weight_axis can only be 0,1,-1, but got {weight_axis}.\")\n        host_layer.add_initialized_input_tensor(scale_np, scale_key, **vs_setting)\n    return weight_np", "\ndef weight_fn_fp8(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n    scale_key = weight_key + '_scale'\n    scale_np = np.array([-1]).astype(np.int32)\n    if num_replicas > 1:\n        scale_np = np.repeat(np.expand_dims(scale_np, 0), num_replicas, axis=0)\n    host_layer.add_initialized_input_tensor(scale_np, scale_key, **vs_setting)\n    weight_np = convert_float_to_uint8(weight_np.astype(np.float32), 'F143', -1)\n    return weight_np\n\ndef prepare_float32_16_matmul(graph, x, weight):\n    return x, weight", "\ndef prepare_float32_16_matmul(graph, x, weight):\n    return x, weight\n\ndef prepare_int4_matmul(graph, x, weight):\n    scale = weight + '_scale'\n    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n        weight = ops.int4_to_half(graph, weight, scale, x, axis=1)\n    return x, weight\n\ndef prepare_fp8_matmul(graph, x, weight):\n    scale = weight + '_scale'\n    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n        x = ops.half_to_uint8(graph, x, scale)\n    return x, weight", "\ndef prepare_fp8_matmul(graph, x, weight):\n    scale = weight + '_scale'\n    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n        x = ops.half_to_uint8(graph, x, scale)\n    return x, weight\n\ndef prepare_fp8_weight_matmul(graph, x, weight):\n    return x, weight\n\ndef matmul_identity(graph, x, weight):\n    return ops.matmul(graph, x, weight)", "\ndef matmul_identity(graph, x, weight):\n    return ops.matmul(graph, x, weight)\n\ndef matmul_int4(graph, x, weight):\n    return matmul_identity(graph, x, weight)\n\ndef matmul_fp8(graph, x, weight):\n    scale = weight + '_scale'\n    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n        return ops.fp8_matmul(graph, x, weight, scale, scale, 'F143', 'F143')\n    return ops.matmul(graph, x, weight)", "\ndef post_process_float32_16_matmul(graph, y):\n    return y\n\ndef post_process_int4_matmul(graph, y):\n    return y\n\ndef post_process_fp8_matmul(graph, y):\n    return y\n", "\n\nPrecisionStrategyMap = {\n    'fp16': {\n        'weight_fn': weight_fn_identity,\n        'prepare_matmul': prepare_float32_16_matmul,\n        'matmul_fn': matmul_identity,\n        'post_process_matmul': post_process_float32_16_matmul},\n    'fp32': {\n        'weight_fn': weight_fn_identity,", "    'fp32': {\n        'weight_fn': weight_fn_identity,\n        'prepare_matmul': prepare_float32_16_matmul,\n        'matmul_fn': matmul_identity,\n        'post_process_matmul': post_process_float32_16_matmul},\n    'int4': {\n        'weight_fn': weight_fn_int4,\n        'prepare_matmul': prepare_int4_matmul,\n        'matmul_fn': matmul_int4,\n        'post_process_matmul': post_process_int4_matmul},", "        'matmul_fn': matmul_int4,\n        'post_process_matmul': post_process_int4_matmul},\n    'fp8': {\n        'weight_fn': weight_fn_fp8,\n        'prepare_matmul': prepare_fp8_matmul,\n        'matmul_fn': matmul_fp8,\n        'post_process_matmul': post_process_fp8_matmul},\n    'fp8_weight': {\n        'weight_fn': weight_fn_fp8,\n        'prepare_matmul': prepare_fp8_weight_matmul,", "        'weight_fn': weight_fn_fp8,\n        'prepare_matmul': prepare_fp8_weight_matmul,\n        'matmul_fn': matmul_fp8,\n        'post_process_matmul': post_process_fp8_matmul}\n}\n"]}
{"filename": "poptransformer/utils/param_handler/tensor_parallel_strategy.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom math import ceil\nimport numpy as np\n\nfrom poptransformer import ops\nfrom poptransformer.utils.registry import REGISTRY", "from poptransformer import ops\nfrom poptransformer.utils.registry import REGISTRY\n\n\ndef shard(param: np.ndarray, num_replicas: int, axis: int) -> np.array:\n    \"\"\"Shard array along a given axis\"\"\"\n    if axis < 0:\n        axis = len(param.shape) + axis\n    if param.shape[axis] % num_replicas != 0:\n        pads = [(0, 0) for i in range(len(param.shape))]\n        pads[axis] = (0, num_replicas - param.shape[axis] % num_replicas)\n        param = np.pad(param, pads)\n\n    return np.ascontiguousarray(np.concatenate(np.split(param[np.newaxis, ...], num_replicas, axis=axis + 1)))", "\ndef repeat(param: np.ndarray, num_replicas: int, axis: int = 0) -> np.array:\n    \"\"\"Repeat array along new axis inserted at position `axis`\"\"\"\n    return np.repeat(np.expand_dims(param, axis), num_replicas, axis=axis)\n\ndef build_sharded_weight(param, num_replicas, vocab_size, embedding_size):\n    shard_size = ceil(vocab_size / num_replicas)\n    n_pad = shard_size * num_replicas - param.shape[0]\n    param = np.pad(param, ((0, n_pad), (0, 0)))\n    param = param.reshape(num_replicas, shard_size, embedding_size)\n    param = np.pad(param, ((0, 0), (0, 1), (0, 0)))\n    return param", "\ndef shard_fused_qkv(param, num_replicas, axis):\n    q_split, k_split, v_split = [\n        np.split(part, num_replicas, axis=axis) for part in np.split(param, 3, axis=axis)\n    ]\n    sharded_param = np.concatenate(\n        [np.concatenate([q_split[i], k_split[i], v_split[i]], axis=axis)[np.newaxis, ...]\n            for i in range(num_replicas)]\n    )\n    sharded_param = np.ascontiguousarray(sharded_param)\n    return sharded_param", "\ndef shard_multi_query_qkv(param, n_shards, axis):\n    q_size = REGISTRY.get(\"query_size\")\n    kv_size = REGISTRY.get(\"key_value_size\")\n    q_split, k_split, v_split = [\n        np.split(part, n_shards, axis=axis) for part in np.split(param, [q_size, q_size+kv_size], axis=axis)\n    ]\n    sharded_param = np.concatenate(\n        [np.concatenate([q_split[i], k_split[i], v_split[i]], axis=axis)[np.newaxis, ...]\n            for i in range(n_shards)]\n    )\n    sharded_param = np.ascontiguousarray(sharded_param)\n    return sharded_param", "\ndef identity(param, num_replicas, axis):\n    return param\n\ndef identity_prepare_matmul(graph, x, weight):\n    return x, weight\n\ndef identity_post_process_matmul(graph, y):\n    return y\n", "\n\nTPStragetgyMap = {\n    'start': {\n        'weight_fn': shard,\n        'weight_axis': -1,\n        'bias_fn': shard,\n        'prepare_matmul': identity_prepare_matmul,\n        'post_process_matmul': identity_post_process_matmul},\n    'end': {", "        'post_process_matmul': identity_post_process_matmul},\n    'end': {\n        'weight_fn': shard,\n        'weight_axis': 0,\n        'bias_fn': repeat,\n        'prepare_matmul': identity_prepare_matmul,\n        'post_process_matmul': ops.replicated_all_reduce},\n    'fused_qkv': {\n        'weight_fn': shard_fused_qkv,\n        'weight_axis': -1,", "        'weight_fn': shard_fused_qkv,\n        'weight_axis': -1,\n        'bias_fn': shard_fused_qkv,\n        'prepare_matmul': identity_prepare_matmul,\n        'post_process_matmul': identity_post_process_matmul},\n    'multi_query_qkv': {\n        'weight_fn': shard_multi_query_qkv,\n        'weight_axis': -1,\n        'bias_fn': shard_multi_query_qkv,\n        'prepare_matmul': identity_prepare_matmul,", "        'bias_fn': shard_multi_query_qkv,\n        'prepare_matmul': identity_prepare_matmul,\n        'post_process_matmul': identity_post_process_matmul},\n    'identity': {\n        'weight_fn': identity,\n        'weight_axis': 0,\n        'bias_fn': identity,\n        'prepare_matmul': identity_prepare_matmul,\n        'post_process_matmul': identity_post_process_matmul},\n}", "        'post_process_matmul': identity_post_process_matmul},\n}\n"]}
{"filename": "poptransformer/utils/param_handler/__init__.py", "chunked_list": [""]}
{"filename": "poptransformer/ops/customized.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport ctypes\nimport numpy as np\nimport poprt\nso_path = '../../custom_ops.so'\nctypes.cdll.LoadLibrary(so_path)", "so_path = '../../custom_ops.so'\nctypes.cdll.LoadLibrary(so_path)\n\n\ndef kv_cache(graph, step, updater, max_len, sequence_axis=1, step_len=1):\n    output = graph.customOp(\n        inputs=[step, updater],\n        opName=\"KVCache\",\n        domain=\"ai.graphcore\",\n        opVersion=1,\n        numOutputs=1,\n        attributes={\n            'max_len': max_len,\n            'step_len':  step_len,\n            'sequence_axis': sequence_axis,\n            'fp8': False,\n        },\n    )[0]\n    return output", "\n\ndef remap_tensor(graph, x, fwd_after_matmul=0, name='remap'):\n    output = graph.customOp(\n        opName='Remap',\n        domain='ai.graphcore',\n        opVersion=1,\n        inputs=[x],\n        attributes={\n            'fwd_grain_size': 8,\n            'bwd_grain_size': 8,\n            'fwd_clone_layout': 0,\n            'bwd_clone_layout': 0,\n            'fwd_after_matmul': fwd_after_matmul,\n            'bwd_after_matmul': 0,\n            'debug_str': name\n        }\n    )[0]\n    return output", "\n\ndef softmax_ce(graph, x, dim, stable_mode=True):\n    output = graph.customOp(\n        opName=\"SoftmaxV3\",\n        domain=\"ai.graphcore\",\n        opVersion=1,\n        inputs=[x],\n        attributes={'axis': dim, 'stable_mode': stable_mode}\n    )[0]\n    return output", "\n\n# TODO: NormCE -> FastNorm\ndef layer_norm_ce(graph, x, gamma, beta, eps, batch_size, sequence_length, input_size):\n    return _layer_norm_ce(graph, x, gamma, beta, batch_size, sequence_length, input_size, eps)\n\n\ndef _layer_norm_ce(\n        graph, x, weight, bias, batch_size, sequence_length, input_size, epsilon,\n        fwd_grain_size=-1, bwd_grain_size=-1, fwd_after_matmul=0, bwd_after_matmul=0,\n        stable_algo=False, name='layer_norm_ce'):\n    with graph.nameScope(name):\n\n        data_size = np.prod([batch_size, sequence_length, input_size])\n        num_groups = int(data_size // input_size)\n        assert input_size & 1 == 0\n        assert input_size < 8192\n        fwd_grain_size = input_size if fwd_grain_size != -1 else fwd_grain_size\n        bwd_grain_size = input_size if bwd_grain_size != -1 else bwd_grain_size\n\n        output = graph.customOp(\n            opName=\"NormCE\",\n            domain=\"ai.graphcore\",\n            opVersion=1,\n            inputs=[x, weight, bias],\n            attributes={\n                'fwd_after_matmul': 1 if fwd_after_matmul else 0,\n                'bwd_after_matmul': 1 if bwd_after_matmul else 0,\n                'fwd_grain_size': fwd_grain_size,\n                'bwd_grain_size': bwd_grain_size,\n                'epsilon': epsilon,\n                'num_groups': num_groups,\n                'stable_algo': stable_algo,\n                'debug_str': graph.getNameScope() + 'op'\n            }\n        )[0]\n    return output", "\n\ndef replicated_allgather(graph, matmul_output):\n    output = graph.customOp(\n        opName=\"ReplicatedAllGather\",\n        opVersion=1,\n        domain=\"ai.graphcore\",\n        inputs=[matmul_output],\n        attributes={})[0]  # shape is 1 dim\n    return output", "\n\ndef int4_to_half(graph, x, scale, ref, axis=1, remap=1):\n    x = graph.customOp(\n            inputs=[x, scale, ref],\n            opName=\"Int4ToHalf\",\n            domain=\"ai.graphcore\",\n            opVersion=1,\n            attributes={\n                \"axis\": axis,\n                \"remap\": remap},\n        )[0]\n    return x", "\ndef half_to_uint8(graph, x, fp8_scale, fp8_format='F143'):\n    output = graph.customOp(\n        opName=\"CastToUint8WithoutShapeInfer\",\n        opVersion=1,\n        domain=\"ai.graphcore\",\n        inputs=[x, fp8_scale],\n        attributes={\"fp8Format\": fp8_format},\n    )[0]\n    return output", "\ndef fp8_matmul(graph, input_l, input_r, fp8_scale_lhs, fp8_scale_rhs, fp8_format_lhs='F143', fp8_format_rhs='F143'):\n    output = graph.customOp(\n        opName=\"FP8MatMulWithoutShapeInfer\",\n        opVersion=1,\n        domain=\"ai.graphcore\",\n        inputs=[input_l, input_r, fp8_scale_lhs, fp8_scale_rhs],\n        attributes={\n            \"fp8FormatLHS\": fp8_format_lhs,\n            \"fp8FormatRHS\": fp8_format_rhs,\n        },\n    )[0]\n    return output", ""]}
{"filename": "poptransformer/ops/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .popart import *\nfrom .customized import *\n"]}
{"filename": "poptransformer/ops/popart.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom poptransformer.ops.customized import int4_to_half\nfrom poptransformer.utils.registry import REGISTRY\n\n\ndef pad(graph, x, pad_list, mode='constant', constant_value=0.0):\n    return graph.aiOnnx.pad([x], pads=pad_list, mode=mode, value=constant_value)", "\n\ndef pad(graph, x, pad_list, mode='constant', constant_value=0.0):\n    return graph.aiOnnx.pad([x], pads=pad_list, mode=mode, value=constant_value)\n\ndef randomnormal(graph, shape, dtype, scale):\n    return graph.aiOnnx.randomnormal(shape=shape, dtype=dtype, scale=scale)\n\ndef random_sample(graph, logits, k, dim=1):\n    _, next_token = topk(graph, logits, axis=dim, k=k)\n    rand = randomnormal(graph, [1, k], 1, 1)\n    rand = argmax(graph, rand, 1)\n    return dynamic_slice(graph, next_token, rand, axes=[dim], sizes=[1])", "def random_sample(graph, logits, k, dim=1):\n    _, next_token = topk(graph, logits, axis=dim, k=k)\n    rand = randomnormal(graph, [1, k], 1, 1)\n    rand = argmax(graph, rand, 1)\n    return dynamic_slice(graph, next_token, rand, axes=[dim], sizes=[1])\n\ndef add(graph, x, y):\n    return graph.aiOnnx.add([x, y])\n\ndef less(graph, x, y):\n    return graph.aiOnnx.less([x, y])", "\ndef less(graph, x, y):\n    return graph.aiOnnx.less([x, y])\n\ndef greater(graph, x, y):\n    return graph.aiOnnx.greater([x, y])\n\ndef sum_op(graph, inputs):\n    return graph.aiOnnx.sum(inputs)\n\ndef reduce_sum(graph, x, axes, keepdims):\n    return graph.aiOnnx.reducesum([x], axes=axes, keepdims=keepdims)", "\ndef reduce_sum(graph, x, axes, keepdims):\n    return graph.aiOnnx.reducesum([x], axes=axes, keepdims=keepdims)\n\ndef bitwiseor(graph, x, y):\n    return graph.aiGraphcore.bitwiseor([x, y])\n\ndef sub(graph, x, y):\n    return graph.aiOnnx.sub([x, y])\n\ndef div(graph, x, y):\n    return graph.aiOnnx.div([x, y])", "\ndef div(graph, x, y):\n    return graph.aiOnnx.div([x, y])\n\ndef matmul(graph, x, y):\n    o = graph.aiOnnx.matmul([x, y])\n    amp = REGISTRY.get('amp')\n    partialtype = REGISTRY.get('partialtype')\n    serial_factor = REGISTRY.get('serial_factor')\n    serial_mode = REGISTRY.get('serial_mode')\n    if amp is not None:\n        graph.setAvailableMemoryProportion(o, amp)\n    if partialtype is not None:\n        graph.setPartialsType(o, partialtype)\n    if serial_factor is not None:\n        graph.setSerializeMatMul({o}, mode=serial_mode, factor=serial_factor)\n    return o", "\ndef constant(graph, tensor, tensor_name='constant'):\n    return graph.aiOnnx.constant(tensor, debugContext=tensor_name)\n\ndef _group_norm(graph, x, gamma, beta, eps):\n    return graph.aiGraphcore.groupnormalization([x, gamma, beta], 1, eps)[0]\n\ndef group_norm(graph, x, gamma, beta, eps, batch_size, sequence_length, input_size):\n    x = reshape(graph, x, [batch_size * sequence_length, input_size])\n    x = _group_norm(graph, x, gamma, beta, eps)\n    x = reshape(graph, x, [batch_size, sequence_length, input_size])\n    return x", "\ndef gather(graph, weight, input_ids):\n    return graph.aiOnnx.gather([weight, input_ids])\n\ndef grouped_gather(graph, weight, indexs, axis=0, group_size=1):\n    return graph.aiGraphcore.groupedgather([weight, indexs], axis=axis, group_size=group_size)\n\ndef mul(graph, x, y):\n    return graph.aiOnnx.mul([x, y])\n\ndef tanh(graph, x):\n    return graph.aiOnnx.tanh([x])", "\ndef tanh(graph, x):\n    return graph.aiOnnx.tanh([x])\n\ndef split(graph, x, num_outputs, axis, splits, name='split'):\n    return graph.aiOnnx.split([x], num_outputs=num_outputs, axis=axis, split=splits, debugContext=name)\n\ndef transpose(graph, x, perm):\n    return graph.aiOnnx.transpose([x], perm=perm)\n\ndef reshape(graph, x, shape):\n    shape = constant(graph, np.asarray(shape, dtype=np.int32))\n    return graph.aiOnnx.reshape([x, shape])", "\ndef reshape(graph, x, shape):\n    shape = constant(graph, np.asarray(shape, dtype=np.int32))\n    return graph.aiOnnx.reshape([x, shape])\n\ndef static_slice(graph, x, starts, ends, axes):\n    return graph.aiGraphcore.slice([x], starts=starts, ends=ends, axes=axes)\n\ndef dynamic_slice(graph, x, index, axes, sizes):\n    return graph.aiGraphcore.dynamicslice([x, index], axes=axes, sizes=sizes)", "def dynamic_slice(graph, x, index, axes, sizes):\n    return graph.aiGraphcore.dynamicslice([x, index], axes=axes, sizes=sizes)\n\ndef dynamic_update(graph, x, index, slice_tensor, axes, sizes):\n    return graph.aiGraphcore.dynamicupdate([x, index, slice_tensor], axes=axes, sizes=sizes)\n\ndef cast(graph, x, popart_float_type):\n    return graph.aiOnnx.cast([x], popart_float_type)\n\ndef unsqueeze(graph, x, dim_list):\n    return graph.aiOnnx.unsqueeze([x], dim_list)", "\ndef unsqueeze(graph, x, dim_list):\n    return graph.aiOnnx.unsqueeze([x], dim_list)\n\ndef squeeze(graph, x, dim_list):\n    return graph.aiOnnx.squeeze([x], dim_list)\n\ndef equal(graph, x, y):\n    return graph.aiOnnx.equal([x, y])\n\ndef where(graph, cond, x1, x2):\n    return graph.aiOnnx.where([cond, x1, x2])", "\ndef where(graph, cond, x1, x2):\n    return graph.aiOnnx.where([cond, x1, x2])\n\ndef softmax(graph, x, dim, stable_mode=None):\n    if stable_mode:\n        raise ValueError('set use unstable softmax in session, or use type ce ')\n    return graph.aiOnnx.softmax([x], dim)\n\ndef argmax(graph, x, axis, keepdims=True):\n    return graph.aiOnnx.argmax([x], axis=axis, keepdims=keepdims)", "\ndef argmax(graph, x, axis, keepdims=True):\n    return graph.aiOnnx.argmax([x], axis=axis, keepdims=keepdims)\n\ndef topk(graph, x, axis, k):\n    k = constant(graph, np.array(k).astype(np.int64), 'k')\n    return graph.aiOnnx.topk([x, k], axis=axis)\n\ndef printtensor(graph, x, title):\n    return graph.aiGraphcore.printtensor([x], title=title)", "def printtensor(graph, x, title):\n    return graph.aiGraphcore.printtensor([x], title=title)\n\ndef gelu(graph, x):\n    return graph.aiGraphcore.gelu([x])\n\ndef concat(graph, x, y, axis):\n    return graph.aiOnnx.concat([x, y], axis=axis)\n\ndef concat_sequence(graph, inputs, axis):\n    return graph.aiOnnx.concat(inputs, axis=axis)", "\ndef concat_sequence(graph, inputs, axis):\n    return graph.aiOnnx.concat(inputs, axis=axis)\n\ndef log_softmax(graph, x, axis=-1):\n    return graph.aiOnnx.logsoftmax([x], axis)\n\ndef clip(graph, x, clip_min=10, clip_max=100):\n    output = graph.aiOnnx.clip([x], max=clip_max, min=clip_min)\n    return output", "\ndef expand(graph, x, shape_list):\n    expand_shape = constant(graph, np.array(\n        shape_list, dtype=np.int32), 'shape')\n    return graph.aiOnnx.expand([x, expand_shape])\n\ndef bitwiseand(graph, x, y):\n    return graph.aiGraphcore.bitwiseand([x, y])\n\ndef tile(graph, x, repeats):\n    repeats = constant(graph, np.array(repeats, dtype=np.int64), 'repeats')\n    return graph.aiOnnx.tile([x, repeats], 'tile')", "\ndef tile(graph, x, repeats):\n    repeats = constant(graph, np.array(repeats, dtype=np.int64), 'repeats')\n    return graph.aiOnnx.tile([x, repeats], 'tile')\n\ndef loop(graph, max_loop_num, loop_graph_input_list, loop_graph):\n    max_loop = constant(graph, np.array(max_loop_num).astype(np.int32), 'max_loop')\n    init_cond = constant(graph, np.array(True).astype(np.bool_), 'cond')\n    loop_outputs = graph.aiOnnx.loop(\n        [max_loop, init_cond] + loop_graph_input_list,\n        len(loop_graph_input_list),\n        loop_graph,\n    )\n    return loop_outputs", "\ndef call_sub_graph(graph, input_list, sub_graph):\n    return graph.aiGraphcore.call(input_list, 1, sub_graph)\n\ndef replicated_all_reduce(graph, x):\n    return graph.aiGraphcore.replicatedallreduce([x])\n\ndef conv(graph, x, weight, bias, kernel_shape, strides, pads, dilations, group):\n    args = [x, weight] if bias is None else [x, weight, bias]\n    o = graph.aiOnnx.conv(\n        args=args,\n        kernel_shape=kernel_shape,\n        strides=strides,\n        pads=pads,\n        dilations=dilations,\n        group=group\n    )\n    amp = REGISTRY.get('amp')\n    partialtype = REGISTRY.get('partialtype')\n    if amp is not None:\n        graph.setAvailableMemoryProportion(o, amp)\n    if partialtype is not None:\n        graph.setPartialsType(o, partialtype)\n    return o", "\ndef relu(graph, x):\n    return graph.aiOnnx.relu([x])\n\ndef sigmoid(graph, x):\n    return graph.aiOnnx.sigmoid([x])\n\ndef exp(graph, x):\n    return graph.aiOnnx.exp([x])\n\ndef maximum(graph, x, y):\n    cond = greater(graph, x, y)\n    output = where(graph, cond, x, y)\n    return output", "\ndef maximum(graph, x, y):\n    cond = greater(graph, x, y)\n    output = where(graph, cond, x, y)\n    return output\n\ndef swish(graph, x):\n    return graph.aiGraphcore.swish([x])\n\ndef mean(graph, x):\n    return graph.aiOnnx.mean([x])", "\ndef mean(graph, x):\n    return graph.aiOnnx.mean([x])\n\ndef sqrt(graph, x):\n    return graph.aiOnnx.sqrt([x])\n\ndef reciprocal(graph, x):\n    return graph.aiOnnx.reciprocal([x])\n\ndef reducemean(graph, x):\n    return graph.aiOnnx.reducemean([x],axes=[-1])", "\ndef reducemean(graph, x):\n    return graph.aiOnnx.reducemean([x],axes=[-1])\n"]}
{"filename": "poptransformer/layers/conv.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom .base_layer import BaseLayer\n\n\nclass BaseConv1d(BaseLayer):\n    def __init__(\n        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.output_size = output_size\n        self.kernels = [kernels]\n        self.strides = [strides]\n        self.pads = [pads] * 2 if isinstance(pads, int) else pads\n        self.dilations = [dilations]\n        self.groups = groups\n        self.bias = bias\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_shape = [self.output_size, self.input_size // self.groups] + self.kernels\n        weight_np = self.get_param_from_state_dict(weight_key, weight_shape)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n        if self.bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x):\n        x = ops.conv(\n            graph=graph,\n            x=x,\n            weight=self.weight_id,\n            bias=self.bias_id if self.bias else None,\n            kernel_shape=self.kernels,\n            strides=self.strides,\n            pads=self.pads,\n            dilations=self.dilations,\n            group=self.groups\n        )\n        return x", "\nclass BaseConv1d(BaseLayer):\n    def __init__(\n        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.output_size = output_size\n        self.kernels = [kernels]\n        self.strides = [strides]\n        self.pads = [pads] * 2 if isinstance(pads, int) else pads\n        self.dilations = [dilations]\n        self.groups = groups\n        self.bias = bias\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_shape = [self.output_size, self.input_size // self.groups] + self.kernels\n        weight_np = self.get_param_from_state_dict(weight_key, weight_shape)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n        if self.bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x):\n        x = ops.conv(\n            graph=graph,\n            x=x,\n            weight=self.weight_id,\n            bias=self.bias_id if self.bias else None,\n            kernel_shape=self.kernels,\n            strides=self.strides,\n            pads=self.pads,\n            dilations=self.dilations,\n            group=self.groups\n        )\n        return x", "\n\nclass TPConv1d(BaseConv1d):\n    pass\n\n\nclass BaseConv2d(BaseLayer):\n    def __init__(\n        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.output_size = output_size\n        self.kernels = [kernels] * 2 if isinstance(kernels, int) else kernels\n        self.strides = [strides] * 2 if isinstance(strides, int) else strides\n        self.pads = [pads] * 4 if isinstance(pads, int) else pads\n        self.dilations = [dilations] * 2 if isinstance(dilations, int) else dilations\n        self.groups = groups\n        self.bias = bias\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_shape = [self.output_size, self.input_size // self.groups] + self.kernels\n        weight_np = self.get_param_from_state_dict(weight_key, weight_shape)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n        if self.bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x):\n        x = ops.conv(\n            graph=graph,\n            x=x,\n            weight=self.weight_id,\n            bias=self.bias_id if self.bias else None,\n            kernel_shape=self.kernels,\n            strides=self.strides,\n            pads=self.pads,\n            dilations=self.dilations,\n            group=self.groups\n        )\n        return x", "\nclass TPConv2d(BaseConv2d):\n    pass\n\n\nclass Conv1d(TPConv1d, BaseConv1d):\n\n    layer_class_map = {\n        'tp': TPConv1d,\n        'shard': BaseConv1d\n    }\n\n    def __init__(\n        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, output_size, kernels, strides, pads, dilations, groups, bias)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", "\n\nclass Conv2d(TPConv2d, BaseConv2d):\n\n    layer_class_map = {\n        'tp': TPConv2d,\n        'shard': BaseConv2d\n    }\n\n    def __init__(\n        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, output_size, kernels, strides, pads, dilations, groups, bias)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/layer_norm.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom .base_layer import BaseLayer\n\n\nclass BaseLayerNorm(BaseLayer):\n\n    norm_fn_map = {'group': ops.group_norm, 'ce': ops.layer_norm_ce}\n\n    def __init__(self, context, name, input_size, eps):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.eps = eps\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n        bias_key = '.'.join([self.context, 'bias'])\n        bias_np = self.get_param_from_state_dict(bias_key, [self.input_size])\n        self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x, sequence_length, norm_type):\n        with graph.nameScope(self.context):\n            norm_fn = self.norm_fn_map.get(norm_type, None)\n            if not norm_fn:\n                raise ValueError(f\"Invalid norm_fn {norm_type}, options: {self.norm_fn_map.keys()}\")\n            x = norm_fn(\n                graph, x, self.weight_id, self.bias_id, self.eps, self.batch_size, sequence_length, self.input_size)\n            return x", "\nclass BaseLayerNorm(BaseLayer):\n\n    norm_fn_map = {'group': ops.group_norm, 'ce': ops.layer_norm_ce}\n\n    def __init__(self, context, name, input_size, eps):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.eps = eps\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n        bias_key = '.'.join([self.context, 'bias'])\n        bias_np = self.get_param_from_state_dict(bias_key, [self.input_size])\n        self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x, sequence_length, norm_type):\n        with graph.nameScope(self.context):\n            norm_fn = self.norm_fn_map.get(norm_type, None)\n            if not norm_fn:\n                raise ValueError(f\"Invalid norm_fn {norm_type}, options: {self.norm_fn_map.keys()}\")\n            x = norm_fn(\n                graph, x, self.weight_id, self.bias_id, self.eps, self.batch_size, sequence_length, self.input_size)\n            return x", "\n\nclass TPLayerNorm(BaseLayerNorm):\n    pass\n\n\nclass LayerNorm(TPLayerNorm, BaseLayerNorm):\n\n    layer_class_map = {\n        'tp': TPLayerNorm,\n        'shard': BaseLayerNorm}\n\n    def __init__(self, context, name, input_size, eps):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, eps)\n\n    def __call__(self, graph, x, sequence_length, norm_type):\n        return self.layer_class.__call__(self, graph, x, sequence_length, norm_type)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.utils import build_sharded_weight\nfrom .base_layer import BaseLayer", "from poptransformer.utils import build_sharded_weight\nfrom .base_layer import BaseLayer\n\n\nclass BaseEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embed_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.vocab_size, self.embed_size])\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            return ops.gather(graph, self.weight_id, input_ids)", "\n\nclass TPEmbedding(BaseEmbedding):\n\n    def collect_bind_layer_weights(self):\n        self.vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n        self.vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.vocab_size, self.embed_size])\n        weight_np = build_sharded_weight(weight_np, self.num_replicas, self.vocab_size, self.embed_size)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key, **self.vs_setting)\n\n        index_offset_np = np.expand_dims(np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * self.vocab_per_ipu\n        self.index_offset = self.add_initialized_input_tensor(index_offset_np, 'index_offset', **self.vs_setting)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            replicated_input_ids = ops.sub(graph, input_ids, self.index_offset)\n            cond1 = ops.less(graph, replicated_input_ids, ops.constant(graph, np.array([0], dtype=np.int32)))\n            cond2 = ops.greater(\n                graph, replicated_input_ids, ops.constant(graph, np.array([self.vocab_per_ipu], dtype=np.int32)))\n            cond = ops.bitwiseor(graph, ops.cast(graph, cond1, 'INT32'), ops.cast(graph, cond2, 'INT32'))\n            cond = ops.cast(graph, cond, 'BOOL')\n            fill_value_np = self.vocab_per_ipu * np.ones(\n                (self.num_replicas, self.batch_size, sequence_length), dtype=np.int32)\n            fill_value = self.add_initialized_input_tensor(fill_value_np, 'fill_value', **self.vs_setting)\n            updated_replicated_input_ids = ops.where(graph, cond, fill_value, replicated_input_ids)\n            output = ops.gather(graph, self.weight_id, updated_replicated_input_ids)\n            return output", "\n\nclass Embedding(TPEmbedding, BaseEmbedding):\n    layer_class_map = {\n        'tp': TPEmbedding,\n        'shard': BaseEmbedding}\n\n    def __init__(self, context, name, vocab_size, embed_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, vocab_size, embed_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom .base_layer import BaseLayer\nfrom .linear import Linear\n\n\nclass BaseMLP(BaseLayer):\n    act_fn_map = {'gelu': ops.gelu}\n    def __init__(self, context, name, input_size, hidden_size, act_fn):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.act_fn = self.act_fn_map.get(act_fn, None)\n        if not self.act_fn:\n            raise ValueError(f\"Invalid act_fn {act_fn}, options: {self.act_fn_map.keys()}\")\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.c_fc = Linear(self.context, 'c_fc', self.input_size, self.hidden_size)\n        self.c_proj = Linear(self.context, 'c_proj', self.hidden_size, self.input_size)\n\n    def __call__(self, graph, x):\n        with graph.nameScope(self.context):\n            x = ops.reshape(graph, x, [-1, self.input_size])\n            x = self.c_fc(graph, x)\n            x = self.act_fn(graph, x)\n            x = self.c_proj(graph, x)\n            x = ops.reshape(graph, x, [self.batch_size, -1, self.input_size])\n            return x", "\n\nclass BaseMLP(BaseLayer):\n    act_fn_map = {'gelu': ops.gelu}\n    def __init__(self, context, name, input_size, hidden_size, act_fn):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.act_fn = self.act_fn_map.get(act_fn, None)\n        if not self.act_fn:\n            raise ValueError(f\"Invalid act_fn {act_fn}, options: {self.act_fn_map.keys()}\")\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.c_fc = Linear(self.context, 'c_fc', self.input_size, self.hidden_size)\n        self.c_proj = Linear(self.context, 'c_proj', self.hidden_size, self.input_size)\n\n    def __call__(self, graph, x):\n        with graph.nameScope(self.context):\n            x = ops.reshape(graph, x, [-1, self.input_size])\n            x = self.c_fc(graph, x)\n            x = self.act_fn(graph, x)\n            x = self.c_proj(graph, x)\n            x = ops.reshape(graph, x, [self.batch_size, -1, self.input_size])\n            return x", "\n\nclass TPMLP(BaseMLP):\n\n    def collect_bind_layer_weights(self):\n        fc_tp_setting = {\n            'strategy_name': 'start',\n        }\n        self.c_fc = Linear(self.context, 'c_fc', self.input_size, self.hidden_size, **fc_tp_setting)\n        proj_tp_setting = {\n            'strategy_name': 'end',\n        }\n        self.c_proj = Linear(self.context, 'c_proj', self.hidden_size, self.input_size, **proj_tp_setting)", "\n\nclass MLP(TPMLP, BaseMLP):\n    layer_class_map = {\n        'tp': TPMLP,\n        'shard': BaseMLP}\n\n    def __init__(self, context, name, input_size, hidden_size, act_fn):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, hidden_size, act_fn)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/lm_head.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport numpy as np\nfrom poptransformer import ops\nfrom .linear import Linear\nfrom .base_layer import BaseLayer", "from .linear import Linear\nfrom .base_layer import BaseLayer\n\n\nclass BaseLMHead(BaseLayer):\n\n    def __init__(self, context, name, topk, vocab_size, embedding_size, tie_weight=None):\n        super().__init__(context, name)\n        self.topk = topk\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.virtual_id = None\n        self.use_tied_embedding = False\n        self.tie_weight = tie_weight\n        self.collect_bind_layer_weights()\n\n    def tie_embedding(self, weight_id):\n        self.head.weight_id = weight_id\n        self.use_tied_embedding = True\n        self.logger.info(f'setting weight id to {weight_id}')\n\n    def set_virtual_id(self, virtual_id):\n        self.virtual_id = virtual_id\n        self.logger.info(f'setting virtual id to {virtual_id}')\n\n    def collect_bind_layer_weights(self):\n        self.head = Linear(self.context, None, self.embedding_size, self.vocab_size, False)\n        if self.tie_weight is not None:\n            self.tie_embedding(self.tie_weight)\n\n    def __call__(self, graph, logits, sequence_length):\n        with graph.virtualGraph(self.virtual_id):\n            if self.use_tied_embedding:\n                self.head.weight_id = ops.transpose(graph, self.head.weight_id, [1, 0])\n            logits = self.head(graph, logits)\n            if self.topk != 1:\n                next_token = ops.random_sample(graph, logits, k=self.topk, dim=2)\n            else:\n                _, next_token = ops.topk(graph, logits, axis=2, k=1)\n            next_token = ops.squeeze(graph, next_token, [2])\n            return next_token", "\n\nclass TPLMHead(BaseLMHead):\n\n    def collect_bind_layer_weights(self):\n        if not self.tie_weight:\n            lm_tp_settings = {\n                'strategy_name': 'start',\n            }\n        else:\n            lm_tp_settings = {\n                'strategy_name': 'identity',\n            }\n        self.head = Linear(self.context, None, self.embedding_size, self.vocab_size, False, **lm_tp_settings)\n        if self.tie_weight:\n            self.tie_embedding(self.tie_weight)\n\n    def __call__(self, graph, logits, sequence_length):\n        with graph.virtualGraph(self.virtual_id):\n            vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n            vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n            index_offset_np = np.expand_dims(np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * vocab_per_ipu\n            index_offset = self.add_initialized_input_tensor(index_offset_np, 'index_offset', **vs_setting)\n            if self.use_tied_embedding:\n                self.head.weight_id = ops.transpose(graph, self.head.weight_id, [1, 0])\n                self.use_tied_embedding = False # Avoid to transpose twice in 2 stage mode.\n            logits = self.head(graph, logits)\n\n            pad_idx = ops.equal(graph, ops.constant(graph, np.array(0.0).astype(self.np_float_type), 'zero'), logits)\n            logits = ops.where(\n                graph, pad_idx, ops.constant(graph, np.array(-10000.0).astype(self.np_float_type), '-10000'), logits)\n            next_token_prob, next_token_ = ops.topk(graph, logits, axis=2, k=self.topk)\n            next_token = ops.add(graph, next_token_, index_offset)\n            next_token_prob = ops.replicated_allgather(graph, next_token_prob)\n            next_token_topk = ops.replicated_allgather(graph, next_token)\n            next_token_prob_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n            next_token_prob = ops.transpose(\n                graph,\n                ops.reshape(graph, next_token_prob, next_token_prob_shape),\n                [1, 0]\n            )\n            next_token_topk_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n            next_token_topk = ops.transpose(\n                graph,\n                ops.reshape(graph, next_token_topk, next_token_topk_shape),\n                [1, 0]\n            )\n            next_token_prob = ops.reshape(\n                graph, next_token_prob, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n            next_token_topk = ops.reshape(\n                graph, next_token_topk, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\n            next_token_idx = ops.argmax(graph, next_token_prob, axis=2)  # [B,1,1]\n            next_token_idx = ops.squeeze(graph, next_token_idx, [1, 2])  # (B,)\n            next_token_topk = ops.squeeze(graph, next_token_topk, [1])  # [B,topk * num_replica]\n            next_token = ops.grouped_gather(\n                graph, next_token_topk, next_token_idx, axis=1, group_size=self.batch_size)  # (B,)\n            next_token = ops.reshape(graph, next_token, [self.batch_size, -1])\n            return next_token", "\n\nclass LMHead(TPLMHead, BaseLMHead):\n    layer_class_map = {\n        'tp': TPLMHead,\n        'shard': BaseLMHead}\n\n    def __init__(self, context, name, topk, vocab_size, embedding_size, tie_weight=None):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, topk, vocab_size, embedding_size, tie_weight)\n\n    def __call__(self, graph, logits, sequence_length):\n        return self.layer_class.__call__(self, graph, logits, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/base_layer.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nimport numpy as np\nfrom popart import VariableRetrievalMode, VariableSettings, CommGroupType, CommGroup\nfrom poptransformer.utils import REGISTRY, DeviceScope, OptionsScope\n", "from poptransformer.utils import REGISTRY, DeviceScope, OptionsScope\n\n\nclass BaseLayer:\n    # class attributes which can be used by child classes\n    logger = REGISTRY.get('logger')\n    vs_type_map = {\n        'consecutive': CommGroupType.Consecutive,\n        'all': CommGroupType.All,\n        'orthogonal': CommGroupType.Orthogonal}\n    retrieval_mode_map = {\n        'one_per_group': VariableRetrievalMode.OnePerGroup,\n        'all_replicas': VariableRetrievalMode.AllReplicas}\n\n    def __init__(self, context, name):\n        self.context = '.'.join([context, name]) if (context and name) else context or name\n        self.name = name\n\n    @property\n    def model_type(self):\n        return REGISTRY.get('model_type')\n\n    @property\n    def batch_per_step(self):\n        return REGISTRY.get('batch_per_step')\n\n    @property\n    def batch_size(self):\n        return REGISTRY.get('batch_size')\n\n    @property\n    def num_replicas(self):\n        return REGISTRY.get('num_replicas')\n\n    @property\n    def main_graph(self):\n        return REGISTRY.get('main_graph')\n\n    @property\n    def state_dict(self):\n        return REGISTRY.get('state_dict')\n\n    @property\n    def popart_float_type(self):\n        return REGISTRY.get('tensor_type').popart_float_type\n\n    @property\n    def np_float_type(self):\n        return REGISTRY.get('tensor_type').np_float_type\n\n    @property\n    def precision(self):\n        return REGISTRY.get('tensor_type').precision\n\n    @property\n    def enable_pipeline(self):\n        return REGISTRY.get('enable_pipeline')\n\n    def option_scope(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n        return OptionsScope(amp, partialtype, serial_factor, serial_mode)\n\n    def device_scope(self, graph, virtual_graph_id=None, pipeline_stage_id=None, outline_attr=None):\n        return DeviceScope(graph, virtual_graph_id, pipeline_stage_id, self.enable_pipeline, outline_attr)\n\n    @abstractmethod\n    def __call__(self, graph, *args):\n        # build the graph with weights / layers built from the collect_bind_layer_weights\n        pass\n\n    @abstractmethod\n    def collect_bind_layer_weights(self):\n        # build/process the weight / layers\n        # should be exectute in the __init__ fn, we may have to find a way to exectute it automatically\n        pass\n\n    def build_variable_setting(self, vs_type='consecutive', group_size=1, retrieval_mode='one_per_group'):\n        vs_type = self.vs_type_map.get(vs_type, None)\n        retrieval_mode = self.retrieval_mode_map.get(retrieval_mode, None)\n        assert vs_type, f\"Invalid vs_type: {vs_type}\"\n        assert retrieval_mode, f\"Invalid retrieval_mode: {retrieval_mode}\"\n        variableSettings = VariableSettings(\n            CommGroup(vs_type, replicaGroupSize=group_size),\n            retrieval_mode\n        )\n        return variableSettings\n\n    def add_initialized_input_tensor(self, weight_np, weight_key, **variable_setting):\n        if variable_setting:\n            variable_setting = self.build_variable_setting(**variable_setting)\n            weight_id = self.main_graph.addInitializedInputTensor(weight_np, variable_setting, weight_key)\n        else:\n            weight_id = self.main_graph.addInitializedInputTensor(weight_np, debugContext=weight_key)\n        return weight_id\n\n    def get_param_from_state_dict(self, weight_key, shape_list):\n        weight_np = self.state_dict.get(weight_key, None)\n\n        if weight_np is None:\n            self.logger.info(f'{weight_key} not found, using random tensor')\n            assert shape_list, f'no shape provided for random initializing the tensor {weight_key}'\n            weight_np = np.random.randn(*shape_list).astype(dtype=self.np_float_type)\n        else:\n            weight_np = weight_np.numpy()\n            if shape_list:\n                self.logger.info(f\"loading {weight_key} with shape {weight_np.shape}, dtype {weight_np.dtype}.\")\n                if self.precision not in ['int4', 'fp8', 'fp8_weight']:\n                    assert sum(\n                        (abs(a - b) for a, b in zip(weight_np.shape, shape_list))\n                        ) == 0, f'{weight_key} shape {weight_np.shape} not matched with provided {shape_list}'\n            else:\n                self.logger.warning(f'shape not provided or using int4/fp8 weight, skip shape check for {weight_key}')\n        return weight_np", ""]}
{"filename": "poptransformer/layers/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .base_layer import BaseLayer\nfrom .lm_head import LMHead\nfrom .linear import Linear\nfrom .mlp import MLP\nfrom .embedding import Embedding", "from .mlp import MLP\nfrom .embedding import Embedding\nfrom .layer_norm import LayerNorm\nfrom .conv import Conv2d\nfrom .conv import Conv1d\n"]}
{"filename": "poptransformer/layers/linear.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.utils import ParamHandler\nfrom .base_layer import BaseLayer\n\n\nclass BaseLinear(BaseLayer):\n    def __init__(self, context, name, input_size, output_size, use_bias=True, **kwargs):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = use_bias\n        self.kwargs = kwargs\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.param_handler = ParamHandler(host_layer=self)\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.output_size, self.input_size])\n        weight_np = weight_np.transpose(1, 0)\n        weight_np = self.param_handler.process_linear_weight(weight_np, weight_key)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n        if self.use_bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            bias_np = self.param_handler.process_linear_bias(bias_np)\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x):\n        with graph.nameScope(self.context):\n            x = self.param_handler.matmul(graph, x, self.weight_id)\n            x = ops.add(graph, x, self.bias_id) if self.use_bias else x\n        return x", "\n\nclass BaseLinear(BaseLayer):\n    def __init__(self, context, name, input_size, output_size, use_bias=True, **kwargs):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = use_bias\n        self.kwargs = kwargs\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.param_handler = ParamHandler(host_layer=self)\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.output_size, self.input_size])\n        weight_np = weight_np.transpose(1, 0)\n        weight_np = self.param_handler.process_linear_weight(weight_np, weight_key)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n        if self.use_bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            bias_np = self.param_handler.process_linear_bias(bias_np)\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\n    def __call__(self, graph, x):\n        with graph.nameScope(self.context):\n            x = self.param_handler.matmul(graph, x, self.weight_id)\n            x = ops.add(graph, x, self.bias_id) if self.use_bias else x\n        return x", "\n\nclass TPLinear(BaseLinear):\n\n    def collect_bind_layer_weights(self):\n        vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n        self.param_handler = ParamHandler(\n            host_layer=self,\n            tp_strategy_name=self.kwargs.get('strategy_name'),\n            **vs_setting\n        )\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.output_size, self.input_size])\n        weight_np = weight_np.transpose(1, 0)\n        weight_np = self.param_handler.process_linear_weight(weight_np, weight_key)\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key, **vs_setting)\n\n        if self.use_bias:\n            bias_key = '.'.join([self.context, 'bias'])\n            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n            bias_np = self.param_handler.process_linear_bias(bias_np)\n            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key, **vs_setting)", "\n\nclass Linear(TPLinear, BaseLinear):\n\n    layer_class_map = {\n        'tp': TPLinear,\n        'shard': BaseLinear\n    }\n\n    def __init__(self, context, name, input_size, output_size, use_bias=True, **kwargs):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, output_size, use_bias, **kwargs)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/layers/rms_layer_norm.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers.layer_norm import BaseLayerNorm\n\n\nclass BaseRMSLayerNorm(BaseLayerNorm):\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n    def __call__(self, graph, x):\n        variance_epsilon = ops.constant(graph, np.array(self.eps).astype(np.float32), 'variance_epsilon')\n        variance = ops.cast(graph, x, 'FLOAT')\n        variance = ops.mul(graph, variance, variance)\n        variance = ops.reducemean(graph, variance)\n        variance = ops.add(graph, variance, variance_epsilon)\n        variance = ops.sqrt(graph, variance)\n        variance = ops.reciprocal(graph, variance)\n        variance = ops.cast(graph, variance, self.popart_float_type)\n        x = ops.mul(graph, x, variance)\n        return ops.mul(graph, x, self.weight_id)", "\n\nclass BaseRMSLayerNorm(BaseLayerNorm):\n\n    def collect_bind_layer_weights(self):\n        weight_key = '.'.join([self.context, 'weight'])\n        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\n    def __call__(self, graph, x):\n        variance_epsilon = ops.constant(graph, np.array(self.eps).astype(np.float32), 'variance_epsilon')\n        variance = ops.cast(graph, x, 'FLOAT')\n        variance = ops.mul(graph, variance, variance)\n        variance = ops.reducemean(graph, variance)\n        variance = ops.add(graph, variance, variance_epsilon)\n        variance = ops.sqrt(graph, variance)\n        variance = ops.reciprocal(graph, variance)\n        variance = ops.cast(graph, variance, self.popart_float_type)\n        x = ops.mul(graph, x, variance)\n        return ops.mul(graph, x, self.weight_id)", "\n\nclass TPRMSLayerNorm(BaseRMSLayerNorm):\n    pass\n\n\nclass RMSLayerNorm(TPRMSLayerNorm, BaseRMSLayerNorm):\n\n    layer_class_map = {\n        'tp': TPRMSLayerNorm,\n        'shard': BaseRMSLayerNorm}\n\n    def __init__(self, context, name, input_size, eps):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, eps)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", "    "]}
{"filename": "poptransformer/models/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .base_model import HFDecBaseModel, HFDec2stageBaseModel\nfrom .gpt2.model import GPT2DecModel\nfrom .chatglm.model import ChatGLMDecModel\nfrom .rwkv.model import RWKVDecodeModel\nfrom .chatglm2.model import ChatGLM2DecModel", "from .rwkv.model import RWKVDecodeModel\nfrom .chatglm2.model import ChatGLM2DecModel\nfrom .llama2.model import LLAMA2DecModel\n"]}
{"filename": "poptransformer/models/base_model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nfrom collections import OrderedDict\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig\nfrom transformers import AutoModel", "from transformers import AutoConfig\nfrom transformers import AutoModel\nfrom transformers import AutoModelForCausalLM\nimport numpy as np\nimport popart\nfrom poptransformer.utils import REGISTRY, DeviceScope, OptionsScope\nfrom poptransformer import ops\n\n\nclass BaseModel:\n\n    graph = REGISTRY.get('main_graph')\n    logger = REGISTRY.get('logger')\n    tensor_type = REGISTRY.get('tensor_type')\n\n    def __init__(self, **kwargs):\n        self.logger.info(f'Initializing model class: {self.__class__.__name__}')\n        self.anchor_return_type = kwargs.get('anchor_return_type', 'ALL')\n        self.layer_per_ipu = kwargs.get('layer_per_ipu', [])\n\n    @property\n    def enable_pipeline(self):\n        return REGISTRY.get('enable_pipeline')\n\n    @property\n    def batch_size(self):\n        return REGISTRY.get('batch_size')\n\n    @property\n    def batch_per_step(self):\n        return REGISTRY.get('batch_per_step')\n\n    @property\n    def model_type(self):\n        return REGISTRY.get('model_type')\n\n    @property\n    def num_replicas(self):\n        return REGISTRY.get('num_replicas')\n\n    @property\n    def stage_num(self):\n        return max(len(self.layer_per_ipu), 1)\n\n    @property\n    def popart_float_type(self):\n        return REGISTRY.get('tensor_type').popart_float_type\n\n    @property\n    def np_float_type(self):\n        return REGISTRY.get('tensor_type').np_float_type\n\n    @property\n    def precision(self):\n        return REGISTRY.get('tensor_type').precision\n\n    @abstractmethod\n    def prepare_state_dict(self):\n        # build self.state_dict\n        # then it will be registed to REGISTER for layers usage\n        pass\n\n    @abstractmethod\n    def build_graph(self):\n        # build model's graph\n        pass\n\n    @abstractmethod\n    def build_input_dict(self, **kwargs):\n        # process input, build dict,\n        # will be wrapped with stepio and feed to graph later\n        pass\n\n    @abstractmethod\n    def build_output_dict(self, anchor_arrays):\n        # process outputs in session.anchor_arrays,\n        # return a dict for layer usage\n        pass\n\n    def device_scope(self, graph, virtual_graph_id=None, pipeline_stage_id=None, outline_attr=None):\n        return DeviceScope(graph, virtual_graph_id, pipeline_stage_id, self.enable_pipeline, outline_attr)\n\n    def option_scope(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n        return OptionsScope(amp, partialtype, serial_factor, serial_mode)\n\n    def register_state_dict(self):\n        # register the state dict to REGISTER, will be used in layer\n        assert self.state_dict\n        REGISTRY.update('state_dict', self.state_dict)\n\n    @property\n    def initializers(self):\n        # all weights' id added into the graph\n        return [tensor_id for tensor_id in self.graph.getInputTensorIds()\n                if self.graph.isInitializer(tensor_id)]\n\n    @property\n    def model_output(self):\n        output_tensor_ids = self.get_output_tensor_ids(self.graph)\n        anchor_return_type = popart.AnchorReturnType(self.anchor_return_type)\n        return {key: anchor_return_type for key in output_tensor_ids}\n\n    @property\n    def model_proto(self):\n        return self.graph.getModelProto()\n\n    def add_input_tensor(self, graph, popart_dtype, shape, name):\n        if self.model_type in ['shard']:\n            return graph.addInputTensor(\n                popart.TensorInfo(popart_dtype, shape), debugContext=name)\n        if self.model_type == 'tp':\n            input_setting = popart.InputSettings(\n                popart.ReplicatedStreamMode.Broadcast)\n            return graph.addInputTensor(\n                popart.TensorInfo(popart_dtype, shape), input_setting, debugContext=name)\n        raise ValueError(f\"Invalid model_type: {self.model_type}\")\n\n    def add_input_tensor_from_parent_graph(self, graph, tensor_id):\n        if isinstance(tensor_id, str):\n            graph.addInputTensorFromParentGraph(tensor_id)\n        elif isinstance(tensor_id, list):\n            for i in tensor_id:\n                self.add_input_tensor_from_parent_graph(graph, i)\n        else:\n            raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\n    def add_untyped_input_tensor(self, graph, tensor_id):\n        if isinstance(tensor_id, list):\n            all_tensor_ids = []\n            for i in tensor_id:\n                all_tensor_ids.append(graph.addUntypedInputTensor(i))\n            return all_tensor_ids\n        if isinstance(tensor_id, str):\n            return graph.addUntypedInputTensor(tensor_id)\n        raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\n    def add_output_tensor(self, graph, output):\n        if isinstance(output, list):\n            for i in output:\n                graph.addOutputTensor(i)\n        elif isinstance(output, str):\n            graph.addOutputTensor(output)\n        else:\n            raise ValueError(f\"Invalid output: {output}\")\n\n    def get_input_tensor_ids(self, graph):\n        return [tensor_id for tensor_id in graph.getInputTensorIds()\n                if not graph.isInitializer(tensor_id)]\n\n    def get_output_tensor_ids(self, graph):\n        return graph.getOutputTensorIds()\n\n    def create_sub_graph(self, graph, name, sub_graph_inputs, is_loop_graph=False):\n        sub_graph = graph.createSubgraphBuilder()\n        sub_graph.setGraphName(name)\n        if is_loop_graph:\n            self.add_input_tensor(sub_graph, 'INT32', [], 'max_loop')\n            self.add_input_tensor(sub_graph, 'BOOL', [], 'cond')\n        self.add_input_tensor_from_parent_graph(sub_graph, sub_graph_inputs)\n        return sub_graph", "\nclass BaseModel:\n\n    graph = REGISTRY.get('main_graph')\n    logger = REGISTRY.get('logger')\n    tensor_type = REGISTRY.get('tensor_type')\n\n    def __init__(self, **kwargs):\n        self.logger.info(f'Initializing model class: {self.__class__.__name__}')\n        self.anchor_return_type = kwargs.get('anchor_return_type', 'ALL')\n        self.layer_per_ipu = kwargs.get('layer_per_ipu', [])\n\n    @property\n    def enable_pipeline(self):\n        return REGISTRY.get('enable_pipeline')\n\n    @property\n    def batch_size(self):\n        return REGISTRY.get('batch_size')\n\n    @property\n    def batch_per_step(self):\n        return REGISTRY.get('batch_per_step')\n\n    @property\n    def model_type(self):\n        return REGISTRY.get('model_type')\n\n    @property\n    def num_replicas(self):\n        return REGISTRY.get('num_replicas')\n\n    @property\n    def stage_num(self):\n        return max(len(self.layer_per_ipu), 1)\n\n    @property\n    def popart_float_type(self):\n        return REGISTRY.get('tensor_type').popart_float_type\n\n    @property\n    def np_float_type(self):\n        return REGISTRY.get('tensor_type').np_float_type\n\n    @property\n    def precision(self):\n        return REGISTRY.get('tensor_type').precision\n\n    @abstractmethod\n    def prepare_state_dict(self):\n        # build self.state_dict\n        # then it will be registed to REGISTER for layers usage\n        pass\n\n    @abstractmethod\n    def build_graph(self):\n        # build model's graph\n        pass\n\n    @abstractmethod\n    def build_input_dict(self, **kwargs):\n        # process input, build dict,\n        # will be wrapped with stepio and feed to graph later\n        pass\n\n    @abstractmethod\n    def build_output_dict(self, anchor_arrays):\n        # process outputs in session.anchor_arrays,\n        # return a dict for layer usage\n        pass\n\n    def device_scope(self, graph, virtual_graph_id=None, pipeline_stage_id=None, outline_attr=None):\n        return DeviceScope(graph, virtual_graph_id, pipeline_stage_id, self.enable_pipeline, outline_attr)\n\n    def option_scope(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n        return OptionsScope(amp, partialtype, serial_factor, serial_mode)\n\n    def register_state_dict(self):\n        # register the state dict to REGISTER, will be used in layer\n        assert self.state_dict\n        REGISTRY.update('state_dict', self.state_dict)\n\n    @property\n    def initializers(self):\n        # all weights' id added into the graph\n        return [tensor_id for tensor_id in self.graph.getInputTensorIds()\n                if self.graph.isInitializer(tensor_id)]\n\n    @property\n    def model_output(self):\n        output_tensor_ids = self.get_output_tensor_ids(self.graph)\n        anchor_return_type = popart.AnchorReturnType(self.anchor_return_type)\n        return {key: anchor_return_type for key in output_tensor_ids}\n\n    @property\n    def model_proto(self):\n        return self.graph.getModelProto()\n\n    def add_input_tensor(self, graph, popart_dtype, shape, name):\n        if self.model_type in ['shard']:\n            return graph.addInputTensor(\n                popart.TensorInfo(popart_dtype, shape), debugContext=name)\n        if self.model_type == 'tp':\n            input_setting = popart.InputSettings(\n                popart.ReplicatedStreamMode.Broadcast)\n            return graph.addInputTensor(\n                popart.TensorInfo(popart_dtype, shape), input_setting, debugContext=name)\n        raise ValueError(f\"Invalid model_type: {self.model_type}\")\n\n    def add_input_tensor_from_parent_graph(self, graph, tensor_id):\n        if isinstance(tensor_id, str):\n            graph.addInputTensorFromParentGraph(tensor_id)\n        elif isinstance(tensor_id, list):\n            for i in tensor_id:\n                self.add_input_tensor_from_parent_graph(graph, i)\n        else:\n            raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\n    def add_untyped_input_tensor(self, graph, tensor_id):\n        if isinstance(tensor_id, list):\n            all_tensor_ids = []\n            for i in tensor_id:\n                all_tensor_ids.append(graph.addUntypedInputTensor(i))\n            return all_tensor_ids\n        if isinstance(tensor_id, str):\n            return graph.addUntypedInputTensor(tensor_id)\n        raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\n    def add_output_tensor(self, graph, output):\n        if isinstance(output, list):\n            for i in output:\n                graph.addOutputTensor(i)\n        elif isinstance(output, str):\n            graph.addOutputTensor(output)\n        else:\n            raise ValueError(f\"Invalid output: {output}\")\n\n    def get_input_tensor_ids(self, graph):\n        return [tensor_id for tensor_id in graph.getInputTensorIds()\n                if not graph.isInitializer(tensor_id)]\n\n    def get_output_tensor_ids(self, graph):\n        return graph.getOutputTensorIds()\n\n    def create_sub_graph(self, graph, name, sub_graph_inputs, is_loop_graph=False):\n        sub_graph = graph.createSubgraphBuilder()\n        sub_graph.setGraphName(name)\n        if is_loop_graph:\n            self.add_input_tensor(sub_graph, 'INT32', [], 'max_loop')\n            self.add_input_tensor(sub_graph, 'BOOL', [], 'cond')\n        self.add_input_tensor_from_parent_graph(sub_graph, sub_graph_inputs)\n        return sub_graph", "\n\nclass HFBaseModel(BaseModel):\n    \"\"\"\n    hugggingface base model,\n    with the process fn for loading hf model, config and tokenizer\n    \"\"\"\n    hf_model_class_name_map = {\n        'auto_model': AutoModel,\n        'auto_model_for_causallm': AutoModelForCausalLM}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.hf_model_name = kwargs.get('hf_model_name', None)\n        self.hf_model_class_name = kwargs.get('hf_model_class_name', None)\n        self.override_hfconfig_from_json = kwargs.get('override_hfconfig_from_json', None)\n        self.hf_cache_dir = kwargs.get('hf_cache_dir', './temp/')\n        self.prepare_state_dict()\n\n    def process_hf_model_state_dict(self):\n        self.logger.info('no prescale process on hf state dict')\n\n    def prepare_state_dict(self):\n        assert self.hf_model_name, f\"Invalid hf_model_name: {self.hf_model_name}\"\n        self.hf_tokenizer = AutoTokenizer.from_pretrained(\n            self.hf_model_name,\n            cache_dir=self.hf_cache_dir,\n            pad_token='[PAD]'\n        )\n        self.logger.info(f'initialized tokenizer by model_name: {self.hf_model_name}')\n        if not self.override_hfconfig_from_json:\n            model_class = self.hf_model_class_name_map.get(self.hf_model_class_name, None)\n            assert model_class, f\"Invalid hf_model_class_name: {self.hf_model_class_name}\"\n            self.logger.info(f'initializing hf model class: {model_class.__name__}')\n            self.hf_model = model_class.from_pretrained(self.hf_model_name, cache_dir=self.hf_cache_dir)\n            self.logger.info(f'loading pretrained hf model: {self.hf_model_name}')\n            self.hf_config = self.hf_model.config\n            self.process_hf_model_state_dict()\n            if self.precision != 'fp32':\n                self.hf_model.half()\n                self.logger.info(f'casting model to {self.precision}')\n            self.state_dict = self.hf_model.state_dict()\n        else:\n            self.logger.info('using overrided config, no state dict loaded')\n            self.hf_config = AutoConfig.from_pretrained(self.override_hfconfig_from_json)\n            self.state_dict = {}    # No states if using your own model configurations.\n        self.register_state_dict()", "\n\nclass HFDecBaseModel(HFBaseModel):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.early_stop = kwargs.get('early_stop', False)\n        self.max_length = kwargs.get('max_length', 128)\n        max_loop = kwargs.get('max_loop', None)\n        self.max_loop = max_loop if max_loop else self.max_length\n\n    @abstractmethod\n    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n        # TODO: Add params docstring.\n        # return model graph\n        pass\n\n    def build_default_inputs(self):\n        # build input tensor here, no need to feed to step io if not used.\n        default_inputs = OrderedDict()\n        default_inputs['input_ids_container'] = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'input_ids')\n        default_inputs['step'] = self.add_input_tensor(\n            self.graph, 'INT32', [], 'step')\n        default_inputs['attention_mask'] = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'attention_mask')\n        default_inputs['position_ids'] = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'position_ids')\n        default_inputs['stop_mask'] = self.add_input_tensor(\n            self.graph, 'INT32', [], 'stop_mask')\n        return default_inputs\n\n    def build_model_graph_inputs(self, graph, inputs):\n        with graph.nameScope('mask'):\n            attention_mask_value = [[0] + [-10000 for i in range(self.max_length - 1)]] * self.batch_size\n            attention_mask_value = np.array(attention_mask_value).astype(self.np_float_type)\n            attention_mask = ops.constant(\n                graph, attention_mask_value, 'attention_mask')\n            inputs['attention_mask'] = attention_mask\n        with graph.nameScope('step'):\n            inputs['step'] = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n        with graph.nameScope('stop_mask'):\n            inputs['stop_mask'] = ops.constant(\n                self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n        del inputs['position_ids']\n        return inputs\n\n    def build_graph(self):\n        default_inputs = self.build_default_inputs()\n        with self.graph.virtualGraph(0):\n            model_graph_inputs = self.build_model_graph_inputs(self.graph, default_inputs)\n        model_graph = self.create_sub_graph(\n            self.graph,\n            'model_graph',\n            list(model_graph_inputs.values()),\n            True\n        )\n        model_outputs = self.build_model_graph(model_graph, model_graph_inputs, sequence_length=1)\n        self.build_post_model_graph(model_graph, model_graph_inputs, model_outputs)\n        with self.device_scope(self.graph, 0):\n            outputs = ops.loop(\n                self.graph,\n                self.max_loop,\n                list(model_graph_inputs.values()),\n                model_graph\n            )[:2]\n        self.add_output_tensor(self.graph, outputs)\n\n\n    def build_post_model_graph(self, model_graph, model_graph_inputs, model_outputs):\n        # continue build model graph for post inference step\n        stage_offset = model_outputs['stage_offset']\n        next_ids = model_outputs['next_ids']\n        attention_mask = model_graph_inputs['attention_mask']\n        step = model_graph_inputs['step']\n        input_ids_container = model_graph_inputs['input_ids_container']\n        stop_mask = model_graph_inputs['stop_mask']\n\n        with self.device_scope(model_graph, 0, pipeline_stage_id=stage_offset):\n            next_iput_ids_container, next_step, next_attention_mask, id_to_update= self.step_containers(\n                model_graph, input_ids_container, step, attention_mask, next_ids\n            )\n            next_stop_mask, keep_going_cond = self.step_loop_cond(model_graph, id_to_update, stop_mask)\n\n        self.add_output_tensor(\n            model_graph,\n            [keep_going_cond, next_iput_ids_container, next_step, next_attention_mask, next_stop_mask])\n        # build model graph for post inference step\n\n    def step_containers(self, graph, input_ids_container, step, attention_mask, next_ids):\n        with graph.nameScope('step_containers'):\n            if self.hf_tokenizer.pad_token_id:\n                pad_token_id = self.hf_tokenizer.pad_token_id\n            else:\n                pad_token_id = self.hf_config.pad_token_id\n            step_add_value = ops.constant(graph, np.array(1).astype(np.int32), '1')\n            next_step = ops.add(graph, step, step_add_value)\n            if attention_mask is not None:\n                attention_mask_add = ops.constant(\n                    graph, np.zeros((self.batch_size, 1), dtype=self.np_float_type), 'attention_mask_add')\n                next_attention_mask = ops.dynamic_update(\n                    graph, attention_mask, next_step, attention_mask_add, axes=[1], sizes=[1])\n            else:\n                next_attention_mask = None\n            input_ids_slice = ops.dynamic_slice(\n                graph, input_ids_container, next_step, axes=[1], sizes=[1])\n            pad_id = ops.constant(graph, np.array(pad_token_id), 'pad_id')\n            id_update_cond = ops.equal(graph, input_ids_slice, pad_id)\n            id_to_update = ops.where(graph, id_update_cond, next_ids, input_ids_slice)\n            next_iput_ids_container = ops.dynamic_update(\n                graph, input_ids_container, next_step, id_to_update, axes=[1], sizes=[1])\n        return next_iput_ids_container, next_step, next_attention_mask, id_to_update\n\n    def step_loop_cond(self, graph, id_to_update, stop_mask):\n        with graph.nameScope('step_loop_cond'):\n            if self.hf_tokenizer.eos_token_id:\n                eos_token_id = self.hf_tokenizer.eos_token_id\n            else:\n                eos_token_id = self.hf_config.eos_token_id\n            temp_id_to_update = ops.squeeze(graph, id_to_update, [1])\n            eos_id = ops.constant(graph, np.array(eos_token_id), 'eos_id')\n            current_stop_mask = ops.equal(graph, temp_id_to_update, eos_id)\n            current_stop_mask = ops.cast(graph, current_stop_mask, 'INT32')\n            next_stop_mask = ops.bitwiseor(graph, stop_mask, current_stop_mask)\n            if self.early_stop:\n                mask = ops.reduce_sum(graph, stop_mask, axes=[0], keepdims=False)\n                batch_size_constant = ops.constant(\n                    graph, np.array(self.batch_size).astype(np.int32), 'batch_size')\n                keep_going_cond = ops.less(graph, mask, batch_size_constant)\n            else:\n                keep_going_cond = ops.constant(graph, np.array(True).astype(np.bool_), 'cond')\n        return next_stop_mask, keep_going_cond", "\n\nclass HFDec2stageBaseModel(HFDecBaseModel):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.input_length = kwargs.get('input_length', 32)\n        max_loop = kwargs.get('max_loop', None)\n        self.max_loop = max_loop if max_loop else self.max_length - self.input_length\n\n    @abstractmethod\n    def build_model_graph_1st(\n        self,\n        input_ids_container,\n        step,\n        attention_mask,\n        position_ids,\n        init_past_list,\n        sequence_length,\n    ):\n        # return model graph\n        pass\n\n    @abstractmethod\n    def build_model_graph_2nd(\n        self,\n        input_ids_container,\n        step,\n        attention_mask,\n        stop_mask,\n        position_ids,\n        layer_past_list,\n        sequence_length,\n    ):\n        # return model graph\n        pass\n\n    @abstractmethod\n    def build_positions_ids(self, position_ids_container):\n        pass\n\n    def build_graph(self):\n        input_ids_container = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'input_ids')\n        position_ids_container = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'position_ids')\n        attention_mask = self.add_input_tensor(\n            self.graph, 'INT32', [self.batch_size, self.max_length], 'attention_mask')\n        step = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n        stop_mask = ops.constant(\n            self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n        cache_shape = (\n            2,\n            self.batch_size,\n            self.hf_config.num_attention_heads // self.num_replicas,\n            self.max_length,\n            self.hf_config.hidden_size // self.hf_config.num_attention_heads,\n        )\n        init_past_list = [\n            ops.constant(\n                self.graph,\n                np.zeros(cache_shape).astype(self.np_float_type),\n                f\"init_past_{str(i)}\",\n            )\n            for i in range(self.hf_config.num_layers)\n        ]\n        with self.graph.virtualGraph(0):\n            position_ids_stage_1 = ops.static_slice(\n                self.graph, position_ids_container, [0], [self.input_length], [1]\n            )\n            attention_mask = ops.sub(\n                self.graph,\n                attention_mask,\n                ops.constant(self.graph, np.ones(1, dtype=np.int32), '1'),\n            )\n            attention_mask = ops.cast(self.graph, attention_mask, self.popart_float_type)\n            attention_mask = ops.mul(\n                self.graph,\n                attention_mask,\n                ops.constant(self.graph, np.ones(1, dtype=self.np_float_type)*10000, '10000')\n            )\n            attention_mask = ops.unsqueeze(self.graph, attention_mask, [1, 2])\n            attention_mask_stage_1 = ops.static_slice(\n                self.graph,\n                attention_mask,\n                starts=[0],\n                ends=[self.input_length],\n                axes=[3],\n            )\n        input_ids_container, step, _, layer_present_list = self.build_model_graph_1st(\n            input_ids_container,\n            step,\n            attention_mask_stage_1,\n            position_ids_stage_1,\n            init_past_list,\n            self.input_length\n        )\n        with self.graph.virtualGraph(0):\n            position_ids_stage_2 = ops.static_slice(\n                self.graph,\n                position_ids_container,\n                [self.input_length - 1],\n                [self.input_length],\n                [1],\n            )\n            attention_mask_updater = ops.constant(\n                self.graph,\n                np.zeros((self.batch_size, 1, 1, 1), dtype=self.np_float_type),\n                \"attention_mask_updater\",\n            )\n            # next_attention_mask = [0] + attention_mask_container[:-1]\n            attention_mask = ops.static_slice(\n                self.graph, attention_mask, starts=[0], ends=[-1], axes=[3]\n            )\n            attention_mask_stage_2 = ops.concat(\n                self.graph, attention_mask_updater, attention_mask, axis=3\n            )\n\n        stage2_graph = self.build_model_graph_2nd(\n            input_ids_container,\n            step,\n            attention_mask_stage_2,\n            stop_mask,\n            position_ids_stage_2,\n            layer_present_list,\n            1\n        )\n        with self.device_scope(self.graph, 0, 0):\n            output_id, step = ops.loop(\n                self.graph,\n                self.max_loop,\n                [input_ids_container,\n                 step, attention_mask_stage_2,\n                 stop_mask,\n                 position_ids_stage_2] + layer_present_list,\n                stage2_graph\n            )[:2]\n\n        self.add_output_tensor(self.graph, [output_id, step])", ""]}
{"filename": "poptransformer/models/chatglm/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport math\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel", "import numpy as np\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\n\nfrom poptransformer import ops\nfrom poptransformer.layers import LayerNorm, BaseLayer, MLP\nfrom poptransformer.models import HFDec2stageBaseModel\nfrom poptransformer.models.chatglm.embedding import ChatGLMEmbedding\nfrom poptransformer.models.chatglm.attention import RotaryAttention\nfrom poptransformer.models.chatglm.lm_head import LMHead\n", "from poptransformer.models.chatglm.lm_head import LMHead\n\n\ndef vocab_size_calibration(vocab_size,\n                           num_embedding_splits=1,\n                           num_lmhead_splits=2):\n    r1 = math.ceil(vocab_size / num_embedding_splits)\n    r2 = math.ceil((r1 + 2) / num_lmhead_splits)\n    corrected_vocab_size = (r2 * num_lmhead_splits - 2) * num_embedding_splits\n    print(f\"Vocab size adjust from {vocab_size} to {corrected_vocab_size}.\")\n    return corrected_vocab_size", "\n\nclass ChatGLMBlock(BaseLayer):\n    def __init__(\n        self,\n        context,\n        name,\n        input_size,\n        eps,\n        num_attention_heads,\n        num_layers,\n        max_length,\n        layer_index,\n    ):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.eps = eps\n        self.num_attention_heads = num_attention_heads\n        self.max_length = max_length\n        self.num_layers = num_layers\n        self.layer_index = layer_index\n        self.rotary_dim = input_size // (num_attention_heads * 2)\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.layer_norm1 = LayerNorm(\n            self.context, \"input_layernorm\", self.input_size, self.eps\n        )\n        self.attention = RotaryAttention(\n            self.context,\n            \"attention\",\n            self.input_size,\n            self.num_attention_heads,\n            self.max_length,\n            self.layer_index,\n            self.rotary_dim,\n        )\n        self.layer_norm2 = LayerNorm(\n            self.context, \"post_attention_layernorm\", self.input_size, self.eps\n        )\n        self.mlp = MLP(\n            self.context, \"mlp\", self.input_size, self.input_size * 4, \"gelu\"\n        )\n\n    def __call__(\n        self,\n        graph,\n        x,\n        layer_past,\n        position_ids,\n        block_position_ids,\n        sequence_length,\n        step,\n        attention_mask,\n        norm_type=\"ce\",\n        softmax_type=\"ce\",\n        **kwargs,\n    ):\n        matmul_kwargs = {\n            \"amp\": 0.2,\n            \"partialtype\": \"half\" if sequence_length > 1 else \"float\",\n        }\n        with graph.nameScope(self.context):\n            attention_input = self.layer_norm1(\n                graph, x, sequence_length, norm_type)\n            with self.option_scope(**matmul_kwargs):\n                attention_output, layer_present = self.attention(\n                    graph,\n                    attention_input,\n                    layer_past,\n                    position_ids,\n                    block_position_ids,\n                    step,\n                    attention_mask,\n                    sequence_length,\n                    softmax_type,\n                )\n            alpha = ops.constant(graph, np.array([(2.0 * self.num_layers) ** 0.5], dtype=self.np_float_type))\n            temp_x = ops.add(graph, ops.mul(graph, attention_input, alpha), attention_output)\n            mlp_input = self.layer_norm2(graph, temp_x, sequence_length, norm_type)\n            with self.option_scope(**matmul_kwargs):\n                mlp_output = self.mlp(graph, mlp_input)\n            output = ops.add(graph, ops.mul(graph, mlp_input, alpha), mlp_output)\n        return output, layer_present", "\n\nclass Transformer(BaseLayer):\n    def __init__(\n        self,\n        context,\n        name,\n        vocab_size,\n        hidden_size,\n        eps,\n        num_attention_heads,\n        max_length,\n        num_layers,\n        layer_per_ipu,\n        max_position,\n        num_embedding_partitions=4,\n    ):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_embedding_partitions = num_embedding_partitions\n        self.eps = eps\n        self.num_attention_heads = num_attention_heads\n        self.max_length = max_length\n        self.num_layers = num_layers\n        self.layer_per_ipu = layer_per_ipu\n        self.max_position = max_position\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.embedding = ChatGLMEmbedding(\n            self.context,\n            None,\n            self.vocab_size,\n            self.hidden_size,\n            self.num_embedding_partitions,\n        )\n        self.blocks = [\n            ChatGLMBlock(\n                self.context,\n                \"layers.\" + str(layer_index),\n                self.hidden_size,\n                self.eps,\n                self.num_attention_heads,\n                self.num_layers,\n                self.max_length,\n                layer_index,\n            )\n            for layer_index in range(self.num_layers)\n        ]\n        self.layer_norm = LayerNorm(self.context, \"final_layernorm\", self.hidden_size, self.eps)\n\n    def __call__(\n        self,\n        graph,\n        input_ids,\n        layer_past_list,\n        position_ids,\n        block_position_ids,\n        step,\n        attention_mask,\n        sequence_length,\n        **kwargs,\n    ):\n        norm_type = kwargs.get(\"norm_type\", \"ce\")\n        softmax_type = kwargs.get(\"softmax_type\", \"ce\")\n        return_last = kwargs.get(\"return_last\", True)\n        outline_blocks = kwargs.get(\"outline_blocks\", \"single_block\")\n        if outline_blocks:\n            self.logger.info(\"outlining transformer blocks\")\n            self.logger.info(\"please make sure disable outlining in session options is set to False\")\n        with graph.virtualGraph(0):\n            hidden_states = self.embedding(graph, input_ids, sequence_length)\n            hidden_states = ops.replicated_all_reduce(graph, hidden_states)\n        end_points = np.cumsum(self.layer_per_ipu)\n        layer_present_list = []\n        for i in range(self.num_layers):\n            stage_offset = sum(i >= end_points)\n            if outline_blocks is None:\n                outline_attr = None\n            elif outline_blocks == 'single_block':\n                outline_attr = {'block': f'sub_{i}'}\n            elif outline_blocks == 'multi_block':\n                outline_attr = {'block': f'sub_{stage_offset}'}\n            else:\n                raise ValueError(\n                    f'invalid value {outline_blocks} for outline_blocks')\n            with self.device_scope(graph, stage_offset, stage_offset, outline_attr):\n                hidden_states, present = self.blocks[i](\n                    graph,\n                    hidden_states,\n                    layer_past_list[i],\n                    position_ids,\n                    block_position_ids,\n                    sequence_length,\n                    step,\n                    attention_mask,\n                    norm_type,\n                    softmax_type,\n                )\n            self.logger.info(f\"block {i} placed on IPU {stage_offset}\")\n            layer_present_list.append(present)\n\n        with graph.virtualGraph(stage_offset):\n            hidden_states = self.layer_norm(graph, hidden_states, sequence_length, norm_type)\n            if return_last:\n                hidden_states = ops.static_slice(\n                    graph, hidden_states, [sequence_length - 1], [sequence_length], [1]\n                )\n                hidden_states = ops.squeeze(graph, hidden_states, [1])\n        return hidden_states, layer_present_list", "\n\nclass ChatGLMDecModel(HFDec2stageBaseModel):\n    def __init__(self, **kwargs):\n        self.num_embedding_partitions = kwargs.get(\"num_embedding_partitions\", 4)\n        super().__init__(**kwargs)\n        self.outline_blocks = kwargs.get(\"outline_blocks\", True)\n        self.transformer = Transformer(\n            None,\n            \"transformer\",\n            self.hf_config.vocab_size,\n            self.hf_config.hidden_size,\n            self.hf_config.layernorm_epsilon,\n            self.hf_config.num_attention_heads,\n            self.max_length,\n            self.hf_config.num_layers,\n            self.layer_per_ipu,\n            self.hf_config.max_sequence_length,\n            self.num_embedding_partitions,\n        )\n        self.topk = 1\n        self.lm_head = LMHead(\n            context=\"\",\n            name='lm_head',\n            vocab_size=self.hf_config.vocab_size,\n            topk=self.topk,\n            embedding_size=self.hf_config.hidden_size,\n            embedding_weights=self.transformer.embedding.weight_ids,\n            num_embedding_partitions=self.num_embedding_partitions,\n            token_offsets=self.transformer.embedding.token_offsets,\n            embedding_pad_mask=self.transformer.embedding.embedding_pad_mask,\n        )\n        self.lm_head.set_virtual_id(0)\n\n    def prepare_state_dict(self):\n        hf_args = {\n            \"pretrained_model_name_or_path\": self.hf_model_name,\n            \"trust_remote_code\": True,\n            \"revision\": \"v1.1.0\" if self.precision != \"int4\" else \"v0.1.0\",\n            \"cache_dir\": self.hf_cache_dir,\n        }\n        self.hf_tokenizer = AutoTokenizer.from_pretrained(**hf_args)\n        self.logger.info(\n            f\"initialized tokenizer by model_name: {self.hf_model_name}\")\n        self.hf_config = AutoConfig.from_pretrained(**hf_args)\n        if self.precision == \"int4\":\n            if self.model_type == \"shard\":\n                self.hf_config.vocab_size = vocab_size_calibration(\n                                                self.hf_config.vocab_size,\n                                                num_embedding_splits=self.num_embedding_partitions,\n                                                num_lmhead_splits=6)\n        self.hf_config.num_layers = sum(self.layer_per_ipu)\n        self.logger.info(f\"loading pretrained hf model: {self.hf_model_name}\")\n        self.hf_model = AutoModel.from_pretrained(**hf_args).half().eval()\n        self.state_dict = self.hf_model.state_dict()\n        tensor_names = list(self.state_dict.keys())\n        for k in tensor_names:\n            if \"dense_h_to_4h\" in k:\n                self.state_dict[k.replace(\"dense_h_to_4h\", \"c_fc\")] = self.state_dict.pop(k)\n            if \"dense_4h_to_h\" in k:\n                self.state_dict[k.replace(\"dense_4h_to_h\", \"c_proj\")] = self.state_dict.pop(k)\n        self.register_state_dict()\n\n    def build_model_graph_1st(\n        self,\n        input_ids_container,\n        step,\n        attention_mask,\n        position_ids,\n        init_past_list,\n        sequence_length=1,\n    ):\n        model_graph = self.graph\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            input_ids = ops.static_slice(\n                model_graph,\n                input_ids_container,\n                starts=[0],\n                ends=[sequence_length],\n                axes=[1],\n            )\n            # stage 1: block_position_ids = [0, 0, ..., 0]\n            block_position_ids = ops.constant(\n                model_graph,\n                np.zeros((self.batch_size, sequence_length)).astype(np.int32),\n                \"block_position_ids\",\n            )\n        hidden_states, layer_present_list = self.transformer(\n            model_graph,\n            input_ids,\n            init_past_list,\n            position_ids,\n            block_position_ids,\n            step,\n            attention_mask,\n            sequence_length=sequence_length,\n            outline_blocks=self.outline_blocks,\n        )\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            matmul_kwargs = {\n                \"amp\": 0.2,\n                \"partialtype\": \"half\",\n            }\n            if self.precision==\"int4\" and self.model_type==\"shard\":\n                matmul_kwargs.update({\n                    \"serial_factor\": 6,\n                    \"serial_mode\": \"output_channels\"\n                })\n            with self.option_scope(**matmul_kwargs):\n                next_ids = self.lm_head(model_graph, hidden_states, 1)\n\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            next_input_ids = ops.constant(\n                model_graph,\n                np.ones((self.batch_size, 1), dtype=np.int32)\n                * self.hf_tokenizer.bos_token_id,\n                \"decode_start_id\",\n            )\n            # next_ids is useless for stage 2, use sub and add to avoid being pruned\n            next_input_ids = ops.add(model_graph, next_input_ids, next_ids)\n            next_input_ids = ops.sub(model_graph, next_input_ids, next_ids)\n            step_add_value = ops.constant(\n                model_graph, np.array(self.input_length).astype(np.int32), \"1\"\n            )\n            next_step = ops.add(model_graph, step, step_add_value)\n\n            next_iput_ids_container = ops.dynamic_update(\n                model_graph,\n                input_ids_container,\n                next_step,\n                next_input_ids,\n                axes=[1],\n                sizes=[1],\n            )\n        return next_iput_ids_container, next_step, position_ids, layer_present_list\n\n    def build_model_graph_2nd(\n        self,\n        input_ids_container,\n        step,\n        attention_mask,\n        stop_mask,\n        position_ids,\n        layer_past_list,\n        sequence_length=1,\n    ):\n        model_graph = self.graph.createSubgraphBuilder()\n        model_graph.setGraphName(\"model_graph\")\n        # add inputs for loop op\n        self.add_input_tensor(model_graph, \"BOOL\", [], \"cond_place_holder\")\n        self.add_input_tensor(model_graph, \"INT32\", [], \"max_loop_place_holder\")\n        # add inputs for graph\n        input_ids_container = self.add_untyped_input_tensor(model_graph, input_ids_container)\n        step = self.add_untyped_input_tensor(model_graph, step)\n        attention_mask = self.add_untyped_input_tensor(model_graph, attention_mask)\n        stop_mask = self.add_untyped_input_tensor(model_graph, stop_mask)\n        position_ids = self.add_untyped_input_tensor(model_graph, position_ids)\n        layer_past_list = self.add_untyped_input_tensor(model_graph, layer_past_list)\n\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            input_ids = ops.dynamic_slice(\n                model_graph,\n                input_ids_container,\n                step,\n                axes=[1],\n                sizes=[sequence_length],\n            )\n            # stage 2: block_position_ids = [step-input_length+1]\n            constant_pos = ops.constant(\n                model_graph,\n                np.array(self.input_length - 1).astype(np.int32),\n                \"block_pos_constant\",\n            )\n            block_position_ids = ops.sub(model_graph, step, constant_pos)\n\n        hidden_states, layer_present_list = self.transformer(\n            model_graph,\n            input_ids,\n            layer_past_list,\n            position_ids,\n            block_position_ids,\n            step,\n            attention_mask,\n            sequence_length=sequence_length,\n            outline_blocks=self.outline_blocks,\n        )\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            matmul_kwargs = {\n                \"amp\": 0.2,\n                \"partialtype\": \"float\",\n            }\n            if self.precision==\"int4\" and self.model_type==\"shard\":\n                matmul_kwargs.update({\n                    \"serial_factor\": 6,\n                    \"serial_mode\": \"output_channels\"\n                })\n            with self.option_scope(**matmul_kwargs):\n                next_ids = self.lm_head(model_graph, hidden_states, sequence_length)\n\n        with self.device_scope(model_graph, virtual_graph_id=0):\n            next_input_ids = next_ids\n            step_add_value = ops.constant(model_graph, np.array(1).astype(np.int32), \"1\")\n            next_step = ops.add(model_graph, step, step_add_value)\n\n            next_iput_ids_container = ops.dynamic_update(\n                model_graph,\n                input_ids_container,\n                next_step,\n                next_input_ids,\n                axes=[1],\n                sizes=[1],\n            )\n\n            attention_mask_updater = ops.constant(\n                model_graph,\n                np.zeros((self.batch_size, 1, 1, 1), dtype=self.np_float_type),\n                \"attention_mask_updater\",\n            )\n            attention_mask = ops.static_slice(\n                model_graph, attention_mask, starts=[0], ends=[-1], axes=[3]\n            )\n            next_attention_mask = ops.concat(\n                model_graph, attention_mask_updater, attention_mask, axis=3\n            )\n\n            next_stop_mask, keep_going_cond = self.step_loop_cond(\n                model_graph, next_input_ids, stop_mask\n            )\n\n        output_list = [\n            keep_going_cond,\n            next_iput_ids_container,\n            next_step,\n            next_attention_mask,\n            next_stop_mask,\n            position_ids,\n        ]\n\n        self.add_output_tensor(\n            model_graph,\n            output_list + layer_present_list,\n        )\n        return model_graph\n\n    def build_input_dict(self, **kwargs):\n        query = kwargs.get(\"input_string\", \"\u665a\u4e0a\u7761\u4e0d\u7740\u600e\u4e48\u529e\")\n        use_history = kwargs.get(\"use_history\", False)\n        global_round_count = 0\n        if use_history:\n            prompt = f\"[Round {global_round_count}]\\n\u95ee\uff1a{query}\\n\u7b54\uff1a\"\n            round_count = 0\n            for i in range(len(history) - 1, -1, -1):\n                old_query, response = history[i]\n                history_text = f\"[Round {i}]\\n\u95ee\uff1a{old_query}\\n\u7b54\uff1a{response}\\n\"\n                prompt_length = self.hf_tokenizer(\n                    history_text + prompt, return_tensors=\"pt\"\n                )[\"input_ids\"].shape[1]\n                if prompt_length <= self.input_length + 1:  # don't count '130001'\n                    prompt = history_text + prompt\n                    round_count += 1\n                else:\n                    break\n            history = history[-round_count:]\n        else:\n            prompt = query\n\n        inputs = self.hf_tokenizer(prompt, return_tensors=\"pt\", max_length=self.input_length)\n\n        input_ids = inputs[\"input_ids\"][..., :-1]\n        input_ids_container = torch.zeros(1, self.max_length).int()\n        input_ids_container[:, : input_ids.size(1)] = input_ids[0]\n        input_ids_container = input_ids_container.repeat(self.batch_size, 1)\n        position_ids = inputs[\"position_ids\"][:, 0, :-1]\n        last_position_id = inputs[\"position_ids\"][:, 0, -1]\n        position_ids_container = torch.zeros(1, self.max_length).int()\n        position_ids_container[:, : position_ids.size(1)] = position_ids[0]\n        position_ids_container[:, position_ids.size(1):] = last_position_id\n        position_ids_container = position_ids_container.repeat(self.batch_size, 1)\n\n        attention_mask_container = torch.zeros(1, self.max_length).int()\n        # where 1 is masked while 0 is not\n        attention_mask_container[:, : input_ids.size(1)] = 1\n        attention_mask_container = attention_mask_container.repeat(self.batch_size, 1)\n\n        return {\n            \"input_ids\": input_ids_container.numpy(),\n            \"position_ids\": position_ids_container.numpy(),\n            \"attention_mask\": attention_mask_container.numpy(),\n        }\n\n    def build_output_dict(self, anchor_arrays):\n        def process_response(response):\n            response = response.strip()\n            response = response.replace(\"[[\u8bad\u7ec3\u65f6\u95f4]]\", \"2023\u5e74\")\n            punkts = [\n                [\",\", \"\uff0c\"],\n                [\"!\", \"\uff01\"],\n                [\":\", \"\uff1a\"],\n                [\";\", \"\uff1b\"],\n                [\"\\?\", \"\uff1f\"],\n            ]\n            for item in punkts:\n                response = re.sub(\n                    r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response\n                )\n                response = re.sub(\n                    r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response\n                )\n            return response\n\n        output, decode_step = (\n            anchor_arrays[\"Loop:0\"],\n            anchor_arrays[\"Loop:1\"],\n        )\n\n        if len(output.shape) == 3:  # [num_replicas, batch_size, max_length]\n            output, decode_step = output[0], decode_step[0]\n        # Fixme: only support 1 batch.\n        output = output.tolist()[0] # [max_length]\n        if self.hf_config.bos_token_id in output:\n            output = output[output.index(self.hf_config.bos_token_id):]\n        if self.hf_config.eos_token_id in output:\n            output = output[: output.index(self.hf_config.eos_token_id)]\n        output = self.hf_tokenizer.decode(output)\n        output = process_response(output)\n\n        return {\"output\": output, \"decode_step\": decode_step}", ""]}
{"filename": "poptransformer/models/chatglm/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Embedding\n", "from poptransformer.layers import BaseLayer, Embedding\n\n\ndef split_embedding_weight(weight, vocab_size, count, pad_num=0):\n    \"\"\"\n    1. For example vocab_size=150528, count=4, the weight will be splited into 4 parts,\n    Each part has 37634 tokens = 37632 valid + 2 pad:\n\n        Emb1: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n        Emb2: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n        Emb3: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n        Emb4: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n\n    2. Besides, input ids should be cliped to avoid out of bounds:\n\n                    IPU-0             IPU-1              IPU-2                IPU-3\n                |--37632--|      |---37632---|      |---37632---|       |----37632----|\n        input:  [0, ..., 37631, 37632, ..., 75263, 75264, ..., 112895, 112896, ..., 150527]\n\n        input1: clip(input, -1, 37632) -> sub(-1)\n        input2: clip(input, 37631, 75264) -> sub(37631)\n        input3: clip(input, 75263, 112896) -> sub(75263)\n        input4: clip(input, 112895, 150528) -> sub(112895)\n\n        final_input = input1 + input2 + input3 + input4\n    \"\"\"\n    split = []  # [tensor, clip_min, clip_max, offset]\n    assert vocab_size % count == 0\n    num_valid_token = vocab_size // count\n\n    weight_list = np.split(weight, count, 0)\n    pad = np.zeros((1, weight.shape[1]), dtype=weight.dtype)\n    for i in range(count):\n        weight_ = weight_list[i]\n        # Each partial embedding has 2 pad tokens.\n        weight_ = np.concatenate([pad, weight_], axis=0)\n        weight_ = np.concatenate([weight_, pad], axis=0)\n\n        clip_min = i * num_valid_token - 1\n        clip_max = (i + 1) * num_valid_token\n        offset = clip_min\n        split.append([weight_, clip_min, clip_max, offset])\n\n    pad_mask = np.zeros((1, weight_.shape[0]), dtype=weight_.dtype)\n    if pad_num > 0:\n        pad_mask[:, -(pad_num + 1) :] = -1000.0\n    return split, pad_mask", "\n\nclass BaseChatGLMEmbedding(BaseLayer):\n    def __init__(self, context, name, vocab_size, embd_size, num_embedding_partitions=1):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n\n        self.num_embedding_partitions = num_embedding_partitions\n        self.embedding_split_size = vocab_size // num_embedding_partitions + 2\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        raise NotImplementedError\n\n    def __call__(self, input_ids, sequence_length):\n        raise NotImplementedError", "\n\nclass ShardChatGLMEmbedding(BaseChatGLMEmbedding):\n    def collect_bind_layer_weights(self):\n        weight_key = \".\".join([self.context, \"word_embeddings\", \"weight\"])\n        weight_np = self.get_param_from_state_dict(weight_key, None)\n        pad_num = self.vocab_size - weight_np.shape[0]\n        if pad_num > 0:\n            pad_weight = np.zeros((pad_num, weight_np.shape[1]), dtype=weight_np.dtype)\n            weight_np = np.concatenate([weight_np, pad_weight], axis=0)\n        self.weights, self.embedding_pad_mask = split_embedding_weight(\n            weight_np, self.vocab_size, self.num_embedding_partitions, pad_num\n        )\n        self.weight_ids = []\n        self.token_offsets = []\n        self.clips = []\n        for i in range(self.num_embedding_partitions):\n            weight_, clip_min, clip_max, offset = self.weights[i]\n            weight_id = self.add_initialized_input_tensor(weight_, f\"token_weight_{i}\")\n            self.weight_ids.append(weight_id)\n            self.token_offsets.append(offset)\n            self.clips.append([clip_min, clip_max])\n\n    def __call__(self, graph, input_ids, sequence_length):\n        inputs_embeds_list = []\n        with graph.nameScope(self.context):\n            for i in range(self.num_embedding_partitions):\n                with graph.virtualGraph(i):\n                    # input_ids = ops.add(graph, input_ids, ops.constant(graph, np.array([0], dtype=np.int32)))\n                    input_ids_ = ops.clip(\n                        graph, input_ids, self.clips[i][0], self.clips[i][1]\n                    )\n                    input_ids_ = ops.sub(\n                        graph,\n                        input_ids_,\n                        ops.constant(\n                            graph,\n                            np.array([self.token_offsets[i]], dtype=np.int32),\n                            f\"token_offset_{i}\",\n                        ),\n                    )\n                    inputs_embeds_ = ops.gather(graph, self.weight_ids[i], input_ids_)\n                    inputs_embeds_list.append(inputs_embeds_)\n            with graph.virtualGraph(self.num_embedding_partitions - 1):\n                inputs_embeds = ops.sum_op(graph, inputs_embeds_list)\n                inputs_embeds = ops.remap_tensor(graph, inputs_embeds)\n\n        return inputs_embeds", "\n\nclass TPChatGLMEmbedding(BaseChatGLMEmbedding):\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, 'word_embeddings', self.vocab_size, self.embd_size)\n        # For unify the interface\n        self.weight_ids = self.wte.weight_id\n        self.token_offsets = [1]\n        self.embedding_pad_mask = None\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.wte(graph, input_ids, sequence_length)\n            # embeds = ops.remap_tensor(graph, input_embeds)\n            embeds = graph.aiGraphcore.replicatedallreduce([input_embeds])\n        return embeds", "\n\nclass ChatGLMEmbedding(TPChatGLMEmbedding, ShardChatGLMEmbedding):\n    layer_class_map = {\n        \"tp\": TPChatGLMEmbedding,\n        \"shard\": ShardChatGLMEmbedding,\n    }\n\n    def __init__(self, context, name, vocab_size, embd_size, num_embedding_partitions):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(\n                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n            )\n        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(context, name, vocab_size, embd_size, num_embedding_partitions)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/chatglm/lm_head.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Linear\n", "from poptransformer.layers import BaseLayer, Linear\n\n\nclass BaseLMHead(BaseLayer):\n\n    def __init__(self,\n                 context,\n                 name,\n                 vocab_size,\n                 topk,\n                 embedding_size,\n                 embedding_weights,\n                 num_embedding_partitions=1,\n                 token_offsets=None,\n                 embedding_pad_mask=None):\n        super().__init__(context, name)\n        self.embedding_size = embedding_size\n        self.vocab_size = vocab_size\n        self.topk = topk\n        self.embedding_weights = embedding_weights\n        self.num_embedding_partitions = num_embedding_partitions\n        self.embedding_split_size = vocab_size // num_embedding_partitions + 2\n        self.lm_split_count = 6 if self.precision == \"int4\" else 2\n        self.lm_split_size = self.embedding_split_size // self.lm_split_count\n        self.logits_pad_mask = np.zeros(\n            (1, self.embedding_split_size), dtype=self.np_float_type\n        )\n        self.logits_pad_mask[:, 0] = -1000.0\n        self.logits_pad_mask[:, -1] = -1000.0\n        self.token_offsets = token_offsets\n        self.embedding_pad_mask = embedding_pad_mask\n        # for TP\n        self.virtual_id = None\n        self.use_tied_embedding = False\n        self.tie_weight = embedding_weights\n        self.collect_bind_layer_weights()\n\n    def set_virtual_id(self, virtual_id):\n        pass\n\n    def collect_bind_layer_weights(self):\n        pass\n\n    def __call__(self, graph, hidden_states, sequence_length):\n        next_token_prob_list = []\n        next_token_list = []\n        for i in range(self.num_embedding_partitions):\n            with graph.virtualGraph(i):\n                transposed_token_weight_ = ops.transpose(\n                    graph, self.embedding_weights[i], [1, 0]\n                )\n                # (B, H) * (H, V/n + 2) -> (B, V/n + 2)\n                res = []\n                for j in range(self.lm_split_count):\n                    y_ = ops.static_slice(\n                        graph,\n                        transposed_token_weight_,\n                        [self.lm_split_size * j],\n                        [self.lm_split_size * (j+1)],\n                        [1]\n                    )\n                    with self.option_scope(amp=0.2):\n                        o_ = ops.matmul(graph, hidden_states, y_)\n                    res.append(o_)\n                logits_ = ops.concat_sequence(graph, res, axis=1)\n                logits_ = ops.add(\n                    graph,\n                    logits_,\n                    ops.constant(graph, self.logits_pad_mask,\n                                 f\"logits_pad_mask_{i}\"),\n                )\n                if i == self.num_embedding_partitions - 1:\n                    logits_ = ops.add(\n                        graph,\n                        logits_,\n                        ops.constant(\n                            graph,\n                            self.embedding_pad_mask,\n                            f\"embedding_pad_mask_{i}\",\n                        ),\n                    )\n\n                # (B, V/n + 2) -> (B, k)\n                next_token_prob_, next_token_ = ops.topk(\n                    graph, logits_, axis=1, k=self.topk)\n                next_token_ = ops.add(\n                    graph,\n                    next_token_,\n                    ops.constant(\n                        graph,\n                        np.array([self.token_offsets[i]], dtype=np.int32),\n                        f\"token_offset_{i}\",\n                    ),\n                )\n                next_token_prob_list.append(next_token_prob_)\n                next_token_list.append(next_token_)\n\n        with graph.virtualGraph(self.num_embedding_partitions - 1):\n            next_token_probs = ops.concat_sequence(\n                graph, next_token_prob_list, 1)\n            next_tokens = ops.concat_sequence(graph, next_token_list, 1)\n            idx = ops.argmax(graph, next_token_probs, 1)\n            idx = ops.squeeze(graph, idx, [0])\n            next_token = ops.dynamic_slice(graph, next_tokens, idx, [1], [1])\n        return next_token", "\n\nclass TPLMHead(BaseLMHead):\n    def tie_embedding(self, weight_id):\n        self.head.weight_id = weight_id\n        self.use_tied_embedding = True\n        self.logger.info(f'setting weight id to {weight_id}')\n\n    def set_virtual_id(self, virtual_id):\n        self.virtual_id = virtual_id\n        self.logger.info(f'setting virtual id to {virtual_id}')\n\n    def collect_bind_layer_weights(self):\n        if not self.tie_weight:\n            lm_tp_settings = {\n                'strategy_name': 'start',\n            }\n        else:\n            lm_tp_settings = {\n                'strategy_name': 'identity',\n            }\n        self.head = Linear(self.context, None, self.embedding_size,\n                           self.vocab_size, False, **lm_tp_settings)\n        if self.tie_weight:\n            self.tie_embedding(self.tie_weight)\n\n    def __call__(self, graph, logits, sequence_length):\n        with graph.virtualGraph(self.virtual_id):\n            vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n            vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n            index_offset_np = np.expand_dims(\n                np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * vocab_per_ipu\n            index_offset = self.add_initialized_input_tensor(\n                index_offset_np, 'index_offset', **vs_setting)\n            if self.use_tied_embedding:\n                self.head.weight_id = ops.transpose(\n                    graph, self.head.weight_id, [1, 0])\n                # Avoid to transpose twice in 2 stage mode.\n                self.use_tied_embedding = False\n            logits = self.head(graph, logits)\n            logits = ops.unsqueeze(graph, logits, [1])  # Align with sharding\n            pad_idx = ops.equal(graph, ops.constant(graph, np.array(\n                0.0).astype(self.np_float_type), 'zero'), logits)\n            logits = ops.where(\n                graph, pad_idx, ops.constant(graph, np.array(-10000.0).astype(self.np_float_type), '-10000'), logits)\n            next_token_prob, next_token_ = ops.topk(\n                graph, logits, axis=2, k=self.topk)\n            next_token = ops.add(graph, next_token_, index_offset)\n            next_token_prob = ops.replicated_allgather(graph, next_token_prob)\n            next_token_topk = ops.replicated_allgather(graph, next_token)\n            next_token_prob_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n            next_token_prob = ops.transpose(\n                graph,\n                ops.reshape(graph, next_token_prob, next_token_prob_shape),\n                [1, 0]\n            )\n            next_token_topk_shape = [\n                sequence_length * self.topk * self.num_replicas, self.batch_size]\n            next_token_topk = ops.transpose(\n                graph,\n                ops.reshape(graph, next_token_topk, next_token_topk_shape),\n                [1, 0]\n            )\n            next_token_prob = ops.reshape(\n                graph, next_token_prob, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n            next_token_topk = ops.reshape(\n                graph, next_token_topk, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\n            next_token_idx = ops.argmax(graph, next_token_prob, axis=2)  # [B,1,1]\n            next_token_idx = ops.squeeze(graph, next_token_idx, [1, 2])  # (B,)\n            next_token_topk = ops.squeeze(graph, next_token_topk, [1])  # [B,topk * num_replica]\n            next_token = ops.grouped_gather(\n                graph, next_token_topk, next_token_idx, axis=1, group_size=self.batch_size)  # (B,)\n            next_token = ops.reshape(graph, next_token, [self.batch_size, -1])\n            return next_token", "\n\nclass LMHead(TPLMHead, BaseLMHead):\n    layer_class_map = {\"tp\": TPLMHead, \"shard\": BaseLMHead}\n\n    def __init__(\n        self,\n        context,\n        name,\n        vocab_size,\n        topk,\n        embedding_size,\n        embedding_weights,\n        num_embedding_partitions=1,\n        token_offsets=None,\n        embedding_pad_mask=None,\n    ):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(\n                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n            )\n        self.logger.debug(\n            f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(\n            context, name, vocab_size, topk, embedding_size, embedding_weights,\n            num_embedding_partitions, token_offsets, embedding_pad_mask)\n\n    def __call__(self, graph, hidden_states, sequence_length):\n        return self.layer_class.__call__(self, graph, hidden_states, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/chatglm/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear", "from poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\n\n\nclass BaseRotaryAttention(BaseLayer):\n    softmax_fn_map = {\n        \"aionnx\": ops.softmax,\n        \"ce\": ops.softmax_ce,\n    }\n\n    def __init__(self, context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.num_head = num_head\n        self.head_size = self.input_size // self.num_head\n        self.cache_max_length = cache_max_length\n        self.layer_index = layer_index\n        self.rotary_dim = rotary_dim\n        self.scale = input_size // num_head\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.c_attn = Linear(self.context, \"query_key_value\",\n                             self.input_size, self.input_size * 3)\n        self.c_proj = Linear(self.context, \"dense\",\n                             self.input_size, self.input_size)\n\n    def kv_cache(self, graph, kv, layer_past, sequence_length):\n        \"\"\"Implement cache for key-value pairs without using custom op.\n\n        Equivalent to `ops.kv_cache`.\n        \"\"\"\n        with graph.nameScope(\"attn_past_update\"):\n            # layer_past: [2, B, N, L, h]\n            layer_past = ops.static_slice(\n                graph, layer_past, [0], [-sequence_length], [3]\n            )\n            layer_past = ops.concat(graph, kv, layer_past, 3)\n\n        return layer_past\n\n    def forward_qkv(self, graph, x, layer_past, step, position_ids, block_position_ids):\n        qkv = self.c_attn(graph, x)\n        qkv = ops.reshape(\n            graph,\n            qkv,\n            [self.batch_size, self.sequence_length,\n                self.num_head, 3 * self.head_size],\n        )\n\n        temp_splits = [self.head_size] * 3\n        q, k, v = ops.split(graph, qkv, num_outputs=3,\n                            axis=3, splits=temp_splits)\n\n        cos1, sin1 = self.fixed_pos_embedding(graph, position_ids)\n        cos2, sin2 = self.fixed_pos_embedding(graph, block_position_ids)\n\n        q1, q2 = ops.split(\n            graph, q, num_outputs=2, axis=-1, splits=[self.rotary_dim, self.rotary_dim], name=\"rope_split_q\",)\n        k1, k2 = ops.split(\n            graph, k, num_outputs=2, axis=-1, splits=[self.rotary_dim, self.rotary_dim], name=\"rope_split_k\")\n\n        q1, k1 = self.apply_rotary_pos_emb_index(graph, q1, k1, cos1, sin1)\n        q2, k2 = self.apply_rotary_pos_emb_index(graph, q2, k2, cos2, sin2)\n        q = ops.concat(graph, q1, q2, axis=-1)\n        k = ops.concat(graph, k1, k2, axis=-1)\n        q = ops.transpose(graph, q, [0, 2, 1, 3])  # q: [B, N, L, h]\n        kv = ops.concat(graph, k, v, axis=0)\n\n        kv = ops.reshape(\n            graph,\n            kv,\n            shape=[2, self.batch_size, self.sequence_length,\n                   self.num_head, self.head_size]\n        )\n        kv = ops.transpose(graph, kv, perm=[0, 1, 3, 2, 4])\n        layer_present = self.kv_cache(\n            graph, kv, layer_past, self.sequence_length)\n        layer_present = ops.remap_tensor(graph, layer_present)\n        if self.sequence_length != 1 and self.sequence_length < self.cache_max_length:\n            layer_present_temp = kv\n        else:\n            layer_present_temp = layer_present\n        k, v = ops.split(\n            graph, layer_present_temp, 2, axis=0, splits=[1, 1], name=\"split_past\"\n        )\n        k = ops.squeeze(graph, k, [0])\n        v = ops.squeeze(graph, v, [0])\n        return q, k, v, layer_present\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        temp_k = ops.transpose(graph, k, [0, 1, 3, 2])\n        layer_scale_coeff_value = np.array(\n            [self.layer_index + 1], dtype=self.np_float_type)\n        layer_scale_coeff = ops.constant(\n            graph, layer_scale_coeff_value, f\"softmax_scale_{self.layer_index}\"\n        )\n        attention_scale_value = np.array(\n            [1.0 / (math.sqrt(self.head_size) * (self.layer_index + 1))],\n            dtype=self.np_float_type,\n        )\n        attention_scale = ops.constant(\n            graph, attention_scale_value, f\"attention_scale_{self.layer_index}\"\n        )\n        q = ops.mul(graph, q, attention_scale)\n        score = ops.matmul(graph, q, temp_k)\n        if self.sequence_length != 1:\n            score = ops.remap_tensor(graph, score, fwd_after_matmul=True)\n        score = ops.mul(graph, score, layer_scale_coeff)\n        score = ops.add(graph, score, attention_mask)\n        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n        if not softmax_fn:\n            raise ValueError(\n                f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\"\n            )\n        score = softmax_fn(\n            graph, score, -1, stable_mode=self.sequence_length != 1)\n        return score\n\n    def forward_output(self, graph, score, v):\n        score = ops.matmul(graph, score, v)\n        score = ops.transpose(graph, score, [0, 2, 1, 3])\n        score = ops.reshape(\n            graph, score, [self.batch_size, self.sequence_length, -1]\n        )\n        output = self.c_proj(graph, score)\n        return output\n\n    def fixed_pos_embedding(self, graph, position_id):\n        # rotary_dim = hidden_size_per_head // 2\n        # cos, sin = [L, rotary_dim]\n        inv_freq_value = np.array(\n            [\n                1.0 / (10000 ** (i / self.rotary_dim))\n                for i in range(0, self.rotary_dim, 2)\n            ]\n        ).astype(self.np_float_type)\n        inv_freq = ops.constant(graph, inv_freq_value, \"inv_freq\")\n        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n        # position_id -> [L, B]\n        position_id = ops.cast(\n            graph, ops.reshape(graph, position_id,\n                               [-1, 1]), self.popart_float_type\n        )\n        freqs = ops.matmul(graph, position_id, inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = ops.concat(graph, freqs, freqs, axis=-1)\n        # emb -> [L, rotary_dim] -> [1, L, 1, rotary_dim]\n        emb = ops.reshape(graph, emb, shape=[1, -1, 1, self.rotary_dim])\n\n        cos, sin = graph.aiOnnx.cos([emb]), graph.aiOnnx.sin([emb])\n        return cos, sin\n\n    def rotate_half(self, graph, x):\n        x1, x2 = ops.split(\n            graph,\n            x,\n            num_outputs=2,\n            axis=-1,\n            splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n            name=\"rope_split\",\n        )\n        x2 = ops.mul(graph, x2, ops.constant(\n            graph, np.array([-1]).astype(self.np_float_type)))\n        return ops.concat(graph, x2, x1, axis=-1)\n\n    def apply_rotary_pos_emb_index(self, graph, q, k, cos, sin):\n        # position_id: [B, L], q, k: [B, L, N, rotary_dim], sin, cos: [L, rotary_dim] -> [1, L, 1, rotary_dim]\n        q = ops.add(\n            graph,\n            ops.mul(graph, q, cos),\n            ops.mul(graph, self.rotate_half(graph, q), sin)\n        )\n        k = ops.add(\n            graph,\n            ops.mul(graph, k, cos),\n            ops.mul(graph, self.rotate_half(graph, k), sin),\n        )\n        return q, k\n\n    def __call__(\n        self,\n        graph,\n        x,\n        layer_past,\n        position_ids,\n        block_position_ids,\n        step,\n        attention_mask,\n        sequence_length,\n        softmax_type=\"ce\"\n    ):\n        with graph.nameScope(self.context):\n            self.sequence_length = sequence_length\n            q, k, v, layer_present = self.forward_qkv(\n                graph, x, layer_past, step, position_ids, block_position_ids\n            )\n            score = self.forward_attention(\n                graph, q, k, attention_mask, softmax_type)\n            output = self.forward_output(graph, score, v)\n            return output, layer_present", "\n\nclass TPRotaryAttention(BaseRotaryAttention):\n    def collect_bind_layer_weights(self):\n        self.num_head_before_tp = self.num_head\n        self.num_head = self.num_head // self.num_replicas\n        assert self.num_head_before_tp == self.num_head * self.num_replicas, \\\n            f\"Heads {self.num_head_before_tp} can not be exact divided by replicas {self.num_replicas}.\"\n\n        qkv_tp_setting = {\n            'strategy_name': 'start',\n        }\n        proj_tp_setting = {\n            'strategy_name': 'end',\n        }\n        self.c_attn = Linear(\n            self.context, \"query_key_value\", self.input_size, self.input_size * 3, **qkv_tp_setting)\n        self.c_proj = Linear(\n            self.context, \"dense\", self.input_size, self.input_size, **proj_tp_setting)", "\n\nclass RotaryAttention(TPRotaryAttention, BaseRotaryAttention):\n    layer_class_map = {\n        \"tp\": TPRotaryAttention,\n        \"shard\": BaseRotaryAttention,\n    }\n\n    def __init__(self, context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(\n            f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim)\n\n    def __call__(\n        self,\n        graph,\n        x,\n        layer_past,\n        position_ids,\n        block_position_ids,\n        step,\n        attention_mask,\n        sequence_length,\n        softmax_type=\"ce\"\n    ):\n        return self.layer_class.__call__(\n            self, graph, x, layer_past, position_ids, block_position_ids,\n            step, attention_mask, sequence_length, softmax_type,)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\n    def forward_qkv(self, graph, x, layer_past, step, position_ids, block_position_ids):\n        return self.layer_class.forward_qkv(\n            self, graph, x, layer_past, step, position_ids, block_position_ids\n        )\n\n    def forward_output(self, graph, score, v):\n        return self.layer_class.forward_output(self, graph, score, v)\n\n    def fixed_pos_embedding(self, graph, position_id):\n        return self.layer_class.fixed_pos_embedding(self, graph, position_id)\n\n    def rotate_half(self, graph, x):\n        return self.layer_class.rotate_half(self, graph, x)\n\n    def apply_rotary_pos_emb_index(self, graph, q, k, cos, sin):\n        return self.layer_class.apply_rotary_pos_emb_index(self, graph, q, k, cos, sin)", ""]}
{"filename": "poptransformer/models/rwkv/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import LayerNorm, BaseLayer, LMHead\nfrom poptransformer.models.base_model import HFDecBaseModel", "from poptransformer.layers import LayerNorm, BaseLayer, LMHead\nfrom poptransformer.models.base_model import HFDecBaseModel\nfrom poptransformer.models.rwkv.ffn import RWKVFeedforward\nfrom poptransformer.models.rwkv.attention import RWKVAttention\nfrom poptransformer.models.rwkv.embedding import RWKVEmbedding\n\n\nclass RWKVBlock(BaseLayer):\n    def __init__(self, context, name, hidden_size, intermediate_size, attention_hidden_size, eps, layer_id):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.attention_hidden_size = attention_hidden_size\n        self.eps = eps\n        self.layer_id = layer_id\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        if self.layer_id == 0:\n            self.pre_ln = LayerNorm(self.context, 'pre_ln', self.hidden_size, self.eps)\n        self.ln1 = LayerNorm(self.context, 'ln1', self.hidden_size, self.eps)\n        self.ln2 = LayerNorm(self.context, 'ln2', self.hidden_size, self.eps)\n        self.attention = RWKVAttention(\n            self.context, 'attention', self.hidden_size, self.attention_hidden_size, self.layer_id)\n        self.feed_forward = RWKVFeedforward(\n            self.context, 'feed_forward', self.hidden_size, self.intermediate_size, self.layer_id)\n\n    def __call__(self, graph, hidden, layer_state, sequence_length, norm_type='group'):\n        if self.layer_id == 0:\n            hidden = self.pre_ln(graph, hidden, sequence_length, norm_type)\n        temp = self.ln1(graph, hidden, sequence_length, norm_type)\n        attention, layer_state = self.attention(graph, temp, layer_state)\n        hidden = ops.add(graph, hidden, attention)\n        temp = self.ln2(graph, hidden, sequence_length, norm_type)\n        feed_forward, layer_state = self.feed_forward(graph, temp, layer_state)\n        hidden = ops.add(graph, hidden, feed_forward)\n        return hidden, layer_state", "\n\nclass RWKVModel(BaseLayer):\n    def __init__(\n        self,\n        context,\n        name,\n        hidden_size,\n        intermediate_size,\n        attention_hidden_size,\n        eps,\n        vocab_size,\n        num_hidden_layers,\n        rescale_every,\n        layer_per_ipu\n    ):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.attention_hidden_size = attention_hidden_size\n        self.eps = eps\n        self.vocab_size = vocab_size\n        self.num_hidden_layers = num_hidden_layers\n        self.rescale_every = rescale_every\n        self.layer_per_ipu = layer_per_ipu\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.embeddings = RWKVEmbedding(self.context, 'embeddings', self.vocab_size, self.hidden_size)\n        self.blocks = [\n            RWKVBlock(\n                context=self.context,\n                name=f'blocks.{i}',\n                hidden_size=self.hidden_size,\n                intermediate_size=self.intermediate_size,\n                attention_hidden_size=self.attention_hidden_size,\n                eps=self.eps,\n                layer_id=i\n            ) for i in range(self.num_hidden_layers)\n        ]\n        self.ln_out = LayerNorm(self.context, 'ln_out', self.hidden_size, self.eps)\n\n    def __call__(self, graph, input_ids, states, sequence_length, norm_type='ce', **kwargs):\n        hidden = self.embeddings(graph, input_ids, sequence_length)\n        new_states = []\n        outline_blocks = kwargs.get('outline_blocks', None)\n\n        end_points = np.cumsum(self.layer_per_ipu)\n        for index, block in enumerate(self.blocks):\n            stage_offset = sum(index >= end_points)\n            if outline_blocks is None:\n                outline_attr = None\n            elif outline_blocks == 'single_block':\n                outline_attr = {'block': f'sub_{index}'}\n            elif outline_blocks == 'multi_block':\n                outline_attr = {'block': f'sub_{stage_offset}'}\n            else:\n                raise ValueError(f'invalid value {outline_blocks} for outline_blocks')\n            layer_state = states[5 * index: 5 * index + 5]\n            with self.device_scope(graph, stage_offset, None, outline_attr):\n                hidden, layer_state = block(graph, hidden, layer_state, sequence_length, norm_type)\n                if (index + 1) % self.rescale_every == 0:\n                    hidden = ops.div(graph, hidden, ops.constant(graph, np.array(2, dtype=self.np_float_type), '2'))\n                new_states.append(layer_state)\n        hidden = self.ln_out(graph, hidden, sequence_length, norm_type=norm_type)\n        return hidden, new_states", "\n\nclass RWKVDecodeModel(HFDecBaseModel):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.max_length = kwargs.get('max_length')\n        self.layer_per_ipu = kwargs.get('layer_per_ipu')\n        self.outline_blocks = kwargs.get('outline_blocks', True)\n        self.topk = kwargs.get('topk')\n        self.cell = RWKVModel(\n            context=None,\n            name='rwkv',\n            hidden_size=self.hf_config.hidden_size,\n            intermediate_size=self.hf_config.intermediate_size,\n            attention_hidden_size=self.hf_config.attention_hidden_size,\n            eps=self.hf_config.layer_norm_epsilon,\n            vocab_size=self.hf_config.vocab_size,\n            num_hidden_layers=self.hf_config.num_hidden_layers,\n            rescale_every=self.hf_config.rescale_every,\n            layer_per_ipu=self.layer_per_ipu\n        )\n        self.head = LMHead(\n            context=None,\n            name='head',\n            topk=self.topk,\n            vocab_size=self.hf_config.vocab_size,\n            embedding_size=self.hf_config.hidden_size\n        )\n        self.head.set_virtual_id(len(self.layer_per_ipu) - 1)\n\n    def process_hf_model_state_dict(self):\n        with torch.no_grad():\n            for block_id, block in enumerate(self.hf_model.rwkv.blocks):\n                if self.hf_config.rescale_every > 0:\n                    block.attention.output.weight.div_(2 ** int(block_id // self.hf_config.rescale_every))\n                    block.feed_forward.value.weight.div_(2 ** int(block_id // self.hf_config.rescale_every))\n\n    def build_model_graph_inputs(self, graph, inputs):\n        with graph.nameScope('step'):\n            inputs['step'] = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n        with graph.nameScope('stop_mask'):\n            inputs['stop_mask'] = ops.constant(\n                self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n        with graph.nameScope('states'):\n            for i in range(self.hf_config.num_hidden_layers):\n                for j in range(2):\n                    inputs[f'state_{i}_{j}'] = ops.constant(\n                        self.graph,\n                        np.zeros(\n                        (self.batch_size, 1, self.hf_config.hidden_size), dtype=self.np_float_type),\n                        f'layer{i}_{j}'\n                    )\n                for j in range(2, 4):\n                    inputs[f'state_{i}_{j}'] = ops.constant(\n                        self.graph,\n                        np.zeros(\n                            (self.batch_size, 1, self.hf_config.hidden_size // self.num_replicas), dtype=np.float32),\n                        f'layer{i}_{j}'\n                    )\n                inputs[f'state_{i}_{j+1}'] = ops.constant(\n                    self.graph,\n                    np.zeros(\n                        (self.batch_size, 1, self.hf_config.hidden_size // self.num_replicas), dtype=np.float32) - 1e30,\n                    f'layer{i}_{j+1}'\n                )\n        del inputs['position_ids']\n        del inputs['attention_mask']\n        return inputs\n\n    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n        input_ids_container = model_graph_inputs['input_ids_container']\n        step = model_graph_inputs['step']\n        states = [i for i in model_graph_inputs.values() if 'states' in i]\n        with self.device_scope(model_graph, 0, 0):\n            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n            logits, states = self.cell(\n                graph=model_graph,\n                input_ids=input_ids,\n                states=states,\n                sequence_length=sequence_length,\n                norm_type='ce',\n                outline_blocks=self.outline_blocks\n            )\n\n        next_ids = self.head(\n            model_graph,\n            logits,\n            sequence_length=sequence_length\n        )\n        model_outputs =  {'next_ids': next_ids, 'stage_offset': 1}\n        for index_i, i in enumerate(states):\n            for index_j, j in enumerate(i):\n                model_outputs[f'states/layer_{index_i}_{index_j}'] = j\n        return model_outputs\n\n    def build_post_model_graph(self, model_graph, model_graph_inputs, model_outputs):\n        # continue build model graph for post inference step\n        stage_offset = model_outputs['stage_offset']\n        next_ids = model_outputs['next_ids']\n        step = model_graph_inputs['step']\n        input_ids_container = model_graph_inputs['input_ids_container']\n        stop_mask = model_graph_inputs['stop_mask']\n\n        with self.device_scope(model_graph, 0, pipeline_stage_id=stage_offset):\n            next_iput_ids_container, next_step, _, id_to_update= self.step_containers(\n                model_graph, input_ids_container, step, None, next_ids\n            )\n            next_stop_mask, keep_going_cond = self.step_loop_cond(model_graph, id_to_update, stop_mask)\n        self.add_output_tensor(\n            model_graph,\n            [keep_going_cond, next_iput_ids_container, next_step, next_stop_mask] +\n            [v for i, v in model_outputs.items() if 'states' in i]\n        )\n\n    def build_input_dict(self, **kwargs):\n        input_string_list = list(kwargs.get('input_string_list', []))\n        batch_size = self.batch_size * self.batch_per_step\n        if len(input_string_list) >= batch_size:\n            input_string_list = input_string_list[:batch_size]\n            self.logger.info('num input strings is larger than batch size, truncating')\n        else:\n            input_string_list.extend([''] * (batch_size - len(input_string_list)))\n            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n        inputs = self.hf_tokenizer(\n            input_string_list,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='np',\n            return_attention_mask=False,\n            add_special_tokens=False\n        )\n        return {'input_ids': inputs['input_ids']}\n\n    def build_output_dict(self, anchor_arrays):\n        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n        if len(output.shape) == 3:\n            output, decode_step = output[0], decode_step[0]\n        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n        output = {str(i): v for i, v in enumerate(output)}\n        return {'output': output, 'decode_step': decode_step}", ""]}
{"filename": "poptransformer/models/rwkv/ffn.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\n\n\nclass BaseRWKVFeedforward(BaseLayer):\n    def __init__(self, context, name, hidden_size, intermediate_size, layer_id):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.layer_id = layer_id\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.key_linear = Linear(self.context, 'key', self.hidden_size, self.intermediate_size, use_bias=False)\n        self.receptance_linear = Linear(self.context, 'receptance', self.hidden_size, self.hidden_size, use_bias=False)\n        self.value_linear = Linear(self.context, 'value', self.intermediate_size, self.hidden_size, use_bias=False)\n\n        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\n        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\n    def gate(self, graph, a, w, b):\n        y1 = ops.mul(graph, a, w)\n        y2 = ops.sub(graph, ops.constant(graph, np.array(1).astype(self.np_float_type)), w)\n        y2 = ops.mul(graph, y2, b)\n        y = ops.add(graph, y1, y2)\n        return y\n\n    def __call__(self, graph, hidden, layer_state):\n        temp_layer_state = layer_state[0]\n        key = self.gate(graph, hidden, self.time_mix_key, temp_layer_state)\n        receptance = self.gate(graph, hidden, self.time_mix_receptance, temp_layer_state)\n        layer_state[0] = hidden\n        key = self.key_linear(graph, key)\n        key = ops.relu(graph, key)\n        key = ops.mul(graph, key, key)\n        value = self.value_linear(graph, key)\n        receptance = self.receptance_linear(graph, receptance)\n        receptance = ops.sigmoid(graph, receptance)\n        output = ops.mul(graph, receptance, value)\n        return output, layer_state", "\n\nclass BaseRWKVFeedforward(BaseLayer):\n    def __init__(self, context, name, hidden_size, intermediate_size, layer_id):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.layer_id = layer_id\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.key_linear = Linear(self.context, 'key', self.hidden_size, self.intermediate_size, use_bias=False)\n        self.receptance_linear = Linear(self.context, 'receptance', self.hidden_size, self.hidden_size, use_bias=False)\n        self.value_linear = Linear(self.context, 'value', self.intermediate_size, self.hidden_size, use_bias=False)\n\n        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\n        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\n    def gate(self, graph, a, w, b):\n        y1 = ops.mul(graph, a, w)\n        y2 = ops.sub(graph, ops.constant(graph, np.array(1).astype(self.np_float_type)), w)\n        y2 = ops.mul(graph, y2, b)\n        y = ops.add(graph, y1, y2)\n        return y\n\n    def __call__(self, graph, hidden, layer_state):\n        temp_layer_state = layer_state[0]\n        key = self.gate(graph, hidden, self.time_mix_key, temp_layer_state)\n        receptance = self.gate(graph, hidden, self.time_mix_receptance, temp_layer_state)\n        layer_state[0] = hidden\n        key = self.key_linear(graph, key)\n        key = ops.relu(graph, key)\n        key = ops.mul(graph, key, key)\n        value = self.value_linear(graph, key)\n        receptance = self.receptance_linear(graph, receptance)\n        receptance = ops.sigmoid(graph, receptance)\n        output = ops.mul(graph, receptance, value)\n        return output, layer_state", "\n\nclass TPRWKVFeedforward(BaseRWKVFeedforward):\n    def collect_bind_layer_weights(self):\n        key_tp_setting = {\n            'strategy_name': 'start',\n        }\n        value_tp_setting = {\n            'strategy_name': 'end',\n        }\n        self.key_linear = Linear(\n            self.context, 'key', self.hidden_size, self.intermediate_size, use_bias=False, **key_tp_setting)\n        self.receptance_linear = Linear(\n            self.context, 'receptance', self.hidden_size, self.hidden_size, use_bias=False, **key_tp_setting)\n        self.value_linear = Linear(\n            self.context, 'value', self.intermediate_size, self.hidden_size, use_bias=False, **value_tp_setting)\n\n        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\n        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\n    def __call__(self, graph, hidden, layer_state):\n        temp_layer_state = layer_state[0]\n        key = self.gate(graph, hidden, self.time_mix_key, temp_layer_state)\n        receptance = self.gate(graph, hidden, self.time_mix_receptance, temp_layer_state)\n        layer_state[0] = hidden\n        key = self.key_linear(graph, key)\n        key = ops.relu(graph, key)\n        key = ops.mul(graph, key, key)\n        value = self.value_linear(graph, key)\n        receptance = self.receptance_linear(graph, receptance)\n        receptance = ops.replicated_allgather(graph, receptance)\n        receptance = ops.sigmoid(graph, receptance)\n        output = ops.mul(graph, receptance, value)\n        return output, layer_state", "\n\nclass RWKVFeedforward(TPRWKVFeedforward, BaseRWKVFeedforward):\n    layer_class_map = {\n        'tp': TPRWKVFeedforward,\n        'shard': BaseRWKVFeedforward}\n\n    def __init__(self, context, name, hidden_size, intermediate_size, layer_id):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, hidden_size, intermediate_size, layer_id)\n\n    def __call__(self, graph, hidden, layer_state):\n        return self.layer_class.__call__(self, graph, hidden, layer_state)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/rwkv/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Embedding\n\n\nclass BaseRWKVEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.embedding = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.embedding(graph, input_ids, sequence_length)\n        return input_embeds", "\nclass BaseRWKVEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.embedding = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.embedding(graph, input_ids, sequence_length)\n        return input_embeds", "\n\nclass TPRWKVEmbedding(BaseRWKVEmbedding):\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.embedding(graph, input_ids, sequence_length)\n            input_embeds = graph.aiGraphcore.replicatedallreduce([input_embeds])\n        return input_embeds\n", "\n\nclass RWKVEmbedding(TPRWKVEmbedding, BaseRWKVEmbedding):\n    layer_class_map = {\n        'tp': TPRWKVEmbedding,\n        'shard': BaseRWKVEmbedding}\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, vocab_size, embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/rwkv/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\nfrom poptransformer.utils.param_handler.tensor_parallel_strategy import shard\n", "from poptransformer.utils.param_handler.tensor_parallel_strategy import shard\n\n\nclass BaseRWKVAttention(BaseLayer):\n    def __init__(self, context, name, hidden_size, attention_hidden_size, layer_id):\n        super().__init__(context, name)\n\n        self.hidden_size = hidden_size\n        self.attention_hidden_size = attention_hidden_size\n        self.layer_id = layer_id\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.key_linear = Linear(\n            self.context, 'key', self.hidden_size, self.attention_hidden_size, use_bias=False)\n        self.receptance_linear = Linear(\n            self.context, 'receptance', self.hidden_size, self.attention_hidden_size, use_bias=False)\n        self.value_linear = Linear(\n            self.context, 'value', self.hidden_size, self.attention_hidden_size, use_bias=False)\n        self.output_linear = Linear(\n            self.context, 'output', self.attention_hidden_size, self.hidden_size, use_bias=False)\n\n        time_decay_key = '.'.join([self.context, 'time_decay'])\n        time_decay_np = self.get_param_from_state_dict(time_decay_key, [self.hidden_size])\n        self.time_decay = self.add_initialized_input_tensor(time_decay_np, time_decay_key)\n\n        time_first_key = '.'.join([self.context, 'time_first'])\n        time_first_np = self.get_param_from_state_dict(time_first_key, [self.hidden_size])\n        self.time_first = self.add_initialized_input_tensor(time_first_np, time_first_key)\n\n        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\n        time_mix_value_name = '.'.join([self.context, 'time_mix_value'])\n        time_mix_value_np = self.get_param_from_state_dict(time_mix_value_name, [1, 1, self.hidden_size])\n        self.time_mix_value = self.add_initialized_input_tensor(time_mix_value_np, time_mix_value_name)\n\n        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\n    def gate(self, graph, a, w, b):\n        y1 = ops.mul(graph, a, w)\n        y2 = ops.sub(graph, ops.constant(graph, np.array(1).astype(self.np_float_type)), w)\n        y2 = ops.mul(graph, y2, b)\n        y = ops.add(graph, y1, y2)\n        return y\n\n    def __call__(self, graph, hidden, layer_state):\n        with graph.nameScope('extract_key_value'):\n            num_state, den_state, max_state = layer_state[2:]\n            key = self.gate(graph, hidden, self.time_mix_key, layer_state[1])\n            value = self.gate(graph, hidden, self.time_mix_value, layer_state[1])\n            receptance = self.gate(graph, hidden, self.time_mix_receptance, layer_state[1])\n            layer_state[1] = hidden\n            key = self.key_linear(graph, key)\n            value = self.value_linear(graph, value)\n            receptance = self.receptance_linear(graph, receptance)\n            receptance = ops.sigmoid(graph, receptance)\n\n        if self.precision == 'fp16':\n            time_decay = ops.cast(graph, self.time_decay, 'FLOAT')\n            key = ops.cast(graph, key, 'FLOAT')\n            value = ops.cast(graph, value, 'FLOAT')\n            time_first = ops.cast(graph, self.time_first, 'FLOAT')\n\n        with graph.nameScope('rwkv_linear_attention'):\n            temp1 = ops.add(graph, key, time_first)\n            max_for_output = ops.maximum(graph, max_state, temp1)\n            e1 = ops.exp(graph, ops.sub(graph, max_state, max_for_output))\n            e2 = ops.exp(graph, ops.sub(graph, temp1, max_for_output))\n            numerator = ops.add(graph, ops.mul(graph, e1, num_state), ops.mul(graph, e2, value))\n            denominator = ops.add(graph, ops.mul(graph, e1, den_state), e2)\n            output = ops.div(graph, numerator, denominator)\n\n            time_decay = ops.exp(graph, time_decay)\n            temp2 = ops.sub(graph, max_state, time_decay)\n            max_for_state = ops.maximum(graph, temp2, key)\n            e1 = ops.exp(graph, ops.sub(graph, temp2, max_for_state))\n            e2 = ops.exp(graph, ops.sub(graph, key, max_for_state))\n            layer_state[2] = ops.add(graph, ops.mul(graph, e1, num_state), ops.mul(graph, e2, value))\n            layer_state[3] = ops.add(graph, ops.mul(graph, e1, den_state), e2)\n            layer_state[4] = max_for_state\n        if self.precision == 'fp16':\n            output = ops.cast(graph, output, self.popart_float_type)\n        output = ops.mul(graph, receptance, output)\n        output = self.output_linear(graph, output)\n        return output, layer_state", "\n\nclass TPRWKVAttention(BaseRWKVAttention):\n\n    def collect_bind_layer_weights(self):\n        key_tp_setting = {\n            'strategy_name': 'start',\n        }\n        output_tp_setting = {\n            'strategy_name': 'end',\n        }\n        self.key_linear = Linear(\n            self.context, 'key', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n        self.receptance_linear = Linear(\n            self.context, 'receptance', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n        self.value_linear = Linear(\n            self.context, 'value', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n        self.output_linear = Linear(\n            self.context, 'output', self.attention_hidden_size, self.hidden_size, use_bias=False, **output_tp_setting)\n\n        vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n        time_decay_key = '.'.join([self.context, 'time_decay'])\n        time_decay_np = self.get_param_from_state_dict(time_decay_key, [self.hidden_size])\n        time_decay_np = shard(time_decay_np, self.num_replicas, -1)\n        self.time_decay = self.add_initialized_input_tensor(time_decay_np, time_decay_key, **vs_setting)\n\n        time_first_key = '.'.join([self.context, 'time_first'])\n        time_first_np = self.get_param_from_state_dict(time_first_key, [self.hidden_size])\n        time_first_np = shard(time_first_np, self.num_replicas, -1)\n        self.time_first = self.add_initialized_input_tensor(time_first_np, time_first_key, **vs_setting)\n\n        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\n        time_mix_value_name = '.'.join([self.context, 'time_mix_value'])\n        time_mix_value_np = self.get_param_from_state_dict(time_mix_value_name, [1, 1, self.hidden_size])\n        self.time_mix_value = self.add_initialized_input_tensor(time_mix_value_np, time_mix_value_name)\n\n        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)", "\n\nclass RWKVAttention(TPRWKVAttention, BaseRWKVAttention):\n    layer_class_map = {\n        'tp': TPRWKVAttention,\n        'shard': BaseRWKVAttention}\n\n    def __init__(self, context, name, hidden_size, attention_hidden_size, layer_id):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, hidden_size, attention_hidden_size, layer_id)\n\n    def __call__(self, graph, hidden, layer_state):\n        return self.layer_class.__call__(self, graph, hidden, layer_state)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/gpt2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport numpy as np\nfrom poptransformer import ops\nfrom poptransformer.layers import MLP, LayerNorm, BaseLayer, LMHead\nfrom poptransformer.models import HFDecBaseModel", "from poptransformer.layers import MLP, LayerNorm, BaseLayer, LMHead\nfrom poptransformer.models import HFDecBaseModel\nfrom poptransformer.models.gpt2.embedding import TransformerEmbedding\nfrom poptransformer.models.gpt2.attention import Attention\n\n\nclass TransformerBlock(BaseLayer):\n    def __init__(self, context, name, input_size, eps, n_head, max_length):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.eps = eps\n        self.n_head = n_head\n        self.max_length = max_length\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.layer_norm1 = LayerNorm(self.context, 'ln_1', self.input_size, self.eps)\n        self.attention = Attention(\n            self.context, 'attn', self.input_size, self.n_head, self.max_length)\n        self.layer_norm2 = LayerNorm(self.context, 'ln_2', self.input_size, self.eps)\n        self.mlp = MLP(self.context, 'mlp', self.input_size, self.input_size * 4, 'gelu')\n\n    def __call__(self, graph, x, sequence_length, step, attention_mask, norm_type='ce', softmax_type='ce', **kwargs):\n        with graph.nameScope(self.context):\n            temp_x = self.layer_norm1(graph, x, sequence_length, norm_type)\n            temp_x = self.attention(graph, temp_x, step, attention_mask, sequence_length, softmax_type)\n            x = ops.add(graph, x, temp_x)\n            temp_x = self.layer_norm2(graph, x, sequence_length, norm_type)\n            temp_x = self.mlp(graph, temp_x)\n            x = ops.add(graph, x, temp_x)\n        return x", "\n\nclass Transformer(BaseLayer):\n    def __init__(self, context, name, vocab_size, embd_size, eps,\n                 n_head, max_length, n_layer, layer_per_ipu, max_position):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.eps = eps\n        self.n_head = n_head\n        self.max_length = max_length\n        self.n_layer = n_layer\n        self.layer_per_ipu = layer_per_ipu\n        self.max_position = max_position\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n\n        self.embedding = TransformerEmbedding(\n            self.context,\n            None,\n            self.vocab_size,\n            self.embd_size,\n            self.max_position\n        )\n        self.blocks = [\n            TransformerBlock(\n                self.context,\n                'h.'+str(i),\n                self.embd_size,\n                self.eps,\n                self.n_head,\n                self.max_length,\n            )\n            for i in range(self.n_layer)\n        ]\n        self.layer_norm = LayerNorm(self.context, 'ln_f', self.embd_size, self.eps)\n\n    def __call__(self, graph, input_ids, position_ids, step, attention_mask, sequence_length, **kwargs):\n        # TODO type hints  sequence length is int here\n        norm_type = kwargs.get('norm_type', 'ce')\n        softmax_type = kwargs.get('softmax_type', 'ce')\n        return_last = kwargs.get('return_last', False)\n        outline_blocks = kwargs.get('outline_blocks', 'single_block')\n        if outline_blocks:\n            self.logger.info('outlining transformer blocks')\n            self.logger.info('please make sure disable outlining in session options is set to False')\n        with self.device_scope(graph, 0, 0):\n            hidden_states = self.embedding(graph, input_ids, position_ids, sequence_length)\n\n        end_points = np.cumsum(self.layer_per_ipu)\n        for i in range(self.n_layer):\n            stage_offset = sum(i >= end_points)\n            if outline_blocks is None:\n                outline_attr = None\n            elif outline_blocks == 'single_block':\n                outline_attr = {'block': f'sub_{i}'}\n            elif outline_blocks == 'multi_block':\n                outline_attr = {'block': f'sub_{stage_offset}'}\n            else:\n                raise ValueError(f'invalid value {outline_blocks} for outline_blocks')\n            with self.device_scope(graph, stage_offset, stage_offset, outline_attr):\n                hidden_states = self.blocks[i](\n                    graph, hidden_states, sequence_length, step, attention_mask, norm_type, softmax_type)\n                self.logger.info(f'block {i} placed on IPU {stage_offset}')\n\n        with self.device_scope(graph, stage_offset, stage_offset):\n            if return_last:\n                hidden_states = ops.static_slice(graph, hidden_states, [0], [1], [1])\n            last_sequence_length = 1 if return_last else sequence_length\n            hidden_states = self.layer_norm(graph, hidden_states, last_sequence_length, norm_type)\n\n        return hidden_states, stage_offset", "\n\nclass GPT2DecModel(HFDecBaseModel):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # TODO support topk > 1 for TP\n        self.topk = kwargs.get('topk', 1)\n        if self.model_type == 'tp':\n            assert self.topk == 1\n        self.outline_blocks = kwargs.get('outline_blocks', True)\n        self.transformer = Transformer(\n            None,\n            'transformer',\n            self.hf_config.vocab_size,\n            self.hf_config.n_embd,\n            self.hf_config.layer_norm_epsilon,\n            self.hf_config.n_head,\n            self.max_length,\n            self.hf_config.n_layer,\n            self.layer_per_ipu,\n            self.hf_config.n_positions,\n        )\n        self.lm_head = LMHead(\n            context=None,\n            name='lm_head',\n            topk=self.topk,\n            vocab_size=self.hf_config.vocab_size,\n            embedding_size=self.hf_config.n_embd,\n            tie_weight=self.transformer.embedding.wte.weight_id\n        )\n        self.lm_head.set_virtual_id(0)\n\n    def process_hf_model_state_dict(self):\n        with torch.no_grad():\n            n_embd = self.hf_config.n_embd\n            head_size = n_embd // self.hf_config.n_head\n            scale_value = float(1 / head_size ** 0.5)\n            for block in self.hf_model.transformer.h:\n                block.attn.c_attn.weight[:, :n_embd] = block.attn.c_attn.weight[:, :n_embd] * scale_value\n                block.attn.c_attn.bias[: n_embd] = block.attn.c_attn.bias[:n_embd] * scale_value\n                block.attn.c_attn.weight.transpose_(0, 1)\n                block.attn.c_proj.weight.transpose_(0, 1)\n                block.mlp.c_fc.weight.transpose_(0, 1)\n                block.mlp.c_proj.weight.transpose_(0, 1)\n        self.logger.info('prescale applied')\n\n    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n        input_ids_container = model_graph_inputs['input_ids_container']\n        attention_mask = model_graph_inputs['attention_mask']\n        step = model_graph_inputs['step']\n        with self.device_scope(model_graph, 0, 0):\n            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n\n            if sequence_length != 1:\n                position_ids_value = np.array([np.arange(self.max_length)] * self.batch_size)\n                position_ids_container = ops.constant(\n                    model_graph, position_ids_value.astype(np.int32), 'position_ids')\n                position_ids = ops.dynamic_slice(model_graph, position_ids_container, step, [1], [sequence_length])\n            else:\n                position_ids = ops.unsqueeze(model_graph, step, [0])\n\n        logits, stage_offset = self.transformer(\n            model_graph,\n            input_ids,\n            position_ids,\n            step,\n            temp_attention_mask,\n            sequence_length=sequence_length,\n            outline_blocks=self.outline_blocks\n        )\n\n        with self.device_scope(model_graph, pipeline_stage_id=stage_offset):\n            next_ids = self.lm_head(\n                model_graph,\n                logits,\n                sequence_length=sequence_length\n            )\n        model_outputs =  {'next_ids': next_ids, 'stage_offset': stage_offset}\n        return model_outputs\n\n    def build_input_dict(self, **kwargs):\n        input_string_list = list(kwargs.get('input_string_list', []))\n        batch_size = self.batch_size * self.batch_per_step\n        if len(input_string_list) >= batch_size:\n            input_string_list = input_string_list[:batch_size]\n            self.logger.info('num input strings is larger than batch size, truncating')\n        else:\n            input_string_list.extend([''] * (batch_size - len(input_string_list)))\n            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n\n        inputs = self.hf_tokenizer(\n            input_string_list,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='np',\n            return_attention_mask=False,\n            add_special_tokens=False\n        )\n        return {'input_ids': inputs['input_ids']}\n\n    def build_output_dict(self, anchor_arrays):\n        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n        if len(output.shape) == 3:\n            output, decode_step = output[0], decode_step[0]\n        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n        output = {str(i): v for i, v in enumerate(output)}\n        return {'output': output, 'decode_step': decode_step}", ""]}
{"filename": "poptransformer/models/gpt2/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Embedding\n\n\nclass BaseTransformerEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size, max_position):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.max_position = max_position\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, 'wte', self.vocab_size, self.embd_size)\n        self.wpe = Embedding(self.context, 'wpe', self.max_position, self.embd_size)\n\n    def __call__(self, graph, input_ids, position_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.wte(graph, input_ids, sequence_length)\n            pos_embeds = self.wpe(graph, position_ids, sequence_length)\n            embeds = ops.add(graph, input_embeds, pos_embeds)\n        return ops.remap_tensor(graph, embeds)", "\nclass BaseTransformerEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size, max_position):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.max_position = max_position\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, 'wte', self.vocab_size, self.embd_size)\n        self.wpe = Embedding(self.context, 'wpe', self.max_position, self.embd_size)\n\n    def __call__(self, graph, input_ids, position_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.wte(graph, input_ids, sequence_length)\n            pos_embeds = self.wpe(graph, position_ids, sequence_length)\n            embeds = ops.add(graph, input_embeds, pos_embeds)\n        return ops.remap_tensor(graph, embeds)", "\n\nclass TPTransformerEmbedding(BaseTransformerEmbedding):\n\n    def __call__(self, graph, input_ids, position_ids, sequence_length):\n        with graph.nameScope(self.context):\n            input_embeds = self.wte(graph, input_ids, sequence_length)\n            pos_embeds = self.wpe(graph, position_ids, sequence_length)\n            embeds = ops.add(graph, input_embeds, pos_embeds)\n            embeds = ops.remap_tensor(graph, embeds)\n            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n        return embeds", "\n\nclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n    layer_class_map = {\n        'tp': TPTransformerEmbedding,\n        'shard': BaseTransformerEmbedding}\n\n    def __init__(self, context, name, vocab_size, embd_size, max_position):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, vocab_size, embd_size, max_position)\n\n    def __call__(self, graph, input_ids, position_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, position_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/gpt2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\n\n\nclass BaseAttention(BaseLayer):\n    softmax_fn_map = {\n        'aionnx': ops.softmax,\n        'ce': ops.softmax_ce,\n    }\n    def __init__(self, context, name, input_size, num_head, cache_max_length):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.num_head = num_head\n        self.head_size = self.input_size // self.num_head\n        self.cache_max_length = cache_max_length\n        self.scale = input_size // num_head\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.c_attn = Linear(self.context, 'c_attn', self.input_size, self.input_size * 3)\n        self.c_proj = Linear(self.context, 'c_proj', self.input_size, self.input_size)\n\n    def forward_qkv(self, graph, x, step):\n        qkv = self.c_attn(graph, x)\n        temp_splits = [self.input_size, self.input_size * 2]\n        q, kv = ops.split(graph, qkv, num_outputs=2, axis=2, splits=temp_splits)\n\n        temp_q_shape = [self.batch_size, self.sequence_length, self.num_head, self.head_size]\n        q = ops.reshape(graph, q, temp_q_shape)\n\n        temp_kv_shape = [self.batch_size, self.sequence_length, 2, self.num_head, self.head_size]\n        kv = ops.reshape(graph, kv, temp_kv_shape)\n\n        q = ops.transpose(graph, q, [0, 2, 1, 3])\n        kv = ops.transpose(graph, kv, [2, 0, 3, 1, 4])\n        layer_past = ops.kv_cache(graph, step, kv, self.cache_max_length, 3, self.sequence_length)\n        layer_past = ops.remap_tensor(graph, layer_past)\n        if self.sequence_length != 1 and self.sequence_length < self.cache_max_length:\n            layer_past = ops.static_slice(graph, layer_past, [0], [self.sequence_length], [3])\n        k, v = ops.split(graph, layer_past, 2, axis=0, splits=[1, 1], name='split_past')\n        k = ops.squeeze(graph, k, [0])\n        v = ops.squeeze(graph, v, [0])\n        return q, k, v\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        temp_k = ops.transpose(graph, k, [0, 1, 3, 2])\n        score = ops.matmul(graph, q, temp_k)\n        if self.sequence_length != 1:\n            score = ops.remap_tensor(graph, score, fwd_after_matmul=True)\n\n        score = ops.add(graph, score, attention_mask)\n        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n        if not softmax_fn:\n            raise ValueError(f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\")\n        score = softmax_fn(graph, score, -1, stable_mode=self.sequence_length != 1)\n        return score\n\n    def forward_output(self, graph, score, v):\n        score = ops.matmul(graph, score, v)\n        score = ops.transpose(graph, score, [0, 2, 1, 3])\n        score = ops.reshape(graph, score, [self.batch_size, self.sequence_length, self.input_size])\n        output = self.c_proj(graph, score)\n        return output\n\n    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n        with graph.nameScope(self.context):\n            self.sequence_length = sequence_length\n            q, k, v = self.forward_qkv(graph, x, step)\n            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n            output = self.forward_output(graph, score, v)\n            return output", "\n\nclass BaseAttention(BaseLayer):\n    softmax_fn_map = {\n        'aionnx': ops.softmax,\n        'ce': ops.softmax_ce,\n    }\n    def __init__(self, context, name, input_size, num_head, cache_max_length):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.num_head = num_head\n        self.head_size = self.input_size // self.num_head\n        self.cache_max_length = cache_max_length\n        self.scale = input_size // num_head\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.c_attn = Linear(self.context, 'c_attn', self.input_size, self.input_size * 3)\n        self.c_proj = Linear(self.context, 'c_proj', self.input_size, self.input_size)\n\n    def forward_qkv(self, graph, x, step):\n        qkv = self.c_attn(graph, x)\n        temp_splits = [self.input_size, self.input_size * 2]\n        q, kv = ops.split(graph, qkv, num_outputs=2, axis=2, splits=temp_splits)\n\n        temp_q_shape = [self.batch_size, self.sequence_length, self.num_head, self.head_size]\n        q = ops.reshape(graph, q, temp_q_shape)\n\n        temp_kv_shape = [self.batch_size, self.sequence_length, 2, self.num_head, self.head_size]\n        kv = ops.reshape(graph, kv, temp_kv_shape)\n\n        q = ops.transpose(graph, q, [0, 2, 1, 3])\n        kv = ops.transpose(graph, kv, [2, 0, 3, 1, 4])\n        layer_past = ops.kv_cache(graph, step, kv, self.cache_max_length, 3, self.sequence_length)\n        layer_past = ops.remap_tensor(graph, layer_past)\n        if self.sequence_length != 1 and self.sequence_length < self.cache_max_length:\n            layer_past = ops.static_slice(graph, layer_past, [0], [self.sequence_length], [3])\n        k, v = ops.split(graph, layer_past, 2, axis=0, splits=[1, 1], name='split_past')\n        k = ops.squeeze(graph, k, [0])\n        v = ops.squeeze(graph, v, [0])\n        return q, k, v\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        temp_k = ops.transpose(graph, k, [0, 1, 3, 2])\n        score = ops.matmul(graph, q, temp_k)\n        if self.sequence_length != 1:\n            score = ops.remap_tensor(graph, score, fwd_after_matmul=True)\n\n        score = ops.add(graph, score, attention_mask)\n        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n        if not softmax_fn:\n            raise ValueError(f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\")\n        score = softmax_fn(graph, score, -1, stable_mode=self.sequence_length != 1)\n        return score\n\n    def forward_output(self, graph, score, v):\n        score = ops.matmul(graph, score, v)\n        score = ops.transpose(graph, score, [0, 2, 1, 3])\n        score = ops.reshape(graph, score, [self.batch_size, self.sequence_length, self.input_size])\n        output = self.c_proj(graph, score)\n        return output\n\n    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n        with graph.nameScope(self.context):\n            self.sequence_length = sequence_length\n            q, k, v = self.forward_qkv(graph, x, step)\n            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n            output = self.forward_output(graph, score, v)\n            return output", "\n\nclass TPAttention(BaseAttention):\n\n    def collect_bind_layer_weights(self):\n        qkv_tp_setting = {'strategy_name': 'fused_qkv'}\n        self.c_attn = Linear(self.context, 'c_attn', self.input_size, self.input_size * 3, **qkv_tp_setting)\n        proj_tp_setting = {'strategy_name': 'end'}\n        self.c_proj = Linear(self.context, 'c_proj', self.input_size, self.input_size, **proj_tp_setting)\n        self.input_size = self.input_size // self.num_replicas\n        self.num_head = self.num_head // self.num_replicas", "\n\nclass Attention(TPAttention, BaseAttention):\n    layer_class_map = {\n        'tp': TPAttention,\n        'shard': BaseAttention}\n\n    def __init__(self, context, name, input_size, num_head, cache_max_length):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, num_head, cache_max_length)\n\n    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n        return self.layer_class.__call__(self, graph, x, step, attention_mask, sequence_length, softmax_type)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\n    def forward_qkv(self, graph, x, step):\n        return self.layer_class.forward_qkv(self, graph, x, step)\n\n    def forward_output(self, graph, score, v):\n        return self.layer_class.forward_output(self, graph, score, v)", ""]}
{"filename": "poptransformer/models/llama2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# This is a re-implementation of Llama 2 by Graphcore Ltd\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nimport numpy as np\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig\n\nfrom poptransformer import ops", "\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.models.llama2.embedding import TransformerEmbedding\nfrom poptransformer.models.llama2.mlp import MLP\nfrom poptransformer.layers.rms_layer_norm import RMSLayerNorm\nfrom poptransformer.models.llama2.attention import Attention\nfrom poptransformer.layers.lm_head import LMHead\nfrom poptransformer.models import HFDecBaseModel\n", "from poptransformer.models import HFDecBaseModel\n\n\nclass TransformerBlock(BaseLayer):\n    def __init__(self, context, name, input_size, eps, n_head,intermediate_size, max_length, fp8_cache=False):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.eps = eps\n        self.n_head = n_head\n        self.intermediate_size = intermediate_size\n        self.max_length = max_length\n        self.fp8_cache = fp8_cache\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.layer_norm1 = RMSLayerNorm(self.context, 'input_layernorm', self.input_size, self.eps)\n        self.attention = Attention(\n            self.context, 'self_attn', self.input_size, self.n_head, self.max_length, self.fp8_cache)\n        self.layer_norm2 = RMSLayerNorm(self.context, 'post_attention_layernorm', self.input_size, self.eps)\n        self.mlp = MLP(self.context, 'mlp', self.input_size, self.intermediate_size, 'swish')\n\n    def __call__(self, graph, x, sequence_length, step, attention_mask, norm_type='ce', softmax_type='ce', **kwargs):\n        with graph.nameScope(self.context):\n            temp_x = self.layer_norm1(graph, x)\n            temp_x = self.attention(graph, temp_x, step, attention_mask, sequence_length, softmax_type)\n            x = ops.add(graph, x, temp_x)\n            temp_x = self.layer_norm2(graph, x)\n            temp_x = self.mlp(graph, temp_x)\n            x = ops.add(graph, x, temp_x)\n        return x", "\n\nclass Transformer(BaseLayer):\n    def __init__(self, context, name, vocab_size, embd_size, eps,\n                 n_head, intermediate_size, max_length, n_layer, layer_per_ipu, fp8_cache=False):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.eps = eps\n        self.n_head = n_head\n        self.intermediate_size = intermediate_size\n        self.max_length = max_length\n        self.n_layer = n_layer\n        self.layer_per_ipu = layer_per_ipu\n        self.fp8_cache = fp8_cache\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n\n        self.embedding = TransformerEmbedding(\n            self.context,\n            \"embed_tokens\",\n            self.vocab_size,\n            self.embd_size,\n        )\n        self.blocks = [\n            TransformerBlock(\n                self.context,\n                'layers.'+str(i),\n                self.embd_size,\n                self.eps,\n                self.n_head,\n                self.intermediate_size,\n                self.max_length,\n                self.fp8_cache\n            )\n            for i in range(self.n_layer)\n        ]\n        self.layer_norm = RMSLayerNorm(self.context, 'norm', self.embd_size, self.eps)\n\n    def __call__(self, graph, input_ids, position_ids, step, attention_mask, sequence_length, **kwargs):\n        norm_type = kwargs.get('norm_type', 'ce')\n        softmax_type = kwargs.get('softmax_type', 'ce')\n        outline_blocks = kwargs.get('outline_blocks', 'single_block')\n        if outline_blocks:\n            self.logger.info('outlining transformer blocks')\n            self.logger.info('please make sure disable outlining in session options is set to False')\n\n        with self.device_scope(graph,0,0):\n            hidden_states = self.embedding(graph, input_ids, sequence_length)\n        end_points = np.cumsum(self.layer_per_ipu)\n\n        for i in range(self.n_layer):\n            stage_offset = sum(i >= end_points)\n            with self.device_scope(graph, stage_offset, stage_offset):\n                hidden_states = self.blocks[i](\n                    graph, hidden_states, sequence_length, step, attention_mask, norm_type, softmax_type)\n                self.logger.info(f'block {i} placed on IPU {stage_offset}')\n\n        with self.device_scope(graph,stage_offset,stage_offset):\n            hidden_states = self.layer_norm(graph, hidden_states)\n\n        return hidden_states", "\n\nclass LLAMA2DecModel(HFDecBaseModel):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.fp8_cache = kwargs.get('fp8_cache', False)\n        self.topk = kwargs.get('topk', 1)\n        self.outline_blocks = kwargs.get('outline_blocks', False)\n        self.transformer = Transformer(\n            None,\n            'model',\n            self.hf_config.vocab_size,\n            self.hf_config.hidden_size,\n            self.hf_config.rms_norm_eps,\n            self.hf_config.num_attention_heads,\n            self.hf_config.intermediate_size,\n            self.max_length,\n            self.hf_config.num_hidden_layers,\n            self.layer_per_ipu,\n            self.fp8_cache,\n        )\n        self.lm_head = LMHead(None, 'lm_head', self.topk, self.hf_config.vocab_size, self.hf_config.hidden_size)\n\n    def prepare_state_dict(self):\n        self.hf_tokenizer = AutoTokenizer.from_pretrained(\n            self.hf_model_name,\n            cache_dir=self.hf_cache_dir,\n            pad_token='[PAD]'\n        )\n        #TODO find out why need this setting of padding_side\n        self.hf_tokenizer.padding_side = \"right\"\n\n        self.logger.info(f'initialized tokenizer by model_name: {self.hf_model_name}')\n        if not self.override_hfconfig_from_json:\n            model_class = self.hf_model_class_name_map.get(self.hf_model_class_name, None)\n            assert model_class, f\"Invalid hf_model_class_name: {self.hf_model_class_name}\"\n            assert self.hf_model_name, f\"Invalid hf_model_name: {self.hf_model_name}\"\n            self.logger.info(f'initializing hf model class: {model_class.__name__}')\n            self.hf_model = model_class.from_pretrained(self.hf_model_name, cache_dir=self.hf_cache_dir)\n            self.logger.info(f'loading pretrained hf model: {self.hf_model_name}')\n            self.hf_config = self.hf_model.config\n            if self.precision != 'fp32':\n                self.hf_model.half()\n                self.logger.info(f'casting model to {self.precision}')\n            self.state_dict = self.hf_model.state_dict()\n            self.register_state_dict()\n        else:\n            self.logger.info('using overrided config, no state dict loaded')\n            self.hf_config = AutoConfig.from_pretrained(self.override_hfconfig_from_json)\n            self.state_dict = {}\n\n    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n        input_ids_container = model_graph_inputs['input_ids_container']\n        attention_mask = model_graph_inputs['attention_mask']\n        step = model_graph_inputs['step']\n\n        with self.device_scope(model_graph,0,0):\n            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n            if sequence_length != 1:\n                position_ids_value = np.array([np.arange(self.max_length)] * self.batch_size)\n                position_ids_container = ops.constant(\n                    model_graph, position_ids_value.astype(np.int32), 'position_ids')\n                position_ids = ops.dynamic_slice(model_graph, position_ids_container, step, [1], [sequence_length])\n            else:\n                position_ids = ops.unsqueeze(model_graph, step, [0])\n\n        logits = self.transformer(\n            model_graph,\n            input_ids,\n            position_ids,\n            step,\n            temp_attention_mask,\n            sequence_length=sequence_length,\n            outline_blocks=self.outline_blocks\n        )\n\n        with self.device_scope(model_graph,len(self.layer_per_ipu)-1,len(self.layer_per_ipu)-1):\n            self.lm_head.set_virtual_id(len(self.layer_per_ipu)-1)\n            next_ids = self.lm_head(\n                model_graph,\n                logits,\n                sequence_length=sequence_length\n            )\n\n        model_outputs =  {'next_ids': next_ids, 'stage_offset': len(self.layer_per_ipu)}\n        return model_outputs\n\n    def build_input_dict(self, **kwargs):\n        input_string_list = list(kwargs.get('input_string_list', []))\n        batch_size = self.batch_size * self.batch_per_step\n        if len(input_string_list) >= batch_size:\n            input_string_list = input_string_list[:batch_size]\n            self.logger.info('num input strings is larger than batch size, truncating')\n        else:\n            input_string_list.extend([input_string_list[0]] * (batch_size - len(input_string_list)))\n            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n\n        inputs = self.hf_tokenizer(\n            input_string_list,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='np',\n            return_attention_mask=False,\n            add_special_tokens=False\n        )\n\n        return {'input_ids': inputs['input_ids']}\n\n    def build_output_dict(self, anchor_arrays):\n        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n        if len(output.shape) == 3:\n            output, decode_step = output[0], decode_step[0]\n            # TODO try deal with this automatically\n        if len(output.shape) == 4:\n            output, decode_step = output[1][0], decode_step[0][0]\n\n        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n        output = {str(i): v for i, v in enumerate(output)}\n        return {'output': output, 'decode_step': decode_step}", ""]}
{"filename": "poptransformer/models/llama2/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# This is a re-implementation of Llama 2 by Graphcore Ltd\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Embedding\n\n\nclass BaseTransformerEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n        return  embeds", "class BaseTransformerEmbedding(BaseLayer):\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n        return  embeds", "\n\nclass TPTransformerEmbedding(BaseTransformerEmbedding):\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n        return embeds\n", "\n\nclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n    layer_class_map = {\n        'tp': TPTransformerEmbedding,\n        'shard': BaseTransformerEmbedding}\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, vocab_size, embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/llama2/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# This is a re-implementation of Llama 2 by Graphcore Ltd\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import Linear\nfrom poptransformer.layers.mlp import BaseMLP\n\n\nclass ShardMLP(BaseMLP):\n    act_fn_map = {\n        'gelu': ops.gelu,\n        'swish': ops.swish\n    }\n    def collect_bind_layer_weights(self):\n        self.gate_proj = Linear(self.context, 'gate_proj', self.input_size, self.hidden_size, use_bias=False)\n        self.up_proj = Linear(self.context, 'up_proj', self.input_size, self.hidden_size, use_bias=False)\n        self.down_proj = Linear(self.context, 'down_proj', self.hidden_size, self.input_size, use_bias=False)\n\n    def __call__(self, graph, x):\n        x = ops.reshape(graph, x, [-1, self.input_size])\n        gate_output = self.gate_proj(graph, x)\n        gate_output = self.act_fn(graph, gate_output)\n        up_output = self.up_proj(graph, x)\n        up_output = ops.mul(graph, up_output, gate_output)\n        output = self.down_proj(graph, up_output)\n        output = ops.reshape(graph, output, [self.batch_size, -1, self.input_size])\n        return output", "\nclass ShardMLP(BaseMLP):\n    act_fn_map = {\n        'gelu': ops.gelu,\n        'swish': ops.swish\n    }\n    def collect_bind_layer_weights(self):\n        self.gate_proj = Linear(self.context, 'gate_proj', self.input_size, self.hidden_size, use_bias=False)\n        self.up_proj = Linear(self.context, 'up_proj', self.input_size, self.hidden_size, use_bias=False)\n        self.down_proj = Linear(self.context, 'down_proj', self.hidden_size, self.input_size, use_bias=False)\n\n    def __call__(self, graph, x):\n        x = ops.reshape(graph, x, [-1, self.input_size])\n        gate_output = self.gate_proj(graph, x)\n        gate_output = self.act_fn(graph, gate_output)\n        up_output = self.up_proj(graph, x)\n        up_output = ops.mul(graph, up_output, gate_output)\n        output = self.down_proj(graph, up_output)\n        output = ops.reshape(graph, output, [self.batch_size, -1, self.input_size])\n        return output", "\n\nclass TPMLP(ShardMLP):\n\n    def collect_bind_layer_weights(self):\n        gate_tp_settings = {\n            'strategy_name': 'start',\n        }\n        up_proj_tp_settings = {\n            'strategy_name': 'start',\n        }\n        down_proj_tp_settings = {\n            'strategy_name': 'end',\n        }\n        self.gate_proj = Linear(\n            self.context, 'gate_proj', self.input_size, self.hidden_size, use_bias=False, **gate_tp_settings)\n        self.up_proj = Linear(\n            self.context, 'up_proj', self.input_size, self.hidden_size, use_bias=False, **up_proj_tp_settings)\n        self.down_proj = Linear(\n            self.context, 'down_proj', self.hidden_size, self.input_size, use_bias=False, **down_proj_tp_settings)", "\n\nclass MLP(TPMLP, ShardMLP):\n    layer_class_map = {\n        'tp': TPMLP,\n        'shard': ShardMLP}\n\n    def __init__(self, context, name, input_size, hidden_size, act_fn):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, hidden_size, act_fn)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/llama2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# This is a re-implementation of Llama 2 by Graphcore Ltd\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nimport math\nimport numpy as np\n\nfrom poptransformer import ops\nfrom poptransformer.utils import shard, repeat, shard_fused_qkv\nfrom poptransformer.layers import BaseLayer", "from poptransformer.utils import shard, repeat, shard_fused_qkv\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\n\nclass BaseAttention(BaseLayer):\n    softmax_fn_map = {\n        'aionnx': ops.softmax,\n        'ce': ops.softmax_ce,\n    }\n    def __init__(self, context, name, input_size, num_head, cache_max_length, fp8_cache=False):\n        super().__init__(context, name)\n        self.input_size = input_size\n        self.num_head = num_head\n        self.head_size = self.input_size // self.num_head\n        self.cache_max_length = cache_max_length\n        self.fp8_cache = fp8_cache\n        self.scale = input_size // num_head\n        self.collect_bind_layer_weights()\n\n    def fixed_pos_embedding(self, graph, step, head_dim):\n        inv_freq_value = np.array(\n            [1.0 / (10000 ** (i / head_dim)) for i in range(0, head_dim, 2)]).astype(self.np_float_type)\n        inv_freq = ops.constant(graph, inv_freq_value, 'inv_freq')\n        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n\n        ind = ops.reshape(graph, step, [-1, 1])\n        ind = ops.cast(graph, ind, self.popart_float_type)\n\n        sinusoid_inp = ops.matmul(graph, ind, inv_freq)\n        return (graph.aiOnnx.sin([sinusoid_inp]), graph.aiOnnx.cos([sinusoid_inp]))\n\n    def rotate_half(self, graph, x, n_head, head_dim, batch_size=1):\n        x1, x2 = ops.split(graph, x, 2, 3, [head_dim//2, head_dim//2], \"split_rotate_every_two\")\n        x2 = ops.mul(graph, x2, ops.constant(graph, np.array([-1]).astype(self.np_float_type)))\n        x = ops.concat(graph, x2, x1, 3)\n        return ops.reshape(graph, x, [batch_size, n_head, 1, head_dim])\n\n    def apply_rotary_pos_emb(self, graph, q, k, sincos,  n_head, head_dim,batch_size=1):\n        sin = ops.concat(graph, sincos[0], sincos[0],1)\n        sin = ops.reshape(graph, sin, [1, 1, 1, -1])\n        cos = ops.concat(graph, sincos[1], sincos[1],1)\n        cos = ops.reshape(graph, cos, [1, 1, 1, -1])\n\n        q_rotate_every_two = self.rotate_half(graph, q, n_head, head_dim, batch_size)\n        q = ops.add(graph, ops.mul(graph, q, cos), ops.mul(graph, q_rotate_every_two, sin))\n        k_rotate_every_two = self.rotate_half(graph, k, n_head, head_dim, batch_size)\n        k = ops.add(graph, ops.mul(graph, k, cos), ops.mul(graph, k_rotate_every_two, sin))\n        return q, k\n\n    def collect_bind_layer_weights(self):\n        self.q_proj = Linear(self.context, 'q_proj', self.input_size, self.input_size, use_bias=False)\n        self.k_proj = Linear(self.context, 'k_proj', self.input_size, self.input_size, use_bias=False)\n        self.v_proj = Linear(self.context, 'v_proj', self.input_size, self.input_size, use_bias=False)\n        self.o_proj = Linear(self.context, 'o_proj', self.input_size, self.input_size, use_bias=False)\n\n    def forward_qkv(self, graph, x, step):\n        q = self.q_proj(graph, x)\n        k = self.k_proj(graph, x)\n        v = self.v_proj(graph, x)\n\n        q = ops.reshape(graph, q, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n        k = ops.reshape(graph, k, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n        v = ops.reshape(graph, v, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n\n        q = ops.transpose(graph, q, [0, 2, 1, 3])  # q: [B, N, L, H]\n        k = ops.transpose(graph, k, [0, 2, 1, 3])  # k: [B, N, L, H]\n        v = ops.transpose(graph, v, [0, 2, 1, 3])  # v: [B, N, L, H]\n\n        sincos = self.fixed_pos_embedding(graph, step, self.head_size)\n        q,k = self.apply_rotary_pos_emb(graph, q, k, sincos, self.num_head, self.head_size, self.batch_size)\n\n        kv = ops.concat(graph, k, v, 0)  #kv: [2, B, N, L, H]\n        kv = ops.reshape( graph, kv, [2, self.batch_size, self.num_head, self.sequence_length, self.head_size])\n\n        # layer_past: [2, B, N, L, h]\n        with graph.nameScope('attn_past_update'):\n            layer_past = ops.kv_cache(graph, step, kv, self.cache_max_length, 3, self.sequence_length)\n            layer_past_key, layer_past_value = ops.split(\n                graph, layer_past, 2, axis=0, splits=[1, 1], name='split_past'\n            )\n            layer_past_key = ops.squeeze(graph, layer_past_key, [0])\n            layer_past_value = ops.squeeze(graph, layer_past_value, [0])\n            layer_past_key_temp = ops.transpose(\n                graph, layer_past_key, [0, 1, 3, 2])\n\n        return  q, layer_past_key_temp, layer_past_value\n\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        w = ops.matmul(graph, q, k)\n        w = ops.mul(graph, w, ops.constant(graph, np.array([1/math.sqrt(self.head_size)]).astype(self.np_float_type)))\n        w = ops.add(graph, w, attention_mask)\n\n        w = ops.cast(graph, w, 'FLOAT')\n        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n        if not softmax_fn:\n            raise ValueError(f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\")\n        w = softmax_fn(graph, w, -1, stable_mode=self.sequence_length != 1)\n        w = ops.cast(graph, w, self.popart_float_type)\n        return w\n\n    def forward_output(self, graph, score, v):\n        a = ops.matmul(graph, score, v)\n        a = ops.transpose(graph, a, [0, 2, 1, 3])\n        a = ops.reshape(graph, a, [self.batch_size, self.sequence_length, -1])\n        return self.o_proj(graph, a)\n\n    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n        with graph.nameScope(self.context):\n            self.sequence_length = sequence_length\n            q, k, v = self.forward_qkv(graph, x, step)\n            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n            output = self.forward_output(graph, score, v)\n            return output", "\n\nclass TPAttention(BaseAttention):\n\n    def collect_bind_layer_weights(self):\n        self.num_head_beforeTP = self.num_head\n        self.num_head = math.ceil(self.num_head / self.num_replicas)\n        assert self.num_head_beforeTP == self.num_head * self.num_replicas\n        qkv_tp_settings = {\n            'strategy_name': 'start',\n        }\n        proj_tp_setting = {\n            'strategy_name': 'end',\n        }\n        self.q_proj = Linear(\n            self.context, 'q_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n        self.k_proj = Linear(\n            self.context, 'k_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n        self.v_proj = Linear(\n            self.context, 'v_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n        self.o_proj = Linear(\n            self.context, 'o_proj', self.input_size, self.input_size, use_bias=False, **proj_tp_setting)", "\n\nclass Attention(TPAttention, BaseAttention):\n    layer_class_map = {\n        'tp': TPAttention,\n        'shard': BaseAttention}\n\n    def __init__(self, context, name, input_size, num_head, cache_max_length, fp8_cache=False):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n        super().__init__(context, name, input_size, num_head, cache_max_length, fp8_cache)\n\n    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n        return self.layer_class.__call__(self, graph, x, step, attention_mask, sequence_length, softmax_type)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\n    def forward_qkv(self, graph, x, step):\n        return self.layer_class.forward_qkv(self, graph, x, step)\n\n    def forward_output(self, graph, score, v):\n        return self.layer_class.forward_output(self, graph, score, v)", ""]}
{"filename": "poptransformer/models/chatglm2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport numpy as np\n\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\n", "from transformers import AutoModel, AutoConfig, AutoTokenizer\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, LMHead\nfrom poptransformer.layers.rms_layer_norm import RMSLayerNorm\nfrom poptransformer.models.chatglm2.emebdding import TransformerEmbedding\nfrom poptransformer.models.chatglm2.attention import MultiQueryAttention\nfrom poptransformer.models.chatglm2.mlp import MLP\nfrom poptransformer.models import HFDecBaseModel\nfrom poptransformer.utils import REGISTRY", "from poptransformer.models import HFDecBaseModel\nfrom poptransformer.utils import REGISTRY\n\n\ndef rearrange_tp_weights(weight, num_heads, num_replicas, axis=0):\n    hidden_size = weight.shape[axis]\n    offset = hidden_size // 2\n    head_size = hidden_size // num_heads\n    group_size = num_heads // num_replicas // 2\n    if axis == 1:\n        weight = weight.T\n    weights = []\n    # Divide weight into groups for each replica and rearange them.\n    for i in range(0, num_heads // 2, group_size):\n        weight_1 = weight[\n            i * head_size : (i + group_size) * head_size\n        ]\n        weight_2 = weight[\n            offset + i * head_size : offset + (i + group_size) * head_size\n        ]\n        weights.extend([weight_1, weight_2])\n    weight_new = torch.cat(weights, axis=0)\n    if axis == 1:\n        weight_new = weight_new.T\n    return weight_new", "\n\nclass ChatGLM2Block(BaseLayer):\n    def __init__(\n        self,\n        context,\n        name,\n        hidden_size,\n        layernorm_epsilon,\n        multi_query_group_num,\n        num_attention_heads,\n        ffn_hidden_size,\n        max_length,\n        layer_number,\n    ):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.multi_query_group_num = multi_query_group_num\n        self.num_attention_heads = num_attention_heads\n        self.ffn_hidden_size = ffn_hidden_size\n        self.layernorm_epsilon = layernorm_epsilon\n        self.max_length = max_length\n        self.layer_number = layer_number\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.layer_norm1 = RMSLayerNorm(\n            self.context, \"input_layernorm\", self.hidden_size, self.layernorm_epsilon\n        )\n        self.attention = MultiQueryAttention(\n            self.context,\n            \"self_attention\",\n            self.hidden_size,\n            self.multi_query_group_num,\n            self.num_attention_heads,\n            self.max_length,\n            self.layer_number,\n        )\n        self.layer_norm2 = RMSLayerNorm(\n            self.context,\n            \"post_attention_layernorm\",\n            self.hidden_size,\n            self.layernorm_epsilon,\n        )\n        self.mlp = MLP(self.context, \"mlp\", self.hidden_size, self.ffn_hidden_size)\n\n    def __call__(\n        self,\n        graph,\n        x,\n        sequence_length,\n        step,\n        attention_mask,\n        softmax_type=\"ce\",\n        **kwargs,\n    ):\n        matmul_kwargs = {\"amp\": 0.4, \"partialtype\": \"half\"}\n        with graph.nameScope(self.context):\n            temp_x = self.layer_norm1(graph, x)\n            with self.option_scope(**matmul_kwargs):\n                temp_x = self.attention(\n                    graph, temp_x, step, attention_mask, sequence_length, softmax_type\n                )\n            x = ops.add(graph, x, temp_x)\n            temp_x = self.layer_norm2(graph, x)\n            with self.option_scope(**matmul_kwargs):\n                temp_x = self.mlp(graph, temp_x)\n            x = ops.add(graph, x, temp_x)\n        return x", "\n\nclass Transformer(BaseLayer):\n    def __init__(\n        self,\n        context,\n        name,\n        padded_vocab_size,\n        num_embedding_partitions,\n        hidden_size,\n        layernorm_epsilon,\n        multi_query_group_num,\n        num_attention_heads,\n        ffn_hidden_size,\n        max_length,\n        num_layers,\n        layer_per_ipu,\n    ):\n        super().__init__(context, name)\n        self.padded_vocab_size = padded_vocab_size\n        self.num_embedding_partitions = num_embedding_partitions\n        self.hidden_size = hidden_size\n        self.layernorm_epsilon = layernorm_epsilon\n        self.multi_query_group_num = multi_query_group_num\n        self.num_attention_heads = num_attention_heads\n        self.ffn_hidden_size = ffn_hidden_size\n        self.max_length = max_length\n        self.num_layers = num_layers\n        self.layer_per_ipu = layer_per_ipu\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.embedding = TransformerEmbedding(\n            self.context,\n            \"embedding.word_embeddings\",\n            self.padded_vocab_size,\n            self.hidden_size,\n        )\n        self.blocks = [\n            ChatGLM2Block(\n                self.context,\n                \"encoder.layers.\" + str(i),\n                self.hidden_size,\n                self.layernorm_epsilon,\n                self.multi_query_group_num,\n                self.num_attention_heads,\n                self.ffn_hidden_size,\n                self.max_length,\n                layer_number=i + 1,\n            )\n            for i in range(self.num_layers)\n        ]\n        self.layer_norm = RMSLayerNorm(\n            self.context,\n            \"encoder.final_layernorm\",\n            self.hidden_size,\n            self.layernorm_epsilon,\n        )\n\n    def __call__(\n        self,\n        graph,\n        input_ids,\n        position_ids,\n        step,\n        attention_mask,\n        sequence_length,\n        **kwargs,\n    ):\n        softmax_type = kwargs.get(\"softmax_type\", \"ce\")\n        with self.device_scope(graph, 0, 0):\n            hidden_states = self.embedding(graph, input_ids, sequence_length)\n        end_points = np.cumsum(self.layer_per_ipu)\n        for i in range(self.num_layers):\n            stage_offset = sum(i >= end_points)\n            with self.device_scope(graph, stage_offset, stage_offset):\n                hidden_states = self.blocks[i](\n                    graph,\n                    hidden_states,\n                    sequence_length,\n                    step,\n                    attention_mask,\n                    softmax_type,\n                )\n                self.logger.info(f\"block {i} placed on IPU {stage_offset}\")\n\n        with self.device_scope(graph, stage_offset, stage_offset):\n            hidden_states = self.layer_norm(\n                graph, hidden_states\n            )\n\n        return hidden_states", "\n\nclass ChatGLM2DecModel(HFDecBaseModel):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.outline_blocks = kwargs.get(\"outline_blocks\", False)\n        self.num_embedding_partitions = kwargs.get(\"num_embedding_partitions\", 1)\n        self.transformer = Transformer(\n            None,\n            \"transformer\",\n            self.hf_config.padded_vocab_size,\n            self.num_embedding_partitions,\n            self.hf_config.hidden_size,\n            self.hf_config.layernorm_epsilon,\n            self.hf_config.multi_query_group_num,\n            self.hf_config.num_attention_heads,\n            self.hf_config.ffn_hidden_size,\n            self.max_length,\n            self.hf_config.num_layers,\n            self.layer_per_ipu,\n        )\n        self.lm_head = LMHead(\n            None,\n            \"transformer.output_layer\",\n            1,\n            self.hf_config.padded_vocab_size,\n            self.hf_config.hidden_size,\n        )\n\n    def prepare_state_dict(self):\n        hf_args = {\n            \"pretrained_model_name_or_path\": self.hf_model_name,\n            \"trust_remote_code\": True,\n            \"cache_dir\": self.hf_cache_dir,\n        }\n        self.hf_tokenizer = AutoTokenizer.from_pretrained(**hf_args)\n        self.logger.info(f\"initialized tokenizer by model_name: {self.hf_model_name}\")\n        self.hf_config = AutoConfig.from_pretrained(**hf_args)\n        # self.hf_config.num_layers = 4\n        self.logger.info(f\"loading pretrained hf model: {self.hf_model_name}\")\n        self.hf_model = AutoModel.from_pretrained(**hf_args).eval()\n        if self.precision != \"fp32\":\n            self.hf_model.half()\n            self.logger.info(f\"casting model to {self.precision}\")\n        self.state_dict = self.hf_model.state_dict()\n        tensor_names = list(self.state_dict.keys())\n        for k in tensor_names:\n            # Split qkv weight into q & kv for the need of TP mode\n            if \"query_key_value\" in k:\n                weight_np = self.state_dict.pop(k)\n                weight_1, weight_2 = np.split(\n                    weight_np, [self.hf_config.hidden_size], axis=0\n                )\n                self.state_dict[k.replace(\"query_key_value\", \"query\")] = weight_1\n                self.state_dict[k.replace(\"query_key_value\", \"key_value\")] = weight_2\n            if \"dense_h_to_4h\" in k:\n                weight_np = self.state_dict.pop(k)\n                weight_1, weight_2 = weight_np.chunk(2, dim=0)\n                self.state_dict[\n                    k.replace(\"dense_h_to_4h\", \"dense_h_to_4h_1\")\n                ] = weight_1\n                self.state_dict[\n                    k.replace(\"dense_h_to_4h\", \"dense_h_to_4h_2\")\n                ] = weight_2\n        REGISTRY.register(\"query_size\", self.hf_config.hidden_size)\n        REGISTRY.register(\n            \"key_value_size\",\n            self.hf_config.multi_query_group_num * self.hf_config.kv_channels,\n        )\n        # Rearange weights for tp mode\n        if self.model_type == \"tp\":\n            for k in self.state_dict.keys():\n                if \"query\" in k or (\"dense.weight\" in k and \"scale\" not in k):\n                    self.state_dict[k] = rearrange_tp_weights(\n                        self.state_dict[k],\n                        self.hf_config.num_attention_heads,\n                        num_replicas=self.num_replicas,\n                        axis=0 if \"query\" in k else 1,\n                    )\n        self.register_state_dict()\n\n    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n        input_ids_container = model_graph_inputs[\"input_ids_container\"]\n        attention_mask = model_graph_inputs[\"attention_mask\"]\n        step = model_graph_inputs[\"step\"]\n\n        with self.device_scope(model_graph, 0, 0):\n            input_ids = ops.dynamic_slice(\n                model_graph,\n                input_ids_container,\n                step,\n                axes=[1],\n                sizes=[sequence_length],\n            )\n            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n            if sequence_length != 1:\n                position_ids_value = np.array(\n                    [np.arange(self.max_length)] * self.batch_size\n                )\n                position_ids_container = ops.constant(\n                    model_graph, position_ids_value.astype(np.int32), \"position_ids\"\n                )\n                position_ids = ops.dynamic_slice(\n                    model_graph, position_ids_container, step, [1], [sequence_length]\n                )\n            else:\n                position_ids = ops.unsqueeze(model_graph, step, [0])\n\n        logits = self.transformer(\n            model_graph,\n            input_ids,\n            position_ids,\n            step,\n            temp_attention_mask,\n            sequence_length=sequence_length,\n            outline_blocks=self.outline_blocks,\n        )\n\n        with self.device_scope(\n            model_graph, len(self.layer_per_ipu) - 1, len(self.layer_per_ipu) - 1\n        ):\n            self.lm_head.set_virtual_id(len(self.layer_per_ipu) - 1)\n            next_ids = self.lm_head(\n                model_graph, logits, sequence_length=sequence_length\n            )\n\n        model_outputs = {\n            \"next_ids\": next_ids,\n            \"stage_offset\": self.transformer.num_layers + 1,\n        }\n        return model_outputs\n\n    def build_input_dict(self, **kwargs):\n        query = kwargs.get(\"input_string\", \"\u665a\u4e0a\u7761\u4e0d\u7740\u5e94\u8be5\u600e\u4e48\u529e\")\n        prompt = self.hf_tokenizer.build_prompt(query, history=[])\n        inputs = self.hf_tokenizer([prompt], return_tensors=\"np\")\n        input_ids = inputs[\"input_ids\"]\n        input_ids_padding = np.zeros((1, self.max_length), dtype=np.int32)\n        input_ids_padding[:, : input_ids.shape[1]] = input_ids\n\n        return {\"input_ids\": input_ids_padding}\n\n    def build_output_dict(self, anchor_arrays):\n        output, decode_step = anchor_arrays[\"Loop:0\"], anchor_arrays[\"Loop:1\"]\n        if len(output.shape) == 3:\n            output, decode_step = output[0], decode_step[0]\n        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n        output = {str(i): v for i, v in enumerate(output)}\n        return {\"output\": output, \"decode_step\": decode_step}", ""]}
{"filename": "poptransformer/models/chatglm2/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import Linear\n\nfrom poptransformer.layers import BaseLayer\n", "from poptransformer.layers import BaseLayer\n\n\nclass BaseMLP(BaseLayer):\n    def __init__(self, context, name, hidden_size, ffn_hidden_size):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.ffn_hidden_size = ffn_hidden_size\n        self.act_fn = ops.swish\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.gate_proj = Linear(\n            self.context,\n            \"dense_h_to_4h_1\",\n            self.hidden_size,\n            self.ffn_hidden_size,\n            use_bias=False,\n        )\n        self.up_proj = Linear(\n            self.context,\n            \"dense_h_to_4h_2\",\n            self.hidden_size,\n            self.ffn_hidden_size,\n            use_bias=False,\n        )\n        self.down_proj = Linear(\n            self.context,\n            \"dense_4h_to_h\",\n            self.ffn_hidden_size,\n            self.hidden_size,\n            use_bias=False,\n        )\n\n    def __call__(self, graph, x):\n        x = ops.reshape(graph, x, [-1, self.hidden_size])\n        gate_output = self.gate_proj(graph, x)\n        gate_output = self.act_fn(graph, gate_output)\n        up_output = self.up_proj(graph, x)\n        up_output = ops.mul(graph, up_output, gate_output)\n        output = self.down_proj(graph, up_output)\n        output = ops.reshape(graph, output, [self.batch_size, -1, self.hidden_size])\n        return output", "\n\nclass TPMLP(BaseMLP):\n    def collect_bind_layer_weights(self):\n        gate_tp_settings = {\n            \"strategy_name\": \"start\",\n        }\n        up_proj_tp_settings = {\n            \"strategy_name\": \"start\",\n        }\n        down_proj_tp_settings = {\n            \"strategy_name\": \"end\",\n        }\n        self.gate_proj = Linear(\n            self.context,\n            \"dense_h_to_4h_1\",\n            self.hidden_size,\n            self.ffn_hidden_size,\n            use_bias=False,\n            **gate_tp_settings,\n        )\n        self.up_proj = Linear(\n            self.context,\n            \"dense_h_to_4h_2\",\n            self.hidden_size,\n            self.ffn_hidden_size,\n            use_bias=False,\n            **up_proj_tp_settings,\n        )\n        self.down_proj = Linear(\n            self.context,\n            \"dense_4h_to_h\",\n            self.ffn_hidden_size,\n            self.hidden_size,\n            use_bias=False,\n            **down_proj_tp_settings,\n        )", "\n\nclass MLP(TPMLP, BaseMLP):\n    layer_class_map = {\"tp\": TPMLP, \"shard\": BaseMLP}\n\n    def __init__(self, context, name, hidden_size, ffn_hidden_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(f\"Invalid model_type {model_type}\")\n        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(context, name, hidden_size, ffn_hidden_size)\n\n    def __call__(self, graph, x):\n        return self.layer_class.__call__(self, graph, x)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/chatglm2/emebdding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer, Embedding\n\n\nclass BaseTransformerEmbedding(BaseLayer):\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n        return embeds", "\nclass BaseTransformerEmbedding(BaseLayer):\n    def __init__(self, context, name, vocab_size, embd_size):\n        super().__init__(context, name)\n        self.vocab_size = vocab_size\n        self.embd_size = embd_size\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n        return embeds", "\n\nclass TPTransformerEmbedding(BaseTransformerEmbedding):\n    def __call__(self, graph, input_ids, sequence_length):\n        with graph.nameScope(self.context):\n            embeds = self.wte(graph, input_ids, sequence_length)\n            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n        return embeds\n\n\nclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n    layer_class_map = {\"tp\": TPTransformerEmbedding, \"shard\": BaseTransformerEmbedding}\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(\n                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n            )\n        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(context, name, vocab_size, embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", "\n\nclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n    layer_class_map = {\"tp\": TPTransformerEmbedding, \"shard\": BaseTransformerEmbedding}\n\n    def __init__(self, context, name, vocab_size, embd_size):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(\n                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n            )\n        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(context, name, vocab_size, embd_size)\n\n    def __call__(self, graph, input_ids, sequence_length):\n        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)", ""]}
{"filename": "poptransformer/models/chatglm2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport numpy as np\n\nfrom poptransformer import ops\nfrom poptransformer.layers import BaseLayer", "from poptransformer import ops\nfrom poptransformer.layers import BaseLayer\nfrom poptransformer.layers import Linear\nfrom poptransformer.layers.linear import BaseLinear\n\n\nclass BaseMultiQueryAttention(BaseLayer):\n    softmax_fn_map = {\n        \"aionnx\": ops.softmax,\n        \"ce\": ops.softmax_ce,\n    }\n\n    def __init__(\n        self,\n        context,\n        name,\n        hidden_size,\n        multi_query_group_num,\n        num_attention_heads,\n        cache_max_length,\n        layer_number,\n        add_qkv_bias,\n    ):\n        super().__init__(context, name)\n        self.hidden_size = hidden_size\n        self.multi_query_group_num = multi_query_group_num\n        self.num_attention_heads = num_attention_heads\n        self.hidden_size_per_attention_head = (\n            self.hidden_size // self.num_attention_heads\n        )\n        self.cache_max_length = cache_max_length\n        self.layer_number = layer_number\n        self.add_qkv_bias = add_qkv_bias\n\n        # 1 stage mode\n        self.input_length = 1\n        self.rotary_dim = self.hidden_size_per_attention_head\n\n        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n        self.collect_bind_layer_weights()\n\n    def collect_bind_layer_weights(self):\n        self.query = Linear(\n            self.context,\n            \"query\",\n            self.hidden_size,\n            self.hidden_size,\n            use_bias=self.add_qkv_bias,\n        )\n        self.key_value = Linear(\n            self.context,\n            \"key_value\",\n            self.hidden_size,\n            2 * self.hidden_size_per_attention_head * self.multi_query_group_num,\n            use_bias=self.add_qkv_bias,\n        )\n        self.dense = Linear(\n            self.context, \"dense\", self.hidden_size, self.hidden_size, use_bias=False\n        )\n\n    def fixed_pos_embedding(self, graph, position_id, dim):\n        # H = 4096, n = 32, h = 128, dim = 128 // 2 = 64\n        # cos, sin = [L, dim]\n        inv_freq_value = np.array(\n            [1.0 / (10000 ** (i / dim)) for i in range(0, dim, 2)]\n        ).astype(np.float32)\n        inv_freq = ops.constant(graph, inv_freq_value, \"inv_freq\")\n        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n        # position_id -> [L, B]\n        position_id = ops.reshape(graph, position_id, [-1, 1])\n        # Notice: fp16 precision is not suitable for large integers.\n        position_id = ops.cast(graph, position_id, \"FLOAT\")\n        freqs = ops.matmul(graph, position_id, inv_freq)\n        emb = ops.concat(graph, freqs, freqs, axis=-1)\n        # emb -> [L, dim] -> [1, L, 1, dim]\n        emb = ops.reshape(graph, emb, shape=[1, -1, 1, dim])\n\n        emb = ops.cast(graph, emb, self.popart_float_type)\n        cos, sin = graph.aiOnnx.cos([emb]), graph.aiOnnx.sin([emb])\n        return cos, sin\n\n    def rotate_half(self, graph, x):\n        x1, x2 = ops.split(\n            graph,\n            x,\n            num_outputs=2,\n            axis=-1,\n            splits=[self.rotary_dim // 4, self.rotary_dim // 4],\n            name=\"rope_split\",\n        )\n        x2 = ops.mul(graph, x2, ops.constant(graph, np.array([-1]).astype(np.float32)))\n        return ops.concat(graph, x2, x1, axis=-1)\n\n    def apply_rotary_pos_emb(self, graph, x, apply_rotary_pos_emb, num_heads):\n        # position_id: [B, L], x: [B, L, N, rotary_dim], sin, cos: [L, rotary_dim] -> [1, L, 1, rotary_dim]\n        cos, sin = apply_rotary_pos_emb\n        x = ops.reshape(\n            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1, 2]\n        )\n        x = ops.transpose(graph, x, perm=[0, 1, 2, 4, 3])\n        x = ops.reshape(\n            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1]\n        )\n        x = ops.add(\n            graph,\n            ops.mul(graph, x, cos),\n            ops.mul(graph, self.rotate_half(graph, x), sin),\n        )\n        x = ops.reshape(\n            graph, x, shape=[self.batch_size, self.input_length, num_heads, 2, -1]\n        )\n        x = ops.transpose(graph, x, perm=[0, 1, 2, 4, 3])\n        x = ops.reshape(\n            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1]\n        )\n        return x\n\n    def rotary_embedding(self, graph, q, k, position_ids):\n        with graph.nameScope(\"build_rotary\"):\n            rotary_pos_emb = self.fixed_pos_embedding(\n                graph, position_ids, dim=self.rotary_dim // 2\n            )\n            q1, q2 = ops.split(\n                graph,\n                q,\n                num_outputs=2,\n                axis=-1,\n                splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n                name=\"rope_split_q\",\n            )\n            k1, k2 = ops.split(\n                graph,\n                k,\n                num_outputs=2,\n                axis=-1,\n                splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n                name=\"rope_split_k\",\n            )\n        with graph.nameScope(\"apply_rotary\"):\n            q1 = self.apply_rotary_pos_emb(\n                graph, q1, rotary_pos_emb, self.num_attention_heads\n            )\n            k1 = self.apply_rotary_pos_emb(\n                graph, k1, rotary_pos_emb, self.multi_query_group_num\n            )\n            q = ops.concat(graph, q1, q2, axis=-1)\n            k = ops.concat(graph, k1, k2, axis=-1)\n        return q, k\n\n    def forward_qkv(self, graph, x, step):\n        q = self.query(graph, x)\n        mixed_kv = self.key_value(graph, x)\n        k, v = ops.split(\n            graph,\n            mixed_kv,\n            num_outputs=2,\n            axis=-1,\n            splits=[\n                self.multi_query_group_num * self.hidden_size_per_attention_head,\n                self.multi_query_group_num * self.hidden_size_per_attention_head,\n            ],\n        )\n        q = ops.reshape(\n            graph,\n            q,\n            shape=[\n                self.batch_size,\n                self.sequence_length,\n                self.num_attention_heads,\n                self.hidden_size_per_attention_head,\n            ],\n        )\n        k = ops.reshape(\n            graph,\n            k,\n            shape=[\n                self.batch_size,\n                self.sequence_length,\n                self.multi_query_group_num,\n                self.hidden_size_per_attention_head,\n            ],\n        )\n        v = ops.reshape(\n            graph,\n            v,\n            shape=[\n                self.batch_size,\n                self.sequence_length,\n                self.multi_query_group_num,\n                self.hidden_size_per_attention_head,\n            ],\n        )\n\n        q, k = self.rotary_embedding(graph, q, k, position_ids=step)\n\n        # q = [B, N, L, h]\n        q = ops.transpose(graph, q, perm=[0, 2, 1, 3])\n        kv = ops.concat(graph, k, v, 0)\n        kv = ops.reshape(\n            graph,\n            kv,\n            [\n                2,\n                self.batch_size,\n                self.sequence_length,\n                self.multi_query_group_num,\n                self.hidden_size_per_attention_head,\n            ],\n        )\n        # kv = [2, B, n, L, h]\n        kv = ops.transpose(graph, kv, perm=[0, 1, 3, 2, 4])\n\n        with graph.nameScope(\"attn_past_update\"):\n            layer_past = ops.kv_cache(\n                graph, step, kv, self.cache_max_length, 3, self.sequence_length\n            )\n            k, v = ops.split(\n                graph, layer_past, 2, axis=0, splits=[1, 1], name=\"split_past\"\n            )\n            # k, v = [B, n, L, h]\n            k = ops.squeeze(graph, k, [0])\n            v = ops.squeeze(graph, v, [0])\n            # [B, n, L, h] -> [B, N, L, h]\n            k = ops.unsqueeze(graph, k, [2])\n            k = ops.expand(\n                graph,\n                k,\n                [1, 1, self.num_attention_heads // self.multi_query_group_num, 1, 1],\n            )\n            k = ops.reshape(\n                graph,\n                k,\n                shape=[\n                    self.batch_size,\n                    self.num_attention_heads,\n                    self.cache_max_length,\n                    self.hidden_size_per_attention_head,\n                ],\n            )\n            v = ops.unsqueeze(graph, v, [2])\n            v = ops.expand(\n                graph,\n                v,\n                [1, 1, self.num_attention_heads // self.multi_query_group_num, 1, 1],\n            )\n            v = ops.reshape(\n                graph,\n                v,\n                shape=[\n                    self.batch_size,\n                    self.num_attention_heads,\n                    self.cache_max_length,\n                    self.hidden_size_per_attention_head,\n                ],\n            )\n            # k = [B, N, h, L]\n            k = ops.transpose(graph, k, [0, 1, 3, 2])\n        return q, k, v\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        attention_scores = ops.matmul(graph, q, k)\n        norm_factor = ops.constant(\n            graph, np.array([1.0 / self.norm_factor]).astype(self.np_float_type)\n        )\n        attention_scores = ops.mul(graph, attention_scores, norm_factor)\n        attention_scores = ops.add(graph, attention_scores, attention_mask)\n\n        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n        if not softmax_fn:\n            raise ValueError(\n                f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\"\n            )\n        attention_probs = softmax_fn(\n            graph, attention_scores, -1, stable_mode=self.sequence_length != 1\n        )\n        attention_probs = ops.cast(graph, attention_probs, self.popart_float_type)\n        return attention_probs\n\n    def forward_output(self, graph, score, v):\n        context_layer = ops.matmul(graph, score, v)\n        context_layer = ops.transpose(graph, context_layer, [0, 2, 1, 3])\n        context_layer = ops.reshape(\n            graph, context_layer, [self.batch_size, self.sequence_length, -1]\n        )\n        output = self.dense(graph, context_layer)\n        return output\n\n    def __call__(\n        self, graph, x, step, attention_mask, sequence_length, softmax_type=\"ce\"\n    ):\n        with graph.nameScope(self.context):\n            self.sequence_length = sequence_length\n            q, k, v = self.forward_qkv(graph, x, step)\n            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n            output = self.forward_output(graph, score, v)\n        return output", "\n\nclass TPMultiQueryAttention(BaseMultiQueryAttention):\n    def collect_bind_layer_weights(self):\n        qkv_tp_settings = {\n            \"strategy_name\": \"multi_query_qkv\",}\n        proj_tp_setting = {\n            \"strategy_name\": \"end\",\n        }\n        self.query = Linear(\n            self.context,\n            \"query\",\n            self.hidden_size,\n            self.hidden_size,\n            use_bias=self.add_qkv_bias,\n            **qkv_tp_settings,\n        )\n        self.key_value = BaseLinear(\n            self.context,\n            \"key_value\",\n            self.hidden_size,\n            2 * self.hidden_size_per_attention_head * self.multi_query_group_num,\n            use_bias=self.add_qkv_bias,\n        )\n        self.dense = Linear(\n            self.context,\n            \"dense\",\n            self.hidden_size,\n            self.hidden_size,\n            use_bias=False,\n            **proj_tp_setting,\n        )\n\n        self.num_attention_heads = self.num_attention_heads // self.num_replicas", "\n\nclass MultiQueryAttention(TPMultiQueryAttention, BaseMultiQueryAttention):\n    layer_class_map = {\n        \"tp\": TPMultiQueryAttention,\n        \"shard\": BaseMultiQueryAttention,\n    }\n\n    def __init__(\n        self,\n        context,\n        name,\n        hidden_size,\n        multi_query_group_num,\n        num_head,\n        cache_max_length,\n        layer_number,\n        add_qkv_bias=True,\n    ):\n        model_type = self.model_type\n        self.layer_class = self.layer_class_map.get(model_type, None)\n        if not self.layer_class:\n            raise ValueError(\n                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n            )\n        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n        super().__init__(\n            context,\n            name,\n            hidden_size,\n            multi_query_group_num,\n            num_head,\n            cache_max_length,\n            layer_number,\n            add_qkv_bias,\n        )\n\n    def __call__(\n        self, graph, x, step, attention_mask, sequence_length, softmax_type=\"ce\"\n    ):\n        return self.layer_class.__call__(\n            self, graph, x, step, attention_mask, sequence_length, softmax_type\n        )\n\n    def collect_bind_layer_weights(self):\n        return self.layer_class.collect_bind_layer_weights(self)\n\n    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n        return self.layer_class.forward_attention(\n            self, graph, q, k, attention_mask, softmax_type\n        )\n\n    def forward_qkv(self, graph, x, step):\n        return self.layer_class.forward_qkv(self, graph, x, step)\n\n    def forward_output(self, graph, score, v):\n        return self.layer_class.forward_output(self, graph, score, v)", ""]}
{"filename": "examples/chatglm/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport sys\nimport os\nimport hydra\nsys.path.append(\"../../\")", "import hydra\nsys.path.append(\"../../\")\nos.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\nfrom poptransformer.utils import prepare_model_session\n\n\ndef run_inference(session, model, *args, **kwargs):\n    input_dict = model.build_input_dict(*args, **kwargs)\n    all_time = []\n    for _ in range(10):\n        session.run(input_dict)\n\n    for _ in range(10):\n        start = time.perf_counter()\n        session.run(input_dict)\n        end = time.perf_counter()\n        all_time.append(end - start)\n\n    output = model.build_output_dict(session.anchor_arrays)\n    model.logger.info(output)\n    decode_step = output[\"decode_step\"].item()\n    num_output_tokens = decode_step - model.input_length\n    latency = sum(all_time) / len(all_time)\n    latency_per_token = latency / decode_step\n    latency_per_output_token = latency / num_output_tokens\n    throughput = int(model.batch_size * num_output_tokens / latency)\n    performance = (\n        \"\\nPerformance: \\n\"\n        + f\"batch size: {model.batch_size}, precision: {model.precision}\\n\"\n        + f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\"\n        + f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\"\n        + f\"Latency per output token: {latency_per_output_token*1000.0:.3f} ms/token\\n\"\n        + f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n    )\n    model.logger.info(performance)", "\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\ndef main(config):\n    session, model = prepare_model_session(config)\n    run_inference(session, model, **config.inputs)\n\nif __name__ == \"__main__\":\n    main()\n", ""]}
{"filename": "examples/rwkv/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport sys\nimport os\nimport hydra\nsys.path.append('../../')", "import hydra\nsys.path.append('../../')\nos.environ['HYDRA_FULL_ERROR'] = '1'\nfrom poptransformer.utils import prepare_model_session\n\n\ndef run_inference(session, model, *args, **kwargs):\n    input_dict = model.build_input_dict(*args, **kwargs)\n    all_time = []\n    for _ in range(10):\n        session.run(input_dict)\n\n    for _ in range(10):\n        start = time.perf_counter()\n        session.run(input_dict)\n        end = time.perf_counter()\n        all_time.append(end-start)\n\n    output = model.build_output_dict(session.anchor_arrays)\n    decode_step = output['decode_step'].item()\n    latency = sum(all_time) / len(all_time)\n    latency_per_token = latency / (decode_step * model.batch_per_step)\n    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n    model.logger.info(output)\n    performance = \"\\nPerformance: \\n\" + \\\n                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n                f\"batch per step: {model.batch_per_step}\\n\" + \\\n                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n    model.logger.info(performance)", "\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\ndef main(config):\n    session, model = prepare_model_session(config)\n    run_inference(session, model, **config.inputs)\n\nif __name__ == '__main__':\n    main()\n    ", "    "]}
{"filename": "examples/gpt2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport sys\nimport os\nimport hydra\nsys.path.append('../../')", "import hydra\nsys.path.append('../../')\nos.environ['HYDRA_FULL_ERROR'] = '1'\nfrom poptransformer.utils import prepare_model_session\n\n\ndef run_inference(session, model, *args, **kwargs):\n    input_dict = model.build_input_dict(*args, **kwargs)\n    all_time = []\n    for _ in range(10):\n        session.run(input_dict)\n\n    for _ in range(10):\n        start = time.perf_counter()\n        session.run(input_dict)\n        end = time.perf_counter()\n        all_time.append(end-start)\n\n    output = model.build_output_dict(session.anchor_arrays)\n    decode_step = output['decode_step'].item()\n    latency = sum(all_time) / len(all_time)\n    latency_per_token = latency / (decode_step * model.batch_per_step)\n    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n    model.logger.info(output)\n    performance = \"\\nPerformance: \\n\" + \\\n                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n                f\"batch per step: {model.batch_per_step}\\n\" + \\\n                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n    model.logger.info(performance)", "\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\ndef main(config):\n    session, model = prepare_model_session(config)\n    run_inference(session, model, **config.inputs)\n\nif __name__ == '__main__':\n    main()\n    ", "    "]}
{"filename": "examples/llama2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport sys\nimport os\nimport hydra\nsys.path.append('../../')", "import hydra\nsys.path.append('../../')\nos.environ['HYDRA_FULL_ERROR'] = '1'\nfrom poptransformer.utils import prepare_model_session\n\n\ndef run_inference(session, model, *args, **kwargs):\n    input_dict = model.build_input_dict(*args, **kwargs)\n    all_time = []\n    for _ in range(1):\n        session.run(input_dict)\n\n    for _ in range(3):\n        start = time.perf_counter()\n        session.run(input_dict)\n        end = time.perf_counter()\n        all_time.append(end-start)\n\n    output = model.build_output_dict(session.anchor_arrays)\n    decode_step = output['decode_step'].item()\n    latency = sum(all_time) / len(all_time)\n    latency_per_token = latency / (decode_step * model.batch_per_step)\n    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n    model.logger.info(output)\n    performance = \"\\nPerformance: \\n\" + \\\n                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n                f\"batch per step: {model.batch_per_step}\\n\" + \\\n                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n    model.logger.info(performance)", "\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\ndef main(config):\n    session, model = prepare_model_session(config)\n    run_inference(session, model, **config.inputs)\n\nif __name__ == '__main__':\n    main()\n    ", "    "]}
{"filename": "examples/chatglm2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport sys\nimport os\nimport hydra\nsys.path.append(\"../../\")", "import hydra\nsys.path.append(\"../../\")\nos.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\nfrom poptransformer.utils import prepare_model_session\n\n\ndef run_inference(session, model, *args, **kwargs):\n    input_dict = model.build_input_dict(*args, **kwargs)\n    all_time = []\n    for _ in range(10):\n        session.run(input_dict)\n\n    for _ in range(10):\n        start = time.perf_counter()\n        session.run(input_dict)\n        end = time.perf_counter()\n        all_time.append(end - start)\n\n    output = model.build_output_dict(session.anchor_arrays)\n    decode_step = output[\"decode_step\"].item()\n    num_output_tokens = decode_step\n    latency = sum(all_time) / len(all_time)\n    latency_per_output_token = latency / num_output_tokens\n    throughput = int(model.batch_size * num_output_tokens / latency)\n    model.logger.info(output)\n    performance = (\n        \"\\nPerformance: \\n\"\n        + f\"batch size: {model.batch_size}, precision: {model.precision}\\n\"\n        + f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\"\n        + f\"Latency per output token: {latency_per_output_token*1000.0:.3f} ms/token\\n\"\n        + f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n    )\n    model.logger.info(performance)", "\n\n@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\ndef main(config):\n    session, model = prepare_model_session(config)\n    run_inference(session, model, **config.inputs)\n\n\nif __name__ == \"__main__\":\n    main()", "if __name__ == \"__main__\":\n    main()\n"]}
