{"filename": "SynLens/import_lenses.py", "chunked_list": ["import xmltodict\nimport os, math\nfrom renderer import process_mesh\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\ndef parse_lenses_xml(xml_path):\n    with open(xml_path, 'r', encoding='utf-8') as file:\n        my_xml = file.read()\n\n    # Use xmltodict to parse and convert the XML document\n    lens_dict = xmltodict.parse(my_xml)\n\n    lenses = lens_dict.get(\"lensdatabase\", {}).get(\"lens\", [])\n    lenses_data = []\n\n    for i, lens_model in enumerate(lenses):\n        if \"calibration\" not in lens_model or not lens_model[\"calibration\"]:\n            continue\n        calibration = lens_model[\"calibration\"]\n        sensor_width = 36.0 / float(lens_model.get(\"cropfactor\", 1.0))\n        focal_length = None\n        fov = None\n\n        if \"distortion\" in calibration:\n            distortions = calibration[\"distortion\"]\n            if not isinstance(distortions, list):\n                distortions = [distortions]\n            for distortion in distortions:\n                if \"@model\" not in distortion:\n                    continue\n                if distortion[\"@model\"] == \"poly3\":\n                    if \"@k1\" not in distortion:\n                        continue\n                    coeffs = [0., float(distortion[\"@k1\"]), 0.]\n                    lens_type = \"brown\"\n                elif distortion[\"@model\"] == \"ptlens\":\n                    if \"@a\" not in distortion or \"@b\" not in distortion or \"@c\" not in distortion:\n                        continue\n                    coeffs = [float(distortion[\"@a\"]), float(distortion[\"@b\"]), float(distortion[\"@c\"])]\n                    lens_type = \"abc\"\n                elif distortion[\"@model\"] == \"poly5\":\n                    if \"@k1\" not in distortion or \"@k2\" not in distortion:\n                        continue\n                    coeffs = [float(distortion[\"@k1\"]), float(distortion[\"@k2\"]), 0.]\n                    lens_type = \"brown\"\n                else:\n                    continue\n\n                focal_length = float(distortion[\"@focal\"])\n                fov = 2 * math.atan(sensor_width / (2 * focal_length)) * (180 / math.pi)\n\n                if fov is not None and 60 <= fov <= 80:\n                    lens_model_dict = {\n                        \"sensor_width\": sensor_width,\n                        \"model_name\": str(i),\n                        \"focal\": fov,\n                        \"coeffs\": coeffs,\n                        \"lens_type\": lens_type\n                    }\n                    lenses_data.append(lens_model_dict)\n                    break\n\n        elif \"vignetting\" in calibration:\n            vignetting = calibration[\"vignetting\"]\n\n    return lenses_data", "def parse_lenses_xml(xml_path):\n    with open(xml_path, 'r', encoding='utf-8') as file:\n        my_xml = file.read()\n\n    # Use xmltodict to parse and convert the XML document\n    lens_dict = xmltodict.parse(my_xml)\n\n    lenses = lens_dict.get(\"lensdatabase\", {}).get(\"lens\", [])\n    lenses_data = []\n\n    for i, lens_model in enumerate(lenses):\n        if \"calibration\" not in lens_model or not lens_model[\"calibration\"]:\n            continue\n        calibration = lens_model[\"calibration\"]\n        sensor_width = 36.0 / float(lens_model.get(\"cropfactor\", 1.0))\n        focal_length = None\n        fov = None\n\n        if \"distortion\" in calibration:\n            distortions = calibration[\"distortion\"]\n            if not isinstance(distortions, list):\n                distortions = [distortions]\n            for distortion in distortions:\n                if \"@model\" not in distortion:\n                    continue\n                if distortion[\"@model\"] == \"poly3\":\n                    if \"@k1\" not in distortion:\n                        continue\n                    coeffs = [0., float(distortion[\"@k1\"]), 0.]\n                    lens_type = \"brown\"\n                elif distortion[\"@model\"] == \"ptlens\":\n                    if \"@a\" not in distortion or \"@b\" not in distortion or \"@c\" not in distortion:\n                        continue\n                    coeffs = [float(distortion[\"@a\"]), float(distortion[\"@b\"]), float(distortion[\"@c\"])]\n                    lens_type = \"abc\"\n                elif distortion[\"@model\"] == \"poly5\":\n                    if \"@k1\" not in distortion or \"@k2\" not in distortion:\n                        continue\n                    coeffs = [float(distortion[\"@k1\"]), float(distortion[\"@k2\"]), 0.]\n                    lens_type = \"brown\"\n                else:\n                    continue\n\n                focal_length = float(distortion[\"@focal\"])\n                fov = 2 * math.atan(sensor_width / (2 * focal_length)) * (180 / math.pi)\n\n                if fov is not None and 60 <= fov <= 80:\n                    lens_model_dict = {\n                        \"sensor_width\": sensor_width,\n                        \"model_name\": str(i),\n                        \"focal\": fov,\n                        \"coeffs\": coeffs,\n                        \"lens_type\": lens_type\n                    }\n                    lenses_data.append(lens_model_dict)\n                    break\n\n        elif \"vignetting\" in calibration:\n            vignetting = calibration[\"vignetting\"]\n\n    return lenses_data", "\ndef visualize(k1, k2, k3, lens_name):\n    cmap = cm.get_cmap('hsv')\n    H, W = 15, 15\n\n    radial = np.array([[(j + 0.5, i + 0.5) for j in range(W * 20)] for i in range(H * 20)])\n    radial_dist = (radial - np.array([[W * 10, H * 10]])) / np.array([[W * 10, H * 10]])\n\n    r2 = radial_dist ** 2\n    r2 = r2[:, :, 0] + r2[:, :, 1]\n\n    vis = r2*(1 + k1*r2**2 + k2*r2**4 + k3*r2**6)\n    norm = vis / np.abs(vis).max()\n\n    ori = np.array([[-(np.arctan(radial_dist[i,j,1] / radial_dist[i,j,0]) ) for j in range(W*20)] for i in range(H*20)])\n    ori[:, 0:W*10] = np.pi + ori[:, 0:W*10]\n    ori[H*10:H*20, W*10:W*20] = 2 * np.pi + ori[H*10:H*20, W*10:W*20]\n    ori = ori / ori.max()\n\n    color = np.array([[cmap(ori[i][j]) for j in range(W*20)]for i in range(H * 20)])\n\n    norm = norm ** 0.7\n    color_dist = color * norm[:, :, None]\n\n    img = Image.fromarray((color_dist[:, :, :3] * 255).astype(np.uint8))\n    img.save(\"db_frames_vis/\" + lens_name + \"_colordist.png\")", "\nif __name__ == \"__main__\":\n    db_dir = 'db/'\n    lens_type = 'abc'\n    num_views = 50\n    image_size = 512\n    render_rgb = True\n    out_fd = \"db_output\"\n\n    for xml_path in os.listdir(db_dir):\n        if xml_path.endswith(\".xml\"):\n            lens_name = xml_path.split(\"/\")[-1][:-4]\n            lenses_data = parse_lenses_xml(os.path.join(db_dir,xml_path))\n\n            for i in range(len(lenses_data)):\n                if lenses_data[i][\"lens_type\"] == lens_type:\n                    process_mesh(lenses_data[i], lens_name, num_views, image_size, render_rgb, out_fd)", ""]}
{"filename": "SynLens/lens_distortion.py", "chunked_list": ["import numpy as np\nimport json\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndef invert_function(x, func):\n    from scipy import interpolate\n    from scipy.interpolate import dfitpack\n    y = func(x)\n    dy = np.concatenate(([0], np.diff(y)))\n    y = y[dy>=0]\n    x = x[dy>=0]\n    try:\n        inter = interpolate.InterpolatedUnivariateSpline(y, x)\n    # dfitpack.error\n    except Exception: # pragma: no cover\n        inter = lambda x: x\n    return inter", "\n\nclass LensDistortion():  # pragma: no cover\n    def __init__(self):\n        offset = np.array([0, 0])\n        scale = 1\n\n    def imageFromDistorted(self, points):\n        # return the points as they are\n        return points\n\n    def distortedFromImage(self, points):\n        # return the points as they are\n        return points\n\n    def undistortImage(self, image):\n        return image", "\n\nclass NoDistortion(LensDistortion):\n    \"\"\"\n    The default model for the lens distortion which does nothing.\n    \"\"\"\n    pass\n\nclass Vignetting(LensDistortion):\n    def __init__(self, k1=None, k2=None, k3=None, projection=None):\n        self.k1 = k1\n        self.k2 = k2\n        self.k3 = k3\n        self.projection = projection\n        if projection is not None:\n            self.image_width_px = projection['image_width_px']\n            self.image_height_px = projection['image_height_px']\n            self.center_x_px = projection['center_x_px']\n            self.center_y_px = projection['center_y_px']\n            self.scale = np.min([self.image_width_px, self.image_height_px]) / 2\n            self.offset = np.array([self.center_x_px, self.center_y_px])\n\n    def _convert_radius(self, r):\n        return 1 + self.k1*r**2 + self.k2*r**4 + self.k3*r**6\n\n    def compute_intensity(self, points, intensity):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        r = r / r.max()\n        # scale the intensities\n        r_t =  torch.from_numpy(self._convert_radius(r)).T\n        intensity = intensity * r_t\n        return intensity\n\n    def vignet(self, image):\n        x, y = torch.meshgrid([torch.arange(0, self.image_width_px),\n                    torch.arange(0, self.image_height_px)], indexing='ij')\n        x = x.float() + 0.5\n        y = y.float() + 0.5\n        coord = torch.cat((y.unsqueeze(-1), x.unsqueeze(-1)), 2).view(-1, 2)\n        image = image.permute(2,0,1)\n        vignet_image = self.compute_intensity(coord, image.view(3, -1)).reshape(3, self.image_width_px, self.image_height_px)\n\n        return vignet_image.permute(1,2,0)", "class Vignetting(LensDistortion):\n    def __init__(self, k1=None, k2=None, k3=None, projection=None):\n        self.k1 = k1\n        self.k2 = k2\n        self.k3 = k3\n        self.projection = projection\n        if projection is not None:\n            self.image_width_px = projection['image_width_px']\n            self.image_height_px = projection['image_height_px']\n            self.center_x_px = projection['center_x_px']\n            self.center_y_px = projection['center_y_px']\n            self.scale = np.min([self.image_width_px, self.image_height_px]) / 2\n            self.offset = np.array([self.center_x_px, self.center_y_px])\n\n    def _convert_radius(self, r):\n        return 1 + self.k1*r**2 + self.k2*r**4 + self.k3*r**6\n\n    def compute_intensity(self, points, intensity):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        r = r / r.max()\n        # scale the intensities\n        r_t =  torch.from_numpy(self._convert_radius(r)).T\n        intensity = intensity * r_t\n        return intensity\n\n    def vignet(self, image):\n        x, y = torch.meshgrid([torch.arange(0, self.image_width_px),\n                    torch.arange(0, self.image_height_px)], indexing='ij')\n        x = x.float() + 0.5\n        y = y.float() + 0.5\n        coord = torch.cat((y.unsqueeze(-1), x.unsqueeze(-1)), 2).view(-1, 2)\n        image = image.permute(2,0,1)\n        vignet_image = self.compute_intensity(coord, image.view(3, -1)).reshape(3, self.image_width_px, self.image_height_px)\n\n        return vignet_image.permute(1,2,0)", "\n\nclass BrownLensDistortion(LensDistortion):\n    r\"\"\"\n    The most common distortion model is the Brown's distortion model. In CameraTransform, we only consider the radial part\n    of the model, as this covers all common cases and the merit of tangential components is disputed. This model relies on\n    transforming the radius with even polynomial powers in the coefficients :math:`k_1, k_2, k_3`. This distortion model is\n    e.g. also used by OpenCV or AgiSoft PhotoScan.\n    Adjust scale and offset of x and y to be relative to the center:\n    .. math::\n        x' &= \\frac{x-c_x}{f_x}\\\\\n        y' &= \\frac{y-c_y}{f_y}\n    Transform the radius from the center with the distortion:\n    .. math::\n        r &= \\sqrt{x'^2 + y'^2}\\\\\n        r' &= r \\cdot (1 + k_1 \\cdot r^2 + k_2 \\cdot r^4 + k_3 \\cdot r^6)\\\\\n        x_\\mathrm{distorted}' &= x' / r \\cdot r'\\\\\n        y_\\mathrm{distorted}' &= y' / r \\cdot r'\n    Readjust scale and offset to obtain again pixel coordinates:\n    .. math::\n        x_\\mathrm{distorted} &= x_\\mathrm{distorted}' \\cdot f_x + c_x\\\\\n        y_\\mathrm{distorted} &= y_\\mathrm{distorted}' \\cdot f_y + c_y\n    \"\"\"\n    projection = None\n\n    def __init__(self, k1=None, k2=None, k3=None, projection=None):\n        self.k1 = k1\n        self.k2 = k2\n        self.k3 = k3\n        self.projection = projection\n        if projection is not None:\n            self.image_width_px = projection['image_width_px']\n            self.image_height_px = projection['image_height_px']\n            self.center_x_px = projection['center_x_px']\n            self.center_y_px = projection['center_y_px']\n\n        self._init_inverse()\n\n    def _init_inverse(self):\n        r = np.arange(0, 2, 0.1)\n        self._convert_radius_inverse = invert_function(r, self._convert_radius)\n\n        if self.projection is not None:\n            self.scale = np.min([self.image_width_px, self.image_height_px]) / 2\n            self.offset = np.array([self.center_x_px, self.center_y_px])\n        else:\n            self.scale = 1\n            self.offset = np.array([0, 0])\n\n    def _convert_radius(self, r):\n        return r*(1 + self.k1*r**2 + self.k2*r**4 + self.k3*r**6)\n\n    def imageFromDistorted(self, points):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        # transform the points\n        points = points / r * self._convert_radius_inverse(r)\n        # rescale back to the image\n        return points * self.scale + self.offset\n\n    def distortedFromImage(self, points):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        # transform the points\n        points = points / r * self._convert_radius(r)\n        # rescale back to the image\n        return points * self.scale + self.offset\n\n    def distortImage(self, image):\n        x, y = torch.meshgrid([torch.arange(0, self.image_width_px),\n                    torch.arange(0, self.image_height_px)], indexing='ij')\n        x = x.float() + 0.5\n        y = y.float() + 0.5\n        coord = torch.cat((y.unsqueeze(-1), x.unsqueeze(-1)), 2).view(-1, 2)\n        grid = self.imageFromDistorted(coord).reshape(self.image_width_px, self.image_height_px, 2)\n        image_size_xy = torch.tensor([self.image_height_px, self.image_width_px])\n        grid = (grid / image_size_xy.view(1, 1, 2)) * 2 - 1\n        image = image.permute(2,0,1)\n        distort_image = F.grid_sample(image.float().unsqueeze(0), grid.float().unsqueeze(0), align_corners=False, padding_mode=\"border\")[0]\n        distort_image = distort_image.permute(1,2,0)\n\n        return distort_image", "\nclass ABCDistortion(LensDistortion):\n    r\"\"\"\n    The ABC model is a less common distortion model, that just implements radial distortions. Here the radius is transformed\n    using a polynomial of 4th order. It is used e.g. in PTGui.\n    Adjust scale and offset of x and y to be relative to the center:\n    .. math::\n        s &= 0.5 \\cdot \\mathrm{min}(\\mathrm{im}_\\mathrm{width}, \\mathrm{im}_\\mathrm{height})\\\\\n        x' &= \\frac{x-c_x}{s}\\\\\n        y' &= \\frac{y-c_y}{s}\n    Transform the radius from the center with the distortion:\n    .. math::\n        r &= \\sqrt{x^2 + y^2}\\\\\n        r' &= d \\cdot r + c \\cdot r^2 + b \\cdot r^3 + a \\cdot r^4\\\\\n        d &= 1 - a - b - c\n    Readjust scale and offset to obtain again pixel coordinates:\n    .. math::\n        x_\\mathrm{distorted} &= x_\\mathrm{distorted}' \\cdot s + c_x\\\\\n        y_\\mathrm{distorted} &= y_\\mathrm{distorted}' \\cdot s + c_y\n    \"\"\"\n    projection = None\n\n    def __init__(self, a=None, b=None, c=None, projection=None):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.projection = projection\n        if projection is not None:\n            self.image_width_px = projection['image_width_px']\n            self.image_height_px = projection['image_height_px']\n            self.center_x_px = projection['center_x_px']\n            self.center_y_px = projection['center_y_px']\n\n        self._init_inverse()\n\n    def _init_inverse(self):\n        self.d = 1 - self.a - self.b - self.c\n        r = np.arange(0, 2, 0.1)\n        self._convert_radius_inverse = invert_function(r, self._convert_radius)\n\n        if self.projection is not None:\n            self.scale = np.min([self.image_width_px, self.image_height_px]) / 2\n            self.offset = np.array([self.center_x_px, self.center_y_px])\n        else:\n            self.scale = 1\n            self.offset = np.array([0, 0])\n\n    def _convert_radius(self, r):\n        return self.d * r + self.c * r**2 + self.b * r**3 + self.a * r**4\n\n    def imageFromDistorted(self, points):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        # transform the points\n        points = points / r * self._convert_radius_inverse(r)\n        # rescale back to the image\n        return points * self.scale + self.offset\n\n    def distortedFromImage(self, points):\n        # ensure that the points are provided as an array\n        # and rescale the points to that the center is at 0 and the border at 1\n        points = (np.array(points)-self.offset)/self.scale\n        # calculate the radius form the center\n        r = np.linalg.norm(points, axis=-1)[..., None]\n        # transform the points\n        points = points / r * self._convert_radius(r)\n        # rescale back to the image\n        return points * self.scale + self.offset\n\n    def distortImage(self, image):\n        x, y = torch.meshgrid([torch.arange(0, self.image_width_px),\n                    torch.arange(0, self.image_height_px)], indexing='ij')\n        x = x.float() + 0.5\n        y = y.float() + 0.5\n        coord = torch.cat((y.unsqueeze(-1), x.unsqueeze(-1)), 2).view(-1, 2)\n        grid = self.imageFromDistorted(coord).reshape(self.image_width_px, self.image_height_px, 2)\n        image_size_xy = torch.tensor([self.image_height_px, self.image_width_px])\n        grid = (grid / image_size_xy.view(1, 1, 2)) * 2 - 1\n        image = image.permute(2,0,1)\n        distort_image = F.grid_sample(image.float().unsqueeze(0), grid.float().unsqueeze(0), align_corners=False, padding_mode=\"border\")[0]\n        distort_image = distort_image.permute(1,2,0)\n\n        return distort_image", ""]}
{"filename": "SynLens/renderer.py", "chunked_list": ["import os\nimport torch\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport json\nimport imageio\nimport math\n# Util function for loading meshes\nfrom pytorch3d.io import load_objs_as_meshes, save_obj", "# Util function for loading meshes\nfrom pytorch3d.io import load_objs_as_meshes, save_obj\n\nfrom pytorch3d.loss import (\n    chamfer_distance,\n    mesh_edge_loss,\n    mesh_laplacian_smoothing,\n    mesh_normal_consistency,\n)\nimport torch.nn.functional as F", ")\nimport torch.nn.functional as F\n\n# Data structures and functions for rendering\nfrom pytorch3d.structures import Meshes\nfrom pytorch3d.renderer import (\n    look_at_view_transform,\n    FoVPerspectiveCameras,\n    PointLights,\n    TexturesUV,", "    PointLights,\n    TexturesUV,\n    DirectionalLights,\n    Materials,\n    RasterizationSettings,\n    MeshRenderer,\n    MeshRasterizer,\n    SoftPhongShader,\n    SoftSilhouetteShader,\n    SoftPhongShader,", "    SoftSilhouetteShader,\n    SoftPhongShader,\n    TexturesVertex,\n    HardPhongShader\n)\n\n\n# add path for demo utils functions\nimport sys\nimport os", "import sys\nimport os\nsys.path.append(os.path.abspath(''))\n\n\n# Setup\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    torch.cuda.set_device(device)\n    print(\"cuda is available.\")\nelse:\n    device = torch.device(\"cpu\")", "\nclass NumpyEncoder(json.JSONEncoder):\n    \"\"\"Special json encoder for numpy types\"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)", "\ndef camera_arrays(target_cameras, image_size):\n    n_sample = 24\n    i, j = np.meshgrid(\n        np.linspace(-1, 1, n_sample),\n        np.linspace(-1, 1, n_sample),\n        indexing=\"ij\",\n    )\n    i = i.T\n    j = j.T\n    P_sensor = torch.from_numpy(np.stack((i, j), axis=-1)).float()\n    P_sensor_flat = P_sensor.reshape((-1, 2))\n    P_world = torch.cat((P_sensor_flat, torch.ones((P_sensor_flat.shape[0], 1))), dim=1)\n    world_coords = torch.cat((P_sensor_flat, torch.zeros((P_sensor_flat.shape[0], 1))), dim=1)\n    world_coords = world_coords.reshape((-1, 3)).numpy()\n\n    x_coords = []\n    y_coords = []\n    for cameras in target_cameras:\n        ndc_points = cameras.transform_points_ndc(P_world)\n        camera_points = cameras.transform_points(P_world)\n        screen_points = cameras.transform_points_screen(P_world, image_size=(image_size, image_size))\n        x_coords.append(screen_points[:,0])\n        y_coords.append(screen_points[:,1])\n\n    x_coords = torch.stack(x_coords).cpu().numpy()\n    y_coords = torch.stack(y_coords).cpu().numpy()\n    return (x_coords, y_coords, world_coords)", "\ndef image_grid(\n    images,\n    rows=None,\n    cols=None,\n    fill: bool = True,\n    show_axes: bool = False,\n    rgb: bool = True,\n):\n    \"\"\"\n    A util function for plotting a grid of images.\n\n    Args:\n        images: (N, H, W, 4) array of RGBA images\n        rows: number of rows in the grid\n        cols: number of columns in the grid\n        fill: boolean indicating if the space between images should be filled\n        show_axes: boolean indicating if the axes of the plots should be visible\n        rgb: boolean, If True, only RGB channels are plotted.\n            If False, only the alpha channel is plotted.\n\n    Returns:\n        None\n    \"\"\"\n    if (rows is None) != (cols is None):\n        raise ValueError(\"Specify either both rows and cols or neither.\")\n\n    if rows is None:\n        rows = len(images)\n        cols = 1\n\n    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n    bleed = 0\n    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n\n    for ax, im in zip(axarr.ravel(), images):\n        if rgb:\n            # only render RGB channels\n            ax.imshow(im[..., :3])\n        else:\n            # only render Alpha channel\n            ax.imshow(im[..., 3])\n        if not show_axes:\n            ax.set_axis_off()", "\ndef image_grid_vis(\n    images,\n    x_coords,\n    y_coords,\n    rows=None,\n    cols=None,\n    fill: bool = True,\n    show_axes: bool = False,\n    rgb: bool = True,\n):\n    \"\"\"\n    A util function for plotting a grid of images.\n\n    Args:\n        images: (N, H, W, 4) array of RGBA images\n        rows: number of rows in the grid\n        cols: number of columns in the grid\n        fill: boolean indicating if the space between images should be filled\n        show_axes: boolean indicating if the axes of the plots should be visible\n        rgb: boolean, If True, only RGB channels are plotted.\n            If False, only the alpha channel is plotted.\n\n    Returns:\n        None\n    \"\"\"\n    if (rows is None) != (cols is None):\n        raise ValueError(\"Specify either both rows and cols or neither.\")\n\n    if rows is None:\n        rows = len(images)\n        cols = 1\n\n    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n    bleed = 0\n    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n\n    for ax, im, coord_x, coord_y in zip(axarr.ravel(), images, x_coords, y_coords):\n        ax.imshow(im[..., :3])\n        ax.scatter(coord_x, coord_y, marker='o', s=8, color='red')\n        if not show_axes:\n            ax.set_axis_off()", "\ndef render_plane():\n\n    # Define the texture and create a TexturesVertex object\n    texture_image = plt.imread('target_mesh/target.png')\n    texture_image = texture_image[:, :, :3]  # remove alpha channel\n    texture_image = torch.from_numpy(texture_image)\n    texture_image = texture_image.permute(2, 0, 1).float() / 255.0\n    texture_image = torch.unsqueeze(texture_image, dim=0)\n\n    verts_uvs = torch.tensor([\n        [0.0, 0.0],\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [0.0, 1.0],\n    ], dtype=torch.float32)\n\n    faces_uvs = torch.tensor([\n        [0, 1, 2],\n        [0, 2, 3],\n    ], dtype=torch.int64)\n\n    # Create a TexturesUV object\n    textures = TexturesUV(\n        maps=texture_image,\n        faces_uvs=faces_uvs.unsqueeze(0),\n        verts_uvs=verts_uvs.unsqueeze(0),\n    )\n\n    # Create a square mesh with UV coordinates\n    vertices = torch.tensor([    [-1.0, -1.0, 0.0],  # bottom left\n        [ 1.0, -1.0, 0.0],  # bottom right\n        [ 1.0,  1.0, 0.0],  # top right\n        [-1.0,  1.0, 0.0],  # top left\n    ], dtype=torch.float32)\n\n    faces = torch.tensor([    [0, 1, 2],\n        [0, 2, 3],\n    ], dtype=torch.int64)\n\n    mesh = Meshes(\n        verts=[vertices],\n        faces=[faces],\n        textures=textures,\n    )\n\n    # Move the mesh to the center of the scene\n    verts = mesh.verts_packed()\n    N = verts.shape[0]\n    center = verts.mean(0)\n    scale = max((verts - center).abs().max(0)[0])\n    mesh.offset_verts_(-center)\n    mesh.scale_verts_((1.0 / float(scale)))\n\n    num_views = 10\n    # Define the camera and rasterization settings\n    # Get a batch of viewing angles.\n    elev = torch.linspace(0, 0, num_views)\n    azim = torch.linspace(-20, 20, num_views)\n    # Place a point light in front of the object. As mentioned above, the front of\n    # the cow is facing the -z direction.\n    lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n\n    # Initialize an OpenGL perspective camera that represents a batch of different\n    # viewing angles. All the cameras helper methods support mixed type inputs and\n    # broadcasting. So we can view the camera from the a distance of dist=2.7, and\n    # then specify elevation and azimuth angles for each viewpoint as tensors.\n    at = np.array([[np.cos(t) * 0.5, np.sin(t) * 0.5, 0] for t in np.linspace(-np.pi, np.pi, num_views)])\n\n    R, T = look_at_view_transform(dist=3.6, elev=elev, azim=azim) #, at=at)\n\n    fov = lens_model[\"focal\"]\n    cameras = FoVPerspectiveCameras(device=device, fov=fov, R=R, T=T)\n\n    raster_settings = RasterizationSettings(image_size=595, blur_radius=0.0, faces_per_pixel=1)\n\n    # Create a MeshRasterizer and MeshRenderer object\n    rasterizer = MeshRasterizer(cameras=cameras, raster_settings=raster_settings)\n    shader = HardPhongShader(device=torch.device(\"cpu\"), lights=None)\n\n    # Render the mesh from different camera perspectives\n\n\n    # Display the rendered images\n    fig, axs = plt.subplots(1, len(images), figsize=(20, 5))\n    for i, image in enumerate(images):\n        axs[i].imshow(image.squeeze().cpu().numpy())\n    plt.show()", "\ndef render_mesh(obj_filename, num_views, image_size, render_rgb, lens_model):\n    # the number of different viewpoints from which we want to render the mesh.\n    # num_views = 200\n    # Set paths\n    # obj_filename = \"schops_mesh/cube.obj\"\n\n    # Load obj file\n    # mesh = load_objs_as_meshes([obj_filename], device=device)\n\n    # Define the texture and create a TexturesVertex object\n    texture_image = plt.imread('target_mesh/target.png')\n    texture_image = texture_image[:, :, :3]  # remove alpha channel\n    texture_image = torch.from_numpy(texture_image)\n    texture_image = texture_image.permute(2, 0, 1).float() / 255.0\n    texture_image = torch.unsqueeze(texture_image, dim=0)\n\n    verts_uvs = torch.tensor([\n        [0.0, 0.0],\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [0.0, 1.0],\n    ], dtype=torch.float32)\n\n    faces_uvs = torch.tensor([\n        [0, 1, 2],\n        [0, 2, 3],\n    ], dtype=torch.int64)\n\n    # Create a TexturesUV object\n    textures = TexturesUV(\n        maps=texture_image,\n        faces_uvs=faces_uvs.unsqueeze(0),\n        verts_uvs=verts_uvs.unsqueeze(0),\n    )\n\n    # Create a square mesh with UV coordinates\n    vertices = torch.tensor([    [-1.0, -1.0, 0.0],  # bottom left\n        [ 1.0, -1.0, 0.0],  # bottom right\n        [ 1.0,  1.0, 0.0],  # top right\n        [-1.0,  1.0, 0.0],  # top left\n    ], dtype=torch.float32)\n\n    faces = torch.tensor([    [0, 1, 2],\n        [0, 2, 3],\n    ], dtype=torch.int64)\n\n    mesh = Meshes(\n        verts=[vertices],\n        faces=[faces],\n        textures=textures,\n    )\n\n\n    # We scale normalize and center the target mesh to fit in a sphere of radius 1\n    # centered at (0,0,0). (scale, center) will be used to bring the predicted mesh\n    # to its original center and scale.  Note that normalizing the target mesh,\n    # speeds up the optimization but is not necessary!\n    verts = mesh.verts_packed()\n    N = verts.shape[0]\n    center = verts.mean(0)\n    scale = max((verts - center).abs().max(0)[0])\n    mesh.offset_verts_(-center)\n    mesh.scale_verts_((1.0 / float(scale)))\n\n    # Get a batch of viewing angles.\n    elev = torch.linspace(0, 0, num_views)\n    azim = torch.linspace(-20, 20, num_views)\n    # Place a point light in front of the object. As mentioned above, the front of\n    # the cow is facing the -z direction.\n    lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n\n    # Initialize an OpenGL perspective camera that represents a batch of different\n    # viewing angles. All the cameras helper methods support mixed type inputs and\n    # broadcasting. So we can view the camera from the a distance of dist=2.7, and\n    # then specify elevation and azimuth angles for each viewpoint as tensors.\n    at = np.array([[np.cos(t) * 0.5, np.sin(t) * 0.5, 0] for t in np.linspace(-np.pi, np.pi, num_views)])\n\n    R, T = look_at_view_transform(dist=3.6, elev=elev, azim=azim) #, at=at)\n\n    fov = lens_model[\"focal\"]\n    cameras = FoVPerspectiveCameras(device=device, fov=fov, R=R, T=T)\n    target_cameras = [FoVPerspectiveCameras(device=device, fov=fov, R=R[None, i, ...],\n                                            T=T[None, i, ...]) for i in range(num_views)]\n\n    if render_rgb:\n        # Define the settings for rasterization and shading. Here we set the output\n        # image to be of size 512X512. As we are rendering images for visualization\n        # purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to\n        # rasterize_meshes.py for explanations of these parameters.  We also leave\n        # bin_size and max_faces_per_bin to their default values of None, which sets\n        # their values using heuristics and ensures that the faster coarse-to-fine\n        # rasterization method is used.  Refer to docs/notes/renderer.md for an\n        # explanation of the difference between naive and coarse-to-fine rasterization.\n        raster_settings = RasterizationSettings(\n            image_size=image_size,\n            blur_radius=0.0,\n            faces_per_pixel=1,\n        )\n        # Create a Phong renderer by composing a rasterizer and a shader. The textured\n        # Phong shader will interpolate the texture uv coordinates for each vertex,\n        # sample from a texture image and apply the Phong lighting model\n\n        renderer = MeshRenderer(\n            rasterizer=MeshRasterizer(\n                cameras=cameras,\n                raster_settings=raster_settings\n            ),\n            shader=SoftPhongShader(\n                device=device,\n                cameras=cameras,\n                lights=lights\n            )\n        )\n        # Create a batch of meshes by repeating the mesh and associated textures.\n        # Meshes has a useful `extend` method which allows us do this very easily.\n        # This also extends the textures.\n        meshes = mesh.extend(num_views)\n\n        # Render the mesh from each viewing angle\n        target_images = renderer(meshes, cameras=cameras, lights=lights)\n        # Our multi-view cow dataset will be represented by these 2 lists of tensors,\n        # each of length num_views.\n        target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n    else:\n        target_rgb = None\n\n    return (target_cameras, target_rgb)", "\ndef process_mesh(lens_model, lens_name, num_views, image_size, render_rgb, out_fd):\n    # print(lens_model)\n    obj_filename = \"target_mesh/cube.obj\"\n    if not os.path.exists(out_fd):\n        os.mkdir(out_fd)\n\n    # render_plane()\n    # sys.exit()\n    target_cameras, target_rgb = render_mesh(obj_filename, num_views, image_size, render_rgb, lens_model)\n    x_coords, y_coords, world_coords = camera_arrays(target_cameras, image_size)\n    screen_coords = np.array([x_coords, y_coords]).T\n\n    from lens_distortion import BrownLensDistortion, ABCDistortion\n\n    projection = {}\n    projection['image_width_px'] = image_size\n    projection['image_height_px'] = image_size\n    projection['center_x_px'] = image_size / 2\n    projection['center_y_px'] = image_size / 2\n\n    coeffs = lens_model[\"coeffs\"]\n    lens_type = lens_model[\"lens_type\"]\n    if lens_type == \"abc\":\n        lens = ABCDistortion(a=coeffs[0], b=coeffs[1]*4, c=coeffs[2], projection=projection)\n        cam_coords = lens.distortedFromImage(screen_coords)\n    elif lens_type == \"brown\":\n        lens = BrownLensDistortion(k1=coeffs[0], k2=coeffs[1], k3=coeffs[2], projection=projection)\n        cam_coords = lens.distortedFromImage(screen_coords)\n\n    board_coords = []\n    frame_coords = []\n\n    for i in range(len(target_cameras)):\n        frame_coords.append(cam_coords[:, i, :])\n        board_coords.append(world_coords)\n\n    with open(\"{}/{}_{:02d}_points.json\".format(out_fd, lens_name, int(lens_model[\"model_name\"])), \"w\") as outf:\n            json.dump(\n                {\n                    \"frame_coordinates_xy\": frame_coords,\n                    \"board_coordinates_xyz\": board_coords,\n                    \"resolution_wh\": (image_size, image_size),\n                },\n                outf,\n                cls=NumpyEncoder,\n                indent=4,\n                sort_keys=False,\n            )\n\n    if target_rgb is not None:\n        from lens_distortion import Vignetting\n\n        image_grid_vis(target_rgb[:10], x_coords[:10], y_coords[:10], rows=2, cols=5, rgb=True)\n        # image_grid(target_rgb[:10], rows=2, cols=5, rgb=True)\n        # plt.show()\n\n        vig = Vignetting(k1=-0.19, k2=0.09, k3=-0.39, projection=projection)\n        vig_rgb = [vig.vignet(lens.distortImage(target_rgb[i])) for i in range(num_views)]\n        vig_rgb = np.stack(vig_rgb, 0)\n        image_grid(vig_rgb, rows=2, cols=5, rgb=True)\n\n        undist_rgb = [lens.distortImage(target_rgb[i]) for i in range(num_views)]\n\n        brown_undist_rgb = np.stack(undist_rgb, 0)\n        image_grid_vis(brown_undist_rgb, cam_coords.T[0, :], cam_coords.T[1, :], rows=2, cols=5, rgb=True)\n        # plt.show()\n\n        # image_grid(brown_undist_rgb, rows=2, cols=5, rgb=True)\n        # plt.show()\n\n        output_dir = \"{}/{}_{:02d}_frames\".format(out_fd, lens_name, int(lens_model[\"model_name\"]))\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        vig_output_dir = \"{}/{}_{:02d}_vig_frames\".format(out_fd, lens_name, int(lens_model[\"model_name\"]))\n        if not os.path.exists(vig_output_dir):\n            os.makedirs(vig_output_dir)\n\n        for i in range(len(target_cameras)):\n            imageio.imwrite(\n                '{}/frame_{:05d}.png'.format(vig_output_dir, i), np.uint8(vig_rgb[i] * 255))\n            imageio.imwrite(\n                '{}/frame_{:05d}.png'.format(output_dir, i), np.uint8(brown_undist_rgb[i] * 255))", ""]}
{"filename": "calibration/networks.py", "chunked_list": ["import FrEIA.framework as Ff\nimport FrEIA.modules as Fm\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom toolz import curry\nfrom torch import nn\n\n\ndef init_weights_zero(m):\n    if isinstance(m, nn.Linear):\n        m.weight.data.fill_(0.0)\n        m.bias.data.fill_(0.0)", "\ndef init_weights_zero(m):\n    if isinstance(m, nn.Linear):\n        m.weight.data.fill_(0.0)\n        m.bias.data.fill_(0.0)\n\n\n@curry\ndef subnet_fc(c_in, c_out, zero_init=False):\n    seq_block = nn.Sequential(nn.Linear(c_in, 256), nn.ELU(), nn.Linear(256, c_out))\n    if zero_init:\n        seq_block.apply(init_weights_zero)\n    return seq_block", "def subnet_fc(c_in, c_out, zero_init=False):\n    seq_block = nn.Sequential(nn.Linear(c_in, 256), nn.ELU(), nn.Linear(256, c_out))\n    if zero_init:\n        seq_block.apply(init_weights_zero)\n    return seq_block\n\n\nclass LensNet(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # self.save_hyperparameters()\n        self.bidirectional_lens = Ff.SequenceINN(2)\n        depth = 12\n        for k in range(depth):\n            self.bidirectional_lens.append(\n                Fm.AllInOneBlock,\n                subnet_constructor=subnet_fc(zero_init=(k == (depth - 1))),\n                permute_soft=True,\n            )\n\n    def forward(self, rays, sensor_to_frustum=True):\n        if sensor_to_frustum:\n            return self.bidirectional_lens(rays)[0]\n        else:\n            return self.bidirectional_lens(rays, rev=True)[0]", "\n\nclass iResNet(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.batch_size = 16\n        self.tol = 1e-6\n        self.inp_size_linear = (2,)\n\n        torch.manual_seed(0)\n        nodes = [Ff.graph_inn.InputNode(*self.inp_size_linear, name=\"input\")]\n        for i in range(5):\n        \n            nodes.append(\n                Ff.graph_inn.Node(\n                    nodes[-1],\n                    Fm.IResNetLayer,\n                    {\n                        \"hutchinson_samples\": 1, \n                        \"internal_size\": 1024, \n                        \"n_internal_layers\": 4, \n                    },\n                    conditions=[],\n                    name=f\"i_resnet_{i}\",\n                )\n            )\n        nodes.append(Ff.graph_inn.OutputNode(nodes[-1], name=\"output\"))\n        self.i_resnet_linear = Ff.GraphINN(nodes, verbose=False)\n\n        for node in self.i_resnet_linear.node_list:\n            if isinstance(node.module, Fm.IResNetLayer):\n                node.module.lipschitz_correction()\n\n    def forward(self, rays, sensor_to_frustum=True):\n        if sensor_to_frustum:\n            return self.i_resnet_linear(rays, jac=False)[0]\n        else:\n            return self.i_resnet_linear(rays, rev=True, jac=False)[0]\n\n    def test_inverse(self):\n        x = torch.randn(self.batch_size, *self.inp_size_linear)\n        x = x * torch.randn_like(x)\n        x = x + torch.randn_like(x)\n\n        y = self.i_resnet_linear(x, jac=False)[0]\n        x_hat = self.i_resnet_linear(y, rev=True, jac=False)[0]\n\n        print(\"Check that inverse is close to input\")\n        assert torch.allclose(x, x_hat, atol=self.tol)", "\n\nclass LipBoundedPosEnc(nn.Module):\n    def __init__(self, inp_features, n_freq, cat_inp=True):\n        super().__init__()\n        self.inp_feat = inp_features\n        self.n_freq = n_freq\n        self.cat_inp = cat_inp\n        self.out_dim = 2 * self.n_freq * self.inp_feat\n        if self.cat_inp:\n            self.out_dim += self.inp_feat\n\n    def forward(self, x):\n        \"\"\"\n        :param x: (bs, npoints, inp_features)\n        :return: (bs, npoints, 2 * out_features + inp_features)\n        \"\"\"\n        assert len(x.size()) == 3\n        bs, npts = x.size(0), x.size(1)\n        const = (2 ** torch.arange(self.n_freq) * np.pi).view(1, 1, 1, -1)\n        const = const.to(x)\n\n        # Out shape : (bs, npoints, out_feat)\n        cos_feat = torch.cos(const * x.unsqueeze(-1)).view(bs, npts, self.inp_feat, -1)\n        sin_feat = torch.sin(const * x.unsqueeze(-1)).view(bs, npts, self.inp_feat, -1)\n        out = torch.cat([sin_feat, cos_feat], dim=-1).view(\n            bs, npts, 2 * self.inp_feat * self.n_freq\n        )\n        const_norm = (\n            torch.cat([const, const], dim=-1)\n            .view(1, 1, 1, self.n_freq * 2)\n            .expand(-1, -1, self.inp_feat, -1)\n            .reshape(1, 1, 2 * self.inp_feat * self.n_freq)\n        )\n\n        if self.cat_inp:\n            out = torch.cat([out, x], dim=-1)\n            const_norm = torch.cat(\n                [const_norm, torch.ones(1, 1, self.inp_feat).to(x)], dim=-1\n            )\n\n            return out / const_norm / np.sqrt(self.n_freq * 2 + 2)\n        else:\n\n            return out / const_norm / np.sqrt(self.n_freq * 2)", "\n\nclass InvertibleResBlockLinear(nn.Module):\n    def __init__(\n        self, inp_dim, hid_dim, nblocks=1, nonlin=\"leaky_relu\", pos_enc_freq=None\n    ):\n        super().__init__()\n        self.dim = inp_dim\n        self.nblocks = nblocks\n\n        self.pos_enc_freq = pos_enc_freq\n        if self.pos_enc_freq is not None:\n            inp_dim_af_pe = self.dim * (self.pos_enc_freq * 2 + 1)\n            self.pos_enc = LipBoundedPosEnc(self.dim, self.pos_enc_freq)\n        else:\n            self.pos_enc = lambda x: x\n            inp_dim_af_pe = inp_dim\n\n        self.blocks = nn.ModuleList()\n        self.blocks.append(nn.utils.spectral_norm(nn.Linear(inp_dim_af_pe, hid_dim)))\n        for _ in range(self.nblocks):\n            self.blocks.append(\n                nn.utils.spectral_norm(\n                    nn.Linear(hid_dim, hid_dim),\n                )\n            )\n        self.blocks.append(\n            nn.utils.spectral_norm(\n                nn.Linear(hid_dim, self.dim),\n            )\n        )\n\n        self.nonlin = nonlin.lower()\n        if self.nonlin == \"leaky_relu\":\n            self.act = nn.LeakyReLU()\n        elif self.nonlin == \"relu\":\n            self.act = nn.ReLU()\n        elif self.nonlin == \"elu\":\n            self.act = nn.ELU()\n        elif self.nonlin == \"softplus\":\n            self.act = nn.Softplus()\n        else:\n            raise NotImplementedError\n\n    def forward_g(self, x):\n        orig_dim = len(x.size())\n        if orig_dim == 2:\n            x = x.unsqueeze(0)\n\n        y = self.pos_enc(x)\n        for block in self.blocks[:-1]:\n            y = self.act(block(y))\n        y = self.blocks[-1](y)\n\n        if orig_dim == 2:\n            y = y.squeeze(0)\n\n        return y\n\n    def forward(self, x):\n        return x + self.forward_g(x)\n\n    def invert(self, y, verbose=False, iters=35):\n        return self.fixed_point_invert(\n            lambda x: self.forward_g(x), y, iters=iters, verbose=verbose\n        )\n\n    def fixed_point_invert(self, g, y, iters=35, verbose=False):\n        with torch.no_grad():\n            x = y\n            dim = x.size(-1)\n            for i in range(iters):\n                x = y - g(x)\n                if verbose:\n                    err = (y - (x + g(x))).view(-1, dim).norm(dim=-1).mean()\n                    err = err.detach().cpu().item()\n                    print(\"iter:%d err:%s\" % (i, err))\n        return x", "\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dim = 2\n        self.out_dim = 2\n        self.hidden_size = 512\n        self.n_blocks = 5\n        self.n_g_blocks = 5\n        self.tol = 1e-6\n\n        # Network modules\n        self.blocks = nn.ModuleList()\n        for _ in range(self.n_blocks):\n            self.blocks.append(\n                InvertibleResBlockLinear(\n                    self.dim,\n                    self.hidden_size,\n                    nblocks=self.n_g_blocks,\n                    nonlin=\"elu\",\n                    pos_enc_freq=5,\n                )\n            )\n\n    def forward(self, rays, sensor_to_frustum=True):\n        if sensor_to_frustum:\n            rays = rays.unsqueeze(0)\n            out = rays\n            for block in self.blocks:\n                out = block(out)\n            return out[0]\n        else:\n            rays = rays.unsqueeze(0)\n            x = rays\n            for block in self.blocks[::-1]:\n                x = block.invert(x, verbose=False, iters=35)\n            return x[0]\n\n    def test_inverse(self):\n        x = torch.rand(7, self.dim) * 2 - 1\n\n        y = self.forward(x)\n        print(y.max(dim=0), y.min(dim=0))", ""]}
{"filename": "calibration/model.py", "chunked_list": ["from typing import Optional\nimport torch\nfrom torch import nn\nimport pytorch_lightning as pl\nimport time\nfrom collections import deque\nfrom ray import tune\nfrom hydra.utils import instantiate\n\nfrom . import standard_models", "\nfrom . import standard_models\nfrom .config import ConfigMarker, ConfigImageFormation\nfrom .marker import Marker\n\n\nclass Model(pl.LightningModule):\n    \"\"\"\n    This model represents the entire process from marker creation to recording to point detection.\n\n    If called without an already detected and extracted marker, it will randomly project\n    the marker and simulate the entire image formation pipeline, then run it through\n    the center point detection and return the results (true center points and predicted center\n    points) to enable optimization of the entire model.\n\n    If a detection is provided, it uses the existing detection and provides the estimated\n    marker center point.\n    \"\"\"\n\n    def __init__(\n        self,\n        marker: ConfigMarker,\n        image_formation: Optional[ConfigImageFormation] = None,\n        batch_size=4,\n        log_every=-1,\n        lr=1e-2,\n        lr_fcn_fac=1.0,\n        lr_marker_fac=100.0,\n        n_latent=200,\n        n_hidden=2,\n        reg_weight=0.0,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.marker = instantiate(marker)\n        if image_formation is not None:\n            self.image_formation = instantiate(image_formation)\n        else:\n            self.image_formation = None\n        self.batch_size = batch_size\n        # Use an efficient predictor to create a compact, latent representation.\n        self.predictor = standard_models.mobilenet_v3_small(\n            pretrained=True, progress=True\n        )\n        # Use an FCN to find the center coordinate.\n        self.fcn = nn.Sequential(\n            *(\n                [\n                    nn.Linear(1000, n_latent, True),\n                    nn.ReLU(),\n                ]\n                + [nn.Linear(n_latent, n_latent, True), nn.ReLU()] * (n_hidden - 1)\n                + [\n                    nn.Linear(n_latent, 3),\n                ]\n            )\n        )\n        self._last_log_written = 0\n        self.log_every = log_every\n        self.lr = lr\n        self.lr_fcn_fac = lr_fcn_fac\n        self.lr_marker_fac = lr_marker_fac\n        self.loss_deque = deque(maxlen=50)\n        self.reg_weight = reg_weight\n        self.gnllloss = nn.GaussianNLLLoss(reduction=\"sum\")\n\n    def forward(self, detection=None):\n        if detection is None:\n            # Go through the image formation process.\n            self._marker, self._marker_center = self.marker(self.batch_size)\n            self._detection, self._detection_center = self.image_formation(\n                self._marker, self._marker_center\n            )\n        else:\n            # Test time, we're looking for detections with unknown center.\n            self._detection_center = None\n            self._detection = detection\n        clres = self.predictor(self._detection)\n        fcnres = self.fcn(clres)\n        coords = torch.sigmoid(fcnres[:, :2]) * self.marker.working_size\n        # We are predicting the standard deviation and are limiting\n        # it to a reasonable size to stabilize the optimization.\n        sds = torch.sigmoid(fcnres[:, 2:3]) * (self.marker.working_size / 2)\n        variances = torch.square(sds)\n        return coords, variances\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            [\n                {\"params\": self.predictor.parameters(), \"lr\": self.lr},\n                {\"params\": self.fcn.parameters(), \"lr\": self.lr * self.lr_fcn_fac},\n                {\n                    \"params\": self.marker.parameters(),\n                    \"lr\": self.lr * self.lr_marker_fac,\n                },\n            ]\n        )\n        return optimizer\n\n    def training_step(self, train_batch, batch_idx):\n        coords, variances = self.forward()\n        # L1 loss.\n        # self._loss = torch.sum((coords - self._detection_center).abs())\n        # Gaussian negative log likelihood, homoscedastic.\n        self._loss = self.gnllloss(coords, self._detection_center, variances)\n        distances = torch.linalg.vector_norm(coords - self._detection_center, dim=1)\n        if self.reg_weight > 0.0:\n            self._regularizer = torch.sum((1.0 - self._marker).abs()) * self.reg_weight\n        else:\n            self._regularizer = 0.0\n        self.maybe_log(\n            coords, self._loss, self._regularizer, distances, variances, batch_idx\n        )\n        self.loss_deque.append(self._loss.item())\n        if len(self.loss_deque) >= 50:\n            self.loss_deque.popleft()\n        if tune.is_session_enabled():\n            tune.report(iterations=batch_idx, accuracy=self._loss.item())\n        return self._loss + self._regularizer\n\n    def maybe_log(self, coords, loss, regularizer, distances, variances, batch_idx):\n        if time.time() - self._last_log_written > self.log_every:\n            # Scalars.\n            self.log(\"max/distance\", distances.max())\n            self.log(\"max/variance\", variances.max())\n            self.log(\"average/distance\", distances.mean())\n            self.log(\"average/variance\", variances.mean())\n            self.log(\"average/95perc_conf_dist\", 2.0 * torch.sqrt(variances).mean())\n            self.log(\"train/loss\", loss)\n            self.log(\"train/regularizer\", regularizer)\n            if hasattr(self, \"_marker_grad\") and self._marker_grad is not None:\n                self.logger.experiment.add_histogram(\n                    \"grad/marker\", self.marker.marker_mem.grad, batch_idx\n                )\n            # Histograms.\n            self.logger.experiment.add_histogram(\n                \"val/marker\", self.marker.marker_mem, batch_idx\n            )\n            self.logger.experiment.add_histogram(\"val/distances\", distances, batch_idx)\n            self.logger.experiment.add_histogram(\"val/variances\", variances, batch_idx)\n            # Add images.\n            self.logger.experiment.add_image(\n                f\"images/marker\",\n                (self._marker[0] * 255.0).to(torch.uint8),\n                batch_idx,\n                dataformats=\"CWH\",\n            )\n            self.logger.experiment.add_image(\n                f\"images/detections\",\n                (self._detection * 255.0).to(torch.uint8),\n                batch_idx,\n                dataformats=\"NCHW\",\n            )\n            center_vis = self._detection.clone()\n            for idx in range(self._detection.shape[0]):\n                center_vis[\n                    idx,\n                    :,\n                    self._detection_center[idx, 0].long()\n                    - 3 : self._detection_center[idx, 0].long()\n                    + 3,\n                    self._detection_center[idx, 1].long()\n                    - 3 : self._detection_center[idx, 1].long()\n                    + 3,\n                ] = 0.0\n                resize_fac = self.marker.working_size / self.marker.size\n                center_vis[\n                    idx,\n                    :,\n                    (self._marker_center[idx, 0] * resize_fac).long()\n                    - 3 : (self._marker_center[idx, 0] * resize_fac).long()\n                    + 3,\n                    (self._marker_center[idx, 1] * resize_fac).long()\n                    - 3 : (self._marker_center[idx, 1] * resize_fac).long()\n                    + 3,\n                ] = 0.0\n                center_vis[\n                    idx,\n                    :,\n                    coords[idx, 0].long() - 3 : coords[idx, 0].long() + 3,\n                    coords[idx, 1].long() - 3 : coords[idx, 1].long() + 3,\n                ] = 0.0\n                # red detection center.\n                center_vis[\n                    idx,\n                    0,\n                    self._detection_center[idx, 0].long()\n                    - 3 : self._detection_center[idx, 0].long()\n                    + 3,\n                    self._detection_center[idx, 1].long()\n                    - 3 : self._detection_center[idx, 1].long()\n                    + 3,\n                ] = 1.0\n                # green: original marker center.\n                center_vis[\n                    idx,\n                    1,\n                    (self._marker_center[idx, 0] * resize_fac).long()\n                    - 3 : (self._marker_center[idx, 0] * resize_fac).long()\n                    + 3,\n                    (self._marker_center[idx, 1] * resize_fac).long()\n                    - 3 : (self._marker_center[idx, 1] * resize_fac).long()\n                    + 3,\n                ] = 1.0\n                # blue: model estimation for the center.\n                center_vis[\n                    idx,\n                    2,\n                    coords[idx, 0].long() - 3 : coords[idx, 0].long() + 3,\n                    coords[idx, 1].long() - 3 : coords[idx, 1].long() + 3,\n                ] = 1.0\n            self.logger.experiment.add_image(\n                f\"images/centers\",\n                (center_vis * 255.0).to(torch.uint8),\n                batch_idx,\n                dataformats=\"NCHW\",\n            )\n            self._last_log_written = time.time()\n\n    def backward(self, loss, optimizer, optimizer_idx):\n        loss.backward()\n        self._marker_grad = self.marker.marker_mem.grad", ""]}
{"filename": "calibration/keypoint_detection.py", "chunked_list": ["import logging\nfrom os import path\nimport cv2\nimport math\nimport numpy as np\nimport torch\n\nfrom .target import calculate_parameters\nfrom .model import Model\nfrom .config import Config, get_latest_checkpoint, ARUCO_DICT", "from .model import Model\nfrom .config import Config, get_latest_checkpoint, ARUCO_DICT\n\n\nLOGGER = logging.getLogger(__name__)\nMODEL = None\nARUCO_PS = None\nTARGET_PS = None\n\n\ndef process_frame(frame, cfg: Config):\n    \"\"\"\n    Process a frame and run the keypoint detector over it.\n\n    First locates the ArUco markers. Then infers a rough position\n    for all keypoints. Runs the keypoint detector on all of these\n    areas and returns their centers, the validity, coordinates\n    and a visualization.\n\n    Returns centers_found, centers_valid, centers_coords, vis_frame.\n    \"\"\"\n    global MODEL, ARUCO_PS, TARGET_PS\n    if MODEL is None:\n        LOGGER.info(f\"Loading model from experiment `{cfg.target.exp_name}`...\")\n        latest_checkpoint = get_latest_checkpoint(cfg.target.exp_name, cfg)\n        MODEL = Model.load_from_checkpoint(latest_checkpoint).cuda()\n        LOGGER.info(\"Loading successful.\")\n    if ARUCO_PS is None:\n        LOGGER.info(\n            f\"Setting up ArUco parameters for marker `{cfg.target.aruco_id}`...\"\n        )\n        ARUCO_PS = (\n            cv2.aruco.Dictionary_get(ARUCO_DICT[cfg.target.aruco_id]),\n            cv2.aruco.DetectorParameters_create(),\n        )\n        ARUCO_PS[1].cornerRefinementMethod = cv2.aruco.CORNER_REFINE_APRILTAG\n        LOGGER.info(\"Setup successful.\")\n    if TARGET_PS is None:\n        LOGGER.info(\"Calculating target parameters...\")\n        TARGET_PS = calculate_parameters(cfg)\n        LOGGER.info(\"Done.\")\n    centers_found = []\n    centers_valid = []\n    centers_coords = []\n    # TODO: resize frame before detection?\n    (corners, ids, _) = cv2.aruco.detectMarkers(\n        frame, ARUCO_PS[0], parameters=ARUCO_PS[1]\n    )\n    vis_frame = frame.copy()\n    corners_marker = np.array(\n        [\n            [0.0, 0.0],  # tl\n            [cfg.model.marker.working_size, 0.0],  # tr\n            [cfg.model.marker.working_size, cfg.model.marker.working_size],  # br\n            [0.0, cfg.model.marker.working_size],  # bl\n        ],\n        dtype=np.float32,\n    )\n    if len(corners) > 0:\n        if cfg.dbg:\n            cv2.namedWindow(\"[dbg] markers\")\n            dbg_frame = frame.copy()\n            cv2.aruco.drawDetectedMarkers(dbg_frame, corners)\n            cv2.imshow(\"[dbg] markers\", dbg_frame)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] markers\")\n        cv2.aruco.drawDetectedMarkers(vis_frame, corners, ids)\n        ids = ids.flatten()\n        for (markerCorner, markerID) in zip(corners, ids):\n            if markerID not in [23, 24, 25, 26]:\n                continue\n            corners = markerCorner.reshape((4, 2))\n            # The marker is printed upside-down (that's why tl-tr-br-bl is swapped\n            # to bl-br-tr-tl).\n            corners_grid = np.array(\n                [\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23],\n                        TARGET_PS.tag_y_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                    ],  # bl\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                        TARGET_PS.tag_y_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                    ],  # br\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                        TARGET_PS.tag_y_grid[markerID - 23],\n                    ],  # tr\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23],\n                        TARGET_PS.tag_y_grid[markerID - 23],\n                    ],  # tl\n                ],\n                dtype=np.float32,\n            )\n            LOGGER.debug(f\"ArUco marker grid position: {corners_grid}.\")\n            # Define a grid coordinate system over the target as:\n            # +------+------+------+------+\n            # |      |      |      |      |\n            # +------X------+------X------+\n            # |      |      |      |      |\n            # +------+------+------+------+\n            # |      |      |      |      |\n            # +------X------+------X------+\n            # |      |      |      |      |\n            # +------+------+------+------+\n            # with the four X's marking the corners of the ArUco\n            # marker.\n            hom_grid_frame, _ = cv2.findHomography(corners_grid, corners)\n            # debugging:\n            # corners_grid_hom = np.array(\n            #     [\n            #         [TARGET_PS.tag_x_grid, TARGET_PS.tag_y_grid, 1],  # tl\n            #         [TARGET_PS.tag_x_grid + 1, TARGET_PS.tag_y_grid, 1],  # tr\n            #         [TARGET_PS.tag_x_grid + 1, TARGET_PS.tag_y_grid + 1, 1],  # br\n            #         [TARGET_PS.tag_x_grid, TARGET_PS.tag_y_grid + 1, 1],  # bl\n            #     ],\n            #     dtype=np.float32,\n            # )\n            # corners_hom = (hom_grid_frame @ corners_grid_hom.T).T\n            # corners_calc = corners_hom[:, :2] / corners_hom[:, 2:3]\n            with BatchedKeypointDetector(\n                centers_found, centers_valid, centers_coords, cfg\n            ) as bkd:\n                for x in range(\n                    max(\n                        0,\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        - TARGET_PS.x_part_length_grid,\n                    ),\n                    min(\n                        TARGET_PS.n_squares_x,\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares\n                        + TARGET_PS.x_part_length_grid,\n                    ),\n                ):\n                    for y in range(\n                        max(\n                            0,\n                            TARGET_PS.tag_y_grid[markerID - 23]\n                            - TARGET_PS.y_part_length_grid,\n                        ),\n                        min(\n                            TARGET_PS.n_squares_y,\n                            TARGET_PS.tag_y_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                            + TARGET_PS.y_part_length_grid,\n                        ),\n                    ):\n                        if (\n                            x >= TARGET_PS.tag_x_grid[markerID - 23]\n                            and x\n                            < TARGET_PS.tag_x_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                            and y >= TARGET_PS.tag_y_grid[markerID - 23]\n                            and y\n                            < TARGET_PS.tag_y_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                        ):\n                            # This is the ArUco marker section.\n                            continue\n                        corners_grid_hom = np.array(\n                            [\n                                [x, y, 1.0],  # tl\n                                [x + 1, y, 1.0],  # tr\n                                [x + 1, y + 1, 1.0],  # br\n                                [x, y + 1, 1.0],  # bl\n                            ],\n                            dtype=np.float32,\n                        )\n                        corners_frame_hom = (hom_grid_frame @ corners_grid_hom.T).T\n                        corners_frame = (\n                            corners_frame_hom[:, :2] / corners_frame_hom[:, 2:3]\n                        )\n                        valid = True\n                        for corner_x, corner_y in corners_frame:\n                            if (\n                                corner_x < 0\n                                or corner_y < 0\n                                or corner_x > frame.shape[1]\n                                or corner_y > frame.shape[0]\n                            ):\n                                valid = False\n                                break\n                        line_thickness = 2\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[0].astype(np.int32)),\n                            tuple(corners_frame[1].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[1].astype(np.int32)),\n                            tuple(corners_frame[2].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[2].astype(np.int32)),\n                            tuple(corners_frame[3].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[3].astype(np.int32)),\n                            tuple(corners_frame[0].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        if not valid:\n                            LOGGER.debug(\n                                f\"Marker at position {x}, {y} (x, y) not fully visible: {corners_frame}.\"\n                            )\n                            continue\n                        hom_boxframe_marker, _ = cv2.findHomography(\n                            corners_frame, corners_marker\n                        )\n                        # We're not using `np.linalg.inv(hom_boxframe_marker)` here,\n                        # which would be an option but is less numerically stable and\n                        # accurate than optimizing for the new homography.\n                        hom_marker_boxframe, _ = cv2.findHomography(\n                            corners_marker, corners_frame\n                        )\n                        flat_marker = cv2.warpPerspective(\n                            frame,\n                            hom_boxframe_marker,\n                            (\n                                cfg.model.marker.working_size,\n                                cfg.model.marker.working_size,\n                            ),\n                            flags=cv2.INTER_NEAREST,\n                        )\n                        flat_marker_pt = (\n                            torch.from_numpy(flat_marker)\n                            .cuda()\n                            .permute(2, 0, 1)[None, ...]\n                            / 255.0\n                        )\n                        bkd.process_marker(flat_marker_pt, hom_marker_boxframe, x, y)\n        for center_coord, center_valid in zip(centers_found, centers_valid):\n            clr = (255, 0, 0) if center_valid else (0, 0, 255)\n            vis_frame[\n                int(center_coord[1]) - 3 : int(center_coord[1]) + 3,\n                int(center_coord[0]) - 3 : int(center_coord[0]) + 3,\n                :,\n            ] = clr\n        if cfg.dbg:\n            # Plot all detections in the frame.\n            frame_dbg = frame.copy()\n            for center_coord, center_valid in zip(centers_found, centers_valid):\n                clr = (255, 0, 0) if center_valid else (0, 0, 255)\n                frame_dbg[\n                    int(center_coord[1]) - 3 : int(center_coord[1]) + 3,\n                    int(center_coord[0]) - 3 : int(center_coord[0]) + 3,\n                    :,\n                ] = clr\n            cv2.namedWindow(\"[dbg] frame with centers\")\n            cv2.imshow(\"[dbg] frame with centers\", frame_dbg)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] frame with centers\")\n    return centers_found, centers_valid, centers_coords, vis_frame", "\n\ndef process_frame(frame, cfg: Config):\n    \"\"\"\n    Process a frame and run the keypoint detector over it.\n\n    First locates the ArUco markers. Then infers a rough position\n    for all keypoints. Runs the keypoint detector on all of these\n    areas and returns their centers, the validity, coordinates\n    and a visualization.\n\n    Returns centers_found, centers_valid, centers_coords, vis_frame.\n    \"\"\"\n    global MODEL, ARUCO_PS, TARGET_PS\n    if MODEL is None:\n        LOGGER.info(f\"Loading model from experiment `{cfg.target.exp_name}`...\")\n        latest_checkpoint = get_latest_checkpoint(cfg.target.exp_name, cfg)\n        MODEL = Model.load_from_checkpoint(latest_checkpoint).cuda()\n        LOGGER.info(\"Loading successful.\")\n    if ARUCO_PS is None:\n        LOGGER.info(\n            f\"Setting up ArUco parameters for marker `{cfg.target.aruco_id}`...\"\n        )\n        ARUCO_PS = (\n            cv2.aruco.Dictionary_get(ARUCO_DICT[cfg.target.aruco_id]),\n            cv2.aruco.DetectorParameters_create(),\n        )\n        ARUCO_PS[1].cornerRefinementMethod = cv2.aruco.CORNER_REFINE_APRILTAG\n        LOGGER.info(\"Setup successful.\")\n    if TARGET_PS is None:\n        LOGGER.info(\"Calculating target parameters...\")\n        TARGET_PS = calculate_parameters(cfg)\n        LOGGER.info(\"Done.\")\n    centers_found = []\n    centers_valid = []\n    centers_coords = []\n    # TODO: resize frame before detection?\n    (corners, ids, _) = cv2.aruco.detectMarkers(\n        frame, ARUCO_PS[0], parameters=ARUCO_PS[1]\n    )\n    vis_frame = frame.copy()\n    corners_marker = np.array(\n        [\n            [0.0, 0.0],  # tl\n            [cfg.model.marker.working_size, 0.0],  # tr\n            [cfg.model.marker.working_size, cfg.model.marker.working_size],  # br\n            [0.0, cfg.model.marker.working_size],  # bl\n        ],\n        dtype=np.float32,\n    )\n    if len(corners) > 0:\n        if cfg.dbg:\n            cv2.namedWindow(\"[dbg] markers\")\n            dbg_frame = frame.copy()\n            cv2.aruco.drawDetectedMarkers(dbg_frame, corners)\n            cv2.imshow(\"[dbg] markers\", dbg_frame)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] markers\")\n        cv2.aruco.drawDetectedMarkers(vis_frame, corners, ids)\n        ids = ids.flatten()\n        for (markerCorner, markerID) in zip(corners, ids):\n            if markerID not in [23, 24, 25, 26]:\n                continue\n            corners = markerCorner.reshape((4, 2))\n            # The marker is printed upside-down (that's why tl-tr-br-bl is swapped\n            # to bl-br-tr-tl).\n            corners_grid = np.array(\n                [\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23],\n                        TARGET_PS.tag_y_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                    ],  # bl\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                        TARGET_PS.tag_y_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                    ],  # br\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares,\n                        TARGET_PS.tag_y_grid[markerID - 23],\n                    ],  # tr\n                    [\n                        TARGET_PS.tag_x_grid[markerID - 23],\n                        TARGET_PS.tag_y_grid[markerID - 23],\n                    ],  # tl\n                ],\n                dtype=np.float32,\n            )\n            LOGGER.debug(f\"ArUco marker grid position: {corners_grid}.\")\n            # Define a grid coordinate system over the target as:\n            # +------+------+------+------+\n            # |      |      |      |      |\n            # +------X------+------X------+\n            # |      |      |      |      |\n            # +------+------+------+------+\n            # |      |      |      |      |\n            # +------X------+------X------+\n            # |      |      |      |      |\n            # +------+------+------+------+\n            # with the four X's marking the corners of the ArUco\n            # marker.\n            hom_grid_frame, _ = cv2.findHomography(corners_grid, corners)\n            # debugging:\n            # corners_grid_hom = np.array(\n            #     [\n            #         [TARGET_PS.tag_x_grid, TARGET_PS.tag_y_grid, 1],  # tl\n            #         [TARGET_PS.tag_x_grid + 1, TARGET_PS.tag_y_grid, 1],  # tr\n            #         [TARGET_PS.tag_x_grid + 1, TARGET_PS.tag_y_grid + 1, 1],  # br\n            #         [TARGET_PS.tag_x_grid, TARGET_PS.tag_y_grid + 1, 1],  # bl\n            #     ],\n            #     dtype=np.float32,\n            # )\n            # corners_hom = (hom_grid_frame @ corners_grid_hom.T).T\n            # corners_calc = corners_hom[:, :2] / corners_hom[:, 2:3]\n            with BatchedKeypointDetector(\n                centers_found, centers_valid, centers_coords, cfg\n            ) as bkd:\n                for x in range(\n                    max(\n                        0,\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        - TARGET_PS.x_part_length_grid,\n                    ),\n                    min(\n                        TARGET_PS.n_squares_x,\n                        TARGET_PS.tag_x_grid[markerID - 23]\n                        + cfg.target.aruco_length_in_squares\n                        + TARGET_PS.x_part_length_grid,\n                    ),\n                ):\n                    for y in range(\n                        max(\n                            0,\n                            TARGET_PS.tag_y_grid[markerID - 23]\n                            - TARGET_PS.y_part_length_grid,\n                        ),\n                        min(\n                            TARGET_PS.n_squares_y,\n                            TARGET_PS.tag_y_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                            + TARGET_PS.y_part_length_grid,\n                        ),\n                    ):\n                        if (\n                            x >= TARGET_PS.tag_x_grid[markerID - 23]\n                            and x\n                            < TARGET_PS.tag_x_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                            and y >= TARGET_PS.tag_y_grid[markerID - 23]\n                            and y\n                            < TARGET_PS.tag_y_grid[markerID - 23]\n                            + cfg.target.aruco_length_in_squares\n                        ):\n                            # This is the ArUco marker section.\n                            continue\n                        corners_grid_hom = np.array(\n                            [\n                                [x, y, 1.0],  # tl\n                                [x + 1, y, 1.0],  # tr\n                                [x + 1, y + 1, 1.0],  # br\n                                [x, y + 1, 1.0],  # bl\n                            ],\n                            dtype=np.float32,\n                        )\n                        corners_frame_hom = (hom_grid_frame @ corners_grid_hom.T).T\n                        corners_frame = (\n                            corners_frame_hom[:, :2] / corners_frame_hom[:, 2:3]\n                        )\n                        valid = True\n                        for corner_x, corner_y in corners_frame:\n                            if (\n                                corner_x < 0\n                                or corner_y < 0\n                                or corner_x > frame.shape[1]\n                                or corner_y > frame.shape[0]\n                            ):\n                                valid = False\n                                break\n                        line_thickness = 2\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[0].astype(np.int32)),\n                            tuple(corners_frame[1].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[1].astype(np.int32)),\n                            tuple(corners_frame[2].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[2].astype(np.int32)),\n                            tuple(corners_frame[3].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        cv2.line(\n                            vis_frame,\n                            tuple(corners_frame[3].astype(np.int32)),\n                            tuple(corners_frame[0].astype(np.int32)),\n                            (0, 255, 0),\n                            thickness=line_thickness,\n                        )\n                        if not valid:\n                            LOGGER.debug(\n                                f\"Marker at position {x}, {y} (x, y) not fully visible: {corners_frame}.\"\n                            )\n                            continue\n                        hom_boxframe_marker, _ = cv2.findHomography(\n                            corners_frame, corners_marker\n                        )\n                        # We're not using `np.linalg.inv(hom_boxframe_marker)` here,\n                        # which would be an option but is less numerically stable and\n                        # accurate than optimizing for the new homography.\n                        hom_marker_boxframe, _ = cv2.findHomography(\n                            corners_marker, corners_frame\n                        )\n                        flat_marker = cv2.warpPerspective(\n                            frame,\n                            hom_boxframe_marker,\n                            (\n                                cfg.model.marker.working_size,\n                                cfg.model.marker.working_size,\n                            ),\n                            flags=cv2.INTER_NEAREST,\n                        )\n                        flat_marker_pt = (\n                            torch.from_numpy(flat_marker)\n                            .cuda()\n                            .permute(2, 0, 1)[None, ...]\n                            / 255.0\n                        )\n                        bkd.process_marker(flat_marker_pt, hom_marker_boxframe, x, y)\n        for center_coord, center_valid in zip(centers_found, centers_valid):\n            clr = (255, 0, 0) if center_valid else (0, 0, 255)\n            vis_frame[\n                int(center_coord[1]) - 3 : int(center_coord[1]) + 3,\n                int(center_coord[0]) - 3 : int(center_coord[0]) + 3,\n                :,\n            ] = clr\n        if cfg.dbg:\n            # Plot all detections in the frame.\n            frame_dbg = frame.copy()\n            for center_coord, center_valid in zip(centers_found, centers_valid):\n                clr = (255, 0, 0) if center_valid else (0, 0, 255)\n                frame_dbg[\n                    int(center_coord[1]) - 3 : int(center_coord[1]) + 3,\n                    int(center_coord[0]) - 3 : int(center_coord[0]) + 3,\n                    :,\n                ] = clr\n            cv2.namedWindow(\"[dbg] frame with centers\")\n            cv2.imshow(\"[dbg] frame with centers\", frame_dbg)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] frame with centers\")\n    return centers_found, centers_valid, centers_coords, vis_frame", "\n\nclass BatchedKeypointDetector:\n    def __init__(self, center_list, valid_list, centers_board_coords, cfg):\n        self._markers_flat = []\n        self._homographies = []\n        self._coordinates = []\n        self.center_list = center_list\n        self.valid_list = valid_list\n        self.centers_board_coords = centers_board_coords\n        self.cfg = cfg\n\n    def process_marker(self, marker_pt, hom_marker_boxframe, x, y):\n        self._markers_flat.append(marker_pt)\n        self._homographies.append(hom_marker_boxframe)\n        self._coordinates.append([x, y, 0])\n        if len(self._markers_flat) >= MODEL.batch_size:\n            self.run_forward()\n\n    def run_forward(self):\n        marker_batch = torch.cat(self._markers_flat, dim=0)\n        model_centers, model_vars = MODEL.forward(detection=marker_batch)\n        center_coords = model_centers.cpu().detach().numpy()\n        variances = model_vars.cpu().detach().numpy()\n        LOGGER.debug(f\"Center coordinates: {center_coords}.\")\n        # if cfg.dbg and False:\n        #     flat_marker_dbg = flat_marker.copy()\n        #     flat_marker_dbg[\n        #         int(center_coords[0]) - 3 : int(center_coords[0]) + 3,\n        #         int(center_coords[1]) - 3 : int(center_coords[1]) + 3,\n        #         :,\n        #     ] = (255, 0, 0)\n        #     LOGGER.info(f\"[dbg] Variance: {variance}.\")\n        #     cv2.namedWindow(\"[dbg] normalized marker\")\n        #     cv2.imshow(\"[dbg] normalized marker\", flat_marker_dbg)\n        #     cv2.waitKey(0)\n        #     cv2.destroyWindow(\"[dbg] normalized marker\")\n        for center_coord, variance, hom_marker_boxframe, board_coord in zip(\n            center_coords, variances, self._homographies, self._coordinates\n        ):\n            # Check for reliability.\n            reliable = not (\n                center_coord[0] < 0.0\n                or center_coord[0] >= self.cfg.model.marker.working_size\n                or center_coord[1] < 0.0\n                or center_coord[1] > self.cfg.model.marker.working_size\n                or math.sqrt(variance) > self.cfg.model.marker.std_thresh\n            )\n            center_coords_hom = np.array(\n                [[center_coord[1], center_coord[0], 1.0]], dtype=np.float32\n            )\n            center_coords_frame_hom = (hom_marker_boxframe @ center_coords_hom.T).T\n            center_coords_frame = (\n                center_coords_frame_hom[0, :2] / center_coords_frame_hom[0, 2:3]\n            )\n            self.center_list.append(center_coords_frame)\n            self.valid_list.append(reliable)\n            self.centers_board_coords.append(board_coord)\n        self._homographies = []\n        self._markers_flat = []\n        self._coordinates = []\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if len(self._markers_flat) > 0:\n            self.run_forward()", ""]}
{"filename": "calibration/config.py", "chunked_list": ["from dataclasses import dataclass, field\nfrom enum import Enum\nfrom os import path\nfrom glob import glob\nfrom natsort import natsorted\nfrom typing import List, Optional, Dict, Tuple, Union\nimport cv2\n\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING", "from hydra.core.config_store import ConfigStore\nfrom omegaconf import MISSING\n\n\nARUCO_DICT = {\n    \"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n    \"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n    \"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n    \"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n    \"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,", "    \"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n    \"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n    \"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n    \"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n    \"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n    \"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n    \"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n    \"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n    \"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n    \"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,", "    \"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n    \"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n    \"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n    \"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n    \"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n    \"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n    \"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n    \"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n    \"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n    \"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11,", "    \"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n    \"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11,\n}\n\n\n@dataclass\nclass ConfigCalibration:\n    subs_fac: int = 5\n\n", "\n\n@dataclass\nclass ConfigMarker:\n    _target_: str = \"calibration.marker.Marker\"\n    size: int = MISSING\n    working_size: int = MISSING\n    seed: int = 1\n    # 95% of points are within 2 * std_thresh.\n    std_thresh: float = 0.20", "    # std_thresh: float = 0.0\n\n\n@dataclass\nclass ConfigRandomMotionBlur:\n    _target_: str = \"kornia.augmentation.RandomMotionBlur\"\n    p: float = 0.0\n    kernel_size: int = 5\n    angle: float = 90.0\n    direction: float = 1.0\n    border_type: str = \"reflect\"", "\n\n@dataclass\nclass ConfigRandomAffine:\n    _target_: str = \"kornia.augmentation.RandomAffine\"\n    p: float = 0.0\n    degrees: float = 180.0\n    translate: Tuple[float, float] = (0.05, 0.05)\n    scale: Tuple[float, float, float, float] = (0.95, 1.05, 0.95, 1.05)\n    shear: float = 0.02\n    return_transform: bool = True\n    padding_mode: str = \"reflection\"", "\n\n@dataclass\nclass ConfigRandomGaussianBlur:\n    _target_: str = \"kornia.augmentation.RandomGaussianBlur\"\n    p: float = 0.3\n    # We aim to find a model that can reliably detect the marker\n    # up to 1/2 it's size (in pixels).\n    # For down-scaling by factor two, \u03c3=3.3 is a good value.\n    # The half filter size should be\n    # around 3\u03c3, so we need a full filter size of 2*3*3.3, so around\n    # 19.\n    # Must be an odd, positive integer.\n    kernel_size: int = (11, 11)\n    # sigma: Tuple[float, float] = (3.3, 3.3)\n    sigma: Tuple[float, float] = (1.6, 1.6)\n    border_type: str = \"reflect\"", "\n\n@dataclass\nclass ConfigColorJitter:\n    _target_: str = \"kornia.augmentation.ColorJitter\"\n    brightness: float = 0.1\n    contrast: float = 0.1\n    saturation: float = 0.1\n    hue: float = 0.1\n    return_transform: bool = False\n    same_on_batch: bool = False\n    p: float = 0.0", "\n\n@dataclass\nclass ConfigRandomElasticTransform:\n    _target_: str = \"calibration.transformations.RandomElasticTransform\"\n    # Reactivate once a fix or mitigation is found:\n    # https://fb.workplace.com/groups/349226332644221/permalink/949543442612504/\n    kernel_size: Tuple[int, int] = (63, 63)\n    sigma: Tuple[float, float] = (32.0, 32.0)\n    alpha: Tuple[float, float] = (1.0, 1.0)\n    align_corners: bool = False\n    mode: str = \"bilinear\"\n    padding_mode: str = \"mirror\"\n    return_transform: bool = True\n    same_on_batch: bool = False\n    p: float = 0.0", "\n\n@dataclass\nclass ConfigRandomGaussianNoise:\n    _target_: str = \"kornia.augmentation.RandomGaussianNoise\"\n    mean: float = 0.0\n    std: float = 0.1\n    p: float = 1.0\n\n", "\n\n@dataclass\nclass ConfigImageFormation:\n    _target_: str = \"calibration.image_formation.ImageFormation\"\n    # print_dist: ConfigRandomElasticTransform = ConfigRandomElasticTransform()\n    motion_dist: ConfigRandomMotionBlur = ConfigRandomMotionBlur()\n    affine_dist: ConfigRandomAffine = ConfigRandomAffine()\n    blur_dist: ConfigRandomGaussianBlur = ConfigRandomGaussianBlur()\n    color_dist: ConfigColorJitter = ConfigColorJitter()\n    noise_dist: ConfigRandomGaussianNoise = ConfigRandomGaussianNoise()\n    working_size: int = MISSING", "\n\n@dataclass\nclass ConfigModel:\n    _target_: str = \"calibration.model.Model\"\n    marker: ConfigMarker = MISSING\n    image_formation: ConfigImageFormation = ConfigImageFormation()\n    batch_size: int = 20\n    log_every: float = 30.0\n    lr: float = 1e-3\n    lr_fcn_fac: float = 1.0\n    lr_marker_fac: float = 100.0\n    n_latent: int = 200\n    n_hidden: int = 2\n    reg_weight: float = 0.0", "\n\n@dataclass\nclass ConfigTrainer:\n    _target_: str = \"pytorch_lightning.Trainer\"\n    logger: bool = True\n    enable_checkpointing: bool = True\n    default_root_dir: str = \"\"\n    max_steps: int = 100000\n    log_every_n_steps: int = 50\n    enable_model_summary: bool = True\n    accelerator: str = \"gpu\"\n    gpus: Tuple[int] = (0,)\n    auto_select_gpus: bool = True", "\n\n@dataclass\nclass ConfigTarget:\n    exp_name: str = MISSING\n    aruco_id: str = \"DICT_4X4_1000\"\n    margin_in_cm: float = 0.4\n    approx_square_length_in_cm: float = 1.25\n    aruco_length_in_squares: int = 4\n    page_size_pt: Tuple[float, float] = (595.2755905511812, 841.8897637795277)", "\n\n@dataclass\nclass Config:\n    gpus: int = MISSING\n    disable_tqdm: bool = MISSING\n    model: ConfigModel = MISSING\n    calibration: ConfigCalibration = ConfigCalibration()\n    trainer: ConfigTrainer = ConfigTrainer()\n    exp_name: str = MISSING\n    target: ConfigTarget = ConfigTarget()\n    dbg: bool = False\n    vis: bool = False\n    video_fp: Optional[str] = None", "\n\ncs = ConfigStore.instance()\ncs.store(name=\"calibration_base_config\", node=Config)\n\n\ndef get_latest_checkpoint(exp_name: str, cfg: Config):\n    return path.abspath(\n        natsorted(\n            glob(\n                path.join(\n                    path.dirname(__file__),\n                    \"..\",\n                    \"experiments\",\n                    cfg.target.exp_name,\n                    \"lightning_logs\",\n                    \"version_0\",\n                    \"checkpoints\",\n                    \"*.ckpt\",\n                )\n            )\n        )[-1]\n    )", ""]}
{"filename": "calibration/persistence.py", "chunked_list": ["import json\n\nimport numpy as np\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    \"\"\"Special json encoder for numpy types\"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)", ""]}
{"filename": "calibration/target.py", "chunked_list": ["import math\nimport logging\nfrom collections import namedtuple\n\nfrom reportlab.lib.units import cm\n\nfrom .config import Config\n\ntarget_params = namedtuple(\n    \"TargetParams\",", "target_params = namedtuple(\n    \"TargetParams\",\n    \"n_squares_x, n_squares_y, pattern_start_x_pt, pattern_start_y_pt, \"\n    \"square_length_pt, tag_start_x_pt, tag_start_y_pt, tag_x_grid, tag_y_grid, \"\n    \"tag_square_length_pt, x_part_length_grid, y_part_length_grid\",\n)\nLOGGER = logging.getLogger(__name__)\n\n\ndef calculate_parameters(cfg: Config):\n    width_pt, height_pt = cfg.target.page_size_pt\n    margin_size_pt = cfg.target.margin_in_cm * cm\n    start_x_pt = margin_size_pt\n    end_x_pt = width_pt - margin_size_pt\n    start_y_pt = margin_size_pt\n    end_y_pt = height_pt - margin_size_pt\n    print_area_width_pt = abs(end_x_pt - start_x_pt)\n    print_area_height_pt = abs(end_y_pt - start_y_pt)\n    LOGGER.info(\n        f\"Print area size: {print_area_width_pt}pt X {print_area_height_pt}pt (width X height).\"\n    )\n    approx_square_length_pt = cfg.target.approx_square_length_in_cm * cm\n    squares_length_width_rounded_pt = print_area_width_pt / round(\n        print_area_width_pt / approx_square_length_pt\n    )\n    squares_length_height_rounded_pt = print_area_height_pt / round(\n        print_area_height_pt / approx_square_length_pt\n    )\n    square_length_pt = min(\n        squares_length_width_rounded_pt, squares_length_height_rounded_pt\n    )\n    n_squares_x = int(math.floor(print_area_width_pt / square_length_pt))\n    n_squares_y = int(math.floor(print_area_height_pt / square_length_pt))\n    unused_x_pt = print_area_width_pt - n_squares_x * square_length_pt\n    pattern_start_x_pt = start_x_pt + 0.5 * unused_x_pt\n    unused_y_pt = print_area_height_pt - n_squares_y * square_length_pt\n    pattern_start_y_pt = start_y_pt + 0.5 * unused_y_pt  # - square_length_pt\n    assert cfg.target.aruco_length_in_squares % 2 == 0\n    tag_start_x_pt = []\n    tag_start_y_pt = []\n    tag_x_grid = []\n    tag_y_grid = []\n    x_part_length_grid = int(\n        round((n_squares_x - 2 * cfg.target.aruco_length_in_squares) // 4)\n    )\n    y_part_length_grid = int(\n        round((n_squares_y - 2 * cfg.target.aruco_length_in_squares) / 4)\n    )\n    for id_x in range(2):\n        for id_y in range(2):\n            tag_x_grid.append(\n                x_part_length_grid\n                + (cfg.target.aruco_length_in_squares + 2 * x_part_length_grid) * id_x\n            )\n            tag_start_x_pt.append(\n                pattern_start_x_pt + tag_x_grid[-1] * square_length_pt\n            )\n\n            tag_y_grid.append(\n                y_part_length_grid\n                + (cfg.target.aruco_length_in_squares + 2 * y_part_length_grid) * id_y\n            )\n            tag_start_y_pt.append(\n                pattern_start_y_pt + tag_y_grid[-1] * square_length_pt\n            )\n    tag_square_length_pt = cfg.target.aruco_length_in_squares * square_length_pt\n    return target_params(\n        n_squares_x,\n        n_squares_y,\n        pattern_start_x_pt,\n        pattern_start_y_pt,\n        square_length_pt,\n        tag_start_x_pt,\n        tag_start_y_pt,\n        tag_x_grid,\n        tag_y_grid,\n        tag_square_length_pt,\n        x_part_length_grid,\n        y_part_length_grid,\n    )", "\ndef calculate_parameters(cfg: Config):\n    width_pt, height_pt = cfg.target.page_size_pt\n    margin_size_pt = cfg.target.margin_in_cm * cm\n    start_x_pt = margin_size_pt\n    end_x_pt = width_pt - margin_size_pt\n    start_y_pt = margin_size_pt\n    end_y_pt = height_pt - margin_size_pt\n    print_area_width_pt = abs(end_x_pt - start_x_pt)\n    print_area_height_pt = abs(end_y_pt - start_y_pt)\n    LOGGER.info(\n        f\"Print area size: {print_area_width_pt}pt X {print_area_height_pt}pt (width X height).\"\n    )\n    approx_square_length_pt = cfg.target.approx_square_length_in_cm * cm\n    squares_length_width_rounded_pt = print_area_width_pt / round(\n        print_area_width_pt / approx_square_length_pt\n    )\n    squares_length_height_rounded_pt = print_area_height_pt / round(\n        print_area_height_pt / approx_square_length_pt\n    )\n    square_length_pt = min(\n        squares_length_width_rounded_pt, squares_length_height_rounded_pt\n    )\n    n_squares_x = int(math.floor(print_area_width_pt / square_length_pt))\n    n_squares_y = int(math.floor(print_area_height_pt / square_length_pt))\n    unused_x_pt = print_area_width_pt - n_squares_x * square_length_pt\n    pattern_start_x_pt = start_x_pt + 0.5 * unused_x_pt\n    unused_y_pt = print_area_height_pt - n_squares_y * square_length_pt\n    pattern_start_y_pt = start_y_pt + 0.5 * unused_y_pt  # - square_length_pt\n    assert cfg.target.aruco_length_in_squares % 2 == 0\n    tag_start_x_pt = []\n    tag_start_y_pt = []\n    tag_x_grid = []\n    tag_y_grid = []\n    x_part_length_grid = int(\n        round((n_squares_x - 2 * cfg.target.aruco_length_in_squares) // 4)\n    )\n    y_part_length_grid = int(\n        round((n_squares_y - 2 * cfg.target.aruco_length_in_squares) / 4)\n    )\n    for id_x in range(2):\n        for id_y in range(2):\n            tag_x_grid.append(\n                x_part_length_grid\n                + (cfg.target.aruco_length_in_squares + 2 * x_part_length_grid) * id_x\n            )\n            tag_start_x_pt.append(\n                pattern_start_x_pt + tag_x_grid[-1] * square_length_pt\n            )\n\n            tag_y_grid.append(\n                y_part_length_grid\n                + (cfg.target.aruco_length_in_squares + 2 * y_part_length_grid) * id_y\n            )\n            tag_start_y_pt.append(\n                pattern_start_y_pt + tag_y_grid[-1] * square_length_pt\n            )\n    tag_square_length_pt = cfg.target.aruco_length_in_squares * square_length_pt\n    return target_params(\n        n_squares_x,\n        n_squares_y,\n        pattern_start_x_pt,\n        pattern_start_y_pt,\n        square_length_pt,\n        tag_start_x_pt,\n        tag_start_y_pt,\n        tag_x_grid,\n        tag_y_grid,\n        tag_square_length_pt,\n        x_part_length_grid,\n        y_part_length_grid,\n    )", ""]}
{"filename": "calibration/marker.py", "chunked_list": ["import torch\nimport pytorch_lightning as pl\n\n\nclass Marker(pl.LightningModule):\n    \"\"\"\n    This represents one keypoint marker on a calibration target pattern.\n    \"\"\"\n\n    def __init__(self, size, working_size, seed, std_thresh=0.0):\n        super().__init__()\n        assert (\n            size % 2 == 1\n        ), \"marker size must be an odd number to have a well-defined center.\"\n        self.size = size\n        self.working_size = working_size\n        torch.random.manual_seed(seed)\n        # We assume the pixel 'coordinate' to point to the center of the pixel. Hence,\n        # the center of the pattern is axactly the coordinate of its center pixel.\n        self.register_buffer(\n            \"center\",\n            torch.tensor(\n                [size // 2, size // 2],\n                dtype=torch.float32,\n                requires_grad=False,\n            ),\n            persistent=True,\n        )\n        # This *is* the actual marker, ready to be optimized through back-propagation.\n        # We initialize the memory with values in [-4, 4[ (which means that the values\n        # range from close to 0 to 1 when run through a sigmoid) - we use the sigmoid\n        # function to guarantee the bounds for the parameters even after gradient\n        # updates.\n        init = (\n            torch.rand((3, size, size), dtype=torch.float32, requires_grad=True) * 8.0\n            - 4.0\n        )\n        self.marker_mem = torch.nn.Parameter(\n            data=init,\n            requires_grad=True,\n        )\n        # Standard-deviation threshold for detection.\n        self.std_thresh = std_thresh\n\n    def forward(self, batch_size):\n        return (\n            torch.sigmoid(self.marker_mem)[None, ...].expand(\n                batch_size,\n                self.marker_mem.shape[0],\n                self.marker_mem.shape[1],\n                self.marker_mem.shape[2],\n            ),\n            self.center[None, ...].expand(batch_size, 2),\n        )", ""]}
{"filename": "calibration/image_formation.py", "chunked_list": ["import torch\nimport pytorch_lightning as pl\nimport kornia.augmentation as K\n\n# from .transformations import RandomElasticTransform\n\n\nclass ImageFormation(pl.LightningModule):\n    \"\"\"\n    This models the image formation process, including printing of the pattern.\n\n    This transformation must randomly distort the input images with valid\n    distortions and at the same time provide the position of the marker\n    center (given those distortions).\n    \"\"\"\n\n    def __init__(\n        self,\n        # print_dist: RandomElasticTransform,\n        motion_dist: K.RandomMotionBlur,\n        affine_dist: K.RandomAffine,\n        blur_dist: K.RandomGaussianBlur,\n        color_dist: K.ColorJitter,\n        noise_dist: K.RandomGaussianNoise,\n        working_size: int,\n    ):\n        \"\"\"\n        print_dist: K.RandomElasticTransform; model distortion of the\n            pattern through printing.\n        motion_dist: K.RandomMotionBlur; model motion blur during\n            recording of the pattern.\n        affine_dist: K.RandomAffine; model imperfect retrieval of the\n            pattern.\n        color_dist: K.ColorJitter; model color distortion of the recording.\n\n        \"\"\"\n        super().__init__()\n        # self.print_dist = print_dist\n        self.motion_dist = motion_dist\n        self.affine_dist = affine_dist\n        self.blur_dist = blur_dist\n        self.color_dist = color_dist\n        self.noise_dist = noise_dist\n        self.working_size = working_size\n        self.resizer = K.Resize(self.working_size)\n\n    def _affine_mul(self, mat1, mat2):\n        # Pad mat2.\n        mat2 = torch.cat(\n            (\n                mat2[:, 1:2],\n                mat2[:, 0:1],\n                torch.ones((mat2.shape[0], 1), dtype=mat2.dtype, device=mat2.device),\n            ),\n            dim=1,\n        )\n        multiplied = torch.matmul(mat1, mat2[:, :, None])\n        normalized = multiplied[:, :2, 0] / multiplied[:, 2:3, 0]\n        return torch.cat((normalized[:, 1:2], normalized[:, 0:1]), dim=1)\n\n    def forward(self, pattern, center):\n        # Assume the pattern is used in a grid.\n        pattern_grid = pattern.repeat(1, 1, 3, 3)\n        center = (\n            center\n            + torch.tensor(\n                pattern.shape[2:4], dtype=torch.float32, device=pattern.device\n            )[None, ...]\n        )\n        # print the pattern.\n        # printed, transform = self.print_dist(pattern_grid)\n        # # Apply shift to\n        # center = self.print_dist.apply_to_coordinates(center)\n        # Now we get to the recording stage.\n        # We assume the pattern is moving, so it could have a\n        # certain amount of motion blur.\n        recorded = self.motion_dist(pattern_grid)\n        # We assume the pattern is recorded using an arbitrary\n        # affine transform; however we can normalize that away using\n        # the AprilTag. Hence, we drop the homography for now.\n        # However, we assume that we can't retrieve the pattern perfectly.\n        # That's why we add a random affine transformation.\n        recorded, transform = self.affine_dist(recorded)\n        center = self._affine_mul(transform, center)\n        # Add sensor noise.\n        noised = self.noise_dist(recorded)\n        # We simulate that this could have been recorded from further\n        # away and later resized, so we have quite a bit of loss\n        # of detail.\n        # Rule of thumb for Gauss filter: filter half-width should be\n        # about 3\u03c3.\n        # Convolving two times with Gaussian kernel of width \u03c3 is\n        # same as convolving once with kernel of width \u03c3\u221a2.\n        blurred = self.blur_dist(noised)\n        # We assume that we don't have color-calibrated cameras (at this\n        # stage), so we have to be robust to all kinds of color-shifts.\n        color_distorted = self.color_dist(blurred)\n        # Get only the center crop.\n        cropped = color_distorted[\n            :,\n            :,\n            pattern.shape[2] : -pattern.shape[2],\n            pattern.shape[3] : -pattern.shape[3],\n        ]\n        center = (\n            center\n            - torch.tensor(\n                pattern.shape[2:4], dtype=torch.float32, device=center.device\n            )[None, ...]\n        )\n        if self.working_size != cropped.shape[2]:\n            center *= self.working_size / cropped.shape[2]\n            cropped = self.resizer(cropped)\n        return cropped, center", ""]}
{"filename": "calibration/util.py", "chunked_list": ["from typing import Callable\nimport torch\n\n\ndef batched_func(func: Callable, tensor: torch.Tensor, batch_size: int):\n    return torch.cat(tuple(func(batch) for batch in batched(tensor, batch_size)), dim=0)\n\n\nclass batched(object):\n    def __init__(self, tensor: torch.Tensor, batch_size: int):\n        self.tensor = tensor\n        self.batch_size = batch_size\n        self._position = -1\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._position + 1 >= self.tensor.shape[0]:\n            raise StopIteration()\n        next_batch = self.tensor[\n            self._position + 1 : self._position + 1 + self.batch_size\n        ]\n        self._position += self.batch_size\n        return next_batch", "class batched(object):\n    def __init__(self, tensor: torch.Tensor, batch_size: int):\n        self.tensor = tensor\n        self.batch_size = batch_size\n        self._position = -1\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._position + 1 >= self.tensor.shape[0]:\n            raise StopIteration()\n        next_batch = self.tensor[\n            self._position + 1 : self._position + 1 + self.batch_size\n        ]\n        self._position += self.batch_size\n        return next_batch", ""]}
{"filename": "calibration/transformations.py", "chunked_list": ["\"\"\"\nTransformations adapted from Kornia (https://kornia.readthedocs.io/)\nto incorporate a coordinate transformation.\n\nOriginal license:\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION", "\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.", "      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.", "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n", "      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work", "      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,", "      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"", "      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n", "      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,", "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,", "      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses", "      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n", "      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and", "      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not", "          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution", "          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or", "      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.", "      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.", "      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the", "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,", "      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing", "\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason", "      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\"\"\"\n\n\nfrom typing import Tuple, Dict, Optional\n", "from typing import Tuple, Dict, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom kornia.filters import filter2d, get_gaussian_kernel2d\nfrom kornia.utils import create_meshgrid\nfrom kornia.augmentation.base import GeometricAugmentationBase2D\n\n\nclass RandomElasticTransform(GeometricAugmentationBase2D):\n    r\"\"\"Add random elastic transformation to a tensor image.\n\n    .. image:: _static/img/RandomElasticTransform.png\n\n    Args:\n        kernel_size: the size of the Gaussian kernel.\n        sigma: The standard deviation of the Gaussian in the y and x directions,\n          respectively. Larger sigma results in smaller pixel displacements.\n        alpha: The scaling factor that controls the intensity of the deformation\n          in the y and x directions, respectively.\n        align_corners: Interpolation flag used by `grid_sample`.\n        mode: Interpolation mode used by `grid_sample`. Either 'bilinear' or 'nearest'.\n        padding_mode: The padding used by ```grid_sample```. Either 'zeros', 'border' or 'refection'.\n\n        return_transform: if ``True`` return the matrix describing the transformation applied to each\n            input tensor. If ``False`` and the input is a tuple the applied transformation won't be concatenated.\n        same_on_batch: apply the same transformation across the batch.\n        p: probability of applying the transformation.\n\n    .. note::\n        This function internally uses :func:`kornia.geometry.transform.elastic_transform2d`.\n\n    Examples:\n        >>> img = torch.ones(1, 1, 2, 2)\n        >>> out = RandomElasticTransform()(img)\n        >>> out.shape\n        torch.Size([1, 1, 2, 2])\n\n    To apply the exact augmenation again, you may take the advantage of the previous parameter state:\n        >>> input = torch.randn(1, 3, 32, 32)\n        >>> aug = RandomElasticTransform(p=1.)\n        >>> (aug(input) == aug(input, params=aug._params)).all()\n        tensor(True)\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: Tuple[int, int] = (63, 63),\n        sigma: Tuple[float, float] = (32.0, 32.0),\n        alpha: Tuple[float, float] = (1.0, 1.0),\n        align_corners: bool = False,\n        mode: str = \"bilinear\",\n        padding_mode: str = \"zeros\",\n        return_transform: bool = False,\n        same_on_batch: bool = False,\n        p: float = 0.5,\n        keepdim: bool = False,\n    ) -> None:\n        super().__init__(\n            p=p,\n            return_transform=return_transform,\n            same_on_batch=same_on_batch,\n            p_batch=1.0,\n            keepdim=keepdim,\n        )\n        self.flags = dict(\n            kernel_size=tuple(kernel_size),\n            sigma=tuple(sigma),\n            alpha=tuple(alpha),\n            align_corners=align_corners,\n            mode=mode,\n            padding_mode=padding_mode,\n        )\n\n    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:\n        B, _, H, W = shape\n        if self.same_on_batch:\n            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(\n                B, 1, 1, 1\n            )\n        else:\n            noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)\n        return dict(noise=noise * 2 - 1)\n\n    # TODO: It is incorrect to return identity\n    def compute_transformation(\n        self, input: torch.Tensor, params: Dict[str, torch.Tensor]\n    ) -> torch.Tensor:\n        return self.identity_matrix(input)\n\n    def apply_transform(\n        self,\n        input: torch.Tensor,\n        params: Dict[str, torch.Tensor],\n        transform: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        return elastic_transform2d(\n            input,\n            params[\"noise\"].to(input),\n            self.flags[\"kernel_size\"],\n            self.flags[\"sigma\"],\n            self.flags[\"alpha\"],\n            self.flags[\"align_corners\"],\n            self.flags[\"mode\"],\n            self.flags[\"padding_mode\"],\n        )\n\n    def apply_to_coordinates(self, coords):\n        assert self._params is not None and \"noise\" in self._params\n        # noise shape is Bx2xHxW and the two offsets are in order x, y.\n        noise = self._params[\"noise\"]\n        result = torch.empty_like(coords)\n        assert torch.all(coords >= 0.0)\n        assert torch.all(coords[:, 0] < self._params[\"forward_input_shape\"][2])\n        assert torch.all(coords[:, 1] < self._params[\"forward_input_shape\"][3])\n        # These are automatically all in-bounds given the above checks.\n        coords_floor = coords.floor()\n        # These have to be made in-bounds.\n        coords_ceil = coords.ceil()\n        coords_ceil[:, 0] = coords_ceil[:, 0].clamp(\n            0, self._params[\"forward_input_shape\"][2]\n        )\n        coords_ceil[:, 1] = coords_ceil[:, 1].clamp(\n            1, self._params[\"forward_input_shape\"][3]\n        )\n        # This is automatically in [0., 1.[\n        coords_rem = coords - coords_floor\n        x_offs_floor = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            0,\n            coords_floor[:, 0].long(),\n            coords_floor[:, 1].long(),\n        ].to(coords.device)\n        x_offs_ceil = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            0,\n            coords_ceil[:, 0].long(),\n            coords_ceil[:, 1].long(),\n        ].to(coords.device)\n        offset_x = (\n            x_offs_floor * (1.0 - coords_rem[:, 1]) + x_offs_ceil * coords_rem[:, 1]\n        )\n        offset_x *= self._params[\"forward_input_shape\"][3]\n        y_offs_floor = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            1,\n            coords_floor[:, 0].long(),\n            coords_floor[:, 1].long(),\n        ].to(coords.device)\n        y_offs_ceil = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            1,\n            coords_ceil[:, 0].long(),\n            coords_ceil[:, 1].long(),\n        ].to(coords.device)\n        offset_y = (\n            y_offs_floor * (1.0 - coords_rem[:, 0]) + y_offs_ceil * coords_rem[:, 0]\n        )\n        offset_y *= self._params[\"forward_input_shape\"][2]\n        result[:, 0] = coords[:, 0] + offset_y\n        result[:, 1] = coords[:, 1] + offset_x\n        # Clamp (as is done in the deformation code, too).\n        result[:, 0] = result[:, 0].clamp(0, self._params[\"forward_input_shape\"][2])\n        result[:, 1] = result[:, 1].clamp(0, self._params[\"forward_input_shape\"][3])\n        return result", "\n\nclass RandomElasticTransform(GeometricAugmentationBase2D):\n    r\"\"\"Add random elastic transformation to a tensor image.\n\n    .. image:: _static/img/RandomElasticTransform.png\n\n    Args:\n        kernel_size: the size of the Gaussian kernel.\n        sigma: The standard deviation of the Gaussian in the y and x directions,\n          respectively. Larger sigma results in smaller pixel displacements.\n        alpha: The scaling factor that controls the intensity of the deformation\n          in the y and x directions, respectively.\n        align_corners: Interpolation flag used by `grid_sample`.\n        mode: Interpolation mode used by `grid_sample`. Either 'bilinear' or 'nearest'.\n        padding_mode: The padding used by ```grid_sample```. Either 'zeros', 'border' or 'refection'.\n\n        return_transform: if ``True`` return the matrix describing the transformation applied to each\n            input tensor. If ``False`` and the input is a tuple the applied transformation won't be concatenated.\n        same_on_batch: apply the same transformation across the batch.\n        p: probability of applying the transformation.\n\n    .. note::\n        This function internally uses :func:`kornia.geometry.transform.elastic_transform2d`.\n\n    Examples:\n        >>> img = torch.ones(1, 1, 2, 2)\n        >>> out = RandomElasticTransform()(img)\n        >>> out.shape\n        torch.Size([1, 1, 2, 2])\n\n    To apply the exact augmenation again, you may take the advantage of the previous parameter state:\n        >>> input = torch.randn(1, 3, 32, 32)\n        >>> aug = RandomElasticTransform(p=1.)\n        >>> (aug(input) == aug(input, params=aug._params)).all()\n        tensor(True)\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: Tuple[int, int] = (63, 63),\n        sigma: Tuple[float, float] = (32.0, 32.0),\n        alpha: Tuple[float, float] = (1.0, 1.0),\n        align_corners: bool = False,\n        mode: str = \"bilinear\",\n        padding_mode: str = \"zeros\",\n        return_transform: bool = False,\n        same_on_batch: bool = False,\n        p: float = 0.5,\n        keepdim: bool = False,\n    ) -> None:\n        super().__init__(\n            p=p,\n            return_transform=return_transform,\n            same_on_batch=same_on_batch,\n            p_batch=1.0,\n            keepdim=keepdim,\n        )\n        self.flags = dict(\n            kernel_size=tuple(kernel_size),\n            sigma=tuple(sigma),\n            alpha=tuple(alpha),\n            align_corners=align_corners,\n            mode=mode,\n            padding_mode=padding_mode,\n        )\n\n    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:\n        B, _, H, W = shape\n        if self.same_on_batch:\n            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(\n                B, 1, 1, 1\n            )\n        else:\n            noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)\n        return dict(noise=noise * 2 - 1)\n\n    # TODO: It is incorrect to return identity\n    def compute_transformation(\n        self, input: torch.Tensor, params: Dict[str, torch.Tensor]\n    ) -> torch.Tensor:\n        return self.identity_matrix(input)\n\n    def apply_transform(\n        self,\n        input: torch.Tensor,\n        params: Dict[str, torch.Tensor],\n        transform: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        return elastic_transform2d(\n            input,\n            params[\"noise\"].to(input),\n            self.flags[\"kernel_size\"],\n            self.flags[\"sigma\"],\n            self.flags[\"alpha\"],\n            self.flags[\"align_corners\"],\n            self.flags[\"mode\"],\n            self.flags[\"padding_mode\"],\n        )\n\n    def apply_to_coordinates(self, coords):\n        assert self._params is not None and \"noise\" in self._params\n        # noise shape is Bx2xHxW and the two offsets are in order x, y.\n        noise = self._params[\"noise\"]\n        result = torch.empty_like(coords)\n        assert torch.all(coords >= 0.0)\n        assert torch.all(coords[:, 0] < self._params[\"forward_input_shape\"][2])\n        assert torch.all(coords[:, 1] < self._params[\"forward_input_shape\"][3])\n        # These are automatically all in-bounds given the above checks.\n        coords_floor = coords.floor()\n        # These have to be made in-bounds.\n        coords_ceil = coords.ceil()\n        coords_ceil[:, 0] = coords_ceil[:, 0].clamp(\n            0, self._params[\"forward_input_shape\"][2]\n        )\n        coords_ceil[:, 1] = coords_ceil[:, 1].clamp(\n            1, self._params[\"forward_input_shape\"][3]\n        )\n        # This is automatically in [0., 1.[\n        coords_rem = coords - coords_floor\n        x_offs_floor = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            0,\n            coords_floor[:, 0].long(),\n            coords_floor[:, 1].long(),\n        ].to(coords.device)\n        x_offs_ceil = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            0,\n            coords_ceil[:, 0].long(),\n            coords_ceil[:, 1].long(),\n        ].to(coords.device)\n        offset_x = (\n            x_offs_floor * (1.0 - coords_rem[:, 1]) + x_offs_ceil * coords_rem[:, 1]\n        )\n        offset_x *= self._params[\"forward_input_shape\"][3]\n        y_offs_floor = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            1,\n            coords_floor[:, 0].long(),\n            coords_floor[:, 1].long(),\n        ].to(coords.device)\n        y_offs_ceil = self._params[\"noise\"][\n            range(self._params[\"forward_input_shape\"][0]),\n            1,\n            coords_ceil[:, 0].long(),\n            coords_ceil[:, 1].long(),\n        ].to(coords.device)\n        offset_y = (\n            y_offs_floor * (1.0 - coords_rem[:, 0]) + y_offs_ceil * coords_rem[:, 0]\n        )\n        offset_y *= self._params[\"forward_input_shape\"][2]\n        result[:, 0] = coords[:, 0] + offset_y\n        result[:, 1] = coords[:, 1] + offset_x\n        # Clamp (as is done in the deformation code, too).\n        result[:, 0] = result[:, 0].clamp(0, self._params[\"forward_input_shape\"][2])\n        result[:, 1] = result[:, 1].clamp(0, self._params[\"forward_input_shape\"][3])\n        return result", "\n\ndef elastic_transform2d(\n    image: torch.Tensor,\n    noise: torch.Tensor,\n    kernel_size: Tuple[int, int] = (63, 63),\n    sigma: Tuple[float, float] = (32.0, 32.0),\n    alpha: Tuple[float, float] = (1.0, 1.0),\n    align_corners: bool = False,\n    mode: str = \"bilinear\",\n    padding_mode: str = \"zeros\",\n) -> torch.Tensor:\n    r\"\"\"Apply elastic transform of images as described in :cite:`Simard2003BestPF`.\n\n    .. image:: _static/img/elastic_transform2d.png\n\n    Args:\n        image: Input image to be transformed with shape :math:`(B, C, H, W)`.\n        noise: Noise image used to spatially transform the input image. Same\n          resolution as the input image with shape :math:`(B, 2, H, W)`. The coordinates order\n          it is expected to be in x-y.\n        kernel_size: the size of the Gaussian kernel.\n        sigma: The standard deviation of the Gaussian in the y and x directions,\n          respectively. Larger sigma results in smaller pixel displacements.\n        alpha : The scaling factor that controls the intensity of the deformation\n          in the y and x directions, respectively.\n        align_corners: Interpolation flag used by ```grid_sample```.\n        mode: Interpolation mode used by ```grid_sample```. Either ``'bilinear'`` or ``'nearest'``.\n        padding_mode: The padding used by ```grid_sample```. Either ``'zeros'``, ``'border'`` or ``'refection'``.\n\n    .. note:\n        ```sigma``` and ```alpha``` can also be a ``torch.Tensor``. However, you could not torchscript\n         this function with tensor until PyTorch 1.8 is released.\n\n    Returns:\n        the elastically transformed input image with shape :math:`(B,C,H,W)`.\n\n    Example:\n        >>> image = torch.rand(1, 3, 5, 5)\n        >>> noise = torch.rand(1, 2, 5, 5, requires_grad=True)\n        >>> image_hat = elastic_transform2d(image, noise, (3, 3))\n        >>> image_hat.mean().backward()\n\n        >>> image = torch.rand(1, 3, 5, 5)\n        >>> noise = torch.rand(1, 2, 5, 5)\n        >>> sigma = torch.tensor([4., 4.], requires_grad=True)\n        >>> image_hat = elastic_transform2d(image, noise, (3, 3), sigma)\n        >>> image_hat.mean().backward()\n\n        >>> image = torch.rand(1, 3, 5, 5)\n        >>> noise = torch.rand(1, 2, 5, 5)\n        >>> alpha = torch.tensor([16., 32.], requires_grad=True)\n        >>> image_hat = elastic_transform2d(image, noise, (3, 3), alpha=alpha)\n        >>> image_hat.mean().backward()\n    \"\"\"\n    if not isinstance(image, torch.Tensor):\n        raise TypeError(f\"Input image is not torch.Tensor. Got {type(image)}\")\n\n    if not isinstance(noise, torch.Tensor):\n        raise TypeError(f\"Input noise is not torch.Tensor. Got {type(noise)}\")\n\n    if not len(image.shape) == 4:\n        raise ValueError(f\"Invalid image shape, we expect BxCxHxW. Got: {image.shape}\")\n\n    if not len(noise.shape) == 4 or noise.shape[1] != 2:\n        raise ValueError(f\"Invalid noise shape, we expect Bx2xHxW. Got: {noise.shape}\")\n\n    # Get Gaussian kernel for 'y' and 'x' displacement\n    kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, (sigma[0], sigma[0]))[\n        None\n    ]\n    kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, (sigma[1], sigma[1]))[\n        None\n    ]\n\n    # Convolve over a random displacement matrix and scale them with 'alpha'\n    disp_x: torch.Tensor = noise[:, :1]\n    disp_y: torch.Tensor = noise[:, 1:]\n\n    disp_x = filter2d(disp_x, kernel=kernel_y, border_type=\"constant\") * alpha[0]\n    disp_y = filter2d(disp_y, kernel=kernel_x, border_type=\"constant\") * alpha[1]\n\n    # stack and normalize displacement\n    disp = torch.cat([disp_x, disp_y], dim=1).permute(0, 2, 3, 1)\n\n    # Warp image based on displacement matrix\n    _, _, h, w = image.shape\n    grid = create_meshgrid(h, w, device=image.device).to(image.dtype)\n    warped = F.grid_sample(\n        image,\n        (grid + disp).clamp(-1, 1),\n        align_corners=align_corners,\n        mode=mode,\n        padding_mode=padding_mode,\n    )\n\n    return warped", ""]}
{"filename": "calibration/camera.py", "chunked_list": ["from typing import Optional, Tuple\nimport numpy as np\nimport torch\n\nimport torch.nn.functional as F\n\nfrom .networks import LensNet\n\n\n# The following methods (rotation_6d_to_matrix and matrix_to_rotation_6d) fall under the PyTorch3D license.", "\n# The following methods (rotation_6d_to_matrix and matrix_to_rotation_6d) fall under the PyTorch3D license.\n\n\ndef rotation_6d_to_matrix(d6: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Converts 6D rotation representation by Zhou et al. [1] to rotation matrix\n    using Gram--Schmidt orthogonalization per Section B of [1].\n    Args:\n        d6: 6D rotation representation, of size (*, 6)\n    Returns:\n        batch of rotation matrices of size (*, 3, 3)\n    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.\n    On the Continuity of Rotation Representations in Neural Networks.\n    IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n    Retrieved from http://arxiv.org/abs/1812.07035\n    \"\"\"\n\n    a1, a2 = d6[..., :3], d6[..., 3:]\n    b1 = F.normalize(a1, dim=-1)\n    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n    b2 = F.normalize(b2, dim=-1)\n    b3 = torch.cross(b1, b2, dim=-1)\n    return torch.stack((b1, b2, b3), dim=-2)", "\n\ndef matrix_to_rotation_6d(matrix: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Converts rotation matrices to 6D rotation representation by Zhou et al. [1]\n    by dropping the last row. Note that 6D representation is not unique.\n    Args:\n        matrix: batch of rotation matrices of size (*, 3, 3)\n    Returns:\n        6D rotation representation, of size (*, 6)\n    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.\n    On the Continuity of Rotation Representations in Neural Networks.\n    IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n    Retrieved from http://arxiv.org/abs/1812.07035\n    \"\"\"\n    batch_dim = matrix.size()[:-2]\n    return matrix[..., :2, :].clone().reshape(batch_dim + (6,))", "\n\ndef zero_K_gradient(grad: torch.Tensor):\n    # We zero the gradients for regions that we want to keep fixed.\n    grad[0, 1] = 0.0\n    grad[1, 0] = 0.0\n    grad[2, :] = 0.0\n\n\nclass Camera:\n    def __init__(\n        self,\n        resolution_w_h: Tuple[int, int],\n        K: torch.Tensor,\n        lensnet: Optional[LensNet] = None,\n        RT=None,\n    ):\n        \"\"\"\n        Parameters\n        ==========\n\n        resolution_w_h: (int, int). Resolution width and height.\n        K: torch.tensor. Intrinsic matrix of the form\n            f_x  0   c_x\n             0  f_y  c_y\n             0   0    1\n        lensnet: LensNet. Distortion model. Can be None\n            if not existent (pinhole model).\n        RT: torch.tensor. Extrinsic w2c matrix of the form\n            r11 r12 r13 t1\n            r21 r22 r23 t2\n            r31 r32 r33 t3\n        \"\"\"\n        assert resolution_w_h[0] > 0\n        assert resolution_w_h[1] > 0\n        assert len(resolution_w_h) == 2\n        self.resolution_w_h = resolution_w_h\n        assert isinstance(K, torch.Tensor)\n        assert K.ndim == 2\n        assert K.shape[0] == 3\n        assert K.shape[1] == 3\n        assert K[0, 0] > 0.0\n        assert K[0, 1] == 0.0\n        assert K[1, 0] == 0.0\n        assert K[1, 1] > 0.0\n        assert K[2, 0] == 0.0\n        assert K[2, 1] == 0.0\n        assert K[2, 2] == 1.0\n        self.K = K\n        self.K.register_hook(zero_K_gradient)\n        assert lensnet is None or isinstance(lensnet, LensNet)\n        self.lensnet = lensnet\n        assert RT is None or isinstance(RT, torch.Tensor)\n        if RT is None:\n            self.RT = torch.eye(3, 4, dtype=torch.float32, device=K.device)\n        else:\n            assert RT.ndim == 2\n            assert RT.shape[0] == 3\n            assert RT.shape[1] == 4\n            self.RT = RT\n\n    @property\n    def RT(self):\n        rot_mat = rotation_6d_to_matrix(self.R[None, ...])[0]\n        return torch.cat((rot_mat, self.T), dim=1)\n\n    @RT.setter\n    def RT(self, value):\n        self.R = matrix_to_rotation_6d(value[:, :3])\n        self.T = value[:, 3:4]\n\n    @staticmethod\n    def homogenize(X: torch.Tensor):\n        assert X.ndim == 2\n        assert X.shape[1] in (2, 3)\n        return torch.cat(\n            (X, torch.ones((X.shape[0], 1), dtype=X.dtype, device=X.device)), dim=1\n        )\n\n    @staticmethod\n    def dehomogenize(X: torch.Tensor):\n        assert X.ndim == 2\n        assert X.shape[1] in (3, 4)\n        return X[:, :-1] / X[:, -1:]\n\n    def world_to_view(self, P_world: torch.Tensor):\n        assert P_world.ndim == 2\n        assert P_world.shape[1] == 3\n        P_world_hom = self.homogenize(P_world)\n        P_view = (self.RT @ P_world_hom.T).T\n        return P_view\n\n    def view_to_world(self, P_view: torch.Tensor):\n        assert P_view.ndim == 2\n        assert P_view.shape[1] == 3\n        P_view_shifted = P_view - self.RT[:, 3][None, ...]\n        P_world = (self.RT[:, :3].T @ P_view_shifted.T).T\n        return P_world\n\n    def view_to_sensor(self, P_view: torch.Tensor):\n        P_view_outsidelens_direction = self.dehomogenize(P_view)  # x' = x/z, y' = y/z\n        if self.lensnet is not None:\n            P_view_insidelens_direction = self.lensnet.forward(\n                P_view_outsidelens_direction, sensor_to_frustum=False\n            )\n        else:\n            P_view_insidelens_direction = P_view_outsidelens_direction\n        P_view_insidelens_direction_hom = self.homogenize(P_view_insidelens_direction)\n        P_sensor = self.dehomogenize((self.K @ P_view_insidelens_direction_hom.T).T)\n        return P_sensor\n\n    def get_rays_view(self, P_sensor: torch.Tensor):\n        assert P_sensor.ndim == 2\n        assert P_sensor.shape[1] == 2\n        P_sensor_hom = self.homogenize(P_sensor)\n        P_view_insidelens_direction_hom = (torch.inverse(self.K) @ P_sensor_hom.T).T\n        P_view_insidelens_direction = self.dehomogenize(P_view_insidelens_direction_hom)\n        if self.lensnet is not None:\n            P_view_outsidelens_direction = self.lensnet.forward(\n                P_view_insidelens_direction, sensor_to_frustum=True\n            )\n        else:\n            P_view_outsidelens_direction = P_view_insidelens_direction\n        return self.homogenize(P_view_outsidelens_direction)\n\n    def get_rays_world(self, P_sensor: torch.tensor):\n        rays_view = self.get_rays_view(P_sensor)\n        origins_view = torch.zeros_like(rays_view)\n        origins_world = self.view_to_world(origins_view)\n        rays_world = (self.RT[:, :3].T @ rays_view.T).T\n        return origins_world, rays_world\n\n    def sensor_to_view(self, P_sensor_and_depth: torch.Tensor):\n        assert P_sensor_and_depth.ndim == 2\n        assert P_sensor_and_depth.shape[1] == 3\n        P_sensor, P_depth = P_sensor_and_depth[:, :2], P_sensor_and_depth[:, 2:3]\n        rays = self.get_rays_view(P_sensor)\n        P_view = rays * P_depth\n        return P_view\n\n    def project_points(self, P_world: torch.Tensor):\n        P_view_outsidelens = self.world_to_view(P_world)\n        P_sensor = self.view_to_sensor(P_view_outsidelens)\n        return P_sensor\n\n    def unproject_points(self, P_sensor: torch.Tensor):\n        P_view_outsidelens = self.sensor_to_view(P_sensor)\n        P_world = self.view_to_world(P_view_outsidelens)\n        return P_world", "\nclass Camera:\n    def __init__(\n        self,\n        resolution_w_h: Tuple[int, int],\n        K: torch.Tensor,\n        lensnet: Optional[LensNet] = None,\n        RT=None,\n    ):\n        \"\"\"\n        Parameters\n        ==========\n\n        resolution_w_h: (int, int). Resolution width and height.\n        K: torch.tensor. Intrinsic matrix of the form\n            f_x  0   c_x\n             0  f_y  c_y\n             0   0    1\n        lensnet: LensNet. Distortion model. Can be None\n            if not existent (pinhole model).\n        RT: torch.tensor. Extrinsic w2c matrix of the form\n            r11 r12 r13 t1\n            r21 r22 r23 t2\n            r31 r32 r33 t3\n        \"\"\"\n        assert resolution_w_h[0] > 0\n        assert resolution_w_h[1] > 0\n        assert len(resolution_w_h) == 2\n        self.resolution_w_h = resolution_w_h\n        assert isinstance(K, torch.Tensor)\n        assert K.ndim == 2\n        assert K.shape[0] == 3\n        assert K.shape[1] == 3\n        assert K[0, 0] > 0.0\n        assert K[0, 1] == 0.0\n        assert K[1, 0] == 0.0\n        assert K[1, 1] > 0.0\n        assert K[2, 0] == 0.0\n        assert K[2, 1] == 0.0\n        assert K[2, 2] == 1.0\n        self.K = K\n        self.K.register_hook(zero_K_gradient)\n        assert lensnet is None or isinstance(lensnet, LensNet)\n        self.lensnet = lensnet\n        assert RT is None or isinstance(RT, torch.Tensor)\n        if RT is None:\n            self.RT = torch.eye(3, 4, dtype=torch.float32, device=K.device)\n        else:\n            assert RT.ndim == 2\n            assert RT.shape[0] == 3\n            assert RT.shape[1] == 4\n            self.RT = RT\n\n    @property\n    def RT(self):\n        rot_mat = rotation_6d_to_matrix(self.R[None, ...])[0]\n        return torch.cat((rot_mat, self.T), dim=1)\n\n    @RT.setter\n    def RT(self, value):\n        self.R = matrix_to_rotation_6d(value[:, :3])\n        self.T = value[:, 3:4]\n\n    @staticmethod\n    def homogenize(X: torch.Tensor):\n        assert X.ndim == 2\n        assert X.shape[1] in (2, 3)\n        return torch.cat(\n            (X, torch.ones((X.shape[0], 1), dtype=X.dtype, device=X.device)), dim=1\n        )\n\n    @staticmethod\n    def dehomogenize(X: torch.Tensor):\n        assert X.ndim == 2\n        assert X.shape[1] in (3, 4)\n        return X[:, :-1] / X[:, -1:]\n\n    def world_to_view(self, P_world: torch.Tensor):\n        assert P_world.ndim == 2\n        assert P_world.shape[1] == 3\n        P_world_hom = self.homogenize(P_world)\n        P_view = (self.RT @ P_world_hom.T).T\n        return P_view\n\n    def view_to_world(self, P_view: torch.Tensor):\n        assert P_view.ndim == 2\n        assert P_view.shape[1] == 3\n        P_view_shifted = P_view - self.RT[:, 3][None, ...]\n        P_world = (self.RT[:, :3].T @ P_view_shifted.T).T\n        return P_world\n\n    def view_to_sensor(self, P_view: torch.Tensor):\n        P_view_outsidelens_direction = self.dehomogenize(P_view)  # x' = x/z, y' = y/z\n        if self.lensnet is not None:\n            P_view_insidelens_direction = self.lensnet.forward(\n                P_view_outsidelens_direction, sensor_to_frustum=False\n            )\n        else:\n            P_view_insidelens_direction = P_view_outsidelens_direction\n        P_view_insidelens_direction_hom = self.homogenize(P_view_insidelens_direction)\n        P_sensor = self.dehomogenize((self.K @ P_view_insidelens_direction_hom.T).T)\n        return P_sensor\n\n    def get_rays_view(self, P_sensor: torch.Tensor):\n        assert P_sensor.ndim == 2\n        assert P_sensor.shape[1] == 2\n        P_sensor_hom = self.homogenize(P_sensor)\n        P_view_insidelens_direction_hom = (torch.inverse(self.K) @ P_sensor_hom.T).T\n        P_view_insidelens_direction = self.dehomogenize(P_view_insidelens_direction_hom)\n        if self.lensnet is not None:\n            P_view_outsidelens_direction = self.lensnet.forward(\n                P_view_insidelens_direction, sensor_to_frustum=True\n            )\n        else:\n            P_view_outsidelens_direction = P_view_insidelens_direction\n        return self.homogenize(P_view_outsidelens_direction)\n\n    def get_rays_world(self, P_sensor: torch.tensor):\n        rays_view = self.get_rays_view(P_sensor)\n        origins_view = torch.zeros_like(rays_view)\n        origins_world = self.view_to_world(origins_view)\n        rays_world = (self.RT[:, :3].T @ rays_view.T).T\n        return origins_world, rays_world\n\n    def sensor_to_view(self, P_sensor_and_depth: torch.Tensor):\n        assert P_sensor_and_depth.ndim == 2\n        assert P_sensor_and_depth.shape[1] == 3\n        P_sensor, P_depth = P_sensor_and_depth[:, :2], P_sensor_and_depth[:, 2:3]\n        rays = self.get_rays_view(P_sensor)\n        P_view = rays * P_depth\n        return P_view\n\n    def project_points(self, P_world: torch.Tensor):\n        P_view_outsidelens = self.world_to_view(P_world)\n        P_sensor = self.view_to_sensor(P_view_outsidelens)\n        return P_sensor\n\n    def unproject_points(self, P_sensor: torch.Tensor):\n        P_view_outsidelens = self.sensor_to_view(P_sensor)\n        P_world = self.view_to_world(P_view_outsidelens)\n        return P_world", ""]}
{"filename": "calibration/scripts/calibrate.py", "chunked_list": ["import json\nimport logging\nfrom os import path\nimport cv2\nimport hydra\nimport imageio\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nimport tqdm", "import torchvision.transforms as transforms\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom torch.nn import functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom ..camera import Camera\nfrom ..config import Config\nfrom ..networks import iResNet, LensNet \nfrom ..util import batched_func", "from ..networks import iResNet, LensNet \nfrom ..util import batched_func\n\n\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Calibrate a lens using previously acquired data in `data/live`.\n\n    Uses gradient-descent based optimization with RLS to be robust w.r.t.\n    outliers to calibrate the lens. Does an automatic comparison with the\n    OpenCV model.\n    \"\"\"\n    LOGGER.info(\"Loading detection data...\")\n    storage_fp = path.join(\"..\", \"..\", \"data/target\")\n\n    with open(path.join(storage_fp, \"points.json\"), \"r\") as inf:\n        cam_info = json.load(inf)\n    n_points = 0\n    n_frames = 0\n    board_coords_val = []\n    frame_coords_val = []\n    subs_fac = cfg.calibration.subs_fac\n    LOGGER.info(\n        \"Converting representation for initialization (subsampling every %dth frame)...\",\n        subs_fac,\n    )\n    for idx, (im_board_coords, im_frame_coords) in enumerate(\n        zip(cam_info[\"board_coordinates_xyz\"], cam_info[\"frame_coordinates_xy\"])\n    ):\n        if idx % subs_fac == 0 and len(im_board_coords) >= 6:\n            board_coords_val.append(np.array(im_board_coords, dtype=np.float32))\n            frame_coords_val.append(np.array(im_frame_coords, dtype=np.float32))\n            n_points += frame_coords_val[-1].shape[0]\n            n_frames += 1\n    LOGGER.info(f\"Using {n_points} points from {n_frames} frames.\")\n    LOGGER.info(\"Finding initialization...\")\n    (\n        retval,\n        camera_matrix,\n        dist_coeffs,\n        r_vecs_obj_to_cam,\n        t_vecs_obj_to_cam,\n        _,  # stdDeviationsIntrinsics,\n        _,  # stdDeviationsExtrinsics,\n        _,  # perViewErrors,\n    ) = cv2.calibrateCameraExtended(\n        board_coords_val,\n        frame_coords_val,\n        tuple(cam_info[\"resolution_wh\"]),\n        None,\n        None,\n        flags=cv2.CALIB_RATIONAL_MODEL,\n    )\n\n    LOGGER.info(f\"Overall RMS reprojection error after initialization: {retval}.\")\n\n    LOGGER.info(\"Initializing lens net...\")\n    RTs_val = []\n    dev = torch.device(\"cuda\")\n    lens_net = iResNet().to(dev)\n    iResNet().test_inverse()\n\n    K = torch.from_numpy(camera_matrix).to(torch.float32).to(dev).requires_grad_()\n    cams_val = []\n    residuals_val = []\n    LOGGER.info(\"Preparing validation data...\")\n    for cam_idx, (r_vecs, t_vecs) in enumerate(\n        zip(r_vecs_obj_to_cam, t_vecs_obj_to_cam)\n    ):\n        RT = (\n            torch.from_numpy(\n                np.hstack((cv2.Rodrigues(r_vecs)[0], t_vecs)),\n            )\n            .to(torch.float32)\n            .to(dev)\n        ).requires_grad_()\n        RTs_val.append(RT)\n        cams_val.append(Camera(cam_info[\"resolution_wh\"], K, lens_net, RT))\n        proj_opencv = cv2.projectPoints(\n            board_coords_val[cam_idx],\n            r_vecs_obj_to_cam[cam_idx],\n            t_vecs_obj_to_cam[cam_idx],\n            camera_matrix,\n            dist_coeffs,\n        )[0][:, 0, :]\n        residuals_val.append(\n            np.square(np.linalg.norm(frame_coords_val[cam_idx] - proj_opencv, axis=1))\n        )\n\n    RMSE_opencv_val = np.sqrt(np.mean(np.concatenate(residuals_val, axis=0)))\n    LOGGER.info(f\"RMSE OpenCV: {RMSE_opencv_val}\")\n    LOGGER.info(\"Assembling training set...\")\n    RTs_train = []\n    cams_train = []\n    board_coords_train = []\n    frame_coords_train = []\n    n_points_train = 0\n    n_frames_train = 0\n    for idx, (im_board_coords, im_frame_coords) in enumerate(\n        zip(cam_info[\"board_coordinates_xyz\"], cam_info[\"frame_coordinates_xy\"])\n    ):\n        if idx % subs_fac != 0 and len(im_board_coords) >= 6:\n            board_coords_train.append(np.array(im_board_coords, dtype=np.float32))\n            frame_coords_train.append(np.array(im_frame_coords, dtype=np.float32))\n            n_points_train += frame_coords_train[-1].shape[0]\n            n_frames_train += 1\n            _, r_vecs, t_vecs = cv2.solvePnP(\n                board_coords_train[-1],\n                frame_coords_train[-1],\n                camera_matrix,\n                dist_coeffs,\n            )\n            RT = (\n                torch.from_numpy(\n                    np.hstack((cv2.Rodrigues(r_vecs)[0], t_vecs)),\n                )\n                .to(torch.float32)\n                .to(dev)\n            ).requires_grad_()\n            RTs_train.append(RT)\n            cams_train.append(Camera(cam_info[\"resolution_wh\"], K, lens_net, RT))\n            board_coords_train[-1] = (\n                torch.from_numpy(board_coords_train[-1]).to(torch.float32).to(dev)\n            )\n            frame_coords_train[-1] = (\n                torch.from_numpy(frame_coords_train[-1]).to(torch.float32).to(dev)\n            )\n    LOGGER.info(\n        f\"Using {n_points_train} points for training from {n_frames_train} frames.\"\n    )\n\n    LOGGER.info(\"Setting up optimizer...\")\n    scale = torch.nn.Parameter(torch.tensor(1.305)).requires_grad_()\n    optimizer_train = torch.optim.Adam(\n        [\n            K,\n        ]\n        + list(lens_net.parameters())\n        + RTs_train\n        + [scale],\n        lr=1e-4,\n    )\n    optimizer_val_RT = torch.optim.Adam(\n        RTs_val,\n        lr=1e-4,\n    )\n\n    if is_eval:\n        PATH = log_dir + \"/lensnet_latest.pt\"\n        checkpoint = torch.load(PATH)\n        lens_net.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer_train.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        RTs_train = checkpoint[\"RTs_train\"]\n        RTs_val = checkpoint[\"RTs_train\"]\n        K = checkpoint[\"K\"]\n\n        loss_vals = []\n        rows, cols = 3, 4\n        gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n        fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 9))\n        bleed = 0\n        fig.subplots_adjust(\n            left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n        )\n        for cam_idx, (cam, board_coord_mat, frame_coord_mat, ax) in enumerate(\n            zip(cams_val, board_coords_val, frame_coords_val, axarr.ravel())\n        ):\n            board_coord_mat = (\n                torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n            )\n            frame_coord_mat = (\n                torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n            )\n            projected = cam.project_points(board_coord_mat)\n            val_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )  # RMSE\n            loss_vals.append(val_error.detach().clone())\n            # visualize keypoints\n            vis, camera_directions_w_lens = vis_lens(cam)\n            # ax.imshow(vis[..., :3])\n            ax.scatter(\n                projected.detach().cpu().numpy()[:, 0],\n                projected.detach().cpu().numpy()[:, 1],\n                marker=\"o\",\n                s=5,\n            )\n            ax.scatter(\n                frame_coord_mat.detach().cpu().numpy()[:, 0],\n                frame_coord_mat.detach().cpu().numpy()[:, 1],\n                marker=\"o\",\n                s=5,\n            )\n            ax.set_xlim((0, 512))\n            ax.set_ylim((0, 512))\n            ax.set_axis_off()\n\n        fig.savefig(log_dir + \"/vis_eval.png\")\n        plt.close(fig)\n\n        rows, cols = 1, 4\n        gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n        fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 3))\n        bleed = 0\n        fig.subplots_adjust(\n            left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n        )\n\n        vis, camera_directions_w_lens = vis_lens(cams_val[0])\n        target_board_ABC = (\n            imageio.imread(path.join(storage_fp, \"target_rainbow_ABC.png\"))[..., :3]\n            / 255.0\n        )\n        target_board_path = path.join(storage_fp, \"target_rainbow.png\")\n        target_board = imageio.imread(target_board_path)[..., :3]\n        transform = transforms.ToTensor()\n        image = transform(target_board).unsqueeze(0).to(dev)\n        flow = camera_directions_w_lens.unsqueeze(0).to(dev) * 1.305\n        output = F.grid_sample(\n            image, flow, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n        )\n        output_img = output[0].permute(1, 2, 0).cpu().numpy()\n        axarr[0].imshow(target_board)\n        axarr[1].imshow(output_img)\n        axarr[2].imshow(target_board_ABC)\n\n        def rgb2gray(rgb):\n            return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\n        emap = abs(target_board_ABC - output_img)\n        axarr[3].imshow(rgb2gray(emap), cmap=plt.get_cmap(\"gray\"))\n\n        for ax in axarr:\n            ax.set_axis_off()\n\n        fig.savefig(log_dir + \"/img_eval.png\")\n        plt.close(fig)\n\n        rmse = torch.sqrt(torch.cat(loss_vals, dim=0).mean())\n        print(\"rmse lensnet: \", rmse.item())\n\n\n    scheduler_type = \"super\"\n    if scheduler_type == \"linear\":\n        step_size = 20000  # 15000\n        final_ratio = 0.01  # 0.05\n        start_ratio = 0.15\n        duration_ratio = 0.6\n\n        def lambda_rule(ep):\n            lr_l = 1.0 - min(\n                1,\n                max(0, ep - start_ratio * step_size)\n                / float(duration_ratio * step_size),\n            ) * (1 - final_ratio)\n            return lr_l\n\n        scheduler = torch.optim.lr_scheduler.LambdaLR(\n            optimizer_train, lr_lambda=lambda_rule\n        )\n        LOGGER.info(\n            f\"LR scheduler has step size {step_size}, final ratio {final_ratio}, start ratio {start_ratio}, duration ratio {duration_ratio}.\"\n        )\n    elif scheduler_type == \"super\":\n        step_size = 12000\n        max_lr = 1e-4\n        pct_start = 0.05\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer_train,\n            max_lr=max_lr,\n            total_steps=step_size,\n            pct_start=pct_start,\n        )\n        LOGGER.info(\n            f\"LR scheduler has step size {step_size}, max lr {max_lr}, pct_start {pct_start}\"\n        )\n\n    LOGGER.info(f\"Lens has {sum(p.numel() for p in lens_net.parameters())} parameters.\")\n    writer = SummaryWriter(log_dir=log_dir)\n    LOGGER.info(f\"Writing logs and tensorboard to `{log_dir}`.\")\n    LOGGER.info(\"Optimizing...\")\n    start_epoch = 0\n    if is_eval:\n        start_epoch = checkpoint[\"epoch\"]\n    rnge = tqdm.trange(start_epoch, 500)\n\n\n    # load data\n    dense_matching = False\n    if dense_matching:\n        target_board_ABC = (\n            imageio.imread(path.join(storage_fp, \"target.png\"))[..., :3]\n            / 255.0\n        )\n        target_board_path = path.join(storage_fp, \"target.png\")\n        target_board = imageio.imread(target_board_path)[..., :3]\n        transform = transforms.ToTensor()\n        image = transform(target_board).unsqueeze(0).to(dev)\n        image_ABC = transform(target_board_ABC).unsqueeze(0).to(dev)\n        l1_loss = torch.nn.L1Loss()\n\n    for epoch_idx in rnge:\n        loss_vals = []\n        loss_vals_train = []\n        loss_train = []\n\n        for _, (cam, board_coord_mat, frame_coord_mat) in tqdm.tqdm(\n            enumerate(zip(cams_train, board_coords_train, frame_coords_train)),\n            total=len(cams_train),\n        ):\n            optimizer_train.zero_grad()\n            projected = cam.project_points(board_coord_mat)\n            loss = torch.linalg.norm(projected - frame_coord_mat, dim=1).mean()\n            loss_vals_train.append(loss.item())\n\n            proj_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )\n            loss_train.append(proj_error)\n\n            # dense match loss\n            if dense_matching:\n                camera_directions_w_lens = cam.project_lens()\n                flow = camera_directions_w_lens.unsqueeze(0) * scale\n                output = F.grid_sample(\n                    image[..., ::2, ::2],\n                    flow,\n                    mode=\"bilinear\",\n                    padding_mode=\"zeros\",\n                    align_corners=False,\n                )\n                dense_loss = l1_loss(image_ABC[..., ::2, ::2], output)\n\n                loss += dense_loss * 50\n\n            loss.backward()\n            optimizer_train.step()\n            scheduler.step()\n\n        avg_loss = np.array(loss_vals_train).mean()\n        if dense_matching:\n            writer.add_scalar(\"dense_loss\", dense_loss, epoch_idx)\n        writer.add_scalar(\"reproj_loss\", avg_loss, epoch_idx)\n\n        for cam_idx, (cam, board_coord_mat, frame_coord_mat) in enumerate(\n            zip(cams_val, board_coords_val, frame_coords_val)\n        ):\n            optimizer_val_RT.zero_grad()\n            board_coord_mat = (\n                torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n            )\n            frame_coord_mat = (\n                torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n            )\n            projected = cam.project_points(board_coord_mat)\n            val_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )  # RMSE\n            loss_vals.append(val_error.detach().clone())\n  \n            loss = torch.linalg.norm(projected - frame_coord_mat, dim=1).mean()\n\n            # dense match loss\n            if dense_matching:\n                camera_directions_w_lens = cam.project_lens()\n                flow = camera_directions_w_lens.unsqueeze(0) * scale\n                output = F.grid_sample(\n                    image[..., ::2, ::2],\n                    flow,\n                    mode=\"bilinear\",\n                    padding_mode=\"zeros\",\n                    align_corners=False,\n                )\n                dense_loss = l1_loss(image_ABC[..., ::2, ::2], output)\n\n                loss += dense_loss * 50\n\n            loss.backward()\n            optimizer_val_RT.step()\n\n            if epoch_idx % 10 == 0 and cam_idx % 100 == 0 and dense_matching:\n\n                def rgb2gray(rgb):\n                    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\n                rows, cols = 1, 4\n                gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n                fig, axarr = plt.subplots(\n                    rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 3)\n                )\n                bleed = 0\n                fig.subplots_adjust(\n                    left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n                )\n\n                axarr[0].imshow(target_board)\n                output_img = output[0].permute(1, 2, 0).detach().cpu().numpy()\n                axarr[1].imshow(output_img)\n                axarr[2].imshow(target_board_ABC[::2, ::2, :])\n                emap = abs(target_board_ABC[::2, ::2, :] - output_img)\n                axarr[3].imshow(rgb2gray(emap), cmap=plt.get_cmap(\"gray\"))\n                for ax in axarr:\n                    ax.set_axis_off()\n                fig.savefig(log_dir + f\"/dense_match_{epoch_idx // 10}.png\")\n\n                plt.close(fig)\n\n        if epoch_idx % 10 == 0:\n            # visualize keypoints\n\n            rows, cols = 3, 4\n            gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n            fig, axarr = plt.subplots(\n                rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 9)\n            )\n            bleed = 0\n            fig.subplots_adjust(\n                left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n            )\n            for cam_idx, (cam, board_coord_mat, frame_coord_mat, ax) in enumerate(\n                zip(cams_val, board_coords_val, frame_coords_val, axarr.ravel())\n            ):\n                board_coord_mat = (\n                    torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n                )\n                frame_coord_mat = (\n                    torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n                )\n                projected = cam.project_points(board_coord_mat)\n                # vis, _ = vis_lens(cam)\n                # ax.imshow(vis[..., :3])\n                ax.scatter(\n                    frame_coord_mat.detach().cpu().numpy()[:, 0],\n                    frame_coord_mat.detach().cpu().numpy()[:, 1],\n                    marker=\"o\",\n                    s=5,\n                )\n                ax.scatter(\n                    projected.detach().cpu().numpy()[:, 0],\n                    projected.detach().cpu().numpy()[:, 1],\n                    marker=\"o\",\n                    s=5,\n                )\n                ax.set_axis_off()\n                ax.grid(False)\n                plt.tight_layout()\n            fig.savefig(log_dir + f\"/vis_{epoch_idx // 10}.png\")\n            vis, _ = vis_lens(cam)\n            writer.add_image(\n                \"vis/lens\", vis, global_step=epoch_idx, walltime=None, dataformats=\"HWC\"\n            )\n            plt.close(fig)\n\n        train_rmse = torch.sqrt(torch.cat(loss_train, dim=0).mean())\n        writer.add_scalar(\"train_rmse\", train_rmse.detach(), epoch_idx)\n        rmse = torch.sqrt(torch.cat(loss_vals, dim=0).mean())\n        writer.add_scalar(\"val_rmse\", rmse, epoch_idx)\n        lr = optimizer_train.param_groups[0][\"lr\"]\n        writer.add_scalar(\"lr\", lr, epoch_idx)\n        print(f\"RMSE (train): {train_rmse}, RMSE (val): {rmse}, lr: {lr}\")\n        if dense_matching:\n            print(\n                f\"Epoch {epoch_idx}, reproj loss: {avg_loss}, dense loss: {dense_loss}\"\n            )\n        else:\n            print(f\"Epoch {epoch_idx}, reproj loss: {avg_loss}\")\n\n        if epoch_idx > 0 and epoch_idx % 25 == 0 and not is_eval:\n            PATH = log_dir + \"/lensnet_latest.pt\"\n            torch.save(\n                {\n                    \"epoch\": epoch_idx,\n                    \"model_state_dict\": lens_net.state_dict(),\n                    \"optimizer_state_dict\": optimizer_train.state_dict(),\n                    \"K\": K,\n                    \"RTs_train\": RTs_train,\n                    \"RTs_val\": RTs_val,\n                    \"loss\": loss,\n                },\n                PATH,\n            )", "@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Calibrate a lens using previously acquired data in `data/live`.\n\n    Uses gradient-descent based optimization with RLS to be robust w.r.t.\n    outliers to calibrate the lens. Does an automatic comparison with the\n    OpenCV model.\n    \"\"\"\n    LOGGER.info(\"Loading detection data...\")\n    storage_fp = path.join(\"..\", \"..\", \"data/target\")\n\n    with open(path.join(storage_fp, \"points.json\"), \"r\") as inf:\n        cam_info = json.load(inf)\n    n_points = 0\n    n_frames = 0\n    board_coords_val = []\n    frame_coords_val = []\n    subs_fac = cfg.calibration.subs_fac\n    LOGGER.info(\n        \"Converting representation for initialization (subsampling every %dth frame)...\",\n        subs_fac,\n    )\n    for idx, (im_board_coords, im_frame_coords) in enumerate(\n        zip(cam_info[\"board_coordinates_xyz\"], cam_info[\"frame_coordinates_xy\"])\n    ):\n        if idx % subs_fac == 0 and len(im_board_coords) >= 6:\n            board_coords_val.append(np.array(im_board_coords, dtype=np.float32))\n            frame_coords_val.append(np.array(im_frame_coords, dtype=np.float32))\n            n_points += frame_coords_val[-1].shape[0]\n            n_frames += 1\n    LOGGER.info(f\"Using {n_points} points from {n_frames} frames.\")\n    LOGGER.info(\"Finding initialization...\")\n    (\n        retval,\n        camera_matrix,\n        dist_coeffs,\n        r_vecs_obj_to_cam,\n        t_vecs_obj_to_cam,\n        _,  # stdDeviationsIntrinsics,\n        _,  # stdDeviationsExtrinsics,\n        _,  # perViewErrors,\n    ) = cv2.calibrateCameraExtended(\n        board_coords_val,\n        frame_coords_val,\n        tuple(cam_info[\"resolution_wh\"]),\n        None,\n        None,\n        flags=cv2.CALIB_RATIONAL_MODEL,\n    )\n\n    LOGGER.info(f\"Overall RMS reprojection error after initialization: {retval}.\")\n\n    LOGGER.info(\"Initializing lens net...\")\n    RTs_val = []\n    dev = torch.device(\"cuda\")\n    lens_net = iResNet().to(dev)\n    iResNet().test_inverse()\n\n    K = torch.from_numpy(camera_matrix).to(torch.float32).to(dev).requires_grad_()\n    cams_val = []\n    residuals_val = []\n    LOGGER.info(\"Preparing validation data...\")\n    for cam_idx, (r_vecs, t_vecs) in enumerate(\n        zip(r_vecs_obj_to_cam, t_vecs_obj_to_cam)\n    ):\n        RT = (\n            torch.from_numpy(\n                np.hstack((cv2.Rodrigues(r_vecs)[0], t_vecs)),\n            )\n            .to(torch.float32)\n            .to(dev)\n        ).requires_grad_()\n        RTs_val.append(RT)\n        cams_val.append(Camera(cam_info[\"resolution_wh\"], K, lens_net, RT))\n        proj_opencv = cv2.projectPoints(\n            board_coords_val[cam_idx],\n            r_vecs_obj_to_cam[cam_idx],\n            t_vecs_obj_to_cam[cam_idx],\n            camera_matrix,\n            dist_coeffs,\n        )[0][:, 0, :]\n        residuals_val.append(\n            np.square(np.linalg.norm(frame_coords_val[cam_idx] - proj_opencv, axis=1))\n        )\n\n    RMSE_opencv_val = np.sqrt(np.mean(np.concatenate(residuals_val, axis=0)))\n    LOGGER.info(f\"RMSE OpenCV: {RMSE_opencv_val}\")\n    LOGGER.info(\"Assembling training set...\")\n    RTs_train = []\n    cams_train = []\n    board_coords_train = []\n    frame_coords_train = []\n    n_points_train = 0\n    n_frames_train = 0\n    for idx, (im_board_coords, im_frame_coords) in enumerate(\n        zip(cam_info[\"board_coordinates_xyz\"], cam_info[\"frame_coordinates_xy\"])\n    ):\n        if idx % subs_fac != 0 and len(im_board_coords) >= 6:\n            board_coords_train.append(np.array(im_board_coords, dtype=np.float32))\n            frame_coords_train.append(np.array(im_frame_coords, dtype=np.float32))\n            n_points_train += frame_coords_train[-1].shape[0]\n            n_frames_train += 1\n            _, r_vecs, t_vecs = cv2.solvePnP(\n                board_coords_train[-1],\n                frame_coords_train[-1],\n                camera_matrix,\n                dist_coeffs,\n            )\n            RT = (\n                torch.from_numpy(\n                    np.hstack((cv2.Rodrigues(r_vecs)[0], t_vecs)),\n                )\n                .to(torch.float32)\n                .to(dev)\n            ).requires_grad_()\n            RTs_train.append(RT)\n            cams_train.append(Camera(cam_info[\"resolution_wh\"], K, lens_net, RT))\n            board_coords_train[-1] = (\n                torch.from_numpy(board_coords_train[-1]).to(torch.float32).to(dev)\n            )\n            frame_coords_train[-1] = (\n                torch.from_numpy(frame_coords_train[-1]).to(torch.float32).to(dev)\n            )\n    LOGGER.info(\n        f\"Using {n_points_train} points for training from {n_frames_train} frames.\"\n    )\n\n    LOGGER.info(\"Setting up optimizer...\")\n    scale = torch.nn.Parameter(torch.tensor(1.305)).requires_grad_()\n    optimizer_train = torch.optim.Adam(\n        [\n            K,\n        ]\n        + list(lens_net.parameters())\n        + RTs_train\n        + [scale],\n        lr=1e-4,\n    )\n    optimizer_val_RT = torch.optim.Adam(\n        RTs_val,\n        lr=1e-4,\n    )\n\n    if is_eval:\n        PATH = log_dir + \"/lensnet_latest.pt\"\n        checkpoint = torch.load(PATH)\n        lens_net.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer_train.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        RTs_train = checkpoint[\"RTs_train\"]\n        RTs_val = checkpoint[\"RTs_train\"]\n        K = checkpoint[\"K\"]\n\n        loss_vals = []\n        rows, cols = 3, 4\n        gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n        fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 9))\n        bleed = 0\n        fig.subplots_adjust(\n            left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n        )\n        for cam_idx, (cam, board_coord_mat, frame_coord_mat, ax) in enumerate(\n            zip(cams_val, board_coords_val, frame_coords_val, axarr.ravel())\n        ):\n            board_coord_mat = (\n                torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n            )\n            frame_coord_mat = (\n                torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n            )\n            projected = cam.project_points(board_coord_mat)\n            val_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )  # RMSE\n            loss_vals.append(val_error.detach().clone())\n            # visualize keypoints\n            vis, camera_directions_w_lens = vis_lens(cam)\n            # ax.imshow(vis[..., :3])\n            ax.scatter(\n                projected.detach().cpu().numpy()[:, 0],\n                projected.detach().cpu().numpy()[:, 1],\n                marker=\"o\",\n                s=5,\n            )\n            ax.scatter(\n                frame_coord_mat.detach().cpu().numpy()[:, 0],\n                frame_coord_mat.detach().cpu().numpy()[:, 1],\n                marker=\"o\",\n                s=5,\n            )\n            ax.set_xlim((0, 512))\n            ax.set_ylim((0, 512))\n            ax.set_axis_off()\n\n        fig.savefig(log_dir + \"/vis_eval.png\")\n        plt.close(fig)\n\n        rows, cols = 1, 4\n        gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n        fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 3))\n        bleed = 0\n        fig.subplots_adjust(\n            left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n        )\n\n        vis, camera_directions_w_lens = vis_lens(cams_val[0])\n        target_board_ABC = (\n            imageio.imread(path.join(storage_fp, \"target_rainbow_ABC.png\"))[..., :3]\n            / 255.0\n        )\n        target_board_path = path.join(storage_fp, \"target_rainbow.png\")\n        target_board = imageio.imread(target_board_path)[..., :3]\n        transform = transforms.ToTensor()\n        image = transform(target_board).unsqueeze(0).to(dev)\n        flow = camera_directions_w_lens.unsqueeze(0).to(dev) * 1.305\n        output = F.grid_sample(\n            image, flow, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n        )\n        output_img = output[0].permute(1, 2, 0).cpu().numpy()\n        axarr[0].imshow(target_board)\n        axarr[1].imshow(output_img)\n        axarr[2].imshow(target_board_ABC)\n\n        def rgb2gray(rgb):\n            return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\n        emap = abs(target_board_ABC - output_img)\n        axarr[3].imshow(rgb2gray(emap), cmap=plt.get_cmap(\"gray\"))\n\n        for ax in axarr:\n            ax.set_axis_off()\n\n        fig.savefig(log_dir + \"/img_eval.png\")\n        plt.close(fig)\n\n        rmse = torch.sqrt(torch.cat(loss_vals, dim=0).mean())\n        print(\"rmse lensnet: \", rmse.item())\n\n\n    scheduler_type = \"super\"\n    if scheduler_type == \"linear\":\n        step_size = 20000  # 15000\n        final_ratio = 0.01  # 0.05\n        start_ratio = 0.15\n        duration_ratio = 0.6\n\n        def lambda_rule(ep):\n            lr_l = 1.0 - min(\n                1,\n                max(0, ep - start_ratio * step_size)\n                / float(duration_ratio * step_size),\n            ) * (1 - final_ratio)\n            return lr_l\n\n        scheduler = torch.optim.lr_scheduler.LambdaLR(\n            optimizer_train, lr_lambda=lambda_rule\n        )\n        LOGGER.info(\n            f\"LR scheduler has step size {step_size}, final ratio {final_ratio}, start ratio {start_ratio}, duration ratio {duration_ratio}.\"\n        )\n    elif scheduler_type == \"super\":\n        step_size = 12000\n        max_lr = 1e-4\n        pct_start = 0.05\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer_train,\n            max_lr=max_lr,\n            total_steps=step_size,\n            pct_start=pct_start,\n        )\n        LOGGER.info(\n            f\"LR scheduler has step size {step_size}, max lr {max_lr}, pct_start {pct_start}\"\n        )\n\n    LOGGER.info(f\"Lens has {sum(p.numel() for p in lens_net.parameters())} parameters.\")\n    writer = SummaryWriter(log_dir=log_dir)\n    LOGGER.info(f\"Writing logs and tensorboard to `{log_dir}`.\")\n    LOGGER.info(\"Optimizing...\")\n    start_epoch = 0\n    if is_eval:\n        start_epoch = checkpoint[\"epoch\"]\n    rnge = tqdm.trange(start_epoch, 500)\n\n\n    # load data\n    dense_matching = False\n    if dense_matching:\n        target_board_ABC = (\n            imageio.imread(path.join(storage_fp, \"target.png\"))[..., :3]\n            / 255.0\n        )\n        target_board_path = path.join(storage_fp, \"target.png\")\n        target_board = imageio.imread(target_board_path)[..., :3]\n        transform = transforms.ToTensor()\n        image = transform(target_board).unsqueeze(0).to(dev)\n        image_ABC = transform(target_board_ABC).unsqueeze(0).to(dev)\n        l1_loss = torch.nn.L1Loss()\n\n    for epoch_idx in rnge:\n        loss_vals = []\n        loss_vals_train = []\n        loss_train = []\n\n        for _, (cam, board_coord_mat, frame_coord_mat) in tqdm.tqdm(\n            enumerate(zip(cams_train, board_coords_train, frame_coords_train)),\n            total=len(cams_train),\n        ):\n            optimizer_train.zero_grad()\n            projected = cam.project_points(board_coord_mat)\n            loss = torch.linalg.norm(projected - frame_coord_mat, dim=1).mean()\n            loss_vals_train.append(loss.item())\n\n            proj_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )\n            loss_train.append(proj_error)\n\n            # dense match loss\n            if dense_matching:\n                camera_directions_w_lens = cam.project_lens()\n                flow = camera_directions_w_lens.unsqueeze(0) * scale\n                output = F.grid_sample(\n                    image[..., ::2, ::2],\n                    flow,\n                    mode=\"bilinear\",\n                    padding_mode=\"zeros\",\n                    align_corners=False,\n                )\n                dense_loss = l1_loss(image_ABC[..., ::2, ::2], output)\n\n                loss += dense_loss * 50\n\n            loss.backward()\n            optimizer_train.step()\n            scheduler.step()\n\n        avg_loss = np.array(loss_vals_train).mean()\n        if dense_matching:\n            writer.add_scalar(\"dense_loss\", dense_loss, epoch_idx)\n        writer.add_scalar(\"reproj_loss\", avg_loss, epoch_idx)\n\n        for cam_idx, (cam, board_coord_mat, frame_coord_mat) in enumerate(\n            zip(cams_val, board_coords_val, frame_coords_val)\n        ):\n            optimizer_val_RT.zero_grad()\n            board_coord_mat = (\n                torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n            )\n            frame_coord_mat = (\n                torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n            )\n            projected = cam.project_points(board_coord_mat)\n            val_error = torch.square(\n                torch.linalg.norm(projected - frame_coord_mat, dim=1)\n            )  # RMSE\n            loss_vals.append(val_error.detach().clone())\n  \n            loss = torch.linalg.norm(projected - frame_coord_mat, dim=1).mean()\n\n            # dense match loss\n            if dense_matching:\n                camera_directions_w_lens = cam.project_lens()\n                flow = camera_directions_w_lens.unsqueeze(0) * scale\n                output = F.grid_sample(\n                    image[..., ::2, ::2],\n                    flow,\n                    mode=\"bilinear\",\n                    padding_mode=\"zeros\",\n                    align_corners=False,\n                )\n                dense_loss = l1_loss(image_ABC[..., ::2, ::2], output)\n\n                loss += dense_loss * 50\n\n            loss.backward()\n            optimizer_val_RT.step()\n\n            if epoch_idx % 10 == 0 and cam_idx % 100 == 0 and dense_matching:\n\n                def rgb2gray(rgb):\n                    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\n                rows, cols = 1, 4\n                gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n                fig, axarr = plt.subplots(\n                    rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 3)\n                )\n                bleed = 0\n                fig.subplots_adjust(\n                    left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n                )\n\n                axarr[0].imshow(target_board)\n                output_img = output[0].permute(1, 2, 0).detach().cpu().numpy()\n                axarr[1].imshow(output_img)\n                axarr[2].imshow(target_board_ABC[::2, ::2, :])\n                emap = abs(target_board_ABC[::2, ::2, :] - output_img)\n                axarr[3].imshow(rgb2gray(emap), cmap=plt.get_cmap(\"gray\"))\n                for ax in axarr:\n                    ax.set_axis_off()\n                fig.savefig(log_dir + f\"/dense_match_{epoch_idx // 10}.png\")\n\n                plt.close(fig)\n\n        if epoch_idx % 10 == 0:\n            # visualize keypoints\n\n            rows, cols = 3, 4\n            gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0}\n            fig, axarr = plt.subplots(\n                rows, cols, gridspec_kw=gridspec_kw, figsize=(12, 9)\n            )\n            bleed = 0\n            fig.subplots_adjust(\n                left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed)\n            )\n            for cam_idx, (cam, board_coord_mat, frame_coord_mat, ax) in enumerate(\n                zip(cams_val, board_coords_val, frame_coords_val, axarr.ravel())\n            ):\n                board_coord_mat = (\n                    torch.from_numpy(board_coord_mat).to(torch.float32).to(dev)\n                )\n                frame_coord_mat = (\n                    torch.from_numpy(frame_coord_mat).to(torch.float32).to(dev)\n                )\n                projected = cam.project_points(board_coord_mat)\n                # vis, _ = vis_lens(cam)\n                # ax.imshow(vis[..., :3])\n                ax.scatter(\n                    frame_coord_mat.detach().cpu().numpy()[:, 0],\n                    frame_coord_mat.detach().cpu().numpy()[:, 1],\n                    marker=\"o\",\n                    s=5,\n                )\n                ax.scatter(\n                    projected.detach().cpu().numpy()[:, 0],\n                    projected.detach().cpu().numpy()[:, 1],\n                    marker=\"o\",\n                    s=5,\n                )\n                ax.set_axis_off()\n                ax.grid(False)\n                plt.tight_layout()\n            fig.savefig(log_dir + f\"/vis_{epoch_idx // 10}.png\")\n            vis, _ = vis_lens(cam)\n            writer.add_image(\n                \"vis/lens\", vis, global_step=epoch_idx, walltime=None, dataformats=\"HWC\"\n            )\n            plt.close(fig)\n\n        train_rmse = torch.sqrt(torch.cat(loss_train, dim=0).mean())\n        writer.add_scalar(\"train_rmse\", train_rmse.detach(), epoch_idx)\n        rmse = torch.sqrt(torch.cat(loss_vals, dim=0).mean())\n        writer.add_scalar(\"val_rmse\", rmse, epoch_idx)\n        lr = optimizer_train.param_groups[0][\"lr\"]\n        writer.add_scalar(\"lr\", lr, epoch_idx)\n        print(f\"RMSE (train): {train_rmse}, RMSE (val): {rmse}, lr: {lr}\")\n        if dense_matching:\n            print(\n                f\"Epoch {epoch_idx}, reproj loss: {avg_loss}, dense loss: {dense_loss}\"\n            )\n        else:\n            print(f\"Epoch {epoch_idx}, reproj loss: {avg_loss}\")\n\n        if epoch_idx > 0 and epoch_idx % 25 == 0 and not is_eval:\n            PATH = log_dir + \"/lensnet_latest.pt\"\n            torch.save(\n                {\n                    \"epoch\": epoch_idx,\n                    \"model_state_dict\": lens_net.state_dict(),\n                    \"optimizer_state_dict\": optimizer_train.state_dict(),\n                    \"K\": K,\n                    \"RTs_train\": RTs_train,\n                    \"RTs_val\": RTs_val,\n                    \"loss\": loss,\n                },\n                PATH,\n            )", "\n\ndef colorize(uv_im, max_mag=None):\n    hsv = np.zeros((uv_im.shape[0], uv_im.shape[1], 3), dtype=np.uint8)\n    hsv[..., 1] = 255\n    mag, ang = cv2.cartToPolar(uv_im[..., 0], uv_im[..., 1])\n    hsv[..., 0] = ang * 180 / np.pi / 2\n    # print(mag.max())\n    if max_mag is None:\n        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n    else:\n        mag = np.clip(mag, 0.0, max_mag)\n        mag = mag / max_mag * 255.0\n        hsv[..., 2] = mag.astype(np.uint8)\n    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    return rgb", "\ndef image_grid_vis(\n    coords_x,\n    coords_y,\n    rows=None,\n    cols=None,\n    fill: bool = True,\n    show_axes: bool = False,\n    rgb: bool = True,\n):\n    \"\"\"\n    A util function for plotting a grid of images.\n\n    Args:\n        images: (N, H, W, 4) array of RGBA images\n        rows: number of rows in the grid\n        cols: number of columns in the grid\n        fill: boolean indicating if the space between images should be filled\n        show_axes: boolean indicating if the axes of the plots should be visible\n        rgb: boolean, If True, only RGB channels are plotted.\n            If False, only the alpha channel is plotted.\n\n    Returns:\n        None\n    \"\"\"\n    if (rows is None) != (cols is None):\n        raise ValueError(\"Specify either both rows and cols or neither.\")\n\n    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n    bleed = 0\n    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n\n    for ax, coord_x, coord_y in zip(axarr.ravel(), coords_x, coords_y):\n        ax.scatter(coord_x, coord_y, marker=\"o\")\n        if not show_axes:\n            ax.set_axis_off()\n    plt.close(fig)", "\n\ndef project_lens(camera: Camera):\n    i, j = np.meshgrid(\n        np.linspace(0, camera.resolution_w_h[0] - 1, camera.resolution_w_h[0]),\n        np.linspace(0, camera.resolution_w_h[1] - 1, camera.resolution_w_h[1]),\n        indexing=\"ij\",\n    )\n    i = i.T\n    j = j.T\n    P_sensor = (\n        torch.from_numpy(np.stack((i, j), axis=-1))\n        .to(torch.float32)\n        .to(camera.K.device)\n    )\n    # camera_directions_w_lens = batched_func(\n    #     camera.get_rays_view, P_sensor.reshape((-1, 2)), 10000\n    # )\n    camera_directions_w_lens = camera.get_rays_view(P_sensor.reshape((-1, 2)))\n    camera_directions_w_lens = camera_directions_w_lens.reshape(\n        (P_sensor.shape[0], P_sensor.shape[1], 3)\n    )[:, :, :2]\n\n    return camera_directions_w_lens", "\n\ndef vis_lens(camera: Camera):\n    i, j = np.meshgrid(\n        np.linspace(0, camera.resolution_w_h[0] - 1, camera.resolution_w_h[0]),\n        np.linspace(0, camera.resolution_w_h[1] - 1, camera.resolution_w_h[1]),\n        indexing=\"ij\",\n    )\n    i = i.T\n    j = j.T\n    P_sensor = (\n        torch.from_numpy(np.stack((i, j), axis=-1))\n        .to(torch.float32)\n        .to(camera.K.device)\n    )\n    with torch.no_grad():\n        camera_directions_w_lens = batched_func(\n            camera.get_rays_view, P_sensor.reshape((-1, 2)), 10000\n        )\n        camera_directions_w_lens = camera_directions_w_lens.reshape(\n            (P_sensor.shape[0], P_sensor.shape[1], 3)\n        )[:, :, :2]\n\n        camera_no_lens = Camera(camera.resolution_w_h, camera.K)\n        camera_directions_wo_lens = batched_func(\n            camera_no_lens.get_rays_view, P_sensor.reshape((-1, 2)), 10000\n        ).reshape((P_sensor.shape[0], P_sensor.shape[1], 3))[:, :, :2]\n        direction_diff = camera_directions_w_lens - camera_directions_wo_lens\n        flow_color = colorize(\n            direction_diff.detach().cpu().numpy(),\n            max_mag=0.1,\n        )\n      \n    return flow_color, camera_directions_w_lens", "\n\nif __name__ == \"__main__\":\n    cli()\n"]}
{"filename": "calibration/scripts/project.py", "chunked_list": ["import torch\nfrom ..camera import Camera\nfrom ..networks import LensNet\n\n\ndef cli():\n    RT = torch.eye(3, 4)\n    RT[0, 3] = 5.0\n    cam = Camera(torch.eye(3), lensnet=LensNet(), RT=RT)\n    # import pdb\n    # pdb.set_trace()\n    pt = torch.tensor([[1.0, 4.0, 5.0]], dtype=torch.float32)\n    proj = cam.project_points(pt)\n    print(f\"proj: {proj}\")\n    unproj = cam.unproject_points(torch.tensor([[proj[0, 0], proj[0, 1], 5.0]]))\n    print(f\"unproj: {unproj}\")\n    ray = cam.get_rays_view(torch.tensor([[0.2, 0.8]]))\n    print(f\"ray: {ray}\")", ""]}
{"filename": "calibration/scripts/optimize_marker.py", "chunked_list": ["from os import path\nfrom datetime import timedelta\nimport logging\nimport hydra\nfrom torch.utils.data import Dataset, DataLoader\nfrom hydra.utils import instantiate\nfrom calibration.config import Config\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport numpy as np", "from pytorch_lightning.callbacks import ModelCheckpoint\nimport numpy as np\n\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\nclass EmptyDataset(Dataset):\n    \"\"\"\n    Dummy dataset object.\n\n    Since the training data is generated on the fly, no\n    actual data loader is required. This dataset type\n    makes this training paradigm compatible with PyTorch\n    lightning.\n    \"\"\"\n\n    def __init__(self, size):\n        self.size = size\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return []", "\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Optimize the marker appearance and keypoint detector.\n\n    See README.md for further information. Example call:\n    `pdm run optimize_marker -- exp_name=[your experiment name]`.\n    \"\"\"\n    LOGGER.info(\n        f\"Optimizing marker of size {cfg.model.marker.size}x{cfg.model.marker.size}.\"\n    )\n    LOGGER.info(\"Instantiating model...\")\n    model = instantiate(cfg.model, _recursive_=False)\n    log_dir = path.join(\n        path.abspath(path.dirname(__file__)), \"..\", \"..\", \"experiments\", cfg.exp_name\n    )\n    LOGGER.info(f\"Writing logs and tensorboard to `{log_dir}`.\")\n    train_loader = DataLoader(\n        EmptyDataset(cfg.model.batch_size * cfg.trainer.max_steps),\n        batch_size=cfg.model.batch_size,\n    )\n    trainer = instantiate(\n        cfg.trainer,\n        default_root_dir=log_dir,\n        callbacks=[ModelCheckpoint(train_time_interval=timedelta(minutes=10.0))],\n    )\n    trainer.fit(model, train_dataloaders=train_loader)\n    return np.mean(model.loss_deque)", "\n\nif __name__ == \"__main__\":\n    cli()\n"]}
{"filename": "calibration/scripts/dbg_process_frame.py", "chunked_list": ["import cv2\nimport logging\nfrom os import path\nimport hydra\nimport imageio\nfrom ..config import Config, ARUCO_DICT\nfrom ..keypoint_detection import process_frame\n\n\nLOGGER = logging.getLogger(__name__)", "\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Process a single frame for debugging purposes.\n\n    Uses `tmp.png` in the project folder. Enable debugging visualizations\n    by adding `dbg=true`. Example call:\n    `pdm run dbg_process_frame -- target.exp_name=[your experiment name] dbg=true`.\n    \"\"\"\n    frame = imageio.imread(path.join(path.dirname(__file__), \"..\", \"..\", \"tmp.png\"))[\n        :, :, ::-1\n    ]\n    process_frame(frame, cfg)", ""]}
{"filename": "calibration/scripts/assemble_pattern.py", "chunked_list": ["import logging\nimport numpy as np\nfrom os import path\nfrom calibration.target import calculate_parameters\n\nimport hydra\nimport cv2\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.utils import ImageReader as RLImageReader\nimport imageio", "from reportlab.lib.utils import ImageReader as RLImageReader\nimport imageio\nimport tempfile\n\nfrom ..model import Model\nfrom ..config import Config, ARUCO_DICT, get_latest_checkpoint\n\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n", "CONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Assemble a calibration pattern using an optimized marker.\n\n    For more information, please see README.md. Example run:\n    `pdm run assemble_pattern -- target.exp_name=[your experiment name]`.\n    \"\"\"\n    LOGGER.info(\n        f\"Assembling calibration target with section from experiment `{cfg.target.exp_name}`.\"\n    )\n    latest_checkpoint = get_latest_checkpoint(cfg.target.exp_name, cfg)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        LOGGER.info(f\"Loading optimized marker from `{latest_checkpoint}`...\")\n        model = Model.load_from_checkpoint(latest_checkpoint)\n        marker, center = model.marker(1)\n        marker = (marker[0].cpu().detach().numpy().transpose(1, 2, 0) * 255.0).astype(\n            np.uint8\n        )\n        assert center[0, 0] == marker.shape[1] // 2\n        assert center[0, 1] == marker.shape[0] // 2\n        imageio.imsave(path.join(tmp_dir, \"marker.png\"), marker)\n        marker_im = RLImageReader(path.join(tmp_dir, \"marker.png\"))\n        if cfg.dbg:\n            cv2.namedWindow(\"[dbg] Optimized Marker\")\n            cv2.imshow(\"[dbg] Optimized Marker\", marker)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] Optimized Marker\")\n        LOGGER.info(f\"Loading ArUco marker with ID `{cfg.target.aruco_id}`...\")\n        ar_markers = [\n            cv2.aruco.drawMarker(\n                cv2.aruco.getPredefinedDictionary(ARUCO_DICT[cfg.target.aruco_id]),\n                idx,\n                200,\n            )\n            for idx in range(23, 27)\n        ]\n        for marker_idx, ar_marker in enumerate(ar_markers):\n            imageio.imsave(\n                path.join(tmp_dir, \"ar_marker_%d.png\" % (marker_idx)), ar_marker\n            )\n        ar_marker_ims = [\n            RLImageReader(path.join(tmp_dir, \"ar_marker_%d.png\" % (idx)))\n            for idx in range(4)\n        ]\n        if cfg.dbg:\n            cv2.namedWindow(\"[dbg] ArUco Marker\")\n            cv2.imshow(\"[dbg] ArUco Marker\", ar_marker)\n            cv2.waitKey(0)\n            cv2.destroyWindow(\"[dbg] ArUco Marker\")\n        pdf_path = path.abspath(\n            path.join(\n                path.dirname(__file__),\n                \"..\",\n                \"..\",\n                \"experiments\",\n                cfg.target.exp_name,\n                \"target.pdf\",\n            )\n        )\n        LOGGER.info(f\"Generating PDF at `{pdf_path}`...\")\n        canv = canvas.Canvas(pdf_path, pagesize=cfg.target.page_size_pt)\n        params = calculate_parameters(cfg)\n        LOGGER.info(\n            f\"Squares printed: {params.n_squares_x} X {params.n_squares_y} (x X y).\"\n        )\n        LOGGER.info(\"Drawing markers...\")\n        for x in range(0, params.n_squares_x):\n            for y in range(0, params.n_squares_y):\n                corner_coord_x = params.pattern_start_x_pt + x * params.square_length_pt\n                corner_coord_y = params.pattern_start_y_pt + y * params.square_length_pt\n                canv.drawImage(\n                    marker_im,\n                    corner_coord_x,\n                    corner_coord_y,\n                    width=params.square_length_pt,\n                    height=params.square_length_pt,\n                )\n        LOGGER.info(\"Drawing ArUco tags...\")\n        for marker_idx, marker_im in enumerate(ar_marker_ims):\n            tag_width_px = ar_marker.shape[1]\n            tag_height_px = ar_marker.shape[0]\n            assert tag_width_px == tag_height_px, \"Non-square markers not supported!\"\n            canv.drawImage(\n                marker_im,\n                params.tag_start_x_pt[marker_idx],\n                params.tag_start_y_pt[marker_idx],\n                width=params.tag_square_length_pt,\n                height=params.tag_square_length_pt,\n            )\n        canv.showPage()\n        canv.save()\n        LOGGER.info(\"Done.\")\n        import time\n\n        time.sleep(2)", "\n\nif __name__ == \"__main__\":\n    cli()\n"]}
{"filename": "calibration/scripts/hyperopt.py", "chunked_list": ["import ray\nimport time\nimport logging\nimport coloredlogs\nfrom ray import tune\nfrom ray.tune.schedulers import AsyncHyperBandScheduler\nimport click\nfrom hydra.experimental import initialize, compose\nfrom .optimize_marker import cli as opcli\n", "from .optimize_marker import cli as opcli\n\n\nLOGGER = logging.getLogger(__name__)\n\n\n@click.command()\n@click.option(\n    \"--smoke\",\n    type=click.BOOL,", "    \"--smoke\",\n    type=click.BOOL,\n    is_flag=True,\n    default=False,\n    help=\"Finish quickly for testing.\",\n)\n@click.option(\n    \"--max_epochs\",\n    type=click.INT,\n    default=10000,", "    type=click.INT,\n    default=10000,\n    help=\"Maximum number of steps per experiment.\",\n)\ndef cli(smoke=False, max_epochs=10000):\n    \"\"\"\n    Optimize hyperparameters for pattern and detector optimization.\n\n    Run as `pdm run hyperopt`.\n    \"\"\"\n    LOGGER.info(\"Running hyperparameter search.\")\n    ray.init(num_gpus=1)\n    LOGGER.info(\"Creating HyperBand scheduler...\")\n    scheduler = AsyncHyperBandScheduler(\n        time_attr=\"training_iteration\",\n        max_t=max_epochs,\n        grace_period=1000,\n    )\n    # 'training_iteration' is incremented every time `trainable.step` is called\n    stopping_criteria = {\"training_iteration\": 10 if smoke else max_epochs}\n    LOGGER.info(\"Stopping criteria: %s.\", stopping_criteria)\n    LOGGER.info(\"Starting search...\")\n    analysis = tune.run(\n        optimize_pattern_objective,\n        name=\"optimize_pattern_asha\",\n        scheduler=scheduler,\n        metric=\"accuracy\",\n        mode=\"min\",\n        stop=stopping_criteria,\n        num_samples=20,\n        verbose=1,\n        resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n        config={  # Hyperparameter space\n            \"model.lr\": tune.loguniform(1e-4, 1e-1),\n            \"model.lr_fcn_fac\": tune.loguniform(1e-2, 1e2),\n            \"model.lr_marker_fac\": tune.loguniform(1e0, 1e4),\n            \"model.n_latent\": tune.randint(20, 400),\n            \"model.n_hidden\": tune.randint(1, 3),\n        },\n    )\n    LOGGER.info(\"Best hyperparameters found were: \", analysis.best_config)", "\n\ndef optimize_pattern_objective(config):\n    overrides = [f\"{key}={val}\" for key, val in config.items()]\n    overrides.append(f\"exp_name=hyperopt_{time.time()}\")\n    overrides.append(f\"trainer.max_steps=10000\")\n    with initialize(config_path=\"../../conf\"):\n        cfg = compose(config_name=\"calibration_config\", overrides=overrides)\n    opcli(cfg)\n", "\n\nif __name__ == \"__main__\":\n    coloredlogs.install(level=logging.INFO)\n    cli()\n"]}
{"filename": "calibration/scripts/record.py", "chunked_list": ["import sys\nimport cv2\nimport numpy as np\nimport logging\nimport os\nfrom os import path\nfrom calibration.persistence import NumpyEncoder\nimport hydra\nimport imageio\nimport json", "import imageio\nimport json\nimport shutil\nimport click\nfrom ..persistence import NumpyEncoder\nfrom ..config import Config\nfrom ..keypoint_detection import process_frame\n\n\nLOGGER = logging.getLogger(__name__)", "\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    Run target detection live or for a video and store the results in `data/live`.\n\n    By default uses the video camera 0. You can also specify a pre-recorded video\n    by using the parameter `video_fp=[path to your.mp4]`. Example call:\n    `pdm run record -- target.exp_name=[your experiment name] [video_fp=[path to .mp4]]`.\n    Add `vis=true` to see a live visualization of the detections.\n    \"\"\"\n    LOGGER.info(\n        f\"Starting live detection for board with marker `{cfg.target.aruco_id}`...\"\n    )\n    storage_fp = path.abspath(path.join(CONF_FP, \"..\", \"..\", \"data\", \"live\"))\n    if path.exists(storage_fp):\n        LOGGER.info(f\"Data collection path exists ({storage_fp}).\")\n        if click.confirm(\"Do you want to delete it to collect new data?\"):\n            shutil.rmtree(storage_fp)\n        else:\n            LOGGER.error(\"Can't continue. Aborting...\")\n            sys.exit(1)\n    os.makedirs(storage_fp)\n    LOGGER.info(\"Starting live view...\")\n    if cfg.video_fp is not None:\n        cap = cv2.VideoCapture(path.abspath(\"../../../testcap.mp4\"))\n    else:\n        cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        raise Exception(\"Cannot open camera!\")\n    if cfg.vis:\n        LOGGER.info(\"Starting witness view...\")\n        cv2.namedWindow(\"coverage\")\n    coverage_vis = None\n    LOGGER.info(\"Live view running!\")\n    frame_coords = []\n    board_coords = []\n    # TODO: make robust w.r.t. inaccurate marker detections / improve marker accuracy.\n    # TODO: reject detections where the center is too far from expected.\n    while True:\n        ret, frame = cap.read()\n        if coverage_vis is None:\n            coverage_vis = np.zeros_like(frame)\n        if not ret:\n            LOGGER.error(\"Can't read frame (stream end?). Exiting ...\")\n            break\n        frame_orig = frame.copy()\n        try:\n            (\n                keypoint_coords_xy,\n                keypoints_valid,\n                keypoint_board_coords,\n                vis_frame,\n            ) = process_frame(frame, cfg)\n        except Exception as ex:\n            LOGGER.error(ex)\n            continue\n        this_frame_coords = []\n        this_board_coords = []\n        resize_fac = 3\n        frame = cv2.resize(\n            frame, None, fx=resize_fac, fy=resize_fac, interpolation=cv2.INTER_LINEAR\n        )\n        for center_coord, valid, board_coord in zip(\n            keypoint_coords_xy, keypoints_valid, keypoint_board_coords\n        ):\n            clr = (0, 255, 0) if valid else (0, 0, 255)\n            frame[\n                int(center_coord[1] * resize_fac)\n                - 3 : int(center_coord[1] * resize_fac)\n                + 3,\n                int(center_coord[0] * resize_fac)\n                - 3 : int(center_coord[0] * resize_fac)\n                + 3,\n                :,\n            ] = clr\n            text = str(\"(%d, %d)\" % (int(board_coord[0]), int(board_coord[1])))\n            text_width, text_height = cv2.getTextSize(\n                text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1\n            )[0]\n            cv2.putText(\n                frame,\n                text,\n                (\n                    int(center_coord[0] * resize_fac),\n                    int(center_coord[1] * resize_fac) - text_height,\n                ),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.3,\n                (0, 255, 0),\n                1,\n            )\n            if valid:\n                coverage_vis[int(center_coord[1]), int(center_coord[0]), 1] = min(\n                    255,\n                    50 + coverage_vis[int(center_coord[1]), int(center_coord[0]), 1],\n                )\n                this_frame_coords.append(center_coord)\n                this_board_coords.append(board_coord)\n        if len(this_frame_coords) > 0:\n            frame_coords.append(this_frame_coords)\n            board_coords.append(this_board_coords)\n            cv2.imwrite(\n                path.join(storage_fp, \"%05d.png\" % (len(board_coords) - 1)), frame_orig\n            )\n            cv2.imwrite(\n                path.join(storage_fp, \"vis-%05d.png\" % (len(board_coords) - 1)),\n                vis_frame,\n            )\n        # Display the resulting frame and update coverage.\n        if cfg.vis:\n            cv2.imshow(\"frame\", frame)\n            cv2.imshow(\"coverage\", coverage_vis)\n            key_code = cv2.waitKey(1)\n            if key_code == ord(\"q\"):\n                break\n            elif key_code == ord(\"s\"):\n                imageio.imsave(\n                    path.join(path.dirname(__file__), \"..\", \"..\", \"tmp.png\"),\n                    frame_orig[:, :, ::-1],\n                )\n    LOGGER.info(\"Shutting down...\")\n    cap.release()\n    if cfg.vis:\n        cv2.destroyAllWindows()\n    LOGGER.info(\"Writing results...\")\n    imageio.imwrite(path.join(storage_fp, \"coverage.png\"), coverage_vis)\n    with open(path.join(storage_fp, \"points.json\"), \"w\") as outf:\n        json.dump(\n            {\n                \"frame_coordinates_xy\": frame_coords,\n                \"board_coordinates_xyz\": board_coords,\n                \"resolution_wh\": (coverage_vis.shape[1], coverage_vis.shape[0]),\n            },\n            outf,\n            cls=NumpyEncoder,\n            indent=4,\n            sort_keys=False,\n        )", ""]}
{"filename": "calibration/scripts/dbg_live_detect.py", "chunked_list": ["import cv2\nimport logging\nfrom os import path\nimport hydra\nimport imageio\nfrom ..config import Config, ARUCO_DICT\nfrom ..keypoint_detection import process_frame\n\n\nLOGGER = logging.getLogger(__name__)", "\nLOGGER = logging.getLogger(__name__)\nCONF_FP: str = path.join(\"..\", \"..\", \"conf\")\n\n\n@hydra.main(config_path=CONF_FP, config_name=\"calibration_config\")\ndef cli(cfg: Config):\n    \"\"\"\n    [debugging] Script to detect a board live and show markers.\n\n    Press `s` to save a frame to `tmp.png` for later processing, press\n    `q` to exit. Example run:\n    `pdm run dbg_live_detect -- target.exp_name=[your experiment name]`.\n    You can also read a pre-recorded video using the option\n    `video_fp=[path to video]`.\n    \"\"\"\n    LOGGER.info(\n        f\"Starting live detection for board with marker `{cfg.target.aruco_id}`...\"\n    )\n    LOGGER.info(\"Loading marker...\")\n    arucoDict = cv2.aruco.Dictionary_get(ARUCO_DICT[cfg.target.aruco_id])\n    arucoParams = cv2.aruco.DetectorParameters_create()\n    arucoParams.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_APRILTAG\n    LOGGER.info(\"Starting live view...\")\n    if cfg.video_fp is not None:\n        print(path.abspath(cfg.video_fp))\n        cap = cv2.VideoCapture(path.abspath(cfg.video_fp))\n    else:\n        cap = cv2.VideoCapture(0)\n    if not cap.isOpened():\n        raise Exception(\"Cannot open camera!\")\n    LOGGER.info(\"Live view running!\")\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            raise Exception(\"Can't read frame (stream end?). Exiting ...\")\n        frame_orig = frame.copy()\n        (corners, ids, rejected) = cv2.aruco.detectMarkers(\n            frame, arucoDict, parameters=arucoParams\n        )\n        if len(corners) > 0:\n            keypoint_coords_xy, keypoints_valid, _, frame = process_frame(frame, cfg)\n        # Display the resulting frame\n        cv2.imshow(\"frame\", frame)\n        key_code = cv2.waitKey(1)\n        if key_code == ord(\"q\"):\n            break\n        elif key_code == ord(\"s\"):\n            imageio.imsave(\n                path.join(path.dirname(__file__), \"..\", \"..\", \"tmp.png\"),\n                frame_orig[:, :, ::-1],\n            )\n    LOGGER.info(\"Shutting down...\")\n    cap.release()\n    cv2.destroyAllWindows()", ""]}
{"filename": "calibration/standard_models/mobilenetv3.py", "chunked_list": ["import warnings\nfrom functools import partial\nfrom typing import Any, Callable, List, Optional, Sequence\nfrom types import FunctionType\n\nimport torch\nfrom torch import nn, Tensor\n\ntry:\n    from torch.hub import load_state_dict_from_url  # noqa: 401\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url  # noqa: 401", "try:\n    from torch.hub import load_state_dict_from_url  # noqa: 401\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url  # noqa: 401\n\n__all__ = [\"MobileNetV3\", \"mobilenet_v3_large\", \"mobilenet_v3_small\"]\n\n\nmodel_urls = {\n    \"mobilenet_v3_large\": \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\",", "model_urls = {\n    \"mobilenet_v3_large\": \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\",\n    \"mobilenet_v3_small\": \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\",\n}\n\n\nclass ConvNormActivation(torch.nn.Sequential):\n    \"\"\"\n    Configurable block used for Convolution-Normalzation-Activation blocks.\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the Convolution-Normalzation-Activation block\n        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n        stride (int, optional): Stride of the convolution. Default: 1\n        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in wich case it will calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolutiuon layer. If ``None`` this layer wont be used. Default: ``torch.nn.BatchNorm2d``\n        activation_layer (Callable[..., torch.nn.Module], optinal): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n        dilation (int): Spacing between kernel elements. Default: 1\n        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n        bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        padding: Optional[int] = None,\n        groups: int = 1,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        dilation: int = 1,\n        inplace: Optional[bool] = True,\n        bias: Optional[bool] = None,\n    ) -> None:\n        if padding is None:\n            padding = (kernel_size - 1) // 2 * dilation\n        if bias is None:\n            bias = norm_layer is None\n        layers = [\n            torch.nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride,\n                padding,\n                dilation=dilation,\n                groups=groups,\n                bias=bias,\n            )\n        ]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if activation_layer is not None:\n            params = {} if inplace is None else {\"inplace\": inplace}\n            layers.append(activation_layer(**params))\n        super().__init__(*layers)\n        _log_api_usage_once(self)\n        self.out_channels = out_channels", "\n\nclass SElayer(torch.nn.Module):\n    \"\"\"\n    This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1).\n    Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in in eq. 3.\n    Args:\n        input_channels (int): Number of channels in the input image\n        squeeze_channels (int): Number of squeeze channels\n        activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU``\n        scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``\n    \"\"\"\n\n    def __init__(\n        self,\n        input_channels: int,\n        squeeze_channels: int,\n        activation: Callable[..., torch.nn.Module] = torch.nn.ReLU,\n        scale_activation: Callable[..., torch.nn.Module] = torch.nn.Sigmoid,\n    ) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n        self.fc1 = torch.nn.Conv2d(input_channels, squeeze_channels, 1)\n        self.fc2 = torch.nn.Conv2d(squeeze_channels, input_channels, 1)\n        self.activation = activation()\n        self.scale_activation = scale_activation()\n\n    def _scale(self, input: Tensor) -> Tensor:\n        scale = self.avgpool(input)\n        scale = self.fc1(scale)\n        scale = self.activation(scale)\n        scale = self.fc2(scale)\n        return self.scale_activation(scale)\n\n    def forward(self, input: Tensor) -> Tensor:\n        scale = self._scale(input)\n        return scale * input", "\n\ndef _log_api_usage_once(obj: Any) -> None:\n\n    \"\"\"\n    Logs API usage(module and name) within an organization.\n    In a large ecosystem, it's often useful to track the PyTorch and\n    TorchVision APIs usage. This API provides the similar functionality to the\n    logging module in the Python stdlib. It can be used for debugging purpose\n    to log which methods are used and by default it is inactive, unless the user\n    manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n    Please note it is triggered only once for the same API call within a process.\n    It does not collect any data from open-source users since it is no-op by default.\n    For more information, please refer to\n    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n    Args:\n        obj (class instance or method): an object to extract info from.\n    \"\"\"\n    if not obj.__module__.startswith(\"torchvision\"):\n        return\n    name = obj.__class__.__name__\n    if isinstance(obj, FunctionType):\n        name = obj.__name__\n    torch._C._log_api_usage_once(f\"{obj.__module__}.{name}\")", "\n\ndef _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v", "\n\nclass SqueezeExcitation(SElayer):\n    \"\"\"DEPRECATED\"\"\"\n\n    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n        super().__init__(\n            input_channels, squeeze_channels, scale_activation=nn.Hardsigmoid\n        )\n        self.relu = self.activation\n        delattr(self, \"activation\")\n        warnings.warn(\n            \"This SqueezeExcitation class is deprecated and will be removed in future versions. \"\n            \"Use torchvision.ops.misc.SqueezeExcitation instead.\",\n            FutureWarning,\n        )", "\n\nclass InvertedResidualConfig:\n    # Stores information listed at Tables 1 and 2 of the MobileNetV3 paper\n    def __init__(\n        self,\n        input_channels: int,\n        kernel: int,\n        expanded_channels: int,\n        out_channels: int,\n        use_se: bool,\n        activation: str,\n        stride: int,\n        dilation: int,\n        width_mult: float,\n    ):\n        self.input_channels = self.adjust_channels(input_channels, width_mult)\n        self.kernel = kernel\n        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n        self.out_channels = self.adjust_channels(out_channels, width_mult)\n        self.use_se = use_se\n        self.use_hs = activation == \"HS\"\n        self.stride = stride\n        self.dilation = dilation\n\n    @staticmethod\n    def adjust_channels(channels: int, width_mult: float):\n        return _make_divisible(channels * width_mult, 8)", "\n\nclass InvertedResidual(nn.Module):\n    # Implemented as described at section 5 of MobileNetV3 paper\n    def __init__(\n        self,\n        cnf: InvertedResidualConfig,\n        norm_layer: Callable[..., nn.Module],\n        se_layer: Callable[..., nn.Module] = partial(\n            SElayer, scale_activation=nn.Hardsigmoid\n        ),\n    ):\n        super().__init__()\n        if not (1 <= cnf.stride <= 2):\n            raise ValueError(\"illegal stride value\")\n\n        self.use_res_connect = (\n            cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n        )\n\n        layers: List[nn.Module] = []\n        activation_layer = nn.Hardswish if cnf.use_hs else nn.ReLU\n\n        # expand\n        if cnf.expanded_channels != cnf.input_channels:\n            layers.append(\n                ConvNormActivation(\n                    cnf.input_channels,\n                    cnf.expanded_channels,\n                    kernel_size=1,\n                    norm_layer=norm_layer,\n                    activation_layer=activation_layer,\n                )\n            )\n\n        # depthwise\n        stride = 1 if cnf.dilation > 1 else cnf.stride\n        layers.append(\n            ConvNormActivation(\n                cnf.expanded_channels,\n                cnf.expanded_channels,\n                kernel_size=cnf.kernel,\n                stride=stride,\n                dilation=cnf.dilation,\n                groups=cnf.expanded_channels,\n                norm_layer=norm_layer,\n                activation_layer=activation_layer,\n            )\n        )\n        if cnf.use_se:\n            squeeze_channels = _make_divisible(cnf.expanded_channels // 4, 8)\n            layers.append(se_layer(cnf.expanded_channels, squeeze_channels))\n\n        # project\n        layers.append(\n            ConvNormActivation(\n                cnf.expanded_channels,\n                cnf.out_channels,\n                kernel_size=1,\n                norm_layer=norm_layer,\n                activation_layer=None,\n            )\n        )\n\n        self.block = nn.Sequential(*layers)\n        self.out_channels = cnf.out_channels\n        self._is_cn = cnf.stride > 1\n\n    def forward(self, input: Tensor) -> Tensor:\n        result = self.block(input)\n        if self.use_res_connect:\n            result += input\n        return result", "\n\nclass MobileNetV3(nn.Module):\n    def __init__(\n        self,\n        inverted_residual_setting: List[InvertedResidualConfig],\n        last_channel: int,\n        num_classes: int = 1000,\n        block: Optional[Callable[..., nn.Module]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n        dropout: float = 0.2,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        MobileNet V3 main class\n        Args:\n            inverted_residual_setting (List[InvertedResidualConfig]): Network structure\n            last_channel (int): The number of channels on the penultimate layer\n            num_classes (int): Number of classes\n            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet\n            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n            dropout (float): The droupout probability\n        \"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        if not inverted_residual_setting:\n            raise ValueError(\"The inverted_residual_setting should not be empty\")\n        elif not (\n            isinstance(inverted_residual_setting, Sequence)\n            and all(\n                [\n                    isinstance(s, InvertedResidualConfig)\n                    for s in inverted_residual_setting\n                ]\n            )\n        ):\n            raise TypeError(\n                \"The inverted_residual_setting should be List[InvertedResidualConfig]\"\n            )\n\n        if block is None:\n            block = InvertedResidual\n\n        if norm_layer is None:\n            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n\n        layers: List[nn.Module] = []\n\n        # building first layer\n        firstconv_output_channels = inverted_residual_setting[0].input_channels\n        layers.append(\n            ConvNormActivation(\n                3,\n                firstconv_output_channels,\n                kernel_size=3,\n                stride=2,\n                norm_layer=norm_layer,\n                activation_layer=nn.Hardswish,\n            )\n        )\n\n        # building inverted residual blocks\n        for cnf in inverted_residual_setting:\n            layers.append(block(cnf, norm_layer))\n\n        # building last several layers\n        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n        lastconv_output_channels = 6 * lastconv_input_channels\n        layers.append(\n            ConvNormActivation(\n                lastconv_input_channels,\n                lastconv_output_channels,\n                kernel_size=1,\n                norm_layer=norm_layer,\n                activation_layer=nn.Hardswish,\n            )\n        )\n\n        self.features = nn.Sequential(*layers)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(lastconv_output_channels, last_channel),\n            nn.Hardswish(inplace=True),\n            nn.Dropout(p=dropout, inplace=True),\n            nn.Linear(last_channel, num_classes),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        x = self.features(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n\n        x = self.classifier(x)\n\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)", "\n\ndef _mobilenet_v3_conf(\n    arch: str,\n    width_mult: float = 1.0,\n    reduced_tail: bool = False,\n    dilated: bool = False,\n    **kwargs: Any,\n):\n    reduce_divider = 2 if reduced_tail else 1\n    dilation = 2 if dilated else 1\n\n    bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n    adjust_channels = partial(\n        InvertedResidualConfig.adjust_channels, width_mult=width_mult\n    )\n\n    if arch == \"mobilenet_v3_large\":\n        inverted_residual_setting = [\n            bneck_conf(16, 3, 16, 16, False, \"RE\", 1, 1),\n            bneck_conf(16, 3, 64, 24, False, \"RE\", 2, 1),  # C1\n            bneck_conf(24, 3, 72, 24, False, \"RE\", 1, 1),\n            bneck_conf(24, 5, 72, 40, True, \"RE\", 2, 1),  # C2\n            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n            bneck_conf(40, 3, 240, 80, False, \"HS\", 2, 1),  # C3\n            bneck_conf(80, 3, 200, 80, False, \"HS\", 1, 1),\n            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n            bneck_conf(80, 3, 480, 112, True, \"HS\", 1, 1),\n            bneck_conf(112, 3, 672, 112, True, \"HS\", 1, 1),\n            bneck_conf(\n                112, 5, 672, 160 // reduce_divider, True, \"HS\", 2, dilation\n            ),  # C4\n            bneck_conf(\n                160 // reduce_divider,\n                5,\n                960 // reduce_divider,\n                160 // reduce_divider,\n                True,\n                \"HS\",\n                1,\n                dilation,\n            ),\n            bneck_conf(\n                160 // reduce_divider,\n                5,\n                960 // reduce_divider,\n                160 // reduce_divider,\n                True,\n                \"HS\",\n                1,\n                dilation,\n            ),\n        ]\n        last_channel = adjust_channels(1280 // reduce_divider)  # C5\n    elif arch == \"mobilenet_v3_small\":\n        inverted_residual_setting = [\n            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),  # C1\n            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),  # C2\n            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),  # C3\n            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n            bneck_conf(48, 5, 288, 96 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n            bneck_conf(\n                96 // reduce_divider,\n                5,\n                576 // reduce_divider,\n                96 // reduce_divider,\n                True,\n                \"HS\",\n                1,\n                dilation,\n            ),\n            bneck_conf(\n                96 // reduce_divider,\n                5,\n                576 // reduce_divider,\n                96 // reduce_divider,\n                True,\n                \"HS\",\n                1,\n                dilation,\n            ),\n        ]\n        last_channel = adjust_channels(1024 // reduce_divider)  # C5\n    else:\n        raise ValueError(f\"Unsupported model type {arch}\")\n\n    return inverted_residual_setting, last_channel", "\n\ndef _mobilenet_v3(\n    arch: str,\n    inverted_residual_setting: List[InvertedResidualConfig],\n    last_channel: int,\n    pretrained: bool,\n    progress: bool,\n    **kwargs: Any,\n):\n    model = MobileNetV3(inverted_residual_setting, last_channel, **kwargs)\n    if pretrained:\n        if model_urls.get(arch, None) is None:\n            raise ValueError(f\"No checkpoint is available for model type {arch}\")\n        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n        model.load_state_dict(state_dict)\n    return model", "\n\ndef mobilenet_v3_large(\n    pretrained: bool = False, progress: bool = True, **kwargs: Any\n) -> MobileNetV3:\n    \"\"\"\n    Constructs a large MobileNetV3 architecture from\n    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    arch = \"mobilenet_v3_large\"\n    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, **kwargs)\n    return _mobilenet_v3(\n        arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs\n    )", "\n\ndef mobilenet_v3_small(\n    pretrained: bool = False, progress: bool = True, **kwargs: Any\n) -> MobileNetV3:\n    \"\"\"\n    Constructs a small MobileNetV3 architecture from\n    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    arch = \"mobilenet_v3_small\"\n    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, **kwargs)\n    return _mobilenet_v3(\n        arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs\n    )", ""]}
{"filename": "calibration/standard_models/__init__.py", "chunked_list": ["from .mobilenetv3 import MobileNetV3, mobilenet_v3_large, mobilenet_v3_small\n"]}
{"filename": "conf/__init__.py", "chunked_list": [""]}
