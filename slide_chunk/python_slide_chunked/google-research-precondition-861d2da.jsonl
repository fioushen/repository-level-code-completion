{"filename": "precondition/quantization_utils.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helper routines for quantization.\"\"\"\n\nfrom typing import Any\n", "from typing import Any\n\nimport chex\nfrom flax import struct\nimport jax.numpy as jnp\n\n\n# pylint:disable=no-value-for-parameter\n@struct.dataclass\nclass QuantizedValue:\n  \"\"\"State associated with quantized value.\"\"\"\n  quantized: chex.Array\n  diagonal: chex.Array  # Diagonal (if extract_diagonal is set)\n  bucket_size: chex.Array\n  quantized_dtype: jnp.dtype = struct.field(\n      pytree_node=False)  # Dtype for the quantized value.\n  extract_diagonal: bool = struct.field(\n      pytree_node=False)  # In case its centered.\n  shape: Any = struct.field(pytree_node=False)  # Shape of the tensor.\n\n  @classmethod\n  def from_float_value(cls, fvalue, quantized_dtype, extract_diagonal=False):\n    if isinstance(fvalue, list) and not fvalue:\n      return QuantizedValue([], [], [], quantized_dtype, extract_diagonal, [])  # pytype: disable=wrong-arg-types  # numpy-scalars\n    quantized, diagonal_fvalue, bucket_size = QuantizedValue.quantize(\n        fvalue, quantized_dtype, extract_diagonal)\n    return QuantizedValue(quantized, diagonal_fvalue, bucket_size,\n                          quantized_dtype, extract_diagonal,\n                          list(quantized.shape))\n\n  # Quantization is from Lingvo JAX optimizers.\n  # We extend it for int16 quantization of PSD matrices.\n  @classmethod\n  def quantize(cls, fvalue, quantized_dtype, extract_diagonal=False):\n    \"\"\"Returns quantized value and the bucket.\"\"\"\n    if quantized_dtype == jnp.float32:\n      return fvalue, [], []\n    elif quantized_dtype == jnp.bfloat16:\n      return fvalue.astype(jnp.bfloat16), [], []\n\n    float_dtype = fvalue.dtype\n    if quantized_dtype == jnp.int8:\n      # value -128 is not used.\n      num_buckets = jnp.array(127.0, dtype=float_dtype)\n    elif quantized_dtype == jnp.int16:\n      # value -32768 is not used.\n      num_buckets = jnp.array(32767.0, dtype=float_dtype)\n    else:\n      raise ValueError(f'Quantized dtype {quantized_dtype} not supported.')\n    # max value is mapped to num_buckets\n\n    if extract_diagonal and fvalue.ndim != 2:\n      raise ValueError(\n          f'Input array {fvalue} must be 2D to work with extract_diagonal.')\n\n    diagonal_fvalue = []\n    if extract_diagonal:\n      diagonal_fvalue = jnp.diag(fvalue)\n      # Remove the diagonal entries.\n      fvalue = fvalue - jnp.diag(diagonal_fvalue)\n\n    # TODO(rohananil): Extend this by making use of information about the blocks\n    # SM3 style which will be useful for diagonal statistics\n    # We first decide the scale.\n    if fvalue.ndim < 1:\n      raise ValueError(\n          f'Input array {fvalue} must have a strictly positive number of '\n          'dimensions.')\n\n    max_abs = jnp.max(jnp.abs(fvalue), axis=0)\n    bucket_size = max_abs / num_buckets\n    bs_expanded = bucket_size[jnp.newaxis, ...]\n    # To avoid divide by 0.0\n    bs_nonzero = jnp.where(bs_expanded > 0.0, bs_expanded,\n                           jnp.ones_like(bs_expanded))\n    ratio = fvalue / bs_nonzero\n    # We use rounding to remove bias.\n    quantized = jnp.round(ratio)\n    return quantized.astype(quantized_dtype), diagonal_fvalue, bucket_size\n\n  def to_float(self):\n    \"\"\"Returns the float value.\"\"\"\n    if isinstance(self.quantized, list) and not self.quantized:\n      return self.quantized\n\n    if self.quantized_dtype == jnp.float32:\n      return self.quantized\n\n    if self.quantized_dtype == jnp.bfloat16:\n      return self.quantized.astype(jnp.float32)\n\n    float_dtype = self.bucket_size.dtype\n    bucket_size = self.bucket_size[jnp.newaxis, ...]\n    val = self.quantized.astype(float_dtype) * bucket_size\n    if self.extract_diagonal:\n      val += jnp.diag(self.diagonal)\n    return val", "@struct.dataclass\nclass QuantizedValue:\n  \"\"\"State associated with quantized value.\"\"\"\n  quantized: chex.Array\n  diagonal: chex.Array  # Diagonal (if extract_diagonal is set)\n  bucket_size: chex.Array\n  quantized_dtype: jnp.dtype = struct.field(\n      pytree_node=False)  # Dtype for the quantized value.\n  extract_diagonal: bool = struct.field(\n      pytree_node=False)  # In case its centered.\n  shape: Any = struct.field(pytree_node=False)  # Shape of the tensor.\n\n  @classmethod\n  def from_float_value(cls, fvalue, quantized_dtype, extract_diagonal=False):\n    if isinstance(fvalue, list) and not fvalue:\n      return QuantizedValue([], [], [], quantized_dtype, extract_diagonal, [])  # pytype: disable=wrong-arg-types  # numpy-scalars\n    quantized, diagonal_fvalue, bucket_size = QuantizedValue.quantize(\n        fvalue, quantized_dtype, extract_diagonal)\n    return QuantizedValue(quantized, diagonal_fvalue, bucket_size,\n                          quantized_dtype, extract_diagonal,\n                          list(quantized.shape))\n\n  # Quantization is from Lingvo JAX optimizers.\n  # We extend it for int16 quantization of PSD matrices.\n  @classmethod\n  def quantize(cls, fvalue, quantized_dtype, extract_diagonal=False):\n    \"\"\"Returns quantized value and the bucket.\"\"\"\n    if quantized_dtype == jnp.float32:\n      return fvalue, [], []\n    elif quantized_dtype == jnp.bfloat16:\n      return fvalue.astype(jnp.bfloat16), [], []\n\n    float_dtype = fvalue.dtype\n    if quantized_dtype == jnp.int8:\n      # value -128 is not used.\n      num_buckets = jnp.array(127.0, dtype=float_dtype)\n    elif quantized_dtype == jnp.int16:\n      # value -32768 is not used.\n      num_buckets = jnp.array(32767.0, dtype=float_dtype)\n    else:\n      raise ValueError(f'Quantized dtype {quantized_dtype} not supported.')\n    # max value is mapped to num_buckets\n\n    if extract_diagonal and fvalue.ndim != 2:\n      raise ValueError(\n          f'Input array {fvalue} must be 2D to work with extract_diagonal.')\n\n    diagonal_fvalue = []\n    if extract_diagonal:\n      diagonal_fvalue = jnp.diag(fvalue)\n      # Remove the diagonal entries.\n      fvalue = fvalue - jnp.diag(diagonal_fvalue)\n\n    # TODO(rohananil): Extend this by making use of information about the blocks\n    # SM3 style which will be useful for diagonal statistics\n    # We first decide the scale.\n    if fvalue.ndim < 1:\n      raise ValueError(\n          f'Input array {fvalue} must have a strictly positive number of '\n          'dimensions.')\n\n    max_abs = jnp.max(jnp.abs(fvalue), axis=0)\n    bucket_size = max_abs / num_buckets\n    bs_expanded = bucket_size[jnp.newaxis, ...]\n    # To avoid divide by 0.0\n    bs_nonzero = jnp.where(bs_expanded > 0.0, bs_expanded,\n                           jnp.ones_like(bs_expanded))\n    ratio = fvalue / bs_nonzero\n    # We use rounding to remove bias.\n    quantized = jnp.round(ratio)\n    return quantized.astype(quantized_dtype), diagonal_fvalue, bucket_size\n\n  def to_float(self):\n    \"\"\"Returns the float value.\"\"\"\n    if isinstance(self.quantized, list) and not self.quantized:\n      return self.quantized\n\n    if self.quantized_dtype == jnp.float32:\n      return self.quantized\n\n    if self.quantized_dtype == jnp.bfloat16:\n      return self.quantized.astype(jnp.float32)\n\n    float_dtype = self.bucket_size.dtype\n    bucket_size = self.bucket_size[jnp.newaxis, ...]\n    val = self.quantized.astype(float_dtype) * bucket_size\n    if self.extract_diagonal:\n      val += jnp.diag(self.diagonal)\n    return val", "\n"]}
{"filename": "precondition/sm3.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"SM3 Implementation.\"\"\"\n\nimport functools\nfrom typing import Any, NamedTuple", "import functools\nfrom typing import Any, NamedTuple\n\nimport chex\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom precondition.quantization_utils import QuantizedValue\n", "from precondition.quantization_utils import QuantizedValue\n\n\nclass SM3State(NamedTuple):\n  count: chex.Array\n  stats: Any\n\n\n# Per parameter optimizer state used in data-parallel training.\nclass ParameterStats(NamedTuple):\n  \"\"\"State associated to each parameter of the model being trained.\"\"\"\n  diagonal_statistics: chex.Array  # Accumulator for diagonal preconditioner\n  diagonal_momentum: QuantizedValue  # Momentum for the diagonal preconditioner", "# Per parameter optimizer state used in data-parallel training.\nclass ParameterStats(NamedTuple):\n  \"\"\"State associated to each parameter of the model being trained.\"\"\"\n  diagonal_statistics: chex.Array  # Accumulator for diagonal preconditioner\n  diagonal_momentum: QuantizedValue  # Momentum for the diagonal preconditioner\n\n\ndef sm3(\n    learning_rate,\n    beta1=0.9,\n    beta2=0.999,\n    diagonal_epsilon=1e-10,\n    weight_decay=0.0,\n    normalize_grads=False):\n  \"\"\"SM3 optimizer.\n\n  Memory-Efficient Adaptive Optimization, Rohan Anil, Vineet Gupta, Tomer Koren,\n    Yoram Singer\n\n  https://arxiv.org/abs/1901.11150\n\n  Args:\n    learning_rate: the step size used to update the parameters.\n    beta1: momentum parameter.\n    beta2: second moment averaging parameter.\n    diagonal_epsilon: epsilon for sm3\n    weight_decay: the amount of weight decay regularization to apply. defaults\n      to 0.0.\n    normalize_grads: Whether to normalize grads. Author finds it useful when\n      grads are high variance.\n\n  Returns:\n    a GradientTransformation.\n  \"\"\"\n\n  def _quantize_momentum(momentum_statistics):\n    return QuantizedValue.from_float_value(momentum_statistics, jnp.int8)\n\n  def init_fn(params):\n    \"\"\"Initialise the optimiser's state.\"\"\"\n\n    def _init(param):\n      accumulators = [jnp.zeros([s]) for s in param.shape]\n      momentum = _quantize_momentum(jnp.zeros_like(param))\n      return ParameterStats(accumulators, momentum)  # pytype: disable=wrong-arg-types  # numpy-scalars\n\n    return SM3State(\n        count=jnp.zeros([], jnp.int32), stats=jax.tree_map(_init, params))\n\n  def _get_expanded_shape(shape, i):\n    rank = len(shape)\n    # Replaces a `shape` of [M, N, K] with 1 in all dimensions except for i.\n    # For eg: i = 1 returns [1, N, 1].\n    return [1] * i + [shape[i]] + [1] * (rank - i - 1)\n\n  def _moving_averages(grad, accumulators):\n    w = (1.0 - beta2) if beta2 != 1.0 else 1.0\n    if grad.ndim < 2:\n      return beta2 * accumulators[0] + w * grad**2\n    else:\n      min_accumulator = functools.reduce(jnp.minimum, accumulators)\n      return beta2 * min_accumulator + w * grad**2\n\n  def _moving_averages_momentum(grad, momentum):\n    w = (1.0 - beta1) if beta1 != 1.0 else 1.0\n    return beta1 * momentum.to_float() + w * grad\n\n  def _sketch_diagonal_statistics(grad, updated_diagonal_statistics):\n    all_diagonal_statistics = []\n    for i in range(grad.ndim):\n      axes = list(range(i)) + list(range(i + 1, grad.ndim))\n      dim_diagonal_statistics = jnp.max(updated_diagonal_statistics, axis=axes)\n      all_diagonal_statistics.append(dim_diagonal_statistics)\n    if grad.ndim == 1:\n      all_diagonal_statistics[0] = updated_diagonal_statistics\n    return all_diagonal_statistics\n\n  def update_fn(updates, state, params):\n    stats = state.stats\n    if normalize_grads:\n      updates = jax.tree_map(\n          lambda g: g / (jnp.linalg.norm(g) + 1e-16), updates)\n    # Reshape all vectors into N-d tensors to compute min over them.\n    # [n], [m] -> [n, 1], [1, m]\n    expanded_diagonal_statistics = jax.tree_map(\n        lambda grad, state:  # pylint:disable=g-long-lambda\n        [\n            jnp.reshape(state.diagonal_statistics[i],\n                        _get_expanded_shape(grad.shape, i))\n            for i in range(grad.ndim)\n        ],\n        updates,\n        stats)\n\n    # Compute new diagonal statistics\n    new_diagonal_statistics = jax.tree_map(_moving_averages, updates,\n                                           expanded_diagonal_statistics)\n\n    # Compute preconditioners (1/sqrt(s)) where s is the statistics.\n    new_preconditioners = jax.tree_map(\n        lambda t: 1.0 / jnp.sqrt(t + diagonal_epsilon), new_diagonal_statistics)\n    preconditioned_grads = jax.tree_map(lambda g, p: g * p, updates,\n                                        new_preconditioners)\n\n    # Compute updated momentum (also handle quantization)\n    updated_momentum = jax.tree_map(\n        lambda preconditioned_grad, state:  # pylint:disable=g-long-lambda\n        _moving_averages_momentum(preconditioned_grad, state.diagonal_momentum),\n        preconditioned_grads,\n        stats)\n\n    # Update diagonal statistics.\n    updated_diagonal_statistics = jax.tree_map(_sketch_diagonal_statistics,\n                                               updates, new_diagonal_statistics)\n\n    # Update momentum.\n    new_sm3_stats = jax.tree_map(\n        lambda momentum, diagonal_stats:  # pylint:disable=g-long-lambda\n        ParameterStats(diagonal_stats, _quantize_momentum(momentum)),\n        updated_momentum,\n        updated_diagonal_statistics)\n\n    # Apply weight decay\n    updated_momentum_with_wd = updated_momentum\n    if weight_decay > 0.0:\n      updated_momentum_with_wd = jax.tree_map(lambda g, p: g + weight_decay * p,\n                                              updated_momentum, params)\n\n    lr = learning_rate\n    if callable(learning_rate):\n      lr = learning_rate(state.count)\n\n    new_updates = jax.tree_map(lambda pg: -lr * pg, updated_momentum_with_wd)\n    return new_updates, SM3State(count=state.count+1, stats=new_sm3_stats)\n\n  return optax.GradientTransformation(init_fn, update_fn)", ""]}
{"filename": "precondition/__init__.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"precondition API.\"\"\"\n\n# A new PyPI release will be pushed everytime `__version__` is increased\n# When changing this, also update the CHANGELOG.md", "# A new PyPI release will be pushed everytime `__version__` is increased\n# When changing this, also update the CHANGELOG.md\n__version__ = '0.3.0'\n"]}
{"filename": "precondition/distributed_shampoo_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for distributed_shampoo.\"\"\"\n\nimport functools\nimport itertools", "import functools\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport chex\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom precondition import distributed_shampoo", "import numpy as np\nfrom precondition import distributed_shampoo\nimport scipy\n\n\nclass PaddingTest(parameterized.TestCase):\n\n  def assertAllClose(self, x, y, atol=1e-5, rtol=1e-5):\n    np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n\n  @parameterized.named_parameters(\n      {\n          'testcase_name': 'NoPadding',\n          'max_size': 3,\n          'result': [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n      },\n      {\n          'testcase_name':\n              'Padding',\n          'max_size':\n              5,\n          'result': [[1., 1., 1., 0., 0.], [1., 1., 1., 0., 0.],\n                     [1., 1., 1., 0., 0.], [0., 0., 0., 1., 0.],\n                     [0., 0., 0., 0., 1.]],\n      },\n  )\n  def test_pad_square_matrix(self, max_size, result):\n    self.assertAllClose(\n        distributed_shampoo.pad_square_matrix(\n            mat=jnp.ones(shape=(3, 3), dtype=jnp.float32), max_size=max_size),\n        jnp.asarray(result, dtype=jnp.float32))\n\n  @parameterized.named_parameters(\n      {\n          'testcase_name': 'TooLarge',\n          'shape': (3, 3),\n          'max_size': 2\n      },\n      {\n          'testcase_name': 'NotSquare',\n          'shape': (3, 4),\n          'max_size': 5\n      },\n  )\n  def test_pad_square_matrix_error(self, shape, max_size):\n    with self.assertRaises(ValueError):\n      distributed_shampoo.pad_square_matrix(\n          mat=jnp.ones(shape=shape), max_size=max_size)", "\n\ndef _pth_root_difference_cases():\n  \"\"\"Returns cases for _pth_root_difference() test.\"\"\"\n  cases = []\n  # The test checks accuracy of\n  # (w + a)^(-1/p) - (w + b)^(-1/p)\n  # so generate corresponding parameters.\n  p_vals = [2, 4, 6, 8]\n  a_vals = b_vals = [1e-6, 1e-5, 0.0, 1.0]\n  w_vals = [1e-6, 1e-5, 1.0, 1e3]\n  for p, a, b, w in itertools.product(p_vals, a_vals, b_vals, w_vals):\n    cases.append({'p': p, 'a': a, 'b': b, 'w': w})\n  return cases", "\n\nclass DistributedShampooTest(chex.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.init_params = (jnp.array([[1., 3.],\n                                   [2., 4.]]), jnp.array([[3., 4.], [3., 4.]]))\n    self.per_step_updates = (jnp.array([[500., 5.], [500., 5.]]),\n                             jnp.array([[300., 3.], [300., 3.]]))\n    self.per_step_updates_custom_preconditioner = (self.per_step_updates,\n                                                   (jnp.array([[200., 4.],\n                                                               [200., 4.]]),\n                                                    jnp.array([[600., 2.],\n                                                               [600., 2.]])))\n    self.rng = np.random.default_rng(1234)\n    shape = ([2, 5], [6, 3])\n    dt = self.init_params[0].dtype\n\n    def make_shape(bigger_first_entry):\n      x = tuple(self.rng.standard_normal(size=s) for s in shape)\n      if bigger_first_entry:\n        for xx in x:\n          xx[..., 0] *= 100\n      return tuple(jnp.array(xx).astype(dt) for xx in x)\n\n    self.init_params_larger = make_shape(False)\n    self.per_step_updates_larger = make_shape(True)\n\n  @chex.all_variants(with_pmap=False)\n  @parameterized.named_parameters(\n      {\n          'testcase_name': 'default',\n          'best_effort_memory_usage_reduction': True,\n          'expected_value': -0.57,\n      },\n      {\n          'testcase_name': 'default_nomerge',\n          'best_effort_memory_usage_reduction': True,\n          'merge_small_dims_block_size': 1,\n          'expected_value': -0.57,\n      },\n      {\n          'testcase_name': 'default_larger',\n          'best_effort_memory_usage_reduction': True,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'default_larger_nomerge',\n          'best_effort_memory_usage_reduction': True,\n          'slightly_larger': True,\n          'merge_small_dims_block_size': 1,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'materialize_statistics',\n          'best_effort_memory_usage_reduction': True,\n      },\n      {\n          'testcase_name': 'blocked_statistics',\n          'best_effort_memory_usage_reduction': True,\n      },\n      {\n          'testcase_name': 'default_quantized',\n      },\n      {\n          'testcase_name': 'materialize_statistics_quantized',\n      },\n      {\n          'testcase_name': 'blocked_statistics_quantized',\n      },\n      {\n          'testcase_name': 'pos_compression_rank',\n          'compression_rank': 1,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'pos_compression_rank_nomerge',\n          'compression_rank': 1,\n          'slightly_larger': True,\n          'merge_small_dims_block_size': 1,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'neg_compression_rank',\n          'compression_rank': -1,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'neg_compression_rank_nomerge',\n          'compression_rank': -1,\n          'slightly_larger': True,\n          'merge_small_dims_block_size': 1,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'no_training_metrics',\n          'generate_training_metrics': False,\n      },\n      {\n          'testcase_name': 'larger_reuse',\n          'best_effort_memory_usage_reduction': True,\n          'reuse_preconditioner': True,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'larger_reuse_highmem',\n          'best_effort_memory_usage_reduction': False,\n          'reuse_preconditioner': True,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n      {\n          'testcase_name': 'larger_reuse_highmem_nomerge',\n          'best_effort_memory_usage_reduction': False,\n          'merge_small_dims_block_size': 1,\n          'reuse_preconditioner': True,\n          'slightly_larger': True,\n          'expected_value': -0.17019942,\n      },\n  )\n  def test_distributed_shampoo(\n      self,\n      best_effort_memory_usage_reduction=False,\n      compression_rank=0,\n      merge_small_dims_block_size=4096,\n      generate_training_metrics=True,\n      slightly_larger=False,\n      expected_value=None,\n      reuse_preconditioner=False,\n  ):\n    params = self.init_params_larger if slightly_larger else self.init_params\n\n    optim = distributed_shampoo.distributed_shampoo(\n        0.1,\n        32,\n        batch_axis_name='batch',\n        preconditioning_compute_steps=2,\n        best_effort_memory_usage_reduction=best_effort_memory_usage_reduction,\n        relative_matrix_epsilon=True,\n        compression_rank=compression_rank,\n        merge_small_dims_block_size=merge_small_dims_block_size,\n        generate_training_metrics=generate_training_metrics,\n        reuse_preconditioner=reuse_preconditioner,\n    )\n    init_fn = self.variant(optim.init)\n    transform_fn = self.variant(optim.update)\n\n    if slightly_larger:\n      updates = self.per_step_updates_larger\n    else:\n      updates = self.per_step_updates\n\n    def _update(unused_batch):\n      return transform_fn(updates, state, params)\n\n    state = init_fn(params)\n    chex.assert_tree_all_finite(state)\n    pmap_fn = jax.pmap(_update, axis_name='batch')\n\n    updates, state = pmap_fn(jnp.array([1.0]))\n    chex.assert_tree_all_finite((params, updates, state))\n    if expected_value is not None:\n      last_entry = updates[1][-1, -1, -1]\n      self.assertLess(\n          abs(last_entry - expected_value),\n          1e-4,\n          msg=f'{last_entry=}, {expected_value=}')\n    for _ in range(5):\n      updates, state = pmap_fn(jnp.array([1.0]))\n      chex.assert_tree_all_finite((params, updates, state))\n\n  @chex.all_variants(with_pmap=False)\n  @parameterized.named_parameters([\n      {\n          'testcase_name': 'default',\n      },\n      {\n          'testcase_name': 'no_training_metrics',\n          'generate_training_metrics': False,\n      },\n  ])\n  def test_distributed_shampoo_no_pmap(self, generate_training_metrics=True):\n    params = self.init_params\n\n    optim = distributed_shampoo.distributed_shampoo(\n        0.1,\n        32,\n        batch_axis_name=None,\n        preconditioning_compute_steps=2,\n        generate_training_metrics=generate_training_metrics)\n    init_fn = self.variant(optim.init)\n    transform_fn = self.variant(optim.update)\n    state = init_fn(params)\n    chex.assert_tree_all_finite(state)\n    updates, state = transform_fn(self.per_step_updates, state, params)\n    chex.assert_tree_all_finite((params, updates, state))\n\n  @chex.all_variants(with_pmap=False)\n  @parameterized.named_parameters([\n      {\n          'testcase_name': 'preconditioning_compute_steps_schedule',\n          'preconditioning_compute_steps': 2,\n          'end_preconditioning_steps': 100,\n      },\n      {\n          'testcase_name': (\n              'preconditioning_compute_steps_schedule_short_circuit'\n          ),\n          'preconditioning_compute_steps': 1,\n          'end_preconditioning_steps': 1,\n      },\n  ])\n  def test_distributed_shampoo_preconditioning_compute_steps_schedule(\n      self, preconditioning_compute_steps, end_preconditioning_steps\n  ):\n    params = self.init_params\n\n    base_lr = 0.1\n\n    def lr_fn(t):\n      decay_factor = (t + 1) ** -0.5\n      return base_lr * decay_factor\n\n    optim = distributed_shampoo.distributed_shampoo(\n        lr_fn,\n        32,\n        batch_axis_name='batch',\n        preconditioning_compute_steps=preconditioning_compute_steps,\n        decay_preconditioning_compute_steps=True,\n        end_preconditioning_compute_steps=end_preconditioning_steps,\n    )\n    init_fn = self.variant(optim.init)\n    transform_fn = self.variant(optim.update)\n\n    updates = self.per_step_updates\n\n    def _update(unused_batch):\n      return transform_fn(updates, state, params)\n\n    state = init_fn(params)\n    chex.assert_tree_all_finite(state)\n    pmap_fn = jax.pmap(_update, axis_name='batch')\n\n    updates, state = pmap_fn(jnp.array([1.0]))\n    chex.assert_tree_all_finite((params, updates, state))\n    for _ in range(5):\n      updates, state = pmap_fn(jnp.array([1.0]))\n      chex.assert_tree_all_finite((params, updates, state))\n\n  def _gen_symmetrix_matrix(self, dim, condition_number):\n    u = scipy.stats.ortho_group.rvs(\n        dim=dim, random_state=self.rng).astype(np.float64)\n    v = u.T\n    diag = np.diag([condition_number**(-i / (dim - 1)) for i in range(dim)])\n    return u @ diag @ v\n\n  def test_matrix_inverse_root(self):\n    \"\"\"Test for matrix inverse pth root.\"\"\"\n\n    # Fails after it reaches a particular condition number.\n    for e in range(2, 12):\n      condition_number = 10**e\n      ms = self._gen_symmetrix_matrix(16, condition_number)\n      self.assertLess(\n          np.abs(np.linalg.cond(ms) - condition_number),\n          condition_number * 0.01)\n      metrics = distributed_shampoo.matrix_inverse_pth_root(\n          ms.astype(np.float32), 4, ridge_epsilon=1e-12)[1]\n      error = metrics.inverse_pth_root_errors\n      if e < 7:\n        self.assertLess(error, 0.1)\n      else:\n        # No guarantee of success after e >= 7\n        pass\n\n  @parameterized.parameters([{'sz': sz} for sz in [4, 32]])\n  def test_matrix_inverse_root_padding(self, sz):\n    \"\"\"Test padding does not affect result much.\"\"\"\n\n    # Note sz == 1 case will not pass tests here b/c the method\n    # is exact for scalars (but padding triggers coupled iteration).\n\n    condition_number = 1e3\n    ms = self._gen_symmetrix_matrix(sz, condition_number).astype(np.float32)\n\n    # Shift matrix norm down by some large factor, so that improper padding\n    # handling results in an error by increasing the condition number.\n    ms = jnp.array(ms) * 1e-3\n\n    rt, metrics = distributed_shampoo.matrix_inverse_pth_root(\n        ms, 4, ridge_epsilon=1e-3)\n    err = metrics.inverse_pth_root_errors\n    pad_ms = distributed_shampoo.pad_square_matrix(ms, sz * 2)\n    pad_rt, metrics = distributed_shampoo.matrix_inverse_pth_root(\n        pad_ms, 4, ridge_epsilon=1e-3, padding_start=sz)\n    pad_err = metrics.inverse_pth_root_errors\n    pad_rt_principal = pad_rt[:sz, :sz]\n    np.testing.assert_allclose(\n        rt,\n        pad_rt_principal,\n        # The fact that this is so large keeps vladf up at night,\n        # but without padding_start argument it's even worse (>1).\n        rtol=1e-2 if sz == 4 else 5e-2,\n        err_msg=np.array2string(rt - pad_rt_principal))\n    self.assertLessEqual(pad_err, 4 * err)\n    self.assertEqual(np.abs(pad_rt[sz:]).sum(), 0)\n    self.assertEqual(np.abs(pad_rt[:, sz:]).sum(), 0)\n\n  def test_all_padding(self):\n    \"\"\"Test full padding matrix.\"\"\"\n    empty = jnp.zeros([0, 0])\n    padded = distributed_shampoo.pad_square_matrix(empty, 10)\n    rt, metrics = distributed_shampoo.matrix_inverse_pth_root(\n        padded, 4, ridge_epsilon=1e-3, padding_start=0)\n    err = metrics.inverse_pth_root_errors\n    self.assertEqual(np.abs(rt).sum(), 0.0)\n    self.assertEqual(np.abs(err).sum(), 0.0)\n\n  def _make_pth_diff_message(self, w, alpha, beta, p):\n    left = f'({w} + {alpha})^(-1.0 / {p}) - '\n    right = f'({w} + {beta})^(-1.0 / {p})'\n    return left + right\n\n  @parameterized.parameters(_pth_root_difference_cases())\n  def test_pth_root_difference(self, p, a, b, w):\n    \"\"\"Test stable difference computation.\"\"\"\n    pth_rt_diff = jax.jit(\n        functools.partial(distributed_shampoo._pth_root_difference, p=p))\n    actual = pth_rt_diff(w, a, b)\n    # in float64\n    exp = (-1.0 / p)\n    expected = (w + a)**exp - (w + b)**exp\n\n    self.assertAlmostEqual(\n        actual,\n        expected,\n        msg=self._make_pth_diff_message(w, a, b, p),\n        delta=1e-2)\n\n  @parameterized.parameters([{'p': p} for p in [2, 4, 8]])\n  def test_lobpcg_preconditioning(self, p):\n    \"\"\"Checks that root calculation is valid with top-k preconditioning.\"\"\"\n    rng = np.random.RandomState(seed=42)\n    n = 11\n    epsilon = jnp.float32(1e-4)\n    a_asymm = jnp.array(rng.random((n, n)), jnp.float32)\n    a = jnp.matmul(a_asymm.T, a_asymm, precision=jax.lax.Precision.HIGHEST)\n    log2 = (p - 1).bit_length()\n    assert 2**log2 == p, (p, log2)\n\n    root = functools.partial(\n        distributed_shampoo.matrix_inverse_pth_root, ridge_epsilon=epsilon, p=p)\n    root_lobpcg = functools.partial(\n        root, lobpcg_topk_precondition=2, lobpcg_max_iter=10)\n\n    methods = {'default': root, 'precond': root_lobpcg}\n    spectrum_err, entry_err = {}, {}\n    for k, method in methods.items():\n      rt = jax.jit(method)(a)[0]\n\n      # Recover the inverse by repeated squaring of inverse p-th root.\n      inv = np.asarray(rt).astype(np.float64)\n      for _ in range(log2):\n        inv = inv.dot(inv)\n\n      approx_id = inv.dot(a)\n      spectrum = np.linalg.eigvalsh(approx_id)\n      spectrum_err[k] = np.abs(1 - spectrum)\n      entry_err[k] = np.mean(np.abs(approx_id - np.eye(n)))\n\n    with np.printoptions(precision=2):\n\n      def print_dict(d):\n        return '\\n'.join(f'{k} {v}' for k, v in d.items())\n\n      err_msg = (f'p={p} log2(p)={log2}\\n'\n                 f'spectrum error\\n{print_dict(spectrum_err)}\\n'\n                 f'entry_err\\n{print_dict(entry_err)}')\n\n      self.assertLessEqual(\n          np.median(spectrum_err['precond']),\n          2 * np.median(spectrum_err['default']),\n          msg=err_msg)\n\n      self.assertLessEqual(\n          entry_err['precond'], entry_err['default'] * 2, msg=err_msg)", "\n\nclass LowRankInverseRootTest(chex.TestCase):\n\n  def test_dynamic_exponent(self):\n    \"\"\"Test that exponent for various 'p' is correct.\"\"\"\n    root = jax.jit(\n        functools.partial(\n            distributed_shampoo._low_rank_root,\n            compression_rank=1,\n            ridge_epsilon=0.0,\n            relative_matrix_epsilon=False,\n        ))\n    for p in range(2, 9):\n      # Requires padding (else why would we compress to low rank?)\n      a = np.zeros([4, 4], jnp.float32)\n      a[0, 0] = 2**p\n      exact = jnp.float32(1 / 2)\n      r, metrics = root(a, p)\n      e = metrics.inverse_pth_root_errors\n      error = jnp.abs(r[0, 1] - exact)\n      self.assertLessEqual(error, 10 * np.finfo(np.float32).eps)\n      self.assertLessEqual(e, 10 * np.finfo(np.float32).eps)\n\n  @parameterized.parameters(\n      {\n          'size': 5,\n          'padded_size': 5,\n          'compression_rank': 2\n      },\n      {\n          'size': 5,\n          'padded_size': 8,\n          'compression_rank': 2\n      },\n      {\n          'size': 5,\n          'padded_size': 5,\n          'compression_rank': -2\n      },\n      {\n          'size': 5,\n          'padded_size': 8,\n          'compression_rank': -2\n      },\n  )\n  def test_basic(self, size, padded_size, compression_rank):\n    \"\"\"Validate low rank returned matrix.\"\"\"\n    assert size > abs(compression_rank) + 2\n    eps = 0.1\n    root = jax.jit(\n        functools.partial(\n            distributed_shampoo._low_rank_root,\n            p=2,\n            compression_rank=compression_rank,\n            ridge_epsilon=eps,\n            relative_matrix_epsilon=False,\n            padding_start=size))\n    rng = np.random.default_rng(1234)\n    a = rng.standard_normal(size=[size, size])\n    a = a.T.dot(a)\n    s, v = np.linalg.eigh(a + eps * np.eye(size))\n    padded_a = np.zeros([padded_size, padded_size])\n    padded_a[:size, :size] = a\n    r, metrics = root(padded_a)\n    packing_dim = abs(compression_rank) + 2\n    assert list(r.shape) == [padded_size, packing_dim]\n    e = metrics.inverse_pth_root_errors\n    rv, re, rc, _ = distributed_shampoo._low_rank_unpack(\n        r[:size, :packing_dim], compression_rank)\n    complement = np.eye(size) - rv.dot(rv.T)\n    root = complement * rc + rv.dot(np.diag(re).dot(rv.T))\n    s = s**(-0.5)\n    if compression_rank > 0:\n      s[:-compression_rank] = np.mean(s[:-compression_rank])\n    else:\n      s[-compression_rank:] = np.mean(s[-compression_rank:])\n    exact = v.dot(np.diag(s).dot(v.T))\n    error = np.max(np.abs(exact - root))\n    self.assertLessEqual(error, 1e-2)\n    self.assertLessEqual(e, 1e-2)\n\n  @parameterized.parameters(True, False)\n  def test_nonzero_epsilon(self, relative):\n    \"\"\"Tests that the proper epsilon is added.\"\"\"\n    root = jax.jit(\n        functools.partial(\n            distributed_shampoo._low_rank_root,\n            compression_rank=1,\n            ridge_epsilon=0.1,\n            relative_matrix_epsilon=relative,\n        ))\n    p = 2\n    a = np.zeros([4, 4], jnp.float32)\n    a[0, 0] = 2**p\n    eps = 0.1 * (a[0, 0] if relative else 1)\n    corner = jnp.float32((a[0, 0] + eps)**(-1.0 / p))\n    ridge = jnp.float32(eps**(-1.0 / p))\n    r, metrics = root(a, p)\n    v, e, c, _ = distributed_shampoo._low_rank_unpack(r, compression_rank=1)\n    err = metrics.inverse_pth_root_errors\n    assert list(v.shape) == [4, 1]\n    assert list(e.shape) == [1], e.shape\n    assert not c.shape\n    error = np.max(np.abs(corner - e[0]))\n    self.assertLessEqual(error, 10 * np.finfo(np.float32).eps)\n    self.assertLessEqual(err, 10 * np.finfo(np.float32).eps)\n    vec_error = np.max(np.abs(np.array([1, 0, 0, 0]) - v.ravel()))\n    self.assertLessEqual(vec_error, 10 * np.finfo(np.float32).eps, msg=v)\n    self.assertLessEqual(abs(c - ridge), 1e-4)", "\n\ndef _make_pack_unpack_cases():\n  sizes = [4, 8]\n  paddings = [0, 10]\n  cases = []\n  for zero_tail in [True, False]:\n    for size in sizes:\n      for padding in paddings:\n        for null_rank in [0, 1]:\n          nonzero_rank = size - 2 - null_rank - 1\n          for has_zeros in [True, False]:\n            cases.append({\n                'zero_tail': zero_tail,\n                'size': size,\n                'padding': padding,\n                'null_rank': null_rank,\n                'nonzero_rank': nonzero_rank,\n                'has_zeros': has_zeros,\n            })\n  return cases", "\n\nclass FDLowRankInverseRootTest(chex.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.rank = None\n    self.p = None\n    self.rng = np.random.default_rng(1234)\n\n  def tearDown(self):\n    self.rank = None\n    self.p = None\n    self.rng = None\n    super().tearDown()\n\n  def _fd_update(\n      self,\n      grad=None,\n      prev=None,\n      ridge_epsilon=0.0,\n      relative_matrix_epsilon=False,\n      padding_start=None,\n  ):\n    assert self.p is not None\n    assert self.rank is not None\n    assert grad is not None or prev is not None\n    padded_size = len(grad) if grad is not None else len(prev)\n    if grad is None:\n      grad = np.zeros([padded_size, padded_size], np.float32)\n    else:\n      grad = np.array(grad).astype(np.float32)\n      assert list(grad.shape) == [padded_size, padded_size], grad.shape\n    if prev is None:\n      prev = np.zeros([padded_size, self.rank + 2], np.float32)\n    else:\n      prev = np.array(prev).astype(np.float32)\n      assert list(prev.shape) == [padded_size, self.rank + 2\n                                 ], (prev.shape, [padded_size, self.rank + 2])\n    if padding_start is None:\n      padding_start = padded_size\n    assert padding_start <= padded_size\n    fd_update = jax.jit(\n        functools.partial(\n            distributed_shampoo._fd_update_root,\n            rank=self.rank,\n            ridge_epsilon=ridge_epsilon,\n            relative_matrix_epsilon=relative_matrix_epsilon,\n            error_tolerance=0.0,\n            padding_start=padding_start,\n        ))\n    updated, _ = fd_update(grad, p=self.p, prev=prev)\n    x = {}\n    (x['eigvecs'], x['eigvals'], x['inverted_eigvals'], x['inverted_tail'],\n     x['tail'], x['has_zeros']) = distributed_shampoo._fd_low_rank_unpack(\n         updated, self.rank)\n    assert list(x['eigvecs'].shape) == [padded_size, self.rank]\n    assert list(x['eigvals'].shape) == [self.rank]\n    assert list(x['inverted_eigvals'].shape) == [self.rank]\n    assert not x['inverted_tail'].shape\n    assert not x['tail'].shape\n    if x['tail'] > 0:\n      self.assertAlmostEqual(x['tail']**(-1 / self.p), x['inverted_tail'])\n      np.testing.assert_allclose((x['eigvals'] + x['tail'])**(-1 / self.p),\n                                 x['inverted_eigvals'])\n    return x\n\n  def _make_fd_state(self, eigvecs, eigs, start_tail, padded_size=None):\n    size, dim = eigvecs.shape\n    assert dim == self.rank\n    assert self.rank is not None\n    assert self.p is not None\n    assert self.rank + 2 < size\n    assert len(eigs) == self.rank\n    if padded_size is None:\n      padded_size = size\n    assert size <= padded_size\n    eigvecs = jnp.pad(eigvecs, ((0, padded_size - size), (0, 0)))\n    eigs = np.array(eigs).astype(np.float32)\n    inv_eigs = np.where(eigs == 0.0, 0.0, eigs**(-1 / self.p))\n    inv_tail = 0.0 if start_tail == 0.0 else start_tail**(-1 / self.p)\n    return distributed_shampoo._fd_low_rank_pack(\n        eigvecs,\n        eigs,\n        inv_eigs,\n        inv_tail,\n        start_tail,\n        False,\n        self.rank,\n    )\n\n  def _make_eye_state(self, size, eigs, start_tail, padded_size=None):\n    eigvecs = np.eye(size, self.rank, dtype=np.float32)\n    return self._make_fd_state(eigvecs, eigs, start_tail, padded_size)\n\n  @parameterized.parameters(range(2, 9, 2))\n  def test_dynamic_exponent(self, p):\n    \"\"\"Test that exponent for various 'p' is correct.\"\"\"\n    self.rank = 1\n    self.p = p\n    size = 4\n    prev = self._make_eye_state(size, [0], 0.0)\n    grad = np.zeros([4, 4], np.float32)\n    grad[0, 0] = 2**(p / 2)  # Grad will get squared.\n    ret = self._fd_update(grad, prev)\n    self.assertAlmostEqual(ret['inverted_eigvals'], 1 / 2, delta=1e-6)\n\n    prev = self._make_eye_state(size, [2**p], 0.0)\n    grad = np.zeros([4, 4], np.float32)\n    ret = self._fd_update(grad, prev)\n    self.assertAlmostEqual(ret['inverted_eigvals'], 1 / 2, delta=1e-6)\n\n  @parameterized.parameters(True, False)\n  def test_nonzero_epsilon(self, relative):\n    \"\"\"Tests that the proper epsilon is added.\"\"\"\n    self.rank = 1\n    self.p = 2\n    size = 4\n\n    eig0 = 2**self.p\n    prev = self._make_eye_state(size, [eig0], 0.0)\n    grad = np.zeros([size, size])\n    ridge_epsilon = 0.1\n    ret = self._fd_update(\n        grad,\n        prev,\n        ridge_epsilon=ridge_epsilon,\n        relative_matrix_epsilon=relative,\n    )\n    eps = 0.1 * (eig0 if relative else 1)\n    self.assertAlmostEqual(eig0 + eps, ret['eigvals'][0], delta=1e-5)\n    v = ret['eigvecs'][:, 0]\n    vec_error = np.max(np.abs(np.array([1, 0, 0, 0]) - v.ravel()))\n    self.assertLessEqual(vec_error, 10 * np.finfo(np.float32).eps, msg=v)\n    self.assertAlmostEqual(ret['tail'], 0.0)\n\n  def _make_rand_state(self, size, eigs, start_tail, padded_size=None):\n    b = self.rng.standard_normal(size=[size, size])\n    b = b.dot(b.T)\n    _, v = np.linalg.eigh(b)\n    return self._make_fd_state(v[:, :self.rank], eigs, start_tail, padded_size)\n\n  @parameterized.parameters(_make_pack_unpack_cases())\n  def test_pack_unpack(self, size, padding, nonzero_rank, null_rank, zero_tail,\n                       has_zeros):\n    self.p = 2\n    self.rank = nonzero_rank + null_rank\n    eigs = np.concatenate(\n        [self.rng.uniform(size=[nonzero_rank]),\n         np.zeros([null_rank])])\n    self.rng.shuffle(eigs)\n    start_tail = 0.0 if zero_tail else self.rng.uniform()\n    padded_size = size + padding\n    packed = self._make_rand_state(size, eigs, start_tail, padded_size)\n    packed = packed.at[-1, -2].set(jnp.asarray(has_zeros).astype(jnp.float32))\n    ret = distributed_shampoo._fd_low_rank_unpack(packed, self.rank)\n    repacked = distributed_shampoo._fd_low_rank_pack(*(ret + (self.rank,)))\n    np.testing.assert_array_equal(packed, repacked)\n\n  @parameterized.parameters(itertools.product([0, 1], [0, 3], [True, False]))\n  def test_basic(self, axis, padding, use_fd_update):\n    \"\"\"Validate low rank returned matrix.\"\"\"\n    size = 5\n    padded_size = size + padding\n    self.rank = 2\n    self.p = 2\n    assert size > self.rank + 2\n\n    grad = self.rng.standard_normal(size=[size, size])\n    for i in range(size):\n      grad[i] /= np.linalg.norm(grad[i])\n    added_cov = grad.dot(grad.T) if axis == 0 else grad.T.dot(grad)\n    top_added_eig = np.linalg.eigvalsh(added_cov).max()\n    start_tail = 0.0\n    eigs = np.array([top_added_eig * 4, top_added_eig / 4])\n    prev = self._make_rand_state(size, eigs, start_tail, padded_size)\n    prev_eigvecs, *_ = distributed_shampoo._fd_low_rank_unpack(prev, self.rank)\n\n    if use_fd_update:\n      grad = distributed_shampoo.frequent_directions_update(\n          np.zeros([]),  # Ignored argument.\n          grad,\n          axis,\n          0.0,  # Ignored argument.\n          0.0,  # Ignored argument.\n      )\n    else:\n      # Align the axis to extract covariance for to dimension 0.\n      grad = grad if axis == 0 else grad.T\n    grad = np.pad(grad, ((0, padding), (0, padding)))\n    updated = self._fd_update(\n        grad,\n        prev,\n        padding_start=size,\n    )\n\n    prev_eigvecs = prev_eigvecs[:size, :]\n    self.assertEqual(np.abs(prev_eigvecs[size:]).sum(), 0.0)\n    half = np.float64(prev_eigvecs) * np.sqrt(np.float64(eigs))\n    full_cov = half.dot(half.T) + added_cov\n    s, new_v = np.linalg.eigh(np.float64(full_cov))\n    s = np.flip(s)  # Descending order.\n    new_v = np.flip(new_v, axis=1)\n    expected_v = new_v[:, :self.rank]\n    rv, re, tail = (updated[k] for k in ['eigvecs', 'eigvals', 'tail'])\n    self.assertEqual(np.abs(rv[size:]).sum(), 0.0)\n    rv = rv[:size, :]\n\n    cross_error = np.abs(rv.T.dot(expected_v))\n    self.assertLessEqual(\n        np.max(np.abs(np.eye(self.rank) - cross_error)), 1e-2, msg=cross_error)\n\n    cutoff = s[self.rank]\n    self.assertAlmostEqual(tail, start_tail + cutoff, delta=1e-2)\n\n    s[:self.rank] -= cutoff\n    np.testing.assert_allclose(s[:self.rank] + tail, re + tail, rtol=1e-2)\n\n  @parameterized.parameters(itertools.product([0, 3], [True, False]))\n  def test_basic_1(self, padding, use_fd_update):\n    \"\"\"Validate low rank returned matrix.\"\"\"\n    size = 5\n    padded_size = size + padding\n    self.rank = 2\n    self.p = 2\n    assert size > self.rank + 2\n\n    grad = self.rng.standard_normal(size=[size])\n    grad /= np.linalg.norm(grad)\n    added_cov = np.multiply.outer(grad, grad)\n    top_added_eig = 1.0  # Normalized vector outer product.\n    start_tail = 0.0\n    eigs = np.array([top_added_eig * 4, top_added_eig / 4])\n    prev = self._make_rand_state(size, eigs, start_tail, padded_size)\n    prev_eigvecs, *_ = distributed_shampoo._fd_low_rank_unpack(prev, self.rank)\n\n    if use_fd_update:\n      axis = 0\n      grad = distributed_shampoo.frequent_directions_update(\n          np.zeros([]),  # Ignored argument.\n          grad,\n          axis,\n          0.0,  # Ignored argument.\n          0.0,  # Ignored argument.\n      )\n    else:\n      grad = np.pad(grad.reshape(-1, 1), ((0, 0), (0, size - 1)))\n    grad = np.pad(grad, ((0, padding), (0, padding)))\n    updated = self._fd_update(\n        grad,\n        prev,\n        padding_start=size,\n    )\n\n    prev_eigvecs = prev_eigvecs[:size, :]\n    self.assertEqual(np.abs(prev_eigvecs[size:]).sum(), 0.0)\n    half = np.float64(prev_eigvecs) * np.sqrt(np.float64(eigs))\n    full_cov = half.dot(half.T) + added_cov\n    s, new_v = np.linalg.eigh(np.float64(full_cov))\n    s = np.flip(s)  # Descending order.\n    new_v = np.flip(new_v, axis=1)\n    expected_v = new_v[:, :self.rank]\n    rv, re, tail = (updated[k] for k in ['eigvecs', 'eigvals', 'tail'])\n    self.assertEqual(np.abs(rv[size:]).sum(), 0.0)\n    rv = rv[:size, :]\n\n    cross_error = np.abs(rv.T.dot(expected_v))\n    self.assertLessEqual(\n        np.max(np.abs(np.eye(self.rank) - cross_error)), 1e-2, msg=cross_error)\n\n    cutoff = s[self.rank]\n    self.assertAlmostEqual(tail, start_tail + cutoff, delta=1e-2)\n\n    s[:self.rank] -= cutoff\n    np.testing.assert_allclose(s[:self.rank] + tail, re + tail, rtol=1e-2)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/sm3_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for distributed_shampoo.\"\"\"\n\nfrom absl.testing import absltest\nimport chex", "from absl.testing import absltest\nimport chex\nimport jax\nimport jax.numpy as jnp\n\nfrom precondition import sm3\n\n\nclass SM3Test(chex.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.init_params = (\n        jnp.array([[0.5, 0.5], [0.5, 0.5]]))\n    self.per_step_updates = (jnp.array([[0.1, -0.1], [0.01, 0.01]]))\n\n  @chex.all_variants(with_pmap=False)\n  def test_sm3_basic(self):\n    params = self.init_params\n\n    optim = sm3.sm3(0.1, 0.9, 0.999)\n    init_fn = self.variant(optim.init)\n    transform_fn = self.variant(optim.update)\n\n    def _update(unused_batch):\n      return transform_fn(self.per_step_updates, state, params)\n    state = init_fn(params)\n    chex.assert_tree_all_finite(state)\n    pmap_fn = jax.pmap(_update, axis_name='batch')\n\n    updates, state = pmap_fn(jnp.array([1.0]))\n    chex.assert_tree_all_finite((params, updates, state))", "class SM3Test(chex.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.init_params = (\n        jnp.array([[0.5, 0.5], [0.5, 0.5]]))\n    self.per_step_updates = (jnp.array([[0.1, -0.1], [0.01, 0.01]]))\n\n  @chex.all_variants(with_pmap=False)\n  def test_sm3_basic(self):\n    params = self.init_params\n\n    optim = sm3.sm3(0.1, 0.9, 0.999)\n    init_fn = self.variant(optim.init)\n    transform_fn = self.variant(optim.update)\n\n    def _update(unused_batch):\n      return transform_fn(self.per_step_updates, state, params)\n    state = init_fn(params)\n    chex.assert_tree_all_finite(state)\n    pmap_fn = jax.pmap(_update, axis_name='batch')\n\n    updates, state = pmap_fn(jnp.array([1.0]))\n    chex.assert_tree_all_finite((params, updates, state))", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/distributed_shampoo.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Distributed Shampoo Implementation.\"\"\"\n\nimport enum\nimport functools", "import enum\nimport functools\nimport itertools\nimport logging\nfrom typing import (Any, Callable, cast, List, NamedTuple, Optional, Sequence,\n                    Tuple, TypeVar, Union)\n\nimport chex\nfrom flax import struct\nimport jax", "from flax import struct\nimport jax\nfrom jax import lax\nfrom jax.experimental import pjit\nfrom jax.experimental.sparse import linalg\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\n\nfrom precondition.quantization_utils import QuantizedValue", "\nfrom precondition.quantization_utils import QuantizedValue\n\n# Dtype for inverse-pth root routine\n# Switch to f64 if you have hardware that supports it. Enable the jax flag\n# jax_enable_x64 for this to work, otherwise it will default to float32.\n_MAT_INV_PTH_ROOT_DTYPE = jnp.float64\n\n# Small epsilon to avoid divide by zero.\n_EPSILON = 1e-25", "# Small epsilon to avoid divide by zero.\n_EPSILON = 1e-25\n\n\ndef preconditioning_compute_steps_schedule(\n    lr_fn,\n    start_preconditioning_compute_steps,\n    end_preconditioning_compute_steps,\n    step,\n):\n  \"\"\"Schedule preconditioning_compute_steps over time.\n\n  Increases preconditioning_compute_steps following learning_rate schedule fn\n  from start_preconditioning_compute_steps to end_preconditioning_compute_steps.\n\n  Rounds the current preconditioning_compute_steps_t value to the nearest 10.\n\n  Args:\n    lr_fn: Learning rate schedule function.\n    start_preconditioning_compute_steps: The value of\n      preconditioning_compute_steps to use at start of training.\n    end_preconditioning_compute_steps: The value of\n      preconditioning_compute_steps to use at end of training.\n    step: Current training step.\n\n  Returns:\n    The current number of steps to compute preconditioners after.\n  \"\"\"\n  base_lr = lr_fn(0)\n  lr = lr_fn(step)\n  decay_factor = lr / base_lr\n\n  preconditioning_compute_steps_t = (\n      start_preconditioning_compute_steps\n      + (1 - decay_factor) * end_preconditioning_compute_steps\n  )\n  return jnp.maximum((preconditioning_compute_steps_t // 10) * 10, 1)", "\n\ndef _default_zero_field():\n  return struct.field(\n      default_factory=functools.partial(jnp.array, 0, jnp.float32))\n\n\nT = TypeVar(\"T\")\n\n\ndef _maybe_ix(ls: Optional[Sequence[T]], ix: int) -> Optional[T]:\n  \"\"\"Return ls[ix] if not None else None.\"\"\"\n  if ls is None:\n    return None\n  return ls[ix]", "\n\ndef _maybe_ix(ls: Optional[Sequence[T]], ix: int) -> Optional[T]:\n  \"\"\"Return ls[ix] if not None else None.\"\"\"\n  if ls is None:\n    return None\n  return ls[ix]\n\n\ndef _maybe(f):\n  \"\"\"Lifts f to Maybe monad; ie return None if first arg is.\"\"\"\n\n  def wrap_f(x, *args, **kwargs):\n    if x is None:\n      return None\n    return f(x, *args, **kwargs)\n\n  return wrap_f", "\ndef _maybe(f):\n  \"\"\"Lifts f to Maybe monad; ie return None if first arg is.\"\"\"\n\n  def wrap_f(x, *args, **kwargs):\n    if x is None:\n      return None\n    return f(x, *args, **kwargs)\n\n  return wrap_f", "\n\nInversePthRootDiagnosticsSubtype = TypeVar(\n    \"InversePthRootDiagnosticsSubtype\", bound=\"InversePthRootDiagnostics\")\n\n\n@struct.dataclass\nclass InversePthRootDiagnostics:\n  \"\"\"Diagnostics for inverse p-th root iterative procedure.\n\n  Given an inverse pth root B = A^(-1/p), contains the average and\n  maximum diagonal and off diagonal absolute entrywise errors between\n  (B^p A) and I.\n  \"\"\"\n  max_diag_error: chex.Array = _default_zero_field()\n  avg_diag_error: chex.Array = _default_zero_field()\n  max_off_diag_error: chex.Array = _default_zero_field()\n  avg_off_diag_error: chex.Array = _default_zero_field()\n  p: chex.Array = _default_zero_field()\n\n  @classmethod\n  def create(cls: type[InversePthRootDiagnosticsSubtype],\n             pth_inverse_root: jnp.ndarray, matrix: jnp.ndarray,\n             p: Union[jnp.ndarray, int]) -> InversePthRootDiagnosticsSubtype:\n    \"\"\"Generates a diagnostics struct from (-1/p) root result.\"\"\"\n    mat_m = jnp.matmul(\n        mat_power(pth_inverse_root, p),\n        matrix,\n        precision=jax.lax.Precision.HIGHEST)\n    num_off_diag_entries = mat_m.size - jnp.diag(mat_m).size\n    diag_error = jnp.abs(jnp.diag(mat_m) - 1).astype(jnp.float32)\n    off_diag_error = jnp.abs(mat_m - jnp.diag(jnp.diag(mat_m))).astype(\n        jnp.float32)\n    return cls(\n        max_diag_error=jnp.max(diag_error).astype(jnp.float32),\n        avg_diag_error=jnp.mean(diag_error).astype(jnp.float32),\n        max_off_diag_error=jnp.max(off_diag_error).astype(jnp.float32),\n        avg_off_diag_error=(jnp.sum(off_diag_error) /\n                            num_off_diag_entries).astype(jnp.float32),\n        p=jnp.array(p, jnp.float32))", "\n\nLOBPCGDiagnosticsSubtype = TypeVar(\n    \"LOBPCGDiagnosticsSubtype\", bound=\"LOBPCGDiagnostics\")\n\n\n@struct.dataclass\nclass LOBPCGDiagnostics:\n  \"\"\"Diagnostics for iterative LOBPCG eigenvalue routine.\n\n  Contains consistency error for LOBPCG eigenvalue routine, which\n  refers to |A v - lambda v| / (lambda + |A v|) for a proposed eigenpair\n  (v, lambda). This metics dataclass retains consistency error\n  and other useful LOBPCG values.\n  \"\"\"\n  lobpcg_iters: chex.Array = _default_zero_field()\n  max_consistency_error: chex.Array = _default_zero_field()\n  avg_consistency_error: chex.Array = _default_zero_field()\n  # Average of absolute value of off-diagonal of V^T V for eigenvalues V.\n  avg_orthogonality_error: chex.Array = _default_zero_field()\n  max_eigenvalue: chex.Array = _default_zero_field()\n  min_eigenvalue: chex.Array = _default_zero_field()\n  num_topk_eigenvectors: chex.Array = _default_zero_field()\n\n  @classmethod\n  def create(cls: type[LOBPCGDiagnosticsSubtype], matrix: jnp.ndarray,\n             eigvals: jnp.ndarray, eigvecs: jnp.ndarray,\n             lobpcg_iters: jnp.ndarray) -> LOBPCGDiagnosticsSubtype:\n    \"\"\"Generates LOBPCG diagnostics from the result of the routine.\"\"\"\n    num_topk = len(eigvals)\n    num_off_diag = num_topk * (num_topk - 1)\n    precision = jax.lax.Precision.HIGHEST\n\n    mat_eigvecs = matrix.dot(eigvecs, precision=precision)\n    consistency_error_unnormalized = jnp.linalg.norm(\n        mat_eigvecs - eigvals * eigvecs, ord=2, axis=0)\n    normalization = jnp.linalg.norm(mat_eigvecs, ord=2, axis=0) + eigvals\n    consistency_error = consistency_error_unnormalized / normalization\n\n    orthogonality_error = eigvecs.T.dot(eigvecs, precision=precision)\n    orthogonality_error -= jnp.diag(jnp.diag(orthogonality_error))\n\n    return cls(\n        lobpcg_iters=jnp.array(lobpcg_iters, jnp.float32),\n        max_consistency_error=jnp.max(consistency_error).astype(jnp.float32),\n        avg_consistency_error=jnp.mean(consistency_error).astype(jnp.float32),\n        avg_orthogonality_error=(jnp.sum(orthogonality_error) /\n                                 num_off_diag).astype(jnp.float32),\n        max_eigenvalue=jnp.max(eigvals).astype(jnp.float32),\n        min_eigenvalue=jnp.min(eigvals).astype(jnp.float32),\n        num_topk_eigenvectors=jnp.array(num_topk, jnp.float32),\n    )", "\n\n@struct.dataclass\nclass FDDiagnostics:\n  \"\"\"Diagnostics for FD updates.\"\"\"\n\n  # General size information: block max size, rank (statically known, but\n  # convenient to have in stats) along with the actual dim we're\n  # preconditioning, size_padding_start.\n  size_max_size: chex.Array = _default_zero_field()\n  size_rank: chex.Array = _default_zero_field()\n  size_padding_start: chex.Array = _default_zero_field()\n\n  # Most recent eigenvalue deflation amount.\n  rho: chex.Array = _default_zero_field()\n  tail: chex.Array = _default_zero_field()\n\n  # Eigenvalue statistics. Sparsity includes zeros\n  # added as a result of zeroing out unstable eigenvalues.\n  eig_sparsity: chex.Array = _default_zero_field()\n  eig_max: chex.Array = _default_zero_field()\n  eig_min: chex.Array = _default_zero_field()\n\n  # Gradient staistics.\n  new_grad_abs_max: chex.Array = _default_zero_field()\n  new_grad_sparsity: chex.Array = _default_zero_field()\n  new_grad_col_sparsity: chex.Array = _default_zero_field()\n  ggt_eig_max: chex.Array = _default_zero_field()\n  ggt_intrinsic_dimension: chex.Array = _default_zero_field()\n\n  # Orthogonality error for eigenvectors.\n  max_ortho_err: chex.Array = _default_zero_field()\n\n  # After deflation, how many eigenvalues were negative?\n  num_neg_eigs: chex.Array = _default_zero_field()\n\n  # After deflation, but before zeroing out numerically unstable eigenvalues,\n  # how many eigenvalues were already zero?\n  num_zero_initial_eigs: chex.Array = _default_zero_field()\n\n  # How many returned singular vectors were more than 1% off unit norm.\n  num_unsafe_norms: chex.Array = _default_zero_field()\n\n  # Of safely-normed vectors, how many had 1% total absolute value in\n  # padding dimensions?\n  num_has_padding: chex.Array = _default_zero_field()\n\n  # Square frobenius norm error from the top-k svd approximation itself.\n  square_frob: chex.Array = _default_zero_field()\n\n  # A heuristic expected square frobenius error, the sum of the remaining\n  # eigenvalues (which currently relies on the f32 svd's computation\n  # of the remaining eigenvalues, but in the future could be the difference\n  # ||A||_F^2 - ||A_k||_F^2. Note this is heuristic, and not the actual\n  # expectation.\n  heuristic_frob: chex.Array = _default_zero_field()\n\n  # Among the expected-nonzero entries in the matrix we're decomposing,\n  # what is the average entrywise error from the top-k approximation?\n  # ||A - A_k||_1 / size(A)\n  entrywise_err: chex.Array = _default_zero_field()\n\n  # Trace ||A||_F^2 = tr A^2, which normalizes the frobenius error.\n  # It's equal to the sum of squared singular values of A.\n  total_frob: chex.Array = _default_zero_field()\n\n  @classmethod\n  def create(\n      cls: type[\"FDDiagnostics\"],\n      rho: jnp.ndarray,\n      tail: jnp.ndarray,\n      eigs: jnp.ndarray,\n      new_grad: jnp.ndarray,\n      eigvecs: jnp.ndarray,\n      padding_start: jnp.ndarray,\n      max_size: jnp.ndarray,\n      num_neg_eigs: jnp.ndarray,\n      num_zero_initial_eigs: jnp.ndarray,\n      num_unsafe_norms: jnp.ndarray,\n      num_has_padding: jnp.ndarray,\n      frob: jnp.ndarray,\n      expected_frob: jnp.ndarray,\n      entrywise_svd_err: jnp.ndarray,\n      total_frob: jnp.ndarray,\n  ) -> \"FDDiagnostics\":\n    \"\"\"Generates FD diagnostics from the result of the routine.\"\"\"\n    max_size, rank = eigvecs.shape\n    assert rank + 2 < max_size\n    assert eigs.shape == (rank,), eigs.shape\n\n    eig_sparsity = (eigs == 0).mean()\n    eig_max = jnp.max(eigs).astype(jnp.float32)\n    eig_min = jnp.min(eigs, where=eigs.astype(bool), initial=eig_max)\n\n    padding_ix = (jnp.arange(max_size) >= padding_start)\n    nonpadding_ix = 1 - padding_ix\n    mask = nonpadding_ix * nonpadding_ix[:, jnp.newaxis]\n    new_grad *= mask\n    new_grad_abs_max = jnp.max(jnp.abs(new_grad)).astype(jnp.float32)\n    new_grad_sparsity = jnp.sum(mask * (new_grad == 0)).astype(\n        jnp.float32) / padding_start**2\n    new_grad_col_l1 = jnp.abs(new_grad).sum(axis=0)\n    new_grad_col_sparsity = (\n        jnp.sum(nonpadding_ix * (new_grad_col_l1 == 0)).astype(jnp.float32)\n        / padding_start\n    )\n\n    # TODO(vladf): consider power iteration instead.\n    ggt = jnp.matmul(new_grad, new_grad.T, precision=jax.lax.Precision.HIGHEST)\n    ggt_eigs = jnp.linalg.eigvalsh(ggt)\n    ggt_eig_max = jnp.max(ggt_eigs)\n    ggt_intrinsic_dimension = jnp.trace(ggt) / ggt_eig_max\n\n    cross = jnp.matmul(eigvecs.T, eigvecs, precision=jax.lax.Precision.HIGHEST)\n    ortho_err = jnp.abs(cross - jnp.diag(jnp.diag(cross)))\n    max_ortho_err = jnp.max(ortho_err).astype(jnp.float32)\n\n    return cls(\n        size_padding_start=jnp.array(padding_start).astype(jnp.float32),\n        size_max_size=jnp.array(max_size).astype(jnp.float32),\n        size_rank=jnp.array(eigvecs.shape[1]).astype(jnp.float32),\n        rho=rho.astype(jnp.float32),\n        tail=tail.astype(jnp.float32),\n        eig_sparsity=eig_sparsity,\n        eig_max=eig_max,\n        eig_min=eig_min,\n        new_grad_abs_max=new_grad_abs_max,\n        new_grad_sparsity=new_grad_sparsity,\n        new_grad_col_sparsity=new_grad_col_sparsity,\n        ggt_eig_max=ggt_eig_max,\n        ggt_intrinsic_dimension=ggt_intrinsic_dimension,\n        max_ortho_err=max_ortho_err,\n        num_neg_eigs=num_neg_eigs.astype(jnp.float32),\n        num_zero_initial_eigs=num_zero_initial_eigs.astype(jnp.float32),\n        num_unsafe_norms=num_unsafe_norms.astype(jnp.float32),\n        num_has_padding=num_has_padding.astype(jnp.float32),\n        square_frob=frob,\n        heuristic_frob=expected_frob,\n        entrywise_err=entrywise_svd_err,\n        total_frob=total_frob,\n    )", "\n\n@struct.dataclass\nclass TrainingMetrics:\n  \"\"\"Diagnostic metrics from training.\"\"\"\n  # Error for inverse-pth roots.\n  inverse_pth_root_errors: chex.Array = _default_zero_field()\n  # Iteration count for inverse-pth roots.\n  inverse_pth_root_iters: chex.Array = _default_zero_field()\n  # If final iteration error increases sufficiently, iteration terminates early.\n  # This field records the ratio of the final iteration error.\n  final_error_ratio: chex.Array = _default_zero_field()\n  # Max eigen value from either the power iteration or from LOBPCG.\n  max_eigen_value: chex.Array = _default_zero_field()\n  # Total retries of inverse pth root iterative method.\n  total_retries: chex.Array = _default_zero_field()\n\n  lobpcg_diagnostics: LOBPCGDiagnostics = struct.field(\n      default_factory=LOBPCGDiagnostics)\n  # Rich matrix entrywise error diagnostics, if enabled.\n  inverse_pth_root_diagnostics: InversePthRootDiagnostics = struct.field(\n      default_factory=InversePthRootDiagnostics)\n  # Diagnostics applied to the conditioned p-th root problem, after top\n  # eigenvectors are removed, if LOBPCG is being applied.\n  conditioned_inverse_pth_root_diagnostics: InversePthRootDiagnostics = (\n      struct.field(default_factory=InversePthRootDiagnostics))\n  fd: Union[FDDiagnostics,\n            optax.MaskedNode] = struct.field(default_factory=optax.MaskedNode)", "\n\n# Per parameter optimizer state used in data-parallel training.\nclass ParameterStats(NamedTuple):\n  \"\"\"State associated to each parameter of the model being trained.\"\"\"\n  diagonal_statistics: QuantizedValue  # Accumulator for diagonal preconditioner\n  statistics: Optional[List[Any]]  # Statistics (QuantizedValue, chex.Array)\n  preconditioners: List[Any]  # Preconditioners (QuantizedValue, chex.Array)\n  diagonal_momentum: QuantizedValue  # Momentum for the diagonal preconditioner\n  momentum: QuantizedValue  # Momentum for the shampoo preconditioner\n  avg_grad: Union[chex.Array, optax.MaskedNode]  # Average gradient for FD.\n  training_metrics: Union[TrainingMetrics, optax.MaskedNode]  # Optional.", "\n\n# For training extremely large model; We keep a global state with a concatenated\n# statistics and preconditioner states for all vars. This is so that we can\n# annotate the leading axis to be sharded to save memory at the cost of\n# communication.\n@struct.dataclass\nclass GlobalShardedParameterStats:\n  statistics: chex.Array  # Statistics\n  preconditioners: chex.Array  # Preconditioners\n  exponents: chex.Array  # exponents", "\n\n# These are per-parameter local states; All statistics here mirror the parameter\n# Thus the sharding is copied over from the param specification.\n@struct.dataclass\nclass LocalShardedParameterStats:\n  \"\"\"State associated to each parameter of the model being trained.\"\"\"\n  diagonal_statistics: QuantizedValue  # Accumulator for diagonal preconditioner\n  diagonal_momentum: QuantizedValue  # Momentum for the diagonal preconditioner\n  momentum: QuantizedValue  # Momentum for the shampoo preconditioner\n  avg_grad: Union[chex.Array, optax.MaskedNode]  # Average gradient for FD.\n  training_metrics: Union[TrainingMetrics, optax.MaskedNode]\n  index_start: np.int32 = struct.field(\n      pytree_node=False)  # Index into global statistics array\n  sizes: Any = struct.field(pytree_node=False)  # Sizes of the statistics.", "\n\ndef init_avg_grad(\n    param: chex.Array,\n    frequent_directions: bool) -> Union[chex.Array, optax.MaskedNode]:\n  \"\"\"Initializes a gradient average variable, if active.\"\"\"\n  if not frequent_directions:\n    return optax.MaskedNode()\n  return jnp.zeros_like(param)\n", "\n\ndef init_avg_grad_shape(\n    param: chex.Array,\n    frequent_directions: bool) -> Union[Any, optax.MaskedNode]:\n  \"\"\"Returns gradient average variable shape/dtype, if active.\"\"\"\n  if not frequent_directions:\n    return optax.MaskedNode()\n  return [param.shape, param.dtype]\n", "\n\ndef init_avg_grad_pspec(\n    param: jax.sharding.PartitionSpec,\n    frequent_directions: bool) -> Union[Any, optax.MaskedNode]:\n  if not frequent_directions:\n    return optax.MaskedNode()\n  return param\n\n\ndef default_training_metrics(\n    generate_fd_metrics: bool = False,\n):\n  \"\"\"Create a default TrainingMetrics.\"\"\"\n  if generate_fd_metrics:\n    return TrainingMetrics(fd=FDDiagnostics())\n  return TrainingMetrics()", "\n\ndef default_training_metrics(\n    generate_fd_metrics: bool = False,\n):\n  \"\"\"Create a default TrainingMetrics.\"\"\"\n  if generate_fd_metrics:\n    return TrainingMetrics(fd=FDDiagnostics())\n  return TrainingMetrics()\n", "\n\ndef init_training_metrics(\n    num_statistics: int,\n    generate_training_metrics: bool,\n    generate_fd_metrics: bool = False,\n) -> Union[TrainingMetrics, optax.MaskedNode]:\n  \"\"\"Initialize TrainingMetrics, masked if disabled.\"\"\"\n  if not generate_training_metrics:\n    return optax.MaskedNode()\n  return jax.tree_map(\n      functools.partial(jnp.repeat, repeats=num_statistics),\n      default_training_metrics(\n          generate_fd_metrics=generate_fd_metrics,\n      ))", "\n\ndef init_training_metrics_shapes(\n    num_statistics: int,\n    generate_training_metrics: bool,\n    generate_fd_metrics: bool = False,\n) -> Union[TrainingMetrics, optax.MaskedNode]:\n  \"\"\"Initialize training metrics shape/dtype.\"\"\"\n  seed = init_training_metrics(\n      num_statistics,\n      generate_training_metrics,\n      generate_fd_metrics=generate_fd_metrics,\n  )\n  return jax.tree_map(lambda arr: [list(arr.shape), arr.dtype], seed)", "\n\ndef init_training_metrics_pspec(\n    generate_training_metrics: bool,\n    generate_fd_metrics: bool = False,\n) -> Union[TrainingMetrics, optax.MaskedNode]:\n  \"\"\"Initialize training metrics partition specification.\"\"\"\n  if not generate_training_metrics:\n    return optax.MaskedNode()\n  return jax.tree_map(\n      lambda _: jax.sharding.PartitionSpec(),\n      default_training_metrics(\n          generate_fd_metrics=generate_fd_metrics,\n      ))", "\n\nclass ShardedShampooStats(NamedTuple):\n  \"\"\"Shampoo state in sharded mode.\"\"\"\n  global_stats: Any\n  local_stats: Any\n\n\nclass ShampooState(NamedTuple):\n  count: chex.Array\n  stats: Any", "class ShampooState(NamedTuple):\n  count: chex.Array\n  stats: Any\n\n\nclass InitFnState(NamedTuple):\n  init_fn: Any\n  pspec_fn: Any\n  shape_and_dtype_fn: Any\n", "\n\nclass GraftingType(enum.IntEnum):\n  NONE = 0\n  SGD = 1\n  ADAGRAD = 2\n  RMSPROP = 3\n  RMSPROP_NORMALIZED = 4\n  SQRT_N = 5\n  ADAGRAD_NORMALIZED = 6", "\n\nclass PreconditionerType(enum.IntEnum):\n  # Default, computes preconditioner for each dim\n  ALL = 1\n  # One sided Shampoo, in this cases only on input dim.\n  # Assumes last dim is always the output dim and everything else input dim.\n  INPUT = 2\n  # One sided Shampoo, in this cases only on output dim.\n  # Assumes last dim is always the output dim and everything else input dim.\n  OUTPUT = 3", "\n\ndef _precond_dim(compression_rank, dim):\n  \"\"\"Returns compressed dimension if it's smaller than dim with metadata.\"\"\"\n  if not compression_rank:\n    return dim\n  # If storing the compressed rank preconditioner and metadata\n  # would require more memory, just don't compress.\n  #\n  # See compression_rank argument of distributed_shampoo() for why it can\n  # be negative.\n  compressed_size = abs(compression_rank) + 2\n  if compressed_size >= dim:\n    return dim\n  return compressed_size", "\n\ndef _should_compress(compression_rank: int, dim: jnp.ndarray):\n  \"\"\"Consistent with _precond_dim, whether we should use compression.\"\"\"\n  return compression_rank != 0 and abs(compression_rank) + 2 < dim\n\n\ndef _low_rank_unpack(preconditioner, compression_rank):\n  \"\"\"Unpacks low-rank preconditioning params from rectangular preconditioner.\"\"\"\n  eigvecs, eigvals, inverted_eigvals, const, tail, has_zeros = (\n      _fd_low_rank_unpack(preconditioner, compression_rank))\n  del eigvals, tail\n  return eigvecs, inverted_eigvals, const, has_zeros", "\n\ndef _low_rank_pack(eigvecs, eigvals, const, compression_rank):\n  \"\"\"Packs low-rank preconditioning params from rectangular preconditioner.\"\"\"\n  has_zeros = False\n  return _fd_low_rank_pack(eigvecs, jnp.zeros_like(eigvals), eigvals, const,\n                           0.0, has_zeros, compression_rank)\n\n\ndef _fd_low_rank_unpack(preconditioner, compression_rank):\n  \"\"\"Unpacks like in low rank but includes original eigvals.\"\"\"\n  r = abs(compression_rank)\n  assert r != 0, compression_rank\n  assert len(preconditioner.shape) == 2, preconditioner.shape\n  dim, storage_dim = preconditioner.shape\n  assert storage_dim < dim\n  assert storage_dim == r + 2\n  eigvecs = preconditioner[:, :r]\n  inverted_eigvals = preconditioner[:r, -2]\n  const = preconditioner[0, -1]\n  eigvals = preconditioner[-r:, -1]\n  tail = preconditioner[1, -1]\n  has_zeros = preconditioner[-1, -2].astype(bool)\n  return eigvecs, eigvals, inverted_eigvals, const, tail, has_zeros", "\ndef _fd_low_rank_unpack(preconditioner, compression_rank):\n  \"\"\"Unpacks like in low rank but includes original eigvals.\"\"\"\n  r = abs(compression_rank)\n  assert r != 0, compression_rank\n  assert len(preconditioner.shape) == 2, preconditioner.shape\n  dim, storage_dim = preconditioner.shape\n  assert storage_dim < dim\n  assert storage_dim == r + 2\n  eigvecs = preconditioner[:, :r]\n  inverted_eigvals = preconditioner[:r, -2]\n  const = preconditioner[0, -1]\n  eigvals = preconditioner[-r:, -1]\n  tail = preconditioner[1, -1]\n  has_zeros = preconditioner[-1, -2].astype(bool)\n  return eigvecs, eigvals, inverted_eigvals, const, tail, has_zeros", "\n\ndef _fd_low_rank_pack(eigvecs, deflated_eigs, inverted_eigs, new_const,\n                      new_tail, has_zeros, rank):\n  \"\"\"Inverses _fd_low_rank_unpack.\"\"\"\n  # Note this respects low_rank_root packing.\n  rank = abs(rank)\n  assert rank > 0\n  d = eigvecs.shape[0]\n  assert eigvecs.shape[1] == rank\n  assert eigvecs.ndim == 2\n  assert list(deflated_eigs.shape) == [rank], deflated_eigs.shape\n  assert list(inverted_eigs.shape) == [rank], inverted_eigs.shape\n  assert _precond_dim(rank, d) == rank + 2\n  assert _precond_dim(rank, d) < d\n  precond = jnp.zeros((d, rank + 2))\n  precond = precond.at[:, :rank].set(eigvecs)\n  precond = precond.at[:rank, -2].set(inverted_eigs)\n  precond = precond.at[0, -1].set(new_const)\n  precond = precond.at[1, -1].set(new_tail)\n  precond = precond.at[-rank:, -1].set(deflated_eigs)\n  precond = precond.at[-1, -2].set(jnp.asarray(has_zeros).astype(jnp.float32))\n  return precond", "\n\ndef power_iteration(\n    matrix: jnp.ndarray,\n    num_iters: int = 100,\n    error_tolerance: float = 1e-6,\n    precision: lax.Precision = lax.Precision.HIGHEST,\n    padding_start: Union[int, jnp.ndarray, None] = None,\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n  r\"\"\"Power iteration algorithm.\n\n  The power iteration algorithm takes a symmetric PSD matrix `A`, and produces\n  a scalar `\\lambda` , which is the greatest (in absolute value) eigenvalue\n  of `A`, and a vector v, which is the corresponding eigenvector of `A`.\n\n  References:\n    [Wikipedia, 2021](https://en.wikipedia.org/wiki/Power_iteration)\n\n  Args:\n    matrix: the symmetric PSD matrix.\n    num_iters: Number of iterations.\n    error_tolerance: Iterative exit condition.\n    precision: precision XLA related flag, the available options are: a)\n      lax.Precision.DEFAULT (better step time, but not precise) b)\n      lax.Precision.HIGH (increased precision, slower) c) lax.Precision.HIGHEST\n      (best possible precision, slowest)\n    padding_start: if set, assumes rows and columns after padding_start are\n      zero.\n\n  Returns:\n    eigen vector, eigen value\n  \"\"\"\n  matrix_size = matrix.shape[-1]\n\n  def _iter_condition(state):\n    i, unused_v, unused_s, unused_s_v, run_step = state\n    return jnp.logical_and(i < num_iters, run_step)\n\n  def _iter_body(state):\n    \"\"\"One step of power iteration.\"\"\"\n    i, new_v, s, s_v, unused_run_step = state\n    new_v = new_v / jnp.linalg.norm(new_v)\n\n    s_v = jnp.einsum(\"ij,j->i\", matrix, new_v, precision=precision)\n    s_new = jnp.einsum(\"i,i->\", new_v, s_v, precision=precision)\n    return (i + 1, s_v, s_new, s_v,\n            jnp.greater(jnp.abs(s_new - s), error_tolerance))\n\n  # Figure out how to use step as seed for random.\n  v_0 = np.random.RandomState(1729).uniform(-1.0, 1.0,\n                                            matrix_size).astype(matrix.dtype)\n  v_0 = jnp.array(v_0)\n  if padding_start is not None:\n    v_0 *= (jnp.arange(len(v_0), dtype=jnp.int32) < padding_start)\n\n  init_state = tuple([0, v_0, jnp.zeros([], dtype=matrix.dtype), v_0, True])\n  _, v_out, s_out, _, _ = lax.while_loop(_iter_condition, _iter_body,\n                                         init_state)\n  v_out = v_out / jnp.linalg.norm(v_out)\n  return v_out, s_out", "\n\ndef mat_power(\n    mat_m: jnp.ndarray,\n    p: Union[int, jnp.ndarray],\n    precision: lax.Precision = lax.Precision.HIGHEST,\n) -> jnp.ndarray:\n  \"\"\"A simple matrix power method. M^p where p can be TracedValue.\"\"\"\n  power = jnp.eye(mat_m.shape[0], dtype=_MAT_INV_PTH_ROOT_DTYPE)\n\n  def _iter_condition(state):\n    i, _, _ = state\n    return i > 0\n\n  def _iter_body(state):\n    i, power, mat = state\n\n    power = jax.lax.cond(i % 2 == 1,\n                         lambda: jnp.matmul(mat, power, precision=precision),\n                         lambda: power)\n    i //= 2\n    mat = jnp.matmul(mat, mat, precision=precision)\n    return i, power, mat\n\n  _, result, _ = lax.while_loop(_iter_condition, _iter_body, (p, power, mat_m))\n  return result", "\n\ndef _pth_root_difference(w: jnp.ndarray, alpha: jnp.ndarray, beta: jnp.ndarray,\n                         p: Union[int, jnp.ndarray]) -> jnp.ndarray:\n  \"\"\"Computes (w+alpha)^(-1/p)-(w+beta)^(-1/p).\"\"\"\n\n  a = w + alpha\n  b = w + beta\n  a_minus_b = alpha - beta\n  exp = -1 / p\n\n  def _stable_subtract(b, a_minus_b):\n    # Mathematically identical to the target expression, with (w+beta)^(-1/p)\n    # term factored out and w cancellation in the subtraction.\n    return (b**exp) * jnp.expm1(exp * jnp.log1p(a_minus_b / b))\n\n  return jnp.where(\n      # Choose the branch with the best log1p approximation.\n      jnp.abs(a_minus_b / b) < jnp.abs(a_minus_b / a),\n      -_stable_subtract(a, -a_minus_b),\n      _stable_subtract(b, a_minus_b))", "\n\ndef matrix_inverse_pth_root(\n    matrix: jnp.ndarray,\n    p: Union[int, jnp.ndarray],\n    num_iters: int = 100,\n    ridge_epsilon: float = 1e-6,\n    error_tolerance: float = 1e-6,\n    precision: lax.Precision = lax.Precision.HIGHEST,\n    relative_matrix_epsilon: bool = True,\n    lobpcg_topk_precondition: int = 0,\n    lobpcg_max_iter: int = 0,\n    padding_start: Union[int, jnp.ndarray, None] = None,\n    prev: Optional[jnp.ndarray] = None,\n    eigh=False,\n) -> Tuple[jnp.ndarray, TrainingMetrics]:\n  \"\"\"Computes `matrix^(-1/p)`, where `p` is a positive integer.\n\n  This function uses the Eigh or Coupled newton iterations algorithm for\n  the computation of a matrix's inverse pth root.\n\n\n  References:\n    [Functions of Matrices, Theory and Computation,\n     Nicholas J Higham, Pg 184, Eq 7.18](\n     https://epubs.siam.org/doi/book/10.1137/1.9780898717778)\n\n  Args:\n    matrix: the symmetric PSD matrix whose power it to be computed\n    p: exponent, for p a positive integer.\n    num_iters: Maximum number of iterations.\n    ridge_epsilon: Ridge epsilon added to make the matrix positive definite.\n    error_tolerance: Error indicator, useful for early termination.\n    precision: precision XLA related flag, the available options are: a)\n      lax.Precision.DEFAULT (better step time, but not precise) b)\n      lax.Precision.HIGH (increased precision, slower) c) lax.Precision.HIGHEST\n      (best possible precision, slowest)\n    relative_matrix_epsilon: Whether to use relative epsilon to the max eigen\n      value when computing inverse-pth root.\n    lobpcg_topk_precondition: If nonzero, specifies the number of top\n      eigenvectors to subtract out before performing LOBPCG. Note this makes\n      relative_matrix_epsilon essentially free.\n    lobpcg_max_iter: Maximum iteration count for LOBPCG, defaults to\n      `lobpcg_topk_precondition`.\n    padding_start: If the input matrix was padded, then zeros out columns and\n      rows at the padding start.\n    prev: previous iteration's solution, zero-padded (unused)\n    eigh: If True, uses eigh for inverse-pth root computation.\n\n  Returns:\n    `(matrix + eps)^(-1/p)` and error metrics.\n\n    Note `eps` is not added to zeroed out padding rows and\n    columns. `eps` is just `ridge_epsilon` if\n    `relative_matrix_epsilon` is set to `False`, otherwise, it is the\n    ridge epsilon value scaled by the derived maximum eigenvalue of\n    the input matrix.\n  \"\"\"\n\n  if eigh:\n    return matrix_inverse_pth_root_eigh(matrix, p, ridge_epsilon,\n                                        error_tolerance, precision,\n                                        relative_matrix_epsilon, padding_start,\n                                        prev)\n  del prev\n\n  assert matrix.shape[0] == matrix.shape[1]\n\n  # We use _MAT_INV_PTH_ROOT_DTYPE for the matrix inverse pth root.\n  # Switch to f64 if you have hardware that supports it. Enable the jax flag\n  # jax_enable_x64 for this to work.\n  matrix_size = matrix.shape[0]\n  orig_dtype = matrix.dtype\n  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)\n  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)\n  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)\n\n  if padding_start is not None:\n    # Zero out padding in identity as well for convergence checks.\n    ix = (jnp.arange(matrix_size, dtype=jnp.int32) < padding_start).astype(\n        matrix.dtype)\n    matrix *= ix[jnp.newaxis, :]\n    matrix *= ix[:, jnp.newaxis]\n    identity *= ix\n\n  original_matrix = matrix\n\n  # Only used in lobpcg branches, but required by pytype.\n  eigvals, eigvecs, lobpcg_diagnostics = None, None, None\n  if lobpcg_topk_precondition > 0:\n    # TODO(vladf): reuse previous top-k as the initial search directions\n    pad_shape = (matrix_size - lobpcg_topk_precondition,\n                 lobpcg_topk_precondition)\n    search_dirs = jnp.concatenate(\n        (jnp.eye(lobpcg_topk_precondition), jnp.zeros(pad_shape)), axis=0)\n    eigvals, eigvecs, lobpcg_iters = linalg.lobpcg_standard(\n        matrix, search_dirs,\n        lobpcg_topk_precondition if lobpcg_max_iter == 0 else lobpcg_max_iter)\n    lobpcg_diagnostics = LOBPCGDiagnostics.create(\n        matrix,\n        eigvals,\n        eigvecs,\n        lobpcg_iters,\n    )\n\n    # The minimal eigenvalue among top-k becomes the maximal one in the whole\n    # matrix after deflation.\n    deflation = eigvals - jnp.min(eigvals)\n    scaled_vecs = eigvecs * jnp.sqrt(deflation)\n\n    # Deflate out top eigenvectors to reduce matrix condition number.\n    matrix -= scaled_vecs.dot(\n        scaled_vecs.T, precision=jax.lax.Precision.HIGHEST)\n\n  if relative_matrix_epsilon:\n    if eigvals is not None:\n      max_ev = jnp.max(eigvals)\n    else:\n      # Only use power iteration if lobpcg wasn't already used to derive the\n      # top eigenvalue.\n      _, max_ev = power_iteration(\n          matrix=matrix,\n          num_iters=100,\n          error_tolerance=1e-6,\n          precision=precision,\n          padding_start=padding_start)\n  else:\n    # Use absolute matrix epsilon scaling otherwise.\n    max_ev = 1.0\n\n  ridge_epsilon = ridge_epsilon * jnp.maximum(max_ev, _EPSILON)\n\n  # Sometimes error increases after an iteration before decreasing and\n  # converging. 1.2 factor is used to bound the maximal allowed increase.\n  max_error_ratio = 1.2\n\n  def _iter_condition(state):\n    i, unused_mat_m, unused_mat_h, unused_old_mat_h, error, error_ratio = state\n    error_above_threshold = jnp.logical_and(error > error_tolerance,\n                                            error_ratio < max_error_ratio)\n    return jnp.logical_and(i < num_iters, error_above_threshold)\n\n  def _iter_body(state):\n    (i, mat_m, mat_h, unused_old_mat_h, error, unused_error_ratio) = state\n    mat_m_i = (1 - alpha) * identity + alpha * mat_m\n    new_mat_m = jnp.matmul(mat_power(mat_m_i, p), mat_m, precision=precision)\n    new_mat_h = jnp.matmul(mat_h, mat_m_i, precision=precision)\n    new_error = jnp.max(jnp.abs(new_mat_m - identity))\n    return (i + 1, new_mat_m, new_mat_h, mat_h, new_error, new_error / error)\n\n  if matrix_size == 1:\n    damped_matrix = matrix + ridge_epsilon\n    resultant_mat_h = damped_matrix**alpha\n    error = jnp.array(0, jnp.float32)\n    iters = 0\n    error_ratio = 0.0\n  else:\n\n    retry_loop_error_threshold = 0.05\n    num_tries = 6\n    init_outer_state = tuple([0, identity, 1000.0, 100, 1.0, True])\n\n    def _outer_iter_condition_fn(state):\n      i, _, _, _, _, iter_failed = state\n      return jnp.logical_and(iter_failed, i < num_tries)\n\n    def _outer_body_fn(state):\n      i, _, _, _, _, _ = state\n      # Update the epsilon based on the loop iteration.\n      damped_matrix = matrix + (ridge_epsilon * (10**i) * identity)\n      z = (1 + p) / (2 * jnp.linalg.norm(damped_matrix))\n      new_mat_m_0 = damped_matrix * z\n      new_error = jnp.max(jnp.abs(new_mat_m_0 - identity))\n      new_mat_h_0 = identity * jnp.power(z, 1.0 / p)\n      init_state = tuple(\n          [0, new_mat_m_0, new_mat_h_0, new_mat_h_0, new_error, 1.0])\n      iters, mat_m, mat_h, old_mat_h, error, error_ratio = lax.while_loop(\n          _iter_condition, _iter_body, init_state)\n      error = jnp.max(jnp.abs(mat_m - identity)).astype(jnp.float32)\n      is_converged = jnp.asarray(error_ratio < max_error_ratio, old_mat_h.dtype)\n      resultant_mat_h = is_converged * mat_h + (1 - is_converged) * old_mat_h\n      return (i + 1, resultant_mat_h, error, iters, error_ratio,\n              error > retry_loop_error_threshold)\n\n    total_retries, resultant_mat_h, error, iters, error_ratio, _ = jax.lax.while_loop(\n        _outer_iter_condition_fn, _outer_body_fn, init_outer_state)\n\n  conditioned_resultant_mat = resultant_mat_h\n\n  if lobpcg_topk_precondition > 0:\n    # Since we deflated the top eigenvectors prior to p-th root inverse,\n    # the resultant matrix has larger eigenvalues associated with those\n    # same eigenvectors, which we need to now re-deflate.\n    #\n    # Note that _pth_root_difference returns positive values for this\n    # particular argument ordering as min(eigvals) <= eigvals for the\n    # jnp.sqrt below.\n    pth_diff = _pth_root_difference(ridge_epsilon, jnp.min(eigvals), eigvals, p)\n    scaled_vecs = eigvecs * jnp.sqrt(pth_diff)\n    resultant_mat_h = conditioned_resultant_mat - scaled_vecs.dot(\n        scaled_vecs.T, precision=jax.lax.Precision.HIGHEST)\n\n  error_metrics = TrainingMetrics(\n      inverse_pth_root_errors=jnp.array(error, jnp.float32),\n      inverse_pth_root_iters=jnp.array(iters, jnp.float32),\n      final_error_ratio=jnp.array(error_ratio, jnp.float32),\n      max_eigen_value=jnp.array(max_ev, jnp.float32),\n      total_retries=jnp.array(total_retries, jnp.float32))\n\n  if lobpcg_topk_precondition > 0:\n    damped_matrix = matrix + (ridge_epsilon * (10**total_retries) * identity)\n    conditioned_diagnostics = InversePthRootDiagnostics.create(\n        conditioned_resultant_mat, damped_matrix, p)\n    unconditioned_damped_matrix = original_matrix + ridge_epsilon * identity\n    unconditioned_diagnostics = InversePthRootDiagnostics.create(\n        resultant_mat_h, unconditioned_damped_matrix, p)\n    # The max entrywise error in error_metrics.inverse_pth_root_errors refers\n    # to what was derived from the inverse pth root iteration, which with\n    # LOBPCG refers to the conditioned problem. Make sure to use the error\n    # from the unconditioned problem.\n    unconditional_errors = jnp.maximum(\n        unconditioned_diagnostics.max_diag_error,\n        unconditioned_diagnostics.max_off_diag_error)\n    error_metrics = error_metrics.replace(\n        inverse_pth_root_errors=unconditional_errors,\n        lobpcg_diagnostics=lobpcg_diagnostics,\n        conditioned_inverse_pth_root_diagnostics=conditioned_diagnostics,\n        inverse_pth_root_diagnostics=unconditioned_diagnostics,\n    )\n\n  if padding_start is not None:\n    # Occasionally, pure-padding matrices are handed to the inversion routine\n    # due to some TPU hosts not having the same number of preconditioning\n    # matrices.\n    resultant_mat_h = jnp.where(padding_start == 0, 0.0, resultant_mat_h)\n    error = jnp.where(padding_start == 0, 0.0,\n                      error_metrics.inverse_pth_root_errors)\n    error_metrics = error_metrics.replace(inverse_pth_root_errors=error)\n\n  resultant_mat_h = jnp.asarray(resultant_mat_h, orig_dtype)\n  return resultant_mat_h, error_metrics", "\n\ndef matrix_inverse_pth_root_eigh(\n    matrix: jnp.ndarray,\n    p: Union[int, jnp.ndarray],\n    ridge_epsilon: float = 1e-6,\n    error_tolerance: float = 1e-6,\n    precision: lax.Precision = lax.Precision.HIGHEST,\n    relative_matrix_epsilon: bool = True,\n    padding_start: Union[int, jnp.ndarray, None] = None,\n    prev: Optional[jnp.ndarray] = None,\n) -> Tuple[jnp.ndarray, TrainingMetrics]:\n  \"\"\"Computes `matrix^(-1/p)`, where `p` is a positive integer.\n\n  This function uses eigh for the computation of a matrix's inverse pth\n  root.\n\n  Args:\n    matrix: the symmetric PSD matrix whose power it to be computed\n    p: exponent, for p a positive integer.\n    ridge_epsilon: Ridge epsilon added to make the matrix positive definite.\n    error_tolerance: Error indicator, useful for early termination.\n    precision: precision XLA related flag, the available options are: a)\n      lax.Precision.DEFAULT (better step time, but not precise) b)\n      lax.Precision.HIGH (increased precision, slower) c) lax.Precision.HIGHEST\n      (best possible precision, slowest)\n    relative_matrix_epsilon: Whether to use relative epsilon to the max eigen\n      value when computing inverse-pth root.\n    padding_start: If the input matrix was padded, then zeros out columns and\n      rows at the padding start.\n    prev: previous iteration's solution, zero-padded (unused)\n\n  Returns:\n    `(matrix + eps)^(-1/p)` and error metrics.\n\n    Note `eps` is not added to zeroed out padding rows and\n    columns. `eps` is just `ridge_epsilon` if\n    `relative_matrix_epsilon` is set to `False`, otherwise, it is the\n    ridge epsilon value scaled by the derived maximum eigenvalue of\n    the input matrix.\n  \"\"\"\n  del prev\n  assert matrix.shape[0] == matrix.shape[1]\n  matrix_size = matrix.shape[0]\n  orig_dtype = matrix.dtype\n  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)\n  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)\n  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)\n  if padding_start is not None:\n    ix = (jnp.arange(matrix_size, dtype=jnp.int32) < padding_start).astype(\n        matrix.dtype)\n    matrix *= ix[jnp.newaxis, :]\n    matrix *= ix[:, jnp.newaxis]\n    identity *= ix\n  if relative_matrix_epsilon:\n    _, max_ev = power_iteration(\n        matrix=matrix,\n        num_iters=100,\n        error_tolerance=error_tolerance,\n        precision=precision,\n        padding_start=padding_start)\n  else:\n    # Use absolute matrix epsilon scaling otherwise.\n    max_ev = 1.0\n  ridge_epsilon = ridge_epsilon * jnp.maximum(max_ev, error_tolerance)\n  regularized_input = matrix + ridge_epsilon * identity\n  e, u = jnp.linalg.eigh(regularized_input)\n  # Due to padding, we may have to zero out eigenvalues.\n  if padding_start is not None:\n    e *= jnp.flip(ix)\n  mm = functools.partial(jnp.matmul, precision=precision)\n  inv_e = jnp.where(e == 0.0, 0.0,\n                    jnp.power(jnp.maximum(e, ridge_epsilon), alpha))\n  val = mm(mm(u, jnp.diag(inv_e)), u.T)\n  root = u * jnp.sqrt(inv_e)\n  val = mm(root, root.T)\n  recovered_e = mm(u.T, mm(regularized_input, u))\n  eig_error = recovered_e - jnp.diag(e)\n  if padding_start is not None:\n    eig_error *= jnp.flip(ix)\n  error = jnp.max(jnp.abs(eig_error))\n  error_metrics = TrainingMetrics(\n      inverse_pth_root_errors=jnp.array(error, jnp.float32))\n  if padding_start is not None:\n    val = jnp.where(padding_start == 0, 0.0, val)\n    error = jnp.where(padding_start == 0, 0.0,\n                      error_metrics.inverse_pth_root_errors)\n    error_metrics = error_metrics.replace(inverse_pth_root_errors=error)\n  val = jnp.asarray(val, orig_dtype)\n  return val, error_metrics", "\n\ndef _low_rank_root(\n    matrix: jnp.ndarray,\n    p: Union[int, jnp.ndarray],\n    compression_rank: int = 0,\n    ridge_epsilon: float = 1e-6,\n    error_tolerance: float = 1e-6,\n    relative_matrix_epsilon: bool = True,\n    padding_start: Union[int, jnp.ndarray, None] = None,\n    prev: Optional[jnp.ndarray] = None,\n) -> Tuple[jnp.ndarray, TrainingMetrics]:\n  \"\"\"Returns low-rank plus constant approx to inverse pth root.\"\"\"\n  del prev\n  assert compression_rank != 0\n  assert matrix.shape[0] == matrix.shape[1]\n  matrix_size = matrix.shape[0]\n  assert matrix_size > abs(compression_rank) + 2\n  orig_dtype = matrix.dtype\n  matrix = matrix.astype(_MAT_INV_PTH_ROOT_DTYPE)\n  alpha = jnp.asarray(-1.0 / p, _MAT_INV_PTH_ROOT_DTYPE)\n  identity = jnp.eye(matrix_size, dtype=_MAT_INV_PTH_ROOT_DTYPE)\n  if padding_start is not None:\n    ix = (jnp.arange(matrix_size, dtype=jnp.int32) < padding_start).astype(\n        matrix.dtype)\n    matrix *= ix[jnp.newaxis, :]\n    matrix *= ix[:, jnp.newaxis]\n    identity *= ix\n  if relative_matrix_epsilon:\n    _, max_ev = power_iteration(\n        matrix=matrix,\n        num_iters=100,\n        error_tolerance=error_tolerance,\n        precision=jax.lax.Precision.HIGHEST,\n        padding_start=padding_start)\n  else:\n    # Use absolute matrix epsilon scaling otherwise.\n    max_ev = 1.0\n  ridge_epsilon = ridge_epsilon * jnp.maximum(max_ev, error_tolerance)\n  regularized_input = matrix + ridge_epsilon * identity\n  e, u = jnp.linalg.eigh(regularized_input)\n  # Due to padding, we may have to zero out eigenvalues.\n  if padding_start is not None:\n    e *= jnp.flip(ix)\n  mm = functools.partial(jnp.matmul, precision=jax.lax.Precision.HIGHEST)\n  # Extract top-compression-rank.\n  recovered_e = mm(u.T, mm(regularized_input, u))\n  eig_error = recovered_e - jnp.diag(e)\n  if padding_start is not None:\n    eig_error *= jnp.flip(ix)\n  error = jnp.max(jnp.abs(eig_error))\n  inv_e = jnp.where(e == 0.0, 0.0,\n                    jnp.power(jnp.maximum(e, ridge_epsilon), alpha))\n  assert abs(compression_rank) <= matrix_size\n  d = matrix_size\n  # If padding_start < d, then we should have (d - padding_start)\n  # zeros at the front of the array.\n  if compression_rank < 0:\n    inv_e = jnp.roll(inv_e, -(d - padding_start))\n    u = jnp.roll(u, -(d - padding_start), axis=1)\n    # Denoting the eigenvalues of regularized_input\n    # e == [0, low, hi] before\n    # this roll corresponds to\n    # [low, hi, 0]\n  else:\n    inv_e = jnp.flip(inv_e)\n    u = jnp.flip(u, axis=1)\n    # [hi, low, 0], reversed\n  # Thanks to above transforms, the vectors we'd like to preserve\n  # are always exactly the first abs(compression_rank).\n  split_ix = abs(compression_rank)\n  keep_e, to_avg_e = inv_e[:split_ix], inv_e[split_ix:]\n  u_keep = u[:, :split_ix]\n  # Package the mean of the elided low eigenvalues in the last\n  # column.\n  real_dim = padding_start if padding_start is not None else d\n  num_real_eigs_to_avg = real_dim - abs(compression_rank)\n  const = jnp.sum(to_avg_e) / jnp.where(num_real_eigs_to_avg > 0,\n                                        num_real_eigs_to_avg, 1.0)\n  # Put everything together.\n  val = _low_rank_pack(u_keep, keep_e, const, compression_rank)\n  error_metrics = TrainingMetrics(\n      inverse_pth_root_errors=jnp.array(error, jnp.float32))\n  if padding_start is not None:\n    val = jnp.where(padding_start == 0, 0.0, val)\n    error = jnp.where(padding_start == 0, 0.0,\n                      error_metrics.inverse_pth_root_errors)\n    error_metrics = error_metrics.replace(inverse_pth_root_errors=error)\n  val = jnp.asarray(val, orig_dtype)\n  return val, error_metrics", "\n\ndef _fd_update_root(\n    new_grad: jnp.ndarray,\n    p: Union[int, jnp.ndarray],\n    rank: int = 0,\n    ridge_epsilon: float = 1e-6,\n    error_tolerance: float = 1e-6,\n    relative_matrix_epsilon: bool = True,\n    decay: float = 1.0,\n    padding_start: Union[int, jnp.ndarray, None] = None,\n    prev: Optional[jnp.ndarray] = None,\n    generate_training_metrics: bool = False,\n    generate_fd_metrics: bool = False,\n) -> Tuple[jnp.ndarray, TrainingMetrics]:\n  \"\"\"FD update.\"\"\"\n  assert prev is not None\n  assert rank > 0\n  # new_grad should be a Cholesky factor; R^T R = G^T G.\n  max_size = new_grad.shape[0]\n  assert list(new_grad.shape) == [max_size, max_size]\n  # previous sketch should be a packed, compressed matrix.\n  pd = _precond_dim(rank, max_size)\n  assert list(prev.shape) == [max_size, pd]\n\n  assert rank + 2 == pd\n  assert rank + 2 < max_size\n  # full-rank case handled outside this method.\n\n  # Note 'd' is zero-padded to max_size.\n  sketch_dr, fwd_eigvals_r, inv_eigvals_r, const, tail, had_zeros = (\n      _fd_low_rank_unpack(prev, rank))\n  del inv_eigvals_r, const, had_zeros\n\n  if relative_matrix_epsilon:\n    max_ev = fwd_eigvals_r[0]\n  else:\n    max_ev = 1.0\n  ridge_epsilon = ridge_epsilon * jnp.maximum(max_ev, error_tolerance)\n\n  active_ix_d = padding_start > jnp.arange(max_size)\n  active_ix_r = padding_start > jnp.arange(rank)\n\n  # Numerical error can introduce eigenvectors which lie in padding space.\n  # To prevent compounding error (since SVD is unaware of the padding),\n  # re-zero consistently.\n  sketch_dr *= active_ix_d[:, jnp.newaxis]\n  sketch_dr *= active_ix_r\n  fwd_eigvals_r = fwd_eigvals_r + ridge_epsilon\n  fwd_eigvals_r *= active_ix_r\n  weighted_sketch_dr = sketch_dr * jnp.sqrt(fwd_eigvals_r)\n  padded_grad = new_grad\n  padded_grad *= active_ix_d\n  padded_grad *= active_ix_d[:, jnp.newaxis]\n\n  # SST = sketch.dot(sketch.T) recovers the covariance approximation\n  # GGT = padded_grad.dot(padded_grad.T) is the new covariance observation\n  # updated = [sketch ; padded_grad] has\n  # updated @ updated.T == SST * decay + GGT\n  updated = jnp.concatenate(\n      [\n          jnp.sqrt(decay) * weighted_sketch_dr,\n          # NOTE(vladf): if we multiply the below padded_grad with the constant\n          # jnp.sqrt(1 - decay) note that we'd have nearly the same behavior\n          # (verified experimentally) because over time the entire\n          # covariance should be scaled like \\sum_t decay^(T-t) G @ G.T\n          # which is up to a (1-decay) factor away if we used an EMA\n          # update. In the presence of grafting or LR tuning,\n          # this rescaling is a no-op.\n          padded_grad\n      ],\n      axis=1)\n  u, s, vt = jnp.linalg.svd(updated, full_matrices=False)\n  assert list(u.shape) == [max_size, max_size]\n  cutoff = s[rank]\n  rho_t = cutoff**2\n  top_eigs = s[:rank]\n  # Deflate: top_eigs ** 2 - cutoff ** 2\n  deflated_eigs = (top_eigs - cutoff) * (top_eigs + cutoff)\n  eigvecs = u[:, :rank]\n  tail = tail * decay\n  new_tail = tail + rho_t\n\n  alpha = jnp.asarray(-1.0 / p)\n  new_const = jnp.where(new_tail <= 0, 0.0, new_tail**alpha)\n  new_tail = jnp.where(new_tail <= 0, 0.0, new_tail)\n  num_neg_eigs = jnp.sum(deflated_eigs < 0)\n  num_zero_deflated_eigs = jnp.sum(deflated_eigs == 0.0)\n  deflated_eigs = jnp.where(deflated_eigs <= 0, 0.0, deflated_eigs)\n  eigvecs *= deflated_eigs > 0  # Don't introduce new directions with 0 eigs.\n\n  # Zero out anything that's not within 1% of unit norm. Amazingly, I've seen\n  # 0.25, 0.8, and even 10 returned as the norms.\n  norms = jnp.linalg.norm(eigvecs, axis=0)\n  safe_normed = (0.99 <= norms) & (norms <= 1.01)\n  eigvecs *= safe_normed\n  deflated_eigs *= safe_normed\n  eigvecs /= jnp.where(safe_normed, norms, 1.0)\n  num_unsafe_norms = jnp.sum(~safe_normed) - (\n      num_neg_eigs + num_zero_deflated_eigs\n  )\n\n  # Zero out any eigenvectors that include padding dimensions\n  padding_ix = jnp.arange(eigvecs.shape[0]) >= padding_start\n  padding_eigvecs = eigvecs * padding_ix[:, jnp.newaxis]\n  padding_mass = jnp.linalg.norm(padding_eigvecs, axis=0, ord=1)\n  has_significant_padding = padding_mass > 0.01\n  eigvecs *= 1 - has_significant_padding\n  deflated_eigs *= 1 - has_significant_padding\n  num_has_padding = np.sum(has_significant_padding)\n\n  # Assess the accuracy of our top-k fit. Previously, we inspected the singular\n  # values recovered from the returned singular vectors; however these can have\n  # high relative error for near-0 singular values, causing needless errors.\n  recovered = jnp.matmul(\n      (u[:, :rank] * s[jnp.newaxis, :rank]), vt[:rank, :],\n      precision=jax.lax.Precision.HIGHEST\n  )\n  diff = recovered - updated\n  frob = jnp.square(diff).sum()\n  entrywise = jnp.abs(diff).sum() / (padding_start**2 + padding_start * rank)\n  expected_frob = jnp.square(s[rank:]).sum()\n  total_frob = jnp.square(updated).sum()\n  # TODO(vladf): consider some condition for high frob norm\n\n  # Compute inversion after tail shift.\n  upshifted_eigs = jnp.square(top_eigs) + tail\n  upshifted_eigs *= deflated_eigs > 0.0\n  upshifted_eigs = jnp.where(upshifted_eigs <= 0, 0.0, upshifted_eigs)\n  inverted_eigs = jnp.where(upshifted_eigs <= 0, 0.0, upshifted_eigs**alpha)\n  has_zeros = jnp.any(deflated_eigs <= 0) | jnp.any(new_tail <= 0)\n  packed_precond = _fd_low_rank_pack(\n      eigvecs,\n      deflated_eigs,\n      inverted_eigs,\n      new_const,\n      new_tail,\n      has_zeros,\n      rank,\n  )\n\n  val = packed_precond\n  error_metrics = default_training_metrics(generate_fd_metrics).replace(\n      inverse_pth_root_errors=jnp.array(0.0, jnp.float32))\n  if generate_training_metrics and generate_fd_metrics:\n    error_metrics = error_metrics.replace(\n        fd=FDDiagnostics.create(  # pytype: disable=wrong-arg-types  # jax-ndarray\n            rho_t,\n            new_tail,\n            deflated_eigs,\n            new_grad,\n            eigvecs,\n            padding_start,\n            max_size,\n            num_neg_eigs,\n            num_zero_deflated_eigs,\n            num_unsafe_norms,\n            num_has_padding,\n            frob,\n            expected_frob,\n            entrywise,\n            total_frob,\n        )\n    )\n  if padding_start is not None:\n    val = jnp.where(padding_start == 0, 0.0, val)\n    error = jnp.where(padding_start == 0, 0.0,\n                      error_metrics.inverse_pth_root_errors)\n    error_metrics = error_metrics.replace(inverse_pth_root_errors=error)\n  return val, error_metrics", "\n\ndef merge_small_dims(shape_to_merge, max_dim):\n  \"\"\"Merge small dimensions.\n\n  If there are some small dimensions, we collapse them:\n  e.g. [1, 2, 512, 1, 2048, 1, 3, 4] --> [1024, 2048, 12] if max_dim = 1024\n       [1, 2, 768, 1, 2048] --> [2, 768, 2048]\n\n  Args:\n    shape_to_merge: Shape to merge small dimensions.\n    max_dim: Maximal dimension of output shape used in merging.\n\n  Returns:\n    Merged shape.\n  \"\"\"\n  if shape_to_merge and np.all(np.array(shape_to_merge) == 1):\n    return [1]\n\n  resulting_shape = []\n  product = 1\n  for d in shape_to_merge:\n    if product * d <= max_dim:\n      product *= d\n    else:\n      if product > 1:\n        resulting_shape.append(product)\n      product = d\n  if product > 1:\n    resulting_shape.append(product)\n  return resulting_shape", "\n\ndef pad_square_matrix(mat: jnp.ndarray, max_size: int) -> jnp.ndarray:\n  \"\"\"Pad a square matrix up to max_size.\n\n  Args:\n    mat: a matrix to pad.\n    max_size: matrix size requested.\n\n  Returns:\n    Given M returns [[M, 0], [0, I]]\n  \"\"\"\n  rows, cols = mat.shape\n  if rows != cols:\n    raise ValueError(\"Must have rows == cols, instead got \"\n                     f\"rows={rows}, cols={cols}\")\n  if cols > max_size:\n    raise ValueError(\"Must have cols <= max_size. Instead got \"\n                     f\"cols={cols}, max_size={max_size}.\")\n  if rows == max_size:\n    return mat\n  pad_size = max_size - rows\n\n  zs1 = jnp.zeros([rows, pad_size], dtype=mat.dtype)\n  zs2 = jnp.zeros([pad_size, rows], dtype=mat.dtype)\n  eye = jnp.eye(pad_size, dtype=mat.dtype)\n  mat = jnp.concatenate([mat, zs1], 1)\n  mat = jnp.concatenate([mat, jnp.concatenate([zs2, eye], 1)], 0)\n  return mat", "\n\ndef pad_vector(vec: jnp.ndarray, max_size: int) -> jnp.ndarray:\n  \"\"\"Pad a vector to a max_size.\n\n  Args:\n    vec: a vector to pad.\n    max_size: matrix size requested.\n\n  Returns:\n    Given V returns [V, 0]\n  \"\"\"\n  size = vec.shape[0]\n  assert size <= max_size\n  if size == max_size:\n    return vec\n  pad_size = max_size - size\n  zs1 = jnp.zeros([pad_size], dtype=vec.dtype)\n  return jnp.concatenate([vec, zs1], 0)", "\n\ndef efficient_cond(predicate, compute_fn, init_state, *args, **kwargs):\n  \"\"\"Avoids wasteful buffer allocation with XLA.\"\"\"\n\n  def _iter_body(unused_state):\n    results = compute_fn(*args, **kwargs)\n    return tuple([False] + list(results))\n\n  def _iter_condition(state):\n    return state[0]\n\n  results = jax.lax.while_loop(_iter_condition, _iter_body,\n                               tuple([predicate] + init_state))\n  return tuple(results[1:])", "\n\nclass BlockPartitioner:\n  \"\"\"Partitions a tensor into smaller tensors.\"\"\"\n\n  def __init__(self, param, block_size):\n    self._shape = param.shape\n    self._splits = []\n    split_sizes = []\n    # We split params into smaller blocks. Here we store the metadata to make\n    # that split.\n    for i, d in enumerate(param.shape):\n      if 0 < block_size < d:\n        # d-1, otherwise split appends a 0-size array.\n        nsplit = (d - 1) // block_size\n        indices = (np.arange(nsplit, dtype=np.int32) + 1) * block_size\n        sizes = np.ones(nsplit + 1, dtype=np.int32) * block_size\n        sizes[-1] = d - indices[-1]\n        self._splits.append((i, indices))\n        split_sizes.append(sizes)\n      else:\n        split_sizes.append(np.array([d], dtype=np.int32))\n    self._split_sizes = split_sizes\n\n  def split_sizes(self):\n    return self._split_sizes\n\n  def partition(self, tensor):\n    \"\"\"Partition tensor into blocks.\"\"\"\n\n    assert tensor.shape == self._shape\n    tensors = [tensor]\n    for (i, indices) in self._splits:\n      tensors_local = []\n      for t in tensors:\n        tensors_local.extend(jnp.split(t, indices_or_sections=indices, axis=i))\n      tensors = tensors_local\n    return tensors\n\n  def merge_partitions(self, partitions):\n    \"\"\"Merge partitions back to original shape.\"\"\"\n\n    for (i, indices) in reversed(self._splits):\n      n = len(indices) + 1\n      partial_merged_tensors = []\n      ind = 0\n      while ind < len(partitions):\n        partial_merged_tensors.append(\n            jnp.concatenate(partitions[ind:ind + n], axis=i))\n        ind += n\n      partitions = partial_merged_tensors\n    assert len(partitions) == 1\n    return partitions[0]", "\n\ndef gram_weighted_update(\n    old_stats: chex.Array,\n    g: chex.Array,\n    axis: int,\n    w1: float,\n    w2: float,\n    precision: Optional[lax.Precision] = None) -> chex.Array:\n  \"\"\"Updated statistics via weighted average with new Gram matrix.\n\n    Returns w\u2081 R + w\u2082 G\u1d40 G where R is `old_stats` and G is the matrix whose\n    columns are the flattened slices of the tensor `g` along the given `axis`.\n    (So, `old_stats` and the returned matrix have dimensions n x n where\n    n = `g.shape[axis]`).\n\n  Args:\n    old_stats:  Old statistics.\n    g:  Gradient tensor.\n    axis:  Axis along which to slice `g`.\n    w1:  Scalar weight for old statistics.\n    w2:  Scalar weight for new Gram matrix.\n    precision: Optional precision XLA related flag, the available options are:\n      a) lax.Precision.DEFAULT (better step time, but not precise) b)\n      lax.Precision.HIGH (increased precision, slower) c) lax.Precision.HIGHEST\n      (best possible precision, slowest)\n\n  Returns:\n    Weighted average of old and new statistics.\n  \"\"\"\n  axes = [i for i in range(g.ndim) if i != axis]\n  gram_matrix = jnp.tensordot(g, g, axes=(axes, axes), precision=precision)\n  return w1 * old_stats + w2 * gram_matrix", "\n\ndef frequent_directions_update(old_stats_factor: chex.Array, g: chex.Array,\n                               axis: int, w1: float, w2: float) -> chex.Array:\n  \"\"\"Derive the frequent directions update from the gradient block.\n\n    Computes a square Cholesky factor R corresponding to the gradient G such\n    that R R^T = tensordot(G, G, axes=(except_i, except_i)),\n    where except_i = [i for i in range(G.ndim) if i != axis].\n\n    Notably, the old_stats_factor and w* parameters are competely ignored.\n    Frequent directions updates rely on gradient averaging to accumulate\n    recent grad info and w* parameters come into play during the sketch\n    update (which is the update of the preconditioner).\n\n  Args:\n    old_stats_factor: Ignored.\n    g: Gradient tensor.\n    axis: Axis to precondition.\n    w1: Ignored.\n    w2: Ignored.\n\n  Returns:\n    Cholesky factor R meeting above constraint.\n  \"\"\"\n  del old_stats_factor, w1, w2\n  x = jnp.reshape(jnp.moveaxis(g, axis, 0), (g.shape[axis], -1))\n  # Let d be the dimension of x.shape[0].\n  # Then x @ x.T == tensordot(G, G, axes=(except_i, except_i)).\n  # Suppose x.T == q @ r with q orthonormal\n  # Then r.T @ r == r.T @ q.T @ q @ r = (q @ r).T @ (q @ r) == x @ x.T\n  r = jnp.linalg.qr(x.T, mode=\"r\")\n  r = r.T  # To match R R^T form in docs.\n  assert r.shape == (x.shape[0], min(x.shape))\n  return jnp.pad(r, ((0, 0), (0, x.shape[0] - r.shape[1])))", "\n\nclass Preconditioner:\n  \"\"\"Compute statistics/shape from gradients for preconditioning.\"\"\"\n\n  def __init__(\n      self,\n      param,\n      block_size,\n      merge_small_dims_block_size,\n      best_effort_shape_interpretation,\n      preconditioner_type=PreconditionerType.ALL,\n      compression_rank=0,\n  ):\n    \"\"\"Initializes the preconditioner.\n\n    Args:\n      param: parameter to precondition.\n      block_size: Block size used to split param.\n      merge_small_dims_block_size: Block size for merging dims.\n      best_effort_shape_interpretation: Whether to collapse/merge dims together.\n      preconditioner_type: Type of preconditioner to use.\n      compression_rank: Rank of preconditioner.\n    \"\"\"\n    self._original_shape = param.shape\n    self._transformed_shape = param.shape\n    if best_effort_shape_interpretation:\n      self._transformed_shape = merge_small_dims(self._original_shape,\n                                                 merge_small_dims_block_size)\n    reshaped_param = jnp.reshape(param, self._transformed_shape)\n    self._partitioner = BlockPartitioner(reshaped_param, block_size)\n    self._preconditioner_type = preconditioner_type\n    self._compression_rank = compression_rank\n\n  def updated_statistics_from_grad(\n      self,\n      stats: List[chex.Array],\n      grad: chex.Array,\n      w1: float,\n      w2: float,\n      to_float: Optional[Callable[[chex.Array], chex.Array]] = None,\n      from_float: Optional[Callable[[chex.Array], chex.Array]] = None,\n      precision: Optional[lax.Precision] = None,\n      frequent_directions: bool = False,\n  ) -> List[chex.Array]:\n    \"\"\"Update statistics from gradients.\n\n    Args:\n      stats: Old statistics or its Cholesky factor if `cholesky` is True.\n      grad: Gradient to compute statistics from.\n      w1: Weight for old statistics.\n      w2: Weight for new statistics.\n      to_float: Optional function for converting stats to floating point.\n      from_float: Optional function for converting from floating point.\n      precision: Optional precision XLA related flag, the available options are:\n        a) lax.Precision.DEFAULT (better step time, but not precise) b)\n        lax.Precision.HIGH (increased precision, slower) c)\n        lax.Precision.HIGHEST (best possible precision, slowest)\n      frequent_directions: If True, return Cholesky factors from the\n        possibly-averaged gradient, feeding them directly into the stats; this\n        ignores the previous value of stats and treats them like Cholesky\n        factors does.\n\n    Returns:\n      A list of updated gradient statistics for each partition.\n    \"\"\"\n    to_float = to_float if to_float is not None else (lambda x: x)\n    from_float = from_float if from_float is not None else (lambda x: x)\n    reshaped_grad = jnp.reshape(grad, self._transformed_shape)\n    partitioned_grads = self._partitioner.partition(reshaped_grad)\n    should_preconditioned_dims = self.should_precondition_dims()\n    preconditioned_dims = [\n        i for i, p in enumerate(should_preconditioned_dims) if p\n    ]\n    new_stats = []\n    index = 0\n    for g in partitioned_grads:\n      for axis in preconditioned_dims:\n        update = functools.partial(gram_weighted_update, precision=precision)\n        if frequent_directions:\n          if _should_compress(self._compression_rank, g.shape[axis]):\n            update = frequent_directions_update\n        new_stat = update(to_float(stats[index]), g, axis, w1, w2)\n        new_stats.append(from_float(new_stat))\n        index += 1\n    return new_stats\n\n  def should_precondition_dims(self):\n    \"\"\"A vector containing indicator indicating if the dim is preconditioned.\"\"\"\n    split_sizes = self._partitioner.split_sizes()\n    rank = len(split_sizes)\n    if self._preconditioner_type == PreconditionerType.ALL or rank <= 1:\n      return [True] * rank\n    elif self._preconditioner_type == PreconditionerType.INPUT:\n      return [True] * (rank - 1) + [False]\n    elif self._preconditioner_type == PreconditionerType.OUTPUT:\n      return [False] * (rank - 1) + [True]\n\n  def _preconditioner_shape(self, dim):\n    \"\"\"Returns possibly rank-compressed preconditioner shape.\"\"\"\n    if self._compression_rank:\n      return [dim, _precond_dim(self._compression_rank, dim)]\n    return [dim, dim]\n\n  def _preconds_for_grad(self, preconditioners, rank, start, end):\n    \"\"\"Returns a slice of preconditioners of length rank.\"\"\"\n    preconditioners_for_grad = preconditioners[start:end]\n    if self._preconditioner_type == PreconditionerType.INPUT:\n      # When _preconditioner_type is INPUT, we append a None value to the end of\n      # the list to handle the False index.\n      preconditioners_for_grad = preconditioners_for_grad + [None]\n    elif self._preconditioner_type == PreconditionerType.OUTPUT:\n      # When _preconditioner_type is OUTPUT, we append (rank - 1) many None\n      # values to the beginning of the list to handle the False indices.\n      preconditioners_for_grad = [None] * (rank - 1) + preconditioners_for_grad\n    assert len(preconditioners_for_grad) == rank\n    return preconditioners_for_grad\n\n  def shapes_for_preconditioners(self):\n    \"\"\"Returns shape from statistics.\"\"\"\n    split_sizes = self._partitioner.split_sizes()\n    rank = len(split_sizes)\n    # We ignore preconditioner types if rank == 1\n    preconditioner_shapes = []\n    for t in itertools.product(*split_sizes):\n      if self._preconditioner_type == PreconditionerType.ALL or rank <= 1:\n        preconditioner_shapes.extend(map(self._preconditioner_shape, t))\n      elif self._preconditioner_type == PreconditionerType.INPUT:\n        preconditioner_shapes.extend(map(self._preconditioner_shape, t[:-1]))\n      elif self._preconditioner_type == PreconditionerType.OUTPUT:\n        preconditioner_shapes.extend(map(self._preconditioner_shape, t[-1:]))\n    return preconditioner_shapes\n\n  def exponent_for_preconditioner(self):\n    \"\"\"Returns exponent to use for inverse-pth root M^{-1/p}.\"\"\"\n    should_preconditioned_dims = self.should_precondition_dims()\n    num_preconditioners = sum(should_preconditioned_dims)\n    return 2 * num_preconditioners\n\n  def preconditioned_grad(self, grad, preconditioners):\n    \"\"\"Precondition the gradient.\n\n    Args:\n      grad: A gradient tensor to precondition.\n      preconditioners: A list of preconditioners to apply.\n\n    Returns:\n      A preconditioned gradient.\n    \"\"\"\n    reshaped_grad = jnp.reshape(grad, self._transformed_shape)\n    partitioned_grads = self._partitioner.partition(reshaped_grad)\n    should_preconditioned_dims = self.should_precondition_dims()\n    num_preconditioners = sum(should_preconditioned_dims)\n    preconditioned_partitioned_grads = []\n    for i, g in enumerate(partitioned_grads):\n      preconditioners_for_grad = self._preconds_for_grad(\n          preconditioners,\n          rank=len(should_preconditioned_dims),\n          start=i * num_preconditioners,\n          end=(i + 1) * num_preconditioners,\n      )\n      precond_g = self._precondition_block(\n          g, should_preconditioned_dims, preconditioners_for_grad\n      )\n      preconditioned_partitioned_grads.append(precond_g)\n    merged_grad = self._partitioner.merge_partitions(\n        preconditioned_partitioned_grads\n    )\n    return jnp.reshape(merged_grad, self._original_shape)\n\n  def _precondition_block(self, g, should_precondition_dim, preconditioners):\n    \"\"\"Perform a preconditioning op on a single gradient block.\"\"\"\n    for j, should_precondition in enumerate(should_precondition_dim):\n      # Loop invariant: the dimension to be preconditioned is first; we keep\n      # all axes in the same cyclic order they were originally.\n      # Case: skip preconditioning this dimension.\n      rank = len(g.shape)\n      roll = tuple(range(1, rank)) + (0,)\n      if not should_precondition:\n        g = jnp.transpose(g, axes=roll)\n        continue\n      dim, application_dim = preconditioners[j].shape\n      compress = application_dim != dim\n      # Case: Compressed rank preconditioning.\n      if compress:\n        eigvecs, eigvals, const, skip = _low_rank_unpack(\n            preconditioners[j], abs(self._compression_rank))\n        assert list(eigvecs.shape) == [dim, abs(self._compression_rank)]\n        lowrank_basis = jnp.tensordot(g, eigvecs, axes=[[0], [0]])\n        lowrank_component = jnp.tensordot(\n            lowrank_basis, eigvecs, axes=[[rank - 1], [1]])\n        g = jnp.transpose(g, axes=roll)\n        complement = g - lowrank_component\n        scaled_basis = lowrank_basis * eigvals\n        scaled_lowrank_component = jnp.tensordot(\n            scaled_basis, eigvecs, axes=[[rank - 1], [1]])\n        old_g = g\n        new_g = const * complement + scaled_lowrank_component\n        g = jnp.where(skip, old_g, new_g)\n        continue\n      # Case: full Shampoo matrix precondition this dimension\n      g = jnp.tensordot(g, preconditioners[j], axes=[[0], [0]])\n    return g", "\n\ndef _update_preconditioners_fn(\n    update_preconditioners_every_fn,\n    update_preconditioners_fn,\n    steps,\n    scheduled,\n    quantized=False):\n  \"\"\"Calls the appropriate preconditioner update function, depending on how often we want to update it.\"\"\"\n  if quantized:\n    if scheduled:\n      (quantized_preconditioners_flat, quantized_diagonals_flat,\n       quantized_bucket_sizes_flat, metrics_flat) = lax.cond(\n           steps == 1,\n           update_preconditioners_every_fn,\n           update_preconditioners_fn)\n    else:\n      if steps == 1:\n        (\n            quantized_preconditioners_flat,\n            quantized_diagonals_flat,\n            quantized_bucket_sizes_flat,\n            metrics_flat,\n        ) = update_preconditioners_every_fn()\n      else:\n        (\n            quantized_preconditioners_flat,\n            quantized_diagonals_flat,\n            quantized_bucket_sizes_flat,\n            metrics_flat,\n        ) = update_preconditioners_fn()\n\n    return (\n        quantized_preconditioners_flat,\n        quantized_diagonals_flat,\n        quantized_bucket_sizes_flat,\n        metrics_flat,\n    )\n  else:\n    if scheduled:\n      preconditioners_flat, metrics_flat = lax.cond(\n          steps == 1,\n          update_preconditioners_every_fn,\n          update_preconditioners_fn,\n      )\n    else:\n      if steps == 1:\n        preconditioners_flat, metrics_flat = update_preconditioners_every_fn()\n      else:\n        preconditioners_flat, metrics_flat = update_preconditioners_fn()\n\n    return (preconditioners_flat, metrics_flat, None, None)", "\n\ndef _convert_to_parameter_stats(\n    global_stats,\n    local_stat,\n    compression_rank,\n    convert_statistics=True):\n  \"\"\"Creates parameter stats from sharded stats.\"\"\"\n  index_start = int(local_stat.index_start)\n  index_end = int(len(local_stat.sizes)) + index_start\n  statistics = global_stats.statistics[index_start:index_end, :, :]\n  preconditioners = global_stats.preconditioners[index_start:index_end, :, :]\n  new_statistics = []\n  new_preconditioners = []\n  for i, size in enumerate(local_stat.sizes):\n    new_statistics.append(statistics[i][:size, :size])\n    pd = size\n    pd = _precond_dim(compression_rank, size)\n    new_preconditioners.append(preconditioners[i][:size, :pd])\n  if not convert_statistics:\n    new_statistics = None\n  return ParameterStats(\n      local_stat.diagonal_statistics,\n      new_statistics,\n      new_preconditioners,\n      local_stat.diagonal_momentum,\n      local_stat.momentum,\n      local_stat.avg_grad,\n      local_stat.training_metrics,\n  )", "\n\ndef _convert_from_parameter_stats(parameter_stats, local_stats):\n  \"\"\"Creates sharded stats from paramter stats.\"\"\"\n  return LocalShardedParameterStats(\n      parameter_stats.diagonal_statistics,\n      parameter_stats.diagonal_momentum,\n      parameter_stats.momentum,\n      parameter_stats.avg_grad,\n      parameter_stats.training_metrics,\n      local_stats.index_start,\n      local_stats.sizes,\n  )", "\n\ndef _add_metrics_into_local_stats(local_stats, metrics, keep_old):\n  \"\"\"Adds errors back into local statistics.\"\"\"\n  new_local_stats = []\n  for local_stat in local_stats:\n    index_start = int(local_stat.index_start)\n    index_end = int(len(local_stat.sizes)) + index_start\n    # pylint:disable=cell-var-from-loop Used immediately.\n    per_stat_metrics = jax.tree_map(lambda x: x[index_start:index_end], metrics)\n    # We don't want to update the metrics if we didn't do a new inverse p-th\n    # root calculation to find a new preconditioner, so that TensorBoard curves\n    # look consistent (otherwise they'd oscillate between NaN and measured\n    # values).\n    per_stat_metrics = efficient_cond(keep_old,\n                                      lambda: [local_stat.training_metrics],\n                                      [per_stat_metrics])[0]\n    # pylint:enable=cell-var-from-loop\n    new_local_stats.append(\n        local_stat.replace(training_metrics=per_stat_metrics))\n  return new_local_stats", "\n\ndef batch(x, num_devices):\n  \"\"\"Batch `x` so that so that leading axis is num_devices.\"\"\"\n  n = len(x)\n  b = int(n / num_devices)\n  return jnp.stack([jnp.stack(x[idx:idx + b]) for idx in range(0, n, b)])\n\n\ndef unbatch(batched_values):\n  \"\"\"Unbatch values across leading axis and return a list of elements.\"\"\"\n  b1, b2 = batched_values.shape[0], batched_values.shape[1]\n  results = []\n  for v_array in jnp.split(batched_values, indices_or_sections=b1, axis=0):\n    v_array = jnp.squeeze(v_array)\n    # b2 = batches (number of preconditioner computation) per core.\n    if b2 > 1:\n      for v in jnp.split(v_array, indices_or_sections=b2, axis=0):\n        results.append(jnp.squeeze(v))\n    else:\n      results.append(v_array)\n  return results", "\ndef unbatch(batched_values):\n  \"\"\"Unbatch values across leading axis and return a list of elements.\"\"\"\n  b1, b2 = batched_values.shape[0], batched_values.shape[1]\n  results = []\n  for v_array in jnp.split(batched_values, indices_or_sections=b1, axis=0):\n    v_array = jnp.squeeze(v_array)\n    # b2 = batches (number of preconditioner computation) per core.\n    if b2 > 1:\n      for v in jnp.split(v_array, indices_or_sections=b2, axis=0):\n        results.append(jnp.squeeze(v))\n    else:\n      results.append(v_array)\n  return results", "\n\ndef distributed_shampoo(\n    learning_rate,\n    block_size,\n    beta1=0.9,\n    beta2=0.999,\n    diagonal_epsilon=1e-10,\n    matrix_epsilon=1e-6,\n    weight_decay=0.0,\n    start_preconditioning_step=5,\n    preconditioning_compute_steps=1,\n    decay_preconditioning_compute_steps: bool = False,\n    end_preconditioning_compute_steps: Optional[int] = None,\n    statistics_compute_steps=1,\n    best_effort_shape_interpretation=True,\n    graft_type=GraftingType.SGD,\n    nesterov=True,\n    exponent_override=0,\n    # Pass pmap 'batch axis name' in pmap mode.\n    batch_axis_name=None,\n    ### Only set following 3 params in pjit/spmd mode.\n    ### WARNING: Experimental\n    statistics_partition_spec=None,\n    preconditioner_partition_spec=None,\n    num_devices_for_pjit=None,\n    shard_optimizer_states=False,\n    ###\n    ### Experimental memory reduction mode\n    best_effort_memory_usage_reduction=False,\n    ###\n    inverse_failure_threshold=0.1,\n    moving_average_for_momentum=False,\n    skip_preconditioning_dim_size_gt=4096,\n    clip_by_scaled_gradient_norm=None,\n    precision=lax.Precision.HIGHEST,\n    tensordot_precision: Optional[lax.Precision] = None,\n    relative_matrix_epsilon=True,\n    merge_small_dims_block_size=4096,\n    lobpcg_topk_precondition: int = 0,\n    lobpcg_max_iter: int = 0,\n    precondtioner_type=PreconditionerType.ALL,\n    generate_fd_metrics: bool = False,\n    compression_rank: int = 0,\n    frequent_directions: bool = False,\n    reset_preconditioner: bool = False,\n    average_grad: bool = False,\n    skip_preconditioning_rank_lt=1,\n    decoupled_learning_rate=True,\n    decoupled_weight_decay=False,\n    generate_training_metrics=True,\n    reuse_preconditioner=False,\n    eigh=False,\n):\n  \"\"\"Distributed Shampoo optimizer.\n\n  Distributed Shampoo is a second-order preconditioned method (concretely, a\n  variant of full-matrix Adagrad), that provides significant convergence and\n  wall-clock time improvements compared to conventional first-order methods,\n  and that has been shown to scale to large state-of-the-art deep learning\n  models.\n\n  References:\n    Scalable Second Order Optimization for Deep Learning,\n    Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, Yoram Singer\n\n    Preprint: https://arxiv.org/abs/2002.09018\n\n  Args:\n    learning_rate: the step size used to update the parameters.\n    block_size: Block size for large layers (if > 0). Preconditioning compute\n      operation is cubic in the dimension of the tensor. Block size allows us to\n      chunk the layers into sub-layers of maximal dimension dictated by this\n      value. Use 128 as default (increase if you have compute budget).\n    beta1: momentum parameter.\n    beta2: second moment averaging parameter.\n    diagonal_epsilon: epsilon for diagonal adagrad (only if layerwise grafting\n      to AdaGrad is enabled).\n    matrix_epsilon: epsilon to add to statistics before computing inverse pth\n      root. If you are running in f32 precision for inverse pth root\n      (recommended today) this can go upto 1e-6. If you have latest hardware\n      with native f64 precision, set this upto 1e-12.\n    weight_decay: Weight decay for regularization.\n    start_preconditioning_step: When to start Shampoo update before which\n      diagonal update is used. This is because we dont have enough information\n      to do stable inverse.\n    preconditioning_compute_steps: How often to compute preconditioner.\n      Performance tuning params for controlling memory and compute requirements.\n      Ideally set this and statistics_compute_steps params to 1.\n    decay_preconditioning_compute_steps: Flag to use learning rate schedule to\n      decay preconditioning_compute_steps over time. Defaults to False.\n    end_preconditioning_compute_steps: If decay preconditioning_compute_steps,\n      this sets the maximum value of preconditioning_compute_steps to decay to.\n      Defaults to None.\n    statistics_compute_steps: How often to compute statistics.\n    best_effort_shape_interpretation: If there are some small dimensions,\n      collapse them e.g. [1, 2, 512, 1, 2048, 1, 3, 4] --> [1024, 2048, 12] if\n      block = 1024, [1, 2, 768, 1, 2048] --> [2, 768, 2048]\n    graft_type: Grafting is a technique to fix the layerwise scale of Shampoo\n      optimizer. This allows us to plugin the Shampoo optimizer into settings\n      where SGD/AdaGrad is already well tuned.\n    nesterov: Nesterov momentum.\n    exponent_override: Override the exponent used in matrix inverse.\n    batch_axis_name: labeled axis over pmap for data-parallel training the\n      optimizer used for.\n    statistics_partition_spec: PartitionSpec to be used in sharded mode.\n    preconditioner_partition_spec: PartitionSpec to be used in sharded mode.\n    num_devices_for_pjit: Number of devices to parallelize over when using pjit.\n    shard_optimizer_states: Shard optimizer states to save memory in model\n      parallel training.\n    best_effort_memory_usage_reduction: Best effort memory usage reduction. -\n      diagonal_statistics -> jnp.bfloat16 - momentum buffers (2x) -> jnp.int8 -\n      statistics, preconditioners -> jnp.int16 + diagonals\n    inverse_failure_threshold: numerics are hard and inverses fail sometimes; we\n      determine that using this threshold.\n    moving_average_for_momentum: Whether to use moving average for momentum\n      instead of exponential moving average.\n    skip_preconditioning_dim_size_gt: Skip if preconditioning dim size is\n      greater than this value.\n    clip_by_scaled_gradient_norm: Clip by scaled gradient norm (only useful when\n      using RMSProp Grafting).\n    precision: precision XLA related flag, the available options are: a)\n      lax.Precision.DEFAULT (better step time, but not precise) b)\n      lax.Precision.HIGH (increased precision, slower) c) lax.Precision.HIGHEST\n      (best possible precision, slowest)\n    tensordot_precision: Optional precision to use for the tensordot operation\n      when computing statistics (e.g., G G\u1d40). Same options as `precision` above.\n    relative_matrix_epsilon: Whether to use relative epsilon to the max eigen\n      value when computing inverse-pth root.\n    merge_small_dims_block_size: Used as the maximum block size to merge the\n      shapes.\n    lobpcg_topk_precondition: If nonzero, specifies the number of top\n      eigenvectors to subtract out before performing LOBPCG. Note this makes\n      relative_matrix_epsilon essentially free.\n    lobpcg_max_iter: Number of LOBPCG iterations, if zero defaults to\n      `lobpcg_topk_precondition`.\n    precondtioner_type: Preconditioner type to select all, left only or right\n      only preconditioners.\n    generate_fd_metrics: Generate additional metrics for FD. Ignored if not\n      (generate_training_metrics and frequent_directions).\n    compression_rank: Low rank plus diagonal compression of preconditioner. If\n      negative, uses the lowest eigenvalues of the covariance matrix to\n      approximate the low rank component, rather than the highest. Disables\n      preconditioner & stats quantization if active.\n    frequent_directions: Can only be true if compression_rank > 0. Use frequent\n      directions to update the preconditioner inverse root directly. Statistics\n      are still tracked (for debugging checkpoints) but unused in the learning\n      algorithm. Must set `statistics_compute_steps` to equal to\n      `preconditioning_compute_steps`.\n    reset_preconditioner: If set, resets preconditioner, and therefore\n      FD statistics, every round(1/(1-beta2)) steps (and otherwise behaves like\n      beta2=1.0).\n    average_grad: Can only be true if frequent_directions is set. Averages most\n      recent `statistics_compute_steps` grads before updating the FD structure\n      every `statistics_compute_steps`.\n    skip_preconditioning_rank_lt: Skips preconditioning for parameters with rank\n      less than this value.\n    decoupled_learning_rate: If True, use decoupled learning rate, otherwise\n      couple it with preconditioned gradient computation. (Default True)\n    decoupled_weight_decay: If True, use decoupled weight decay, otherwise\n      couple with weight decay. (Default False)\n    generate_training_metrics: If True, gather training metrics, otherwise avoid\n      generating them (to reduce memory usage).\n    reuse_preconditioner: If True, pass the previous derived preconditioner as a\n      warm start to the next iteratin's inverse pth root computation.\n    eigh: If True, and uses eigen decomposition for inverse-pth root.\n\n  Returns:\n    a GradientTransformation.\n  \"\"\"\n  reset_frequency = None\n\n  if reset_preconditioner and not frequent_directions:\n    raise ValueError(\"reset_preconditioner=True requries frequent_directions\")\n\n  if reset_preconditioner:\n    reset_frequency = int(np.round(1 / (1 - beta2))) if beta2 != 1 else None\n    beta2 = 1.0\n\n  generate_fd_metrics = generate_fd_metrics and frequent_directions\n\n  if frequent_directions and compression_rank <= 0:\n    raise ValueError(\"frequent_directions=True requires compression_rank > 0,\"\n                     f\" found {compression_rank}\")\n\n  if average_grad and not frequent_directions:\n    raise ValueError(\"average_grad requested but frequent_directions is False\")\n\n  if frequent_directions and (statistics_compute_steps !=\n                              preconditioning_compute_steps):\n    raise ValueError(\"frequent_directions=True requires \"\n                     f\"statistics_compute_steps ({statistics_compute_steps}) \"\n                     \"to equal != preconditioning_compute_steps \"\n                     f\"({preconditioning_compute_steps})\")\n\n  def _graft_type_has_diagonal_statistics():\n    \"\"\"Returns True if using diagonal firt order method for grafting.\"\"\"\n    return graft_type not in [\n        GraftingType.SGD, GraftingType.SQRT_N, GraftingType.NONE]\n\n  def quantized_dtype_for_momentum_buffers(var):\n    return jnp.int8 if best_effort_memory_usage_reduction and len(\n        var.shape) > 1 else jnp.float32\n\n  quantize_second_moment = (\n      best_effort_memory_usage_reduction and\n      not compression_rank and not frequent_directions and\n      batch_axis_name)\n\n  # Preconditioner and statistics are both stores as int16 in this mode.\n  # We take out the diagonal to make quantization easier.\n  def quantized_dtype_for_second_moment_statistics_buffers():\n    return jnp.int16 if quantize_second_moment else jnp.float32\n\n  # Preconditioner and statistics are both stores as int16 in this mode.\n  # We take out the diagonal to make quantization easier.\n  def quantized_dtype_for_second_moment_preconditioner_buffers():\n    return jnp.int16 if quantize_second_moment else jnp.float32\n\n  # _quantized_matrix_inverse_pth_root_vmap implementation assumes\n  # that preconditioner is quantized if and only if stats is quantized.\n  qdt_precond = quantized_dtype_for_second_moment_preconditioner_buffers()\n  qdt_stat = quantized_dtype_for_second_moment_statistics_buffers()\n  assert qdt_precond == qdt_stat\n\n  def _to_float(maybe_quantized):\n    if isinstance(maybe_quantized, QuantizedValue):\n      return maybe_quantized.to_float()\n    else:\n      return maybe_quantized\n\n  def _maybe_quantize_statistics(statistics_list):\n    return _maybe_quantize_matrices_with_dtype(\n        statistics_list, quantized_dtype_for_second_moment_statistics_buffers())\n\n  def _maybe_quantize_preconditioners(statistics_list):\n    return _maybe_quantize_matrices_with_dtype(\n        statistics_list,\n        quantized_dtype_for_second_moment_preconditioner_buffers())\n\n  def _maybe_quantize_matrices_with_dtype(statistics_list, quantized_dtype):\n    if quantized_dtype != jnp.float32:\n      return ([\n          QuantizedValue.from_float_value(\n              s, quantized_dtype, extract_diagonal=True)\n          for s in statistics_list\n      ])\n    else:\n      return statistics_list\n\n  def _maybe_dequantize_preconditioners(preconditioner_list):\n    return _maybe_dequantize_matrices_with_dtype(\n        preconditioner_list,\n        quantized_dtype_for_second_moment_preconditioner_buffers())\n\n  def _maybe_dequantize_matrices_with_dtype(statistics_list, quantized_dtype):\n    if quantized_dtype != jnp.float32:\n      return [s.to_float() for s in statistics_list]\n    else:\n      return statistics_list\n\n  def _quantize_diagonal_statistics(diagonal_statistics):\n    return QuantizedValue.from_float_value(diagonal_statistics, jnp.float32)\n\n  def _quantize_momentum(momentum_statistics):\n    return QuantizedValue.from_float_value(\n        momentum_statistics,\n        quantized_dtype_for_momentum_buffers(momentum_statistics))\n\n  def preconditioner_from_params(param):\n    \"\"\"Returns a Preconditioner object for given param.\"\"\"\n    return Preconditioner(\n        param,\n        block_size,\n        merge_small_dims_block_size,\n        best_effort_shape_interpretation,\n        precondtioner_type,\n        compression_rank,\n    )\n\n  def precond_dim(max_size):\n    \"\"\"Derives largest preconditioner dimension.\"\"\"\n    if compression_rank != 0:\n      dim = _precond_dim(compression_rank, max_size)\n      assert dim < max_size, (\"all layers are too small for compression_rank\")\n      return dim\n    return max_size\n\n  def pad_and_maybe_zero_preconditioners(preconditioners, total, max_size,\n                                         step):\n    \"\"\"Pad preconditioners up to total x max_size x precond_dim(max_size).\"\"\"\n    pd = precond_dim(max_size)\n\n    def maybe_reset_preconditioner(step, preconditioner):\n      if reset_frequency is None:\n        return preconditioner\n      return jnp.where(step % reset_frequency == 0, 0.0, 1.0) * preconditioner\n\n    def _pad_preconditioner(preconditioner):\n      assert preconditioner.ndim == 2\n      r, c = preconditioner.shape\n      assert r <= max_size\n      assert c <= pd\n      pad_rows = [(0, max_size - r)]\n      pad_cols = [(0, pd - c)]\n      padding = pad_rows + pad_cols\n      preconditioner = maybe_reset_preconditioner(step, preconditioner)\n      return jnp.pad(preconditioner, padding)\n\n    last_dims_padded = [_pad_preconditioner(p) for p in preconditioners]\n    dt = preconditioners[0].dtype if preconditioners else jnp.float32\n    num_extra = total - len(last_dims_padded)\n    extra = [jnp.zeros([max_size, pd], dtype=dt)] * num_extra\n    return last_dims_padded + extra\n\n  def sharded_init_fn(params):\n    \"\"\"Returns optimizer state (for PJIT mode).\n\n    Args:\n      params: the parameters that should be updated.\n    \"\"\"\n    params_flat, treedef = jax.tree_flatten(params)\n    # Find max size to pad to.\n    max_size = 0\n    for param in params_flat:\n      preconditioner = preconditioner_from_params(param)\n      if not _skip_preconditioning(param):\n        shapes = preconditioner.shapes_for_preconditioners()\n        sizes = [s[0] for s in shapes]\n        max_size = max(max(sizes), max_size)\n\n    padded_statistics = []\n    padded_preconditioners = []\n    local_stats_flat = []\n    exponents = []\n    for param in params_flat:\n      preconditioner = preconditioner_from_params(param)\n      shapes = preconditioner.shapes_for_preconditioners()\n      sizes = []\n\n      statistics = []\n      preconditioners = []\n      index_start = len(padded_statistics)\n      if not _skip_preconditioning(param):\n        sizes = [s[0] for s in shapes]\n        shapes = preconditioner.shapes_for_preconditioners()\n        statistics = [\n            matrix_epsilon * jnp.eye(max_size, dtype=jnp.float32)\n            for s in shapes\n        ]\n        pd = precond_dim(max_size)\n        # If the preconditioner is using a low-rank representation, initialize\n        # it to zero instead of an invalid eye.\n        preconditioners = [\n            jnp.eye(max_size, pd, dtype=jnp.float32) * (pd == max_size)\n            for s in shapes\n        ]\n        padded_statistics.extend(statistics)\n        padded_preconditioners.extend(preconditioners)\n        exponent = (\n            preconditioner.exponent_for_preconditioner()\n            if exponent_override == 0 else exponent_override)\n        exponents.extend([exponent] * len(shapes))\n\n      diagonal_statistics = _quantize_diagonal_statistics(jnp.zeros_like(param))\n      diagonal_momentum = _quantize_momentum(jnp.zeros_like(param))\n      momentum = _quantize_momentum(jnp.zeros_like(param))\n\n      local_stats_flat.append(\n          LocalShardedParameterStats(  # pytype: disable=wrong-arg-types  # numpy-scalars\n              diagonal_statistics,\n              diagonal_momentum,\n              momentum,\n              init_avg_grad(param, frequent_directions and average_grad),\n              init_training_metrics(\n                  len(sizes),\n                  generate_training_metrics,\n                  generate_fd_metrics,\n              ),\n              index_start,\n              sizes))\n\n    local_stats = jax.tree_unflatten(treedef, local_stats_flat)\n    to_pad = -len(padded_statistics) % num_devices_for_pjit\n    if max_size == 0:\n      to_pad = num_devices_for_pjit\n      max_size = block_size\n      stat_dtype = jnp.float32\n    else:\n      stat_dtype = padded_statistics[0].dtype\n    # Pad the statistics and preconditioner matrices to be a multiple of\n    # num devices.\n    # TODO(rohananil): Relax to only the size of the mesh axis where the dim\n    # is split on.\n    padded_statistics.extend(\n        [jnp.eye(max_size, dtype=stat_dtype) for _ in range(to_pad)])\n    pd = precond_dim(max_size)\n    # If the preconditioner is using a low-rank representation, initialize\n    # it to zero instead of an invalid eye.\n    padded_preconditioners.extend([\n        jnp.eye(max_size, pd, dtype=stat_dtype) * (pd == max_size)\n        for _ in range(to_pad)\n    ])\n    exponents.extend([1 for _ in range(to_pad)])\n    global_stats = GlobalShardedParameterStats(\n        jnp.stack(padded_statistics), jnp.stack(padded_preconditioners),\n        jnp.stack(exponents))\n    return ShampooState(\n        count=jnp.zeros([], jnp.int32),\n        stats=ShardedShampooStats(global_stats, local_stats))\n\n  def _max_statistics_size_from_params(params):\n    max_size = 0\n    for param in params:\n      param_clone = jnp.zeros(param.shape, dtype=param.dtype)\n      preconditioner = preconditioner_from_params(param_clone)\n      if not _skip_preconditioning(param):\n        shapes = preconditioner.shapes_for_preconditioners()\n        sizes = [s[0] for s in shapes]\n        max_size = max(max(sizes), max_size)\n    return max_size\n\n  def _remove_leading_sharding_annotation(pspec):\n    \"\"\"Mapping from N-d to (N-1)-d, used for quantization, factoring etc.\"\"\"\n    # None and PSpec(None) are valid PSpecs.\n    if pspec and len(pspec) > 1:\n      return jax.sharding.PartitionSpec(*pspec[1:])\n    else:\n      return []\n\n  def sharded_init_partition_spec_fn(params, params_partition_spec,\n                                     partition_spec_for_statistics):\n    \"\"\"Returns a parallel state tree with PartitionSpec associated with state.\n\n\n    Args:\n      params: A pytree with params.\n      params_partition_spec: A pytree with PartitionSpec for params.\n      partition_spec_for_statistics: PartitionSpec for the statistics.\n    \"\"\"\n    # Parallel lists of spec, and params.\n    param_pspec_flat, _ = jax.tree_flatten(\n        params_partition_spec, is_leaf=lambda x: x is None)\n    params_flat, treedef = jax.tree_flatten(params)\n    assert param_pspec_flat\n    assert params_flat\n    # Step is replicated across cores.\n    # None means cores.\n    local_stats_flat = []\n    num_statistics = 0\n    for param, param_pspec in zip(params_flat, param_pspec_flat):\n      param_clone = jnp.zeros(param.shape, dtype=param.dtype)\n      preconditioner = preconditioner_from_params(param_clone)\n      shapes = preconditioner.shapes_for_preconditioners()\n      sizes = []\n\n      index_start = num_statistics\n      if not _skip_preconditioning(param):\n        sizes = [s[0] for s in shapes]\n        shapes = preconditioner.shapes_for_preconditioners()\n        num_statistics += len(shapes)\n\n      qdtype = quantized_dtype_for_momentum_buffers(param)\n      m1_pspec = param_pspec\n      m2_pspec = param_pspec\n      m1_scale_pspec = []\n      m2_scale_pspec = []\n      if qdtype != jnp.float32:\n        m1_scale_pspec = _remove_leading_sharding_annotation(m1_pspec)\n        m2_scale_pspec = _remove_leading_sharding_annotation(m2_pspec)\n\n      local_stats_flat.append(\n          LocalShardedParameterStats(  # pytype: disable=wrong-arg-types  # numpy-scalars\n              QuantizedValue(param_pspec, [], [], jnp.float32, False,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             list(param.shape)),\n              QuantizedValue(m1_pspec, [], m1_scale_pspec, qdtype, False,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             list(param.shape)),\n              QuantizedValue(m2_pspec, [], m2_scale_pspec, qdtype, False,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             list(param.shape)),\n              init_avg_grad_pspec(param_pspec, frequent_directions and\n                                  average_grad),\n              init_training_metrics_pspec(\n                  generate_training_metrics,\n                  generate_fd_metrics,\n              ),\n              index_start,\n              sizes))\n\n    local_stats = jax.tree_unflatten(treedef, local_stats_flat)\n    global_stats = GlobalShardedParameterStats(partition_spec_for_statistics,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                               partition_spec_for_statistics,\n                                               jax.sharding.PartitionSpec())\n    count_pspec = jax.sharding.PartitionSpec()\n    return ShampooState(  # pytype: disable=wrong-arg-types  # numpy-scalars\n        count=count_pspec, stats=ShardedShampooStats(global_stats, local_stats))\n\n  def sharded_init_shape_and_dtype_fn(params):\n    \"\"\"Returns a parallel state tree with shape, dtype associated with state.\n\n\n    Args:\n      params: A pytree with params.\n    \"\"\"\n    # Parallel lists of spec, and params.\n    params_flat, treedef = jax.tree_flatten(params)\n    assert params_flat\n    # Step is replicated across cores.\n    # None means cores.\n    local_stats_flat = []\n    num_statistics = 0\n    for param in params_flat:\n      param_clone = jnp.zeros(param.shape, dtype=param.dtype)\n      preconditioner = preconditioner_from_params(param_clone)\n      shapes = preconditioner.shapes_for_preconditioners()\n      sizes = []\n\n      index_start = num_statistics\n      if not _skip_preconditioning(param):\n        sizes = [s[0] for s in shapes]\n        shapes = preconditioner.shapes_for_preconditioners()\n        num_statistics += len(shapes)\n\n      qdtype = quantized_dtype_for_momentum_buffers(param)\n      m1_shape_and_dtype = [list(param.shape), param.dtype]\n      m2_shape_and_dtype = [list(param.shape), param.dtype]\n      m1_scale_shape_and_dtype = []\n      m2_scale_shape_and_dtype = []\n      if qdtype != jnp.float32:\n        m1_scale_shape_and_dtype = [list(param.shape)[1:], qdtype]\n        m2_scale_shape_and_dtype = [list(param.shape)[1:], qdtype]\n\n      diagonal_statistics_shape_and_dtype = [list(param.shape), param.dtype]\n      local_stats_flat.append(\n          LocalShardedParameterStats(  # pytype: disable=wrong-arg-types  # numpy-scalars\n              QuantizedValue(diagonal_statistics_shape_and_dtype, [], [],  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             jnp.float32, False, list(param.shape)),\n              QuantizedValue(m1_shape_and_dtype, [], m1_scale_shape_and_dtype,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             qdtype, False, list(param.shape)),\n              QuantizedValue(m2_shape_and_dtype, [], m2_scale_shape_and_dtype,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                             qdtype, False, list(param.shape)),\n              init_avg_grad_shape(param, frequent_directions and average_grad),\n              init_training_metrics_shapes(\n                  len(sizes),\n                  generate_training_metrics,\n                  generate_fd_metrics,\n              ),\n              index_start,\n              sizes,\n          ))\n\n    local_stats = jax.tree_unflatten(treedef, local_stats_flat)\n    max_statistics_size = _max_statistics_size_from_params(params_flat)\n    to_pad = -num_statistics % num_devices_for_pjit\n    num_statistics += to_pad\n    if num_statistics == 0:\n      num_statistics = num_devices_for_pjit\n      max_statistics_size = block_size\n    statistics_shape = [\n        num_statistics, max_statistics_size, max_statistics_size\n    ]\n    preconditioners_shape = [\n        num_statistics, max_statistics_size,\n        precond_dim(max_statistics_size)\n    ]\n    global_stats = GlobalShardedParameterStats(  # pytype: disable=wrong-arg-types  # numpy-scalars\n        [statistics_shape, jnp.float32], [preconditioners_shape, jnp.float32],\n        [[num_statistics], jnp.int32])\n    return ShampooState(  # pytype: disable=wrong-arg-types  # numpy-scalars\n        count=[[], jnp.float32],\n        stats=ShardedShampooStats(global_stats, local_stats))\n\n  def sharded_update_fn(grads, state, params):\n    \"\"\"Transform the input gradient and update all statistics in sharded mode.\n\n    Args:\n      grads: the gradient tensors for the parameters.\n      state: a named tuple containing the state of the optimizer\n      params: the parameters that should be updated.\n\n    Returns:\n      A tuple containing the new parameters and the new optimizer state.\n    \"\"\"\n    params_flat, treedef = jax.tree_flatten(params)\n    grads_flat = treedef.flatten_up_to(grads)\n\n    global_stats = state.stats.global_stats\n    local_stats_flat = treedef.flatten_up_to(state.stats.local_stats)\n    stats_flat = []\n    for local_stat in local_stats_flat:\n      stats_flat.append(\n          _convert_to_parameter_stats(\n              global_stats,\n              local_stat,\n              compression_rank,\n          ))\n\n    new_stats_flat = jax.tree_map(\n        lambda g, s, p: _compute_stats(g, s, p, state.count), grads_flat,\n        stats_flat, params_flat)\n\n    outputs = jax.tree_map(\n        lambda g, s, p: _transform_grad(g, s, p, state.count), grads_flat,\n        new_stats_flat, params_flat)\n    updates_flat, new_stats_flat = list(zip(*outputs)) if outputs else ((), ())\n\n    updates = jax.tree_unflatten(treedef, updates_flat)\n    new_local_stats_flat = []\n    for new_stat, local_stat in zip(new_stats_flat, local_stats_flat):\n      new_local_stats_flat.append(\n          _convert_from_parameter_stats(\n              new_stat,\n              local_stat,\n          ))\n\n    max_size = global_stats.statistics.shape[1]\n    new_padded_statistics = []\n    padding_starts = []\n    for stat in new_stats_flat:\n      new_padded_statistics.extend(\n          [pad_square_matrix(stat, max_size) for stat in stat.statistics])\n      padding_starts.extend([len(stat) for stat in stat.statistics])\n\n    # Create global stats\n    # TODO(rohananil): Preconditioner is not updated every step, so cost of\n    # stack/pad can be obviated away.\n    # Pad the statistics and preconditioner matrices to be a multiple of\n    # num devices.\n    # TODO(rohananil): Relax to only the size of the mesh axis where the dim\n    # is split on.\n    to_pad = -len(new_padded_statistics) % num_devices_for_pjit\n    if not new_padded_statistics:\n      to_pad = num_devices_for_pjit\n      stat_dtype = jnp.float32\n    else:\n      stat_dtype = new_padded_statistics[0].dtype\n\n    new_padded_statistics.extend(\n        [jnp.eye(max_size, dtype=stat_dtype) for _ in range(to_pad)])\n    padding_starts += [0] * to_pad\n\n    if reuse_preconditioner:\n      prev_preconditioners = []\n      for stat in new_stats_flat:\n        prev_preconditioners.extend(stat.preconditioners)\n      prev_padded_preconditioners = pad_and_maybe_zero_preconditioners(\n          prev_preconditioners, len(new_padded_statistics), max_size,\n          state.count)\n    else:\n      prev_padded_preconditioners = None\n\n    new_stacked_padded_statistics = jnp.stack(new_padded_statistics)\n    new_stacked_padded_statistics = pjit.with_sharding_constraint(\n        new_stacked_padded_statistics, statistics_partition_spec)\n    stacked_padding_starts = jnp.array(padding_starts, jnp.int32)\n    prev_stacked_padded_preconditioners = _maybe(jnp.stack)(\n        prev_padded_preconditioners)\n    prev_stacked_padded_preconditioners = _maybe(pjit.with_sharding_constraint)(\n        prev_padded_preconditioners, statistics_partition_spec)\n\n    def _internal_inverse_pth_root_all():\n      preconditioners, metrics = _matrix_inverse_pth_root_pjit(\n          new_stacked_padded_statistics,\n          global_stats.exponents,\n          stacked_padding_starts,\n          prev_stacked_padded_preconditioners,\n          statistics_partition_spec,\n      )\n      return preconditioners, metrics\n\n    scheduled_preconditioning_compute_steps = (\n        decay_preconditioning_compute_steps\n        and end_preconditioning_compute_steps\n        and callable(learning_rate)\n    )\n\n    preconditioning_compute_steps_t = preconditioning_compute_steps\n    if scheduled_preconditioning_compute_steps:\n      preconditioning_compute_steps_t = preconditioning_compute_steps_schedule(\n          learning_rate,\n          preconditioning_compute_steps,\n          end_preconditioning_compute_steps,\n          state.count,\n      )\n\n    perform_step = state.count % preconditioning_compute_steps_t == 0\n\n    def _update_preconditioners():\n      # Passing statistics instead of preconditioners as they are similarly\n      # shaped tensors. Note statistics will be ignored as we are passing in\n      # a large error value.\n      pd = precond_dim(new_stacked_padded_statistics.shape[2])\n      preconditioners_init = new_stacked_padded_statistics[:, :, :pd]\n      n = new_stacked_padded_statistics.shape[0]\n      metrics_init = cast(\n          TrainingMetrics,\n          init_training_metrics(\n              n,\n              generate_training_metrics=True,\n              generate_fd_metrics=generate_fd_metrics,\n          ))\n      new_errors = jnp.ones_like(metrics_init.inverse_pth_root_errors) * (\n          inverse_failure_threshold)\n      metrics_init = metrics_init.replace(inverse_pth_root_errors=new_errors)\n      init_state = [preconditioners_init, metrics_init]\n      return efficient_cond(perform_step, _internal_inverse_pth_root_all,\n                            init_state)\n\n    (new_preconditioners, metrics, _, _) = _update_preconditioners_fn(\n        _internal_inverse_pth_root_all,\n        _update_preconditioners,\n        preconditioning_compute_steps_t,\n        scheduled_preconditioning_compute_steps,\n        quantized=False,\n    )\n\n    if generate_training_metrics:\n      new_local_stats_flat = _add_metrics_into_local_stats(\n          new_local_stats_flat, metrics, ~perform_step)\n    new_local_stats = jax.tree_unflatten(treedef, new_local_stats_flat)\n    errors = metrics.inverse_pth_root_errors\n    errors = errors.reshape((-1, 1, 1))\n    predicate = jnp.logical_or(\n        jnp.isnan(errors),\n        errors >= inverse_failure_threshold).astype(new_preconditioners.dtype)\n    # TODO(rohananil): Check for numerical instabilities.\n    new_conditional_preconditioners = (\n        predicate * global_stats.preconditioners +\n        (1.0 - predicate) * new_preconditioners)\n    new_global_stats = GlobalShardedParameterStats(\n        new_stacked_padded_statistics, new_conditional_preconditioners,\n        global_stats.exponents)\n    new_shampoo_state = ShampooState(\n        count=state.count + 1,\n        stats=ShardedShampooStats(new_global_stats, new_local_stats))\n    return updates, new_shampoo_state\n\n  def init_fn(params):\n    \"\"\"Initialise the optimiser's state.\"\"\"\n\n    def _init(param):\n      preconditioner = preconditioner_from_params(param)\n      statistics = []\n      preconditioners = []\n      if not _skip_preconditioning(param):\n        shapes = preconditioner.shapes_for_preconditioners()\n        statistics = [\n            matrix_epsilon * jnp.eye(s[0], dtype=jnp.float32) for s in shapes\n        ]\n        # If the preconditioner is using a low-rank representation, initialize\n        # it to zero instead of an invalid eye.\n        preconditioners = [\n            jnp.eye(s[0], s[1], dtype=jnp.float32) * (s[0] == s[1])\n            for s in shapes\n        ]\n\n      diagonal_statistics = []\n      if _graft_type_has_diagonal_statistics():\n        diagonal_statistics = jnp.zeros_like(param)\n\n      diagonal_momentum = _quantize_momentum(jnp.zeros_like(param))\n      momentum = _quantize_momentum(jnp.zeros_like(param))\n\n      return ParameterStats(\n          _quantize_diagonal_statistics(diagonal_statistics),\n          _maybe_quantize_statistics(statistics),\n          _maybe_quantize_preconditioners(preconditioners),\n          diagonal_momentum,\n          momentum,\n          init_avg_grad(param, frequent_directions and average_grad),\n          init_training_metrics(\n              len(statistics),\n              generate_training_metrics,\n              generate_fd_metrics,\n          ))\n\n    return ShampooState(\n        count=jnp.zeros([], jnp.int32), stats=jax.tree_map(_init, params))\n\n  def _skip_preconditioning(param):\n    return len(param.shape) < skip_preconditioning_rank_lt or any(\n        [s > skip_preconditioning_dim_size_gt for s in param.shape])\n\n  def _compute_stats(grad, state, param, step):\n    \"\"\"Compute per-parameter statistics.\"\"\"\n    preconditioner = preconditioner_from_params(param)\n    new_statistics = [[]] * len(state.statistics)\n    w1 = beta2\n    w2 = jnp.where(beta2 == 1.0, beta2, 1.0 - beta2)\n    new_avg_grad = optax.MaskedNode()\n    if not _skip_preconditioning(param):\n\n      if frequent_directions and average_grad:\n        new_avg_grad = jnp.where(\n            jnp.logical_or(statistics_compute_steps == 1,\n                           step % statistics_compute_steps == 1), grad,\n            state.avg_grad + grad)\n        grad = new_avg_grad / statistics_compute_steps\n\n      def compute_updated_statistics():\n        return preconditioner.updated_statistics_from_grad(\n            state.statistics,\n            grad,\n            w1=w1,\n            w2=w2,\n            to_float=_to_float,\n            from_float=lambda x: _maybe_quantize_statistics([x])[0],\n            precision=tensordot_precision,\n            frequent_directions=frequent_directions,\n        )\n\n      if statistics_compute_steps > 1:\n        perform_step = step % statistics_compute_steps == 0\n        init_state = state.statistics\n        new_statistics = list(\n            efficient_cond(perform_step, compute_updated_statistics,\n                           init_state))\n      else:\n        new_statistics = compute_updated_statistics()\n\n    return ParameterStats(\n        state.diagonal_statistics,\n        new_statistics,\n        state.preconditioners,\n        state.diagonal_momentum,\n        state.momentum,\n        new_avg_grad,\n        state.training_metrics)\n\n  mi_pth_root = functools.partial(\n      matrix_inverse_pth_root,\n      ridge_epsilon=matrix_epsilon,\n      precision=precision,\n      relative_matrix_epsilon=relative_matrix_epsilon,\n      lobpcg_topk_precondition=lobpcg_topk_precondition,\n      lobpcg_max_iter=lobpcg_max_iter,\n      eigh=eigh)\n\n  # Wrap mi_pth_root(stats, exponents, padding_start) method\n  # around compression_rank handling.\n  if compression_rank != 0:\n    original_mi_pth_root = mi_pth_root\n\n    def small_mi_pth_root(stats, exponents, padding_start, prev):\n      # We would only run the full root if\n      # compression_rank + 2 >= dimension.\n      #\n      # But this should only ever happen for small tensors\n      # below the matrix size thanks to asserts in precond_dim().\n      root, metrics = original_mi_pth_root(\n          stats, exponents, padding_start=padding_start, prev=prev)\n      precond_dim = _precond_dim(compression_rank, stats.shape[0])\n      # By assumption, precond_dim >= padding_start; we're cutting\n      # off zeros here.\n      if generate_training_metrics and generate_fd_metrics:\n        metrics = metrics.replace(fd=FDDiagnostics())\n      return root[:, :precond_dim], metrics\n\n    def new_mi_pth_root(stats, exponents, padding_start, prev):\n      # padding_start == true unpacked gradient dimension size.\n      should_compress = _should_compress(compression_rank, padding_start)\n\n      if frequent_directions:\n        special_root = functools.partial(\n            _fd_update_root,\n            rank=compression_rank,\n            ridge_epsilon=matrix_epsilon,\n            relative_matrix_epsilon=relative_matrix_epsilon,\n            decay=beta2,\n            padding_start=padding_start,\n            prev=prev,\n            generate_training_metrics=generate_training_metrics,\n            generate_fd_metrics=generate_fd_metrics,\n        )\n      else:\n        special_root = functools.partial(\n            _low_rank_root,\n            compression_rank=compression_rank,\n            ridge_epsilon=matrix_epsilon,\n            relative_matrix_epsilon=relative_matrix_epsilon,\n            padding_start=padding_start,\n            prev=prev,\n        )\n\n      return jax.lax.cond(\n          should_compress, special_root,\n          functools.partial(\n              small_mi_pth_root,\n              padding_start=padding_start,\n              prev=prev,\n          ), stats, exponents)\n\n    mi_pth_root = new_mi_pth_root\n\n  def _matrix_inverse_pth_root_vmap(xs, ps, padding_starts, prev):\n    return jax.vmap(mi_pth_root)(\n        xs, ps, padding_start=padding_starts, prev=prev)\n\n  def _quantized_matrix_inverse_pth_root_vmap(qxs,\n                                              qds,\n                                              qbs,\n                                              ps,\n                                              padding_starts,\n                                              qpxs=None,\n                                              qpds=None,\n                                              qpbs=None):\n    assert (qpxs is None) == (qpds is None) == (qpbs is None)\n    assert (qpxs is None) == (not reuse_preconditioner)\n\n    def _quantized_to_float(qx, qd, qb):\n      qv = QuantizedValue(qx, qd, qb, qx.dtype, True, list(qx.shape))\n      return qv.to_float()\n\n    def matrix_inverse_pth_root_wrapper(qx, qd, qb, p, padding_start, qpx, qpd,\n                                        qpb):\n      v = _quantized_to_float(qx, qd, qb)\n      prev = _maybe(_quantized_to_float)(qpx, qpd, qpb)\n      preconditioner, metrics = mi_pth_root(\n          v, p, padding_start=padding_start, prev=prev)\n      qp = QuantizedValue.from_float_value(preconditioner, qx.dtype, True)\n      return qp.quantized, qp.diagonal, qp.bucket_size, metrics\n\n    return jax.vmap(matrix_inverse_pth_root_wrapper)(qxs, qds, qbs, ps,\n                                                     padding_starts, qpxs, qpds,\n                                                     qpbs)\n\n  def _matrix_inverse_pth_root_pjit(xs,\n                                    ps,\n                                    padding_starts,\n                                    prev_preconds=None,\n                                    statistics_partition_spec=None):\n    # Partition the concatenated statistics matrix across all cores.\n    pspec_for_partition = preconditioner_partition_spec\n    partitioned_xs = pjit.with_sharding_constraint(xs, pspec_for_partition)\n    if preconditioner_partition_spec:\n      partitioned_ps_spec = jax.sharding.PartitionSpec(\n          preconditioner_partition_spec[0]\n      )\n    else:\n      partitioned_ps_spec = None\n    partitioned_ps = pjit.with_sharding_constraint(ps, partitioned_ps_spec)\n    partitioned_prev_preconds = _maybe(pjit.with_sharding_constraint)(\n        prev_preconds, preconditioner_partition_spec)\n    partitioned_padding_starts = pjit.with_sharding_constraint(\n        padding_starts, partitioned_ps_spec)  # paddings are scalars like ps.\n    # Run matrix inverse pth root on each shard.\n    partitioned_preconditioners, partitioned_metrics = (\n        _matrix_inverse_pth_root_vmap(\n            partitioned_xs,\n            partitioned_ps,\n            partitioned_padding_starts,\n            prev=partitioned_prev_preconds))\n    # Reshard output to have the same PSpec as input. This is required to avoid\n    # vmap seeing the full set of statistics.\n    partitioned_preconditioners = pjit.with_sharding_constraint(\n        partitioned_preconditioners, pspec_for_partition)\n    # Recombine the outputs at each core.\n    preconditioners = pjit.with_sharding_constraint(partitioned_preconditioners,\n                                                    statistics_partition_spec)\n    metrics = pjit.with_sharding_constraint(partitioned_metrics,\n                                            jax.sharding.PartitionSpec())\n    return preconditioners, metrics\n\n  def _pmap_compute_preconditioners(states, step, statistics,\n                                    num_statistics_per_state, original_shapes,\n                                    exponents, max_size, prev_preconditioners):\n    \"\"\"Computes preconditioners for given statistics in states in PMAP mode.\n\n    Args:\n      states: A list of optimizer states.\n      step: Current step number\n      statistics: A list of statistics for all variables (for every dim)\n      num_statistics_per_state: Number of statistis per state to reconstruct\n        output states.\n      original_shapes: A list of shapes of the statistics.\n      exponents: Exponent power to use for inverse-pth roots.\n      max_size: Maximum dim of the statistics to pad.\n      prev_preconditioners: Previously available preconditioner.\n\n    Returns:\n      New optimizer states after computing the preconditioner.\n    \"\"\"\n    if batch_axis_name:\n      num_devices = lax.psum(1, batch_axis_name)\n    else:\n      num_devices = 1\n    num_statistics = len(statistics)\n    # Pad statistics and exponents to next multiple of num_devices.\n    packed_statistics = [\n        pad_square_matrix(stat, max_size) for stat in statistics\n    ]\n    to_pad = -num_statistics % num_devices\n    packed_statistics.extend([\n        jnp.eye(max_size, dtype=packed_statistics[0].dtype)\n        for _ in range(to_pad)\n    ])\n    exponents.extend([1 for _ in range(to_pad)])\n    paddings = [len(stat) for stat in statistics] + [0] * to_pad\n\n    if not packed_statistics:\n      return states\n\n    if reuse_preconditioner:\n      assert len(prev_preconditioners) == num_statistics\n      packed_preconditioners = pad_and_maybe_zero_preconditioners(\n          prev_preconditioners, len(packed_statistics), max_size, step)\n    else:\n      packed_preconditioners = None\n\n    all_statistics = batch(packed_statistics, num_devices)\n    all_exponents = batch(exponents, num_devices)\n    all_paddings = batch(paddings, num_devices)\n    all_preconditioners = _maybe(batch)(packed_preconditioners, num_devices)\n\n    def _internal_inverse_pth_root_all():\n      if batch_axis_name:\n        current_replica = lax.axis_index(batch_axis_name)\n        preconditioners, metrics = _matrix_inverse_pth_root_vmap(\n            all_statistics[current_replica],\n            all_exponents[current_replica],\n            all_paddings[current_replica],\n            _maybe_ix(all_preconditioners, current_replica),\n        )\n        preconditioners = jax.lax.all_gather(preconditioners, batch_axis_name)\n        metrics = jax.lax.all_gather(metrics, batch_axis_name)\n        preconditioners_flat = unbatch(preconditioners)\n        metrics_flat = jax.tree_map(unbatch, metrics)\n      else:\n        preconditioners, metrics = _matrix_inverse_pth_root_vmap(\n            all_statistics[0],\n            all_exponents[0],\n            all_paddings[0],\n            _maybe_ix(all_preconditioners, 0),\n        )\n        preconditioners_flat = unbatch(jnp.stack([preconditioners]))\n        metrics = jax.tree_map(\n            functools.partial(jnp.expand_dims, axis=0), metrics)\n        metrics_flat = jax.tree_map(unbatch, metrics)\n\n      return preconditioners_flat, metrics_flat\n\n    scheduled_preconditioning_compute_steps = (\n        decay_preconditioning_compute_steps\n        and end_preconditioning_compute_steps\n        and callable(learning_rate)\n    )\n\n    preconditioning_compute_steps_t = preconditioning_compute_steps\n    if scheduled_preconditioning_compute_steps:\n      preconditioning_compute_steps_t = preconditioning_compute_steps_schedule(\n          learning_rate,\n          preconditioning_compute_steps,\n          end_preconditioning_compute_steps,\n          step,\n      )\n\n    perform_step = step % preconditioning_compute_steps_t == 0\n\n    def _update_preconditioners():\n      # Passing statistics instead of preconditioners as they are similarly\n      # shaped tensors. Note statistics will be ignored as we are passing in\n      # a large error value.\n      preconditioners_init = [\n          s[:, :precond_dim(s.shape[0])] for s in packed_statistics\n      ]\n      n = len(packed_statistics)\n      metrics_init = jax.tree_map(\n          lambda x: [x] * n,\n          default_training_metrics(\n              generate_fd_metrics\n          ).replace(inverse_pth_root_errors=inverse_failure_threshold))\n      init_state = [preconditioners_init, metrics_init]\n      return efficient_cond(perform_step, _internal_inverse_pth_root_all,\n                            init_state)\n\n    (preconditioners_flat, metrics_flat, _, _) = _update_preconditioners_fn(\n        _internal_inverse_pth_root_all,\n        _update_preconditioners,\n        preconditioning_compute_steps_t,\n        scheduled_preconditioning_compute_steps,\n        quantized=False,\n    )\n\n    def _skip(error):\n      condition = jnp.logical_or(\n          jnp.isnan(error), error >= inverse_failure_threshold)\n      return condition.astype(error.dtype)\n\n    def _select_preconditioner(error, new_p, old_p):\n      return lax.cond(\n          _skip(error), lambda _: old_p, lambda _: new_p, operand=None)\n\n    new_preconditioners_flat = []\n    new_errors_flat = metrics_flat.inverse_pth_root_errors\n    for p, shape, prev_p, error in zip(preconditioners_flat, original_shapes,\n                                       prev_preconditioners, new_errors_flat):\n      new_preconditioners_flat.append(\n          _select_preconditioner(error, p[:shape[0], :shape[1]], prev_p))\n\n    assert len(states) == len(num_statistics_per_state)\n    assert len(new_preconditioners_flat) == num_statistics\n    assert len(new_errors_flat) == len(packed_statistics), (\n        len(new_errors_flat), len(packed_statistics))\n    assert len(new_errors_flat) == num_statistics + to_pad, (\n        len(new_errors_flat), num_statistics, to_pad)\n\n    # Add back empty preconditioners so we that we can set the optimizer state.\n    preconditioners_for_states = []\n    idx = 0\n    metrics_for_states = []\n    for num_statistics, state in zip(num_statistics_per_state, states):\n      if num_statistics == 0:\n        preconditioners_for_states.append([])\n        metrics_for_states.append(\n            init_training_metrics(0, generate_training_metrics))\n      else:\n        preconditioners_for_state = new_preconditioners_flat[idx:idx +\n                                                             num_statistics]\n        assert len(state.statistics) == len(preconditioners_for_state)\n        preconditioners_for_states.append(preconditioners_for_state)\n\n        if generate_training_metrics:\n          # pylint:disable=cell-var-from-loop Used immediately.\n          metrics_for_state = jax.tree_map(\n              lambda x: jnp.stack(x[idx:idx + num_statistics]),\n              metrics_flat,\n              is_leaf=lambda x: isinstance(x, list))\n          assert jax.tree_util.tree_all(\n              jax.tree_map(lambda x: len(state.statistics) == len(x),\n                           metrics_for_state))\n          # If we skipped preconditioner computation, record old metrics.\n          metrics_for_state = efficient_cond(perform_step,\n                                             lambda: [metrics_for_state],\n                                             [state.training_metrics])[0]\n          # pylint:enable=cell-var-from-loop\n        else:\n          metrics_for_state = optax.MaskedNode()\n        metrics_for_states.append(metrics_for_state)\n\n        idx += num_statistics\n    new_states = []\n    for state, new_preconditioners, new_metrics in zip(\n        states, preconditioners_for_states, metrics_for_states):\n      # Note the preconditioner may have been skipped, but we still update the\n      # metrics with the new error values; whether the preconditioner that's\n      # actively being used is stale can be derived from the new_metrics\n      # being greater than the failure threshold.\n      new_states.append(\n          ParameterStats(\n              state.diagonal_statistics,\n              state.statistics,\n              new_preconditioners,\n              state.diagonal_momentum,\n              state.momentum,\n              state.avg_grad,\n              new_metrics))\n\n    return new_states\n\n  def _pmap_quantized_compute_preconditioners(states, step, statistics,\n                                              num_statistics_per_state,\n                                              original_shapes, exponents,\n                                              max_size, prev_preconditioners):\n    \"\"\"Computes preconditioners for given statistics in states in PMAP mode.\n\n    For quantization, each statistic is represented by three values:\n      quantized matrix, diagonal, and bucket sizes, we run inverse pth-roots\n      without ever recreating the original matrix in f32.\n\n    Args:\n      states: A list of optimizer states.\n      step: Current step number\n      statistics: A list of statistics for all variables (for every dim)\n      num_statistics_per_state: Number of statistis per state to reconstruct\n        output states.\n      original_shapes: A list of shapes of the statistics.\n      exponents: Exponent power to use for inverse-pth roots.\n      max_size: Maximum dim of the statistics to pad.\n      prev_preconditioners: Previously available preconditioner.\n\n    Returns:\n      New optimizer states after computing the preconditioner.\n    \"\"\"\n    num_devices = lax.psum(1, batch_axis_name)\n    num_statistics = len(statistics)\n    quantized_dtype = quantized_dtype_for_second_moment_statistics_buffers()\n    # Complexity here is around: shapes needing be statically shaped,\n    # our custom quantization type requires a different type of packing.\n\n    # Parallel tensors:\n    # quantized [dxd]\n    # diagonals [d] f32\n    # bucket_sizes [d] f32\n    packed_quantized_statistics = [\n        pad_square_matrix(stat.quantized, max_size) for stat in statistics\n    ]\n    packed_quantized_diagonals = [\n        pad_vector(stat.diagonal, max_size) for stat in statistics\n    ]\n    packed_quantized_bucket_sizes = [\n        pad_vector(stat.bucket_size, max_size) for stat in statistics\n    ]\n\n    to_pad = -num_statistics % num_devices\n    padded_eye = jnp.eye(max_size, dtype=jnp.float32)\n    quantized_eye = QuantizedValue.from_float_value(padded_eye, quantized_dtype,\n                                                    True)\n    packed_quantized_statistics.extend(\n        [quantized_eye.quantized for _ in range(to_pad)])\n    packed_quantized_diagonals.extend(\n        [quantized_eye.diagonal for _ in range(to_pad)])\n    packed_quantized_bucket_sizes.extend(\n        [quantized_eye.bucket_size for _ in range(to_pad)])\n    exponents.extend([1 for _ in range(to_pad)])\n    paddings = [len(stat.quantized) for stat in statistics] + [0] * to_pad\n\n    if not packed_quantized_statistics:\n      return states\n\n    if reuse_preconditioner:\n      total = len(packed_quantized_statistics)\n      packed_quantized_precond_mats = pad_and_maybe_zero_preconditioners(\n          [p.quantized for p in prev_preconditioners],\n          total,\n          max_size,\n          step,\n      )\n      packed_quantized_precond_diagonals = [\n          pad_vector(p.diagonal, max_size) for p in prev_preconditioners\n      ] + packed_quantized_diagonals[total - to_pad:]\n      packed_quantized_precond_bucket_sizes = [\n          pad_vector(p.bucket_size, max_size) for p in prev_preconditioners\n      ] + packed_quantized_bucket_sizes[total - to_pad:]\n    else:\n      (packed_quantized_precond_mats, packed_quantized_precond_diagonals,\n       packed_quantized_precond_bucket_sizes) = (None, None, None)\n\n    all_quantized_statistics = batch(packed_quantized_statistics, num_devices)\n    all_quantized_diagonals = batch(packed_quantized_diagonals, num_devices)\n    all_quantized_bucket_sizes = batch(packed_quantized_bucket_sizes,\n                                       num_devices)\n    all_exponents = batch(exponents, num_devices)\n    all_paddings = batch(paddings, num_devices)\n    all_quantized_precond_mats = _maybe(batch)(packed_quantized_precond_mats,\n                                               num_devices)\n    all_quantized_precond_diagonals = _maybe(batch)(\n        packed_quantized_precond_diagonals, num_devices)\n    all_quantized_precond_bucket_sizes = _maybe(batch)(\n        packed_quantized_precond_bucket_sizes, num_devices)\n\n    def _internal_inverse_pth_root_all():\n      current_replica = lax.axis_index(batch_axis_name)\n      (quantized_preconditioners, quantized_diagonals, quantized_bucket_sizes,\n       metrics) = _quantized_matrix_inverse_pth_root_vmap(\n           all_quantized_statistics[current_replica],\n           all_quantized_diagonals[current_replica],\n           all_quantized_bucket_sizes[current_replica],\n           all_exponents[current_replica],\n           all_paddings[current_replica],\n           _maybe_ix(all_quantized_precond_mats, current_replica),\n           _maybe_ix(all_quantized_precond_diagonals, current_replica),\n           _maybe_ix(all_quantized_precond_bucket_sizes, current_replica),\n       )\n      quantized_preconditioners = jax.lax.all_gather(quantized_preconditioners,\n                                                     batch_axis_name)\n      quantized_diagonals = jax.lax.all_gather(quantized_diagonals,\n                                               batch_axis_name)\n      quantized_bucket_sizes = jax.lax.all_gather(quantized_bucket_sizes,\n                                                  batch_axis_name)\n      metrics = jax.lax.all_gather(metrics, batch_axis_name)\n      quantized_preconditioners_flat = unbatch(quantized_preconditioners)\n      quantized_diagonals_flat = unbatch(quantized_diagonals)\n      quantized_bucket_sizes_flat = unbatch(quantized_bucket_sizes)\n      metrics_flat = jax.tree_map(unbatch, metrics)\n      return (quantized_preconditioners_flat, quantized_diagonals_flat,\n              quantized_bucket_sizes_flat, metrics_flat)\n\n    scheduled_preconditioning_compute_steps = (\n        decay_preconditioning_compute_steps\n        and end_preconditioning_compute_steps\n        and callable(learning_rate)\n    )\n\n    preconditioning_compute_steps_t = preconditioning_compute_steps\n    if scheduled_preconditioning_compute_steps:\n      preconditioning_compute_steps_t = preconditioning_compute_steps_schedule(\n          learning_rate,\n          preconditioning_compute_steps,\n          end_preconditioning_compute_steps,\n          step,\n      )\n\n    perform_step = step % preconditioning_compute_steps_t == 0\n\n    def _update_quantized_preconditioners():\n      # Passing statistics instead of preconditioners as they are similarly\n      # shaped tensors. Note statistics will be ignored as we are passing in\n      # a large error value.\n      pd = precond_dim(max_size)\n      quantized_preconditioners_init = [\n          s[:, :pd] for s in packed_quantized_statistics\n      ]\n      quantized_diagonals_init = packed_quantized_diagonals\n      quantized_bucket_sizes_init = packed_quantized_bucket_sizes\n      n = len(quantized_preconditioners_init)\n      metrics_init = jax.tree_map(\n          lambda x: [x] * n,\n          default_training_metrics(\n              generate_fd_metrics\n          ).replace(inverse_pth_root_errors=inverse_failure_threshold))\n      init_state = [\n          quantized_preconditioners_init, quantized_diagonals_init,\n          quantized_bucket_sizes_init, metrics_init\n      ]\n      return efficient_cond(perform_step, _internal_inverse_pth_root_all,\n                            init_state)\n\n    (\n        quantized_preconditioners_flat,\n        quantized_diagonals_flat,\n        quantized_bucket_sizes_flat,\n        metrics_flat,\n    ) = _update_preconditioners_fn(\n        _internal_inverse_pth_root_all,\n        _update_quantized_preconditioners,\n        preconditioning_compute_steps_t,\n        scheduled_preconditioning_compute_steps,\n        quantized=True\n    )\n\n    def _skip(error):\n      condition = jnp.logical_or(\n          jnp.isnan(error), error >= inverse_failure_threshold)\n      return condition.astype(error.dtype)\n\n    def _select_preconditioner(error, new_p, old_p):\n      return lax.cond(\n          _skip(error), lambda _: old_p, lambda _: new_p, operand=None)\n\n    new_quantized_preconditioners_flat = []\n    new_quantized_diagonals_flat = []\n    new_quantized_bucket_sizes_flat = []\n    new_errors_flat = metrics_flat.inverse_pth_root_errors\n    for p, d, b, shape, prev_p, error in zip(quantized_preconditioners_flat,\n                                             quantized_diagonals_flat,\n                                             quantized_bucket_sizes_flat,\n                                             original_shapes,\n                                             prev_preconditioners,\n                                             new_errors_flat):\n      new_quantized_preconditioners_flat.append(\n          _select_preconditioner(error, p[:shape[0], :shape[1]],\n                                 prev_p.quantized))\n      new_quantized_diagonals_flat.append(\n          _select_preconditioner(error, d[:shape[0]], prev_p.diagonal))\n      new_quantized_bucket_sizes_flat.append(\n          _select_preconditioner(error, b[:shape[0]], prev_p.bucket_size))\n\n    assert len(states) == len(num_statistics_per_state)\n    assert len(new_quantized_preconditioners_flat) == num_statistics\n    assert len(new_quantized_diagonals_flat) == num_statistics\n    assert len(new_quantized_bucket_sizes_flat) == num_statistics\n\n    # Add back empty preconditioners so we that we can set the optimizer state.\n    preconditioners_for_states = []\n    metrics_for_states = []\n    idx = 0\n    for num_statistics, state in zip(num_statistics_per_state, states):\n      if num_statistics == 0:\n        preconditioners_for_states.append([])\n        metrics_for_states.append(\n            init_training_metrics(0, generate_training_metrics))\n      else:\n        quantized_preconditioners_for_state = new_quantized_preconditioners_flat[\n            idx:idx + num_statistics]\n        quantized_diagonals_for_state = new_quantized_diagonals_flat[\n            idx:idx + num_statistics]\n        quantized_bucket_sizes_for_state = new_quantized_bucket_sizes_flat[\n            idx:idx + num_statistics]\n\n        if generate_training_metrics:\n          # pylint:disable=cell-var-from-loop Used immediately.\n          metrics_for_state = jax.tree_map(\n              lambda x: jnp.stack(x[idx:idx + num_statistics]),\n              metrics_flat,\n              is_leaf=lambda x: isinstance(x, list))\n\n          assert len(\n              state.statistics) == len(quantized_preconditioners_for_state)\n          assert len(state.statistics) == len(quantized_diagonals_for_state)\n          assert len(state.statistics) == len(quantized_bucket_sizes_for_state)\n          assert jax.tree_util.tree_all(\n              jax.tree_map(lambda x: len(state.statistics) == len(x),\n                           metrics_for_state))\n\n          # If we skipped preconditioner computation, record old metrics.\n          metrics_for_state = efficient_cond(perform_step,\n                                             lambda: [metrics_for_state],\n                                             [state.training_metrics])[0]\n          # pylint:enable=cell-var-from-loop\n        else:\n          metrics_for_state = optax.MaskedNode()\n\n        quantized_preconditioners = []\n        for qv, qd, qb in zip(quantized_preconditioners_for_state,\n                              quantized_diagonals_for_state,\n                              quantized_bucket_sizes_for_state):\n          quantized_preconditioners.append(\n              QuantizedValue(qv, qd, qb, qv.dtype, True, list(qv.shape)))\n        preconditioners_for_states.append(quantized_preconditioners)\n        metrics_for_states.append(metrics_for_state)\n        idx += num_statistics\n    new_states = []\n    for state, new_preconditioners, new_metrics in zip(\n        states, preconditioners_for_states, metrics_for_states):\n      # Note the preconditioner may have been skipped, but we still update the\n      # metrics with the new error values; whether the preconditioner that's\n      # actively being used is stale can be derived from the new_metrics\n      # being greater than the failure threshold.\n      new_states.append(\n          ParameterStats(\n              state.diagonal_statistics,\n              state.statistics,\n              new_preconditioners,\n              state.diagonal_momentum,\n              state.momentum,\n              state.avg_grad,\n              new_metrics))\n\n    return new_states\n\n  def _pjit_compute_preconditioners(states, step, statistics,\n                                    num_statistics_per_state, original_shapes,\n                                    exponents, max_size, prev_preconditioners):\n    \"\"\"Computes preconditioners for given statistics in states in PJIT mode.\n\n    Args:\n      states: A list of optimizer states.\n      step: Current step number\n      statistics: A list of statistics for all variables (for every dim)\n      num_statistics_per_state: Number of statistis per state to reconstruct\n        output states.\n      original_shapes: A list of shapes of the statistics.\n      exponents: Exponent power to use for inverse-pth roots.\n      max_size: Maximum dim of the statistics to pad.\n      prev_preconditioners: Previously available preconditioner.\n\n    Returns:\n      New optimizer states after computing the preconditioner.\n    \"\"\"\n    num_statistics = len(statistics)\n    to_pad = -num_statistics % num_devices_for_pjit\n    padded_statistics = [\n        pad_square_matrix(stat, max_size) for stat in statistics\n    ]\n    padded_statistics.extend([\n        jnp.eye(max_size, dtype=padded_statistics[0].dtype)\n        for _ in range(to_pad)\n    ])\n    exponents.extend([1 for _ in range(to_pad)])\n    paddings = [len(stat) for stat in statistics] + [0] * to_pad\n\n    if reuse_preconditioner:\n      padded_preconditioners = pad_and_maybe_zero_preconditioners(\n          prev_preconditioners, len(padded_statistics), max_size, step)\n    else:\n      padded_preconditioners = None\n\n    all_statistics = jnp.stack(padded_statistics)\n    all_exponents = jnp.stack(exponents)\n    all_paddings = jnp.stack(paddings)\n    all_preconditioners = _maybe(jnp.stack)(padded_preconditioners)\n\n    def _internal_inverse_pth_root_all():\n      preconditioners, metrics = _matrix_inverse_pth_root_pjit(\n          all_statistics, all_exponents, all_paddings, all_preconditioners)\n      b1 = preconditioners.shape[0]\n\n      def split(batched_values):\n        return [\n            jnp.squeeze(v)\n            for v in jnp.split(batched_values, indices_or_sections=b1, axis=0)\n        ]\n\n      return split(preconditioners), jax.tree_map(split, metrics)\n\n    scheduled_preconditioning_compute_steps = (\n        decay_preconditioning_compute_steps\n        and end_preconditioning_compute_steps\n        and callable(learning_rate)\n    )\n\n    preconditioning_compute_steps_t = preconditioning_compute_steps\n    if decay_preconditioning_compute_steps and callable(learning_rate):\n      preconditioning_compute_steps_t = preconditioning_compute_steps_schedule(\n          learning_rate,\n          preconditioning_compute_steps,\n          end_preconditioning_compute_steps,\n          step,\n      )\n\n    def _update_preconditioners():\n      # Passing statistics instead of preconditioners as they are similarly\n      # shaped tensors. Note statistics will be ignored as we are passing in\n      # a large init value for error.\n      pd = precond_dim(max_size)\n      preconditioners_init = [s[:, :pd] for s in padded_statistics]\n      n = len(padded_statistics)\n      metrics_init = jax.tree_map(\n          lambda x: [x] * n,\n          TrainingMetrics(inverse_pth_root_errors=inverse_failure_threshold))\n      init_state = [preconditioners_init, metrics_init]\n      perform_step = step % preconditioning_compute_steps_t == 0\n      return efficient_cond(\n          perform_step, _internal_inverse_pth_root_all, init_state\n      )\n\n    (preconditioners_flat, metrics_flat, _, _) = _update_preconditioners_fn(\n        _internal_inverse_pth_root_all,\n        _update_preconditioners,\n        preconditioning_compute_steps_t,\n        scheduled_preconditioning_compute_steps,\n        quantized=False,\n    )\n\n    def _skip(error):\n      condition = jnp.logical_or(\n          jnp.isnan(error), error >= inverse_failure_threshold)\n      return condition.astype(error.dtype)\n\n    def _select_preconditioner(error, new_p, old_p):\n      return lax.cond(\n          _skip(error), lambda _: old_p, lambda _: new_p, operand=None)\n\n    new_preconditioners_flat = []\n    new_errors_flat = metrics_flat.inverse_pth_root_errors\n    for p, shape, prev_p, error in zip(preconditioners_flat, original_shapes,\n                                       prev_preconditioners, new_errors_flat):\n      new_preconditioners_flat.append(\n          _select_preconditioner(error.inverse_pth_root_errors,\n                                 p[:shape[0], :shape[1]], prev_p))\n\n    assert len(states) == len(num_statistics_per_state)\n    assert len(new_preconditioners_flat) == num_statistics\n\n    # Add back empty preconditioners so we that we can set the optimizer state.\n    preconditioners_for_states = []\n    metrics_for_states = []\n    idx = 0\n    for num_statistics, state in zip(num_statistics_per_state, states):\n      if num_statistics == 0:\n        preconditioners_for_states.append([])\n        metrics_for_states.append(\n            init_training_metrics(0, generate_training_metrics))\n      else:\n        preconditioners_for_state = new_preconditioners_flat[idx:idx +\n                                                             num_statistics]\n        assert len(state.statistics) == len(preconditioners_for_state)\n        preconditioners_for_states.append(preconditioners_for_state)\n\n        if generate_training_metrics:\n          # pylint:disable=cell-var-from-loop Used immediately.\n          metrics_for_state = jax.tree_map(\n              lambda x: jnp.stack(x[idx:idx + num_statistics]),\n              metrics_flat,\n              is_leaf=functools.partial(isinstance, list))\n          assert jax.tree_util.tree_all(\n              jax.tree_map(lambda x: len(state.statistics) == len(x),\n                           metrics_for_state))\n          # pylint:enable=cell-var-from-loop\n        else:\n          metrics_for_state = optax.MaskedNode()\n        metrics_for_states.append(metrics_for_state)\n        idx += num_statistics\n\n    new_states = []\n    for state, new_preconditioners, new_metrics in zip(\n        states, preconditioners_for_states, metrics_for_states):\n      new_states.append(\n          ParameterStats(\n              state.diagonal_statistics,\n              state.statistics,\n              new_preconditioners,\n              state.diagonal_momentum,\n              state.momentum,\n              state.avg_grad,\n              new_metrics))\n\n    return new_states\n\n  def _compute_preconditioners(states, params, step):\n    \"\"\"Computes preconditioners for given statistics in states.\n\n    Args:\n      states: A list of optimizer states.\n      params: A list of params.\n      step: Current step number\n\n    Returns:\n      New optimizer states after computing the preconditioner.\n    \"\"\"\n    statistics = []\n    num_statistics_per_state = []\n    original_shapes = []\n    exponents = []\n    max_size = 0\n    prev_preconditioners = []\n\n    for state, param in zip(states, params):\n      num_statistics = len(state.statistics)\n      num_statistics_per_state.append(num_statistics)\n      original_shapes_for_state = []\n      if num_statistics > 0:\n        preconditioner = preconditioner_from_params(param)\n        for statistic in state.statistics:\n          exponents.append(preconditioner.exponent_for_preconditioner(\n          ) if exponent_override == 0 else exponent_override)\n          original_shapes_for_state.append(statistic.shape)\n          max_size = max(max_size, statistic.shape[0])\n\n        statistics.extend(state.statistics)\n        prev_preconditioners.extend(state.preconditioners)\n        original_shapes.extend(original_shapes_for_state)\n\n    if not shard_optimizer_states:\n      # Quantization is only enabled if batch_axis_name is not set.\n      quantized_dtype = quantized_dtype_for_second_moment_statistics_buffers()\n\n      if quantized_dtype == jnp.float32:\n        return _pmap_compute_preconditioners(states, step, statistics,\n                                             num_statistics_per_state,\n                                             original_shapes, exponents,\n                                             max_size, prev_preconditioners)\n      else:\n        return _pmap_quantized_compute_preconditioners(\n            states, step, statistics, num_statistics_per_state, original_shapes,\n            exponents, max_size, prev_preconditioners)\n\n    else:\n      return _pjit_compute_preconditioners(states, step, statistics,\n                                           num_statistics_per_state,\n                                           original_shapes, exponents, max_size,\n                                           prev_preconditioners)\n\n  def _transform_grad(grad, state, param, step):\n    \"\"\"Transform per-parameter gradients.\"\"\"\n    preconditioner = preconditioner_from_params(param)\n    sgd_update = grad\n    new_diagonal_statistics = state.diagonal_statistics.to_float()\n\n    if (graft_type == GraftingType.ADAGRAD or\n        graft_type == GraftingType.ADAGRAD_NORMALIZED):\n\n      scaled_grad = grad\n      if graft_type == GraftingType.ADAGRAD_NORMALIZED:\n        scaled_grad = grad / (jnp.linalg.norm(grad) + _EPSILON)\n\n      new_diagonal_statistics = (\n          state.diagonal_statistics.to_float() + jnp.square(scaled_grad))\n      adagrad_update = scaled_grad / (\n          jnp.sqrt(new_diagonal_statistics) + diagonal_epsilon)\n      grafting_update = adagrad_update\n    elif (graft_type == GraftingType.RMSPROP or\n          graft_type == GraftingType.RMSPROP_NORMALIZED):\n\n      scaled_grad = grad\n      if graft_type == GraftingType.RMSPROP_NORMALIZED:\n        scaled_grad = grad / (jnp.linalg.norm(grad) + _EPSILON)\n\n      w1 = beta2\n      w2 = jnp.where(beta2 == 1.0, beta2, 1.0 - beta2)\n\n      new_diagonal_statistics = (\n          w1 * state.diagonal_statistics.to_float() +\n          w2 * jnp.square(scaled_grad))\n      rmsprop_update = scaled_grad / (\n          jnp.sqrt(new_diagonal_statistics) + diagonal_epsilon)\n\n      if clip_by_scaled_gradient_norm:\n        scaled_grad_norm = jnp.linalg.norm(rmsprop_update) / (\n            jnp.sqrt(float(rmsprop_update.size)))\n        clipping_denom = jnp.maximum(\n            1., scaled_grad_norm / clip_by_scaled_gradient_norm)\n        rmsprop_update /= clipping_denom\n\n      grafting_update = rmsprop_update\n    elif graft_type == GraftingType.SGD:\n      grafting_update = sgd_update\n    elif graft_type == GraftingType.NONE:\n      grafting_update = sgd_update  # Use SGD during warmup.\n    else:\n      grafting_update = jnp.ones_like(sgd_update) * jnp.sign(sgd_update)\n\n    lr = learning_rate\n    if callable(learning_rate):\n      lr = learning_rate(step)\n\n    preconditioner_multiplier = lr if not decoupled_learning_rate else 1.0\n    grafting_update = grafting_update * preconditioner_multiplier\n\n    precond_grad = grad\n    if not _skip_preconditioning(param):\n      precond_grad = preconditioner.preconditioned_grad(\n          precond_grad,\n          _maybe_dequantize_preconditioners(state.preconditioners))\n    else:\n      if graft_type == GraftingType.NONE:\n        logging.error(\"skipping preconditioning without grafting for param %s\",\n                      param)\n      precond_grad = grafting_update\n\n    grafting_update_norm = jnp.linalg.norm(grafting_update)\n    precond_grad_norm = jnp.linalg.norm(precond_grad)\n\n    if graft_type is not GraftingType.NONE:\n      multiplier = (grafting_update_norm / (precond_grad_norm + _EPSILON))\n    else:\n      multiplier = 1.0\n    shampoo_update = precond_grad * multiplier\n\n    shampoo_update_with_wd = shampoo_update\n    grafting_update_with_wd = grafting_update\n\n    if weight_decay != 0 and not decoupled_weight_decay:\n      shampoo_update_with_wd = shampoo_update + weight_decay * param\n      grafting_update_with_wd = grafting_update + weight_decay * param\n\n    w = (1.0 - beta1) if moving_average_for_momentum else 1.0\n\n    shampoo_update_with_wd_momentum = (\n        state.momentum.to_float() * beta1 + w * shampoo_update_with_wd)\n\n    grafting_update_with_wd_momentum = (\n        state.diagonal_momentum.to_float() * beta1 +\n        w * grafting_update_with_wd)\n\n    run_shampoo = (step >= start_preconditioning_step).astype(\n        grafting_update_with_wd_momentum.dtype)\n\n    momentum_update = (\n        run_shampoo * shampoo_update_with_wd_momentum +\n        (1.0 - run_shampoo) * grafting_update_with_wd_momentum)\n\n    wd_update = (\n        run_shampoo * shampoo_update_with_wd +\n        (1.0 - run_shampoo) * grafting_update_with_wd)\n\n    nesterov_momentum_update = momentum_update\n\n    if nesterov:\n      nesterov_momentum_update = w * wd_update + beta1 * momentum_update\n\n    if weight_decay != 0 and decoupled_weight_decay:\n      wd_lr = 1.0 if decoupled_learning_rate else lr\n      nesterov_momentum_update = (\n          nesterov_momentum_update + wd_lr * weight_decay * param\n      )\n\n    momentum_multiplier = lr if decoupled_learning_rate else 1.0\n    transformed_update = -1.0 * momentum_multiplier * nesterov_momentum_update\n\n    new_diagonal_momentum = grafting_update_with_wd_momentum\n    new_momentum = shampoo_update_with_wd_momentum\n\n    param_stats = ParameterStats(\n        _quantize_diagonal_statistics(new_diagonal_statistics),\n        state.statistics,\n        state.preconditioners,\n        _quantize_momentum(new_diagonal_momentum),\n        _quantize_momentum(new_momentum),\n        state.avg_grad,\n        state.training_metrics)\n\n    return transformed_update, param_stats\n\n  def update_fn(grads, state, params):\n    \"\"\"Transform the input gradient and update all statistics.\n\n    Args:\n      grads: the gradient tensors for the parameters and any custom gradients\n        for preconditioners.\n      state: a named tuple containing the state of the optimizer\n      params: the parameters that should be updated.\n\n    Returns:\n      A tuple containing the new parameters and the new optimizer state.\n    \"\"\"\n    params_flat, treedef = jax.tree_flatten(params)\n    stats_flat = treedef.flatten_up_to(state.stats)\n    grads_flat = treedef.flatten_up_to(grads)\n    stats_grads = grads_flat\n\n    new_stats_flat = jax.tree_map(\n        lambda g, s, p: _compute_stats(g, s, p, state.count), stats_grads,\n        stats_flat, params_flat)\n\n    new_stats_flat = _compute_preconditioners(new_stats_flat, params_flat,\n                                              state.count)\n    outputs = jax.tree_map(\n        lambda g, s, p: _transform_grad(g, s, p, state.count), grads_flat,\n        new_stats_flat, params_flat)\n    updates_flat, new_stats_flat = list(zip(*outputs)) if outputs else ((), ())\n\n    updates = jax.tree_unflatten(treedef, updates_flat)\n    new_stats = jax.tree_unflatten(treedef, new_stats_flat)\n\n    new_state = ShampooState(count=state.count + 1, stats=new_stats)\n    return updates, new_state\n\n  if shard_optimizer_states:\n    # Hijacks the init_fn signature so we can return an OptState with\n    # appropriate init_fns.\n    opt_init_fn = sharded_init_fn\n\n    def _init_fns(unused_params):\n      return InitFnState(\n          init_fn=opt_init_fn,\n          pspec_fn=sharded_init_partition_spec_fn,\n          shape_and_dtype_fn=sharded_init_shape_and_dtype_fn)\n\n    opt_update_fn = sharded_update_fn\n    return optax.GradientTransformation(_init_fns, opt_update_fn)\n  else:\n    return optax.GradientTransformation(init_fn, update_fn)", ""]}
{"filename": "precondition/oco/algorithms.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A collection of functionally-expressed OCO algorithms.\"\"\"\n\nimport dataclasses\nimport enum", "import dataclasses\nimport enum\nfrom typing import Callable, Union\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n\nclass Algorithm(enum.Enum):\n  OGD = enum.auto()\n  ADA = enum.auto()\n  RFD_SON = enum.auto()\n  FD_SON = enum.auto()\n  ADA_FD = enum.auto()\n  S_ADA = enum.auto()", "class Algorithm(enum.Enum):\n  OGD = enum.auto()\n  ADA = enum.auto()\n  RFD_SON = enum.auto()\n  FD_SON = enum.auto()\n  ADA_FD = enum.auto()\n  S_ADA = enum.auto()\n\n\nRuntimeScalar = Union[float, jax.Array]", "\nRuntimeScalar = Union[float, jax.Array]\n\n\n@dataclasses.dataclass\nclass HParams:\n  \"\"\"A union of all hyperparmeters across algorithms in this file.\"\"\"\n\n  # Initial diagonal regularization delta * I.\n  delta: RuntimeScalar\n\n  # Learning rate.\n  lr: RuntimeScalar\n\n  # Sketch size. (0 for non-sketched).\n  sketch_size: int\n\n  # Sketch 'style' disambiguates which of the sketched second-order methods\n  # we should apply.\n  algorithm: Algorithm", "\n\nState = dict[str, jax.Array]\n\n\nNpState = dict[str, np.ndarray]\n\n\ndef as_np(state: State) -> NpState:\n  return {k: np.asarray(v, dtype=v.dtype) for k, v in state.items()}", "def as_np(state: State) -> NpState:\n  return {k: np.asarray(v, dtype=v.dtype) for k, v in state.items()}\n\n\nInitFn = Callable[[], State]\n\n\nUpdateFn = Callable[[State, jax.Array, jax.Array], State]\n\n\ndef generate_init_update(\n    w_shape: tuple[int], hparams: HParams\n) -> tuple[InitFn, UpdateFn]:\n  \"\"\"Bind parameters to appropriate init/update functions for OCO.\"\"\"\n  if hparams.algorithm == Algorithm.OGD:\n    assert hparams.sketch_size == 0, hparams.sketch_size\n    init, update = _ogd_init_fn, _ogd_update_fn\n  elif hparams.algorithm == Algorithm.ADA:\n    assert hparams.sketch_size == 0, hparams.sketch_size\n    init, update = _diag_adagrad_init_fn, _diag_adagrad_update_fn\n  else:\n    assert hparams.sketch_size > 1, hparams.sketch_size\n    init, update = _fd_init_fn, _fd_update_fn\n\n  bound_init_fn = lambda: init(w_shape, hparams)\n  bound_update_fn = lambda state, loss, grad: update(state, loss, grad, hparams)\n  return bound_init_fn, bound_update_fn", "\n\ndef generate_init_update(\n    w_shape: tuple[int], hparams: HParams\n) -> tuple[InitFn, UpdateFn]:\n  \"\"\"Bind parameters to appropriate init/update functions for OCO.\"\"\"\n  if hparams.algorithm == Algorithm.OGD:\n    assert hparams.sketch_size == 0, hparams.sketch_size\n    init, update = _ogd_init_fn, _ogd_update_fn\n  elif hparams.algorithm == Algorithm.ADA:\n    assert hparams.sketch_size == 0, hparams.sketch_size\n    init, update = _diag_adagrad_init_fn, _diag_adagrad_update_fn\n  else:\n    assert hparams.sketch_size > 1, hparams.sketch_size\n    init, update = _fd_init_fn, _fd_update_fn\n\n  bound_init_fn = lambda: init(w_shape, hparams)\n  bound_update_fn = lambda state, loss, grad: update(state, loss, grad, hparams)\n  return bound_init_fn, bound_update_fn", "\n\ndef _ogd_init_fn(w_shape: tuple[int], hparams: HParams) -> State:\n  \"\"\"Initialize OGD state.\"\"\"\n  del hparams\n  return {\n      'w': jnp.zeros(w_shape, dtype=jnp.float64),\n      't': jnp.array(0.0, jnp.float64),\n  }\n", "\n\ndef _ogd_update_fn(\n    state: State, loss: jax.Array, grad: jax.Array, hparams: HParams\n) -> State:\n  \"\"\"Update OGD state.\"\"\"\n  del loss\n  assert state['w'].shape == grad.shape, (state['w'].shape, grad.shape)\n  state['t'] += 1.0\n  state['w'] -= hparams.lr * grad * jax.lax.rsqrt(state['t'] + hparams.delta)\n  return state", "\n\ndef _diag_adagrad_init_fn(w_shape: tuple[int], hparams: HParams) -> State:\n  \"\"\"Initialize adagrad state.\"\"\"\n  return {\n      'w': jnp.zeros(w_shape, dtype=jnp.float64),\n      'diag_h': jnp.ones(w_shape, dtype=jnp.float64) * hparams.delta,\n  }\n\n\ndef _diag_adagrad_update_fn(\n    state: State, loss: jax.Array, grad: jax.Array, hparams: HParams\n) -> State:\n  \"\"\"Update adagrad state.\"\"\"\n  del loss\n  state['diag_h'] = state['diag_h'] + grad**2\n  rsqrt = jax.lax.rsqrt(jnp.where(state['diag_h'] == 0, 1, state['diag_h']))\n  state['w'] -= rsqrt * grad * hparams.lr\n  return state", "\n\ndef _diag_adagrad_update_fn(\n    state: State, loss: jax.Array, grad: jax.Array, hparams: HParams\n) -> State:\n  \"\"\"Update adagrad state.\"\"\"\n  del loss\n  state['diag_h'] = state['diag_h'] + grad**2\n  rsqrt = jax.lax.rsqrt(jnp.where(state['diag_h'] == 0, 1, state['diag_h']))\n  state['w'] -= rsqrt * grad * hparams.lr\n  return state", "\n\ndef _fd_init_fn(w_shape: tuple[int], hparams: HParams) -> State:\n  \"\"\"Initialize sketch algorithm state.\"\"\"\n  state = {\n      'w': jnp.zeros(w_shape, dtype=jnp.float64),\n      't': jnp.array(0.0, dtype=jnp.float64),\n  }\n  grad_size = state['w'].size\n  sketch_size = hparams.sketch_size\n  assert grad_size >= sketch_size\n  assert sketch_size >= 2\n  state['alpha'] = jnp.array(hparams.delta, dtype=jnp.float64)\n  # Preconditioner eigenvectors.\n  state['P'] = jnp.zeros((sketch_size, grad_size), dtype=jnp.float64)\n  # Covariance root eigenvalues (i.e., from the sketch, NOT covariance eigs)\n  state['e'] = jnp.zeros((sketch_size,), dtype=jnp.float64)\n  return state", "\n\n# RFD0 for Online Newton Step, Algorithm 3 in Section 5.\n# https://arxiv.org/pdf/1705.05067.pdf\n# \\mu_t, the exp-convexity constant, is zero.\n# Unique parts: screen/ZxejS5fYRAB7tG8\n#   - updates sketch with sqrt(eta_t) ~ 1/sqrt(t), up to a constant, the LR.\n#   - increments alpha by rho/2\n#   - no LR in the update\n#   - RFD0 has no hparams besides LR", "#   - no LR in the update\n#   - RFD0 has no hparams besides LR\n#\n# The \"alpha0\" hparam for RFD (not RFD0) is exactly hparams['delta']. Note that\n# RFD and RFD0 can also be given a learning rate which scales eta_t; we choose\n# a parameterization consistent with other algorithms' uses of hparams['lr'].\ndef _rfd(state: State, hparams: HParams) -> ...:\n  \"\"\"Create RFD-specific algorithmic changes.\"\"\"\n  sketch_update_factor = jax.lax.rsqrt(state['t'] * hparams.lr)\n  alpha_update_factor = 0.5\n  lr = 1.0\n  eig_inversion = jnp.reciprocal\n  return sketch_update_factor, alpha_update_factor, lr, eig_inversion", "\n\n# FD-SON - Algorithm 1 and 3 in Sections 2 and 3.\n# https://arxiv.org/pdf/1602.02202.pdf\n# Per screen/BqHhFBN3qt4zrLa\n#  - update the scetch with sqrt(eta_t), like in RFD0\n#  - eta_t has itself 1/sqrt(t) decay, unlike RFD0\n#  - we use hparams['delta'] instead of alpha\n#  - we again consolidate all constants on eta_t into hparams['lr']\n#", "#  - we again consolidate all constants on eta_t into hparams['lr']\n#\n# Per screen/AB4SSFXvAh6PYe7, alpha does not update.\ndef _fdson(state: State, hparams: HParams) -> ...:\n  \"\"\"Create FD-SON-specific algorithmic changes.\"\"\"\n  sketch_update_factor = jax.lax.rsqrt(jnp.sqrt(state['t']) * hparams.lr)\n  alpha_update_factor = 0.0\n  lr = 1.0\n  eig_inversion = jnp.reciprocal\n  return sketch_update_factor, alpha_update_factor, lr, eig_inversion", "\n\n# Ada-FD - https://www.ijcai.org/proceedings/2018/0381.pdf\n# Per screen/BMwyfTHcwnbpexn\n#  - Similar to S-Adagrad, except no updating alpha (dynamic diagonal).\n#  - However, they add delta to the *square rooted* eigenvalues, unlike\n#    all the other works.\ndef _adafd(state: State, hparams: HParams) -> ...:\n  \"\"\"Create Ada-FD-specific algorithmic changes.\"\"\"\n  del state\n  sketch_update_factor = 1.0\n  alpha_update_factor = 0.0\n  lr = hparams.lr\n  eig_inversion = 'Ada-FD requires special handling'\n  return sketch_update_factor, alpha_update_factor, lr, eig_inversion", "\n\n# S-Adagrad - https://arxiv.org/pdf/2302.03764.pdf\ndef _sada(state: State, hparams: HParams) -> ...:\n  \"\"\"Create S-Adagrad-specific algorithmic changes.\"\"\"\n  del state\n  sketch_update_factor = 1.0\n  alpha_update_factor = 1.0\n  lr = hparams.lr\n  eig_inversion = jax.lax.rsqrt\n  return sketch_update_factor, alpha_update_factor, lr, eig_inversion", "\n\ndef _fd_method_factors(state: State, hparams: HParams) -> ...:\n  \"\"\"Create algorithmic variants specific to hparams.\"\"\"\n  return {\n      Algorithm.RFD_SON: _rfd(state, hparams),\n      Algorithm.FD_SON: _fdson(state, hparams),\n      Algorithm.ADA_FD: _adafd(state, hparams),\n      Algorithm.S_ADA: _sada(state, hparams),\n  }[hparams.algorithm]", "\n\ndef _fd_update_fn(\n    state: State, loss: jax.Array, grad: jax.Array, hparams: HParams\n) -> State:\n  \"\"\"Update FD algorithm state given loss and grad.\"\"\"\n  del loss\n  state['t'] += 1.0\n  sketch_update_factor, alpha_update_factor, lr, eig_inversion = (\n      _fd_method_factors(state, hparams)\n  )\n  grad_input = grad.ravel() * sketch_update_factor\n\n  B = state['P'] * state['e'].reshape(-1, 1)  # pylint: disable=invalid-name\n  B = B.at[-1].set(grad_input)  # pylint: disable=invalid-name\n  _, s, vt = jnp.linalg.svd(B, full_matrices=False)\n  rho = s[-1]\n  s = (s - rho) * (s + rho)\n  P = state['P'] = vt  # pylint: disable=invalid-name\n  state['e'] = jnp.sqrt(s)\n  state['alpha'] += alpha_update_factor * rho**2\n\n  mm = lambda x, y: jnp.dot(x, y, precision=jax.lax.Precision.HIGHEST)\n  g = grad.ravel()\n  alpha = state['alpha']\n\n  eps = 0.0\n\n  def safe_invert(x, inversion=eig_inversion):\n    return jnp.where(x <= eps, 0.0, inversion(x))\n\n  if eig_inversion == 'Ada-FD requires special handling':\n    e = state['e']\n    d = e / (alpha + e)\n    update = g - mm(P.T, d * mm(P, g))\n    update *= safe_invert(alpha, inversion=jnp.reciprocal)\n  else:\n    e = alpha + s\n    inv_s = safe_invert(e)\n    inv_alpha = safe_invert(alpha)\n    outside_sketch_g = g - mm(P.T, mm(P, g))\n    sketched_precond = mm(P.T, inv_s * mm(P, g))\n    update = sketched_precond + inv_alpha * outside_sketch_g\n\n  state['w'] -= lr * update.reshape(state['w'].shape)\n  return state", ""]}
{"filename": "precondition/oco/train.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities for running an OCO algorithm on a dataset.\"\"\"\n\nimport functools\nfrom typing import Callable, Optional", "import functools\nfrom typing import Callable, Optional\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom precondition.oco import algorithms\nfrom precondition.oco import datasets\n\nLossAndGrad = Callable[", "\nLossAndGrad = Callable[\n    [jax.Array, jax.Array, jax.Array], tuple[jax.Array, jax.Array]\n]\n\n\n@functools.partial(\n    jax.jit,\n    static_argnames=[\n        'loss_and_grad',", "    static_argnames=[\n        'loss_and_grad',\n        'update_fn',\n        # 'algorithm',\n        # 'sketch_size',\n        'extra_loss',\n    ],\n)\ndef _compiled_run_dataset(\n    x: jax.Array,\n    y: jax.Array,\n    state: algorithms.State,\n    obs_ixs: jax.Array,\n    # delta: algorithms.RuntimeScalar,\n    # lr: algorithms.RuntimeScalar,\n    loss_and_grad: LossAndGrad,\n    update_fn: algorithms.UpdateFn,\n    # algorithm: algorithms.Algorithm,\n    # sketch_size: int,\n    extra_loss: Optional[datasets.Loss],\n) -> algorithms.State:\n  \"\"\"Run an OCO algorithm, saving history at obs_ixs.\"\"\"\n\n  # hparams = algorithms.HParams(\n  #    delta,\n  #    lr,\n  #    sketch_size,\n  #    algorithm,\n  # )\n\n  # assume obs_ixs starts at 0 and ends at nrows-1\n  # assume state has various keys pre-initialized, see below.\n\n  def process_row(idx, state):  # fori_loop index, so pylint: disable=unused-argument\n    ix = state['n']\n    r = x[ix]\n    f, g = loss_and_grad(state['w'], r, y[ix])\n    if extra_loss is not None:\n      state['extra_loss'] += extra_loss(state['w'], r, y[ix])\n    state = update_fn(state, f, g)\n    state['loss'] += f\n    state['n'] += 1\n\n    return state\n\n  chunks = jnp.diff(obs_ixs, prepend=0)\n\n  def scan_reduce(state, chunksize):\n    state = jax.lax.fori_loop(0, chunksize, process_row, state)\n    return state, state\n\n  _, history = jax.lax.scan(scan_reduce, state, chunks)\n\n  return history", "def _compiled_run_dataset(\n    x: jax.Array,\n    y: jax.Array,\n    state: algorithms.State,\n    obs_ixs: jax.Array,\n    # delta: algorithms.RuntimeScalar,\n    # lr: algorithms.RuntimeScalar,\n    loss_and_grad: LossAndGrad,\n    update_fn: algorithms.UpdateFn,\n    # algorithm: algorithms.Algorithm,\n    # sketch_size: int,\n    extra_loss: Optional[datasets.Loss],\n) -> algorithms.State:\n  \"\"\"Run an OCO algorithm, saving history at obs_ixs.\"\"\"\n\n  # hparams = algorithms.HParams(\n  #    delta,\n  #    lr,\n  #    sketch_size,\n  #    algorithm,\n  # )\n\n  # assume obs_ixs starts at 0 and ends at nrows-1\n  # assume state has various keys pre-initialized, see below.\n\n  def process_row(idx, state):  # fori_loop index, so pylint: disable=unused-argument\n    ix = state['n']\n    r = x[ix]\n    f, g = loss_and_grad(state['w'], r, y[ix])\n    if extra_loss is not None:\n      state['extra_loss'] += extra_loss(state['w'], r, y[ix])\n    state = update_fn(state, f, g)\n    state['loss'] += f\n    state['n'] += 1\n\n    return state\n\n  chunks = jnp.diff(obs_ixs, prepend=0)\n\n  def scan_reduce(state, chunksize):\n    state = jax.lax.fori_loop(0, chunksize, process_row, state)\n    return state, state\n\n  _, history = jax.lax.scan(scan_reduce, state, chunks)\n\n  return history", "\n\ndef run_dataset(\n    dataset_name: str,\n    num_obs: int,\n    hparams: algorithms.HParams,\n    extra_loss: Optional[datasets.Loss] = None,\n    dataset_cache: str = '/tmp/cache',\n) -> algorithms.State:\n  \"\"\"Run an OCO algorithm on a dataset, saving history at obs_ixs.\"\"\"\n  assert num_obs >= 2\n\n  dataset = datasets.load_dataset(dataset_name, dataset_cache)\n  init_fn, update_fn = algorithms.generate_init_update(dataset.w_shape, hparams)\n\n  obs_ixs = np.round(\n      np.linspace(0, dataset.x.shape[0], num=num_obs, endpoint=True)\n  ).astype(int)\n\n  initial_state = init_fn()\n  loss_and_grad = jax.value_and_grad(dataset.loss)\n\n  assert 'loss' not in initial_state, list(initial_state)\n  assert 'w' in initial_state, list(initial_state)\n  assert 'n' not in initial_state, list(initial_state)\n  initial_state['loss'] = jnp.array(0.0, dtype=jnp.float64)\n  initial_state['n'] = 0\n  if extra_loss is not None:\n    initial_state['extra_loss'] = jnp.array(0.0, dtype=jnp.float64)\n\n  # Unpack the compilable and static HParams separately.\n  return _compiled_run_dataset(\n      dataset.x,\n      dataset.y,\n      initial_state,\n      obs_ixs,\n      # hparams.delta,\n      # hparams.lr,\n      loss_and_grad,\n      update_fn,\n      # hparams.algorithm,\n      # hparams.sketch_size,\n      extra_loss,\n  )", ""]}
{"filename": "precondition/oco/sweep.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Python binary to run and evaluate OCO sweeps.\n\nThis can be used to either run the best hparams from a previous run on a test\nset of data, or to sweep hparams on a training set.", "This can be used to either run the best hparams from a previous run on a test\nset of data, or to sweep hparams on a training set.\n\"\"\"\n\nimport concurrent.futures as concurrent_futures\nimport datetime\nimport itertools\nimport os\nfrom typing import Optional, Sequence, Union\n", "from typing import Optional, Sequence, Union\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom precondition.oco import algorithms", "import pandas as pd\nfrom precondition.oco import algorithms\nfrom precondition.oco import datasets\nfrom precondition.oco import train\n\njax.config.update('jax_enable_x64', True)\n\n\n_SKETCH_SIZE = flags.DEFINE_integer(\n    'sketch_size', 0, 'sketch size for approximate full-matrix algorithms'", "_SKETCH_SIZE = flags.DEFINE_integer(\n    'sketch_size', 0, 'sketch size for approximate full-matrix algorithms'\n)\n\n\n_PARALLEL = flags.DEFINE_integer(\n    'parallel', 16, 'number of threads for launching jax programs'\n)\n\n", "\n\n_DATASET = flags.DEFINE_enum(\n    'dataset', 'a9a', datasets.SUPPORTED_DATASETS, 'dataset to run on'\n)\n\n\n_ALGS = flags.DEFINE_multi_enum_class(\n    'alg',\n    list(algorithms.Algorithm),", "    'alg',\n    list(algorithms.Algorithm),\n    algorithms.Algorithm,\n    'which algorithms to evaluate',\n)\n\n\n_DELTA = flags.DEFINE_multi_float(\n    'delta',\n    [],", "    'delta',\n    [],\n    'initial diagonal regularization hparams. Must be unset if --use_best_from'\n    ' is set (and vice versa).',\n)\n\n\n_LR = flags.DEFINE_multi_float(\n    'lr',\n    [],", "    'lr',\n    [],\n    'lr hparams. Must be unset if --use_best_from is set (and vice versa).',\n)\n\n\n_USE_BEST_FROM = flags.DEFINE_multi_string(\n    'use_best_from',\n    None,\n    'initialize best hparams for each algorithm using results from these'", "    None,\n    'initialize best hparams for each algorithm using results from these'\n    ' directories. If set, --delta and --lr must not be.',\n)\n\n\n_DIR = flags.DEFINE_string(\n    'save_dir',\n    None,\n    'save results to this directory suffix, by default, HH:MM:SS',", "    None,\n    'save results to this directory suffix, by default, HH:MM:SS',\n)\n\n\n_SKETCHING_ALGS = [\n    algorithms.Algorithm.RFD_SON,\n    algorithms.Algorithm.FD_SON,\n    algorithms.Algorithm.ADA_FD,\n    algorithms.Algorithm.S_ADA,", "    algorithms.Algorithm.ADA_FD,\n    algorithms.Algorithm.S_ADA,\n]\n\n\ndef _validate_flags() -> bool:\n  \"\"\"Raises if flags improperly set, returns whether sweeping.\"\"\"\n  if (\n      any(alg in _SKETCHING_ALGS for alg in _ALGS.value)\n      and _SKETCH_SIZE.value <= 1\n  ):\n    raise ValueError('sketch size must be at least 2')\n  hparams_set = _DELTA.value or _LR.value\n  if hparams_set and not (_DELTA.value and _LR.value):\n    raise ValueError('if one of --delta/--lr is set, the other must be')\n  if _USE_BEST_FROM.value and hparams_set:\n    raise ValueError(\n        '--delta/--lr may only be set if --use_best_from is not '\n        '(and vice versa)'\n    )\n  if not _USE_BEST_FROM.value and not hparams_set:\n    raise ValueError(\n        'at least one of --delta/--lr or --use_best_from must be set'\n    )\n  if not _DIR.value:\n    raise ValueError('require a directory to save outputs')\n  return bool(hparams_set)", "\n\ndef _make_directory():\n  \"\"\"Derive which directory to save in, ensure it doesn't exist, make it.\"\"\"\n  suffix = _DIR.value\n  now = datetime.datetime.now()\n  time = now.strftime('%H:%M:%S')\n  date = now.date()\n  directory = f'{suffix}/{date}/{time}'\n  os.makedirs(directory)\n  return directory", "\n\ndef main(argv: ...) -> None:\n  del argv\n\n  is_sweep = _validate_flags()\n  directory = _make_directory()\n  flagfile = os.path.join(directory, 'flagfile.txt')\n  with open(flagfile, 'w') as f:\n    f.write(flags.FLAGS.flags_into_string())\n\n  dataset_name = _DATASET.value\n  # Load dataset to print top-level info and for joblib to cache it in /tmp\n  dataset = datasets.load_dataset(dataset_name)\n  logging.info('loaded dataset %s with dims %s', dataset_name, dataset.x.shape)\n\n  sketch_size = _SKETCH_SIZE.value\n\n  hparams = []\n  if is_sweep:\n    for alg, lr, delta in itertools.product(\n        _ALGS.value, _LR.value, _DELTA.value\n    ):\n      hparam = algorithms.HParams(\n          delta,\n          lr,\n          sketch_size if alg in _SKETCHING_ALGS else 0,\n          alg,\n      )\n      hparams.append(hparam)\n  else:\n    dfs = []\n    for previous_directory in _USE_BEST_FROM.value:\n      dfs.append(_read_pandas(previous_directory, dataset_name, sketch_size))\n    df = pd.concat(dfs, axis=0)\n    df.sort_values('loss', inplace=True)\n    df.drop_duplicates('alg', inplace=True)\n    algs = df.alg.unique()\n    hparams = []\n    for alg in _ALGS.value:\n      if alg.name not in algs:\n        raise ValueError(f'missing {alg} in --use_best_from')\n      row = df.loc[df.alg == alg.name].T.squeeze()\n      hparam = algorithms.HParams(\n          row.delta,\n          row.lr,\n          sketch_size if alg in _SKETCHING_ALGS else 0,\n          alg,\n      )\n      hparams.append(hparam)\n\n  assert hparams\n\n  nobs = 100\n  logging.info('generated %s trials to run with %s obs', len(hparams), nobs)\n  logging.info('%s distinct algs: %s', len(_ALGS.value), _ALGS.value)\n  total = len(hparams)\n  args = [\n      {'idx': i, 'total': total, 'dataset': dataset_name, 'hparam': hparam,\n       'nobs': nobs,}\n      for i, hparam in enumerate(hparams)\n  ]\n  # python threading suffices b/c Jax itself is async\n  executor = concurrent_futures.ThreadPoolExecutor(_PARALLEL.value)\n  histories = list(executor.map(lambda kwargs: _run_oco(**kwargs), args))\n  for e in histories:\n    if isinstance(e, Exception):\n      raise e\n  logging.info('all %s hparam tunings complete', total)\n\n  result_df = _make_pandas(hparams, dataset_name, sketch_size, histories)\n  result_df.sort_values('loss', inplace=True)\n  _save_pandas(directory, result_df)\n  best_df = result_df.drop_duplicates('alg', inplace=False)\n  assert best_df is not None\n  best_txt = best_df.drop(columns='history').to_string(index=False)\n  logging.info('completed runs, results\\n%s', best_txt)\n\n  with open(os.path.join(directory, 'best.txt'), 'w') as f:\n    print(best_txt, file=f)\n\n  cs = itertools.cycle('rbcgk')\n  lss = itertools.cycle(['--', '-', ':'])\n  for loss_type in ['loss', 'extra_loss']:\n    assert best_df is not None\n    for h, alg, ls, c in zip(best_df.history, best_df.alg, lss, cs):\n      if h is None:\n        continue\n      plt.plot(h['n'][1:], h[loss_type][1:] / h['n'][1:], label=alg, ls=ls, c=c)\n\n    loss_name = '0-1 loss' if loss_type == 'extra_loss' else 'logloss'\n    plt.xlabel('examples')\n    plt.ylabel(f'cumulative {loss_name}')\n    plt.title(f'cumulative {loss_name}')\n    plt.legend()\n    ln = loss_name.replace(' ', '-')\n    with open(os.path.join(directory, f'plot-{ln}.pdf'), 'wb') as f:\n      plt.savefig(f, format='pdf', bbox_inches='tight')\n    plt.clf()\n\n  logging.info(\n      'all results saved in %s/{results.pkl,plot*.pdf,flagfile.txt,best.txt}',\n      directory,\n  )", "\n\ndef _run_oco(\n    idx: int, total: int, dataset: str, hparam: algorithms.HParams, nobs: int\n) -> Union[Optional[algorithms.NpState], Exception]:\n  \"\"\"Run online convex algorithm on a worker process.\"\"\"\n  logging.info('job %04d of %04d starting', idx, total)\n  try:\n    history = train.run_dataset(dataset, nobs, hparam, datasets.incorrect)\n    logging.info('job %04d of %04d done', idx, total)\n    return algorithms.as_np(history)\n  except FloatingPointError:\n    logging.info('job %04d of %04d inf', idx, total)\n    return None\n  except Exception as e:  # pylint: disable=broad-exception-caught\n    logging.info('job %04d of %04d errored', idx, total)\n    return e", "\n# Disable for Numpy and Pandas containers.\n# pylint: disable=g-explicit-length-test\n\n\ndef _read_pandas(\n    path: str, dataset_name: str, sketch_size: int\n) -> pd.DataFrame:\n  \"\"\"Read and validate dataframe from previous run.\"\"\"\n  path = os.path.join(path, 'results.pkl')\n  with open(path, 'rb') as f:\n    df = pd.read_pickle(f)\n  assert len(df) > 0, (path, len(df))\n  expected = [\n      'alg',\n      'lr',\n      'delta',\n      'loss',\n      'acc',\n      'dataset',\n      'sketch_size',\n      'history',\n  ]\n  assert set(df.columns) == set(expected), df.columns\n  assert df.dataset.nunique(dropna=False) == 1, (path, df.dataset.unique())\n  stored_dataset = list(df.dataset.unique())[0]\n  assert df.sketch_size.nunique(dropna=True) == 1, (\n      path,\n      df.sketch_size.unique(),\n  )\n  sketch_sizes = [x for x in df.sketch_size.unique() if not pd.isnull(x)]\n  if sketch_sizes:\n    stored_sketch_size = sketch_sizes[0]\n    assert sketch_size == stored_sketch_size, (\n        path,\n        sketch_size,\n        stored_sketch_size,\n    )\n  assert dataset_name in stored_dataset or stored_dataset in dataset_name, (\n      path,\n      stored_dataset,\n      dataset_name,\n  )\n  logging.info(\n      'extracted %s runs from %s with dataset %s matching %s at sketch size %s',\n      len(df),\n      path,\n      stored_dataset,\n      dataset_name,\n      sketch_size,\n  )\n  return df", "\n\ndef _make_pandas(\n    hparams: Sequence[algorithms.HParams],\n    dataset_name: str,\n    sketch_size: int,\n    histories: Sequence[Optional[algorithms.NpState]],\n) -> pd.DataFrame:\n  \"\"\"Generate a dataframe with all serializable run information.\"\"\"\n  records = []\n  for hparam, history in zip(hparams, histories):\n    if history is None:\n      loss = np.inf\n      acc = 0.0\n    else:\n      loss = history['loss'][-1] / history['n'][-1]\n      acc = 1.0 - history['extra_loss'][-1] / history['n'][-1]\n    sketch_size_used = (\n        sketch_size if hparam.algorithm in _SKETCHING_ALGS else np.nan\n    )\n    record = {\n        'alg': hparam.algorithm.name,\n        'lr': hparam.lr,\n        'delta': hparam.delta,\n        'loss': loss,\n        'acc': acc,\n        'dataset': dataset_name,\n        'sketch_size': sketch_size_used,\n        'history': history,\n    }\n    records.append(record)\n  return pd.DataFrame.from_records(records)", "\n\ndef _save_pandas(path: str, df: pd.DataFrame) -> None:\n  \"\"\"Read and validate dataframe from previous run.\"\"\"\n  path = os.path.join(path, 'results.pkl')\n  with open(path, 'wb') as f:\n    df.to_pickle(f)\n\n\nif __name__ == '__main__':\n  app.run(main)", "\nif __name__ == '__main__':\n  app.run(main)\n"]}
{"filename": "precondition/oco/datasets.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Module with sparse datasets on CNS.\"\"\"\n\nimport dataclasses\nimport os", "import dataclasses\nimport os\nfrom typing import Callable\n\nfrom absl import flags\nimport jax\nimport jax.numpy as jnp\nimport joblib\nimport numpy as np\nimport sklearn.datasets", "import numpy as np\nimport sklearn.datasets\n\n_DATA_DIR = flags.DEFINE_string(\n    'data_dir',\n    None,\n    'load data: your directory needs to contain the benchmark datasets'\n    ' (a9a, a9a.t, cifar10, cifar10.t, gisette_scale, gisette_scale.t,'\n    ' where .t stands for the testing set)'\n    ' in libsvm format, available at'", "    ' where .t stands for the testing set)'\n    ' in libsvm format, available at'\n    ' https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/',\n)\n\nSUPPORTED_DATASETS: list[str] = [\n    'a9a',\n    'a9a.t',\n    'cifar10',\n    'cifar10.t',", "    'cifar10',\n    'cifar10.t',\n    'gisette_scale',\n    'gisette_scale.t',\n]\n\n\ndef _logistic_loss(w: jax.Array, x: jax.Array, y: jax.Array) -> jax.Array:\n  \"\"\"Compute logistic loss.\"\"\"\n  # assumes y is binary\n  pred = jnp.dot(w, x, precision=jax.lax.Precision.HIGHEST)\n  lse = lambda x: jax.nn.logsumexp(jnp.array(x))\n  return y * lse([0, -pred]) + (1 - y) * lse([0, pred])", "\n\ndef incorrect(w: jax.Array, x: jax.Array, y: jax.Array) -> jax.Array:\n  \"\"\"Compute binary 0-1 loss.\"\"\"\n  pred = jnp.dot(w, x, precision=jax.lax.Precision.HIGHEST)\n  return (pred > 0) != (y > 0)\n\n\nLoss = Callable[[jax.Array, jax.Array, jax.Array], jax.Array]\n", "Loss = Callable[[jax.Array, jax.Array, jax.Array], jax.Array]\n\n\n@dataclasses.dataclass\nclass SimpleDataset:\n  \"\"\"Simple dense supervised learning dataset for linear learners.\"\"\"\n\n  x: np.ndarray\n  y: np.ndarray\n  loss: Loss\n  w_shape: tuple[int, ...]", "\n\ndef _load_dataset_uncached(name: str) -> SimpleDataset:\n  \"\"\"Generate a dataset with an intercept added.\"\"\"\n  assert name in SUPPORTED_DATASETS, name\n  if not _DATA_DIR.value:\n    raise ValueError('must specify directory where datasets are stored')\n  filename = os.path.join(_DATA_DIR.value, name)\n  with open(filename, 'rb') as f:\n    x, y = sklearn.datasets.load_svmlight_file(f)\n\n  x = x.todense()\n  x = np.concatenate([x, np.ones((len(x), 1))], axis=1)\n  y = y > 0\n  return SimpleDataset(x, y, _logistic_loss, (x.shape[1],))", "\n\ndef load_dataset(name: str, cache: str = '/tmp/cache') -> SimpleDataset:\n  memory = joblib.Memory(cache, verbose=0)\n  cached_fn = memory.cache(_load_dataset_uncached)\n  return cached_fn(name)\n"]}
{"filename": "precondition/tearfree/reshaper_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for momentum implementation.\"\"\"\n\nfrom typing import Sequence\n", "from typing import Sequence\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nfrom precondition.tearfree import reshaper\n\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'smallblock',\n          'invalid_options': reshaper.Options(\n              block_size=1,\n          ),\n      },\n      {\n          'testcase_name': 'smallmerge',\n          'invalid_options': reshaper.Options(\n              merge_dims=0,\n          ),\n      },\n  ]", "\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'smallblock',\n          'invalid_options': reshaper.Options(\n              block_size=1,\n          ),\n      },\n      {\n          'testcase_name': 'smallmerge',\n          'invalid_options': reshaper.Options(\n              merge_dims=0,\n          ),\n      },\n  ]", "\n\ndef _make_expected_shape_cases() -> Sequence[dict[str, ...]]:\n  cases = [\n      {'in_shape': [4], 'merge': 2, 'block': 3, 'out_shape': [6]},\n      {'in_shape': [3], 'merge': 2, 'block': 3, 'out_shape': [3]},\n      {'in_shape': [1, 3, 1], 'merge': 2, 'block': 3, 'out_shape': [3]},\n      {'in_shape': [1, 3, 1], 'merge': 3, 'block': 3, 'out_shape': [3]},\n      {'in_shape': [1, 3, 1], 'merge': 3, 'block': 4, 'out_shape': [3]},\n      {'in_shape': [1, 3, 1, 2], 'merge': 2, 'block': 3, 'out_shape': [3, 2]},\n      {'in_shape': [4, 1, 5], 'merge': 2, 'block': 3, 'out_shape': [6, 6]},\n      {'in_shape': [1], 'merge': 2, 'block': 2, 'out_shape': []},\n      {'in_shape': [1, 1, 1], 'merge': 2, 'block': 2, 'out_shape': []},\n      {'in_shape': [1, 1, 1], 'merge': 2, 'block': 2, 'out_shape': []},\n      {\n          'in_shape': [3, 1, 5, 2, 2],\n          'merge': 4,\n          'block': 10,\n          'out_shape': [3, 5, 4],\n      },\n      {'in_shape': [2, 3, 2], 'merge': 6, 'block': 10, 'out_shape': [6, 2]},\n  ]\n  for case in cases[:]:\n    if all(i <= case['block'] for i in case['in_shape']):\n      block0 = case.copy()\n      block0['block'] = 0\n      cases.append(block0)\n  return cases", "\n\nclass ReshaperTest(parameterized.TestCase):\n  \"\"\"Basic test for shampoo implementation.\"\"\"\n\n  @parameterized.named_parameters(_make_invalid_cases())\n  def test_invalid(self, invalid_options):\n    with self.assertRaises(ValueError):\n      reshaper.merge(invalid_options)\n\n  @parameterized.parameters(_make_expected_shape_cases())\n  def test_expected_shape(self, in_shape, merge, block, out_shape):\n    options = reshaper.Options(merge_dims=merge, block_size=block)\n    init_fn, update_fn = reshaper.merge(options)\n    init = jnp.zeros(in_shape)\n    out, _ = update_fn(init, init_fn(None), init)\n    self.assertSequenceEqual(out.shape, out_shape)\n\n  @parameterized.parameters(_make_expected_shape_cases())\n  def test_inversion(self, in_shape, merge, block, out_shape):\n    del out_shape\n    options = reshaper.Options(merge_dims=merge, block_size=block)\n    init_fn, update_fn = reshaper.merge(options)\n    init = jax.random.normal(jax.random.PRNGKey(0), in_shape)\n    out, _ = update_fn(init, init_fn(None), init)\n    init_fn, update_fn = reshaper.unmerge(options)\n    recover, _ = update_fn(out, init_fn(None), init)\n    np.testing.assert_array_equal(init, recover)\n\n  def test_tree(self):\n    shapes = {\n        'w': [[{'b': (3, 2)}]],\n        'z': (\n            1,\n            2,\n            1,\n        ),\n    }\n    init = jax.tree_map(\n        jnp.zeros, shapes, is_leaf=lambda x: isinstance(x, tuple)\n    )\n    options = reshaper.Options(merge_dims=2, block_size=2)\n    init_fn, update_fn = reshaper.merge(options)\n    out, _ = update_fn(init, init_fn(None), init)\n    out_shapes = jax.tree_map(lambda x: tuple(x.shape), out)\n    expected_shapes = {'w': [[{'b': (4, 2)}]], 'z': (2,)}\n\n    self.assertEqual(out_shapes, expected_shapes)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/shampoo.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Shampoo second-order statistics preconditioning.\"\"\"\n\nimport dataclasses\nimport functools", "import dataclasses\nimport functools\nimport math\nimport string\nfrom typing import Iterator, NamedTuple, Optional, Sequence\n\nimport chex\nimport jax\nfrom jax import numpy as jnp\nimport optax", "from jax import numpy as jnp\nimport optax\nfrom precondition.tearfree import praxis_shim\n\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Shampoo covariance approximation options.\n\n  See https://arxiv.org/abs/2002.09018.\n\n  Attributes:\n    block_size: Determines the block size for Shampoo's block-diagonal gradient\n      covariance approximation.\n    update_preconditioners_freq: Number of steps between preconditioner updates.\n    update_statistics_freq: Number of steps between statistics updates.\n    second_moment_decay: Decay rate for second moment exponential moving\n      average. If 1.0 then sums.\n  \"\"\"\n\n  block_size: int = 1024\n  update_preconditioners_freq: int = 1\n  update_statistics_freq: int = 1\n  second_moment_decay: float = 0.999", "  # TODO(vladf):\n  # lb, rb, b sharding: maybe later?\n  # spmd_mesh_axis_names: Sequence[str] = () maybe later?\n\n\ndef apply(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Return gradient transform for (blocked) shampoo preconditioning.\"\"\"\n\n  _validate(options)\n\n  # raise if no unit dims (must be scalar)\n\n  # not intentional constants from distributed shampoo\n  # exponent_override = 0\n  # matrix_epsilon = 0 and eigh = True\n\n  return praxis_shim.ShardedGradientTransformation(\n      functools.partial(_init, options),\n      functools.partial(_update, options),\n      functools.partial(_pspec, options),\n  )", "\n\nclass _AxesBlocks(NamedTuple):\n  \"\"\"Represents statistics or preconditioner matrices for a single tensor.\n\n  Maintains the second-order statistics for gradients, to be used in second\n  order optimization.\n\n  There are two key matrices that are maintained per axis, when not using any\n  blocking approximations.\n\n  For each axis `i` with length `d_i`, we track the `(d_i, d_i)` covariances\n  and their inverse p-th roots, where `p` is twice the rank of the gradients\n  whose covariances we're tracking, excluding unit dimensions.\n\n  A covariance for a higher-order tensor's i-th axis is recovered from the outer\n  product of contracting all but the axis i. E.g., for an order-3 tensor G, the\n  covariance for its 0th dimension is `C_{ij} = sum_{k,l} G_{ikl}G_{jkl}`.\n\n  Since these matrices can be quite large for tensors with large dimensions, we\n  introduce an approximation which stores just block diagonal components of\n  these matrices. This corresponds to the full covariance statistics of the\n  disjoint partitions of these tensors, with large dimensions blocked by a\n  provided block size.\n\n  Below, we refer to all large dimensions as those at least equal to the block\n  size.\n\n  When storing block diagonal matrices, we store them as `N` blocks of size `B`.\n\n  Attributes:\n    stats: A list of length equal to the tensor's ndim with blocks of matrices\n      of shape [N, B, B] where B is an axis-specific block size, but at most\n      block_size.\n    roots: Same shape as stats, inverse p-th roots for statistics, where p is\n      the twice the rank of the original parameter to optimize.\n  \"\"\"\n\n  stats: list[jax.Array]\n  roots: list[jax.Array]", "\n\nclass _ShampooState(NamedTuple):\n  # A scalar int32 for step count.\n  count: jax.Array\n  # A tree of the same shape as the params of _AxesBlocks leaves of f32.\n  blocks: chex.ArrayTree\n\n\n@dataclasses.dataclass(frozen=True)\nclass _BlocksMetadata:\n  \"\"\"Metadata for _AxesBlocks to track indexing information.\n\n  Attributes:\n    block_sizes: Per-dimension statistics & preconditioner block sizes, length\n      is equal to rank of tensor we have metadata for.\n    num_blocks: `N`, the total number of blocks.\n    debug_name: A way to refer to this parameter in debug messages.\n    large_block_size: The block size originally specified in the shampoo\n      options, this is the minimum size for a dimension to be considered large.\n    param_shape: The shape of the original parameter we're blocking.\n    large_axes: Axes with dimension at least `large_block_size` in the original\n      tensor.\n    blocks_per_large_axis: Number of blocks in the corresponding axis for each\n      axis in large_axes.\n    blocks_axis: The axis of the the `N` in the blocked tensor (see _blockify).\n  \"\"\"\n\n  block_sizes: list[int]\n  num_blocks: int\n  debug_name: str\n  large_block_size: int\n  param_shape: list[int]\n  large_axes: list[int]\n  blocks_per_large_axis: list[int]\n  blocks_axis: int", "\n@dataclasses.dataclass(frozen=True)\nclass _BlocksMetadata:\n  \"\"\"Metadata for _AxesBlocks to track indexing information.\n\n  Attributes:\n    block_sizes: Per-dimension statistics & preconditioner block sizes, length\n      is equal to rank of tensor we have metadata for.\n    num_blocks: `N`, the total number of blocks.\n    debug_name: A way to refer to this parameter in debug messages.\n    large_block_size: The block size originally specified in the shampoo\n      options, this is the minimum size for a dimension to be considered large.\n    param_shape: The shape of the original parameter we're blocking.\n    large_axes: Axes with dimension at least `large_block_size` in the original\n      tensor.\n    blocks_per_large_axis: Number of blocks in the corresponding axis for each\n      axis in large_axes.\n    blocks_axis: The axis of the the `N` in the blocked tensor (see _blockify).\n  \"\"\"\n\n  block_sizes: list[int]\n  num_blocks: int\n  debug_name: str\n  large_block_size: int\n  param_shape: list[int]\n  large_axes: list[int]\n  blocks_per_large_axis: list[int]\n  blocks_axis: int", "\n\ndef _blocks_metadata(\n    options: Options, param_shape: Sequence[int], debug: str\n) -> _BlocksMetadata:\n  \"\"\"Generate the blocks metadata for a parameter.\"\"\"\n  dims = [min(dim, options.block_size) for dim in param_shape]\n  large_axes = [i for i, d in enumerate(param_shape) if d >= options.block_size]\n  blocks_per_large_axis = [\n      param_shape[i] // options.block_size for i in large_axes\n  ]\n  num_blocks = math.prod(blocks_per_large_axis + [1])\n\n  return _BlocksMetadata(\n      block_sizes=dims,\n      num_blocks=num_blocks,\n      debug_name=debug,\n      large_block_size=options.block_size,\n      large_axes=large_axes,\n      param_shape=list(param_shape),\n      blocks_per_large_axis=blocks_per_large_axis,\n      blocks_axis=min(large_axes, default=0),\n  )", "\n\ndef _validate(options: Options) -> None:\n  \"\"\"Raise ValueError if options are invalid.\"\"\"\n  if options.block_size <= 1:\n    raise ValueError(f\"block_size ({options.block_size}) must be >1\")\n\n  if options.update_preconditioners_freq <= 0:\n    raise ValueError(\n        \"update_preconditioners_freq ({}) must be positive\".format(\n            options.update_preconditioners_freq\n        )\n    )\n\n  if options.update_statistics_freq <= 0:\n    raise ValueError(\n        \"update_statistics_freq ({}) must be positive\".format(\n            options.update_statistics_freq\n        )\n    )\n\n  if not (0 <= options.second_moment_decay <= 1):\n    raise ValueError(\n        f\"second_moment_decay ({options.second_moment_decay}) \"\n        \"should be in [0, 1]\"\n    )", "\n\ndef _init(options: Options, params: optax.Params) -> _ShampooState:\n  \"\"\"Inititialize stats to 0 and preconditioners to identity.\"\"\"\n\n  def make_blocks(path: ..., param: jax.Array) -> _AxesBlocks:\n    if any(dim == 1 for dim in param.shape):\n      raise ValueError(\n          \"param {} shape ({}) has unit dimensions\".format(path, param.shape)\n      )\n\n    if sum(dim >= options.block_size for dim in param.shape) > 2:\n      raise ValueError(\n          \"param {} shape ({}) has >2 large dims for block size {}\".format(\n              path, param.shape, options.block_size\n          )\n      )\n\n    if any(\n        dim % options.block_size != 0\n        for dim in param.shape\n        if dim >= options.block_size\n    ):\n      raise ValueError(\n          \"param {} shape ({}) has large dims indivisible by block size {}\"\n          .format(path, param.shape, options.block_size)\n      )\n\n    meta = _blocks_metadata(options, param.shape, str(path))\n    n = meta.num_blocks\n    dims = meta.block_sizes\n    stats = [jnp.zeros((n, d, d)) for d in dims]\n    precond = [jnp.eye(d) * jnp.ones((n, 1, 1)) for d in dims]\n    return _AxesBlocks(stats, precond)\n\n  return _ShampooState(\n      count=jnp.zeros([], jnp.int32),\n      blocks=jax.tree_util.tree_map_with_path(make_blocks, params),\n  )", "\n\ndef _pspec(\n    options: Options, params: praxis_shim.NestedHParams\n) -> praxis_shim.NestedHParams:\n  \"\"\"Generate sharding specification for shampoo state.\"\"\"\n  count_pspec = praxis_shim.WeightHParams(\n      shape=[],\n      init=None,\n      dtype=jnp.int32,\n      collections=None,\n      tensor_split_dims_mapping=[],\n  )\n\n  def make_blocks_pspec(\n      path: ...,\n      param: praxis_shim.WeightHParams,\n  ) -> praxis_shim.NestedHParams:\n    meta = _blocks_metadata(options, param.shape, str(path))\n    num_blocks = meta.num_blocks\n    dims = meta.block_sizes\n    replicated = functools.partial(\n        praxis_shim.WeightHParams,\n        init=None,\n        dtype=jnp.float32,\n        collections=None,\n        tensor_split_dims_mapping=[-1, -1, -1],\n    )\n    stats = [replicated((num_blocks, d, d)) for d in dims]\n    precond = stats\n    return dict(stats=stats, roots=precond)\n\n  return dict(\n      count=count_pspec,\n      blocks=jax.tree_util.tree_map_with_path(\n          make_blocks_pspec, params, is_leaf=lambda x: hasattr(x, \"shape\")\n      ),\n  )", "\n\ndef _update(\n    options: Options,\n    updates: optax.Updates,\n    state: _ShampooState,\n    params: Optional[optax.Params] = None,\n) -> tuple[optax.Updates, _ShampooState]:\n  \"\"\"Update internal shampoo stats and precondition gradients.\"\"\"\n  del params\n  meta = jax.tree_util.tree_map_with_path(\n      lambda path, x: _blocks_metadata(options, x.shape, str(path)), updates\n  )\n  blocks = state.blocks\n  blockified_updates = jax.tree_map(_blockify, updates, meta)\n  is_block = lambda x: isinstance(x, _AxesBlocks)\n\n  stats_updated_blocks = functools.partial(\n      jax.tree_map,\n      functools.partial(_update_block_stats, options.second_moment_decay),\n      blockified_updates,\n      blocks,\n      meta,\n      is_leaf=is_block,\n  )\n  should_update_stats = (state.count % options.update_statistics_freq) == 0\n  blocks = jax.lax.cond(\n      should_update_stats, stats_updated_blocks, lambda: blocks\n  )\n\n  precond_updated_blocks = functools.partial(\n      jax.tree_map,\n      _update_block_precond,\n      blocks,\n      meta,\n      is_leaf=is_block,\n  )\n  should_update_precond = (\n      state.count % options.update_preconditioners_freq\n  ) == 0\n  blocks = jax.lax.cond(\n      should_update_precond, precond_updated_blocks, lambda: blocks\n  )\n  new_state = _ShampooState(count=state.count + 1, blocks=blocks)\n  new_updates = jax.tree_map(\n      _precondition_blocks, blockified_updates, blocks, meta, is_leaf=is_block\n  )\n  new_updates = jax.tree_map(_deblockify, new_updates, meta)\n\n  return new_updates, new_state", "\n\ndef _blockify(x: jax.Array, meta: _BlocksMetadata) -> jax.Array:\n  \"\"\"Reshape the update such that it is blocked along large dimensions.\n\n  Inserts the `N` dimension dimension right on the first axis in\n  `meta.large_axes`, which is the `meta.blocks_axis` in the returned tensor.\n\n  Shifts all original axes in `x` that are on or after `meta.blocks_axis`\n  (including what was originally the first axis in `meta.large_axes`) forward\n  by one. All large axes will now be of length equal to the largest block size.\n\n  In the case that there's no blocking, we put a dummy blocks axis in axis 0\n  with dimension 1, so the handling of the original axes is the same as the\n  blocked cases.\n\n  For example:\n    - Suppose block size is 5 and x is shaped [3, 20, 25, 4]. Then\n      x has two large axes (1 and 2). The resulting blocked value will be\n      shaped [3, (4*5), 5, 5, 4], with the large axes being converted\n      to block size and a new 20-dimensional axis with the product of the\n      4 blocks for the original axis 1 and 5 blocks for the original axis 2.\n      All other axes are kept the same. Note meta.blocks_axis precedes the\n      large axes' new locations, so it's set to 1.\n    - Suppose block size is 5 and x is shaped [5, 2]. The result will be\n      [1, 5, 2], with the first dimension corresponding to the single block at\n      axis 0.\n    - Suppose block size is 5 and x is [3, 4]. There are no large axes, but to\n      get rid of edge cases we still add a meta.blocks_axis at axis 0 with a\n      single block [1, 3, 4].\n    - Suppose block size is 5 and x is [15, 2, 10]. We'll return\n      [(3*2), 5, 2, 5], following the same rules as before. Note the large\n      axes stay in place.\n\n  Args:\n    x: Input to block.\n    meta: Metadata about the input.\n\n  Returns:\n    A blocked version of the input and the dimension with the number of\n    blocks.\n  \"\"\"\n  assert list(x.shape) == meta.param_shape, (x.shape, meta.param_shape)\n\n  if not meta.large_axes:\n    # Just create a unit N/blocks axis.\n    return jnp.expand_dims(x, meta.blocks_axis)\n\n  if len(meta.large_axes) == 1:\n    # Block the only large axis.\n    before, after = _split_exclusively(x.shape, meta.large_axes)\n    new_shape = before + [meta.num_blocks, meta.large_block_size] + after\n    return x.reshape(new_shape)\n\n  assert len(meta.large_axes) == 2, meta.large_axes\n\n  # Extract the blocks from both large axes.\n  l_blocks, r_blocks = meta.blocks_per_large_axis\n  before, middle, after = _split_exclusively(x.shape, meta.large_axes)\n  stitch = lambda l, r: before + l + middle + r + after\n  split_blocked_shape = stitch(\n      [l_blocks, meta.large_block_size], [r_blocks, meta.large_block_size]\n  )\n  split_blocked_x = x.reshape(split_blocked_shape)\n\n  # Move over the blocks from the right axis next to the left one.\n  perm = list(range(len(split_blocked_shape)))\n  l_blocks_ix = len(before)\n  r_blocks_ix = len(before) + 2 + len(middle)\n  perm.pop(r_blocks_ix)\n  perm.insert(l_blocks_ix + 1, r_blocks_ix)\n  adjacent_blocked_x = jnp.transpose(split_blocked_x, perm)\n\n  # Transpose the previous sharding too.\n  new_shape = stitch(\n      [meta.num_blocks, meta.large_block_size], [meta.large_block_size]\n  )\n  reshaped = adjacent_blocked_x.reshape(new_shape)\n  assert l_blocks_ix == meta.blocks_axis\n\n  return reshaped", "\n\ndef _deblockify(blocked_x: jax.Array, meta: _BlocksMetadata) -> jax.Array:\n  \"\"\"Invert _blockify().\"\"\"\n  if not meta.large_axes:\n    return jnp.squeeze(blocked_x, meta.blocks_axis)\n\n  if len(meta.large_axes) == 1:\n    return blocked_x.reshape(meta.param_shape)\n\n  assert len(meta.large_axes) == 2\n\n  # Re-split the blocks axis.\n  assert blocked_x.shape[meta.blocks_axis] == meta.num_blocks\n  before, after = _split_exclusively(blocked_x.shape, [meta.blocks_axis])\n  split_blocks_shape = before + meta.blocks_per_large_axis + after\n  split_blocked_x = blocked_x.reshape(split_blocks_shape)\n\n  # Move the right large axis blocks back in front of their axis.\n  perm = list(range(len(split_blocked_x.shape)))\n  # In blocked_x:\n  # [..., blocks axis, left block, ..., right block, ...]\n  #       ^blocks_axis                  ^large_axes[1] + 1\n  # In split_blocked_x:\n  # [..., left blocks, right blocks, left block, ..., right block, ...]\n  #       ^blocks_axis                                ^large_axes[1] + 2\n  r_blocks_ix = meta.blocks_axis + 1\n  r_blocks_val = perm.pop(r_blocks_ix)\n  # After pop:\n  # [..., left blocks, left block, ..., right block, ...]\n  #       ^blocks_axis                  ^large_axes[1] + 1\n  r_blocked_axis_ix = meta.large_axes[1] + 1\n  perm.insert(r_blocked_axis_ix, r_blocks_val)\n  split_blocked_x = jnp.transpose(split_blocked_x, perm)\n\n  reshaped = jnp.reshape(split_blocked_x, meta.param_shape)\n  return reshaped", "\n\ndef _update_block_stats(\n    second_moment_decay: float,\n    update: jax.Array,\n    block: _AxesBlocks,\n    meta: _BlocksMetadata,\n) -> _AxesBlocks:\n  \"\"\"Update covariance statistics given a blocked gradient.\"\"\"\n\n  new_stats = []\n  with jax.named_scope(\"ShampooStats\"):\n    for axis, cov in enumerate(block.stats):\n      all_axes = list(range(len(meta.param_shape)))\n      all_axes.remove(axis)\n\n      dot_all = functools.partial(jnp.tensordot, axes=(all_axes, all_axes))\n      batched_tensordot = jax.vmap(\n          dot_all, in_axes=meta.blocks_axis, out_axes=0\n      )\n      new_cov = batched_tensordot(update, update)\n      new_stats.append(_ema_update(cov, new_cov, second_moment_decay))\n\n  return _AxesBlocks(stats=new_stats, roots=block.roots)", "\n\ndef _pth_inv_root(p: int, cov: jax.Array) -> jax.Array:\n  \"\"\"Calculate a batch of p-th inverse roots.\"\"\"\n  eps = 1e-6\n  w, v = jnp.linalg.eigh(cov)\n  mask = w <= eps * jnp.max(w)\n  half = jnp.where(mask, 1.0, w) ** (-0.5 / p)\n  half = jnp.where(mask, 0.0, half)\n  half_v = jnp.expand_dims(half, -2) * v\n  return jnp.einsum(\"bik,bjk->bij\", half_v, half_v)", "\n\ndef _update_block_precond(\n    block: _AxesBlocks, meta: _BlocksMetadata\n) -> _AxesBlocks:\n  \"\"\"Update preconditioners.\"\"\"\n  p = len(meta.param_shape) * 2\n\n  with jax.named_scope(\"PthInvRoot\"):\n    new_roots = list(map(functools.partial(_pth_inv_root, p), block.stats))\n\n  return _AxesBlocks(roots=new_roots, stats=block.stats)", "\n\ndef _precondition_blocks(\n    update: jax.Array, blocks: _AxesBlocks, meta: _BlocksMetadata\n) -> jax.Array:\n  \"\"\"Precondition blocked gradients.\"\"\"\n  it = _einsum_letters(meta)\n  blocks_axis_letter = next(it)\n\n  # Contract along the innermost axis of each preconditioner,\n  # making the other equal-length axis the output.\n  contraction_letters = [next(it) for _ in meta.param_shape]\n  output_letters = [next(it) for _ in meta.param_shape]\n  preconditioners = blocks.roots\n  preconditioner_inputs = [\n      blocks_axis_letter + o + c\n      for c, o in zip(contraction_letters, output_letters)\n  ]\n\n  blocked_input = contraction_letters[:]\n  blocked_input.insert(meta.blocks_axis, blocks_axis_letter)\n  blocked_input = \"\".join(blocked_input)\n  blocked_output = output_letters[:]\n  blocked_output.insert(meta.blocks_axis, blocks_axis_letter)\n  blocked_output = \"\".join(blocked_output)\n\n  # Build up the einsum equation and invoke it.\n  inputs = \",\".join([blocked_input] + preconditioner_inputs)\n  formula = inputs + \"->\" + blocked_output\n  with jax.named_scope(\"PreconditionShampoo\"):\n    print(formula, update.shape, [x.shape for x in preconditioners])\n    return jnp.einsum(formula, update, *preconditioners)", "\n\ndef _split_exclusively(\n    ls: Sequence[int], splits: Sequence[int]\n) -> list[list[int]]:\n  \"\"\"Returns possibly-empty segments between sorted split points in ls.\"\"\"\n  assert all(\n      l < r for l, r in zip(splits, splits[1:])\n  ), f\"splits {splits} must be distinct ascending\"\n  assert all(\n      0 <= i < len(ls) for i in splits\n  ), f\"splits {splits} must index into list {ls} of length {len(ls)}\"\n  splits = [-1] + list(splits) + [len(ls)]\n  return [list(ls[l + 1 : r]) for l, r in zip(splits, splits[1:])]", "\n\ndef _einsum_letters(meta: _BlocksMetadata) -> Iterator[str]:\n  for c in string.ascii_letters:\n    yield c\n\n  raise ValueError(\n      f\"shape {meta.param_shape} too high-dimensional for {meta.debug_name}\"\n  )\n", "\n\ndef _ema_update(old: jax.Array, new: jax.Array, decay: float) -> jax.Array:\n  if decay == 1.0:\n    return old + new\n  return old * decay + new * (1 - decay)\n"]}
{"filename": "precondition/tearfree/sketchy.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sketchy low-rank second-order statistics preconditioning.\"\"\"\n\nimport dataclasses\nimport functools", "import dataclasses\nimport functools\nfrom typing import NamedTuple, Optional, Union\n\nimport chex\nimport jax\nfrom jax import numpy as jnp\nimport optax\nfrom precondition.tearfree import praxis_shim\n", "from precondition.tearfree import praxis_shim\n\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Sketchy covariance approximation options.\n\n  See https://arxiv.org/abs/2302.03764.\n\n  Attributes:\n    epsilon: The diagonal positive perturbation added to preconditioner before\n      inversion.\n    rank: The sketch size to use for FD sketch for each tensor's dimension.\n    relative_epsilon: Whether to scale epsilon by the top singular value of the\n      covariance or not.\n    second_moment_decay: Exponential moving average for second-order statistics\n      tracking. If 1.0 then sums.\n    update_freq: Number of steps between covariance updates.\n    add_ggt: whether to store the exponentially moving GGT in the states. Set\n    to TRUE to save the exponentially moving GGT.\n  \"\"\"\n\n  epsilon: float = 1e-7\n  rank: int = 128\n  relative_epsilon: bool = True\n  second_moment_decay: float = 0.999\n  update_freq: int = 1\n  add_ggt: bool = False", "\n\ndef apply(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Return gradient transform for (blocked) shampoo preconditioning.\"\"\"\n\n  _validate(options)\n\n  return praxis_shim.ShardedGradientTransformation(\n      functools.partial(_init, options),\n      functools.partial(_update, options),\n      functools.partial(_pspec, options),\n  )", "\n\nclass _AxisState(NamedTuple):\n  \"\"\"Contains the covariance sketch state for each tensor dimension.\"\"\"\n\n  eigvecs: jax.Array\n  # These refer to the eigenvalues of the running *square root* of of the\n  # covariance.\n  eigvals: jax.Array\n  # Analogously, but the -(1/(2*ndim)) root of the covariance, where ndim\n  # is total tensor rank.\n  inv_eigvals: jax.Array\n  # The tail, however, tracks the cumulative escaped mass, which is the sum\n  # of eigenvalues of the full gradient covariance which were subtracted out.\n  tail: jax.Array\n  # Analogously to inv_eigvals, the -(1/(2*ndim))-th root.\n  inv_tail: jax.Array\n  # Add additional optional state to store ema GGT\n  ema_ggt: Union[optax.MaskedNode, jax.Array]", "\n\nclass _TensorState(NamedTuple):\n  \"\"\"Per-tensor state contains a list of axis states for each dimension.\"\"\"\n\n  axes: list[_AxisState]\n\n\nclass _SketchyState(NamedTuple):\n  # A scalar int32 for step count.\n  count: jax.Array\n  # A tree of the same shape as the params of _TensorState leaves of f32.\n  sketches: chex.ArrayTree", "class _SketchyState(NamedTuple):\n  # A scalar int32 for step count.\n  count: jax.Array\n  # A tree of the same shape as the params of _TensorState leaves of f32.\n  sketches: chex.ArrayTree\n\n\ndef _validate(options: Options) -> None:\n  \"\"\"Raise ValueError if options are invalid.\"\"\"\n\n  if options.update_freq <= 0:\n    raise ValueError(\n        \"update_freq ({}) must be positive\".format(options.update_freq)\n    )\n\n  if not (0 <= options.second_moment_decay <= 1):\n    raise ValueError(\n        f\"second_moment_decay ({options.second_moment_decay}) \"\n        \"should be in [0, 1]\"\n    )\n\n  if options.rank <= 0:\n    raise ValueError(f\"rank ({options.rank}) must be at least 1\")", "\n\ndef _init(options: Options, params: optax.Params) -> _SketchyState:\n  \"\"\"Inititialize sketch.\"\"\"\n\n  def _tensor_state(path: ..., param: jax.Array) -> _TensorState:\n    axes = []\n    add_ggt = options.add_ggt\n    for d in param.shape:\n      if d == 1:\n        raise ValueError(\n            \"param {} shape ({}) has unit dimensions\".format(path, param.shape)\n        )\n\n      k = min(d, options.rank)\n\n      axes.append(\n          _AxisState(\n              eigvecs=jnp.zeros((d, k)),\n              eigvals=jnp.zeros((k,)),\n              inv_eigvals=jnp.zeros((k,)),\n              tail=jnp.zeros(tuple()),\n              inv_tail=jnp.zeros(tuple()),\n              ema_ggt=jnp.zeros((d, d)) if add_ggt else optax.MaskedNode(),\n          )\n      )\n\n    return _TensorState(axes)\n\n  return _SketchyState(\n      count=jnp.zeros([], jnp.int32),\n      sketches=jax.tree_util.tree_map_with_path(_tensor_state, params),\n  )", "\n\ndef _pspec(\n    options: Options, params: praxis_shim.NestedHParams\n) -> praxis_shim.NestedHParams:\n  \"\"\"Generate sharding specification for sketchy state.\"\"\"\n\n  count_pspec = praxis_shim.WeightHParams(\n      shape=[],\n      init=None,\n      dtype=jnp.int32,\n      collections=None,\n      tensor_split_dims_mapping=[],\n  )\n\n  def _tensor_pspec(\n      param: praxis_shim.WeightHParams,\n  ) -> praxis_shim.NestedHParams:\n\n    def _replicated(shape):\n      return praxis_shim.WeightHParams(\n          shape=list(shape),\n          init=None,\n          dtype=jnp.float32,\n          collections=None,\n          tensor_split_dims_mapping=[-1] * len(shape),\n      )\n\n    def _make_axis_state(d: int):\n      k = min(d, options.rank)\n      add_ggt = options.add_ggt\n      return dict(\n          eigvecs=_replicated((d, k)),\n          eigvals=_replicated((k,)),\n          inv_eigvals=_replicated((k,)),\n          tail=_replicated(tuple()),\n          inv_tail=_replicated(tuple()),\n          ema_ggt=_replicated((d, d)) if add_ggt else optax.MaskedNode(),\n      )\n\n    return dict(axes=[_make_axis_state(d) for d in param.shape])\n\n  return dict(\n      count=count_pspec,\n      sketches=jax.tree_util.tree_map(\n          _tensor_pspec, params, is_leaf=lambda x: hasattr(x, \"shape\")\n      ),\n  )", "\n\ndef _update(\n    options: Options,\n    updates: optax.Updates,\n    state: _SketchyState,\n    params: Optional[optax.Params] = None,\n) -> tuple[optax.Updates, _SketchyState]:\n  \"\"\"Update internal shampoo stats and precondition gradients.\"\"\"\n  del params\n  sketches = state.sketches\n  is_tensor_state = lambda x: isinstance(x, _TensorState)\n\n  updated_sketches = functools.partial(\n      jax.tree_map,\n      functools.partial(_update_sketches, options),\n      updates,\n      sketches,\n      is_leaf=is_tensor_state,\n  )\n  should_update_stats = (state.count % options.update_freq) == 0\n  new_sketches = jax.lax.cond(\n      should_update_stats, updated_sketches, lambda: sketches\n  )\n  new_updates = jax.tree_map(\n      functools.partial(_precondition, options),\n      updates,\n      new_sketches,\n      is_leaf=is_tensor_state,\n  )\n  return new_updates, _SketchyState(\n      count=state.count + 1, sketches=new_sketches\n  )", "\n\ndef _update_sketches(\n    options: Options,\n    update: jax.Array,\n    sketches: _TensorState,\n) -> _TensorState:\n  \"\"\"Update sketched covariance statistics given a gradient.\"\"\"\n  new_axes = []\n  for dim, axis_state in enumerate(sketches.axes):\n    with jax.named_scope(\"UpdateSketchDim{}\".format(dim)):\n      new_axes.append(_update_axis(options, dim, update, axis_state))\n  return _TensorState(new_axes)", "\n\ndef _precondition(\n    options: Options, update: jax.Array, sketches: _TensorState\n) -> jax.Array:\n  \"\"\"Precondition gradients.\"\"\"\n  g = update\n  original_shape = g.shape\n  roll = tuple(range(1, g.ndim)) + (0,)\n  for dim, axis_state in enumerate(sketches.axes):\n    with jax.named_scope(\"SketchPreconditionDim{}\".format(dim)):\n      # Rotate g during this loop; keep the axis to precondition first.\n      d = original_shape[dim]\n      k = min(d, options.rank)\n      assert g.shape[0] == d\n      eigvecs = axis_state.eigvecs\n      assert list(eigvecs.shape) == [d, k]\n      lowrank_basis = jnp.tensordot(g, eigvecs, axes=[[0], [0]])\n      lowrank_component = jnp.tensordot(\n          lowrank_basis, eigvecs, axes=[[g.ndim - 1], [1]]\n      )\n      g = jnp.transpose(g, axes=roll)\n      complement = g - lowrank_component\n      scaled_basis = lowrank_basis * axis_state.inv_eigvals\n      scaled_lowrank_component = jnp.tensordot(\n          scaled_basis, eigvecs, axes=[[g.ndim - 1], [1]]\n      )\n      g = axis_state.inv_tail * complement + scaled_lowrank_component\n  return g", "\n\ndef _update_axis(\n    options: Options, dim: int, update: jax.Array, axis_state: _AxisState,\n) -> _AxisState:\n  \"\"\"Perform an FD update for statistics.\"\"\"\n  # _low_rank_root\n  d = update.shape[dim]\n  k = min(d, options.rank)\n\n  sketch_dk = axis_state.eigvecs\n  assert sketch_dk.shape == (d, k), (sketch_dk.shape, d, k, update.shape, dim)\n\n  sketch_dk *= axis_state.eigvals[jnp.newaxis, :]\n  all_but_dim = [i for i in range(update.ndim) if i != dim]\n  g_dm = update.transpose([dim] + all_but_dim).reshape(d, -1)\n  decay = jnp.sqrt(options.second_moment_decay)\n\n  # This implementation uses only O(|gradient size|) memory because\n  # full_matrices is False, but may be slow. Consider LOBPCG instead.\n  updated = jnp.concatenate([sketch_dk * decay, g_dm], axis=1)\n  # This dimensionality reduction with QR is a mathematical no-op but required\n  # to avoid b/286607548.\n  updated = jnp.linalg.qr(updated.T, mode=\"r\").T\n\n  def _safe_svd(x):\n    # Wrap with a nan check due to hangs per b/286654608.\n    svd = lambda y: jnp.linalg.svd(y, full_matrices=False)[:2]\n\n    def _all_nan(y):\n      m = min(y.shape)\n      u = jnp.full((d, m), jnp.nan, jnp.float32)\n      s = jnp.full((m,), jnp.nan, jnp.float32)\n      return u, s\n\n    return jax.lax.cond(jnp.isfinite(x).all(), svd, _all_nan, x)\n\n  u, s = _safe_svd(updated)\n  assert u.shape[0] == d\n  assert u.shape[1] >= k\n\n  cutoff = jnp.maximum(s[k], 0.0) if k < len(s) else 0.0\n  top_eigs = jnp.maximum(s[:k], 0.0)\n  deflated = jnp.sqrt(jnp.maximum(0.0, top_eigs - cutoff)) * jnp.sqrt(\n      top_eigs + cutoff\n  )\n  tail = axis_state.tail * decay + cutoff**2\n  # Avoid numerical error from the sqrt computation and from subtracting\n  # and re-adding cutoff^2 (mathematically, undeflated == deflated^2 + tail).\n  undeflated = jnp.square(jnp.maximum(top_eigs, 0.0)) + axis_state.tail * decay\n  eigvecs = u[:, :k]\n\n  mask = deflated > 0\n  # Would be nice to statically assert deflated == 0 implies undeflated == 0.\n\n  alpha = jnp.asarray(-1.0 / (2 * update.ndim), dtype=jnp.float32)\n  eigvecs *= mask\n  if options.relative_epsilon and options.epsilon > 0:\n    eps = jnp.max(undeflated) * options.epsilon\n  else:\n    eps = options.epsilon\n  inv_eigvals = jnp.where(mask, (undeflated + eps) ** alpha, 0)\n  eigvals = deflated * mask\n  inv_tail = jnp.where(tail > 0, (tail + eps) ** alpha, 0.0)\n  if options.add_ggt:\n    ema_ggt = axis_state.ema_ggt * decay + g_dm.dot(g_dm.T) * (1 - decay)\n  else:\n    ema_ggt = axis_state.ema_ggt\n\n  return _AxisState(eigvecs, eigvals, inv_eigvals, tail, inv_tail, ema_ggt)", ""]}
{"filename": "precondition/tearfree/sketchy_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for momentum implementation.\"\"\"\n\nimport itertools\nfrom typing import Sequence", "import itertools\nfrom typing import Sequence\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nfrom precondition.tearfree import shampoo\nfrom precondition.tearfree import sketchy", "from precondition.tearfree import shampoo\nfrom precondition.tearfree import sketchy\n\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'freq0',\n          'invalid_options': sketchy.Options(\n              update_freq=0,\n          ),\n      },\n      {\n          'testcase_name': 'decay_neg',\n          'invalid_options': sketchy.Options(\n              second_moment_decay=-0.1,\n          ),\n      },\n      {\n          'testcase_name': 'decay_large',\n          'invalid_options': sketchy.Options(\n              second_moment_decay=1.1,\n          ),\n      },\n  ]", "\n\nclass SketchyTest(parameterized.TestCase):\n  \"\"\"Basic test for shampoo implementation.\"\"\"\n\n  def setUp(self):\n    super().setUp()\n    jax.config.update('jax_debug_nans', True)\n\n  @parameterized.parameters(\n      {'shape': (1, 2, 1)},\n      {'shape': (1, 1, 3, 1, 2, 1)},\n      {'shape': (2, 1, 3, 2)},\n      {'shape': (1, 1)},\n      {'shape': (1,)},\n  )\n  def test_unit_dims_raise(self, shape):\n    \"\"\"Assert raises if unit dimensions are present.\"\"\"\n    with self.assertRaises(ValueError):\n      self._unroll(sketchy.apply(sketchy.Options()), 1, shape)\n\n  @parameterized.named_parameters(_make_invalid_cases())\n  def test_invalid(self, invalid_options):\n    with self.assertRaises(ValueError):\n      sketchy.apply(invalid_options)\n\n  def _make_null_state(self, d, k) -> sketchy._AxisState:\n    return sketchy._init(\n        sketchy.Options(rank=k), jnp.zeros((d,))\n    ).sketches.axes[0]\n\n  def _make_eye_state(self, d, eigs, tail, ndim) -> sketchy._AxisState:\n    k = len(eigs)\n    state = self._make_null_state(d, k)\n    state = state._replace(eigvecs=jnp.eye(d, k))\n    state = state._replace(eigvals=state.eigvals + jnp.asarray(eigs))\n    state = state._replace(tail=tail)\n    if tail > 0:\n      state._replace(inv_tail=tail ** (-1 / (2 * ndim)))\n    mask = state.eigvals > 0\n    ie = jnp.where(mask, (state.tail + state.eigvals**2), 1.0) ** (\n        -1 / (2 * ndim)\n    )\n    ie *= mask\n    state._replace(inv_eigvals=ie)\n    return state\n\n  def _no_decay_options(self, sketch_size, epsilon=0.0):\n    return sketchy.Options(\n        rank=sketch_size, second_moment_decay=1, epsilon=epsilon\n    )\n\n  @parameterized.parameters(range(1, 5))\n  def test_dynamic_exponent(self, ndim):\n    \"\"\"Test that exponent for various gradient ndim is correct.\"\"\"\n    size = 4\n    prev = self._make_eye_state(size, [0], 0.0, ndim)\n    grad = np.zeros([size] * ndim, np.float32)\n    grad[(0,) * ndim] = 2**ndim\n    ret = sketchy._update_axis(self._no_decay_options(1), 0, grad, prev)\n    self.assertAlmostEqual(ret.inv_eigvals, 1 / 2, delta=1e-6)\n\n    prev = self._make_eye_state(size, [2**ndim], 0.0, ndim)\n    grad = np.zeros([size] * ndim, np.float32)\n    ret = sketchy._update_axis(self._no_decay_options(1), 0, grad, prev)\n    self.assertAlmostEqual(ret.inv_eigvals, 1 / 2, delta=1e-6)\n\n  def test_epsilon(self):\n    \"\"\"Test that epsilon is properly calculated.\"\"\"\n    size = 4\n    ndim = 2\n    prev = self._make_eye_state(size, [0], 4, ndim)\n    grad = np.zeros([size] * ndim, np.float32)\n    grad[(0,) * ndim] = 2\n    options = self._no_decay_options(1, epsilon=1e-3)\n    ret = sketchy._update_axis(options, 0, grad, prev)\n    self.assertAlmostEqual(\n        ret.inv_eigvals[0], ((4 + 4) * 1.001) ** (-1 / 4), delta=1e-3, msg=ret\n    )\n    self.assertAlmostEqual(ret.inv_tail, (4 * 1.001) ** (-1 / 4), delta=1e-3)\n    options.relative_epsilon = False\n    ret = sketchy._update_axis(options, 0, grad, prev)\n    self.assertAlmostEqual(\n        ret.inv_eigvals[0], (4 + 4 + 0.001) ** (-1 / 4), delta=1e-6\n    )\n    self.assertAlmostEqual(ret.inv_tail, (4 + 0.001) ** (-1 / 4), delta=1e-3)\n\n  def _make_rand_state(self, size, eigs, tail, ndim):\n    rng = np.random.default_rng(1234)\n    b = rng.standard_normal(size=[size, size])\n    b = b.dot(b.T)\n    _, v = np.linalg.eigh(b)\n    state = self._make_eye_state(size, eigs, tail, ndim)\n    state = state._replace(eigvecs=v[:, : len(eigs)])\n    return state\n\n  # test covariance-adding equality from FD\n  # with rand initial state, and with zero\n  #\n  # Do it under ndim 1 2 or 3 (choose random axis for higher dims)\n\n  @parameterized.parameters(\n      itertools.product(\n          [1, 2, 3],\n          [0.1, 0.9, 1.0],\n          ['zero', 'id', 'rand'],\n          [0, 1],\n          [False, True],\n      )\n  )\n  def test_basic(self, ndim, decay, init, tail, last_axis):\n    \"\"\"Validate low rank returned matrix.\"\"\"\n    d = 3\n    k = 2\n    rng = np.random.default_rng(1234)\n\n    # Make other dims slightly larger\n    shape = [d + i for i in range(ndim)]\n    if last_axis:\n      shape = shape[::-1]\n    grad = rng.standard_normal(size=shape)\n\n    if last_axis:\n      grad_2d = grad.reshape(-1, d)\n      added_cov = grad_2d.T.dot(grad_2d)\n    else:\n      grad_2d = grad.reshape(d, -1)\n      added_cov = grad_2d.dot(grad_2d.T)\n    top_added_eig = np.linalg.eigvalsh(added_cov).max()\n    # Test out one eig above, one below.\n    eigs = np.array([top_added_eig * 4, top_added_eig / 4])\n\n    if init == 'zero':\n      prev = self._make_null_state(d, k)\n    elif init == 'id':\n      prev = self._make_eye_state(d, eigs, tail, ndim)\n    else:\n      assert init == 'rand', init\n      prev = self._make_rand_state(d, eigs, tail, ndim)\n\n    options = sketchy.Options(\n        second_moment_decay=decay,\n        rank=k,\n        epsilon=0.0,\n    )\n    dim = ndim - 1 if last_axis else 0\n    updated = sketchy._update_axis(options, dim, grad, prev)\n\n    if updated.tail > 0:\n      self.assertAlmostEqual(\n          updated.tail ** (-1 / (2 * ndim)), updated.inv_tail\n      )\n    else:\n      self.assertAlmostEqual(updated.inv_tail, 0)\n      self.assertAlmostEqual(updated.tail, 0)\n\n    ie = updated.inv_eigvals\n    e = updated.eigvals**2 + updated.tail\n    mask = updated.eigvals > 0\n    expected_ie = mask * np.where(mask, e, 1.0) ** (-1 / (2 * ndim))\n    delta = 1e-5 * min(expected_ie.max(), ie.max())\n    self.assertSequenceAlmostEqual(expected_ie, ie, delta=delta)\n\n    def _make_cov(sketch: sketchy._AxisState, add_tail=True):\n      # Note eigvals refer to the *root* singular values, so squaring as\n      # we do below recovers covariance.\n      eigvals = np.sqrt(add_tail * sketch.tail + np.square(sketch.eigvals))\n      half = sketch.eigvecs * eigvals\n      complement = np.eye(d) - sketch.eigvecs.dot(sketch.eigvecs.T)\n      tail = complement * sketch.tail if add_tail else 0.0\n      return half.dot(half.T) + tail\n\n    self.assertGreaterEqual(updated.tail, prev.tail * decay)\n\n    prev_cov = _make_cov(prev)\n    new_cov = _make_cov(updated)\n    pd_eigs = np.linalg.eigvalsh(new_cov - decay * prev_cov)\n    # Validate positive definiteness up to numerical error.\n    self.assertGreaterEqual(pd_eigs.min(), -pd_eigs.max() * 1e-4)\n\n    prev_no_tail = _make_cov(prev, add_tail=False)\n    w2, v2 = np.linalg.eigh(decay * prev_no_tail + added_cov)\n    w2 = np.maximum(0, w2 - w2[d - k - 1])\n    half = v2 * jnp.sqrt(w2)\n    expected_cov = half.dot(half.T)\n    actual_cov = _make_cov(updated, add_tail=False)\n    np.testing.assert_allclose(expected_cov, actual_cov, rtol=1e-3)\n\n  def _unroll(self, tx, n, shape):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n\n    init = tx.init(params)\n\n    def reduce(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, new_grad\n\n    _, out_grads = jax.lax.scan(reduce, init, grads)\n    return out_grads\n\n  def test_reduction_to_shampoo(self):\n    tx = sketchy.apply(sketchy.Options(second_moment_decay=0.99, epsilon=0.0))\n    shampoo_tx = shampoo.apply(shampoo.Options(second_moment_decay=0.99))\n    # Choose a shape well below sketchy rank & shampoo block size.\n    shape = (4, 5)\n    nsteps = 3\n    sketchy_run = self._unroll(tx, nsteps, shape)\n    # Shampoo 2nd moment is computed as (1 - decay) * update + decay * update\n    # so we must adjust the preconditioned grad by a factor sqrt(1/(1-decay)).\n    shampoo_run = self._unroll(shampoo_tx, nsteps, shape) / 10\n    np.testing.assert_allclose(shampoo_run, sketchy_run, rtol=3e-3, atol=2e-4)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/optimizer_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for tearfree optimizer.\"\"\"\n\nimport dataclasses\n", "import dataclasses\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport optax\nfrom precondition.tearfree import grafting\nfrom precondition.tearfree import momentum", "from precondition.tearfree import grafting\nfrom precondition.tearfree import momentum\nfrom precondition.tearfree import optimizer\nfrom precondition.tearfree import praxis_shim\nfrom precondition.tearfree import second_order\nfrom precondition.tearfree import shampoo\n\n\nclass OptimizerTest(parameterized.TestCase):\n  \"\"\"Basic test for optimizer configurations.\"\"\"\n\n  def setUp(self):\n    super().setUp()\n    jax.config.update('jax_debug_nans', True)\n\n  def _unroll(self, options, shape, transform=None, lr=0.1, n=4):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n\n    if transform is not None:\n      params = transform(params)\n      grads = jnp.stack([transform(g) for g in grads])\n\n    if isinstance(options, optimizer.TearfreeOptions):\n      tx = optimizer.tearfree(lr, options)\n    else:\n      tx = options\n    init = tx.init(params)\n\n    def reduce(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, new_grad\n\n    _, out_grads = jax.lax.scan(reduce, init, grads)\n    return out_grads\n\n  def _no_graft_no_momentum(self):\n    return optimizer.TearfreeOptions(\n        grafting_options=grafting.Options(\n            grafting_type=grafting.GraftingType.NONE,\n            second_moment_decay=0.0,\n            skip_preconditioning_rank1=False,\n        ),\n        momentum_options=momentum.Options(momentum_decay=0.0),\n    )\n\n  def test_merge_dims(self):\n    shape = (2, 2)\n    options = dataclasses.replace(\n        self._no_graft_no_momentum(),\n        second_order_options=second_order.Options(merge_dims=4),\n    )\n    transform = lambda x: x.reshape(4)\n    actual = self._unroll(options, shape)\n    expected = self._unroll(options, shape, transform)\n    np.testing.assert_allclose(actual.reshape(-1, 4), expected)\n\n  def test_block_size(self):\n    shape = (4,)\n    options = dataclasses.replace(\n        self._no_graft_no_momentum(),\n        second_order_options=second_order.Options(\n            shampoo_options=shampoo.Options(block_size=3)\n        ),\n    )\n    actual = self._unroll(options, shape)\n    expected = self._unroll(options, shape)\n    np.testing.assert_allclose(actual, expected)\n\n  @parameterized.parameters(\n      momentum.Options(),  # Default is 0.9, active momentum.\n      momentum.Options(momentum_decay=0.0),\n      momentum.Options(weight_decay=0.01),\n      momentum.Options(weight_decay=0.01, weight_decay_after_momentum=False),\n      momentum.Options(nesterov=False),\n      momentum.Options(ema=True),\n      momentum.Options(ema=True, nesterov=True),\n  )\n  def test_momentum_no_graft(self, momentum_options):\n    shape = (4,)\n    options = self._no_graft_no_momentum()\n    options.momentum_options = momentum_options\n    tx = praxis_shim.sharded_chain(\n        second_order.apply(options.second_order_options),\n        momentum.apply(momentum_options),\n        optax.scale(-0.1),\n    )\n    actual = self._unroll(options, shape)\n    expected = self._unroll(tx, shape)\n    np.testing.assert_allclose(actual, expected)\n\n  def _grafting_tx(\n      self, grafting_options\n  ) -> praxis_shim.ShardedGradientTransformation:\n    id_tx = optax.identity()\n    id_tx_shard = praxis_shim.ShardedGradientTransformation(\n        id_tx.init,\n        id_tx.update,\n        lambda _: optax.EmptyState(),\n    )\n    return grafting.graft(grafting_options, id_tx_shard)\n\n  def _grafting_tx_with_momentum(\n      self, grafting_options, momentum_options, lr=0.1\n  ):\n    return praxis_shim.sharded_chain(\n        self._grafting_tx(grafting_options),\n        momentum.apply(momentum_options),\n        optax.scale(-lr),\n    )\n\n  @parameterized.parameters(\n      grafting.Options(),\n      grafting.Options(\n          grafting_type=grafting.GraftingType.SGD, second_moment_decay=0.0\n      ),\n      grafting.Options(second_moment_decay=1.0),\n  )\n  def test_momentum_yes_graft(self, grafting_options):\n    shape = (4,)\n    nsteps = 4\n    options = self._no_graft_no_momentum()\n    options.momentum_options.momentum_decay = 0.9\n    options.grafting_options = grafting_options\n    grafting_options.start_preconditioning_step = nsteps + 1\n    grafting_options.skip_preconditioning_rank1 = False\n    tx = self._grafting_tx_with_momentum(\n        grafting_options, options.momentum_options\n    )\n    expected = self._unroll(tx, shape, n=nsteps)\n    actual = self._unroll(options, shape, n=nsteps)\n    np.testing.assert_allclose(actual, expected)\n\n  def _precondition_at(self, i):\n    \"\"\"Return optimizer with momentum, grafting, and start precon at step i.\"\"\"\n    return optimizer.TearfreeOptions(\n        grafting_options=grafting.Options(\n            start_preconditioning_step=i, skip_preconditioning_rank1=False\n        )\n    )\n\n  @parameterized.parameters(\n      dict(shape=(1, 1, 1)),\n      dict(shape=(1,)),\n      dict(shape=tuple()),\n  )\n  def test_scalar_is_grafting(self, shape):\n    nsteps = 4\n    options = self._precondition_at(2)\n    tx = self._grafting_tx_with_momentum(\n        options.grafting_options, options.momentum_options\n    )\n    expected = self._unroll(tx, shape, n=nsteps)\n    actual = self._unroll(options, shape, n=nsteps)\n    np.testing.assert_allclose(actual, expected)\n\n  def test_lr(self):\n    shape = (3,)\n    options = self._precondition_at(2)\n    nsteps = 4\n\n    def schedule(count):\n      return (count + 1) * 0.1\n\n    actual = self._unroll(options, shape, lr=schedule, n=nsteps)\n    expected = self._unroll(options, shape, lr=0.1, n=nsteps)\n    expected *= (jnp.arange(nsteps) + 1).reshape(-1, 1)\n    np.testing.assert_allclose(actual, expected)", "class OptimizerTest(parameterized.TestCase):\n  \"\"\"Basic test for optimizer configurations.\"\"\"\n\n  def setUp(self):\n    super().setUp()\n    jax.config.update('jax_debug_nans', True)\n\n  def _unroll(self, options, shape, transform=None, lr=0.1, n=4):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n\n    if transform is not None:\n      params = transform(params)\n      grads = jnp.stack([transform(g) for g in grads])\n\n    if isinstance(options, optimizer.TearfreeOptions):\n      tx = optimizer.tearfree(lr, options)\n    else:\n      tx = options\n    init = tx.init(params)\n\n    def reduce(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, new_grad\n\n    _, out_grads = jax.lax.scan(reduce, init, grads)\n    return out_grads\n\n  def _no_graft_no_momentum(self):\n    return optimizer.TearfreeOptions(\n        grafting_options=grafting.Options(\n            grafting_type=grafting.GraftingType.NONE,\n            second_moment_decay=0.0,\n            skip_preconditioning_rank1=False,\n        ),\n        momentum_options=momentum.Options(momentum_decay=0.0),\n    )\n\n  def test_merge_dims(self):\n    shape = (2, 2)\n    options = dataclasses.replace(\n        self._no_graft_no_momentum(),\n        second_order_options=second_order.Options(merge_dims=4),\n    )\n    transform = lambda x: x.reshape(4)\n    actual = self._unroll(options, shape)\n    expected = self._unroll(options, shape, transform)\n    np.testing.assert_allclose(actual.reshape(-1, 4), expected)\n\n  def test_block_size(self):\n    shape = (4,)\n    options = dataclasses.replace(\n        self._no_graft_no_momentum(),\n        second_order_options=second_order.Options(\n            shampoo_options=shampoo.Options(block_size=3)\n        ),\n    )\n    actual = self._unroll(options, shape)\n    expected = self._unroll(options, shape)\n    np.testing.assert_allclose(actual, expected)\n\n  @parameterized.parameters(\n      momentum.Options(),  # Default is 0.9, active momentum.\n      momentum.Options(momentum_decay=0.0),\n      momentum.Options(weight_decay=0.01),\n      momentum.Options(weight_decay=0.01, weight_decay_after_momentum=False),\n      momentum.Options(nesterov=False),\n      momentum.Options(ema=True),\n      momentum.Options(ema=True, nesterov=True),\n  )\n  def test_momentum_no_graft(self, momentum_options):\n    shape = (4,)\n    options = self._no_graft_no_momentum()\n    options.momentum_options = momentum_options\n    tx = praxis_shim.sharded_chain(\n        second_order.apply(options.second_order_options),\n        momentum.apply(momentum_options),\n        optax.scale(-0.1),\n    )\n    actual = self._unroll(options, shape)\n    expected = self._unroll(tx, shape)\n    np.testing.assert_allclose(actual, expected)\n\n  def _grafting_tx(\n      self, grafting_options\n  ) -> praxis_shim.ShardedGradientTransformation:\n    id_tx = optax.identity()\n    id_tx_shard = praxis_shim.ShardedGradientTransformation(\n        id_tx.init,\n        id_tx.update,\n        lambda _: optax.EmptyState(),\n    )\n    return grafting.graft(grafting_options, id_tx_shard)\n\n  def _grafting_tx_with_momentum(\n      self, grafting_options, momentum_options, lr=0.1\n  ):\n    return praxis_shim.sharded_chain(\n        self._grafting_tx(grafting_options),\n        momentum.apply(momentum_options),\n        optax.scale(-lr),\n    )\n\n  @parameterized.parameters(\n      grafting.Options(),\n      grafting.Options(\n          grafting_type=grafting.GraftingType.SGD, second_moment_decay=0.0\n      ),\n      grafting.Options(second_moment_decay=1.0),\n  )\n  def test_momentum_yes_graft(self, grafting_options):\n    shape = (4,)\n    nsteps = 4\n    options = self._no_graft_no_momentum()\n    options.momentum_options.momentum_decay = 0.9\n    options.grafting_options = grafting_options\n    grafting_options.start_preconditioning_step = nsteps + 1\n    grafting_options.skip_preconditioning_rank1 = False\n    tx = self._grafting_tx_with_momentum(\n        grafting_options, options.momentum_options\n    )\n    expected = self._unroll(tx, shape, n=nsteps)\n    actual = self._unroll(options, shape, n=nsteps)\n    np.testing.assert_allclose(actual, expected)\n\n  def _precondition_at(self, i):\n    \"\"\"Return optimizer with momentum, grafting, and start precon at step i.\"\"\"\n    return optimizer.TearfreeOptions(\n        grafting_options=grafting.Options(\n            start_preconditioning_step=i, skip_preconditioning_rank1=False\n        )\n    )\n\n  @parameterized.parameters(\n      dict(shape=(1, 1, 1)),\n      dict(shape=(1,)),\n      dict(shape=tuple()),\n  )\n  def test_scalar_is_grafting(self, shape):\n    nsteps = 4\n    options = self._precondition_at(2)\n    tx = self._grafting_tx_with_momentum(\n        options.grafting_options, options.momentum_options\n    )\n    expected = self._unroll(tx, shape, n=nsteps)\n    actual = self._unroll(options, shape, n=nsteps)\n    np.testing.assert_allclose(actual, expected)\n\n  def test_lr(self):\n    shape = (3,)\n    options = self._precondition_at(2)\n    nsteps = 4\n\n    def schedule(count):\n      return (count + 1) * 0.1\n\n    actual = self._unroll(options, shape, lr=schedule, n=nsteps)\n    expected = self._unroll(options, shape, lr=0.1, n=nsteps)\n    expected *= (jnp.arange(nsteps) + 1).reshape(-1, 1)\n    np.testing.assert_allclose(actual, expected)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/shampoo_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for momentum implementation.\"\"\"\n\nimport itertools\nfrom typing import Sequence", "import itertools\nfrom typing import Sequence\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nfrom precondition.tearfree import shampoo", "import numpy as np\nfrom precondition.tearfree import shampoo\n\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'block_size0',\n          'invalid_options': shampoo.Options(\n              block_size=0,\n          ),\n      },\n      {\n          'testcase_name': 'precond0',\n          'invalid_options': shampoo.Options(\n              update_preconditioners_freq=0,\n          ),\n      },\n      {\n          'testcase_name': 'stats0',\n          'invalid_options': shampoo.Options(\n              update_statistics_freq=0,\n          ),\n      },\n      {\n          'testcase_name': 'decay_neg',\n          'invalid_options': shampoo.Options(\n              second_moment_decay=-0.1,\n          ),\n      },\n      {\n          'testcase_name': 'decay_large',\n          'invalid_options': shampoo.Options(\n              second_moment_decay=1.1,\n          ),\n      },\n      {\n          'testcase_name': 'block_size1',\n          'invalid_options': shampoo.Options(\n              block_size=1,\n          ),\n      },\n  ]", "\n\ndef _make_blockify_deblockify_cases() -> Sequence[dict[str, ...]]:\n  shapes_blocks = [\n      (tuple(), 2, 'scalar'),\n      ((5,), 6, '1d_0large'),\n      ((5,), 5, '1d_1large'),\n      ((4,), 2, '1d_1large_moreblocks'),\n      ((2, 3), 6, '2d_0large'),\n      ((2, 3), 3, '2d_1large'),\n      ((2, 2), 2, '2d_2large'),\n      ((4, 4), 2, '2d_2large_moreblocks'),\n      ((2, 3, 3, 2), 4, 'highdim_0large'),\n      ((2, 3, 2, 2), 3, 'highdim_1large'),\n      ((2, 2 * 3, 2, 2), 3, 'highdim_1large_moreblocks'),\n      ((2, 3, 3, 2), 3, 'highdim_2large_together'),\n      ((2, 3, 2, 3), 3, 'highdim_2large_separate'),\n  ]\n\n  cases = []\n  for shape, block_size, name in shapes_blocks:\n    cases.append(\n        dict(\n            shape=shape,\n            block_size=block_size,\n            testcase_name=name,\n        )\n    )\n  return cases", "\n\nclass ShampooTest(parameterized.TestCase):\n  \"\"\"Basic test for shampoo implementation.\"\"\"\n\n  def setUp(self):\n    super().setUp()\n    jax.config.update('jax_debug_nans', True)\n\n  def _unroll(self, options, n, shape):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n    return self._unroll_concrete(options, params, grads)\n\n  def _unroll_concrete(self, options, params, grads):\n    \"\"\"Unrolls with provided params and grads.\"\"\"\n    tx = shampoo.apply(options)\n    init = tx.init(params)\n\n    def reduce(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, new_grad\n\n    _, out_grads = jax.lax.scan(reduce, init, grads)\n    return grads, out_grads\n\n  @parameterized.parameters(\n      {'shape': (1, 2, 1)},\n      {'shape': (1, 1, 3, 1, 2, 1)},\n      {'shape': (2, 1, 3, 2)},\n      {'shape': (1, 1)},\n      {'shape': (1,)},\n  )\n  def test_unit_dims_raise(self, shape):\n    \"\"\"Assert raises if unit dimensions are present.\"\"\"\n    with self.assertRaises(ValueError):\n      self._unroll(shampoo.Options(), 1, shape)\n\n  def test_scalars(self):\n    \"\"\"Validate scalar parameters aren't preconditioned.\"\"\"\n    grads, out_grads = self._unroll(shampoo.Options(), 2, tuple())\n    np.testing.assert_allclose(grads, out_grads)\n\n  def _root(self, x, p):\n    \"\"\"Computes the matrix root x**(-1/(2*p)).\"\"\"\n    return shampoo._pth_inv_root(p * 2, x[np.newaxis, ...])[0]\n\n  @parameterized.parameters(1, 2)\n  def test_basic(self, ndim):\n    \"\"\"Check basic numerical example without blocking or decay.\"\"\"\n    options = shampoo.Options(second_moment_decay=1.0)\n    shape = (2,) * ndim\n    nsteps = 2\n    grads, out_grads = self._unroll(options, 2, shape)\n\n    l, r = 0, 0\n    for i in range(nsteps):\n      if len(shape) == 1:\n        l += np.multiply.outer(grads[i], grads[i])\n      elif len(shape) == 2:\n        l += grads[i].dot(grads[i].T)\n        r += grads[i].T.dot(grads[i])\n\n      pl, pr = self._root(l, len(shape)), r\n      if len(shape) == 2:\n        pr = self._root(r, len(shape))\n\n      pg = pl.dot(grads[i])\n      if len(shape) == 2:\n        pg = pg.dot(pr)\n\n      np.testing.assert_allclose(pg, out_grads[i], rtol=1e-3)\n\n  def test_basic_block(self):\n    \"\"\"Check basic numerical example with blocking.\"\"\"\n    options = shampoo.Options(second_moment_decay=1.0, block_size=2)\n    shape = (4,)\n    nsteps = 2\n\n    # Don't use unroll here to allow state-printing.\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (nsteps, *shape))\n\n    tx = shampoo.apply(options)\n    state = tx.init(params)\n    logging.info('init state: %s', state)\n\n    b0, b1 = 0, 0\n    for i in range(nsteps):\n      out_grad, state = tx.update(grads[i], state, params)\n      logging.info('state @ %s: %s', i, state)\n      g0, g1 = grads[i][:2], grads[i][2:]\n      b0 += np.multiply.outer(g0, g0)\n      b1 += np.multiply.outer(g1, g1)\n      p0, p1 = self._root(b0, len(shape)), self._root(b1, len(shape))\n      logging.info('g0 %s g1 %s', g0, g1)\n      logging.info('b0 %s b1 %s', b0, b1)\n      logging.info('p0 %s p1 %s', p0, p1)\n      pg = np.concatenate([p0.dot(g0), p1.dot(g1)], axis=0)\n      np.testing.assert_allclose(pg, out_grad, rtol=1e-3)\n\n  @parameterized.named_parameters(_make_invalid_cases())\n  def test_invalid(self, invalid_options):\n    with self.assertRaises(ValueError):\n      shampoo.apply(invalid_options)\n\n  @parameterized.named_parameters(_make_blockify_deblockify_cases())\n  def test_blockify_deblockify(self, shape, block_size):\n    rng = jax.random.PRNGKey(0)\n    x = jax.random.normal(rng, shape)\n    options = shampoo.Options(block_size=block_size)\n    meta = shampoo._blocks_metadata(options, x.shape, debug='')\n    bx = shampoo._blockify(x, meta)\n    dx = shampoo._deblockify(bx, meta)\n    self.assertSequenceEqual(dx.shape, x.shape)\n    np.testing.assert_array_equal(x, dx)\n\n  @parameterized.parameters(\n      [\n          {'decay': d, 'last': b}\n          for d, b in itertools.product([0, 0.8], [False, True])\n      ]\n  )\n  def test_basic_ema(self, decay, last):\n    \"\"\"Tests EMA accumulation in stats.\"\"\"\n    z = jnp.zeros((2,))\n    g = jnp.array([0.5, -0.5])\n\n    if last:\n      seq = jnp.stack([z, z, g])\n      one = jnp.stack([g])\n      expected_decay = 1 - decay\n    else:\n      seq = jnp.stack([g, z, z, g])\n      one = jnp.stack([g])\n      expected_decay = (1 - decay) * (decay**3 + 1)\n\n    decayed = shampoo.Options(second_moment_decay=decay)\n    no_decay = shampoo.Options(second_moment_decay=1.0)\n\n    last = self._unroll_concrete(decayed, z, seq)[1][-1]\n    last_no_decay = self._unroll_concrete(no_decay, z, one)[1][-1]\n    last_no_decay /= np.sqrt(expected_decay)\n    np.testing.assert_allclose(last, last_no_decay, rtol=1e-3)\n\n  @parameterized.named_parameters(_make_blockify_deblockify_cases())\n  def test_blocks_equality(self, shape, block_size):\n    rng = jax.random.PRNGKey(0)\n    nsteps = 3\n    grads = jax.random.normal(rng, (nsteps, *shape))\n    options = shampoo.Options(block_size=block_size)\n\n    meta = shampoo._blocks_metadata(options, shape, debug='')\n    grads_for_each_block = [[] for _ in range(meta.num_blocks)]\n    for grad in grads:\n      bgrad = shampoo._blockify(grad, meta)\n      for i in range(meta.num_blocks):\n        grads_for_each_block[i].append(jnp.take(bgrad, i, meta.blocks_axis))\n    last_grad = []\n    unblocked_options = shampoo.Options(block_size=1 + block_size)\n    for block in grads_for_each_block:\n      block = jnp.stack(block)\n      block_grads, block_out_grads = self._unroll_concrete(\n          unblocked_options, block[0], block\n      )\n      del block_grads\n      last_grad.append(block_out_grads[-1])\n\n    expected = jnp.stack(last_grad, axis=meta.blocks_axis)\n    expected = shampoo._deblockify(expected, meta)\n    actual = self._unroll_concrete(options, grads[0], grads)[1]\n    np.testing.assert_allclose(expected, actual[-1])\n\n  def test_stats_freq(self):\n    rng = jax.random.PRNGKey(0)\n    grads = jax.random.normal(rng, (9, 3))\n    options = shampoo.Options(update_statistics_freq=3)\n    _, out_grads = self._unroll_concrete(options, grads[0], grads)\n    options = shampoo.Options(update_statistics_freq=1)\n    _, out_grads_skip = self._unroll_concrete(options, grads[0], grads[::3])\n    np.testing.assert_allclose(out_grads[::3], out_grads_skip)\n\n  def test_precond_freq(self):\n    rng = jax.random.PRNGKey(0)\n    rng, key = jax.random.split(rng)\n    freq = 5\n    grads = jax.random.normal(rng, (freq * 2, 3))\n\n    rng1, rng2 = jax.random.split(key, 2)\n    seq1 = jnp.arange(freq, dtype=int)\n    seq2 = jnp.copy(seq1)\n    jax.random.shuffle(rng1, seq1)\n    jax.random.shuffle(rng2, seq2)\n    # Shuffle within groups of <freq>\n    shuffled = jnp.take(grads, jnp.concatenate([seq1, seq2 + freq]), axis=0)\n\n    grads = jnp.concatenate([jnp.zeros((1, 3)), grads])\n    shuffled = jnp.concatenate([jnp.zeros((1, 3)), shuffled])\n\n    options = shampoo.Options(\n        update_preconditioners_freq=freq, second_moment_decay=1\n    )\n    _, out_grads = self._unroll_concrete(options, grads[0], grads)\n    _, out_grads_shuf = self._unroll_concrete(options, grads[0], shuffled)\n    np.testing.assert_allclose(out_grads, out_grads_shuf)\n\n  def test_tree(self):\n    shape = (3, 2)\n    n = 4\n    options = shampoo.Options()\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n    _, out_grads = self._unroll_concrete(options, params, grads)\n\n    params = {'w': [{'b': params}]}\n    grads = {'w': [{'b': grads}]}\n    _, actual_out_grads = self._unroll_concrete(options, params, grads)\n\n    np.testing.assert_allclose(out_grads, actual_out_grads['w'][0]['b'])", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/optimizer.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tearfree optimizer implementation.\n\nOOM making your eyes water? Try the Tearfree Shampoo optimizer.\n", "OOM making your eyes water? Try the Tearfree Shampoo optimizer.\n\nThis module handles logic for\n\n1. Statistics/preconditioner update frequency\n2. Applying momentum\n3. Combining grafting and preconditioning updates, applying grafting\n4. Typical update procedures, like learning rate, momentum, etc.\n\"\"\"\n", "\"\"\"\n\nimport dataclasses\nfrom typing import Union\n\nimport chex\nimport optax\nfrom precondition.tearfree import grafting\nfrom precondition.tearfree import momentum\nfrom precondition.tearfree import praxis_shim", "from precondition.tearfree import momentum\nfrom precondition.tearfree import praxis_shim\nfrom precondition.tearfree import second_order\n\n\n@dataclasses.dataclass\nclass TearfreeOptions:\n  \"\"\"Configuration dataclass for tearfree optimizer.\n\n  Attributes:\n    grafting_options: Grafting options to modify update norm (see\n      `grafting.Options`).\n    second_order_options: Second-order statistics tracking options (see\n      `second_order.Options`).\n    momentum_options: Momentum options (see `momentum.Options`).\n  \"\"\"\n\n  grafting_options: grafting.Options = dataclasses.field(\n      default_factory=grafting.Options\n  )\n  second_order_options: second_order.Options = dataclasses.field(\n      default_factory=second_order.Options\n  )\n  momentum_options: momentum.Options = dataclasses.field(\n      default_factory=momentum.Options\n  )", "\n\ndef tearfree(\n    learning_rate: Union[chex.Numeric, optax.Schedule],\n    options: TearfreeOptions,\n) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Tearfree optimizer, supports pjit and jit.\n\n  Preconditioned, grafted updates with momentum.\n\n  One key difference in the logic is to only use a single momentum between\n  the graft and preconditioned update. `distributed_shampoo` keeps a separate\n  `diagonal_momentum` buffer, but never uses it after preconditioning is\n  active (it is not used to adjust the grafting norm). This implies (1)\n  we save memory (only one momentum buffer), (2) we are identical to\n  `distributed_shampoo` if there is no warmup or no preconditioning\n  (`options.start_preconditioning_step` is inf or 0).\n\n  Args:\n    learning_rate: The learning rate value or schedule. Learning rate is\n      \"decoupled\", i.e., we always apply it last to the update (after weight\n      decay, after momentum, etc.).\n    options: Tearfree optimizer options.\n\n  Returns:\n    The sharded gradient transformation corresponding to an updated,\n      preconditioned gradient, times the negative learning rate.\n  \"\"\"\n\n  second_order_tx = second_order.apply(options.second_order_options)\n  graft_tx = grafting.graft(options.grafting_options, second_order_tx)\n  momentum_tx = momentum.apply(options.momentum_options)\n  if callable(learning_rate):\n    lr_tx = optax.scale_by_schedule(lambda x: -1.0 * learning_rate(x))\n  else:\n    lr_tx = optax.scale(-1.0 * learning_rate)\n  return praxis_shim.sharded_chain(\n      graft_tx,\n      momentum_tx,\n      lr_tx,\n  )", ""]}
{"filename": "precondition/tearfree/optimizer_smoke_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Smoke tests for tearfree.\n\nThe smoke test uses CPU-based sharding to verify that, under a variety of\nsettings, (1) the optimizer results in finite, not-nan gradients and (2)", "The smoke test uses CPU-based sharding to verify that, under a variety of\nsettings, (1) the optimizer results in finite, not-nan gradients and (2)\ndistributed computation options don't change the math.\n\"\"\"\n\nimport copy\nfrom typing import Sequence, Union\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized", "from absl.testing import absltest\nfrom absl.testing import parameterized\nimport chex\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport optax\nfrom precondition.tearfree import grafting\nfrom precondition.tearfree import momentum\nfrom precondition.tearfree import optimizer", "from precondition.tearfree import momentum\nfrom precondition.tearfree import optimizer\nfrom precondition.tearfree import second_order\nfrom precondition.tearfree import shampoo\nfrom precondition.tearfree import sketchy\n\n\ndef _make_distributed_equality_cases() -> list[dict[str, ...]]:\n  \"\"\"Make test cases of options for optimizer checks.\"\"\"\n  cases = []\n\n  # Basic options exercise all of shampoo, grafting after the first step.\n  basic_options = optimizer.TearfreeOptions(\n      grafting_options=grafting.Options(\n          grafting_type=grafting.GraftingType.RMSPROP,\n          second_moment_decay=0.9,\n          epsilon=1e-5,\n          start_preconditioning_step=1,\n          skip_preconditioning_any_dim_gt=4096,\n          skip_preconditioning_rank1=False,\n      ),\n      second_order_options=second_order.Options(\n          second_order_type=second_order.SecondOrderType.SHAMPOO,\n          shampoo_options=shampoo.Options(\n              block_size=1024,\n              update_preconditioners_freq=1,\n              update_statistics_freq=1,\n              second_moment_decay=0.9,\n          ),\n          merge_dims=4096,\n      ),\n      momentum_options=momentum.Options(\n          ema=True,\n          nesterov=True,\n          momentum_decay=0.5,\n          weight_decay=0.0,\n          weight_decay_after_momentum=True,\n      ),\n  )\n\n  basic_case = {\n      'testcase_name': 'basic',\n      'nsteps': 3,\n      'options': basic_options,\n      'lr': 0.1,\n      'shape': (4,),\n  }\n  cases.append(basic_case)\n\n  case = copy.deepcopy(basic_case)\n  case['lr'] = lambda x: 0.1 / (x + 1)\n  case['testcase_name'] = 'schedule'\n  cases.append(case)\n\n  case = copy.deepcopy(basic_case)\n  second_order_options = case['options'].second_order_options\n  second_order_options.second_order_type = second_order.SecondOrderType.SKETCHY\n  second_order_options.shampoo_options = None\n  second_order_options.sketchy_options = sketchy.Options()\n  case['testcase_name'] = 'sketchy'\n  cases.append(case)\n\n  case = copy.deepcopy(case)\n  case['testcase_name'] += '_notrunc_lowrank'\n  sketchy_options = case['options'].second_order_options.sketchy_options\n  sketchy_options.truncate_numerical_noise = False\n  sketchy_options.rank = 2\n  cases.append(case)\n\n  case = copy.deepcopy(basic_case)\n  case['options'].grafting_options.grafting_type = (\n      grafting.GraftingType.ADAFACTOR\n  )\n  case['testcase_name'] = 'adafactor'\n  cases.append(case)\n\n  # Need to test we at least parallelize the identical-to-tensor shapes\n  # without any blocks.\n  # Additional variants:\n  # wd\n  # wd with decay before momentum\n  # grid of nesterov/ema\n  # exercise merge dims 2d doing a merge\n  # exercise merge dims 3d with only one thing merged\n  # skip preconditioning any dim gt activating\n  # skip preconditioning any dim gt rank1 activating\n  # update stats/precond every 2 (6 steps)\n  # update stats/precond every 2/4 (6 steps)\n\n  # Test block-wise parallelism for Shampoo\n\n  return cases", "\n\nclass OptimizerSmokeTest(parameterized.TestCase):\n  \"\"\"Basic test for optimizer configurations.\"\"\"\n\n  def _unroll(self, options, shape, transform=None, lr=0.1, n=4):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.zeros(shape)\n    grads = jax.random.normal(rng, (n, *shape))\n\n    if transform is not None:\n      params = transform(params)\n      grads = jnp.stack([transform(g) for g in grads])\n\n    tx = optimizer.tearfree(lr, options)\n\n    init = tx.init(params)\n\n    def reduce(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, new_grad\n\n    _, out_grads = jax.lax.scan(reduce, init, grads)\n    return out_grads\n\n  @parameterized.named_parameters(_make_distributed_equality_cases())\n  def test_distributed_equality(\n      self,\n      options: optimizer.TearfreeOptions,\n      shape: Sequence[int],\n      lr: Union[float, optax.Schedule],\n      nsteps: int,\n  ) -> None:\n    single_core = self._unroll(options, shape, lr=lr, n=nsteps)\n    multi_core = self._unroll(options, shape, lr=lr, n=nsteps)\n\n    chex.assert_tree_all_finite(single_core)\n    np.testing.assert_allclose(single_core, multi_core)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/second_order.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Various strategies for tracking second order statistics.\"\"\"\n\nimport dataclasses\nimport enum", "import dataclasses\nimport enum\nfrom typing import Optional\n\nimport optax\nfrom precondition.tearfree import praxis_shim\nfrom precondition.tearfree import reshaper\nfrom precondition.tearfree import shampoo\nfrom precondition.tearfree import sketchy\n", "from precondition.tearfree import sketchy\n\n\n@enum.unique\nclass SecondOrderType(enum.Enum):\n  \"\"\"Different second order covariance tracking methods.\"\"\"\n\n  SHAMPOO = 'shampoo'\n  SKETCHY = 'sketchy'\n", "\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Toggle which second order statistics to track.\n\n  Attributes:\n    merge_dims: Merges small dimensions, see `reshaper.Options.merge_dims`.\n    second_order_type: Which optimizer to use for grafting updates.\n    shampoo_options: Options for blocked shampoo.\n    sketchy_options: Options for Sketchy.\n  \"\"\"\n\n  merge_dims: int = 1024\n  second_order_type: SecondOrderType = SecondOrderType.SHAMPOO\n  shampoo_options: Optional[shampoo.Options] = dataclasses.field(\n      default_factory=shampoo.Options\n  )\n  sketchy_options: Optional[sketchy.Options] = None", "\n\ndef apply(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Generate the second order update from options.\"\"\"\n  reshaper_options = _reshaper_options(options)\n  merge_tx = reshaper.merge(reshaper_options)\n  precond_tx = _update_stats_and_precondition(options)\n\n  def wrap_init(params: optax.Params):\n    reshaped_params, _ = merge_tx.update(params, merge_tx.init(params), params)\n    return precond_tx.init(reshaped_params)\n\n  # TODO(vladf): later, we'll need to wrap pspec as well.\n  wrapped_precond_tx = praxis_shim.ShardedGradientTransformation(\n      wrap_init, precond_tx.update, precond_tx.init_partition_spec\n  )\n\n  return praxis_shim.sharded_chain(\n      merge_tx,\n      wrapped_precond_tx,\n      reshaper.unmerge(reshaper_options),\n  )", "\n\ndef _reshaper_options(options: Options) -> reshaper.Options:\n  if options.second_order_type == SecondOrderType.SHAMPOO:\n    assert options.shampoo_options\n    block_size = options.shampoo_options.block_size\n    return reshaper.Options(options.merge_dims, block_size)\n  if options.second_order_type == SecondOrderType.SKETCHY:\n    return reshaper.Options(options.merge_dims, 0)\n  else:\n    raise ValueError(\n        'unknown second order type {}'.format(options.second_order_type)\n    )", "\n\ndef _update_stats_and_precondition(\n    options: Options,\n) -> praxis_shim.ShardedGradientTransformation:\n  if options.second_order_type == SecondOrderType.SHAMPOO:\n    assert options.shampoo_options\n    return shampoo.apply(options.shampoo_options)\n  if options.second_order_type == SecondOrderType.SKETCHY:\n    assert options.sketchy_options\n    return sketchy.apply(options.sketchy_options)\n  else:\n    raise ValueError(\n        'unknown second order type {}'.format(options.second_order_type)\n    )", ""]}
{"filename": "precondition/tearfree/momentum_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for momentum implementation.\"\"\"\n\nimport itertools\nfrom typing import Sequence", "import itertools\nfrom typing import Sequence\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport optax\nfrom precondition.tearfree import momentum", "import optax\nfrom precondition.tearfree import momentum\n\n\ndef _make_no_state_cases() -> Sequence[dict[str, ...]]:\n  bools = [False, True]\n  cases = []\n  for ema, nesterov, wd, wd_after in itertools.product(\n      bools, bools, [0.0, 0.9], bools\n  ):\n    momentum_decay = 0.0\n    options = momentum.Options(\n        ema,\n        nesterov,\n        momentum_decay,\n        wd,\n        wd_after,\n    )\n    cases.append({'options': options})\n  return cases", "\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'momentum_neg',\n          'invalid_options': momentum.Options(\n              momentum_decay=-1.0,\n          ),\n      },\n      {\n          'testcase_name': 'wd_neg',\n          'invalid_options': momentum.Options(\n              weight_decay=-0.1,\n          ),\n      },\n      {\n          'testcase_name': 'momentum_large',\n          'invalid_options': momentum.Options(\n              momentum_decay=1.1,\n          ),\n      },\n  ]", "\n\nclass MomentumTest(parameterized.TestCase):\n  \"\"\"Basic test for momentum implementation.\"\"\"\n\n  def _unroll(self, tx, n, extract=False, wd=0):\n    \"\"\"Generate states and grad updates n times.\"\"\"\n    rng = jax.random.PRNGKey(0)\n    params = jnp.ones((3,))\n    grads = jax.random.normal(rng, (n, 3)) + wd * params\n    init = tx.init(params)\n\n    def scan(state, grad):\n      new_grad, new_state = tx.update(grad, state, params)\n      return new_state, (new_state, new_grad)\n\n    _, (states, out_grad) = jax.lax.scan(scan, init, grads)\n    if not extract:\n      return out_grad\n    return self._extract_velocity(states), out_grad, grads\n\n  def _check_equal(self, expected_tx, actual_tx, nsteps):\n    expected_grads = self._unroll(expected_tx, nsteps)\n    actual_grads = self._unroll(actual_tx, nsteps)\n    np.testing.assert_allclose(expected_grads, actual_grads)\n\n  @parameterized.parameters(0.1, 0.9, 0.99)\n  def test_ema(self, decay):\n    \"\"\"Check that we simulate ema decay.\"\"\"\n    options = momentum.Options(ema=True, nesterov=False, momentum_decay=decay)\n    nsteps = 4\n    actual = momentum.apply(options)\n    expected = optax.ema(decay, debias=False)\n    self._check_equal(expected, actual, nsteps)\n\n  def _extract_velocity(self, state):\n    \"\"\"Asserts only velocity state exists, extracts it.\"\"\"\n    flat = jax.tree_util.tree_flatten(state)[0]\n    self.assertLen(flat, 1)\n    return flat[0]\n\n  @parameterized.parameters(itertools.product([False, True], repeat=2))\n  def test_wd_before_momentum(self, ema, nesterov):\n    options = momentum.Options(\n        ema=ema,\n        nesterov=nesterov,\n        momentum_decay=0.9,\n        weight_decay=0.0,\n    )\n    nsteps = 4\n    tx = momentum.apply(options)\n    expected_grads = self._unroll(tx, nsteps, wd=0.1)\n    options = momentum.Options(\n        ema=ema,\n        nesterov=nesterov,\n        momentum_decay=0.9,\n        weight_decay=0.1,\n        weight_decay_after_momentum=False,\n    )\n    tx = momentum.apply(options)\n    actual_grads = self._unroll(tx, nsteps)\n    np.testing.assert_allclose(expected_grads, actual_grads)\n\n  @parameterized.parameters(itertools.product([False, True], repeat=2))\n  def test_basic(self, ema, decay_after):\n    wd = 0.1 if decay_after else 0.0\n    if decay_after:\n      return\n    decay = 0.9\n    options = momentum.Options(\n        ema=ema,\n        nesterov=True,\n        momentum_decay=decay,\n        weight_decay=wd,\n        weight_decay_after_momentum=True,\n    )\n    tx = momentum.apply(options)\n    v, g, ig = self._unroll(tx, 2, extract=True)\n\n    ev = jnp.zeros((3,))\n    factor = (1 - decay) if ema else 1.0\n    ev += factor * ig[0]\n    self.assertSequenceAlmostEqual(v[0], ev, msg=v)\n    expected_grad = decay * ev + factor * ig[0]\n    expected_grad += jnp.ones((3,)) * wd\n    self.assertSequenceAlmostEqual(g[0], expected_grad)\n\n    ev = ev * decay + factor * ig[1]\n    self.assertSequenceAlmostEqual(v[1], ev, delta=1e-6)\n    expected_grad = decay * ev + factor * ig[1]\n    expected_grad += jnp.ones((3,)) * wd\n    self.assertSequenceAlmostEqual(g[1], expected_grad, delta=1e-6)\n\n  @parameterized.parameters(_make_no_state_cases())\n  def test_no_state(self, options):\n    \"\"\"Ensure no state is created when decay is 0.0.\"\"\"\n    assert options.momentum_decay == 0.0\n    tx = momentum.apply(options)\n    state = tx.init(jnp.zeros((3,)))\n    flat = jax.tree_util.tree_flatten(state)[0]\n    self.assertEmpty(flat)\n\n  @parameterized.named_parameters(_make_invalid_cases())\n  def test_invalid(self, invalid_options):\n    with self.assertRaises(ValueError):\n      momentum.apply(invalid_options)", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/momentum.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Momentum configuration and transform.\"\"\"\n\nimport copy\nimport dataclasses", "import copy\nimport dataclasses\nfrom typing import Union\n\nimport jax\nimport optax\nfrom precondition.tearfree import praxis_shim\n\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Configuration dataclass for momentum.\n\n  Notably, this class contains weight decay parameters. Why?\n\n  In classical convex literature, Nesterov acceleration applied to gradient\n  descent can be viewed as \"revising\" the last iterate's momentum based on\n  the gradient we observe immediately after taking a momentum \"gamble\"\n  (see viz, https://stats.stackexchange.com/a/191727).\n\n  To maintain this interpretation exactly, we would need to go against\n  the grain on how weight decay is implemented. Momentum must be the last*\n  gradient transformation applied to the iterate, which would require the\n  weight decay to be applied to the update before it's used to change\n  the velocity (momentum's state, the first moment).\n\n  In particular, AdamW and Adafactor suggest direct weight downscaling,\n  excluding weight decay from the velocity accumulation.\n\n  As a result, the true meaning of Nesterov acceleration here is better\n  understood literally, described in its parameter doc.\n\n  *Technically, some optimizers include the learning rate in the update used to\n  update the velocity (e.g., Adafactor), but others apply the learning rate\n  scaling last, after momentum (e.g., Adam). We can recover the former from the\n  latter by dividing the decay by the root of the learning rate, so this\n  particular \"gradient transformation\" shouldn't be viewed as affecting\n  the Nesterov interpretation, up to tuning constants.\n\n  Attributs:\n    ema: If true, momentum is computed as an exponential moving\n      average: `velocity(t+1) = decay * velocity(t) + (1 - decay) * update(t)`\n      If false, then uses \"trace\" accumulation for momentum:\n      `velocity(t+1) = decay * velocity(t) + update(t)`. Note that if the\n      updates were the same (they aren't) then these would be the same up to a\n      factor of `(1 - decay)`. This corresponds to distributed_shampoo argument\n      `moving_average_for_momentum`.\n    nesterov: Toggle for Nesterov acceleration. If false, then the new\n      update `update'(t+1)` simply equals `velocity(t+1)`. If true, then\n      `update'(t+1) = maybe_decay * update(t) + decay * velocity(t+1)`, where\n      `maybe_decay` is `(1 - decay)` if `ema` and 1 otherwise.\n    momentum_decay: The decay referred to in `ema` and `nesterov` formulas.\n    weight_decay: Add `weight_decay * x(t)` to the `update(t)` value, where\n      `x(t)` is the value of the current parameters.\n    weight_decay_after_momentum: Whether weight decay addition is performed\n      after the momentum transformation.\n  \"\"\"\n\n  ema: bool = False\n  nesterov: bool = True\n  momentum_decay: float = 0.9\n  weight_decay: float = 0.0\n  weight_decay_after_momentum: bool = True", "\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Configuration dataclass for momentum.\n\n  Notably, this class contains weight decay parameters. Why?\n\n  In classical convex literature, Nesterov acceleration applied to gradient\n  descent can be viewed as \"revising\" the last iterate's momentum based on\n  the gradient we observe immediately after taking a momentum \"gamble\"\n  (see viz, https://stats.stackexchange.com/a/191727).\n\n  To maintain this interpretation exactly, we would need to go against\n  the grain on how weight decay is implemented. Momentum must be the last*\n  gradient transformation applied to the iterate, which would require the\n  weight decay to be applied to the update before it's used to change\n  the velocity (momentum's state, the first moment).\n\n  In particular, AdamW and Adafactor suggest direct weight downscaling,\n  excluding weight decay from the velocity accumulation.\n\n  As a result, the true meaning of Nesterov acceleration here is better\n  understood literally, described in its parameter doc.\n\n  *Technically, some optimizers include the learning rate in the update used to\n  update the velocity (e.g., Adafactor), but others apply the learning rate\n  scaling last, after momentum (e.g., Adam). We can recover the former from the\n  latter by dividing the decay by the root of the learning rate, so this\n  particular \"gradient transformation\" shouldn't be viewed as affecting\n  the Nesterov interpretation, up to tuning constants.\n\n  Attributs:\n    ema: If true, momentum is computed as an exponential moving\n      average: `velocity(t+1) = decay * velocity(t) + (1 - decay) * update(t)`\n      If false, then uses \"trace\" accumulation for momentum:\n      `velocity(t+1) = decay * velocity(t) + update(t)`. Note that if the\n      updates were the same (they aren't) then these would be the same up to a\n      factor of `(1 - decay)`. This corresponds to distributed_shampoo argument\n      `moving_average_for_momentum`.\n    nesterov: Toggle for Nesterov acceleration. If false, then the new\n      update `update'(t+1)` simply equals `velocity(t+1)`. If true, then\n      `update'(t+1) = maybe_decay * update(t) + decay * velocity(t+1)`, where\n      `maybe_decay` is `(1 - decay)` if `ema` and 1 otherwise.\n    momentum_decay: The decay referred to in `ema` and `nesterov` formulas.\n    weight_decay: Add `weight_decay * x(t)` to the `update(t)` value, where\n      `x(t)` is the value of the current parameters.\n    weight_decay_after_momentum: Whether weight decay addition is performed\n      after the momentum transformation.\n  \"\"\"\n\n  ema: bool = False\n  nesterov: bool = True\n  momentum_decay: float = 0.9\n  weight_decay: float = 0.0\n  weight_decay_after_momentum: bool = True", "\n\nState = Union[optax.MaskedNode, optax.TraceState]\n\n\ndef apply(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Generate the momentum update from options.\"\"\"\n  _validate(options)\n\n  momentum_transforms = []\n  if options.momentum_decay:\n    if options.ema:\n      momentum_transforms.append(optax.scale(1 - options.momentum_decay))\n    momentum_transforms.append(\n        _sharded_trace(options.momentum_decay, options.nesterov)\n    )\n\n  wd_transforms = [optax.add_decayed_weights(options.weight_decay)] * (\n      options.weight_decay > 0.0\n  )\n\n  if options.weight_decay_after_momentum:\n    transforms = momentum_transforms + wd_transforms\n  else:\n    transforms = wd_transforms + momentum_transforms\n\n  return praxis_shim.sharded_chain(*transforms)", "\n\ndef _validate(options: Options):\n  \"\"\"Raise ValueError if options are invalid.\"\"\"\n  if not (0 <= options.momentum_decay <= 1):\n    raise ValueError(\n        'momentum_decay ({}) must be in [0, 1]'.format(options.momentum_decay)\n    )\n\n  if not (options.weight_decay >= 0):\n    raise ValueError(\n        'weight_decay ({}) must be >= 0'.format(options.weight_decay)\n    )", "\n\ndef _sharded_trace(\n    momentum: float, nesterov: bool\n) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Extend optax's trace to allow sharding.\"\"\"\n  trace = optax.trace(momentum, nesterov)\n\n  def init_pspec_fn(mdl_params):\n    def _opt_state_sharding_spec(var_hparams):\n      s_var_hparams = copy.deepcopy(var_hparams)\n      s_var_hparams.init = None\n      return s_var_hparams\n\n    mdl_sharding = jax.tree_map(_opt_state_sharding_spec, mdl_params)\n    return optax.TraceState(trace=mdl_sharding)\n\n  return praxis_shim.ShardedGradientTransformation(\n      trace.init, trace.update, init_pspec_fn\n  )", ""]}
{"filename": "precondition/tearfree/reshaper.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Parameter reshaping module.\"\"\"\n\nimport dataclasses\nimport functools", "import dataclasses\nimport functools\n\nimport jax\nfrom jax import numpy as jnp\nimport optax\nfrom precondition import distributed_shampoo\n\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Parameter reshaping options.\n\n  Attributes:\n    merge_dims: Collapse dimensions smaller than this number left-to-right,\n      e.g., [3, 1, 5, 2, 2] becomes [3, 5, 4] with `merge_dims = 4`. Notice\n      ordering, [2, 3, 2] becomes [6, 2] with `merge_dims = 6`, not its reverse.\n    block_size: If nonzero, pads all dimensions larger than the block size to a\n      multiple of the block size.\n  \"\"\"\n\n  merge_dims: int = 1024\n  block_size: int = 1024", "\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Parameter reshaping options.\n\n  Attributes:\n    merge_dims: Collapse dimensions smaller than this number left-to-right,\n      e.g., [3, 1, 5, 2, 2] becomes [3, 5, 4] with `merge_dims = 4`. Notice\n      ordering, [2, 3, 2] becomes [6, 2] with `merge_dims = 6`, not its reverse.\n    block_size: If nonzero, pads all dimensions larger than the block size to a\n      multiple of the block size.\n  \"\"\"\n\n  merge_dims: int = 1024\n  block_size: int = 1024", "\n\n@dataclasses.dataclass\nclass _Shapes:\n  \"\"\"Shape container.\"\"\"\n\n  original_shape: list[int]\n  merged_shape: list[int]\n  padded_shape: list[int]\n", "\n\ndef _derive_shapes(options: Options, param: jax.Array) -> _Shapes:\n  \"\"\"Derive desired shapes from options.\"\"\"\n  merged = distributed_shampoo.merge_small_dims(param.shape, options.merge_dims)\n  if merged == [1]:\n    return _Shapes(\n        original_shape=list(param.shape),\n        merged_shape=[],\n        padded_shape=[],\n    )\n  if options.block_size == 0:\n    padded = merged\n  else:\n    padded = []\n    for s in merged:\n      if s >= options.block_size:\n        s = (s + options.block_size - 1) // options.block_size\n        s *= options.block_size\n      padded.append(s)\n  return _Shapes(\n      original_shape=list(param.shape),\n      merged_shape=merged,\n      padded_shape=padded,\n  )", "\n\ndef merge(options: Options) -> optax.GradientTransformation:\n  \"\"\"Merge and maybe pad gradients, leaving params alone.\"\"\"\n\n  if options.merge_dims < 2:\n    raise ValueError(\n        'merge_dims ({}) must be at least 2'.format(options.merge_dims)\n    )\n\n  if options.block_size < 2 and options.block_size != 0:\n    raise ValueError(\n        'block_size ({}) must be at least 2 (or 0 to disable)'.format(\n            options.block_size\n        )\n    )\n\n  def _merge(update: jax.Array, shapes: _Shapes) -> jax.Array:\n    assert list(update.shape) == shapes.original_shape, (update.shape, shapes)\n    merged = update.reshape(shapes.merged_shape)\n    padding = [\n        (0, p - m) for p, m in zip(shapes.padded_shape, shapes.merged_shape)\n    ]\n    if padding and options.block_size > 0:\n      return jnp.pad(merged, padding)\n    return merged\n\n  def update(\n      updates: optax.Updates,\n      state: optax.MaskedNode,\n      params: optax.Params,\n  ) -> tuple[optax.Updates, optax.MaskedNode]:\n    shapes = jax.tree_map(functools.partial(_derive_shapes, options), params)\n    new_updates = jax.tree_map(_merge, updates, shapes)\n    return new_updates, state\n\n  return optax.GradientTransformation(lambda _: optax.MaskedNode(), update)", "\n\ndef unmerge(options: Options) -> optax.GradientTransformation:\n  \"\"\"Unmerge and unpad gradients, leaving params alone.\"\"\"\n\n  def _unmerge(update: jax.Array, shapes: _Shapes) -> jax.Array:\n    assert list(update.shape) == shapes.padded_shape, (update.shape, shapes)\n    if options.block_size == 0:\n      merged = update\n    else:\n      merged = update[tuple(slice(0, m) for m in shapes.merged_shape)]\n    return merged.reshape(shapes.original_shape)\n\n  def update(\n      updates: optax.Updates,\n      state: optax.MaskedNode,\n      params: optax.Params,\n  ) -> tuple[optax.Updates, optax.MaskedNode]:\n    shapes = jax.tree_map(functools.partial(_derive_shapes, options), params)\n    new_updates = jax.tree_map(_unmerge, updates, shapes)\n    return new_updates, state\n\n  return optax.GradientTransformation(lambda _: optax.MaskedNode(), update)", ""]}
{"filename": "precondition/tearfree/praxis_shim.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Shim interfaces for praxis, to avoid circular dependencies.\"\"\"\n\nimport dataclasses\nfrom typing import Any, NamedTuple, Union", "import dataclasses\nfrom typing import Any, NamedTuple, Union\n\nimport jax\nfrom jax import numpy as jnp\nimport optax\n\n\n@dataclasses.dataclass(frozen=True)\nclass ShardedGradientTransformation:\n  \"\"\"GradientTransformation that supports spmd.\"\"\"\n\n  init: optax.TransformInitFn\n  update: optax.TransformUpdateFn\n  init_partition_spec: Any", "@dataclasses.dataclass(frozen=True)\nclass ShardedGradientTransformation:\n  \"\"\"GradientTransformation that supports spmd.\"\"\"\n\n  init: optax.TransformInitFn\n  update: optax.TransformUpdateFn\n  init_partition_spec: Any\n\n\nNestedHParams = Any", "\nNestedHParams = Any\n\n\nclass WeightHParams(NamedTuple):\n  shape: list[int]\n  init: Any\n  dtype: jnp.dtype\n  collections: Any\n  tensor_split_dims_mapping: list[int]", "\n\ndef sharded_chain(\n    *args: Union[optax.GradientTransformation, ShardedGradientTransformation],\n) -> ShardedGradientTransformation:\n  \"\"\"Chain as in praxis.optimizers.sharded_chain.\"\"\"\n\n  def init_fn(params):\n    return tuple(fn.init(params) for fn in args)\n\n  def update_fn(updates, state, params=None):\n    if len(args) != len(state):\n      raise ValueError(\n          'The number of updates and states has to be the same in '\n          f'sharded chain. got {len(args)=}, {len(state)=}'\n      )\n\n    new_state = []\n    for s, fn in zip(state, args):\n      updates, new_s = fn.update(updates, s, params)\n      # Some of the new states may have None instead of optax.MaskedNode.\n      new_s = jax.tree_map(\n          lambda x: optax.MaskedNode() if x is None else x,\n          new_s,\n          is_leaf=lambda x: x is None,\n      )\n      new_state.append(new_s)\n    return updates, tuple(new_state)\n\n  def init_partition_spec_fn(mdl_vars):\n    partition_specs = []\n    for fn in args:\n      init_partition_spec = getattr(fn, 'init_partition_spec', None)\n      if callable(init_partition_spec):\n        nmap = init_partition_spec(mdl_vars)\n        partition_specs.append(nmap)\n      else:\n        # Raise ValueError as we are attempting to sharded_chain an optimizer\n        # that does not have an `init_partition_spec` method defined.\n        raise ValueError(\n            'Attempting to use an optimizer in sharded_chain that '\n            'does not have an init_partition_spec.'\n        )\n    return optax.MaskedState(inner_state=tuple(partition_specs))\n\n  return ShardedGradientTransformation(\n      init=init_fn, update=update_fn, init_partition_spec=init_partition_spec_fn\n  )", ""]}
{"filename": "precondition/tearfree/grafting_test.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for grafting implementations.\"\"\"\n\nimport functools\nimport itertools", "import functools\nimport itertools\nfrom typing import Sequence\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport optax", "import numpy as np\nimport optax\nfrom precondition.tearfree import grafting\nfrom precondition.tearfree import praxis_shim\n\n\ndef _minustwo() -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Generate a direction-reversing gradient transformation.\"\"\"\n  update = functools.partial(jax.tree_map, lambda x: -2 * x)\n  return praxis_shim.ShardedGradientTransformation(\n      lambda _: optax.EmptyState,\n      lambda u, s, _: (update(u), s),\n      optax.EmptyState,\n  )", "\n\ndef _make_invalid_cases() -> Sequence[dict[str, ...]]:\n  \"\"\"Generate invalid cases which should throw.\"\"\"\n  return [\n      {\n          'testcase_name': 'rmsprop_0',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.RMSPROP,\n              second_moment_decay=0.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'rmsprop_neg',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.RMSPROP,\n              second_moment_decay=-1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'rmsprop_eps_neg',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.RMSPROP,\n              epsilon=-1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_0',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              second_moment_decay=-1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_neg',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              second_moment_decay=-1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_not_less_than_1',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              second_moment_decay=1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_eps_neg',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              epsilon=-1.0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_min_size_0',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              min_dim_size_to_factor=0,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_min_size_neg',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              min_dim_size_to_factor=-1,\n              start_preconditioning_step=0,\n          ),\n      },\n      {\n          'testcase_name': 'adafactor_clip_less_than_1',\n          'invalid_options': grafting.Options(\n              grafting.GraftingType.ADAFACTOR,\n              clipping_threshold=0.5,\n              start_preconditioning_step=0,\n          ),\n      },\n  ]", "\n\nclass GraftingTest(parameterized.TestCase):\n  \"\"\"Basic test for grafting praxis_shim implementations.\"\"\"\n\n  def _check_equal(self, expected_tx, actual_tx, nsteps, shape=(3,)):\n    rng = jax.random.PRNGKey(0)\n    rng, key = jax.random.split(rng)\n    params = jax.random.normal(key, shape)\n    expected_state = expected_tx.init(params)\n    actual_state = actual_tx.init(params)\n\n    for i in range(nsteps):\n      rng, key = jax.random.split(rng)\n      grad = jax.random.normal(key, shape)\n      expected_grad, expected_state = expected_tx.update(\n          grad, expected_state, params\n      )\n      actual_grad, actual_state = actual_tx.update(grad, actual_state, params)\n      np.testing.assert_allclose(expected_grad, actual_grad, err_msg=i)\n\n  def test_no_graft(self):\n    \"\"\"Check that no graft behaves exactly as the base transform.\"\"\"\n    options = grafting.Options(\n        grafting.GraftingType.NONE,\n        0.0,\n        start_preconditioning_step=0,\n        skip_preconditioning_rank1=False,\n    )\n    grafted = grafting.graft(options, _minustwo())\n    nsteps = 4\n    self._check_equal(_minustwo(), grafted, nsteps)\n\n  def _check_norm_direction(\n      self,\n      norm_tx,\n      direction_tx,\n      actual_tx,\n      nsteps,\n      start_precond_step,\n      shape=(3,),\n  ):\n    rng = jax.random.PRNGKey(0)\n    rng, key = jax.random.split(rng)\n    params = jax.random.normal(key, shape)\n    state = actual_tx.init(params)\n    norm_state = norm_tx.init(params)\n    direction_state = norm_tx.init(params)\n\n    for i in range(nsteps):\n      rng, key = jax.random.split(rng)\n      grad = jax.random.normal(key, shape)\n      actual_grad, state = actual_tx.update(grad, state, params)\n\n      norm_grad, norm_state = norm_tx.update(grad, norm_state, params)\n      direction_grad, direction_state = direction_tx.update(\n          grad, direction_state, params\n      )\n\n      if i >= start_precond_step:\n        direction_norm = jnp.linalg.norm(direction_grad)\n        actual_norm = jnp.linalg.norm(actual_grad)\n        norm_norm = jnp.linalg.norm(norm_grad)\n        direction_grad_unit = direction_grad / direction_norm\n        actual_grad_unit = actual_grad / actual_norm\n        np.testing.assert_allclose(\n            direction_grad_unit, actual_grad_unit, rtol=1e-6\n        )\n        np.testing.assert_allclose(actual_norm, norm_norm, rtol=1e-6)\n      else:\n        np.testing.assert_allclose(norm_grad, actual_grad)\n\n  def _norm_tx(self, options):\n    if options.grafting_type == grafting.GraftingType.SGD:\n      return grafting._sgd()\n    if options.grafting_type == grafting.GraftingType.RMSPROP:\n      return grafting._rmsprop(options)\n    if options.grafting_type == grafting.GraftingType.ADAFACTOR:\n      return grafting._adafactor(options)\n    raise ValueError('unsupported grafting type ' + str(options.grafting_type))\n\n  @parameterized.parameters(\n      itertools.product(\n          [0, 1, 2], ['sgd', 'rmsprop', 'adafactor'], [(3,), (3, 2)]\n      )\n  )\n  def test_norm_direction(self, step, graft, shape):\n    \"\"\"Validate initial graft update, then switch to its norm.\"\"\"\n    options = grafting.Options(\n        grafting.GraftingType(graft),\n        0.9 if (graft == 'rmsprop' or graft == 'adafactor') else 0.0,\n        start_preconditioning_step=step,\n        skip_preconditioning_rank1=len(shape) > 1,\n        min_dim_size_to_factor=1\n    )\n    grafted = grafting.graft(options, _minustwo())\n    nsteps = 4\n    norm_tx = self._norm_tx(options)\n    self._check_norm_direction(\n        norm_tx, _minustwo(), grafted, nsteps, step, shape\n    )\n\n  @parameterized.parameters({'shape': s} for s in [tuple(), (3,), (5,), (5, 2)])\n  def test_skip(self, shape):\n    \"\"\"Make sure we skip preconditioning if out-of-bounds.\"\"\"\n    options = grafting.Options(\n        start_preconditioning_step=2,\n        skip_preconditioning_any_dim_gt=4,\n        skip_preconditioning_rank1=True,\n    )\n    grafted = grafting.graft(options, _minustwo())\n    nsteps = 4\n    norm_tx = self._norm_tx(options)\n    self._check_equal(norm_tx, grafted, nsteps, shape)\n\n  @parameterized.named_parameters(_make_invalid_cases())\n  def test_invalid(self, invalid_options):\n    with self.assertRaises(ValueError):\n      grafting.graft(invalid_options, _minustwo())", "\n\nif __name__ == '__main__':\n  absltest.main()\n"]}
{"filename": "precondition/tearfree/grafting.py", "chunked_list": ["# Copyright 2023 The precondition Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Grafting norm adjustment (https://openreview.net/forum?id=FpKgG31Z_i9).\"\"\"\n\nimport copy\nimport dataclasses", "import copy\nimport dataclasses\nimport enum\nimport functools\nfrom typing import Any, NamedTuple\n\nimport chex\nfrom flax import struct\nimport jax\nfrom jax import numpy as jnp", "import jax\nfrom jax import numpy as jnp\nimport optax\nfrom precondition.tearfree import praxis_shim\n\n\n@enum.unique\nclass GraftingType(enum.Enum):\n  \"\"\"Different grafting types.\"\"\"\n\n  NONE = 'none'\n  SGD = 'sgd'\n  RMSPROP = 'rmsprop'\n  ADAFACTOR = 'adafactor'", "\n\n@dataclasses.dataclass\nclass Options:\n  \"\"\"Grafting configuration to change norms for updates.\n\n  A grafting update is computed as if it was running alongside the\n  tearfree optimizer. Its norm is used for updates. During the initial\n  few steps before preconditioning is applied, the grafting update\n  is used entirely.\n\n  Note that the grafting optimizer is ignorant of both weight decay,\n  learning rate, and the momentum.\n\n  Attributes:\n    grafting_type: Which optimizer to use for grafting updates.\n    second_moment_decay: Second moment accumulator decay. For ADA-Factor, this\n    value must be bounded between (0, 1). For RMSProp, the second moment\n    accumulator becomes sum if set to 1 (i.e., Adagrad), should be in (0, 1].\n    Must be 0 if unused (e.g., for SGD/NONE).\n    start_preconditioning_step: When to start applying preconditioning.\n    epsilon: Avoids divide by zero in RMSProp and ADA-Factor by adding this term\n      to the expression `(epsilon + acc)^(-1/2)` when taking the inverse square\n      root of the accumulator; should be non-negative.\n    skip_preconditioning_any_dim_gt: Skip second-order preconditioning if any\n      dimension of a tensor is greater than this value (only apply grafting\n      update). Argument ignored if NONE grafting.\n    skip_preconditioning_rank1: Skip preconditioning the tensor if the rank is 1\n      or less. Argument ignored if NONE grafting.\n    min_dim_size_to_factor: (Applies to ADA-Factor Only.) Only factor the\n      statistics if two array dimensions have at least this size.\n    multiply_by_parameter_scale: (Applies to ADA-Factor Only.) If True, then\n      scale learning_rate by parameter norm. If False, provided learning_rate is\n      absolute step size.\n    clipping_threshold: (Applies to ADA-Factor Only.) Clipping\n      threshold. Must be >= 1.\n  \"\"\"\n\n  grafting_type: GraftingType = GraftingType.RMSPROP\n  second_moment_decay: float = 0.999\n  start_preconditioning_step: int = 0\n  epsilon: float = 1e-23\n  skip_preconditioning_any_dim_gt: int = 4096\n  skip_preconditioning_rank1: bool = True\n  min_dim_size_to_factor: int = 128\n  multiply_by_parameter_scale: float = True\n  clipping_threshold: float = 1.0", "\n\ndef graft(\n    options: Options,\n    direction: praxis_shim.ShardedGradientTransformation,\n) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Generate the grafting update from options and direction update.\n\n  Args:\n    options: The grafting options.\n    direction: A sharded gradient transformation which determines the direction\n      of the update (grafting, if applied, changes the norm of this update).\n\n  Returns:\n    The wrapped transformation which applies either the grafting update\n    directly or the `direction` update with grafting norm, depending on the\n    current step or whether to apply grafting/preconditioning at all.\n  \"\"\"\n  _validate(options)\n\n  if options.grafting_type == GraftingType.NONE:\n    return direction\n\n  if options.grafting_type == GraftingType.SGD:\n    return _graft_with(direction, _sgd(), options)\n\n  if options.grafting_type == GraftingType.RMSPROP:\n    return _graft_with(\n        direction,\n        _rmsprop(options),\n        options,\n    )\n\n  if options.grafting_type == GraftingType.ADAFACTOR:\n    return _graft_with(\n        direction,\n        _adafactor(options),\n        options,\n    )\n  # check options for validity (SGD/none and no 2nd moment, appropriate range)\n  # test to check sharded gradient transform is otherwise identical to\n  #   praxis'\n  raise NotImplementedError", "\n\ndef _validate(options: Options):\n  \"\"\"Raise ValueError if the options have an invalid specification.\"\"\"\n  if options.grafting_type in [GraftingType.RMSPROP, GraftingType.ADAFACTOR]:\n    if options.epsilon < 0:\n      raise ValueError(\n          'epsilon ({}) should be non-negative'.format(options.epsilon)\n      )\n  if options.grafting_type == GraftingType.RMSPROP:\n    if not (0 < options.second_moment_decay <= 1.0):\n      raise ValueError(\n          'second_moment_decay ({}) not in (0, 1] for graft ({})'.format(\n              options.second_moment_decay, options.grafting_type\n          )\n      )\n  if options.grafting_type == GraftingType.ADAFACTOR:\n    if not (0 < options.second_moment_decay < 1.0):\n      raise ValueError(\n          'second_moment_decay ({}) not in (0, 1) for graft ({})'.format(\n              options.second_moment_decay, options.grafting_type\n          )\n      )\n    if not (0 < options.min_dim_size_to_factor):\n      raise ValueError(\n          'min_dim_size_to_factor ({}) should be positive for graft ({})'\n          .format(options.min_dim_size_to_factor, options.grafting_type)\n      )\n    if (options.clipping_threshold < 1):\n      raise ValueError(\n          'clipping_threshold ({}) should be >= 1 for graft ({})'\n          .format(options.clipping_threshold, options.grafting_type)\n      )", "\n\ndef _sgd() -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Create SGD sharded gradient transform.\"\"\"\n  grad_transform = optax.identity()\n  return praxis_shim.ShardedGradientTransformation(\n      grad_transform.init,\n      grad_transform.update,\n      optax.EmptyState,\n  )", "\n\ndef _adafactor(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Create AdaFactor sharded gradient transform.\"\"\"\n  tx = [optax.adafactor(\n      min_dim_size_to_factor=options.min_dim_size_to_factor,\n      decay_rate=options.second_moment_decay,\n      multiply_by_parameter_scale=options.multiply_by_parameter_scale,\n      eps=options.epsilon, clipping_threshold=options.clipping_threshold)]\n  # Sign flip: optax.adafactor uses descent direction in updates.\n  tx.append(optax.scale(-1))\n  grad_transform = optax.chain(*tx)\n\n  def _adafactor_pspec_fn(params_unused):\n    del params_unused\n    raise NotImplementedError\n\n  return praxis_shim.ShardedGradientTransformation(\n      grad_transform.init,\n      grad_transform.update,\n      _adafactor_pspec_fn,\n  )", "\n\n# Dummy wrapper for better state pretty printing, to identify what parameters\n# are for.\nclass RMSPropAccumulator(NamedTuple):\n  \"\"\"State holding the sum/ema of gradient squares so far.\"\"\"\n\n  acc: optax.Updates\n\n\ndef _rmsprop(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Create RMSProp sharded gradient transform.\"\"\"\n\n  def init_fn(params):\n    acc = jax.tree_map(jnp.zeros_like, params)\n    return RMSPropAccumulator(acc=acc)\n\n  def update_fn(updates, state, params=None):\n    del params\n\n    def ema(prev, new):\n      second_moment_decay = options.second_moment_decay\n      snew = jnp.square(new)\n      if second_moment_decay == 1.0:\n        return snew + prev\n      else:\n        return snew * (1 - second_moment_decay) + second_moment_decay * prev\n\n    new_state = RMSPropAccumulator(jax.tree_map(ema, state.acc, updates))\n    epsilon = options.epsilon\n    new_updates = jax.tree_map(\n        lambda g, acc: g * jax.lax.rsqrt(acc + epsilon), updates, new_state.acc\n    )\n    return new_updates, new_state\n\n  def init_partition_spec_fn(mdl_params):\n    def _opt_state_sharding_spec(var_hparams):\n      s_var_hparams = copy.deepcopy(var_hparams)\n      s_var_hparams.init = None\n      return s_var_hparams\n\n    mdl_sharding = jax.tree_map(_opt_state_sharding_spec, mdl_params)\n    return RMSPropAccumulator(acc=mdl_sharding)\n\n  return praxis_shim.ShardedGradientTransformation(\n      init=init_fn, update=update_fn, init_partition_spec=init_partition_spec_fn\n  )", "\n\ndef _rmsprop(options: Options) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Create RMSProp sharded gradient transform.\"\"\"\n\n  def init_fn(params):\n    acc = jax.tree_map(jnp.zeros_like, params)\n    return RMSPropAccumulator(acc=acc)\n\n  def update_fn(updates, state, params=None):\n    del params\n\n    def ema(prev, new):\n      second_moment_decay = options.second_moment_decay\n      snew = jnp.square(new)\n      if second_moment_decay == 1.0:\n        return snew + prev\n      else:\n        return snew * (1 - second_moment_decay) + second_moment_decay * prev\n\n    new_state = RMSPropAccumulator(jax.tree_map(ema, state.acc, updates))\n    epsilon = options.epsilon\n    new_updates = jax.tree_map(\n        lambda g, acc: g * jax.lax.rsqrt(acc + epsilon), updates, new_state.acc\n    )\n    return new_updates, new_state\n\n  def init_partition_spec_fn(mdl_params):\n    def _opt_state_sharding_spec(var_hparams):\n      s_var_hparams = copy.deepcopy(var_hparams)\n      s_var_hparams.init = None\n      return s_var_hparams\n\n    mdl_sharding = jax.tree_map(_opt_state_sharding_spec, mdl_params)\n    return RMSPropAccumulator(acc=mdl_sharding)\n\n  return praxis_shim.ShardedGradientTransformation(\n      init=init_fn, update=update_fn, init_partition_spec=init_partition_spec_fn\n  )", "\n\nclass GraftingState(NamedTuple):\n  \"\"\"State holding the count for grafting.\"\"\"\n\n  count: jax.Array\n  direction: optax.OptState\n  norm: optax.OptState\n\n\ndef _graft_with(\n    direction: praxis_shim.ShardedGradientTransformation,\n    norm: praxis_shim.ShardedGradientTransformation,\n    options: Options,\n) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Created a maybe-grafted update from a base update and a graft one.\"\"\"\n\n  start_preconditioning_step = options.start_preconditioning_step\n  mask = functools.partial(_mask_skipped, options)\n\n  def init_fn(params):\n    return GraftingState(\n        count=jnp.zeros([], jnp.int32),\n        direction=direction.init(mask(params)),\n        norm=norm.init(params),\n    )\n\n  def update_fn(updates, state, params=None):\n    base_updates, base_state = direction.update(\n        mask(updates), state.direction, mask(params)\n    )\n    graft_updates, graft_state = norm.update(updates, state.norm, params)\n    new_state = GraftingState(\n        count=state.count + 1,\n        direction=base_state,\n        norm=graft_state,\n    )\n\n    def maybe_graft(graft_upd, base):\n      if _masked(base):\n        return graft_upd\n      assert graft_upd.shape == base.shape\n\n      base_norm = jnp.linalg.norm(base)\n      multiplier = jnp.where(\n          base_norm > 0.0, jnp.linalg.norm(graft_upd) / base_norm, 0.0\n      )\n      return jnp.where(\n          state.count >= start_preconditioning_step,\n          base * multiplier,\n          graft_upd,\n      )\n\n    new_updates = jax.tree_map(\n        maybe_graft, graft_updates, base_updates, is_leaf=_masked\n    )\n    return new_updates, new_state\n\n  def init_partition_spec_fn(mdl_params):\n    count_pspec = praxis_shim.WeightHParams(\n        shape=[],\n        init=None,\n        dtype=jnp.int32,\n        collections=None,\n        tensor_split_dims_mapping=[],\n    )\n    return dict(\n        count=count_pspec,\n        direction=direction.init_partition_spec(mdl_params),\n        norm=norm.init_partition_spec(mdl_params),\n    )\n\n  return praxis_shim.ShardedGradientTransformation(\n      init_fn,\n      update_fn,\n      init_partition_spec_fn,\n  )", "\n\ndef _graft_with(\n    direction: praxis_shim.ShardedGradientTransformation,\n    norm: praxis_shim.ShardedGradientTransformation,\n    options: Options,\n) -> praxis_shim.ShardedGradientTransformation:\n  \"\"\"Created a maybe-grafted update from a base update and a graft one.\"\"\"\n\n  start_preconditioning_step = options.start_preconditioning_step\n  mask = functools.partial(_mask_skipped, options)\n\n  def init_fn(params):\n    return GraftingState(\n        count=jnp.zeros([], jnp.int32),\n        direction=direction.init(mask(params)),\n        norm=norm.init(params),\n    )\n\n  def update_fn(updates, state, params=None):\n    base_updates, base_state = direction.update(\n        mask(updates), state.direction, mask(params)\n    )\n    graft_updates, graft_state = norm.update(updates, state.norm, params)\n    new_state = GraftingState(\n        count=state.count + 1,\n        direction=base_state,\n        norm=graft_state,\n    )\n\n    def maybe_graft(graft_upd, base):\n      if _masked(base):\n        return graft_upd\n      assert graft_upd.shape == base.shape\n\n      base_norm = jnp.linalg.norm(base)\n      multiplier = jnp.where(\n          base_norm > 0.0, jnp.linalg.norm(graft_upd) / base_norm, 0.0\n      )\n      return jnp.where(\n          state.count >= start_preconditioning_step,\n          base * multiplier,\n          graft_upd,\n      )\n\n    new_updates = jax.tree_map(\n        maybe_graft, graft_updates, base_updates, is_leaf=_masked\n    )\n    return new_updates, new_state\n\n  def init_partition_spec_fn(mdl_params):\n    count_pspec = praxis_shim.WeightHParams(\n        shape=[],\n        init=None,\n        dtype=jnp.int32,\n        collections=None,\n        tensor_split_dims_mapping=[],\n    )\n    return dict(\n        count=count_pspec,\n        direction=direction.init_partition_spec(mdl_params),\n        norm=norm.init_partition_spec(mdl_params),\n    )\n\n  return praxis_shim.ShardedGradientTransformation(\n      init_fn,\n      update_fn,\n      init_partition_spec_fn,\n  )", "\n\n@struct.dataclass\nclass _GraftMask:\n  \"\"\"Helper tuple which masks out params before preconditioning.\"\"\"\n\n  pass\n\n\ndef _mask_skipped(options: Options, tree: chex.ArrayTree) -> chex.ArrayTree:\n  \"\"\"Masks out arrays to which preconditioning should not be applied.\"\"\"\n\n  def _maybe_mask(x: jax.Array):\n    if options.skip_preconditioning_rank1 and x.ndim <= 1:\n      return _GraftMask()\n    if any(s > options.skip_preconditioning_any_dim_gt for s in x.shape):\n      return _GraftMask()\n    return x\n\n  return jax.tree_map(_maybe_mask, tree)", "\ndef _mask_skipped(options: Options, tree: chex.ArrayTree) -> chex.ArrayTree:\n  \"\"\"Masks out arrays to which preconditioning should not be applied.\"\"\"\n\n  def _maybe_mask(x: jax.Array):\n    if options.skip_preconditioning_rank1 and x.ndim <= 1:\n      return _GraftMask()\n    if any(s > options.skip_preconditioning_any_dim_gt for s in x.shape):\n      return _GraftMask()\n    return x\n\n  return jax.tree_map(_maybe_mask, tree)", "\n\ndef _masked(tree_node: Any) -> bool:\n  \"\"\"Returns whether a tree node has been masked out.\"\"\"\n  return isinstance(tree_node, _GraftMask)\n"]}
