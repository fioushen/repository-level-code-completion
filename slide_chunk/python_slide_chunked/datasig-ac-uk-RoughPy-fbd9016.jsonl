{"filename": "setup.py", "chunked_list": ["import io\nimport os\nimport fnmatch\nimport platform\nimport sys\nimport re\nimport sysconfig\nimport importlib.metadata as ilm\n\n", "\n\nfrom skbuild import setup\nfrom pathlib import Path\nfrom setuptools import find_packages\n\n\nPROJECT_ROOT = Path(__file__).parent.absolute()\n\n", "\n\nREADME_PATH = PROJECT_ROOT / \"README.md\"\nCHANGELOG_PATH = PROJECT_ROOT / \"CHANGELOG\"\n\nDESCRIPTION = README_PATH.read_text()\nDESCRIPTION += \"\\n\\n\\n## Changelog\\n\"\nDESCRIPTION += CHANGELOG_PATH.read_text()\n\nVERSION = \"0.0.1\"", "\nVERSION = \"0.0.1\"\n\nif \"VCPKG_INSTALLATION_ROOT\" in os.environ:\n    vcpkg = Path(os.environ[\"VCPKG_INSTALLATION_ROOT\"], \"scripts\", \"buildsystems\", \"vcpkg.cmake\").resolve()\nelse:\n    if not Path(\"vcpkg\").exists():\n        import subprocess as sp\n        sp.run([\"git\", \"clone\", \"https://github.com/Microsoft/vcpkg.git\"])\n    bootstrap_end = \"bat\" if platform.system() == \"Windows\" else \"sh\"\n    sp.run([f\"vcpkg/bootstrap-vcpkg.{bootstrap_end}\"], shell=True, check=True)\n    vcpkg = Path(\"vcpkg\", \"scripts\", \"buildsystems\", \"vcpkg.cmake\").resolve()", "\nprefix_path = []\nif \"CMAKE_PREFIX_PATH\" in os.environ:\n    prefix_path.extend(os.environ[\"CMAKE_PREFIX_PATH\"].split(os.pathsep))\n\n\n\n\n\nCMAKE_SETTINGS = [", "\nCMAKE_SETTINGS = [\n    \"-DROUGHPY_BUILD_TESTS:BOOL=OFF\",\n    \"-DROUGHPY_BUILD_LA_CONTEXTS:BOOL=OFF\",  # Temporarily\n    \"-DROUGHPY_GENERATE_DEVICE_CODE:BOOL=OFF\",  # Until it's finished\n    f\"-DCMAKE_TOOLCHAIN_FILE={vcpkg}\",\n    \"-DVCPKG_BUILD_TYPE=release\",\n    # \"--debug-find\", \"--debug-output\"\n]\n\nif platform.system() == \"MacOs\" and \"CMAKE_OMP_ROOT\" in os.environ:\n    CMAKE_SETTINGS.extend([\n        f\"-DCMAKE_LIBRARY_PATH={os.environ['CMAKE_OMP_ROOT']}/lib\",\n        f\"-DCMAKE_INCLUDE_PATH={os.environ['CMAKE_OMP_ROOT']}/include\",\n    ])", "]\n\nif platform.system() == \"MacOs\" and \"CMAKE_OMP_ROOT\" in os.environ:\n    CMAKE_SETTINGS.extend([\n        f\"-DCMAKE_LIBRARY_PATH={os.environ['CMAKE_OMP_ROOT']}/lib\",\n        f\"-DCMAKE_INCLUDE_PATH={os.environ['CMAKE_OMP_ROOT']}/include\",\n    ])\n\n\ntry:\n    mkl = ilm.distribution(\"mkl-devel\")\n\n    # locate the cmake folder\n    cmake_files = [f for f in mkl.files if f.name.endswith(\"cmake\")]\n    # should be {root}/lib/cmake/mkl/{f}\n    cmake = cmake_files[0].locate().resolve().parent.parent\n    # append {root} to prefix_path\n    prefix_path.append(str(cmake.parent.parent))\n    CMAKE_SETTINGS.append(f\"-DMKL_DIR={cmake}\")\n\nexcept ilm.PackageNotFoundError:\n    # pass\n    raise", "\ntry:\n    mkl = ilm.distribution(\"mkl-devel\")\n\n    # locate the cmake folder\n    cmake_files = [f for f in mkl.files if f.name.endswith(\"cmake\")]\n    # should be {root}/lib/cmake/mkl/{f}\n    cmake = cmake_files[0].locate().resolve().parent.parent\n    # append {root} to prefix_path\n    prefix_path.append(str(cmake.parent.parent))\n    CMAKE_SETTINGS.append(f\"-DMKL_DIR={cmake}\")\n\nexcept ilm.PackageNotFoundError:\n    # pass\n    raise", "\nCMAKE_SETTINGS.append(\n    f\"-DCMAKE_PREFIX_PATH={os.pathsep.join(prefix_path)}\"\n)\nos.environ[\"CMAKE_PREFIX_PATH\"] = os.pathsep.join(prefix_path)\n\ndef filter_cmake_manifests(items):\n    def _filter(item):\n        item = str(item)\n        if item.endswith(\".pc\"):\n            return False\n\n        if item.endswith(\".cmake\"):\n            return False\n\n        if item.endswith(\".cpp\"):\n            return False\n\n        if item.endswith(\".h\"):\n            return False\n\n        if item.endswith(\".a\"):\n            return False\n\n        # m = re.search(r\"[a-zA-Z0-9_]+\\.so(?:\\.\\d+\\.\\d+\\.\\d+)?$\", item)\n        # if m is not None:\n        #     return False\n\n        if item.endswith(\"recombine.so\") or item.endswith(\"recombine.so.2.0.2\"):\n            return False\n\n        return True\n\n\n    manifest = list(filter(_filter, items))\n    return manifest", "\n\nsetup(\n    name=\"RoughPy\",\n    version=VERSION,\n    author=\"The RoughPy Authors\",\n    author_email=\"info@datasig.ac.uk\",\n    license=\"BSD-3-Clause\",\n    keywords=[\"data\", \"streams\", \"rough paths\", \"signatures\"],\n", "    keywords=[\"data\", \"streams\", \"rough paths\", \"signatures\"],\n\n    long_description=DESCRIPTION,\n    long_description_content_type=\"text/markdown\",\n\n    include_package_data=True,\n    packages=[\"roughpy\"],\n    package_data={\n        \"roughpy\": [\"py.typed\"]\n    },", "        \"roughpy\": [\"py.typed\"]\n    },\n\n    cmake_process_manifest_hook=filter_cmake_manifests,\n    cmake_args=CMAKE_SETTINGS,\n\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Developers\",", "        \"Intended Audience :: Science/Research\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"License :: OSI Approved :: BSD License\",\n\n        \"Programming Language :: C++\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",", "        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n\n        \"Operating System :: Microsoft :: Windows\",\n        \"Operating System :: POSIX\",\n        \"Operating System :: Unix\",\n        \"Operating System :: MacOS\"\n    ]\n", "    ]\n\n)\n"]}
{"filename": "tools/version_from_file.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import annotations\n\nimport re\nfrom pathlib import Path\n\n__all__ = [\"dynamic_metadata\"]", "\n__all__ = [\"dynamic_metadata\"]\n\n\ndef __dir__() -> list[str]:\n    return __all__\n\n\ndef dynamic_metadata(\n        fields: frozenset[str],\n        settings: dict[str, object] | None\n) -> dict[str, str | dict[str, str | None]]:\n    if \"version\" not in fields:\n        raise ValueError(\"This plugin gets the version\")\n\n    print(fields, settings)\n\n    if \"regex\" in settings:\n        regex = settings[\"regex\"]\n    else:\n        regex = r\"(?P<version>\\d+\\.\\d+\\.\\d+)\"\n\n    version_path = Path(\"VERSION.txt\")\n    if version_path.exists():\n        version_text = version_path.read_text().strip()\n\n        if (match := re.match(regex, version_text)) is not None:\n            version = match.group(\"version\")\n        else:\n            raise ValueError(\n                \"Could not get version from string \" + version_text)\n    else:\n        version = \"0.0.1\"\n\n    return version", "def dynamic_metadata(\n        fields: frozenset[str],\n        settings: dict[str, object] | None\n) -> dict[str, str | dict[str, str | None]]:\n    if \"version\" not in fields:\n        raise ValueError(\"This plugin gets the version\")\n\n    print(fields, settings)\n\n    if \"regex\" in settings:\n        regex = settings[\"regex\"]\n    else:\n        regex = r\"(?P<version>\\d+\\.\\d+\\.\\d+)\"\n\n    version_path = Path(\"VERSION.txt\")\n    if version_path.exists():\n        version_text = version_path.read_text().strip()\n\n        if (match := re.match(regex, version_text)) is not None:\n            version = match.group(\"version\")\n        else:\n            raise ValueError(\n                \"Could not get version from string \" + version_text)\n    else:\n        version = \"0.0.1\"\n\n    return version", ""]}
{"filename": "tools/python-get-binary-obj-path.py", "chunked_list": ["#  Copyright (c) 2023 Datasig Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import annotations\n\nimport importlib.metadata as ilm\nfrom argparse import ArgumentParser\nfrom pathlib import Path\nfrom typing import Callable, Generator, Iterable", "from pathlib import Path\nfrom typing import Callable, Generator, Iterable\n\nCMAKE_LIST_SEP = ';'\n\nMatcher = Callable[[ilm.PackagePath], bool]\n\n\ndef find_component(package: str, matcher: Matcher) -> Generator[Path]:\n    try:\n        dist = ilm.distribution(package)\n        yield from (f.locate().resolve() for f in dist.files\n                    if matcher(f))\n    except ilm.PackageNotFoundError:\n        pass", "def find_component(package: str, matcher: Matcher) -> Generator[Path]:\n    try:\n        dist = ilm.distribution(package)\n        yield from (f.locate().resolve() for f in dist.files\n                    if matcher(f))\n    except ilm.PackageNotFoundError:\n        pass\n\n\ndef _flatten(gens: Iterable[Generator[Path]]) -> Generator[Path]:\n    for gen in gens:\n        yield from gen", "\ndef _flatten(gens: Iterable[Generator[Path]]) -> Generator[Path]:\n    for gen in gens:\n        yield from gen\n\n\ndef _trim_to_path_dir(path: Path, fragment: Path) -> Path:\n    prefix = path\n    for l, _ in zip(path.parents, fragment.parents):\n        prefix = l\n\n    return prefix", "\n\ndef _trim_to_directory(paths: Generator[Path], pat: Path) -> Generator[Path]:\n    for p in paths:\n        yield _trim_to_path_dir(p, pat)\n\n\ndef make_name_matcher(name: str) -> Matcher:\n    def matcher(path: ilm.PackagePath) -> bool:\n        return path.stem == name\n\n    return matcher", "\n\ndef make_fragment_matcher(fragment: str) -> Matcher:\n    def matcher(path: ilm.PackagePath) -> bool:\n        return path.match(fragment)\n\n    return matcher\n\n\ndef make_matcher(pattern: str, match_name: bool) -> Matcher:\n    if match_name:\n        return make_name_matcher(pattern)\n\n    return make_fragment_matcher(pattern)", "\ndef make_matcher(pattern: str, match_name: bool) -> Matcher:\n    if match_name:\n        return make_name_matcher(pattern)\n\n    return make_fragment_matcher(pattern)\n\n\ndef trim_count(count: int, paths: Generator[Path]) -> Generator[Path]:\n    for _, p in zip(range(count), paths):\n        yield p", "def trim_count(count: int, paths: Generator[Path]) -> Generator[Path]:\n    for _, p in zip(range(count), paths):\n        yield p\n\n\ndef main():\n    parser = ArgumentParser()\n\n    parser.add_argument(\"package\", type=str)\n    parser.add_argument(\"pattern\", nargs=\"+\")\n    parser.add_argument(\n        \"-e\", \"--exitcode\",\n        action=\"store_true\",\n        help=\"Return exit code on failure\"\n    )\n    parser.add_argument(\n        \"-n\", \"--name\",\n        action=\"store_true\",\n        help=\"Find by file name\"\n    )\n    parser.add_argument(\n        \"-d\", \"--directory\",\n        action=\"store_true\",\n        help=\"Find the containing directory\"\n    )\n    parser.add_argument(\n        \"-c\", \"--count\",\n        type=int,\n        default=0,\n        nargs=1,\n        required=False,\n        help=\"Limit the number of entries returned\"\n    )\n\n    args = parser.parse_args()\n\n    paths = []\n\n    for pat in args.pattern:\n        matcher = make_matcher(pat, args.name)\n        found = find_component(args.package, matcher)\n        if not found and args.exitcode:\n            parser.exit(1)\n\n        if args.count > 0:\n            found = trim_count(args.count, found)\n\n        if args.directory:\n            found = _trim_to_directory(found, Path(pat))\n\n        paths.extend(found)\n\n    if not paths and args.exitcode:\n        parser.exit(1)\n\n    print(CMAKE_LIST_SEP.join(map(str, paths)))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\nfrom numpy.random import default_rng\n\nfrom roughpy import _roughpy\n\n\n@pytest.fixture(params=range(2, 6))\ndef width(request):\n    return request.param\n", "\n\n@pytest.fixture(params=range(2, 6))\ndef depth(request):\n    return request.param\n\n\n@pytest.fixture\ndef rng():\n    return default_rng(12345)", "def rng():\n    return default_rng(12345)\n\n\n@pytest.fixture\ndef tensor_size(width, depth):\n    s = depth\n    r = 1\n    while s:\n        r *= width\n        r += 1\n        s -= 1\n    return r", "\n\n@pytest.fixture(params=[_roughpy.DenseVector, _roughpy.SparseVector])\ndef vec_type(request):\n    return request.param\n\n\n@pytest.fixture(params=[_roughpy.DPReal, _roughpy.SPReal])\ndef coeff_type(request):\n    return request.param", "def coeff_type(request):\n    return request.param\n"]}
{"filename": "tests/streams/test_tick_stream.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport pytest\n\nfrom roughpy import DPReal, Lie, RealInterval, TickStream\n", "from roughpy import DPReal, Lie, RealInterval, TickStream\n\nDATA_FORMATS = [\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            (\"second\", \"increment\", 1.0)\n        ]\n    },\n    {", "    },\n    {\n        1.0: {\n            \"first\": (\"increment\", 1.0),\n            \"second\": (\"increment\", 1.0)\n        }\n    },\n    {\n        1.0: [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},", "        1.0: [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},\n            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ]\n    },\n    {\n        1.0: {\n            \"first\": {\"type\": \"increment\", \"data\": 1.0},\n            \"second\": {\"type\": \"increment\", \"data\": 1.0},\n        }", "            \"second\": {\"type\": \"increment\", \"data\": 1.0},\n        }\n    },\n    [\n        (1.0, \"first\", \"increment\", 1.0),\n        (1.0, \"second\", \"increment\", 1.0),\n    ],\n    [\n        (1.0, (\"first\", \"increment\", 1.0)),\n        (1.0, (\"second\", \"increment\", 1.0)),", "        (1.0, (\"first\", \"increment\", 1.0)),\n        (1.0, (\"second\", \"increment\", 1.0)),\n    ],\n    [\n        (1.0, {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0}),\n        (1.0, {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0}),\n    ],\n    [\n        (1.0, [\n            (\"first\", \"increment\", 1.0),", "        (1.0, [\n            (\"first\", \"increment\", 1.0),\n            (\"second\", \"increment\", 1.0),\n        ])\n    ],\n    [\n        (1.0, [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},\n            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ])", "            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ])\n    ],\n    [\n        (1.0, {\n            \"first\": (\"increment\", 1.0),\n            \"second\": (\"increment\", 1.0),\n        })\n    ],\n    {", "    ],\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"label\": \"second\",\n                \"type\": \"increment\",\n                \"data\": 1.0\n            }\n        ]", "            }\n        ]\n    },\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"second\": (\"increment\", 1.0)\n            }\n        ]", "            }\n        ]\n    },\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"second\": {\n                    \"type\": \"increment\",\n                    \"data\": 1.0", "                    \"type\": \"increment\",\n                    \"data\": 1.0\n                }\n            }\n        ]\n    },\n    {\n        1.0: {\n            \"first\": (\"increment\", 1.0),\n            \"second\": {", "            \"first\": (\"increment\", 1.0),\n            \"second\": {\n                \"type\": \"increment\",\n                \"data\": 1.0\n            }\n        }\n    }\n\n]\n", "]\n\n\n@pytest.mark.parametrize(\"data\", DATA_FORMATS)\ndef test_construct_tick_path_from_data(data):\n    stream = TickStream.from_data(data, width=2, depth=2, dtype=DPReal)\n\n    assert stream.width == 2\n    lsig = stream.log_signature(RealInterval(0.0, 2.0), 2)\n\n    expected = Lie([1.0, 1.0, 0.5], width=2, depth=2, dtype=DPReal)\n    assert lsig == expected, f\"{lsig} == {expected}\"", "\n\n\n\ndef test_construct_tick_stream_with_time(rng):\n    data = DATA_FORMATS[0]\n\n    stream = TickStream.from_data(data, width=2, depth=2, include_time=True,\n                                  dtype=DPReal)\n\n    assert stream.width == 3", ""]}
{"filename": "tests/streams/test_lie_increment_path.py", "chunked_list": ["import functools\nimport itertools\nimport operator\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal\n\nimport roughpy\nfrom roughpy import FreeTensor, Lie, RealInterval", "import roughpy\nfrom roughpy import FreeTensor, Lie, RealInterval\nfrom roughpy import LieIncrementStream\n\n\ndef path(*args, **kwargs):\n    return LieIncrementStream.from_increments(*args, **kwargs)\n\n\n@pytest.fixture(params=[0, 1, 5, 10, 20, 50, 100])\ndef length(request):\n    return request.param", "\n@pytest.fixture(params=[0, 1, 5, 10, 20, 50, 100])\ndef length(request):\n    return request.param\n\n\n@pytest.fixture\ndef tick_indices(length):\n    return np.linspace(0.0, 1.0, length)\n", "\n\n@pytest.fixture\ndef tick_data(rng, length, width):\n    return rng.normal(0.0, 1.0, size=(length, width))\n\n\n# @pytest.fixture\n# def tick_data_w_indices(rng, tick_indices, width, length):\n#     if length == 0:", "# def tick_data_w_indices(rng, tick_indices, width, length):\n#     if length == 0:\n#         return np.array([[]])\n#\n#     return np.concatenate([\n#         tick_indices.reshape((length, 1)),\n#         rng.normal(0.0, 1.0, size=(length, width))\n#     ], axis=1)\n\n", "\n\n# def test_path_width_depth_with_data(tick_data_w_indices, width, depth):\n#     p = path(tick_data_w_indices, width=width, depth=depth)\n#\n#     assert p.width == width\n#     assert p.depth == depth\n#     assert isinstance(p.domain(), esig.real_interval)\n#     assert p.domain() == RealInterval(-float(\"inf\"), float(\"inf\"))\n", "#     assert p.domain() == RealInterval(-float(\"inf\"), float(\"inf\"))\n\n\n@pytest.mark.parametrize(\"data\",\n                         [np.array([]), np.array([[]]), np.array([[], []])])\ndef test_path_creation_odd_data(data):\n    p = path(data, width=2, depth=2)\n\n    assert p.width == 2\n", "\n\n# def test_path_creation_flat_length_1_path(width):\n#     width + 1 because we do not supply indices\n# data = np.array([1.0] * (width + 1))\n# p = path(data, width=width, depth=2)\n#\n# assert p.width == width\n# assert p.depth == 2\n# needs fix for broadcasting to the correct shape in function", "# assert p.depth == 2\n# needs fix for broadcasting to the correct shape in function\n# assert data.shape == (width+1,)\n\n\n# def test_path_creation_flat_length_1_path_no_depth(width):\n#     width + 1 because we do not supply indices\n# data = np.array([1.0] * (width + 1))\n# p = path(data, width=width)\n#", "# p = path(data, width=width)\n#\n# assert p.width == width\n# assert p.depth == 2\n# needs fix for broadcasting to the correct shape in function\n# assert data.shape == (width+1,)\n\n\n# @pytest.mark.skip(\"Broken?\")\n# def test_path_restrict_tick_path(tick_data_w_indices, width):", "# @pytest.mark.skip(\"Broken?\")\n# def test_path_restrict_tick_path(tick_data_w_indices, width):\n#     p = path(tick_data_w_indices, width=width, depth=2)\n#\n#     if p.domain() != RealInterval(0.0, 1.0):\n#         pytest.skip(\"Non-standard domain\")\n#\n#     p.restrict(RealInterval(0.0, 0.5))\n#\n#     assert p.domain() == RealInterval(0.0, 0.5)", "#\n#     assert p.domain() == RealInterval(0.0, 0.5)\n\n\ndef test_tick_path_signature_calc_accuracy():\n    data = np.array([[1.0, 2.5]])\n    p = path(data, width=2, depth=2, indices=np.array([0.7]))\n\n    r1 = p.signature(0.0, 0.8, 0.5)\n    expected1 = FreeTensor(np.array([1.0]), width=2, depth=2)\n    assert r1 == expected1, f\"expected {expected1} but was {r1}\"\n\n    r2 = p.signature(0.0, 0.8, 0.25)\n    expected2 = FreeTensor(np.array([0.0, 1.0, 2.5]), width=2, depth=2).exp()\n    assert r2 == expected2, f\"expected {expected2} but was {r2}\"", "\n\n# def test_tick_path_with_time(tick_data_w_indices, width):\n#     if not tick_data_w_indices.size:\n#         pytest.skip(\"empty array not valid data.\")\n#     p = path(tick_data_w_indices, depth=2, include_time=True)\n#\n#     assert p.width == width + 1\n\n", "\n\n# def test_tick_path_with_time_no_depth(tick_data_w_indices, width):\n#     if not tick_data_w_indices.size:\n#         pytest.skip(\"empty array not valid data.\")\n#     p = path(tick_data_w_indices, include_time=True)\n#\n#     assert p.width == width + 1\n#     assert p.depth == 2\n", "#     assert p.depth == 2\n\n\n@pytest.fixture(params=[2, 5, 10, 25, 100])\ndef t_values(request):\n    return np.arange(0.0, 2.0, 2.0 / request.param) + 2.0 / request.param\n\n\n@pytest.fixture\ndef known_path_data(t_values, width):\n    t = np.arange(1.0, width + 1) * (2.0 / len(t_values))\n    return np.tensordot(np.ones(len(t_values)), t, axes=0)", "@pytest.fixture\ndef known_path_data(t_values, width):\n    t = np.arange(1.0, width + 1) * (2.0 / len(t_values))\n    return np.tensordot(np.ones(len(t_values)), t, axes=0)\n\n\n@pytest.fixture\ndef solution_signature(width, depth, tensor_size):\n    letters = list(range(1, width + 1))\n\n    def sig_func(a, b):\n        rv = np.zeros(tensor_size)\n        rv[0] = 1.0\n\n        for let in letters:\n            rv[let] = let * (b - a)\n\n        idx = width\n        factor = 1.0\n        for d in range(2, depth + 1):\n            factor /= d\n\n            for data in itertools.product(letters, repeat=d):\n                idx += 1\n                rv[idx] = factor * functools.reduce(operator.mul, data, 1) * (\n                        b - a) ** d\n        return rv\n\n    return sig_func", "\n\ndef test_tpath_known_signature_calc(width, depth, t_values, known_path_data,\n                                    solution_signature):\n    p = path(known_path_data, indices=t_values, width=width, depth=depth)\n\n    expected = FreeTensor(solution_signature(0.0, 2.0), width=width,\n                          depth=depth)\n    assert_array_almost_equal(p.signature(0.0, 3.125), expected)\n", "\n\ndef test_tpath_known_signature_calc_with_context(width, depth, t_values,\n                                                 known_path_data,\n                                                 solution_signature):\n    p = path(known_path_data, indices=t_values, width=width, depth=2)\n\n    expected = FreeTensor(solution_signature(0.0, 2.0), width=width,\n                          depth=depth)\n    assert_array_almost_equal(\n        np.array(p.signature(0.0, 3.125, depth=depth)),\n        np.array(expected))", "\n\ndef test_tick_sig_deriv_width_3_depth_1_let_2_perturb():\n    p = path(np.array([[0.2, 0.4, 0.6]]), indices=np.array([0.0]), width=3,\n             depth=1)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), width=3, depth=1)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative(interval, perturbation, 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0]), width=3, depth=1)\n\n    assert d == expected, f\"expected {expected} but got {d}\"", "\n\ndef test_tick_sig_deriv_width_3_depth_2_let_2_perturb():\n    p = path(np.array([[0.2, 0.4, 0.6]]), indices=np.array([0.0]), width=3,\n             depth=2)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), width=3, depth=2)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative(interval, perturbation, 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0,\n                                    0.0, 0.1, 0.0,\n                                    0.1, 0.4, 0.3,\n                                    0.0, 0.3, 0.0\n                                    ]), width=3, depth=2)\n\n    assert d == expected, f\"expected {expected} but got {d}\"", "\n\ndef test_tick_sig_deriv_width_3_depth_2_let_2_perturb_with_context():\n    p = path(np.array([[0.2, 0.4, 0.6]]), indices=np.array([0.0]), width=3,\n             depth=2)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), width=3, depth=2)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative(interval, perturbation, 1, depth=2)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0,\n                                    0.0, 0.1, 0.0,\n                                    0.1, 0.4, 0.3,\n                                    0.0, 0.3, 0.0\n                                    ]), width=3, depth=2)\n\n    assert d == expected, f\"expected {expected} but got {d}\"", "\n\ndef test_tick_path_sig_derivative(width, depth, tick_data, tick_indices, rng):\n    p = path(tick_data, indices=tick_indices, width=width, depth=depth)\n\n    def lie():\n        return Lie(rng.uniform(0.0, 5.0, size=(width,)), width=width,\n                   depth=depth)\n\n    perturbations = [\n        (RealInterval(0.0, 0.3), lie()),\n        (RealInterval(0.3, 0.6), lie()),\n        (RealInterval(0.6, 1.1), lie()),\n    ]\n\n    result = p.signature_derivative(perturbations, 5)\n\n    expected = FreeTensor(np.array([0.0]), width=width, depth=depth)\n    for ivl, per in perturbations:\n        expected *= p.signature(ivl, 5)\n        expected += p.signature_derivative(ivl, per, 5)\n\n    # assert result == expected\n    assert_array_almost_equal(result, expected)", "\n\ndef test_lie_incr_stream_from_randints(rng):\n    array = rng.integers(0, 5, size=(4, 3))\n\n    stream = LieIncrementStream.from_increments(array, width=3, depth=2)\n\n    sig = stream.signature(RealInterval(0.0, 5.0), 2)\n\n    assert_array_equal(np.array(sig)[:4],\n                       np.hstack([[1.0], np.sum(array, axis=0)[:]]))", "\ndef test_lie_incr_stream_from_randints_no_deduction(rng):\n    array = rng.integers(0, 5, size=(4, 3))\n\n    stream = LieIncrementStream.from_increments(array, width=3, depth=2,\n                                                dtype=roughpy.DPReal)\n\n    sig = stream.signature(RealInterval(0.0, 5.0), 2)\n\n    assert_array_equal(np.array(sig)[:4],\n                       np.hstack([[1.0], np.sum(array, axis=0)[:]]))", "\ndef test_lie_incr_stream_from_randints_transposed(rng):\n    array = rng.integers(0, 5, size=(4, 3))\n\n    stream = LieIncrementStream.from_increments(array.T, width=3, depth=2)\n\n    sig = stream.signature(RealInterval(0.0, 5.0), 2)\n\n    assert_array_equal(np.array(sig)[:4],\n                       np.hstack([[1.0], np.sum(array, axis=0)[:]]))", "\ndef test_lie_incr_stream_from_randints_no_deduction_transposed(rng):\n    array = rng.integers(0, 5, size=(4, 3))\n\n    stream = LieIncrementStream.from_increments(array.T, width=3, depth=2,\n                                                dtype=roughpy.DPReal)\n\n    sig = stream.signature(RealInterval(0.0, 5.0), 2)\n\n    assert_array_equal(np.array(sig)[:4],\n                       np.hstack([[1.0], np.sum(array, axis=0)[:]]))", ""]}
{"filename": "tests/streams/__init__.py", "chunked_list": [""]}
{"filename": "tests/streams/test_schema.py", "chunked_list": ["#  Copyright (c) 2023 Datasig Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport pytest\n\nfrom roughpy import StreamSchema\n", "from roughpy import StreamSchema\n\n\n@pytest.fixture\ndef sample_data_dict():\n    return {\n        1.0: (\"first\", 1.0),\n        2.0: (\"first\", 2.0),\n        3.0: (\"second\", \"cat1\"),\n        4.0: (\"second\", \"cat2\"),\n        5.0: (\"third\", \"value\", 1.0),\n    }", "\n\n@pytest.fixture\ndef sample_data_seq(sample_data_dict):\n    return [(ts, *args) for ts, args in sample_data_dict.items()]\n\n\ndef test_schema_from_dict(sample_data_dict):\n    schema = StreamSchema.from_data(sample_data_dict)\n\n    assert schema.get_labels() == [\n        \"first\",\n        \"second:cat1\",\n        \"second:cat2\",\n        \"third\",\n    ]", "\n\ndef test_schema_from_seq(sample_data_seq):\n    schema = StreamSchema.from_data(sample_data_seq)\n\n    assert schema.get_labels() == [\n        \"first\",\n        \"second:cat1\",\n        \"second:cat2\",\n        \"third\",\n    ]", "\n\n@pytest.fixture\ndef json_like_schema():\n    return [\n        {\n            \"label\": \"first\",\n            \"type\": \"increment\"\n        },\n        {\n            \"label\": \"second\",\n            \"type\": \"value\",\n        },\n        {\n            \"label\": \"third\",\n            \"type\": \"categorical\",\n            \"categories\": [\"cat1\", \"cat2\"]\n        },\n    ]", "\n\ndef test_parse_jsonlike(json_like_schema):\n    schema = StreamSchema.parse(json_like_schema)\n\n    assert schema.get_labels() == [\n        \"first\",\n        # \"second:lead\",\n        # \"second:lag\",\n        \"second\",\n        \"third:cat1\",\n        \"third:cat2\",\n    ]", "\n\nDATA_FORMATS = [\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            (\"second\", \"increment\", 1.0)\n        ]\n    },\n    {", "    },\n    {\n        1.0: {\n            \"first\": (\"increment\", 1.0),\n            \"second\": (\"increment\", 1.0)\n        }\n    },\n    {\n        1.0: [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},", "        1.0: [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},\n            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ]\n    },\n    {\n        1.0: {\n            \"first\": {\"type\": \"increment\", \"data\": 1.0},\n            \"second\": {\"type\": \"increment\", \"data\": 1.0},\n        }", "            \"second\": {\"type\": \"increment\", \"data\": 1.0},\n        }\n    },\n    [\n        (1.0, \"first\", \"increment\", 1.0),\n        (1.0, \"second\", \"increment\", 1.0),\n    ],\n    [\n        (1.0, (\"first\", \"increment\", 1.0)),\n        (1.0, (\"second\", \"increment\", 1.0)),", "        (1.0, (\"first\", \"increment\", 1.0)),\n        (1.0, (\"second\", \"increment\", 1.0)),\n    ],\n    [\n        (1.0, {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0}),\n        (1.0, {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0}),\n    ],\n    [\n        (1.0, [\n            (\"first\", \"increment\", 1.0),", "        (1.0, [\n            (\"first\", \"increment\", 1.0),\n            (\"second\", \"increment\", 1.0),\n        ])\n    ],\n    [\n        (1.0, [\n            {\"label\": \"first\", \"type\": \"increment\", \"data\": 1.0},\n            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ])", "            {\"label\": \"second\", \"type\": \"increment\", \"data\": 1.0},\n        ])\n    ],\n    [\n        (1.0, {\n            \"first\": (\"increment\", 1.0),\n            \"second\": (\"increment\", 1.0),\n        })\n    ],\n    {", "    ],\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"label\": \"second\",\n                \"type\": \"increment\",\n                \"data\": 1.0\n            }\n        ]", "            }\n        ]\n    },\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"second\": (\"increment\", 1.0)\n            }\n        ]", "            }\n        ]\n    },\n    {\n        1.0: [\n            (\"first\", \"increment\", 1.0),\n            {\n                \"second\": {\n                    \"type\": \"increment\",\n                    \"data\": 1.0", "                    \"type\": \"increment\",\n                    \"data\": 1.0\n                }\n            }\n        ]\n    },\n    {\n        1.0: {\n            \"first\": (\"increment\", 1.0),\n            \"second\": {", "            \"first\": (\"increment\", 1.0),\n            \"second\": {\n                \"type\": \"increment\",\n                \"data\": 1.0\n            }\n        }\n    }\n\n]\n", "]\n\n\n@pytest.fixture(params=DATA_FORMATS)\ndef tick_data(request):\n    return request.param\n\n\ndef test_parse_schema_from_data(tick_data):\n    schema = StreamSchema.from_data(tick_data)\n\n    assert schema.width() == 2\n    assert schema.get_labels() == [\"first\", \"second\"]", "def test_parse_schema_from_data(tick_data):\n    schema = StreamSchema.from_data(tick_data)\n\n    assert schema.width() == 2\n    assert schema.get_labels() == [\"first\", \"second\"]\n\n\nSCHEMA_SPEC_WITH_OPTIONS = [\n    [\n        {", "    [\n        {\n            \"label\": \"first\",\n            \"type\": \"value\",\n            \"lead_lag\": True\n        },\n        {\n            \"label\": \"second\",\n            \"type\": \"value\",\n            \"lead_lag\": False", "            \"type\": \"value\",\n            \"lead_lag\": False\n        }\n    ],\n    {\n        \"first\": {\n            \"type\": \"value\",\n            \"lead_lag\": True\n        },\n        \"second\": {", "        },\n        \"second\": {\n            \"type\": \"value\",\n            \"lead_lag\": False\n        }\n    },\n    [\n        (\"first\", \"value\", {\"lead_lag\": True}),\n        (\"second\", \"value\", {\"lead_lag\": False}),\n    ]", "        (\"second\", \"value\", {\"lead_lag\": False}),\n    ]\n]\n\n\n@pytest.mark.parametrize(\"spec\", SCHEMA_SPEC_WITH_OPTIONS)\ndef test_schema_construction_with_options(spec):\n    schema = StreamSchema.parse(spec)\n\n    assert schema.width() == 3\n    assert schema.get_labels() == [\"first:lead\", \"first:lag\", \"second\"]", ""]}
{"filename": "tests/streams/test_sound_path.py", "chunked_list": ["#  Copyright (c) 2023 RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport os\n\nimport pytest\n", "import pytest\n\nimport roughpy\n\n\n@pytest.fixture(params=[\"test.flac\", \"test.mp3\", \"test.wav\"])\ndef sound_file(request):\n    return os.path.join(os.path.dirname(__file__), \"audio\", request.param)\n\n", "\n\n@pytest.fixture\ndef sound_stream(sound_file):\n    return roughpy.ExternalDataStream.from_uri(sound_file, depth=2)\n\n\ndef test_sound_stream_width_deduction(sound_stream):\n    assert sound_stream.width == 2\n", "\n\ndef test_sound_stream_ctype_deduction(sound_stream):\n    assert sound_stream.dtype == roughpy.DPReal\n\n\ndef test_sound_stream_support_deduction(sound_stream):\n    assert sound_stream.support.inf() == 0.0\n    assert sound_stream.support.sup() == pytest.approx(1.764, abs=0.0005)\n", "\n\ndef test_sound_stream_logsig(sound_stream):\n    lsig = sound_stream.log_signature(5)\n    assert lsig.size() == sound_stream.ctx.lie_size(2)\n\ndef test_sound_stream_value_channels_logsig(sound_file):\n    sound_stream = roughpy.ExternalDataStream.from_uri(\n        sound_file,\n        channel_types=[roughpy.ValueChannel, roughpy.ValueChannel],\n        depth=2\n    )\n    lsig = sound_stream.log_signature(5)\n    assert lsig.size() == sound_stream.ctx.lie_size(2)", ""]}
{"filename": "tests/streams/test_function_path.py", "chunked_list": ["import functools\nimport itertools\nimport operator\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_array_almost_equal\n\nskip = True\nfrom roughpy import RealInterval, FreeTensor, Lie", "skip = True\nfrom roughpy import RealInterval, FreeTensor, Lie\nimport roughpy as rp\n\n\ndef path(*args, **kwargs):\n    return rp.FunctionStream.from_function(*args, **kwargs)\n\n\n# @pytest.mark.skipif(skip, reason=\"path type not available\")\ndef test_function_path_signature_calc_accuracy():\n    def func(t, ctx):\n        if t >= 0.5:\n            return Lie(np.array([t - 0.5, t - 0.5]), ctx=ctx)\n        return Lie(np.array([0.0, 0.0]), ctx=ctx)\n\n    p = path(func, width=2, depth=2, dtype=rp.DPReal)\n\n    r1 = p.signature(0.0, 0.8, 1)\n    expected1 = FreeTensor(np.array([1.0]), width=2, depth=2)\n    assert r1 == expected1\n\n    r2 = p.signature(0.0, 0.8, 2)  # [0, 0.5), [0.5, 0.75)\n    expected2 = p.ctx.lie_to_tensor(func(0.75, p.ctx)).exp()\n    assert r2 == expected2, f\"{r2} {expected2}\"", "\n# @pytest.mark.skipif(skip, reason=\"path type not available\")\ndef test_function_path_signature_calc_accuracy():\n    def func(t, ctx):\n        if t >= 0.5:\n            return Lie(np.array([t - 0.5, t - 0.5]), ctx=ctx)\n        return Lie(np.array([0.0, 0.0]), ctx=ctx)\n\n    p = path(func, width=2, depth=2, dtype=rp.DPReal)\n\n    r1 = p.signature(0.0, 0.8, 1)\n    expected1 = FreeTensor(np.array([1.0]), width=2, depth=2)\n    assert r1 == expected1\n\n    r2 = p.signature(0.0, 0.8, 2)  # [0, 0.5), [0.5, 0.75)\n    expected2 = p.ctx.lie_to_tensor(func(0.75, p.ctx)).exp()\n    assert r2 == expected2, f\"{r2} {expected2}\"", "\n\n@pytest.fixture\ndef solution_signature(width, depth, tensor_size):\n    letters = list(range(1, width + 1))\n\n    def sig_func(a, b):\n        rv = np.zeros(tensor_size)\n        rv[0] = 1.0\n\n        for let in letters:\n            rv[let] = let * (b - a)\n\n        idx = width\n        factor = 1.0\n        for d in range(2, depth + 1):\n            factor /= d\n\n            for data in itertools.product(letters, repeat=d):\n                idx += 1\n                rv[idx] = factor * functools.reduce(operator.mul, data, 1) * (\n                            b - a) ** d\n        return rv\n\n    return sig_func", "\n\ndef test_fpath_known_signature_calc(width, depth, solution_signature):\n    def func(t, ctx):\n        return Lie(np.arange(1.0, width + 1) * t, ctx=ctx)\n\n    p = path(func, width=width, depth=depth, dtype=rp.DPReal)\n\n    expected = FreeTensor(solution_signature(0.0, 2.0), ctx=p.ctx)\n    assert_array_almost_equal(p.signature(0.0, 2.0, 0.0), expected)", "\n\n@pytest.fixture\ndef deriv_function_path():\n    def func(t, ctx):\n        if t > 0.0:\n            return Lie(np.array([0.2, 0.4, 0.6]), ctx=ctx)\n        return Lie(np.array([0.0, 0.0, 0.0]), ctx=ctx)\n\n    return func", "\n\ndef test_func_sig_deriv_s_width_3_depth_1_let_2_perturb(deriv_function_path):\n    p = path(deriv_function_path, width=3, depth=1, dtype=rp.DPReal)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), ctx=p.ctx)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative(interval, perturbation, 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0]), ctx=p.ctx)\n\n    assert d == expected", "\n\ndef test_func_sig_deriv_s_width_3_depth_2_let_2_perturb(deriv_function_path):\n    p = path(deriv_function_path, width=3, depth=2, dtype=rp.DPReal)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), ctx=p.ctx)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative(interval, perturbation, 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0,\n                                    0.0, 0.1, 0.0,\n                                    0.1, 0.4, 0.3,\n                                    0.0, 0.3, 0.0\n                                    ]), ctx=p.ctx)\n\n    assert_array_almost_equal(p.log_signature(interval, 1),\n                              np.array([0.2, 0.4, 0.6, 0.0, 0.0, 0.0]))\n    assert_array_almost_equal(d, expected)", "\n\ndef test_func_sig_deriv_m_width_3_depth_1_let_2_perturb(deriv_function_path):\n    p = path(deriv_function_path, width=3, depth=1, dtype=rp.DPReal)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), ctx=p.ctx)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative([(interval, perturbation)], 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0]), ctx=p.ctx)\n\n    assert_array_almost_equal(d, expected)", "    # assert d == expected\n\n\ndef test_func_sig_deriv_m_width_3_depth_2_let_2_perturb(deriv_function_path):\n    p = path(deriv_function_path, width=3, depth=2, dtype=rp.DPReal)\n    perturbation = Lie(np.array([0.0, 1.0, 0.0]), ctx=p.ctx)\n    interval = RealInterval(0.0, 1.0)\n\n    d = p.signature_derivative([(interval, perturbation)], 1)\n\n    expected = FreeTensor(np.array([0.0, 0.0, 1.0, 0.0,\n                                    0.0, 0.1, 0.0,\n                                    0.1, 0.4, 0.3,\n                                    0.0, 0.3, 0.0\n                                    ]), ctx=p.ctx)\n\n    assert_array_almost_equal(d, expected)", "    # assert d == expected\n"]}
{"filename": "tests/streams/test_brownian_stream.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport pytest\nfrom numpy.testing import assert_array_almost_equal\n\nfrom roughpy import BrownianStream, DPReal, DyadicInterval\n", "from roughpy import BrownianStream, DPReal, DyadicInterval\n\nCONFIGURATIONS = [\n    (2, 2),\n    (2, 5),\n    (5, 2),\n    (10, 2)\n]\n\n", "\n\n@pytest.mark.parametrize(\"config\", CONFIGURATIONS)\ndef test_brownian_stream_signature_multiplicative(config):\n    stream = BrownianStream.with_generator(width=config[0], depth=config[1],\n                                           dtype=DPReal)\n\n    di1 = DyadicInterval(0, 0)\n    di2 = DyadicInterval(1, 0)\n    di3 = DyadicInterval(0, -1)\n\n    left = stream.signature(di1, 5)\n    right = stream.signature(di2, 5)\n    top = stream.signature(di3, 5)\n\n    assert_array_almost_equal(left * right, top), f\"!={top - left * right}\"", ""]}
{"filename": "tests/streams/test_piecewise_lie_path.py", "chunked_list": ["import pytest\nfrom numpy.testing import assert_array_almost_equal\n\nfrom roughpy import DPReal, Lie, PiecewiseAbelianStream, RealInterval, \\\n    VectorType, get_context\n\n# skip = True\nskip = False\n\nWIDTH = 5", "\nWIDTH = 5\nDEPTH = 3\n\n\n@pytest.fixture(params=[1, 2, 5])\ndef count(request):\n    return request.param\n\n", "\n\n@pytest.fixture\ndef piecewise_intervals(count):\n    return [RealInterval(float(i), float(i + 1)) for i in range(count)]\n\n\n@pytest.fixture\ndef piecewise_lie_data(piecewise_intervals, rng):\n    return [\n        (interval,\n         Lie(rng.normal(0.0, 1.0, size=(WIDTH,)), width=WIDTH, depth=DEPTH))\n        for interval in piecewise_intervals\n    ]", "def piecewise_lie_data(piecewise_intervals, rng):\n    return [\n        (interval,\n         Lie(rng.normal(0.0, 1.0, size=(WIDTH,)), width=WIDTH, depth=DEPTH))\n        for interval in piecewise_intervals\n    ]\n\n\n@pytest.fixture\ndef piecewise_lie(piecewise_lie_data):\n    return PiecewiseAbelianStream.construct(piecewise_lie_data, width=WIDTH,\n                                            depth=DEPTH)", "@pytest.fixture\ndef piecewise_lie(piecewise_lie_data):\n    return PiecewiseAbelianStream.construct(piecewise_lie_data, width=WIDTH,\n                                            depth=DEPTH)\n\n\n@pytest.mark.skipif(skip, reason=\"path type not available\")\ndef test_log_signature_full_data(piecewise_lie_data):\n    ctx = get_context(WIDTH, DEPTH, DPReal)\n    piecewise_lie = PiecewiseAbelianStream.construct(piecewise_lie_data,\n                                                     width=WIDTH, depth=DEPTH,\n                                                     dtype=DPReal)\n\n    result = piecewise_lie.log_signature(5)\n    expected = ctx.cbh([d[1] for d in piecewise_lie_data],\n                       vec_type=VectorType.DenseVector)\n    # assert result == expected, f\"{expected}\\n{result}\"\n    assert_array_almost_equal(expected, result)", ""]}
{"filename": "tests/algebra/test_lie.py", "chunked_list": ["import numpy as np\nimport pytest\nfrom numpy.testing import assert_array_equal\n\nimport roughpy\nfrom roughpy import DPReal, Lie, get_context\n\n\n@pytest.fixture\ndef rng():\n    return np.random.default_rng(12345)", "@pytest.fixture\ndef rng():\n    return np.random.default_rng(12345)\n\n\n@pytest.fixture\ndef lie_size(width, depth):\n    ctx = get_context(width, depth, DPReal)\n\n    def func(d=depth):\n        return ctx.lie_size(d)\n\n    return func", "\n\n@pytest.fixture\ndef data1(rng, width, depth, lie_size):\n    return rng.uniform(0.0, 1.0, size=lie_size())\n\n\n@pytest.fixture\ndef data2(rng, width, depth, lie_size):\n    return rng.uniform(1.0, 2.0, size=lie_size())", "def data2(rng, width, depth, lie_size):\n    return rng.uniform(1.0, 2.0, size=lie_size())\n\n\ndef test_create_Lie_width_deduction(width, rng):\n    l = Lie(rng.uniform(0.0, 1.0, size=width))\n\n    assert l.width == width\n    assert l.degree() == 1\n    assert l.size() == width", "\n\ndef test_create_Lie_specified_width(rng, width):\n    ctx = get_context(width, 2, DPReal)\n    l = Lie(rng.uniform(0.0, 1.0, size=ctx.lie_size(2)), width=width)\n\n    assert l.width == width\n    assert l.size() == ctx.lie_size(2)\n    assert l.degree() == 2\n", "\n\ndef test_lie_create_single_float_fails():\n    with pytest.raises(ValueError):\n        l = Lie(1.0, width=2, depth=2)\n\n\ndef test_lie_create_single_int_fails():\n    with pytest.raises(ValueError):\n        l = Lie(1, width=2, depth=2)", "\n\ndef test_lie_create_list_ints():\n    result = Lie([1, 2, 3], width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.DenseVector\n\n\ndef test_lie_create_list_floats():\n    result = Lie([1.0, 2.0, 3.0], width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.DenseVector", "\n\ndef test_lie_create_list_floats():\n    result = Lie([1.0, 2.0, 3.0], width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.DenseVector\n\n\ndef test_Lie_array_roundtrip(width, depth, data1):\n    l = Lie(data1, width=width, depth=depth)\n    assert_array_equal(data1, l)", "\n\ndef test_Lie_array_roundtrip(width, depth, data1):\n    l = Lie(data1, width=width, depth=depth)\n    assert_array_equal(data1, l)\n\n\ndef test_Lie_repr(width, depth, data1, lie_size):\n    l = Lie(data1, width=width, depth=depth)\n    assert repr(l) == f\"Lie({width=}, depth={depth}, ctype=DPReal)\"", "\n\ndef test_Lie_str():\n    width = depth = 2\n\n    l = Lie(np.array([1.0, 2.0, 3.0]), width=width, depth=depth)\n    terms = [\n        \"1(1)\", \"2(2)\",\n        \"3([1,2])\"\n    ]\n    inner = \" \".join(terms)\n\n    assert str(l) == \"{ \" + inner + \" }\"", "\n\ndef test_Lie_addition(width, depth, data1, data2):\n    l1 = Lie(data1, width=width, depth=depth)\n    l2 = Lie(data2, width=width, depth=depth)\n\n    expected = Lie(data1 + data2, width=width, depth=depth)\n    assert l1 + l2 == expected\n\n\ndef test_Lie_subraction(width, depth, data1, data2):\n    l1 = Lie(data1, width=width, depth=depth)\n    l2 = Lie(data2, width=width, depth=depth)\n\n    expected = Lie(data1 - data2, width=width, depth=depth)\n    assert l1 - l2 == expected", "\n\ndef test_Lie_subraction(width, depth, data1, data2):\n    l1 = Lie(data1, width=width, depth=depth)\n    l2 = Lie(data2, width=width, depth=depth)\n\n    expected = Lie(data1 - data2, width=width, depth=depth)\n    assert l1 - l2 == expected\n\n\ndef test_Lie_smul(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(2.0 * data1, width=width, depth=depth)\n    assert l1 * 2.0 == expected", "\n\ndef test_Lie_smul(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(2.0 * data1, width=width, depth=depth)\n    assert l1 * 2.0 == expected\n\n\ndef test_Lie_sdiv(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(data1 / 2.0, width=width, depth=depth)\n    assert l1 / 2.0 == expected", "\ndef test_Lie_sdiv(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(data1 / 2.0, width=width, depth=depth)\n    assert l1 / 2.0 == expected\n\n\ndef test_Lie_mul(width):\n    ctx = get_context(width, 2, DPReal)\n    l1 = Lie(np.array([1.0] + [0.0] * (width - 1)), width=width)\n    l2 = Lie(np.array([0.0, 1.0] + [0.0] * (width - 2)), width=width)\n\n    exp_data = np.zeros(ctx.lie_size(2))\n    exp_data[width] = 1.0\n    expected = Lie(exp_data, width=width)\n\n    assert l1 * l2 == expected", "def test_Lie_mul(width):\n    ctx = get_context(width, 2, DPReal)\n    l1 = Lie(np.array([1.0] + [0.0] * (width - 1)), width=width)\n    l2 = Lie(np.array([0.0, 1.0] + [0.0] * (width - 2)), width=width)\n\n    exp_data = np.zeros(ctx.lie_size(2))\n    exp_data[width] = 1.0\n    expected = Lie(exp_data, width=width)\n\n    assert l1 * l2 == expected", "\n\ndef test_Lie_iadd(width, depth, data1, data2):\n    l1 = Lie(data1, width=width, depth=depth)\n    l2 = Lie(data2, width=width, depth=depth)\n\n    expected = Lie(data1 + data2, width=width, depth=depth)\n    l1 += l2\n    assert l1 == expected\n", "\n\ndef test_Lie_isub(width, depth, data1, data2):\n    l1 = Lie(data1, width=width, depth=depth)\n    l2 = Lie(data2, width=width, depth=depth)\n\n    expected = Lie(data1 - data2, width=width, depth=depth)\n    l1 -= l2\n    assert l1 == expected\n", "\n\ndef test_Lie_ismul(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(2.0 * data1, width=width, depth=depth)\n    l1 *= 2.0\n    assert l1 == expected\n\n\ndef test_Lie_isdiv(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(data1 / 2.0, width=width, depth=depth)\n    l1 /= 2.0\n    assert l1 == expected", "\n\ndef test_Lie_isdiv(width, depth, data1):\n    l1 = Lie(data1, width=width, depth=depth)\n\n    expected = Lie(data1 / 2.0, width=width, depth=depth)\n    l1 /= 2.0\n    assert l1 == expected\n\n\ndef test_Lie_imul(width):\n    ctx = get_context(width, 2, DPReal)\n    l1 = Lie(np.array([1.0] + [0.0] * (width - 1)), width=width)\n    l2 = Lie(np.array([0.0, 1.0] + [0.0] * (width - 2)), width=width)\n\n    exp_data = np.zeros(ctx.lie_size(2))\n    exp_data[width] = 1.0\n    expected = Lie(exp_data, width=width)\n    l1 *= l2\n    assert l1 == expected", "\n\ndef test_Lie_imul(width):\n    ctx = get_context(width, 2, DPReal)\n    l1 = Lie(np.array([1.0] + [0.0] * (width - 1)), width=width)\n    l2 = Lie(np.array([0.0, 1.0] + [0.0] * (width - 2)), width=width)\n\n    exp_data = np.zeros(ctx.lie_size(2))\n    exp_data[width] = 1.0\n    expected = Lie(exp_data, width=width)\n    l1 *= l2\n    assert l1 == expected", ""]}
{"filename": "tests/algebra/test_algebra_context.py", "chunked_list": ["import numpy as np\nimport pytest\n\nfrom roughpy import DPReal, FreeTensor, Lie, get_context as _get_context\n\n\ndef get_context(width, depth):\n    return _get_context(width, depth, DPReal)\n\n\ndef test_get_context_valid_range(width, depth):\n    ctx = get_context(width, depth)\n\n    assert ctx.width == width\n    assert ctx.depth == depth", "\n\ndef test_get_context_valid_range(width, depth):\n    ctx = get_context(width, depth)\n\n    assert ctx.width == width\n    assert ctx.depth == depth\n\n\n@pytest.mark.skip(\"Currently won't fail\")\ndef test_get_context_out_of_bounds():\n    with pytest.raises(ValueError):\n        # utterly absurd values. In the future, we might support arbitrary\n        # alphabets but not now.\n        ctx = get_context(10000, 10000)", "\n@pytest.mark.skip(\"Currently won't fail\")\ndef test_get_context_out_of_bounds():\n    with pytest.raises(ValueError):\n        # utterly absurd values. In the future, we might support arbitrary\n        # alphabets but not now.\n        ctx = get_context(10000, 10000)\n\n\ndef test_lie_size(width, depth):\n    ctx = get_context(width, depth)\n\n    assert ctx.lie_size(1) == width", "\ndef test_lie_size(width, depth):\n    ctx = get_context(width, depth)\n\n    assert ctx.lie_size(1) == width\n\n\ndef test_tensor_size(width, depth):\n    ctx = get_context(width, depth)\n\n    assert ctx.tensor_size(1) == 1 + width", "\n\n# @pytest.mark.skip(\"not yet implemented\")\ndef test_make_zero_lie(width, depth):\n    ctx = get_context(width, depth)\n    l = ctx.zero_lie()\n\n    assert isinstance(l, Lie)\n    assert l.width == width\n    assert l.max_degree == depth\n    assert l.size() == 0\n    assert l.degree() == 0", "\n\n# @pytest.mark.skip(\"not yet implemented\")\ndef test_lie_to_tensor(width, depth):\n    l = Lie(np.array(range(1, width + 1), dtype=np.float64), width=width,\n            depth=depth)\n    ctx = get_context(width, depth)\n\n    t = ctx.lie_to_tensor(l)\n\n    assert isinstance(t, FreeTensor)\n    assert t == FreeTensor(np.array(range(width + 1), dtype=np.float64),\n                           width=width,\n                           depth=depth)", "\n\n# @pytest.mark.skip(\"not yet implemented\")\ndef test_tensor_to_lie(width, depth):\n    t = FreeTensor(np.array(range(width + 1), dtype=np.float64), width=width,\n                   depth=depth)\n    ctx = get_context(width, depth)\n\n    l = ctx.tensor_to_lie(t)\n\n    assert isinstance(l, Lie)\n    assert l == Lie(np.array(range(1, width + 1), dtype=np.float64),\n                    width=width,\n                    depth=depth)", ""]}
{"filename": "tests/algebra/test_lie_basis.py", "chunked_list": ["import pytest\nimport roughpy as rp\n\n\ndef test_lie_basis_iteration():\n\n    ctx = rp.get_context(2, 2, rp.DPReal)\n    basis = ctx.lie_basis\n\n    keys = list(basis)\n\n    assert len(keys) == 3", ""]}
{"filename": "tests/algebra/test_tensor_keys.py", "chunked_list": ["import numpy as np\nimport pytest\n\nfrom roughpy import TensorKey\n\n\ndef tensor_size_impl(width, depth):\n    s = depth\n    r = 1\n    while s:\n        r *= width\n        r += 1\n        s -= 1\n    return r", "\n\ndef test_TensorKey_str_empty(width, depth):\n    key = TensorKey(width=width, depth=depth)\n    assert str(key) == \"()\"\n\n\ndef test_TensorKey_str_letter(width, depth):\n    key = TensorKey(1, width=width, depth=depth)\n    assert str(key) == \"(1)\"", "\n\ndef test_TensorKey_str_n_letters(width, depth):\n    lets = [1] * depth\n    key = TensorKey(lets, width=width, depth=depth)\n\n    assert str(key) == \"(\" + \",\".join(map(str, lets)) + \")\"\n\n\n@pytest.mark.parametrize(\"wdth, dpth, lttrs\",", "\n@pytest.mark.parametrize(\"wdth, dpth, lttrs\",\n                         [(w, d, np.arange(1, w + 1)) for w in range(2, 6) for d\n                          in range(2, 6)])\ndef test_TensorKey_width_derivation(wdth, dpth, lttrs, rng):\n    letters = np.concatenate([[wdth], rng.choice(lttrs, size=(dpth - 1,))])\n    rng.shuffle(letters)\n\n    key = TensorKey(letters)\n    assert key.width == wdth\n    assert key.degree() == dpth", "\n\ndef test_TensorKey_out_of_bounds_fail_single_letter(width):\n    with pytest.raises(ValueError):\n        key = TensorKey(width + 1, width=width)\n\n\ndef test_TensorKey_out_of_bounds_fail_multiple_letter(width, depth):\n    with pytest.raises(ValueError):\n        key = TensorKey([width + 1] + [1] * (depth - 1), width=width)", "\n\n@pytest.mark.parametrize(\"d, data\", [(d, [1] * d) for d in range(1, 6)])\ndef test_TensorKey_depth_derivation(d, data, width):\n    key = TensorKey(data, width=width)\n\n    assert key.degree() == d\n    assert key.max_degree == d\n\n", "\n\n@pytest.mark.parametrize(\"d, data\", [(d, [1] * (d + 1)) for d in range(1, 6)])\ndef test_TensorKey_depth_out_of_bounds_fail(d, data, width):\n    with pytest.raises(ValueError):\n        key = TensorKey(data, width=width, depth=d)\n\n\ndef test_TensorKey_from_index(width, depth, tensor_size):\n    key = TensorKey(index=tensor_size - 1, width=width, depth=depth)\n\n    assert key == TensorKey([width] * depth, width=width, depth=depth)", "def test_TensorKey_from_index(width, depth, tensor_size):\n    key = TensorKey(index=tensor_size - 1, width=width, depth=depth)\n\n    assert key == TensorKey([width] * depth, width=width, depth=depth)\n"]}
{"filename": "tests/algebra/__init__.py", "chunked_list": [""]}
{"filename": "tests/algebra/test_tensor_iterator.py", "chunked_list": ["import itertools\n\nimport numpy as np\nimport pytest\n\nfrom roughpy import FreeTensor, TensorKey\n\n\n@pytest.fixture\ndef TensorKey_iter(width, depth):\n    def itr():\n        yield TensorKey(width=width, depth=depth)\n\n        for let in range(1, width + 1):\n            yield TensorKey(let, width=width, depth=depth)\n\n        for d in range(2, depth + 1):\n            for data in itertools.product(range(1, width + 1), repeat=d):\n                yield TensorKey(data, width=width, depth=depth)\n\n    return itr", "@pytest.fixture\ndef TensorKey_iter(width, depth):\n    def itr():\n        yield TensorKey(width=width, depth=depth)\n\n        for let in range(1, width + 1):\n            yield TensorKey(let, width=width, depth=depth)\n\n        for d in range(2, depth + 1):\n            for data in itertools.product(range(1, width + 1), repeat=d):\n                yield TensorKey(data, width=width, depth=depth)\n\n    return itr", "\n\n# @pytest.mark.xfail\ndef test_FreeTensor_iterator(width, depth, tensor_size, TensorKey_iter):\n    data = np.arange(1.0, float(tensor_size + 1))\n    tens = FreeTensor(data, width=width, depth=depth)\n\n    result = [(i.key(), i.value()) for i in tens]\n    expected = list(zip(TensorKey_iter(), data))\n\n    assert result == expected", ""]}
{"filename": "tests/algebra/test_free_multiply_functions.py", "chunked_list": ["import pytest\n\nimport roughpy as rp\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal\n\n\n@pytest.fixture\ndef tensor_context():\n    return rp.get_context(2, 2, rp.DPReal)", "@pytest.fixture\ndef tensor_context():\n    return rp.get_context(2, 2, rp.DPReal)\n\n\n@pytest.fixture\ndef tensor_data(rng, tensor_context):\n    def generator():\n        return rng.uniform(-1.0, 1.0, size=tensor_context.tensor_size(\n            tensor_context.depth))\n\n    return generator", "\n\ndef test_free_tensor_multiply_shuffles(tensor_data, tensor_context):\n    d1 = tensor_data()\n    d2 = tensor_data()\n    sh1 = rp.ShuffleTensor(d1, ctx=tensor_context)\n    sh2 = rp.ShuffleTensor(d2, ctx=tensor_context)\n\n    result = rp.free_multiply(sh1, sh2)\n\n    ft1 = rp.FreeTensor(d1, ctx=tensor_context)\n    ft2 = rp.FreeTensor(d2, ctx=tensor_context)\n    expected = ft1 * ft2\n\n    assert_array_equal(result, expected)", "\n\ndef test_shuffle_multiply_two_frees(tensor_data, tensor_context):\n    d1 = tensor_data()\n    d2 = tensor_data()\n    ft1 = rp.FreeTensor(d1, ctx=tensor_context)\n    ft2 = rp.FreeTensor(d2, ctx=tensor_context)\n\n    result = rp.shuffle_multiply(ft1, ft2)\n\n    sh1 = rp.ShuffleTensor(d1, ctx=tensor_context)\n    sh2 = rp.ShuffleTensor(d2, ctx=tensor_context)\n    expected = sh1 * sh2\n\n    assert_array_equal(result, expected)", "\n\ndef test_adjoint_of_left_multiplication(tensor_data, tensor_context):\n    d1 = tensor_data()\n    d2 = tensor_data()\n    width = tensor_context.width\n    depth = tensor_context.depth\n    sizes = [0, 1]\n    for i in range(depth):\n        sizes.append(sizes[-1]*width)\n\n    t1 = rp.FreeTensor(d1, ctx=tensor_context)\n    t2 = rp.FreeTensor(d2, ctx=tensor_context)\n\n    result = rp.adjoint_to_free_multiply(t1, t2)\n\n    expected_data = np.zeros(tensor_context.tensor_size(tensor_context.depth))\n    for entry in t2:\n        key = entry.key()\n        for i in range(key.degree()+1):\n            left, right = entry.key().split_n(i)\n            expected_data[right.to_index()] += d1[left.to_index()]*entry.value().to_float()\n\n\n    expected = rp.FreeTensor(expected_data, ctx=tensor_context)\n    assert_array_equal(result, expected)", "    #assert result.size() == t1.size()"]}
{"filename": "tests/algebra/test_lie_keys.py", "chunked_list": ["from roughpy import LieKey\n\n\ndef test_construct_int():\n    key = LieKey(1, width=2, depth=4)\n    assert str(key) == \"1\"\n\n\ndef test_construct_list_pair():\n    key = LieKey([1, 2], width=2, depth=4)\n    assert str(key) == \"[1,2]\"", "def test_construct_list_pair():\n    key = LieKey([1, 2], width=2, depth=4)\n    assert str(key) == \"[1,2]\"\n\n\ndef test_construct_nested_list():\n    key = LieKey([1, [1, 2]], width=2, depth=4);\n    assert str(key) == \"[1,[1,2]]\"\n\n\ndef test_construct_two_nested_lists():\n    key = LieKey([[1, 2], [1, 3]], width=3, depth=4)\n\n    assert str(key) == \"[[1,2],[1,3]]\"", "\n\ndef test_construct_two_nested_lists():\n    key = LieKey([[1, 2], [1, 3]], width=3, depth=4)\n\n    assert str(key) == \"[[1,2],[1,3]]\"\n\n\ndef test_construct_lots_of_nesting_nesting():\n    key = LieKey([[1, [1, 2]], [[1, 2], [1, [1, 3]]]], width=3, depth=4)\n\n    assert str(key) == \"[[1,[1,2]],[[1,2],[1,[1,3]]]]\"", "def test_construct_lots_of_nesting_nesting():\n    key = LieKey([[1, [1, 2]], [[1, 2], [1, [1, 3]]]], width=3, depth=4)\n\n    assert str(key) == \"[[1,[1,2]],[[1,2],[1,[1,3]]]]\"\n"]}
{"filename": "tests/algebra/test_free_tensor.py", "chunked_list": ["import numpy as np\nimport pytest\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal\n\nimport roughpy\nfrom roughpy import FreeTensor, TensorKey\n\nDEPTH_LIMITS = {\n    2: range(2, 26),\n    3: range(2, 16),", "    2: range(2, 26),\n    3: range(2, 16),\n    4: range(2, 11),\n    5: range(2, 11),\n}\n\n\n@pytest.fixture\ndef rng():\n    return np.random.default_rng(12345)", "def rng():\n    return np.random.default_rng(12345)\n\n\n@pytest.fixture\ndef rdata(rng, width):\n    return rng.uniform(0.0, 1.0, size=1 + width)\n\n\n@pytest.fixture\ndef rtensor(width, rdata):\n    return FreeTensor(rdata, width=width)", "\n@pytest.fixture\ndef rtensor(width, rdata):\n    return FreeTensor(rdata, width=width)\n\n\n@pytest.fixture\ndef data1(tensor_size, rng):\n    return rng.uniform(0.0, 1.0, size=tensor_size)\n", "\n\n@pytest.fixture\ndef data2(tensor_size, rng):\n    return rng.uniform(1.0, 2.0, size=tensor_size)\n\n\ndef test_create_single_float_no_ctype():\n    result = FreeTensor(1.0, width=2, depth=2)\n\n    np_result = np.array(result)\n    assert np_result.shape == (1,)\n    assert_array_equal(np_result, np.array([1.]))\n    assert result.width == 2\n    assert result.max_degree == 2", "\n\ndef test_create_single_int_no_ctype():\n    result = FreeTensor(1, width=2, depth=2)\n\n    np_result = np.array(result)\n    assert np_result.shape == (1,)\n    assert_array_equal(np_result, np.array([1.]))\n    assert result.width == 2\n    assert result.max_degree == 2", "\n\ndef test_create_list_floats_no_ctype():\n    result = FreeTensor([0., 1., 2., 3.], width=3, depth=2)\n\n    np_result = np.array(result)\n    assert np_result.shape == (4,)\n    assert np_result.dtype == np.float64\n    assert_array_equal(np_result, np.array([0., 1., 2., 3.]))\n    assert result.width == 3\n    assert result.max_degree == 2", "\n\ndef test_create_list_ints_no_ctype():\n    result = FreeTensor([0, 1, 2, 3], width=3, depth=2)\n\n    np_result = np.array(result)\n    assert np_result.shape == (4,)\n    assert np_result.dtype == np.float64\n    assert_array_equal(np_result, np.array([0., 1., 2., 3.]))\n    assert result.width == 3\n    assert result.max_degree == 2", "\n\ndef test_create_nested_list_ints_no_ctype():\n    with pytest.raises(ValueError):\n        result = FreeTensor([[0, 1, 2, 3]], width=3, depth=3)\n\n\ndef test_create_list_width_deduction():\n    result = FreeTensor([0, 1, 2, 3], depth=2)\n\n    assert result.width == 3", "\n\nclass DLCreateDummy:\n\n    def __init__(self, array):\n        self.array = array\n\n    def __dlpack__(self, stream=None):\n        return self.array.__dlpack__(stream=stream)\n", "\n\n@pytest.mark.skipif(tuple(map(int, np.__version__.split(\".\"))) < (1, 22, 0),\n                    reason=\"dlpack support was added in NumPy 1.22\")\ndef test_create_dlpack():\n    dummy = DLCreateDummy(np.array([0., 1., 2., 3]))\n    result = FreeTensor(dummy, width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert_array_equal(result, np.array([0., 1., 2., 3.]))", "\n\ndef test_create_buffer_doubles():\n    from array import array\n    data = array('d', [0., 1., 2., 3.])\n    result = FreeTensor(data, width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert_array_equal(result, np.array([0., 1., 2., 3.]))", "\n\ndef test_create_buffer_floats():\n    from array import array\n    data = array('f', [0., 1., 2., 3.])\n    result = FreeTensor(data, width=3, depth=2)\n\n    assert result.width == 3\n    assert result.max_degree == 2\n    assert_array_equal(result, np.array([0., 1., 2., 3.], dtype=np.float32))", "\n\ndef test_create_intv_pair():\n    result = FreeTensor((1, 1.0), width=2, depth=2)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.SparseVector\n    # assert_array_equal(result, np.array([0., 1., 0.]))\n", "    # assert_array_equal(result, np.array([0., 1., 0.]))\n\n\ndef test_create_list_intv_pairs():\n    result = FreeTensor([(1, 1.0), (2, 2.0)], width=2, depth=2)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.SparseVector\n    # assert_array_equal(result, np.array([0., 1., 2.]))", "    # assert_array_equal(result, np.array([0., 1., 2.]))\n\n\ndef test_create_tkey_val_pair():\n    k1 = TensorKey(1, width=2, depth=2)\n    result = FreeTensor((k1, 1.0), width=2, depth=2)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.SparseVector", "    # assert result[1] == 1.0\n\n\ndef test_create_list_tkey_val_pairs():\n    data = [\n        (TensorKey(1, width=2, depth=2), 1.0),\n        (TensorKey(2, width=2, depth=2), 2.0)\n    ]\n    result = FreeTensor(data, width=2, depth=2)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.SparseVector", "\n\ndef test_create_list_intv_pairs_dense():\n    result = FreeTensor([(1, 1.0), (2, 2.0)], width=2, depth=2,\n                        vector_type=roughpy.VectorType.DenseVector)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.DenseVector\n    assert_array_equal(result, np.array([0., 1., 2.]))", "\n\ndef test_create_dict_args():\n    result = FreeTensor({1: 1., 2: 2.}, width=2, depth=2)\n\n    assert result.width == 2\n    assert result.max_degree == 2\n    assert result.storage_type == roughpy.VectorType.SparseVector\n\n\ndef test_create_list_dicts():\n    data = [\n        {1: 1., 2: 2.},\n        {0: 1., 2: 1.}\n    ]\n\n    with pytest.raises(ValueError):\n        result = FreeTensor(data, width=2, depth=2)", "\n\ndef test_create_list_dicts():\n    data = [\n        {1: 1., 2: 2.},\n        {0: 1., 2: 1.}\n    ]\n\n    with pytest.raises(ValueError):\n        result = FreeTensor(data, width=2, depth=2)", "\n\ndef test_create_FreeTensor_width_deduction(width, rdata):\n    # Tensor of width n has n+1 elements up to degree 1\n    tens = FreeTensor(rdata)\n\n    assert tens.width == width\n    assert tens.degree() == 1\n    assert tens.size() == rdata.size\n", "\n\ndef test_create_FreeTensor_specified_width(rng, width):\n    data = rng.uniform(0.0, 1.0, size=1 + width * (1 + width))\n    tens = FreeTensor(data, width=width)\n\n    assert tens.width == width\n    assert tens.degree() == 2\n    assert tens.size() == data.size\n", "\n\ndef test_create_FreeTensor_specified_width_incomplete_degree_range(rng, width):\n    data = rng.uniform(1.0, 2.0, size=1 + width + 1)\n    tens = FreeTensor(data, width=width, depth=2)\n\n    assert tens.width == width\n    assert tens.degree() == 2\n    assert tens.size() == data.size\n\n    atens = np.array(tens)\n    assert atens.size == 1 + width * (1 + width)\n    assert_array_almost_equal(tens,\n                              np.concatenate((data, np.zeros(width ** 2 - 1))))", "\n\ndef test_FreeTensor_array_roundtrip(width, rdata, rtensor):\n    assert rtensor.width == width\n    assert_array_equal(rdata, np.array(rtensor))\n\n\ndef test_FreeTensor_repr(rng, width, depth, tensor_size):\n    data = rng.uniform(0.0, 1.0, size=tensor_size)\n    t = FreeTensor(data, width=width, depth=depth)\n\n    assert repr(t) == f\"FreeTensor({width=}, depth={depth}, ctype=DPReal)\"", "\n\ndef test_FreeTensor_str():\n    width = 2\n    depth = 2\n    t = FreeTensor(np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]), width=width,\n                   depth=depth)\n    terms = [\n        \"1()\",\n        \"2(1)\", \"3(2)\",\n        \"4(1,1)\", \"5(1,2)\", \"6(2,1)\", \"7(2,2)\"\n    ]\n    inner = \" \".join(terms)\n\n    assert str(t) == \"{ \" + inner + \" }\"", "\n\ndef test_FreeTensor_addition(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n\n    expected = FreeTensor(data1 + data2, width=width, depth=depth)\n    assert t1 + t2 == expected\n\n\ndef test_FreeTensor_subraction(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n\n    expected = FreeTensor(data1 - data2, width=width, depth=depth)\n\n    assert t1 - t2 == expected", "\n\ndef test_FreeTensor_subraction(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n\n    expected = FreeTensor(data1 - data2, width=width, depth=depth)\n\n    assert t1 - t2 == expected\n", "\n\ndef test_FreeTensor_smul(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(2.0 * rdata, width=width)\n\n    assert t1 * 2.0 == expected\n\n\ndef test_FreeTensor_rdiv(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(rdata / 2.0, width=width)\n\n    assert t1 / 2.0 == expected", "\ndef test_FreeTensor_rdiv(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(rdata / 2.0, width=width)\n\n    assert t1 / 2.0 == expected\n\n\ndef test_FreeTensor_iadd(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n    expected = FreeTensor(data1 + data2, width=width, depth=depth)\n\n    t1 += t2\n    assert t1 == expected", "def test_FreeTensor_iadd(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n    expected = FreeTensor(data1 + data2, width=width, depth=depth)\n\n    t1 += t2\n    assert t1 == expected\n\n\ndef test_FreeTensor_isub(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n    expected = FreeTensor(data1 - data2, width=width, depth=depth)\n\n    t1 -= t2\n    assert t1 == expected", "\ndef test_FreeTensor_isub(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    t2 = FreeTensor(data2, width=width, depth=depth)\n    expected = FreeTensor(data1 - data2, width=width, depth=depth)\n\n    t1 -= t2\n    assert t1 == expected\n\n\ndef test_FreeTensor_ismul(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(2.0 * rdata, width=width)\n\n    t1 *= 2.0\n    assert t1 == expected", "\n\ndef test_FreeTensor_ismul(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(2.0 * rdata, width=width)\n\n    t1 *= 2.0\n    assert t1 == expected\n\n\ndef test_FreeTensor_irdiv(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(rdata / 2.0, width=width)\n\n    t1 /= 2.0\n    assert t1 == expected", "\n\ndef test_FreeTensor_irdiv(width, rdata):\n    t1 = FreeTensor(rdata, width=width)\n    expected = FreeTensor(rdata / 2.0, width=width)\n\n    t1 /= 2.0\n    assert t1 == expected\n\n\ndef test_FreeTensor_mul():\n    t1 = FreeTensor(np.array([1.0, 2.0, 3.0]), width=2, depth=2)\n    t2 = FreeTensor(np.array([1.0, -1.0, -1.0]), width=2, depth=2)\n\n    expected = FreeTensor(np.array([1.0, 1.0, 2.0, -2.0, -2.0, -3.0, -3.0]),\n                          width=2, depth=2)\n\n    assert t1 * t2 == expected", "\n\ndef test_FreeTensor_mul():\n    t1 = FreeTensor(np.array([1.0, 2.0, 3.0]), width=2, depth=2)\n    t2 = FreeTensor(np.array([1.0, -1.0, -1.0]), width=2, depth=2)\n\n    expected = FreeTensor(np.array([1.0, 1.0, 2.0, -2.0, -2.0, -3.0, -3.0]),\n                          width=2, depth=2)\n\n    assert t1 * t2 == expected", "\n\ndef test_FreeTensor_imul():\n    t1 = FreeTensor(np.array([1.0, 2.0, 3.0]), width=2, depth=2)\n    t2 = FreeTensor(np.array([1.0, -1.0, -1.0]), width=2, depth=2)\n\n    expected = FreeTensor(np.array([1.0, 1.0, 2.0, -2.0, -2.0, -3.0, -3.0]),\n                          width=2, depth=2)\n\n    t1 *= t2\n    assert t1 == expected", "\n\ndef test_FreeTensor_mul_single_letter(width):\n    depth = 3\n    t = FreeTensor(np.array([0.0, 1.0] + [0.0] * (width - 1)), width=width,\n                   depth=depth)\n\n    expected = FreeTensor(np.array(\n        [1.0, 1.0] + [0.0] * (width - 1) + [0.5] + [0.0] * (width ** 2 - 1) + [\n            1.0 / 6.0] + [0.0] * (width ** 3 - 1)),\n                          width=width, depth=depth)\n\n    assert t.exp() == expected", "\n\ndef test_FreeTensor_exp(width):\n    depth = 3\n    t = FreeTensor(np.array([1.0] * (1 + width)), width=width, depth=depth)\n\n    tunit = FreeTensor(np.array([1.0]), width=width, depth=depth)\n\n    i = float(t.max_degree)\n    expected = FreeTensor(np.array([1.0]), width=width, depth=depth)\n    assert expected.degree() == 0\n    while i:\n        expected *= (t / i)\n        expected += tunit\n        i -= 1.0\n\n    assert expected.degree() == depth\n    assert_array_almost_equal(t.exp(), expected)", "\n\ndef test_FreeTensor_fmexp(width, depth, data1, data2):\n    t1 = FreeTensor(data1, width=width, depth=depth)\n    data2[0] = 0.0\n    t2 = FreeTensor(data2, width=width, depth=depth)\n\n    expected = t1 * t2.exp()\n    t1.fmexp(t2)\n    assert_array_almost_equal(t1, expected)", "\n\ndef test_FreeTensor_log_exp_roundtrip(width, depth, data1):\n    data1[0] = 0.0\n    t = FreeTensor(data1, width=width, depth=depth)\n\n    assert_array_almost_equal(t.exp().log(), t)\n\n\ndef _tensor_size(width, depth):\n    s = depth\n    r = 1\n    while s:\n        r *= width\n        r += 1\n        s -= 1\n    return r", "\ndef _tensor_size(width, depth):\n    s = depth\n    r = 1\n    while s:\n        r *= width\n        r += 1\n        s -= 1\n    return r\n", "\n\n# def test_FreeTensor_view_into_degree(width, depth, data1):\n#     t = FreeTensor(data1, width=width, depth=depth)\n#\n#     r0 = t.degree_array(0)\n#     assert r0 == data1[0]\n#\n#     for d in range(1, depth+1):\n#         r = t.degree_array(d)", "#     for d in range(1, depth+1):\n#         r = t.degree_array(d)\n#         assert r.shape == tuple([width]*d)\n#\n#         start, end = _tensor_size(width, d-1), _tensor_size(width, d)\n#         assert r.size == end - start\n#\n#         v = data1[start:end].reshape([width]*d)\n#\n#         assert_array_equal(r, v)", "#\n#         assert_array_equal(r, v)\n\n\ndef test_coeff_and_vec_type(width, depth, data1, coeff_type, vec_type):\n    t = FreeTensor(data1, width=width, depth=depth, dtype=coeff_type,\n                   vector_type=vec_type)\n\n    assert t.storage_type == vec_type\n    assert t.dtype == coeff_type", "\n\ndef test_antipode(width, depth, data1, coeff_type, vec_type):\n    t = FreeTensor(data1, width=width, depth=depth, dtype=coeff_type,\n                   vector_type=vec_type)\n\n    result = t.antipode().antipode()\n    assert result == t, f\"{result} {t} {result - t}\"\n", ""]}
{"filename": "tests/algebra/test_alg_poly_coeffs.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport roughpy\nfrom roughpy import FreeTensor, Monomial, ShuffleTensor\n\n\ndef test_construct_tensor_poly_coeffs():\n    data = [1 * Monomial(f\"x{i}\") for i in range(3)]\n\n    ft = FreeTensor(data, width=2, depth=2, dtype=roughpy.RationalPoly)\n\n    assert str(ft) == \"{ { 1(x0) }() { 1(x1) }(1) { 1(x2) }(2) }\"", "\ndef test_construct_tensor_poly_coeffs():\n    data = [1 * Monomial(f\"x{i}\") for i in range(3)]\n\n    ft = FreeTensor(data, width=2, depth=2, dtype=roughpy.RationalPoly)\n\n    assert str(ft) == \"{ { 1(x0) }() { 1(x1) }(1) { 1(x2) }(2) }\"\n\n\ndef test_exp_log_roundtrip_poly_coeffs():\n    data = [0, 1 * Monomial('x1'), 1 * Monomial('x2')]\n    ft = FreeTensor(data, width=2, depth=2, dtype=roughpy.RationalPoly)\n\n    assert ft.exp().log() == ft", "\ndef test_exp_log_roundtrip_poly_coeffs():\n    data = [0, 1 * Monomial('x1'), 1 * Monomial('x2')]\n    ft = FreeTensor(data, width=2, depth=2, dtype=roughpy.RationalPoly)\n\n    assert ft.exp().log() == ft\n\n\ndef test_shuffle_product_poly_coeffs():\n    lhs = ShuffleTensor([1 * Monomial(f\"x{i}\") for i in range(7)], width=2,\n                        depth=2, dtype=roughpy.RationalPoly)\n    rhs = ShuffleTensor([1 * Monomial(f\"y{i}\") for i in range(7)], width=2,\n                        depth=2, dtype=roughpy.RationalPoly)\n\n    result = lhs * rhs\n\n    expected_data = [\n        1 * Monomial('x0') * Monomial('y0'),\n        Monomial('x0') * Monomial('y1')\n        + Monomial('x1') * Monomial('y0'),\n        Monomial('x0') * Monomial('y2')\n        + Monomial('x2') * Monomial('y0'),\n        Monomial(['x0', 'y3'])\n        + 2 * Monomial(['x1', 'y1'])\n        + Monomial(['x3', 'y0']),\n        Monomial(['x0', 'y4'])\n        + Monomial(['x1', 'y2'])\n        + Monomial(['y1', 'x2'])\n        + Monomial(['x4', 'y0']),\n        Monomial(['x0', 'y5'])\n        + Monomial(['x2', 'y1'])\n        + Monomial(['y2', 'x1'])\n        + Monomial(['x5', 'y0']),\n        Monomial(['x0', 'y6'])\n        + 2 * Monomial(['x2', 'y2'])\n        + Monomial(['x6', 'y0'])\n    ]\n    expected = ShuffleTensor(expected_data, width=2, depth=2,\n                             dtype=roughpy.RationalPoly)\n\n    assert result == expected, f\"{expected - result} != 0\"", "def test_shuffle_product_poly_coeffs():\n    lhs = ShuffleTensor([1 * Monomial(f\"x{i}\") for i in range(7)], width=2,\n                        depth=2, dtype=roughpy.RationalPoly)\n    rhs = ShuffleTensor([1 * Monomial(f\"y{i}\") for i in range(7)], width=2,\n                        depth=2, dtype=roughpy.RationalPoly)\n\n    result = lhs * rhs\n\n    expected_data = [\n        1 * Monomial('x0') * Monomial('y0'),\n        Monomial('x0') * Monomial('y1')\n        + Monomial('x1') * Monomial('y0'),\n        Monomial('x0') * Monomial('y2')\n        + Monomial('x2') * Monomial('y0'),\n        Monomial(['x0', 'y3'])\n        + 2 * Monomial(['x1', 'y1'])\n        + Monomial(['x3', 'y0']),\n        Monomial(['x0', 'y4'])\n        + Monomial(['x1', 'y2'])\n        + Monomial(['y1', 'x2'])\n        + Monomial(['x4', 'y0']),\n        Monomial(['x0', 'y5'])\n        + Monomial(['x2', 'y1'])\n        + Monomial(['y2', 'x1'])\n        + Monomial(['x5', 'y0']),\n        Monomial(['x0', 'y6'])\n        + 2 * Monomial(['x2', 'y2'])\n        + Monomial(['x6', 'y0'])\n    ]\n    expected = ShuffleTensor(expected_data, width=2, depth=2,\n                             dtype=roughpy.RationalPoly)\n\n    assert result == expected, f\"{expected - result} != 0\"", ""]}
{"filename": "tests/scalars/test_polynomial.py", "chunked_list": ["\nimport pytest\n\n\nimport roughpy\nfrom roughpy import PolynomialScalar, Monomial\n\n\n\n\ndef test_polynomial_construct_from_dict():\n    data = {\n        Monomial('x1'): 1,\n        Monomial('x2'): 2\n    }\n\n    p = PolynomialScalar(data)\n\n    assert str(p) == \"{ 1(x1) 2(x2) }\"", "\n\ndef test_polynomial_construct_from_dict():\n    data = {\n        Monomial('x1'): 1,\n        Monomial('x2'): 2\n    }\n\n    p = PolynomialScalar(data)\n\n    assert str(p) == \"{ 1(x1) 2(x2) }\"", ""]}
{"filename": "tests/scalars/__init__.py", "chunked_list": [""]}
{"filename": "tests/scalars/test_monomial.py", "chunked_list": ["from fractions import Fraction\n\nimport pytest\n\n\n\nimport roughpy\nfrom roughpy import Monomial\n\n@pytest.fixture(params=[3, 3.1415, Fraction(22, 7)])\ndef scalar_val(request):\n    return request.param", "\n@pytest.fixture(params=[3, 3.1415, Fraction(22, 7)])\ndef scalar_val(request):\n    return request.param\n\ndef test_construct_single_str():\n    m = Monomial(\"x1\")\n\n    assert str(m) == 'x1'\n", "\n\ndef test_mul_monomials():\n    m1 = Monomial('x1')\n    m2 = Monomial('x2')\n\n    assert str(m1*m2) == 'x1 x2'\n\n\ndef test_add_monomials_gives_polynomial():\n    m1 = Monomial('x1')\n    m2 = Monomial('x2')\n\n    p = m1 + m2\n\n    assert type(p) == roughpy.PolynomialScalar", "\ndef test_add_monomials_gives_polynomial():\n    m1 = Monomial('x1')\n    m2 = Monomial('x2')\n\n    p = m1 + m2\n\n    assert type(p) == roughpy.PolynomialScalar\n\n\ndef test_sub_monomials_gives_polynomial():\n    m1 = Monomial('x1')\n    m2 = Monomial('x2')\n\n    p = m1 - m2\n\n    assert type(p) == roughpy.PolynomialScalar", "\n\ndef test_sub_monomials_gives_polynomial():\n    m1 = Monomial('x1')\n    m2 = Monomial('x2')\n\n    p = m1 - m2\n\n    assert type(p) == roughpy.PolynomialScalar\n", "\n\ndef test_scalar_plus_monomial(scalar_val):\n    m = Monomial('x')\n\n    p = scalar_val + m\n\n    assert type(p) == roughpy.PolynomialScalar\n\n\ndef test_monomial_plus_scalar(scalar_val):\n    m = Monomial('x')\n    p = m + scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar", "\n\ndef test_monomial_plus_scalar(scalar_val):\n    m = Monomial('x')\n    p = m + scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar\n\n\ndef test_scalar_mul_monomial(scalar_val):\n    m = Monomial('x')\n\n    p = scalar_val * m\n\n    assert type(p) == roughpy.PolynomialScalar", "\ndef test_scalar_mul_monomial(scalar_val):\n    m = Monomial('x')\n\n    p = scalar_val * m\n\n    assert type(p) == roughpy.PolynomialScalar\n\n\ndef test_monomial_mul_scalar(scalar_val):\n    m = Monomial('x')\n    p = m * scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar", "\ndef test_monomial_mul_scalar(scalar_val):\n    m = Monomial('x')\n    p = m * scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar\n\n\ndef test_monomial_div_scalar(scalar_val):\n    m = Monomial('x')\n    p = m / scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar", "def test_monomial_div_scalar(scalar_val):\n    m = Monomial('x')\n    p = m / scalar_val\n\n    assert type(p) == roughpy.PolynomialScalar\n"]}
{"filename": "tests/scalars/dlpack/__init__.py", "chunked_list": [""]}
{"filename": "tests/scalars/dlpack/test_construct_from_jax.py", "chunked_list": ["\n\nimport pytest\n\ntry:\n    import jax\n    from jax import numpy as jnp\nexcept ImportError:\n    jax = None\n    jnp = None", "\nimport roughpy as rp\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\n@pytest.mark.skipif(jax is None, reason=\"JAX test require jax to be installed\")\nclass TestJaxArrayCreation:\n\n    @pytest.fixture(scope=\"class\")\n    def context(self):\n        return rp.get_context(2, 2, rp.SPReal)\n\n    @pytest.fixture(scope=\"class\")\n    def prng_key(self):\n        return jax.random.PRNGKey(12345)\n\n    def test_increment_stream_from_jax_array(self):\n        array = jnp.array([\n            [-0.25860816, -0.36977386, 0.6619457, -0.50442713, 0.08028925, -1.06701028],\n               [-0.26208243, 0.22464547, 0.39521545, -0.62663144, -0.34344956, -1.67293704],\n               [-0.55824, -0.19376263, 0.86616075, -0.58314389, -0.69254208, -1.53291035],\n               [-0.52306908, -0.09234464, 1.17564034, -0.7388621, -0.91333717, -1.50844121],\n               [-0.80696738, -0.09417236, 0.75135314, -1.20548987, -1.42038512, -1.86834741],\n               [-0.6642682, -0.12166289, 1.04914618, -1.01415539, -1.58841276, -2.54356289]\n        ])\n\n        stream = rp.LieIncrementStream.from_increments(np.array(array), width=6, depth=2, dtype=rp.SPReal)\n\n        lsig01 = stream.log_signature(rp.RealInterval(0, 1))\n        lsig12 = stream.log_signature(rp.RealInterval(1, 2))\n        lsig24 = stream.log_signature(rp.RealInterval(2, 4))\n        lsig46 = stream.log_signature(rp.RealInterval(4, 6))\n\n        assert_array_equal(np.array(lsig01)[:6], array[0, :])\n        assert not np.any(np.isnan(lsig12))\n        assert not np.any(np.isnan(lsig24))\n        assert not np.any(np.isnan(lsig46))\n\n\n\n    @pytest.mark.xfail(condition=True, reason=\"No device support is currently available\")\n    def test_create_tensor_from_jax_array(self, prng_key, context):\n        array = jax.random.uniform(prng_key, shape=(context.tensor_size(2),), dtype=\"float32\", minval=-1.0, maxval=1.0)\n\n        ts = rp.FreeTensor(array, ctx=context)\n\n        assert_array_equal(np.array(ts), np.array(jax.device_get(array), copy=True))", ""]}
{"filename": "tests/intervals/__init__.py", "chunked_list": [""]}
{"filename": "tests/intervals/test_dyadic.py", "chunked_list": ["import itertools\nimport math\n\nimport pytest\n\nfrom roughpy import Dyadic\n\n\n@pytest.mark.parametrize(\"k, n\",\n                         itertools.product(range(-10, 10), range(0, 10)))\ndef test_dyadic_to_float(k, n):\n    d = Dyadic(k, n)\n    assert float(d) == math.ldexp(k, -n)", "@pytest.mark.parametrize(\"k, n\",\n                         itertools.product(range(-10, 10), range(0, 10)))\ndef test_dyadic_to_float(k, n):\n    d = Dyadic(k, n)\n    assert float(d) == math.ldexp(k, -n)\n\n\n@pytest.mark.parametrize(\"n\", range(1, 15))\ndef test_rebase_dyadic(n):\n    d = Dyadic(1, 0)\n\n    d.rebase(n)\n    assert float(d) == 1.0\n    assert d.n == n\n    assert d.k == 1 << n", "def test_rebase_dyadic(n):\n    d = Dyadic(1, 0)\n\n    d.rebase(n)\n    assert float(d) == 1.0\n    assert d.n == n\n    assert d.k == 1 << n\n"]}
{"filename": "tests/intervals/test_intervals.py", "chunked_list": ["import pytest\n\nfrom roughpy import IntervalType, RealInterval\n\n\n@pytest.fixture(params=[IntervalType.Clopen])\ndef interval_type(request):\n    return request.param\n\n", "\n\n@pytest.mark.parametrize(\"inf, sup\", [(0.0, 1.0), (-5.0, 2.0),\n                                      (-float(\"inf\"), float(\"inf\"))])\ndef test_interval_repr(inf, sup, interval_type):\n    ivl = RealInterval(inf, sup, interval_type)\n    type_str = \"clopen\" if interval_type == IntervalType.Clopen else \"opencl\"\n\n    assert repr(\n        ivl) == f\"RealInterval({inf=:0.6f}, {sup=:0.6f}, type={type_str})\"", "\n\ndef test_interval_equality_unit_intervals(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(0.0, 1.0, interval_type)\n\n    assert i1 == i2\n\n\n@pytest.mark.skip(\"OpenCL disabled\")\ndef test_interval_equality_fails_different_types():\n    i1 = RealInterval(0.0, 1.0, IntervalType.Clopen)\n    i2 = RealInterval(0.0, 1.0, IntervalType.Opencl)\n\n    assert i1 != i2", "\n@pytest.mark.skip(\"OpenCL disabled\")\ndef test_interval_equality_fails_different_types():\n    i1 = RealInterval(0.0, 1.0, IntervalType.Clopen)\n    i2 = RealInterval(0.0, 1.0, IntervalType.Opencl)\n\n    assert i1 != i2\n\n\ndef test_interval_equality_fails_mismatched_endpoints(interval_type):\n    i1 = RealInterval(-1.0, 1.0, interval_type)\n    i2 = RealInterval(0.0, 2.0, interval_type)\n\n    assert i1 != i2", "\ndef test_interval_equality_fails_mismatched_endpoints(interval_type):\n    i1 = RealInterval(-1.0, 1.0, interval_type)\n    i2 = RealInterval(0.0, 2.0, interval_type)\n\n    assert i1 != i2\n\n\ndef test_intersects_with_same_interval(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(0.0, 1.0, interval_type)\n\n    assert i1.intersects_with(i2)", "def test_intersects_with_same_interval(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(0.0, 1.0, interval_type)\n\n    assert i1.intersects_with(i2)\n\n\ndef test_intersects_with_overlapping(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(-1.0, 0.5, interval_type)\n\n    assert i1.intersects_with(i2)", "\n\n@pytest.mark.skip(\"OpenCL disabled\")\ndef test_intersects_with_common_endpoints():\n    i1 = RealInterval(0.0, 1.0, IntervalType.Clopen)\n    i2 = RealInterval(-1.0, 0.0, IntervalType.Opencl)\n\n    # (-1.0, 0.0] [0.0, 1.0)\n    assert i1.intersects_with(i2)\n", "\n\ndef test_intersects_with_fails_disjoint(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(2.0, 3.0, interval_type)\n\n    assert not i1.intersects_with(i2)\n\n\ndef test_intersects_with_fails_common_endpoints_same_type(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(-1.0, 0.0, interval_type)\n\n    # [-1.0, 0.0) [0.0, 1.0) or (-1.0, 0.0] (0.0, 1.0]\n    assert not i1.intersects_with(i2)", "\ndef test_intersects_with_fails_common_endpoints_same_type(interval_type):\n    i1 = RealInterval(0.0, 1.0, interval_type)\n    i2 = RealInterval(-1.0, 0.0, interval_type)\n\n    # [-1.0, 0.0) [0.0, 1.0) or (-1.0, 0.0] (0.0, 1.0]\n    assert not i1.intersects_with(i2)\n\n\n@pytest.mark.skip(\"OpenCL disabled\")\ndef test_intersects_with_fails_common_endpoint_opposite_type():\n    i1 = RealInterval(0.0, 1.0, IntervalType.Opencl)\n    i2 = RealInterval(-1.0, 0.0, IntervalType.Clopen)\n\n    # [-1.0, 0.0) (0.0, 1.0]\n    assert not i1.intersects_with(i2)", "\n@pytest.mark.skip(\"OpenCL disabled\")\ndef test_intersects_with_fails_common_endpoint_opposite_type():\n    i1 = RealInterval(0.0, 1.0, IntervalType.Opencl)\n    i2 = RealInterval(-1.0, 0.0, IntervalType.Clopen)\n\n    # [-1.0, 0.0) (0.0, 1.0]\n    assert not i1.intersects_with(i2)\n", ""]}
{"filename": "roughpy/__init__.py", "chunked_list": ["import importlib.metadata as _ilm\nimport os\nimport platform\n\nfrom pathlib import Path\n\ntry:\n    __version__ = _ilm.version(\"RoughPy\")\nexcept _ilm.PackageNotFoundError:\n    __version__ = \"0.0.0\"", "\n\ndef _add_dynload_location(path: Path):\n    if platform.system() == \"Windows\":\n        os.add_dll_directory(str(path))\n        return\n\n\nif platform.system() == \"Windows\":\n    LIBS_DIR = Path(__file__).parent.parent / \"roughpy.libs\"\n    if LIBS_DIR.exists():\n        os.add_dll_directory(str(LIBS_DIR))", "if platform.system() == \"Windows\":\n    LIBS_DIR = Path(__file__).parent.parent / \"roughpy.libs\"\n    if LIBS_DIR.exists():\n        os.add_dll_directory(str(LIBS_DIR))\n\ntry:\n    iomp = _ilm.distribution(\"intel-openmp\")\n    libs = [f for f in iomp.files if f.name.startswith(\"libiomp5\")]\n    if libs:\n        _add_dynload_location(libs[0].locate().resolve().parent)\n    del iomp\n    del libs\nexcept _ilm.PackageNotFoundError:\n    pass", "\nimport roughpy._roughpy\nfrom roughpy._roughpy import *\n"]}
{"filename": "roughpy/streams/tick_stream.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation", "#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE", "#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.", "#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import annotations\n\nfrom abc import abstractmethod, ABC\nfrom typing import Optional, Any, NamedTuple\n\nfrom roughpy._roughpy import TickStreamConstructionHelper, StreamSchema, ChannelType\n\n\nclass BaseTickDataParser(ABC):\n    helper: TickStreamConstructionHelper\n\n    class TickItem(NamedTuple):\n        timestamp: Any\n        label: str\n        type: ChannelType\n        data: Any\n\n    def __init__(self, schema: Optional[StreamSchema] = None, schema_only: bool = False):\n        if schema is not None:\n            self.helper = TickStreamConstructionHelper(schema, schema_only)\n        else:\n            self.helper = TickStreamConstructionHelper(schema_only)\n\n    @abstractmethod\n    def parse_data(self, data: Any):\n        pass\n\n    def convert_channel_type(self, ch_type: Any) -> ChannelType:\n\n        if isinstance(ch_type, ChannelType):\n            return ch_type\n\n        if not isinstance(ch_type, str):\n            raise TypeError(f\"cannot convert {ch_type.__name__} to channel type\")\n\n        if ch_type == \"increment\":\n            return ChannelType.IncrementChannel\n\n        if ch_type == \"value\":\n            return ChannelType.ValueChannel\n\n        if ch_type == \"categorical\":\n            return ChannelType.CategoricalChannel\n\n    def insert(self, item: TickItem):\n        type = self.convert_channel_type(item.type)\n\n        if type == ChannelType.IncrementChannel:\n            self.helper.add_increment(item.label, item.timestamp, item.data)\n        elif type == ChannelType.ValueChannel:\n            self.helper.add_value(item.label, item.timestamp, item.data)\n        elif type == ChannelType.CategoricalChannel:\n            self.helper.add_categorical(item.label, item.timestamp, item.data)", "\n\nclass BaseTickDataParser(ABC):\n    helper: TickStreamConstructionHelper\n\n    class TickItem(NamedTuple):\n        timestamp: Any\n        label: str\n        type: ChannelType\n        data: Any\n\n    def __init__(self, schema: Optional[StreamSchema] = None, schema_only: bool = False):\n        if schema is not None:\n            self.helper = TickStreamConstructionHelper(schema, schema_only)\n        else:\n            self.helper = TickStreamConstructionHelper(schema_only)\n\n    @abstractmethod\n    def parse_data(self, data: Any):\n        pass\n\n    def convert_channel_type(self, ch_type: Any) -> ChannelType:\n\n        if isinstance(ch_type, ChannelType):\n            return ch_type\n\n        if not isinstance(ch_type, str):\n            raise TypeError(f\"cannot convert {ch_type.__name__} to channel type\")\n\n        if ch_type == \"increment\":\n            return ChannelType.IncrementChannel\n\n        if ch_type == \"value\":\n            return ChannelType.ValueChannel\n\n        if ch_type == \"categorical\":\n            return ChannelType.CategoricalChannel\n\n    def insert(self, item: TickItem):\n        type = self.convert_channel_type(item.type)\n\n        if type == ChannelType.IncrementChannel:\n            self.helper.add_increment(item.label, item.timestamp, item.data)\n        elif type == ChannelType.ValueChannel:\n            self.helper.add_value(item.label, item.timestamp, item.data)\n        elif type == ChannelType.CategoricalChannel:\n            self.helper.add_categorical(item.label, item.timestamp, item.data)", "\n\nclass StandardTickDataParser(BaseTickDataParser):\n\n    def parse_data(self, data: Any):\n        for item in self.visit(data, [\"timestamp\", \"label\", \"type\", \"data\"], None):\n            self.insert(item)\n\n    def visit(self,\n              data: Any,\n              labels_remaining: list[str],\n              current: Optional[dict]\n              ):\n        yield from getattr(self, f\"handle_{type(data).__name__}\", self.handle_any)(data, labels_remaining,\n                                                                                   current or {})\n\n    def handle_dict(self,\n                    data: dict,\n                    labels_remaining: list[str],\n                    current: dict\n                    ):\n\n        if all(label in data for label in labels_remaining):\n            yield self.TickItem(**current, **{label: data[label] for label in labels_remaining})\n            return\n\n        key_type, *value_types = labels_remaining\n        if key_type == \"data\":\n            yield self.TickItem(**current, data=data)\n            return\n\n        for key, value in data.items():\n            yield from self.visit(value, value_types, {key_type: key, **current})\n\n    def handle_list(self,\n                    data: Any,\n                    labels_remaining: list[str],\n                    current: dict):\n        first_type, *remaining = labels_remaining\n        if first_type == \"data\":\n            yield self.TickItem(**current, data=data)\n            return\n\n        for value in data:\n            yield from self.visit(value, labels_remaining, current)\n\n    def handle_tuple(self,\n                     data: Any,\n                     labels_remaining: list[str],\n                     current: dict):\n        if len(data) == len(labels_remaining):\n            yield self.TickItem(\n                **(current or {}), **dict(zip(labels_remaining, data))\n            )\n            return\n\n        first_label, *other_labels = labels_remaining\n        if len(data) == 2:\n            yield from self.visit(data[1], other_labels, {**current, first_label: data[0]})\n        else:\n            yield from self.visit(data[1:], other_labels, {**current, first_label: data[0]})\n\n    def handle_any(self, data, labels_remaining, current):\n\n        if len(labels_remaining) == 1:\n            yield self.TickItem(**current, data=data)\n            return\n\n        if \"label\" in labels_remaining or \"timestamp\" in labels_remaining:\n            raise ValueError(\"cannot infer timestamp or label from a single data value\")\n\n        if isinstance(data, (float, int)):\n            # infer value type\n            yield self.TickItem(**current, type=\"increment\", data=data)\n        elif isinstance(data, str):\n            # infer categorical\n            yield self.TickItem(**current, type=\"categorical\", data=data)\n        else:\n            raise ValueError(\"other types cannot be used for anything but data value\")", ""]}
{"filename": "roughpy/streams/__init__.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation", "#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE", "#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.", "#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"]}
{"filename": "examples/signature-kernel-by-signature-dot.py", "chunked_list": ["import numpy as np\nimport roughpy as rp\n\nrng = np.random.default_rng(1635134)\n\n# Sample times\n# should be approximately in [0, 1)\ntimes = np.cumsum(rng.exponential(0.1, 10))\n# Moderate length 2D paths\np1_data = rng.uniform(-1, 1, (10, 2))", "# Moderate length 2D paths\np1_data = rng.uniform(-1, 1, (10, 2))\np2_data = rng.uniform(-1, 1, (10, 2))\ninterval = rp.RealInterval(0, 1)\nprint(\"The interval of definition\", interval)\n\nctx = rp.get_context(width=2, depth=6, coeffs=rp.DPReal)\n\nstream1 = rp.LieIncrementStream.from_increments(p1_data, indices=times, ctx=ctx)\nstream2 = rp.LieIncrementStream.from_increments(p2_data, indices=times, ctx=ctx)", "stream1 = rp.LieIncrementStream.from_increments(p1_data, indices=times, ctx=ctx)\nstream2 = rp.LieIncrementStream.from_increments(p2_data, indices=times, ctx=ctx)\n\nsig1 = stream1.signature(interval)\nsig2 = stream2.signature(interval)\n\nprint(np.inner(np.array(sig1), np.array(sig2)))\n"]}
{"filename": "examples/list_basis_keys.py", "chunked_list": ["\"\"\"\nThis example shows how to generate a list of keys associated with a\nparticular basis.\n\"\"\"\n\nimport roughpy as rp\n\n# Basis objects are tied to contexts, since different backends might provide\n# different bases for the various algebras. For this reason, we must first\n# get a context object using rp.get_context - the scalar type doesn't matter", "# different bases for the various algebras. For this reason, we must first\n# get a context object using rp.get_context - the scalar type doesn't matter\n# - and then access the basis attribute. Note though, that RoughPy requires\n# that all tensor bases have the same ordering.\ncontext = rp.get_context(2, 3, rp.DPReal)\nbasis = context.tensor_basis\n\n# The basis object is iterable, so we can use it in a for-loop to walk\n# through all keys - in order - associated with the basis\nfor key in basis:\n    print(key)", "# through all keys - in order - associated with the basis\nfor key in basis:\n    print(key)\n# ()\n# (1)\n# (2)\n# (1,1)\n# (1,2)\n# (2,1)\n# (2,2)", "# (2,1)\n# (2,2)\n# etc.\n\n\n# Somtimes you might want to write out a list of keys as a string. The\n# easiest way to do this is with str.join, and map.\nall_keys_string = \" \".join(map(str, basis))\n# \"() (1) (2) (1,1) (1,2) (2,1) (2,2) ...\"\n", "# \"() (1) (2) (1,1) (1,2) (2,1) (2,2) ...\"\n"]}
{"filename": "examples/lie_to_tensor_formulae.py", "chunked_list": ["#  Copyright (c) 2023 the RoughPy Developers. All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification,\n#  are permitted provided that the following conditions are met:\n#\n#  1. Redistributions of source code must retain the above copyright notice,\n#  this list of conditions and the following disclaimer.\n#\n#  2. Redistributions in binary form must reproduce the above copyright notice,", "#\n#  2. Redistributions in binary form must reproduce the above copyright notice,\n#  this list of conditions and the following disclaimer in the documentation\n#  and/or other materials provided with the distribution.\n#\n#  3. Neither the name of the copyright holder nor the names of its contributors\n#  may be used to endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"", "#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n#  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n#  OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,", "#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n#  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\nThis example shows how to use RoughPy's polynomial coefficient types to\nderive formulae for the Lie-to-tensor map.\n\"\"\"\nimport roughpy as rp\n", "import roughpy as rp\n\n# Construct the initial Lie object data. The coefficients here are polynomials,\n# which each channel of the Lie algebra space has a unique indeterminate.\nlie_data = [\n    1 * rp.Monomial(\"x1\"),  # channel (1)\n    1 * rp.Monomial(\"x2\"),  # channel (2)\n    1 * rp.Monomial(\"x3\"),  # channel (3)\n    1 * rp.Monomial(\"x4\"),  # channel ([1, 2])\n    1 * rp.Monomial(\"x5\"),  # channel ([1, 3])", "    1 * rp.Monomial(\"x4\"),  # channel ([1, 2])\n    1 * rp.Monomial(\"x5\"),  # channel ([1, 3])\n    1 * rp.Monomial(\"x6\"),  # channel ([2, 3])\n]\n# rp.Monomial creates a monomial object (in this case, a single indeterminate),\n# and multiplying this by 1 makes this monomial into a polynomial.\n\n# Now create the Lie object, with width 3 and depth 2\nlie = rp.Lie(lie_data, width=3, depth=2, dtype=rp.RationalPoly)\n", "lie = rp.Lie(lie_data, width=3, depth=2, dtype=rp.RationalPoly)\n\n# The Lie element constructed here has level 1 data, given by the polynomials\n# constructed above\nprint(lie)\n# { { 1(x1) }(1) { 1(x2) }(2) { 1(x3) }(3) }\n\n# Get a compatible context for access to the lie_to_tensor map.\nctx = rp.get_context(3, 2, rp.RationalPoly)\n", "ctx = rp.get_context(3, 2, rp.RationalPoly)\n\n# perform the lie-to-tensor operation.\ntensor = ctx.lie_to_tensor(lie)\n\n# Print out the result to get the formulae for the Lie-to-tensor map with\n# respect to the original input data.\nprint(tensor)\n# { { 1(x1) }(1) { 1(x2) }(2) { 1(x3) }(3)\n# { 1(x4) }(1,2) { 1(x5) }(1,3) { -1(x4) }(2,1)", "# { { 1(x1) }(1) { 1(x2) }(2) { 1(x3) }(3)\n# { 1(x4) }(1,2) { 1(x5) }(1,3) { -1(x4) }(2,1)\n# { 1(x6) }(2,3) { -1(x5) }(3,1) { -1(x6) }(3,2) }\n\n\n# Now let's repeat the same operation but where we select compute only the\n# channel corresponding to the Lie bracket [1, 2]. This has index 3 in the\n# basis order\nsingle_lie = rp.Lie({3: 1*rp.Monomial(\"x4\")}, width=3, depth=2,\n                    dtype=rp.RationalPoly)", "single_lie = rp.Lie({3: 1*rp.Monomial(\"x4\")}, width=3, depth=2,\n                    dtype=rp.RationalPoly)\n\n# Just one element this time\nprint(single_lie)\n# { { 1(x4) }([1, 2]) }\n\n# As before, use the context to perform the Lie-to-tensor operation\nsingle_tensor = ctx.lie_to_tensor(single_lie)\n", "single_tensor = ctx.lie_to_tensor(single_lie)\n\n# Now there are only two elements in the result.\nprint(single_tensor)\n# { { 1(x4) }(1, 2) { -1(x4) }(2, 1) }\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'RoughPy'\ncopyright = '2023, The RoughPy Authors'", "project = 'RoughPy'\ncopyright = '2023, The RoughPy Authors'\nauthor = 'The RoughPy Authors'\nrelease = '0.0.2'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    \"sphinx.ext.intersphinx\"", "extensions = [\n    \"sphinx.ext.intersphinx\"\n]\n\nsource_suffix = {\n    '.rst': 'restructuredtext',\n    '.txt': 'restructuredtext',\n    '.md': 'markdown'\n}\n", "}\n\ntemplates_path = ['_templates']\nexclude_patterns = []\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'alabaster'\nhtml_static_path = ['_static']\n\n\nintersphinx_mapping = {\n    \"python\": (\"https://python.org/3\", None)\n}\n", "}\n"]}
