{"filename": "src/main.py", "chunked_list": ["import os\nfrom dotenv import load_dotenv\nfrom agent import Agent\nfrom tools.base import AgentTool\nfrom ui.cui import CommandlineUserInterface\nfrom langchain.utilities import GoogleSearchAPIWrapper\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n\n", "\n\n# Set API Keys\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\nassert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\nGOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\", \"\")\nassert GOOGLE_CSE_ID, \"GOOGLE_CSE_ID environment variable is missing from .env\"\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n", "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n\n# Set Agent Settings\nAGENT_NAME = os.getenv(\"AGENT_NAME\", \"\")\nassert AGENT_NAME, \"AGENT_NAME variable is missing from .env\"\nAGENT_ROLE = os.getenv(\"AGENT_ROLE\", \"\")\nassert AGENT_ROLE, \"AGENT_ROLE variable is missing from .env\"\nAGENT_OBJECTIVE = os.getenv(\"AGENT_OBJECTIVE\", \"\")\nassert AGENT_OBJECTIVE, \"AGENT_OBJECTIVE variable is missing from .env\"\nAGENT_DIRECTORY = os.getenv(\"AGENT_DIRECTORY\", \"\")", "assert AGENT_OBJECTIVE, \"AGENT_OBJECTIVE variable is missing from .env\"\nAGENT_DIRECTORY = os.getenv(\"AGENT_DIRECTORY\", \"\")\nassert AGENT_DIRECTORY, \"AGENT_DIRECTORY variable is missing from .env\"\n\nllm = OpenAI(temperature=0.0)\nopenaichat = ChatOpenAI(temperature=0.0)  # Optional\n\n### 1.Create Agent ###\ndir = AGENT_DIRECTORY\n", "dir = AGENT_DIRECTORY\n\nagent = Agent(\n    name=AGENT_NAME,\n    role=AGENT_ROLE,\n    goal=AGENT_OBJECTIVE,\n    ui=CommandlineUserInterface(),\n    openai_api_key=OPENAI_API_KEY,\n    llm=llm,\n    openaichat=openaichat,", "    llm=llm,\n    openaichat=openaichat,\n    dir=dir\n)\n\n### 2. Set up tools for agent ###\nsearch = GoogleSearchAPIWrapper()\nsearch_tool = AgentTool(\n    name=\"google_search\",\n    func=search.run,", "    name=\"google_search\",\n    func=search.run,\n    description=\"\"\"\n        \"With this tool, you can search the web using Google search engine\"\n        \"It is a great way to quickly find information on the web.\"\"\",\n    user_permission_required=False\n)\n\n### 3. Momoize usage of tools to agent ###\nagent.prodedural_memory.memorize_tools([search_tool])", "### 3. Momoize usage of tools to agent ###\nagent.prodedural_memory.memorize_tools([search_tool])\n\n### 4.Run agent ###\nagent.run()\n"]}
{"filename": "src/agent.py", "chunked_list": ["import os\nimport json\nfrom typing import Dict, Any, Optional, Union\nfrom pydantic import BaseModel, Field\nfrom memory.procedual_memory import ProcedualMemory\nfrom memory.episodic_memory import EpisodicMemory, Episode\nfrom memory.semantic_memory import SemanticMemory\nfrom ui.base import BaseHumanUserInterface\nfrom ui.cui import CommandlineUserInterface\nimport llm.reason.prompt as ReasonPrompt", "from ui.cui import CommandlineUserInterface\nimport llm.reason.prompt as ReasonPrompt\nfrom task_manager import Task\nfrom task_manager import TaskManeger\nfrom llm.json_output_parser import LLMJsonOutputParser\nfrom llm.reason.schema import JsonSchema as ReasonSchema\nfrom langchain.llms.base import BaseLLM\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\n", "from langchain.chat_models import ChatOpenAI\n\n# Define the default values\nDEFAULT_AGENT_NAME = \"AI\"\nDEFAULT_AGENT_ROLE = \"Autonomous AI agent that uses both inference and tools to answer many things\"\nDEFAULT_AGENT_GOAL = \"Ending world hunger\"\nDEFAULT_AGENT_DIR = \"./agent_data\"\n\n\n# Define the schema for the llm output", "\n# Define the schema for the llm output\nREASON_JSON_SCHEMA_STR = json.dumps(ReasonSchema.schema)\n\n\nclass Agent(BaseModel):\n    \"\"\"\n    This is the main class for the Agent. It is responsible for managing the tools and the agent.\n    \"\"\"\n    # Define the tools\n    dir: str = Field(\n        DEFAULT_AGENT_DIR, description=\"The folder path to the directory where the agent data is stored and saved\")\n    name: str = Field(DEFAULT_AGENT_NAME, description=\"The name of the agent\")\n    role: str = Field(DEFAULT_AGENT_ROLE, description=\"The role of the agent\")\n    goal: str = Field(DEFAULT_AGENT_GOAL, description=\"The goal of the agent\")\n    ui: BaseHumanUserInterface = Field(\n        CommandlineUserInterface(), description=\"The user interface for the agent\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n    openaichat: Optional[ChatOpenAI] = Field(\n        None, description=\"ChatOpenAI class for the agent\")\n    prodedural_memory: ProcedualMemory = Field(\n        ProcedualMemory(), description=\"The procedural memory about tools agent uses\")\n    episodic_memory: EpisodicMemory = Field(\n        None, description=\"The short term memory of the agent\")\n    semantic_memory: SemanticMemory = Field(\n        None, description=\"The long term memory of the agent\")\n    task_manager: TaskManeger = Field(\n        None, description=\"The task manager for the agent\")\n\n    def __init__(self, openai_api_key: str, dir: str,  **data: Any) -> None:\n        super().__init__(**data)\n        self.task_manager = TaskManeger(llm=self.llm)\n        self.episodic_memory = EpisodicMemory(llm=self.llm)\n        self.semantic_memory = SemanticMemory(llm=self.llm, openaichat=self.openaichat)\n\n        self._get_absolute_path()\n        self._create_dir_if_not_exists()\n\n        if self._agent_data_exists():\n            load_data = self.ui.get_binary_user_input(\n                \"Agent data already exists. Do you want to load the data?\\n\"\n                \"If you choose 'Yes', the data will be loaded.\\n\"\n                \"If you choose 'No', the data will be overwritten.\"\n            )\n            if load_data:\n                self.load_agent()\n            else:\n                self.ui.notify(\"INFO\", \"Agent data will be overwritten.\")\n        self.ui.notify(\n            \"START\", f\"Hello, I am {self.name}. {self.role}. My goal is {self.goal}.\")\n\n    def _get_absolute_path(self) -> None:\n        return os.path.abspath(self.dir)\n\n    def _create_dir_if_not_exists(self) -> None:\n        absolute_path = self._get_absolute_path()\n        if not os.path.exists(absolute_path):\n            os.makedirs(absolute_path)\n\n    def _agent_data_exists(self) -> bool:\n        absolute_path = self._get_absolute_path()\n        return \"agent_data.json\" in os.listdir(absolute_path)\n\n    def run(self):\n        with self.ui.loading(\"Generate Task Plan...\"):\n            self.task_manager.generate_task_plan(\n                name=self.name,\n                role=self.role,\n                goal=self.goal\n            )\n        self.ui.notify(title=\"ALL TASKS\",\n                       message=self.task_manager.get_incomplete_tasks_string(),\n                       title_color=\"BLUE\")\n\n        while True:\n            current_task = self.task_manager.get_current_task_string()\n            if current_task:\n                self.ui.notify(title=\"CURRENT TASK\",\n                               message=current_task,\n                               title_color=\"BLUE\")\n            else:\n                self.ui.notify(title=\"FINISH\",\n                               message=f\"All tasks are completed. {self.name} will end the operation.\",\n                               title_color=\"RED\")\n                break\n\n            # ReAct: Reasoning\n            with self.ui.loading(\"Thinking...\"):\n                try:\n                    reasoning_result = self._reason()\n                    thoughts = reasoning_result[\"thoughts\"]\n                    action = reasoning_result[\"action\"]\n                    tool_name = action[\"tool_name\"]\n                    args = action[\"args\"]\n                except Exception as e:\n                    raise e\n            self.ui.notify(title=\"TASK\", message=thoughts[\"task\"])\n            self.ui.notify(title=\"IDEA\", message=thoughts[\"idea\"])\n            self.ui.notify(title=\"REASONING\", message=thoughts[\"reasoning\"])\n            self.ui.notify(title=\"CRITICISM\", message=thoughts[\"criticism\"])\n            self.ui.notify(title=\"THOUGHT\", message=thoughts[\"summary\"])\n            self.ui.notify(title=\"NEXT ACTION\", message=action)\n\n            # Task Complete\n            if tool_name == \"task_complete\":\n                action_result = args[\"result\"]\n                self._task_complete(action_result)\n                # save agent data\n                with self.ui.loading(\"Save agent data...\"):\n                    self.save_agent()\n\n            # Action with tools\n            else:\n                # Ask for permission to run the action\n                user_permission = self.ui.get_binary_user_input(\n                    \"Do you want to continue?\")\n                if not user_permission:\n                    action_result = \"User Denied to run Action\"\n                    self.ui.notify(title=\"USER INPUT\", message=action_result)\n                else:\n                    try:\n                        action_result = self._act(tool_name, args)\n                    except Exception as e:\n                        raise e\n                    self.ui.notify(title=\"ACTION RESULT\", message=action_result)\n\n            episode = Episode(\n                thoughts=thoughts,\n                action=action,\n                result=action_result\n            )\n\n            summary = self.episodic_memory.summarize_and_memorize_episode(episode)\n            self.ui.notify(title=\"MEMORIZE NEW EPISODE\",\n                           message=summary, title_color=\"blue\")\n\n            entities = self.semantic_memory.extract_entity(action_result)\n            self.ui.notify(title=\"MEMORIZE NEW KNOWLEDGE\",\n                           message=entities, title_color=\"blue\")\n\n    def _reason(self) -> Union[str, Dict[Any, Any]]:\n        current_task_description = self.task_manager.get_current_task_string()\n\n        # Retrie task related memories\n        with self.ui.loading(\"Retrieve memory...\"):\n            # Retrieve memories related to the task.\n            related_past_episodes = self.episodic_memory.remember_related_episodes(\n                current_task_description,\n                k=2)\n            if len(related_past_episodes) > 0:\n                self.ui.notify(title=\"TASK RELATED EPISODE\",\n                               message=related_past_episodes)\n\n            # Retrieve concepts related to the task.\n            related_knowledge = self.semantic_memory.remember_related_knowledge(\n                current_task_description,\n                k=5\n            )\n            if len(related_knowledge) > 0:\n                self.ui.notify(title=\"TASK RELATED KNOWLEDGE\",\n                               message=related_knowledge)\n\n        # Get the relevant tools\n        # If agent has to much tools, use \"remember_relevant_tools\"\n        # because too many tool information will cause context windows overflow.\n        tools = self.prodedural_memory.remember_all_tools()\n\n        # Set up the prompt\n        tool_info = \"\"\n        for tool in tools:\n            tool_info += tool.get_tool_info() + \"\\n\"\n\n        # Get the recent episodes\n        memory = self.episodic_memory.remember_recent_episodes(2)\n\n        # If OpenAI Chat is available, it is used for higher accuracy results.\n        if self.openaichat:\n            propmt = ReasonPrompt.get_chat_template(memory=memory).format_prompt(\n                name=self.name,\n                role=self.role,\n                goal=self.goal,\n                related_past_episodes=related_past_episodes,\n                related_knowledge=related_knowledge,\n                task=current_task_description,\n                tool_info=tool_info\n            ).to_messages()\n            result = self.openaichat(propmt).content\n\n        else:\n            # Get the result from the LLM\n            prompt = ReasonPrompt.get_template(memory=memory)\n            llm_chain = LLMChain(prompt=prompt, llm=self.llm)\n            try:\n                result = llm_chain.predict(\n                    name=self.name,\n                    role=self.role,\n                    goal=self.goal,\n                    related_past_episodes=related_past_episodes,\n                    elated_knowledge=related_knowledge,\n                    task=current_task_description,\n                    tool_info=tool_info\n                )\n            except Exception as e:\n                raise Exception(f\"Error: {e}\")\n\n        # Parse and validate the result\n        try:\n            result_json_obj = LLMJsonOutputParser.parse_and_validate(\n                json_str=result,\n                json_schema=REASON_JSON_SCHEMA_STR,\n                llm=self.llm\n            )\n            return result_json_obj\n        except Exception as e:\n            raise Exception(f\"Error: {e}\")\n\n    def _act(self, tool_name: str, args: Dict) -> str:\n        # Get the tool to use from the procedural memory\n        try:\n            tool = self.prodedural_memory.remember_tool_by_name(tool_name)\n        except Exception as e:\n            raise Exception(\"Invalid command: \" + str(e))\n        try:\n            result = tool.run(**args)\n            return result\n        except Exception as e:\n            raise Exception(\"Could not run tool: \" + str(e))\n\n    def _task_complete(self, result: str) -> str:\n        current_task = self.task_manager.get_current_task_string()\n        self.ui.notify(title=\"COMPLETE TASK\",\n                       message=f\"TASK:{current_task}\\nRESULT:{result}\",\n                       title_color=\"BLUE\")\n\n        self.task_manager.complete_current_task(result)\n\n        return result\n\n    def save_agent(self) -> None:\n        episodic_memory_dir = f\"{self.dir}/episodic_memory\"\n        semantic_memory_dir = f\"{self.dir}/semantic_memory\"\n        filename = f\"{self.dir}/agent_data.json\"\n        self.episodic_memory.save_local(path=episodic_memory_dir)\n        self.semantic_memory.save_local(path=semantic_memory_dir)\n\n        data = {\"name\": self.name,\n                \"role\": self.role,\n                \"episodic_memory\": episodic_memory_dir,\n                \"semantic_memory\": semantic_memory_dir\n                }\n        with open(filename, \"w\") as f:\n            json.dump(data, f)\n\n    def load_agent(self) -> None:\n        absolute_path = self._get_absolute_path()\n        if not \"agent_data.json\" in os.listdir(absolute_path):\n            self.ui.notify(\"ERROR\", \"Agent data does not exist.\", title_color=\"red\")\n\n        with open(os.path.join(absolute_path, \"agent_data.json\")) as f:\n            agent_data = json.load(f)\n            self.name = agent_data[\"name\"]\n            self.role = agent_data[\"role\"]\n\n            try:\n                self.semantic_memory.load_local(agent_data[\"semantic_memory\"])\n            except Exception as e:\n                self.ui.notify(\n                    \"ERROR\", \"Semantic memory data is corrupted.\", title_color=\"red\")\n                raise e\n            else:\n                self.ui.notify(\n                    \"INFO\", \"Semantic memory data is loaded.\", title_color=\"GREEN\")\n\n            try:\n                self.episodic_memory.load_local(agent_data[\"episodic_memory\"])\n            except Exception as e:\n                self.ui.notify(\n                    \"ERROR\", \"Episodic memory data is corrupted.\", title_color=\"RED\")\n                raise e\n            else:\n                self.ui.notify(\n                    \"INFO\", \"Episodic memory data is loaded.\", title_color=\"GREEN\")", ""]}
{"filename": "src/task_manager.py", "chunked_list": ["import json\nfrom pydantic import BaseModel, Field\nfrom pydantic import BaseModel, Field\nfrom langchain.llms.base import BaseLLM\nfrom typing import List, Any\nfrom langchain import LLMChain\nfrom llm.generate_task_plan.prompt import get_template\nfrom llm.list_output_parser import LLMListOutputParser\n\n\nclass Task(BaseModel):\n    \"\"\"Task model.\"\"\"\n    id: int = Field(..., description=\"Task ID\")\n    description: str = Field(..., description=\"Task description\")\n    is_done: bool = Field(False, description=\"Task done or not\")\n    result: str = Field(\"\", description=\"The result of the task\")", "\n\nclass Task(BaseModel):\n    \"\"\"Task model.\"\"\"\n    id: int = Field(..., description=\"Task ID\")\n    description: str = Field(..., description=\"Task description\")\n    is_done: bool = Field(False, description=\"Task done or not\")\n    result: str = Field(\"\", description=\"The result of the task\")\n\n\nclass TaskManeger(BaseModel):\n    \"\"\"Task manager model.\"\"\"\n    tasks: List[Task] = Field([], description=\"The list of tasks\")\n    current_task_id: int = Field(1, description=\"The last task id\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n\n    def generate_task_plan(self, name: str, role: str, goal: str):\n        \"\"\"Generate a task plan for the agent.\"\"\"\n        propmt = get_template()\n        llm_chain = LLMChain(prompt=propmt, llm=self.llm)\n        try:\n            result = llm_chain.predict(\n                name=name,\n                role=role,\n                goal=goal\n            )\n\n        except Exception as e:\n            raise Exception(f\"Error: {e}\")\n\n        # Parse and validate the result\n        try:\n            result_list = LLMListOutputParser.parse(result, separeted_string=\"\\t\")\n        except Exception as e:\n            raise Exception(\"Error: \" + str(e))\n\n        # Add tasks with a serial number\n        for i, e in enumerate(result_list, start=1):\n            id = int(i)\n            description = e\n            self.tasks.append(Task(id=id, description=description))\n\n        self\n\n    def get_task_by_id(self, id: int) -> Task:\n        \"\"\"Get a task by Task id.\"\"\"\n        for task in self.tasks:\n            if task.id == id:\n                return task\n        return None\n\n    def get_current_task(self) -> Task:\n        \"\"\"Get the current task agent is working on.\"\"\"\n        return self.get_task_by_id(self.current_task_id)\n\n    def get_current_task_string(self) -> str:\n        \"\"\"Get the current task agent is working on as a string.\"\"\"\n        task = self.get_current_task()\n        if task is None:\n            return None\n        else:\n            return self._task_to_string(task)\n\n    def complete_task(self, id: int, result: str) -> None:\n        \"\"\"Complete a task by Task id.\"\"\"\n        # Complete the task specified by ID\n        self.tasks[id - 1].is_done = True\n        self.tasks[id - 1].result = result\n        self.current_task_id += 1\n\n    def complete_current_task(self, result: str) -> None:\n        \"\"\"Complete the current task agent is working on.\"\"\"\n        self.complete_task(self.current_task_id, result=result)\n\n    def _task_to_string(self, task: Task) -> str:\n        \"\"\"Convert a task to a string.\"\"\"\n        return f\"{task.id}: {task.description}\"\n\n    def get_incomplete_tasks(self) -> List[Task]:\n        \"\"\"Get the list of incomplete tasks.\"\"\"\n        return [task for task in self.tasks if not task.is_done]\n\n    def get_incomplete_tasks_string(self) -> str:\n        \"\"\"Get the list of incomplete tasks as a string.\"\"\"\n        result = \"\"\n        for task in self.get_incomplete_tasks():\n            result += self._task_to_string(task) + \"\\n\"\n        return result", "\n\nclass TaskManeger(BaseModel):\n    \"\"\"Task manager model.\"\"\"\n    tasks: List[Task] = Field([], description=\"The list of tasks\")\n    current_task_id: int = Field(1, description=\"The last task id\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n\n    def generate_task_plan(self, name: str, role: str, goal: str):\n        \"\"\"Generate a task plan for the agent.\"\"\"\n        propmt = get_template()\n        llm_chain = LLMChain(prompt=propmt, llm=self.llm)\n        try:\n            result = llm_chain.predict(\n                name=name,\n                role=role,\n                goal=goal\n            )\n\n        except Exception as e:\n            raise Exception(f\"Error: {e}\")\n\n        # Parse and validate the result\n        try:\n            result_list = LLMListOutputParser.parse(result, separeted_string=\"\\t\")\n        except Exception as e:\n            raise Exception(\"Error: \" + str(e))\n\n        # Add tasks with a serial number\n        for i, e in enumerate(result_list, start=1):\n            id = int(i)\n            description = e\n            self.tasks.append(Task(id=id, description=description))\n\n        self\n\n    def get_task_by_id(self, id: int) -> Task:\n        \"\"\"Get a task by Task id.\"\"\"\n        for task in self.tasks:\n            if task.id == id:\n                return task\n        return None\n\n    def get_current_task(self) -> Task:\n        \"\"\"Get the current task agent is working on.\"\"\"\n        return self.get_task_by_id(self.current_task_id)\n\n    def get_current_task_string(self) -> str:\n        \"\"\"Get the current task agent is working on as a string.\"\"\"\n        task = self.get_current_task()\n        if task is None:\n            return None\n        else:\n            return self._task_to_string(task)\n\n    def complete_task(self, id: int, result: str) -> None:\n        \"\"\"Complete a task by Task id.\"\"\"\n        # Complete the task specified by ID\n        self.tasks[id - 1].is_done = True\n        self.tasks[id - 1].result = result\n        self.current_task_id += 1\n\n    def complete_current_task(self, result: str) -> None:\n        \"\"\"Complete the current task agent is working on.\"\"\"\n        self.complete_task(self.current_task_id, result=result)\n\n    def _task_to_string(self, task: Task) -> str:\n        \"\"\"Convert a task to a string.\"\"\"\n        return f\"{task.id}: {task.description}\"\n\n    def get_incomplete_tasks(self) -> List[Task]:\n        \"\"\"Get the list of incomplete tasks.\"\"\"\n        return [task for task in self.tasks if not task.is_done]\n\n    def get_incomplete_tasks_string(self) -> str:\n        \"\"\"Get the list of incomplete tasks as a string.\"\"\"\n        result = \"\"\n        for task in self.get_incomplete_tasks():\n            result += self._task_to_string(task) + \"\\n\"\n        return result", ""]}
{"filename": "src/tools/base.py", "chunked_list": ["import inspect\nfrom pydantic import Field, Extra, validator, BaseModel\nfrom typing import Any, Callable, Dict\n\n\nclass AgentToolError(Exception):\n    pass\n\n\nclass AgentTool(BaseModel):\n    \"\"\"\n    Base class for agent tools.    \n    \"\"\"\n    name: str\n    description: str = Field(..., description=\"The description of the tool\")\n    func: Callable[[str], str] = Field(..., description=\"The function to execute\")\n    args: Dict[str, str] = Field(default={})\n    user_permission_required: bool = Field(\n        False, description=\"Whether the user permission is required before using this tool\")\n\n    class Config:\n        extra = Extra.allow\n\n    def run(self, **kwargs: Any) -> str:\n        \"\"\"Run the tool.\"\"\"\n        try:\n            result = self.func(**kwargs)\n        except (Exception, KeyboardInterrupt) as e:\n            raise AgentToolError(str(e))\n        return result\n\n    def get_tool_info(self, include_args=True) -> str:\n        \"\"\"Get the tool info.\"\"\"\n        args_str = \", \".join([f\"{k}: <{v}>\" for k, v in self.args.items()])\n\n        if include_args:\n            return f\"\"\"{self.name}: \"{self.description}\", args: {args_str}\"\"\"\n        else:\n            return f\"\"\"{self.name}: \"{self.description}\"\"\"\n\n    @property\n    def args(self) -> Dict:\n        \"\"\"Get the argument name and argument type from the signature\"\"\"\n        func_signature = inspect.signature(self.func)\n        required_args = {}\n\n        for param in func_signature.parameters.values():\n            param_name = str(param.name)\n            required_args[param_name] = f\"<{param_name}>\"\n\n        return required_args\n\n    @validator(\"name\")\n    def name_to_snake_case(name: str):\n        \"\"\"Convert the name to snake case.\"\"\"\n        if name is None:\n            raise AttributeError(\"NoneType object has no attribute\")\n\n        s = str(name).strip()\n        if not s:\n            raise IndexError(\"Empty string\")\n\n        # Convert all uppercase letters to lowercase.\n        s = s.lower()\n\n        # Replace spaces, dashes, and other separators with underscores.\n        s = s.replace(' ', '_')\n        s = s.replace('-', '_')\n\n        # Remove all characters that are not alphanumeric or underscore.\n        s = ''.join(c for c in s if c.isalnum() or c == '_')\n\n        # Replace multiple consecutive underscores with a single underscore.\n        s = '_'.join(filter(None, s.split('_')))\n\n        return s", "\nclass AgentTool(BaseModel):\n    \"\"\"\n    Base class for agent tools.    \n    \"\"\"\n    name: str\n    description: str = Field(..., description=\"The description of the tool\")\n    func: Callable[[str], str] = Field(..., description=\"The function to execute\")\n    args: Dict[str, str] = Field(default={})\n    user_permission_required: bool = Field(\n        False, description=\"Whether the user permission is required before using this tool\")\n\n    class Config:\n        extra = Extra.allow\n\n    def run(self, **kwargs: Any) -> str:\n        \"\"\"Run the tool.\"\"\"\n        try:\n            result = self.func(**kwargs)\n        except (Exception, KeyboardInterrupt) as e:\n            raise AgentToolError(str(e))\n        return result\n\n    def get_tool_info(self, include_args=True) -> str:\n        \"\"\"Get the tool info.\"\"\"\n        args_str = \", \".join([f\"{k}: <{v}>\" for k, v in self.args.items()])\n\n        if include_args:\n            return f\"\"\"{self.name}: \"{self.description}\", args: {args_str}\"\"\"\n        else:\n            return f\"\"\"{self.name}: \"{self.description}\"\"\"\n\n    @property\n    def args(self) -> Dict:\n        \"\"\"Get the argument name and argument type from the signature\"\"\"\n        func_signature = inspect.signature(self.func)\n        required_args = {}\n\n        for param in func_signature.parameters.values():\n            param_name = str(param.name)\n            required_args[param_name] = f\"<{param_name}>\"\n\n        return required_args\n\n    @validator(\"name\")\n    def name_to_snake_case(name: str):\n        \"\"\"Convert the name to snake case.\"\"\"\n        if name is None:\n            raise AttributeError(\"NoneType object has no attribute\")\n\n        s = str(name).strip()\n        if not s:\n            raise IndexError(\"Empty string\")\n\n        # Convert all uppercase letters to lowercase.\n        s = s.lower()\n\n        # Replace spaces, dashes, and other separators with underscores.\n        s = s.replace(' ', '_')\n        s = s.replace('-', '_')\n\n        # Remove all characters that are not alphanumeric or underscore.\n        s = ''.join(c for c in s if c.isalnum() or c == '_')\n\n        # Replace multiple consecutive underscores with a single underscore.\n        s = '_'.join(filter(None, s.split('_')))\n\n        return s", ""]}
{"filename": "src/tools/__init__.py", "chunked_list": [""]}
{"filename": "src/llm/list_output_parser.py", "chunked_list": ["import re\nfrom typing import List\nfrom pydantic import BaseModel\n\n\nclass LLMListOutputParserException(Exception):\n    \"\"\"Exception for List parsing errors\"\"\"\n    pass\n\n\nclass ParseListException(LLMListOutputParserException):\n    \"\"\"Exception for List parsing errors\"\"\"\n    pass", "\n\nclass ParseListException(LLMListOutputParserException):\n    \"\"\"Exception for List parsing errors\"\"\"\n    pass\n\n\nclass LLMListOutputParser(BaseModel):\n    @classmethod\n    def parse(cls, string_list: str, separeted_string=\",\") -> List[str]:\n        \"\"\"\n        Parses the string list and returns a list of strings.\n        \"\"\"\n        if not string_list:\n            return []\n\n        # Remove square brackets\n        string_list = cls._remove_square_brackets(string_list)\n\n        # Split by comma and convert to list\n        parsed_list = string_list.split(separeted_string)\n\n        # If the string is not comma-separated, raise ValueError\n        if len(parsed_list) == 1 and not parsed_list[0]:\n            raise ParseListException(f\"The string is not {separeted_string}-separated.\")\n\n        return parsed_list\n\n    @staticmethod\n    def _remove_square_brackets(string_list: str) -> str:\n        \"\"\"\n        Removes square brackets from the string.\n        \"\"\"\n        return re.sub(r\"\\[|\\]\", \"\", string_list)", ""]}
{"filename": "src/llm/json_output_parser.py", "chunked_list": ["import json\nimport re\nfrom typing import Any, Dict, Union, List\nfrom pydantic import BaseModel\nfrom jsonschema import validate, ValidationError\nfrom langchain.llms.base import BaseLLM\nimport contextlib\nfrom marvin import ai_fn\n\n\nclass LLMJsonOutputParserException(Exception):\n    \"\"\"Exception for JSON parsing errors\"\"\"\n    pass", "\n\nclass LLMJsonOutputParserException(Exception):\n    \"\"\"Exception for JSON parsing errors\"\"\"\n    pass\n\n\nclass ParseJsonException(LLMJsonOutputParserException):\n    \"\"\"Exception for JSON parsing errors\"\"\"\n    pass", "\n\nclass ValidateJsonException(LLMJsonOutputParserException):\n    \"\"\"Exception for JSON validating errors\"\"\"\n    pass\n\n\nclass FixJsonException(LLMJsonOutputParserException):\n    \"\"\"Exception for JSON fixing errors\"\"\"\n    pass", "\n\n@ai_fn()\ndef auto_fix_json(json_str: str, schema: str) -> str:\n    \"\"\"\n    Fixes the provided JSON string to make it parseable and fully complient with the provided schema.\n    If an object or field specified in the schema isn't contained within the correct JSON,\n    it is ommited.\\n This function is brilliant at guessing  when the format is incorrect.\n\n    Parameters:\n    description: str\n        The description of the function\n    function: str\n        The function to run\n\n    Returns:\n    str\n        The fixed JSON string it is valid.\n    \"\"\"", "\n\nclass LLMJsonOutputParser(BaseModel):\n    \"\"\"Parse the output of the LLM.\"\"\"\n    @classmethod\n    def parse_and_validate(cls, json_str: str, json_schema: str, llm: BaseLLM) -> Union[str, Dict[Any, Any]]:\n        \"\"\"\n        Parses and validates the JSON string.\n        \"\"\"\n        # Parse JSON\n        try:\n            json_str = cls._parse_json(json_str, json_schema, llm)\n        except ParseJsonException as e:\n            raise ParseJsonException(str(e))\n\n        # Validate JSON\n        try:\n            return cls._validate_json(json_str, json_schema, llm)\n        except ValidationError as e:\n            raise ValidateJsonException(str(e))\n\n    @classmethod\n    def _remove_square_brackets(cls, json_str: str) -> str:\n        \"\"\"\n        Removes square brackets from the JSON string.\n        \"\"\"\n        return re.sub(r\"\\[|\\]\", \"\", json_str)\n\n    @classmethod\n    def _parse_json(cls, json_str: str,  json_schema: str, llm: BaseLLM) -> Union[str, Dict[Any, Any]]:\n        \"\"\"\n        Parses the JSON string.\n        \"\"\"\n        with contextlib.suppress(json.JSONDecodeError):\n            json_str = json_str.replace(\"\\t\", \"\")\n            return json.loads(json_str)\n\n        with contextlib.suppress(json.JSONDecodeError):\n            json_str = cls.correct_json(json_str)\n            return json.loads(json_str)\n\n        try:\n            json_str = cls._remove_square_brackets(json_str)\n            brace_index = json_str.index(\"{\")\n            maybe_fixed_json = json_str[brace_index:]\n            last_brace_index = maybe_fixed_json.rindex(\"}\")\n            maybe_fixed_json = maybe_fixed_json[: last_brace_index + 1]\n            return json.loads(maybe_fixed_json)\n        except (json.JSONDecodeError, ValueError):\n            pass\n        # Now try to fix this up using the ai_functions\n        try:\n            ai_fixed_json = cls._fix_json(json_str, json_schema, llm)\n            return json.loads(ai_fixed_json)\n        except FixJsonException as e:\n            raise ParseJsonException(\"Could not parse JSON:\" + str(e))\n\n    @classmethod\n    def _validate_json(cls, json_obj: Union[str, Dict[Any, Any]], json_schema: str, llm: BaseLLM) -> Union[str, Dict[Any, Any]]:\n        \"\"\"\n        Check if the given JSON string is fully complient with the provided schema.\n        \"\"\"\n        schema_obj = json.loads(json_schema)\n        try:\n            validate(json_obj, schema_obj)\n            return json_obj\n        except ValidationError:\n            # Now try to fix this up using the ai_functions\n            try:\n                ai_fixed_json = cls._fix_json(json.dumps(json_obj), json_schema, llm)\n                return json.loads(ai_fixed_json)\n            except FixJsonException as e:\n                raise ValidateJsonException(\"Could not validate JSON:\" + str(e))\n\n    @staticmethod\n    def _fix_json(json_str: str, schema: str, llm: BaseLLM) -> str:\n        \"\"\"\n        Fix the given JSON string to make it parseable and fully complient with the provided schema.\n        \"\"\"\n        try:\n            fixed_json_str = auto_fix_json(json_str, schema)\n        except Exception as e:\n            raise FixJsonException(e)\n        try:\n            json.loads(fixed_json_str)\n            return fixed_json_str\n        except Exception:\n            import traceback\n            call_stack = traceback.format_exc()\n            raise FixJsonException(f\"Failed to fix JSON: '{json_str}' \" + call_stack)\n\n    @staticmethod\n    def _extract_char_position(error_message: str) -> int:\n        \"\"\"\n        Extract the character position from the error message.\n        \"\"\"\n        char_pattern = re.compile(r'\\(char (\\d+)\\)')\n        if match := char_pattern.search(error_message):\n            return int(match[1])\n        else:\n            raise ValueError(\"Character position not found in the error message.\")\n\n    @staticmethod\n    def _add_quotes_to_property_names(json_string: str) -> str:\n        \"\"\"\n        Add quotes to the property names in the JSON string.\n        \"\"\"\n        def replace_func(match):\n            return f'\"{match.group(1)}\":'\n\n        property_name_pattern = re.compile(r'(\\w+):')\n        corrected_json_string = property_name_pattern.sub(\n            replace_func,\n            json_string)\n\n        try:\n            json.loads(corrected_json_string)\n            return corrected_json_string\n        except json.JSONDecodeError as e:\n            raise e\n\n    @staticmethod\n    def _balance_braces(json_string: str) -> str:\n        \"\"\"\n        Add missing braces to the end of the JSON string.\n        \"\"\"\n        open_braces_count = json_string.count(\"{\")\n        close_braces_count = json_string.count(\"}\")\n\n        while open_braces_count > close_braces_count:\n            json_string += \"}\"\n            close_braces_count += 1\n\n        while close_braces_count > open_braces_count:\n            json_string = json_string.rstrip(\"}\")\n            close_braces_count -= 1\n\n        with contextlib.suppress(json.JSONDecodeError):\n            json.loads(json_string)\n            return json_string\n\n    @classmethod\n    def _fix_invalid_escape(cls, json_str: str, error_message: str) -> str:\n        \"\"\"\n        Remove the invalid escape character from the JSON string.\n        \"\"\"\n        while error_message.startswith('Invalid \\\\escape'):\n            bad_escape_location = cls._extract_char_position(error_message)\n            json_str = json_str[:bad_escape_location] + \\\n                json_str[bad_escape_location + 1:]\n            try:\n                json.loads(json_str)\n                return json_str\n            except json.JSONDecodeError as e:\n                error_message = str(e)\n        return json_str\n\n    @classmethod\n    def correct_json(cls, json_str: str) -> str:\n        \"\"\"\n        Correct the given JSON string to make it parseable.\n        \"\"\"\n        try:\n            json.loads(json_str)\n            return json_str\n        except json.JSONDecodeError as e:\n            error_message = str(e)\n            if error_message.startswith('Invalid \\\\escape'):\n                json_str = cls._fix_invalid_escape(json_str, error_message)\n            if error_message.startswith('Expecting property name enclosed in double quotes'):\n                json_str = cls._add_quotes_to_property_names(json_str)\n                try:\n                    json.loads(json_str)\n                    return json_str\n                except json.JSONDecodeError as e:\n                    error_message = str(e)\n            if balanced_str := cls._balance_braces(json_str):\n                return balanced_str\n        return json_str", ""]}
{"filename": "src/llm/extract_entity/schema.py", "chunked_list": ["class JsonSchema:\n    schema = {\n        \"entity1\": \"description of entity1. Please describe the entities using sentences rather than single words.\",\n        \"entity2\": \"description of entity2. Please describe the entities using sentences rather than single words.\",\n        \"entity3\": \"description of entity3. Please describe the entities using sentences rather than single words.\"\n    }\n"]}
{"filename": "src/llm/extract_entity/__init__.py", "chunked_list": [""]}
{"filename": "src/llm/extract_entity/prompt.py", "chunked_list": ["import json\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate\nfrom llm.extract_entity.schema import JsonSchema\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n)\n\n# Convert the schema object to a string\nJSON_SCHEMA_STR = json.dumps(JsonSchema.schema)", "# Convert the schema object to a string\nJSON_SCHEMA_STR = json.dumps(JsonSchema.schema)\n\nENTITY_EXTRACTION_TEMPLATE = \"\"\"\n    You are an AI assistant reading a input text and trying to extract entities from it.\n    Extract ONLY proper nouns from the input text and return them as a JSON object.\n    You should definitely extract all names and places.\n\n    [EXAMPLE]\n    INPUT TEXT:", "    [EXAMPLE]\n    INPUT TEXT:\n     Apple Computer was founded on April 1, 1976, by Steve Wozniak, Steve Jobs and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. \n     It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. \n     The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. \n     Apple Computer went public in 1980 to instant financial success. \n     The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement. \n     By 1985, the high cost of its products, and power struggles between executives, caused problems.\n     Wozniak stepped back from Apple Computer amicably and pursued other ventures, while Jobs resigned bitterly and founded NeXT, taking some Apple Computer employees with him.\n    RESPONCE:", "     Wozniak stepped back from Apple Computer amicably and pursued other ventures, while Jobs resigned bitterly and founded NeXT, taking some Apple Computer employees with him.\n    RESPONCE:\n     {{\n        \"Apple Computer Company\": \"a company founded in 1976 by Steve Wozniak, Steve Jobs, and Ronald Wayne to develop and sell personal computers\",\n        \"Steve Wozniak\": \"an American inventor, electronics engineer, programmer, philanthropist, and technology entrepreneur who co-founded Apple Computer Company with Steve Jobs\",\n        \"Steve Jobs\": \"an American entrepreneur, business magnate, inventor, and industrial designer who co-founded Apple Computer Company with Steve Wozniak and Ronald Wayne, and later founded NeXT\",\n        \"Ronald Wayne\": \"an American retired electronics industry worker and co-founder of Apple Computer Company, who left the company after only 12 days\"\n    }}\n    [INPUT TEXT] (for reference only):\n    {text}", "    [INPUT TEXT] (for reference only):\n    {text}\n    \"\"\"\n\nSCHEMA_TEMPLATE = f\"\"\"\n    [RULE]\n    Your response must be provided exclusively in the JSON format outlined below, without any exceptions. \n    Any additional text, explanations, or apologies outside of the JSON structure will not be accepted. \n    Please ensure the response adheres to the specified format and can be successfully parsed by Python's json.loads function.\n", "    Please ensure the response adheres to the specified format and can be successfully parsed by Python's json.loads function.\n\n    Strictly adhere to this JSON RESPONSE FORMAT for your response.\n    Failure to comply with this format will result in an invalid response. \n    Please ensure your output strictly follows RESPONSE FORMAT.\n\n    [JSON RESPONSE FORMAT]\n    {JSON_SCHEMA_STR}\n\n    [RESPONSE]\"\"\".replace(\"{\", \"{{\").replace(\"}\", \"}}\")", "\n    [RESPONSE]\"\"\".replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\n\ndef get_template() -> PromptTemplate:\n    template = f\"{ENTITY_EXTRACTION_TEMPLATE}\\n{SCHEMA_TEMPLATE}\"\n    return PromptTemplate(input_variables=[\"text\"], template=template)\n\n\ndef get_chat_template() -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(\n        ENTITY_EXTRACTION_TEMPLATE))\n    messages.append(SystemMessagePromptTemplate.from_template(SCHEMA_TEMPLATE))\n    return ChatPromptTemplate.from_messages(messages)", "\ndef get_chat_template() -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(\n        ENTITY_EXTRACTION_TEMPLATE))\n    messages.append(SystemMessagePromptTemplate.from_template(SCHEMA_TEMPLATE))\n    return ChatPromptTemplate.from_messages(messages)\n"]}
{"filename": "src/llm/generate_task_plan/prompt.py", "chunked_list": ["from langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\n\n# Convert the schema object to a string\n\nBASE_TEMPLATE = \"\"\"", "\nBASE_TEMPLATE = \"\"\"\nYou are {name}, {role}\nYour should create task that uses the result of an execution agent\nto create new tasks with the following GOAL:\n\n[GOAL]\n{goal}\n\n[YOUR MISSION]", "\n[YOUR MISSION]\nBased on the [GOAL], create new tasks to be completed by the AI system that do not overlap with incomplete tasks.\n- Tasks should be calculated backward from the GOAL, and effective arrangements should be made.\n- You can create any number of new tasks.\n\n[RESPONSE FORMAT]\nReturn the tasks as a list of string.\n- Enclose each task in double quotation marks.\n- Separate tasks with Tabs.", "- Enclose each task in double quotation marks.\n- Separate tasks with Tabs.\n- Use [] only at the beginning and end\n\n[\"Task 1 that the AI assistant should perform\"\\t\"Task 2 that the AI assistant should perform\",\\t ...]\n\n[RESPONSE]\n\"\"\"\n\n\ndef get_template() -> PromptTemplate:\n    template = BASE_TEMPLATE\n    PROMPT = PromptTemplate(\n        input_variables=[\"name\", \"role\", \"goal\"], template=template)\n    return PROMPT", "\n\ndef get_template() -> PromptTemplate:\n    template = BASE_TEMPLATE\n    PROMPT = PromptTemplate(\n        input_variables=[\"name\", \"role\", \"goal\"], template=template)\n    return PROMPT\n\n\ndef get_chat_template() -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(BASE_TEMPLATE))\n    return ChatPromptTemplate.from_messages(messages)", "\ndef get_chat_template() -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(BASE_TEMPLATE))\n    return ChatPromptTemplate.from_messages(messages)\n"]}
{"filename": "src/llm/summarize/prompt.py", "chunked_list": ["\nfrom typing import List\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,", "from langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\n# Convert the schema object to a string\nBASE_TEMPLATE = \"\"\"\n[THOUGHTS]\n{thoughts}", "[THOUGHTS]\n{thoughts}\n\n[ACTION]\n{action}\n\n[RESULT OF ACTION]\n{result}\n\n[INSTRUSCTION]", "\n[INSTRUSCTION]\nUsing above [THOUGHTS], [ACTION], and [RESULT OF ACTION], please summarize the event.\n\n[SUMMARY]\n\"\"\"\n\n\ndef get_template() -> PromptTemplate:\n    template = BASE_TEMPLATE\n    prompt_template = PromptTemplate(\n        input_variables=[\"thoughts\", \"action\", \"result\"], template=template)\n    return prompt_template", "def get_template() -> PromptTemplate:\n    template = BASE_TEMPLATE\n    prompt_template = PromptTemplate(\n        input_variables=[\"thoughts\", \"action\", \"result\"], template=template)\n    return prompt_template\n\n\ndef get_chat_templatez() -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(BASE_TEMPLATE))\n    return ChatPromptTemplate.from_messages(messages)", ""]}
{"filename": "src/llm/reason/schema.py", "chunked_list": ["class JsonSchema:\n    schema = {\n        \"observation\": \"observation of [RECENT EPISODES]\",\n        \"thoughts\": {\n            \"task\": \"description of [YOUR TASK] assigned to you\",\n            \"knowledge\": \"if there is any helpful knowledge in [RELATED KNOWLEDGE] for the task, summarize the key points here\",\n            \"past_events\": \"if there is any helpful past events in [RELATED PAST EPISODES] for the task, summarize the key points here\",\n            \"idea\": \"thought to perform the task\",\n            \"reasoning\": \"reasoning of the thought\",\n            \"criticism\": \"constructive self-criticism\",\n            \"summary\": \"thoughts summary to say to user\"\n        },\n        \"action\": {\n            \"tool_name\": \"One of the tool names included in [TOOLS]\",\n            \"args\": {\n                \"arg name\": \"value\",\n                \"arg name\": \"value\"\n            }\n        }\n    }", ""]}
{"filename": "src/llm/reason/__init__.py", "chunked_list": [""]}
{"filename": "src/llm/reason/prompt.py", "chunked_list": ["# flake8: noqa\nimport json\nimport time\nfrom langchain.prompts import PromptTemplate\nfrom pydantic import Field\nfrom typing import List\nfrom memory.episodic_memory import Episode\nfrom llm.reason.schema import JsonSchema\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (", "from langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage", "    HumanMessage,\n    SystemMessage\n)\n\n\n# Convert the schema object to a string\nJSON_SCHEMA_STR = json.dumps(JsonSchema.schema)\n\nBASE_TEMPLATE = \"\"\"\n", "BASE_TEMPLATE = \"\"\"\n\nYou are {name}, {role}\nYour decisions must always be made independently without seeking user assistance. \nPlay to your strengths as an LLM and pursue simple strategies with no legal complications.\n\n[GOAL]\n{goal}\n\n[PERFORMANCE EVALUATION]", "\n[PERFORMANCE EVALUATION]\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n\n[RELATED KNOWLEDGE] \nThis reminds you of related knowledge:\n{related_knowledge}\n", "{related_knowledge}\n\n\n[RELATED PAST EPISODES]\nThis reminds you of related past events:\n{related_past_episodes}\n\n\n[YOUR TASK]\nYou are given the following task:", "[YOUR TASK]\nYou are given the following task:\n{task}\n\n[TOOLS]\nYou can ONLY ONE TOOL at a time\ntool name: \"tool description\", arg1: <arg1>, arg2: <arg2>\n{tool_info}\ntask_complete: \"If you think you have completed the task, please use this tool to mark it as done and include your answer to the task in the 'args' field.\", result: <Answer to the assigned task>\n\"\"\"", "task_complete: \"If you think you have completed the task, please use this tool to mark it as done and include your answer to the task in the 'args' field.\", result: <Answer to the assigned task>\n\"\"\"\n\nRECENT_EPISODES_TEMPLETE = \"\"\"\n[RECENT EPISODES]\nThis reminds you of recent events:\n\"\"\"\n\nSCHEMA_TEMPLATE = f\"\"\"\n[RULE]", "SCHEMA_TEMPLATE = f\"\"\"\n[RULE]\nYour response must be provided exclusively in the JSON format outlined below, without any exceptions. \nAny additional text, explanations, or apologies outside of the JSON structure will not be accepted. \nPlease ensure the response adheres to the specified format and can be successfully parsed by Python's json.loads function.\n\nStrictly adhere to this JSON RESPONSE FORMAT for your response:\nFailure to comply with this format will result in an invalid response. \nPlease ensure your output strictly follows JSON RESPONSE FORMAT.\n", "Please ensure your output strictly follows JSON RESPONSE FORMAT.\n\n[JSON RESPONSE FORMAT]\n{JSON_SCHEMA_STR}\n\nDetermine which next command to use, and respond using the format specified above:\n\"\"\".replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\n\ndef get_template(memory: List[Episode] = None) -> PromptTemplate:\n    template = BASE_TEMPLATE\n\n    # If there are past conversation logs, append them\n    if len(memory) > 0:\n        # insert current time and date\n        recent_episodes = RECENT_EPISODES_TEMPLETE\n        RECENT_EPISODES_TEMPLETE += f\"The current time and date is {time.strftime('%c')}\"\n\n        # insert past conversation logs\n        for episode in memory:\n            thoughts_str = json.dumps(episode.thoughts)\n            action_str = json.dumps(episode.action)\n            result = episode.result\n            recent_episodes += thoughts_str + \"/n\" + action_str + \"/n\" + result + \"/n\"\n\n        template += recent_episodes\n\n    template += SCHEMA_TEMPLATE\n\n    PROMPT = PromptTemplate(\n        input_variables=[\"name\", \"role\", \"goal\", \"related_knowledge\", \"related_past_episodes\", \"task\", \"tool_info\"], template=template)\n\n    return PROMPT", "\ndef get_template(memory: List[Episode] = None) -> PromptTemplate:\n    template = BASE_TEMPLATE\n\n    # If there are past conversation logs, append them\n    if len(memory) > 0:\n        # insert current time and date\n        recent_episodes = RECENT_EPISODES_TEMPLETE\n        RECENT_EPISODES_TEMPLETE += f\"The current time and date is {time.strftime('%c')}\"\n\n        # insert past conversation logs\n        for episode in memory:\n            thoughts_str = json.dumps(episode.thoughts)\n            action_str = json.dumps(episode.action)\n            result = episode.result\n            recent_episodes += thoughts_str + \"/n\" + action_str + \"/n\" + result + \"/n\"\n\n        template += recent_episodes\n\n    template += SCHEMA_TEMPLATE\n\n    PROMPT = PromptTemplate(\n        input_variables=[\"name\", \"role\", \"goal\", \"related_knowledge\", \"related_past_episodes\", \"task\", \"tool_info\"], template=template)\n\n    return PROMPT", "\n\ndef get_chat_template(memory: List[Episode] = None) -> ChatPromptTemplate:\n    messages = []\n    messages.append(SystemMessagePromptTemplate.from_template(BASE_TEMPLATE))\n\n    # If there are past conversation logs, append them\n    if len(memory) > 0:\n        # insert current time and date\n        recent_episodes = RECENT_EPISODES_TEMPLETE\n        recent_episodes += f\"The current time and date is {time.strftime('%c')}\"\n\n        # insert past conversation logs\n        for episode in memory:\n            thoughts_str = json.dumps(episode.thoughts)\n            action_str = json.dumps(episode.action)\n            result = episode.result\n            recent_episodes += thoughts_str + \"/n\" + action_str + \"/n\" + result + \"/n\"\n\n        messages.append(SystemMessage(content=recent_episodes))\n    messages.append(SystemMessage(content=SCHEMA_TEMPLATE))\n\n    return ChatPromptTemplate.from_messages(messages)", ""]}
{"filename": "src/ui/base.py", "chunked_list": ["from pydantic import BaseModel, Extra\nfrom abc import abstractmethod\nfrom typing import ContextManager\n\n\nclass BaseHumanUserInterface(BaseModel):\n    \"\"\" Base class for human user interface.\"\"\"\n    class Config:\n        extra = Extra.forbid\n\n    @abstractmethod\n    def get_user_input(self) -> str:\n        # waiting for user input\n        pass\n\n    @abstractmethod\n    def get_binary_user_input(self, message: str) -> bool:\n        # get user permission\n        pass\n\n    @abstractmethod\n    def notify(self, title: str, message: str) -> None:\n        # notify user\n        pass\n\n    @abstractmethod\n    def loading(self) -> ContextManager:\n        # waiting for AI to respond\n        pass", ""]}
{"filename": "src/ui/__init__.py", "chunked_list": [""]}
{"filename": "src/ui/cui.py", "chunked_list": ["import itertools\nimport sys\nimport threading\nimport time\nfrom enum import Enum\nfrom typing import ContextManager, Union\nfrom ui.base import BaseHumanUserInterface\n\n\nclass Color(Enum):\n    \"\"\"Color codes for the commandline\"\"\"\n    BLACK = '\\033[30m'  # (Text) Black\n    RED = '\\033[31m'  # (Text) Red\n    GREEN = '\\033[32m'  # (Text) Green\n    YELLOW = '\\033[33m'  # (Text) Yellow\n    BLUE = '\\033[34m'  # (Text) Blue\n    MAGENTA = '\\033[35m'  # (Text) Magenta\n    CYAN = '\\033[36m'  # (Text) Cyan\n    WHITE = '\\033[37m'  # (Text) White\n    COLOR_DEFAULT = '\\033[39m'  # Reset text color to default", "\nclass Color(Enum):\n    \"\"\"Color codes for the commandline\"\"\"\n    BLACK = '\\033[30m'  # (Text) Black\n    RED = '\\033[31m'  # (Text) Red\n    GREEN = '\\033[32m'  # (Text) Green\n    YELLOW = '\\033[33m'  # (Text) Yellow\n    BLUE = '\\033[34m'  # (Text) Blue\n    MAGENTA = '\\033[35m'  # (Text) Magenta\n    CYAN = '\\033[36m'  # (Text) Cyan\n    WHITE = '\\033[37m'  # (Text) White\n    COLOR_DEFAULT = '\\033[39m'  # Reset text color to default", "\n\nclass CommandlineUserInterface(BaseHumanUserInterface):\n    \"\"\"Commandline user interface.\"\"\"\n\n    def get_user_input(self) -> str:\n        \"\"\"Get user input and return the result as a string\"\"\"\n        user_input = input(\"Input:\")\n        return str(user_input)\n\n    def get_binary_user_input(self, prompt: str) -> bool:\n        \"\"\"Get a binary input from the user and return the result as a bool\"\"\"\n        yes_patterns = [\"y\", \"yes\", \"yeah\", \"yup\", \"yep\"]\n        no_patterns = [\"n\", \"no\", \"nah\", \"nope\"]\n        while True:\n            response = input(prompt + \" (y/n) \").strip().lower()\n            if response in yes_patterns:\n                return True\n            elif response in no_patterns:\n                return False\n            else:\n                self.notify(\"Invalid input\", \"Please enter y or n.\",\n                            title_color=Color.RED)\n                continue\n\n    def notify(self, title: str, message: str, title_color: Union[str, Color] = Color.YELLOW) -> None:\n        \"\"\"Print a notification to the user\"\"\"\n        if isinstance(title_color, str):\n            try:\n                title_color = Color[title_color.upper()]\n            except KeyError:\n                raise ValueError(f\"{title_color} is not a valid Color\")\n        self._print_message(title, message, title_color)\n\n    def loading(self,\n                message: str = \"Thinking...\",\n                delay: float = 0.1) -> ContextManager:\n        \"\"\"Return a context manager that will display a loading spinner\"\"\"\n        return self.Spinner(message=message, delay=delay)\n\n    def _print_message(self, title: str, message: str, title_color: Color) -> None:\n        print(f\"{title_color.value}{title}{Color.COLOR_DEFAULT.value}: {message}\")\n\n    class Spinner:\n        \"\"\"A simple spinner class\"\"\"\n\n        def __init__(self, message=\"Loading...\", delay=0.1):\n            \"\"\"Initialize the spinner class\"\"\"\n            self.spinner = itertools.cycle(['-', '/', '|', '\\\\'])\n            self.delay = delay\n            self.message = message\n            self.running = False\n            self.spinner_thread = None\n\n        def spin(self):\n            \"\"\"Spin the spinner\"\"\"\n            while self.running:\n                sys.stdout.write(next(self.spinner) + \" \" + self.message + \"\\r\")\n                sys.stdout.flush()\n                time.sleep(self.delay)\n                sys.stdout.write('\\b' * (len(self.message) + 2))\n\n        def __enter__(self):\n            \"\"\"Start the spinner\"\"\"\n            self.running = True\n            self.spinner_thread = threading.Thread(target=self.spin)\n            self.spinner_thread.start()\n\n        def __exit__(self, exc_type, exc_value, exc_traceback):\n            \"\"\"Stop the spinner\"\"\"\n            self.running = False\n            self.spinner_thread.join()\n            sys.stdout.write('\\r' + ' ' * (len(self.message) + 2) + '\\r')\n            sys.stdout.flush()", ""]}
{"filename": "src/memory/semantic_memory.py", "chunked_list": ["import json\nfrom typing import Any, Optional\nfrom pydantic import BaseModel, Field\nfrom langchain.llms.base import BaseLLM\nfrom langchain import LLMChain\nfrom langchain.vectorstores import VectorStore, FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom llm.extract_entity.prompt import get_template, get_chat_template\nfrom llm.extract_entity.schema import JsonSchema as ENTITY_EXTRACTION_SCHEMA", "from llm.extract_entity.prompt import get_template, get_chat_template\nfrom llm.extract_entity.schema import JsonSchema as ENTITY_EXTRACTION_SCHEMA\nfrom llm.json_output_parser import LLMJsonOutputParser, LLMJsonOutputParserException\n\nCREATE_JSON_SCHEMA_STR = json.dumps(ENTITY_EXTRACTION_SCHEMA.schema)\n\n\nclass SemanticMemory(BaseModel):\n    num_episodes: int = Field(0, description=\"The number of episodes\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n    openaichat: Optional[ChatOpenAI] = Field(\n        None, description=\"ChatOpenAI class for the agent\")\n    embeddings: HuggingFaceEmbeddings = Field(\n        HuggingFaceEmbeddings(), title=\"Embeddings to use for tool retrieval\")\n    vector_store: VectorStore = Field(\n        None, title=\"Vector store to use for tool retrieval\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def extract_entity(self, text: str) -> dict:\n        \"\"\"Extract an entity from a text using the LLM\"\"\"\n        if self.openaichat:\n            # If OpenAI Chat is available, it is used for higher accuracy results.\n            propmt = get_chat_template().format_prompt(text=text).to_messages()\n            result = self.openaichat(propmt).content\n        else:\n            # Get the result from the LLM\n            llm_chain = LLMChain(prompt=get_template(), llm=self.llm)\n            try:\n                result = llm_chain.predict(text=text)\n            except Exception as e:\n                raise Exception(f\"Error: {e}\")\n\n        # Parse and validate the result\n        try:\n            result_json_obj = LLMJsonOutputParser.parse_and_validate(\n                json_str=result,\n                json_schema=CREATE_JSON_SCHEMA_STR,\n                llm=self.llm\n            )\n        except LLMJsonOutputParserException as e:\n            raise LLMJsonOutputParserException(str(e))\n        else:\n            self._embed_knowledge(result_json_obj)\n            return result_json_obj\n\n    def remember_related_knowledge(self, query: str, k: int = 5) -> dict:\n        \"\"\"Remember relevant knowledge for a query.\"\"\"\n        if self.vector_store is None:\n            return {}\n        relevant_documents = self.vector_store.similarity_search(query, k=k)\n        return {d.metadata[\"entity\"]: d.metadata[\"description\"] for d in relevant_documents}\n\n    def _embed_knowledge(self, entity: dict[str:Any]) -> None:\n        \"\"\"Embed the knowledge into the vector store.\"\"\"\n        description_list = []\n        metadata_list = []\n\n        for entity, description in entity.items():\n            description_list.append(description)\n            metadata_list.append({\"entity\": entity, \"description\": description})\n\n        if self.vector_store is None:\n            self.vector_store = FAISS.from_texts(\n                texts=description_list,\n                metadatas=metadata_list,\n                embedding=self.embeddings\n            )\n        else:\n            self.vector_store.add_texts(\n                texts=description_list,\n                metadatas=metadata_list\n            )\n\n    def save_local(self, path: str) -> None:\n        \"\"\"Save the vector store to a local folder.\"\"\"\n        self.vector_store.save_local(folder_path=path)\n\n    def load_local(self, path: str) -> None:\n        \"\"\"Load the vector store from a local folder.\"\"\"\n        self.vector_store = FAISS.load_local(\n            folder_path=path, embeddings=self.embeddings)", ""]}
{"filename": "src/memory/procedual_memory.py", "chunked_list": ["from pydantic import BaseModel, Field\nfrom langchain.vectorstores import FAISS\nfrom langchain.vectorstores import VectorStore\nfrom langchain.schema import Document\nfrom pydantic import BaseModel, Field\nfrom langchain.vectorstores import VectorStore, FAISS\nfrom langchain.schema import Document\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom typing import List\nfrom tools.base import AgentTool", "from typing import List\nfrom tools.base import AgentTool\n\n\nclass ProcedualMemoryException(Exception):\n    pass\n\n\nclass ToolNotFoundException(ProcedualMemoryException):\n    pass", "class ToolNotFoundException(ProcedualMemoryException):\n    pass\n\n\nclass ProcedualMemory(BaseModel):\n    tools: List[AgentTool] = Field([], title=\"hoge\")\n    embeddings: HuggingFaceEmbeddings = Field(\n        HuggingFaceEmbeddings(), title=\"Embeddings to use for tool retrieval\")\n    docs: List[Document] = Field([], title=\"Documents to use for tool retrieval\")\n    vector_store: VectorStore = Field(\n        None, title=\"Vector store to use for tool retrieval\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def memorize_tools(self, tools: List[AgentTool]) -> None:\n        \"\"\"Memorize tools and embed them.\"\"\"\n        for tool in tools:\n            self.tools.append(tool)\n            self.docs = [Document(page_content=t.description, metadata={\n                                  \"index\": i}) for i, t in enumerate(self.tools)]\n        self._embed_docs()\n\n    def remember_tool_by_name(self, tool_name: str) -> AgentTool:\n        \"\"\"Remember a tool by name and return it.\"\"\"\n        tool = [tool for tool in self.tools if tool.name.lower() == tool_name.lower()]\n\n        if tool:\n            return tool[0]\n        else:\n            raise ToolNotFoundException(f\"Tool {tool_name} not found\")\n\n    def remember_relevant_tools(self, query: str) -> List[AgentTool]:\n        \"\"\"Remember relevant tools for a query.\"\"\"\n        retriever = self.vector_store.as_retriever()\n        relevant_documents = retriever.get_relevant_documents(query)\n        return [self.tools[d.metadata[\"index\"]] for d in relevant_documents]\n\n    def remember_all_tools(self) -> List[AgentTool]:\n        \"\"\"Remember all tools and return them.\"\"\"\n        return self.tools\n\n    def _embed_docs(self) -> None:\n        \"\"\"Embed tools.\"\"\"\n        self.vector_store: FAISS = FAISS.from_documents(\n            self.docs, self.embeddings\n        )", ""]}
{"filename": "src/memory/__init__.py", "chunked_list": [""]}
{"filename": "src/memory/episodic_memory.py", "chunked_list": ["from typing import List, Dict, Any\nfrom pydantic import BaseModel, Field\nfrom langchain.llms.base import BaseLLM\nfrom langchain import LLMChain\nfrom langchain.vectorstores import VectorStore, FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom llm.summarize.prompt import get_template\n\n\nclass Episode(BaseModel):\n    thoughts: Dict[str, Any] = Field(..., description=\"thoughts of the agent\")\n    action: Dict[str, Any] = Field(..., description=\"action of the agent\")\n    result: str = Field(..., description=\"The plan of the event\")\n    summary: str = Field(\"\", description=\"summary of the event\")", "\nclass Episode(BaseModel):\n    thoughts: Dict[str, Any] = Field(..., description=\"thoughts of the agent\")\n    action: Dict[str, Any] = Field(..., description=\"action of the agent\")\n    result: str = Field(..., description=\"The plan of the event\")\n    summary: str = Field(\"\", description=\"summary of the event\")\n\n\nclass EpisodicMemory(BaseModel):\n    num_episodes: int = Field(0, description=\"The number of episodes\")\n    store: Dict[str, Episode] = Field({}, description=\"The list of episodes\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n    embeddings: HuggingFaceEmbeddings = Field(\n        HuggingFaceEmbeddings(), title=\"Embeddings to use for tool retrieval\")\n    vector_store: VectorStore = Field(\n        None, title=\"Vector store to use for tool retrieval\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def memorize_episode(self, episode: Episode) -> None:\n        \"\"\"Memorize an episode.\"\"\"\n        self.num_episodes += 1\n        self.store[str(self.num_episodes)] = episode\n        self._embed_episode(episode)\n\n    def summarize_and_memorize_episode(self, episode: Episode) -> str:\n        \"\"\"Summarize and memorize an episode.\"\"\"\n        summary = self._summarize(episode.thoughts, episode.action, episode.result)\n        episode.summary = summary\n        self.memorize_episode(episode)\n        return summary\n\n    def _summarize(self, thoughts: Dict[str, Any], action: Dict[str, Any], result: str) -> str:\n        \"\"\"Summarize an episode.\"\"\"\n        prompt = get_template()\n        llm_chain = LLMChain(prompt=prompt, llm=self.llm)\n        try:\n            result = llm_chain.predict(\n                thoughts=thoughts,\n                action=action,\n                result=result\n            )\n        except Exception as e:\n            raise Exception(f\"Error: {e}\")\n        return result\n\n    def remember_all_episode(self) -> List[Episode]:\n        \"\"\"Remember all episodes.\"\"\"\n        return self.store\n\n    def remember_recent_episodes(self, n: int = 5) -> List[Episode]:\n        \"\"\"Remember recent episodes.\"\"\"\n        if not self.store:  # if empty\n            return self.store\n        n = min(n, len(self.store))\n        return list(self.store.values())[-n:]\n\n    def remember_last_episode(self) -> Episode:\n        \"\"\"Remember last episode.\"\"\"\n        if not self.store:\n            return None\n        return self.store[-1]\n\n    def remember_related_episodes(self, query: str, k: int = 5) -> List[Episode]:\n        \"\"\"Remember related episodes to a query.\"\"\"\n        if self.vector_store is None:\n            return []\n        relevant_documents = self.vector_store.similarity_search(query, k=k)\n        result = []\n        for d in relevant_documents:\n            episode = Episode(\n                thoughts=d.metadata[\"thoughts\"],\n                action=d.metadata[\"action\"],\n                result=d.metadata[\"result\"],\n                summary=d.metadata[\"summary\"]\n            )\n            result.append(episode)\n        return result\n\n    def _embed_episode(self, episode: Episode) -> None:\n        \"\"\"Embed an episode and add it to the vector store.\"\"\"\n        texts = [episode.summary]\n        metadatas = [{\"index\": self.num_episodes,\n                      \"thoughts\": episode.thoughts,\n                      \"action\": episode.action,\n                      \"result\": episode.result,\n                      \"summary\": episode.summary}]\n        if self.vector_store is None:\n            self.vector_store = FAISS.from_texts(\n                texts=texts, embedding=self.embeddings, metadatas=metadatas)\n        else:\n            self.vector_store.add_texts(texts=texts, metadatas=metadatas)\n\n    def save_local(self, path: str) -> None:\n        \"\"\"Save the vector store locally.\"\"\"\n        self.vector_store.save_local(folder_path=path)\n\n    def load_local(self, path: str) -> None:\n        \"\"\"Load the vector store locally.\"\"\"\n        self.vector_store = FAISS.load_local(\n            folder_path=path, embeddings=self.embeddings)", "class EpisodicMemory(BaseModel):\n    num_episodes: int = Field(0, description=\"The number of episodes\")\n    store: Dict[str, Episode] = Field({}, description=\"The list of episodes\")\n    llm: BaseLLM = Field(..., description=\"llm class for the agent\")\n    embeddings: HuggingFaceEmbeddings = Field(\n        HuggingFaceEmbeddings(), title=\"Embeddings to use for tool retrieval\")\n    vector_store: VectorStore = Field(\n        None, title=\"Vector store to use for tool retrieval\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def memorize_episode(self, episode: Episode) -> None:\n        \"\"\"Memorize an episode.\"\"\"\n        self.num_episodes += 1\n        self.store[str(self.num_episodes)] = episode\n        self._embed_episode(episode)\n\n    def summarize_and_memorize_episode(self, episode: Episode) -> str:\n        \"\"\"Summarize and memorize an episode.\"\"\"\n        summary = self._summarize(episode.thoughts, episode.action, episode.result)\n        episode.summary = summary\n        self.memorize_episode(episode)\n        return summary\n\n    def _summarize(self, thoughts: Dict[str, Any], action: Dict[str, Any], result: str) -> str:\n        \"\"\"Summarize an episode.\"\"\"\n        prompt = get_template()\n        llm_chain = LLMChain(prompt=prompt, llm=self.llm)\n        try:\n            result = llm_chain.predict(\n                thoughts=thoughts,\n                action=action,\n                result=result\n            )\n        except Exception as e:\n            raise Exception(f\"Error: {e}\")\n        return result\n\n    def remember_all_episode(self) -> List[Episode]:\n        \"\"\"Remember all episodes.\"\"\"\n        return self.store\n\n    def remember_recent_episodes(self, n: int = 5) -> List[Episode]:\n        \"\"\"Remember recent episodes.\"\"\"\n        if not self.store:  # if empty\n            return self.store\n        n = min(n, len(self.store))\n        return list(self.store.values())[-n:]\n\n    def remember_last_episode(self) -> Episode:\n        \"\"\"Remember last episode.\"\"\"\n        if not self.store:\n            return None\n        return self.store[-1]\n\n    def remember_related_episodes(self, query: str, k: int = 5) -> List[Episode]:\n        \"\"\"Remember related episodes to a query.\"\"\"\n        if self.vector_store is None:\n            return []\n        relevant_documents = self.vector_store.similarity_search(query, k=k)\n        result = []\n        for d in relevant_documents:\n            episode = Episode(\n                thoughts=d.metadata[\"thoughts\"],\n                action=d.metadata[\"action\"],\n                result=d.metadata[\"result\"],\n                summary=d.metadata[\"summary\"]\n            )\n            result.append(episode)\n        return result\n\n    def _embed_episode(self, episode: Episode) -> None:\n        \"\"\"Embed an episode and add it to the vector store.\"\"\"\n        texts = [episode.summary]\n        metadatas = [{\"index\": self.num_episodes,\n                      \"thoughts\": episode.thoughts,\n                      \"action\": episode.action,\n                      \"result\": episode.result,\n                      \"summary\": episode.summary}]\n        if self.vector_store is None:\n            self.vector_store = FAISS.from_texts(\n                texts=texts, embedding=self.embeddings, metadatas=metadatas)\n        else:\n            self.vector_store.add_texts(texts=texts, metadatas=metadatas)\n\n    def save_local(self, path: str) -> None:\n        \"\"\"Save the vector store locally.\"\"\"\n        self.vector_store.save_local(folder_path=path)\n\n    def load_local(self, path: str) -> None:\n        \"\"\"Load the vector store locally.\"\"\"\n        self.vector_store = FAISS.load_local(\n            folder_path=path, embeddings=self.embeddings)", ""]}
