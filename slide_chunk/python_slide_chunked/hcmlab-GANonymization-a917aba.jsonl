{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nwith open('requirements.txt') as f:\n    required = f.read().splitlines()\n\nsetup(\n    name=\"GANonymization\",\n    version=\"1.0.0\",\n    description=\"GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\",\n    author=\"Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, Elisabeth Andr\u00e9\",", "    description=\"GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\",\n    author=\"Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, Elisabeth Andr\u00e9\",\n    author_email=\"fabio.hellmann@informatik.uni-augsburg.de\",\n    url=\"https://github.com/hcmlab/GANonymization\",\n    packages=find_packages(),\n    install_requires=required,\n)\n"]}
{"filename": "main.py", "chunked_list": ["import os.path\nimport pathlib\nimport shutil\n\nimport cv2\nimport fire\nimport pandas as pd\nimport pytorch_lightning\nimport torchvision.transforms\nfrom loguru import logger", "import torchvision.transforms\nfrom loguru import logger\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.transforms import RandomHorizontalFlip, Compose\nfrom torchvision.utils import save_image\n\nfrom lib.datasets import DatasetSplit\nfrom lib.datasets.label_extractor import extract_labels\nfrom lib.datasets.labeled_dataset import LabeledDataset\nfrom lib.evaluator import eval_classifier", "from lib.datasets.labeled_dataset import LabeledDataset\nfrom lib.evaluator import eval_classifier\nfrom lib.models.generic_classifier import GenericClassifier\nfrom lib.models.pix2pix import Pix2Pix\nfrom lib.trainer import setup_torch_device, setup\nfrom lib.transform import transform, FacialLandmarks478, FaceCrop, FaceSegmentation, ZeroPaddingResize, \\\n    Pix2PixTransformer\nfrom lib.utils import glob_dir, get_last_ckpt, move_files\n\nSEED = 42", "\nSEED = 42\n\n\ndef preprocess(input_path: str, img_size: int = 512, align: bool = True, test_size: float = 0.1, shuffle: bool = True,\n               output_dir: str = None, num_workers: int = 8):\n    \"\"\"\n    Run all pre-processing steps (split, crop, segmentation, landmark) at once.\n    @param input_path: The path to a directory to be processed.\n    @param img_size: The size of the image after the processing step.\n    @param align: True if the images should be aligned centered after padding.\n    @param test_size: The size of the test split. (Default: 0.1)\n    @param shuffle: Shuffle the split randomly.\n    @param output_dir: The output directory.\n    @param num_workers: The number of cpu-workers.\n    \"\"\"\n    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n    if output_dir is None:\n        output_dir = input_path\n    output_dir = os.path.join(output_dir, 'original')\n\n    # Split dataset\n    split_file = extract_labels(input_path, output_dir)\n    if split_file:\n        df = pd.read_csv(split_file)\n        for split, folder in enumerate(['train', 'val', 'test']):\n            move_files([os.path.join(input_path, img_path.replace('\\\\', os.sep)) for img_path in\n                        df[df['split'] == split]['image_path'].values], os.path.join(output_dir, folder))\n        shutil.copyfile(split_file, os.path.join(output_dir, pathlib.Path(split_file).name))\n    else:\n        files = glob_dir(os.path.join(input_path))\n        logger.debug(f'Found {len(files)} images')\n        train_files, test_files = train_test_split(files, test_size=test_size, shuffle=shuffle)\n        move_files(train_files, os.path.join(output_dir, 'train'))\n        move_files(test_files, os.path.join(output_dir, 'val'))\n\n    # Apply Transformers\n    output_dir = transform(output_dir, img_size, False, FaceCrop(align, True), num_workers=num_workers)\n    output_dir = transform(output_dir, img_size, False, FaceSegmentation(), num_workers=num_workers)\n    output_dir = transform(output_dir, img_size, True, FacialLandmarks478(), num_workers=num_workers)", "\n\ndef train_pix2pix(data_dir: str, log_dir: str, models_dir: str, output_dir: str, dataset_name: str, epoch: int = 0,\n                  n_epochs: int = 200, batch_size: int = 1, lr: float = 0.0002, b1: float = 0.5, b2: float = 0.999,\n                  n_cpu: int = 2, img_size: int = 256, checkpoint_interval: int = 500, device: int = 0):\n    \"\"\"\n    Train the pix2pix GAN for generating faces based on given landmarks.\n    @param data_dir: The root path to the data folder.\n    @param log_dir: The log folder path.\n    @param models_dir: The path to the models.\n    @param output_dir: The output directory.\n    @param dataset_name: name of the dataset.\n    @param epoch: epoch to start training from.\n    @param n_epochs: number of epochs of training.\n    @param batch_size: size of the batches.\n    @param lr: adam: learning rate.\n    @param b1: adam: decay of first order momentum of gradient.\n    @param b2: adam: decay of first order momentum of gradient.\n    @param n_cpu: number of cpu threads to use during batch generation.\n    @param img_size: size of image.\n    @param checkpoint_interval: interval between model checkpoints.\n    @param device: The device to run the task on (e.g., device >= 0: cuda; device=-1: cpu).\n    \"\"\"\n    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n    setup_torch_device(device, SEED)\n    ckpt_file = get_last_ckpt(models_dir)\n    resume_ckpt = None\n    if ckpt_file is not None:\n        resume_ckpt = os.path.join(models_dir, ckpt_file)\n        model = Pix2Pix.load_from_checkpoint(resume_ckpt)\n    else:\n        model = Pix2Pix(data_dir, models_dir, output_dir, n_epochs, dataset_name, batch_size, lr, b1,\n                        b2, n_cpu, img_size, device)\n    trainer = setup(model, log_dir, models_dir, n_epochs, device, checkpoint_interval=checkpoint_interval)\n    trainer.fit(model, ckpt_path=resume_ckpt)", "\n\ndef train_classifier(data_dir: str, num_classes: int, learning_rate: float = 0.0003, batch_size: int = 128,\n                     n_epochs: int = 100, device: int = 0, output_dir: str = 'output', monitor: str = 'val_loss',\n                     metric_mode: str = 'min', save_top_k: int = 1, early_stop_n: int = 5, num_workers: int = 8):\n    \"\"\"\n    Run the training.\n    @param data_dir:\n    @param num_classes:\n    @param learning_rate: The learning rate.\n    @param batch_size: The batch size.\n    @param n_epochs: The number of epochs to train.\n    @param device: The device to work on.\n    @param output_dir: The path to the output directory.\n    @param monitor: The metric variable to monitor.\n    @param metric_mode: The mode of the metric to decide which checkpoint to choose (min or max).\n    @param save_top_k: Save checkpoints every k epochs - or every epoch if k=0.\n    @param early_stop_n: Stops training after n epochs of no improvement - default is deactivated.\n    @param num_workers:\n    \"\"\"\n    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n    pytorch_lightning.seed_everything(SEED, workers=True)\n    train_db = LabeledDataset(data_dir, DatasetSplit.TRAIN, Compose([\n        GenericClassifier.weights.transforms(),\n        RandomHorizontalFlip(),\n    ]))\n    val_db = LabeledDataset(data_dir, DatasetSplit.VALIDATION,\n                            Compose([GenericClassifier.weights.transforms()]))\n    model = GenericClassifier(train_db=train_db, val_db=val_db, multi_label=train_db.is_multi_label,\n                              batch_size=batch_size, learning_rate=learning_rate, num_workers=num_workers,\n                              device=setup_torch_device(device, SEED), classes=train_db.classes,\n                              class_weights=train_db.class_weight)\n    models_dir = os.path.join(output_dir, 'models')\n    trainer = setup(model=model, log_dir=os.path.join(output_dir, 'logs'),\n                    models_dir=models_dir, num_epoch=n_epochs,\n                    device=device, monitor=monitor, metric_mode=metric_mode,\n                    save_top_k=save_top_k, early_stop_n=early_stop_n)\n    trainer.fit(model)\n    eval_classifier(models_dir, data_dir, batch_size, device, output_dir, num_workers)", "\n\ndef anonymize_image(model_file: str, input_file: str, output_file: str, img_size: int = 512, align: bool = True,\n                    device: int = 0):\n    \"\"\"\n    Anonymize one face in a single image.\n    @param model_file: The GANonymization model to be used for anonymization.\n    @param input_file: The input image file.\n    @param output_file: The output image file.\n    @param img_size: The size of the image for processing by the model.\n    @param align: Whether to align the image based on the facial orientation.\n    @param device: The device to run the process on.\n    \"\"\"\n    img = cv2.imread(input_file)\n    transform = torchvision.transforms.Compose([\n        FaceCrop(align, False),\n        ZeroPaddingResize(img_size),\n        FacialLandmarks478(),\n        Pix2PixTransformer(model_file, img_size, device)\n    ])\n    transformed_img = transform(img)\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    save_image(transformed_img, output_file, normalize=True)", "\n\ndef anonymize_directory(model_file: str, input_directory: str, output_directory: str, img_size: int = 512,\n                        align: bool = True, device: int = 0):\n    \"\"\"\n    Anonymize a set of images in a directory.\n    @param model_file: The GANonymization model to be used for anonymization.\n    @param input_directory: The input image directory.\n    @param output_directory: The output image directory.\n    @param img_size: The size of the image for processing by the model.\n    @param align: Whether to align the image based on the facial orientation.\n    @param device: The device to run the process on.\n    \"\"\"\n    for file in os.listdir(input_directory):\n        anonymize_image(model_file, file, os.path.join(output_directory, os.path.basename(file)), img_size, align,\n                        device)", "\n\nif __name__ == '__main__':\n    fire.Fire()\n"]}
{"filename": "lib/analysis.py", "chunked_list": ["import math\nimport os.path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom deepface import DeepFace\nfrom loguru import logger\nfrom matplotlib import pyplot as plt", "from loguru import logger\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import shapiro, wilcoxon\nfrom statsmodels.stats.multitest import multipletests\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\n\nfrom lib.datasets import DatasetSplit\nfrom lib.datasets.labeled_dataset import LabeledDataset", "from lib.datasets import DatasetSplit\nfrom lib.datasets.labeled_dataset import LabeledDataset\nfrom lib.models.generic_classifier import GenericClassifier\nfrom lib.transform import FaceSegmentation, Pix2PixTransformer\n\n\ndef emotion_analysis(db_path: str, result_path: str):\n    db_name = os.path.basename(db_path)\n    logger.debug(f'Analysis for {db_name}')\n\n    # Load labeled datasets\n    db_seg = LabeledDataset(os.path.join(db_path, 'segmentation'), DatasetSplit.VALIDATION, Compose([]))\n    db_gan = LabeledDataset(os.path.join(db_path, 'ganonymization'), DatasetSplit.VALIDATION, Compose([]))\n    db_dp2 = LabeledDataset(os.path.join(db_path, 'deepprivacy2'), DatasetSplit.VALIDATION, Compose([]))\n\n    # Load predicted labels\n    npz_seg = np.load(os.path.join(result_path, 'segmentation', 'result_pred_label.npz'))\n    npz_gan = np.load(os.path.join(result_path, 'ganonymization', 'result_pred_label.npz'))\n    npz_dp2 = np.load(os.path.join(result_path, 'deepprivacy2', 'result_pred_label.npz'))\n\n    # Create dataframes with predicted labels and image names\n    df_seg = db_seg.meta_data\n    df_gan = db_gan.meta_data\n    df_dp2 = db_dp2.meta_data\n\n    df_seg_pred = pd.DataFrame(npz_seg['pred'], columns=db_seg.classes)\n    df_gan_pred = pd.DataFrame(npz_gan['pred'], columns=db_seg.classes)\n    df_dp2_pred = pd.DataFrame(npz_dp2['pred'], columns=db_seg.classes)\n\n    df_seg_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_seg['image_path']]\n    df_gan_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_gan['image_path']]\n    df_dp2_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_dp2['image_path']]\n\n    # Calculate mean distance between original and synthesized images for each category\n    synthesized_label = [f'{c}_x' for c in db_seg.classes]\n    original_label = [f'{c}_y' for c in db_seg.classes]\n\n    df = pd.DataFrame(index=[c.replace('_', ' ') for c in db_seg.classes])\n\n    df_seg_gan_pred = pd.merge(df_gan_pred, df_seg_pred, on='image_name')\n    df_seg_dp2_pred = pd.merge(df_dp2_pred, df_seg_pred, on='image_name')\n\n    df['GANonymization'] = [(df_seg_gan_pred[x] - df_seg_gan_pred[y]).abs().mean() for x, y in zip(synthesized_label, original_label)]\n    df['DeepPrivacy2'] = [(df_seg_dp2_pred[x] - df_seg_dp2_pred[y]).abs().mean() for x, y in zip(synthesized_label, original_label)]\n\n    # Save results\n    df.to_csv(os.path.join(result_path, f'{db_name}_mean_distance.csv'))\n    df.to_latex(os.path.join(result_path, f'{db_name}_mean_distance.latex'))\n    if len(original_label) < 20:\n        fig_width = int(round(len(df.index) / 2))\n        if fig_width < 4:\n            fig_width = 4\n        df.plot.bar(figsize=(fig_width, 4))\n        plt.ylabel('Mean Distance')\n        plt.xlabel('Category')\n        plt.legend(loc='upper right')\n        plt.tight_layout()\n        plt.savefig(os.path.join(result_path, f'{db_name}_mean_distance.png'))\n        plt.close()\n    else:\n        df_split = np.array_split(df, int(round(len(original_label) / 20)))\n        for idx, df_tmp in enumerate(df_split):\n            fig_width = int(round(len(df_tmp.index) / 2))\n            if fig_width < 4:\n                fig_width = 4\n            df_tmp.plot.bar(figsize=(fig_width, 4))\n            plt.ylabel('Mean Distance')\n            plt.xlabel('Category')\n            plt.legend(loc='upper center')\n            plt.tight_layout()\n            plt.savefig(os.path.join(result_path, f'{db_name}_{idx}_mean_distance.png'))\n            plt.close()\n\n    # T-Test\n    df_seg_gan_pred = pd.merge(df_gan_pred, df_seg_pred, left_on='image_name', right_on='image_name')\n    ganonymization = np.asarray(\n        [(df_seg_gan_pred[x] - df_seg_gan_pred[y]).abs().to_numpy() for x, y in zip(synthesized_label, original_label)])\n\n    df_seg_dp2_pred = pd.merge(df_dp2_pred, df_seg_pred, left_on='image_name', right_on='image_name')[\n        df_dp2_pred['image_name'].isin(df_gan_pred['image_name'])]\n    deepprivacy2 = np.asarray(\n        [(df_seg_dp2_pred[x] - df_seg_dp2_pred[y]).abs().to_numpy() for x, y in zip(synthesized_label, original_label)])\n\n    ganonymization = ganonymization.transpose()\n    deepprivacy2 = deepprivacy2.transpose()\n\n    results = {}\n    for col_idx in range(len(ganonymization[0])):\n        _, pvalue = shapiro(np.concatenate([ganonymization[:, col_idx], deepprivacy2[:, col_idx]]))\n        logger.debug(f'Shapiro: {pvalue}')\n        result = wilcoxon(ganonymization[:, col_idx], deepprivacy2[:, col_idx], method='approx')\n        n = len(ganonymization[:, col_idx])\n        r = result.zstatistic / math.sqrt(n)\n        logger.info(\n            f'{wilcoxon.__name__}-{original_label[col_idx]}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n        results.setdefault('pvalue', []).append(result.pvalue)\n        # results.setdefault('statistic', []).append(result.statistic)\n        results.setdefault('zstatistic', []).append(result.zstatistic)\n        results.setdefault('r', []).append(r)\n        results.setdefault('n', []).append(n)\n\n    reject, pvals_corrected, _, _ = multipletests(np.asarray(results['pvalue']), method='bonferroni')\n    logger.info(f'\\nP-Values (corrected): {pvals_corrected}')\n\n    df = pd.DataFrame(results,\n                      index=[c.replace('_', ' ') for c in db_seg.classes])\n    df.rename({'pvalue': 'P-Value', 'zstatistic': 'Z-Statistic', 'n': 'N'}, axis=1, inplace=True)\n    df.to_csv(os.path.join(result_path, f'statistic_pvalue.csv'))\n    df.to_latex(os.path.join(result_path, f'statistic_pvalue.latex'))", "\n\ndef face_comparison_analysis(output_dir: str, img_orig_path: str, *img_paths, img_size: int = 512):\n    def original_filename(file):\n        return file.split('-')[-1]\n\n    os.makedirs(output_dir, exist_ok=True)\n    sub_dir = 'val'\n    img_dirs = {\n        os.path.basename(path): [os.path.join(path, sub_dir, f) for f in os.listdir(os.path.join(path, sub_dir))]\n        for path in img_paths}\n    img_dirs_short = {method: [original_filename(path) for path in paths] for method, paths in img_dirs.items()}\n\n    comp_result = []\n    transformers = Compose([Resize(img_size), ToTensor()])\n    for orig_img in tqdm(os.listdir(os.path.join(img_orig_path, sub_dir)), desc='Comparison'):\n        images = {'original': os.path.join(img_orig_path, sub_dir, orig_img)}\n        for method, paths in img_dirs.items():\n            if original_filename(orig_img) in img_dirs_short[method]:\n                images[method] = img_dirs[method][\n                    [original_filename(p) for p in paths].index(original_filename(orig_img))]\n        if len(images) < len(img_paths) + 1:\n            continue\n        for idx, items in enumerate(images.items()):\n            db_name, img_path = items\n            # Predict similarity of faces\n            verify_result = {'distance': 0, 'threshold': 0}\n            if idx > 0:\n                tmp_result = DeepFace.verify(images['original'], img_path, model_name=\"VGG-Face\",\n                                             detector_backend='skip', enforce_detection=False, align=False)\n                if len(verify_result) > 0:\n                    verify_result = tmp_result\n                comp_result.append([db_name, original_filename(img_path), verify_result['distance'],\n                                    verify_result['threshold']])\n        img_sample = torch.cat(\n            [transformers(Image.open(images['original'])), transformers(Image.open(images['deepprivacy2'])),\n             transformers(Image.open(images['ganonymization']))], -2)\n        save_image(img_sample, os.path.join(output_dir, original_filename(orig_img)), normalize=True)\n    df = pd.DataFrame(comp_result, columns=['dataset', 'filename', 'distance', 'threshold'])\n    df.to_csv(os.path.join(output_dir, 'prediction_result.csv'))\n    df_distance_mean = df.groupby(['dataset'])['distance'].mean()\n    threshold_mean = df.groupby(['dataset'])['threshold'].mean()['ganonymization']\n    df_distance_mean.to_csv(os.path.join(output_dir, 'mean_prediction_result.csv'))\n    df_distance_mean.transpose().plot.bar(rot=0)\n    plt.axhline(y=threshold_mean, ls='dashed', color='b')\n    plt.ylabel('Mean Cosine Distance')\n    plt.xlabel('Anonymization Method')\n    plt.savefig(os.path.join(output_dir, 'mean_distance_per_method.png'))\n    plt.close()\n\n    logger.debug(f'Analyze comparison for: {output_dir}')\n    df = pd.read_csv(os.path.join(output_dir, 'prediction_result.csv'))\n    df_gan = df[df['dataset'] == 'ganonymization'].reset_index()\n    df_dp2 = df[df['dataset'] == 'deepprivacy2'].reset_index()\n    df_gan_nan = df_gan[df_gan['distance'].isna()]['filename'].index\n    df_gan.drop(df_gan_nan, inplace=True)\n    df_dp2.drop(df_gan_nan, inplace=True)\n    df_gan = df_gan['distance'].to_numpy()\n    df_dp2 = df_dp2['distance'].to_numpy()\n    _, pvalue = shapiro(np.concatenate([df_gan, df_dp2]))\n    logger.debug(f'Shapiro: {pvalue}')\n    result = wilcoxon(df_gan, df_dp2, method='approx')\n    n = len(df_gan)\n    r = result.zstatistic / math.sqrt(n)\n    logger.info(\n        f'{wilcoxon.__name__}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n    reject, pvals_corrected, _, _ = multipletests(result.pvalue, method='bonferroni')\n    logger.info(\n        f'{wilcoxon.__name__}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n\n    df = pd.DataFrame({'P-Value': pvals_corrected, 'Z-Statistic': result.zstatistic, 'r': r, 'N': n})\n    df.to_csv(os.path.join(output_dir, f'statistic_pvalue.csv'))\n    df.to_latex(os.path.join(output_dir, f'statistic_pvalue.latex'))", "\n\ndef facial_traits_analysis(data_dir: str, model_file: str, output_dir: str):\n    db_seg = LabeledDataset(os.path.join(data_dir, FaceSegmentation.__class__.__name__), DatasetSplit.VALIDATION,\n                            Compose([GenericClassifier.weights.transforms()]))\n    db_seg.meta_data['image_name'] = db_seg.meta_data['image_path'].apply(\n        lambda f: os.path.basename(f).split('-')[-1]).values.tolist()\n    db_gan = LabeledDataset(os.path.join(data_dir, Pix2PixTransformer.__class__.__name__), DatasetSplit.VALIDATION,\n                            Compose([GenericClassifier.weights.transforms()]))\n    db_gan.meta_data['image_name'] = db_gan.meta_data['image_path'].apply(\n        lambda f: os.path.basename(f).split('-')[-1]).values.tolist()\n    db_seg.meta_data.drop(db_seg.meta_data[~db_seg.meta_data['image_name'].isin(db_gan.meta_data['image_name'])].index,\n                          inplace=True)\n    db_seg.meta_data.reset_index(inplace=True)\n    model = GenericClassifier.load_from_checkpoint(model_file, classes=db_seg.classes)\n    result_positive = {label: 0 for label in db_seg.classes}\n    result_negative = {label: 0 for label in db_seg.classes}\n    for idx in tqdm(range(len(db_seg)), total=len(db_seg)):\n        pred_seg = model.predict(torch.unsqueeze(db_seg[idx][0].to(model.device), dim=0))[0]\n        if torch.any(pred_seg > 0.5):\n            pred_gan = model.predict(torch.unsqueeze(db_gan[idx][0].to(model.device), dim=0))[0]\n            pred_seg = pred_seg.cpu().detach().numpy()\n            pred_gan = pred_gan.cpu().detach().numpy()\n            for label_idx in range(db_seg.num_classes):\n                if pred_seg[label_idx] > 0.5 and pred_gan[label_idx] > 0.5:\n                    result_positive[db_seg.classes[label_idx]] += 1\n                elif pred_seg[label_idx] > 0.5 and pred_gan[label_idx] <= 0.5:\n                    result_negative[db_seg.classes[label_idx]] += 1\n    df_pos = pd.read_csv(os.path.join(output_dir, 'result', 'positive.csv'), index_col=0)\n    df_neg = pd.read_csv(os.path.join(output_dir, 'result', 'negative.csv'), index_col=0)\n    df = pd.merge(df_pos, df_neg, right_index=True, left_index=True)\n    result = df['Negative'] / df.sum(axis=1)\n    result = pd.DataFrame(result, columns=['GANonymization'])\n    result = result.rename(index={s: s.replace('_', ' ') for s in result.index.values})\n    result = result.sort_values('GANonymization', ascending=False)\n    result.to_csv(os.path.join(output_dir, 'result', 'traits_removed.csv'))\n    result.to_latex(os.path.join(output_dir, 'result', 'traits_removed.latex'))\n    df_split = np.array_split(result, int(round(len(result) / 20)))\n    for idx, df_tmp in enumerate(df_split):\n        fig_width = int(round(len(df_tmp.index) / 2))\n        if fig_width < 4:\n            fig_width = 4\n        df_tmp.plot.bar(figsize=(fig_width, 4))\n        plt.gca().set_ylim([0, 1])\n        plt.ylabel('Removed (in %)')\n        plt.xlabel('Category')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'result', f'{idx}_traits_removed.png'))\n        plt.close()", ""]}
{"filename": "lib/__init__.py", "chunked_list": [""]}
{"filename": "lib/utils.py", "chunked_list": ["import os\nimport os.path\nimport pathlib\nimport shutil\nfrom glob import glob\nfrom typing import Optional, List\n\nfrom tqdm import tqdm\n\n\ndef move_files(files: List[str], output_dir: str):\n    \"\"\"\n    Move a list of files to another directory.\n    @param files: Files to move.\n    @param output_dir: The output directory.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    for file_path in tqdm(files, desc=output_dir):\n        shutil.copyfile(file_path, os.path.join(output_dir, pathlib.Path(file_path).name))", "\n\ndef move_files(files: List[str], output_dir: str):\n    \"\"\"\n    Move a list of files to another directory.\n    @param files: Files to move.\n    @param output_dir: The output directory.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    for file_path in tqdm(files, desc=output_dir):\n        shutil.copyfile(file_path, os.path.join(output_dir, pathlib.Path(file_path).name))", "\n\ndef get_last_ckpt(ckpt_directory: str) -> Optional[str]:\n    \"\"\"\n    Retrieve the last checkpoint based on the creation timestamp.\n    @param ckpt_directory: The directory where the checkpoints are saved.\n    @return: The checkpoint or None if none was found.\n    \"\"\"\n    ckpt_files = glob(os.path.join(ckpt_directory, '*.ckpt'))\n    if len(ckpt_files) > 0:\n        return max(ckpt_files, key=os.path.getctime)\n    return None", "\n\ndef glob_dir(directory: str, exclude: List[str] = ['Thumbs.db', '.DS_Store']):\n    \"\"\"\n    Recursively search and list all files in the directory filtered by the exclusion clause.\n    @param directory: The directory to search in.\n    @param exclude: The files or file-endings to filter out.\n    @return: A list of files.\n    \"\"\"\n    return list(\n        filter(lambda f: not f.endswith(tuple(exclude)), glob(os.path.join(directory, '**', f'*.*'), recursive=True)))", ""]}
{"filename": "lib/trainer.py", "chunked_list": ["import os\nimport random\n\nimport numpy as np\nimport pytorch_lightning\nimport torch\nfrom loguru import logger\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger", "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n\ndef setup_torch_device(device: int, seed: int) -> str:\n    \"\"\"\n    Set up the torch device with a seed.\n    @param device: The device the model should be run on.\n    @param seed: The seed for the run.\n    @return: The string of the device.\n    \"\"\"\n    torch_device = f'cuda:{device}' if device >= 0 else 'cpu'\n    logger.info(f\"Using {'GPU.' if device >= 0 else 'CPU, as was explicitly requested, or as GPU is not available.'}\")\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    pytorch_lightning.seed_everything(seed)\n    if device >= 0:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.manual_seed_all(seed)\n        torch.cuda.manual_seed(seed)\n    return torch_device", "\n\ndef setup(model: LightningModule, log_dir: str, models_dir: str, num_epoch: int, device: int,\n          save_top_k: int = 0, monitor: str = None, metric_mode: str = None, early_stop_n: int = 0,\n          checkpoint_interval: int = None) -> pytorch_lightning.Trainer:\n    \"\"\"\n    Run the lightning model.\n    :param model: The model to train.\n    :param num_epoch: The number of epochs to train.\n    :param device: The device to work on.\n    :param save_top_k: Save top k checkpoints - or every epoch if k=0.\n    :param monitor: The metric variable to monitor.\n    :param metric_mode: The mode of the metric to decide which checkpoint to choose (min or max).\n    :param early_stop_n: Stops training after n epochs of no improvement - default is deactivated.\n    :param checkpoint_interval: The interval a checkpoint should be saved if save_top_k is 0.\n    \"\"\"\n    model_name = model.__class__.__name__\n    os.makedirs(models_dir, exist_ok=True)\n    os.makedirs(log_dir, exist_ok=True)\n    tb_logger = TensorBoardLogger(log_dir, name='', version='')\n    callbacks = [LearningRateMonitor()]\n    if save_top_k > 0:\n        callbacks.append(ModelCheckpoint(dirpath=models_dir, filename='{epoch}-{' + monitor + ':.6f}',\n                                         save_top_k=save_top_k, monitor=monitor, mode=metric_mode))\n    else:\n        callbacks.append(\n            ModelCheckpoint(dirpath=models_dir, filename='{epoch}-{step}', monitor='step', mode='max',\n                            save_top_k=-1, every_n_train_steps=checkpoint_interval))\n    if early_stop_n > 0:\n        callbacks.append(\n            EarlyStopping(monitor=monitor, min_delta=0.00, patience=early_stop_n, verbose=False, mode=metric_mode))\n\n    trainer = pytorch_lightning.Trainer(deterministic=True,\n                                        accelerator=\"gpu\" if device >= 0 else \"cpu\",\n                                        devices=[device] if device >= 0 else None,\n                                        callbacks=callbacks,\n                                        logger=tb_logger,\n                                        max_epochs=num_epoch,\n                                        val_check_interval=checkpoint_interval,\n                                        log_every_n_steps=1,\n                                        detect_anomaly=True)\n    logger.info(\n        f'Train {model_name} for {num_epoch} epochs and save logs under {tb_logger.log_dir} and models under {models_dir}')\n    model.train()\n    model.to(device)\n    return trainer", ""]}
{"filename": "lib/evaluator.py", "chunked_list": ["import os.path\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport torch\nfrom loguru import logger\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.metrics import classification_report", "from mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassConfusionMatrix, MultilabelConfusionMatrix\nfrom torchvision.transforms import Compose\nfrom tqdm import tqdm\n\nfrom lib.datasets import DatasetSplit\nfrom lib.datasets.labeled_dataset import LabeledDataset\nfrom lib.models.generic_classifier import GenericClassifier", "from lib.datasets.labeled_dataset import LabeledDataset\nfrom lib.models.generic_classifier import GenericClassifier\nfrom lib.utils import get_last_ckpt\n\n\ndef eval_classifier(models_dir: str, data_dir: str, batch_size: int = 128, device: int = 0,\n                    output_dir: str = 'output', num_workers: int = 2):\n    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n    os.makedirs(output_dir, exist_ok=True)\n    val_db = LabeledDataset(data_dir, DatasetSplit.VALIDATION,\n                            Compose([GenericClassifier.weights.transforms()]))\n    model = GenericClassifier.load_from_checkpoint(get_last_ckpt(models_dir), classes=val_db.classes,\n                                                   multi_label=val_db.is_multi_label)\n    model.eval()\n    model.to(device)\n    if model.multi_label:\n        cm = MultilabelConfusionMatrix(num_labels=val_db.num_classes, normalize='true').to(device)\n        logger.debug('Setting up evaluation for multi-label classification')\n    else:\n        cm = MulticlassConfusionMatrix(num_classes=val_db.num_classes, normalize='true').to(device)\n        logger.debug('Setting up evaluation for multi-class classification')\n    dl = DataLoader(val_db, batch_size=batch_size, num_workers=num_workers)\n    all_label = []\n    all_pred = []\n    for batch in tqdm(dl, desc='Evaluation'):\n        images, labels = batch\n        pred = model.predict(images.to(device))\n        labels = labels.to(device)\n        if val_db.is_multi_label:\n            ml_pred = torch.round(pred).int()\n            ml_labels = labels.int()\n            cm.update(ml_pred, ml_labels)\n        else:\n            mc_labels = torch.argmax(labels, dim=1)\n            cm.update(pred, mc_labels)\n        all_label.extend(labels.cpu().detach().numpy())\n        all_pred.extend(pred.cpu().detach().numpy())\n    # Confusion Matrix\n    all_label = np.asarray(all_label)\n    all_pred = np.asarray(all_pred)\n    np.savez_compressed(os.path.join(output_dir, 'result_pred_label.npz'), pred=all_pred, label=all_label)\n    conf_matrix = cm.compute()\n    conf_matrix = conf_matrix.cpu().detach().numpy()\n    logger.debug('Creating Confusion Matrix...')\n    if model.multi_label:\n        n_columns = int(model.num_classes / 4)\n        n_rows = int(model.num_classes / n_columns)\n        fig = plt.figure(figsize=(n_columns * 2, n_rows * 2))\n        plt.tight_layout()\n        grid = ImageGrid(fig, 111,\n                         nrows_ncols=(n_rows, n_columns),\n                         axes_pad=0.5,\n                         share_all=True,\n                         cbar_mode='single',\n                         cbar_location='right',\n                         cbar_size='5%',\n                         cbar_pad=0.5)\n        for idx in tqdm(range(conf_matrix.shape[0]), desc='Multi-Label Confusion Matrices'):\n            label = model.classes[idx]\n            binary_labels = ['Present', 'Absent']\n            df_cm = pd.DataFrame(conf_matrix[idx, :, :], index=binary_labels, columns=binary_labels)\n            df_cm.to_csv(os.path.join(output_dir, f'{os.path.basename(data_dir)}_{label}_confusion_matrix.csv'))\n            sn.set(font_scale=1.3)\n            sn.heatmap(df_cm, annot=True, fmt='.2f', xticklabels=binary_labels, yticklabels=binary_labels,\n                       ax=grid[idx], cbar_ax=grid[0].cax, annot_kws={'fontsize': 13})\n            grid[idx].set_ylabel('Actual')\n            grid[idx].set_xlabel('Predicted')\n            grid[idx].set_title(label.replace('_', ' ').replace('-', ' '))\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.png'))\n        plt.close()\n    else:\n        df_cm = pd.DataFrame(conf_matrix, index=val_db.labels, columns=val_db.labels)\n        df_cm.to_csv(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.csv'))\n        plt.figure(figsize=(\n            len(val_db.labels) + int(len(val_db.labels) / 2), len(val_db.labels) + int(len(val_db.labels) / 2)))\n        plt.tight_layout()\n        sn.set(font_scale=1.3)\n        sn.heatmap(df_cm, annot=True, fmt='.2f', xticklabels=val_db.labels, yticklabels=val_db.labels, annot_kws={\n            'fontsize': 13\n        })\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.savefig(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.png'))\n        plt.close()\n    logger.debug('Confusion Matrix created!')\n    # Classification Report\n    logger.debug('Creating Classification Report...')\n    if model.multi_label:\n        all_label = np.round(all_label)\n        all_pred = np.round(all_pred)\n    else:\n        all_label = np.squeeze(np.argmax(all_label, axis=1))\n        all_pred = np.squeeze(np.argmax(all_pred, axis=1))\n    report = classification_report(all_label, all_pred, target_names=val_db.labels)\n    with open(os.path.join(output_dir, f'{os.path.basename(data_dir)}_classification_report.txt'),\n              'w+') as f:\n        f.write(report)\n    logger.debug('Classification Report created!')", ""]}
{"filename": "lib/models/generic_classifier.py", "chunked_list": ["from typing import List\n\nimport pytorch_lightning as pl\nimport torch\nfrom loguru import logger\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom torch import nn, Tensor\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassAccuracy, MultilabelAccuracy\nfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights", "from torchmetrics.classification import MulticlassAccuracy, MultilabelAccuracy\nfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights\n\nfrom lib.datasets.labeled_dataset import LabeledDataset\n\n\nclass GenericClassifier(pl.LightningModule):\n    \"\"\"\n    The GenericClassifier is based on the ConvNeXt architecture and\n    can be used to train a multi-class or multi-label problem.\n    \"\"\"\n\n    weights = ConvNeXt_Base_Weights.DEFAULT\n\n    def __init__(self, multi_label: bool, classes: List[str], batch_size: int,\n                 learning_rate: float, num_workers: int, device: str, val_db: LabeledDataset = None,\n                 train_db: LabeledDataset = None, class_weights: Tensor = None):\n        \"\"\"\n        Create a new GenericClassifier.\n        @param multi_label: If the problem at hand is multi-label then True otherwise False for multi-class.\n        @param classes: The class names as a list.\n        @param batch_size: The size of the batches.\n        @param learning_rate: The learning rate.\n        @param num_workers: The number of cpu-workers.\n        @param device: The device to run the model on.\n        @param val_db: The validation dataset.\n        @param train_db: The training dataset.\n        @param class_weights: The class weights of the dataset.\n        \"\"\"\n        super().__init__()\n        # Settings\n        self.train_db = train_db\n        self.val_db = val_db\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.multi_label = multi_label\n        self.classes = classes\n        self.num_classes = len(classes)\n        # Define model architecture\n        self.model = convnext_base(weights=self.weights)\n        self.model.classifier[-1] = nn.Linear(self.model.classifier[-1].in_features, self.num_classes)\n        self.class_weights = class_weights.float().to(device)\n        # Loss function, accuracy\n        if self.multi_label:\n            logger.debug(f'Model is a Multi-Label Classificator')\n            self.loss_function = torch.nn.BCEWithLogitsLoss(reduction='none').to(device)\n            self.activation = torch.nn.Sigmoid().to(device)\n            self.metrics = {\n                'accuracy': MultilabelAccuracy(num_labels=self.num_classes).to(device)\n            }\n        else:\n            logger.debug(f'Model is a Multi-Class Classificator')\n            self.loss_function = torch.nn.CrossEntropyLoss(weight=self.class_weights).to(device)\n            self.activation = torch.nn.Softmax(dim=1).to(device)\n            self.metrics = {\n                'top_1_acc': MulticlassAccuracy(top_k=1, num_classes=self.num_classes).to(device),\n                'top_5_acc': MulticlassAccuracy(top_k=5, num_classes=self.num_classes).to(device),\n            }\n        self.save_hyperparameters(ignore=['val_db', 'train_db'])\n\n    def forward(self, x):\n        return self.model(x)\n\n    def predict(self, x):\n        return self.activation(self.forward(x))\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(params=self.parameters(), lr=self.learning_rate, weight_decay=0.001)\n        scheduler = ReduceLROnPlateau(optimizer, monitor='val_loss', patience=3)\n        return [optimizer], [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'val_loss'}]\n\n    def training_step(self, train_batch, batch_idx):\n        loss, _ = self.step('train', train_batch)\n        return loss\n\n    def train_dataloader(self):\n        return DataLoader(self.train_db, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n\n    def validation_step(self, val_batch, batch_idx):\n        _, labels = val_batch\n        _, output = self.step('val', val_batch)\n        if not self.multi_label:\n            labels = torch.argmax(labels, dim=1)\n        for key, metric in self.metrics.items():\n            self.log(f'val_{key}', metric(output, labels), prog_bar=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_db, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n\n    def step(self, tag: str, batch):\n        images, labels = batch\n        output = self.forward(images)\n        if self.multi_label:\n            loss = self.loss_function(output, labels)\n            loss = (loss * self.class_weights).mean()\n        else:\n            loss = self.loss_function(output, torch.argmax(labels, dim=1))\n        self.log(f'{tag}_loss', loss.item(), prog_bar=True)\n        return loss, output", ""]}
{"filename": "lib/models/pix2pix.py", "chunked_list": ["import os.path\n\nimport pytorch_lightning\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom lib.datasets.image_dataset import ImageDataset", "\nfrom lib.datasets.image_dataset import ImageDataset\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0.0)", "\n\n##############################\n#           U-NET\n##############################\n\n\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)", "\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n            nn.InstanceNorm2d(out_size),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x", "\n\nclass GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(GeneratorUNet, self).__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(128, out_channels, 4, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)", "\n\n##############################\n#        Discriminator\n##############################\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels * 2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )\n\n    def forward(self, img_A, img_B):\n        # Concatenate image and condition image by channels to produce input\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)", "\n\n##############################\n#        Lightning\n##############################\n\n\nclass Pix2Pix(pytorch_lightning.LightningModule):\n    def __init__(self, data_dir: str, models_dir: str, output_dir: str, n_epochs: int,\n                 dataset_name: str, batch_size: int, lr: float, b1: float, b2: float, n_cpu: int, img_size: int,\n                 device: int):\n        \"\"\"\n        Create a Pix2Pix Network.\n        @param data_dir: The directory of the data.\n        @param models_dir: The directory of the models.\n        @param output_dir: The directory of the output.\n        @param n_epochs: The number of epochs.\n        @param dataset_name: The name of the dataset which is appended to the output_dir.\n        @param batch_size: The size of the batches to process.\n        @param lr: The learning rate.\n        @param b1: The beta 1 value for the optimizer.\n        @param b2: The beta 2 value for the optimizer.\n        @param n_cpu: The number of cpus.\n        @param img_size: The size of the image.\n        @param device: The device to run the computation on.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self.automatic_optimization = False\n\n        self.data_dir = data_dir\n        self.models_dir = models_dir\n        self.output_dir = output_dir\n        self.n_epochs = n_epochs\n        self.dataset_name = dataset_name\n        self.batch_size = batch_size\n        self.lr = lr\n        self.b1 = b1\n        self.b2 = b2\n        self.n_cpu = n_cpu\n        self.img_size = img_size\n\n        self.out_dir = os.path.join(output_dir, dataset_name)\n        os.makedirs(self.out_dir, exist_ok=True)\n\n        self.transforms_ = [\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n        self.criterion_GAN = torch.nn.MSELoss()\n        self.criterion_GAN.to(device)\n        self.criterion_pixelwise = torch.nn.L1Loss()\n        self.criterion_pixelwise.to(device)\n        self.lambda_pixel = 100\n        self.generator = GeneratorUNet()\n        self.generator.to(device)\n        self.discriminator = Discriminator()\n        self.discriminator.to(device)\n\n        # Initialize weights\n        self.generator.apply(weights_init_normal)\n        self.discriminator.apply(weights_init_normal)\n\n    def forward(self, x):\n        return self.generator(x)\n\n    def training_step(self, batch):\n        optimizer_g, optimizer_d = self.optimizers()\n        # Model inputs\n        real_A = batch[\"B\"]\n        real_B = batch[\"A\"]\n        # Calculate output of image discriminator (PatchGAN)\n        patch = (1, self.img_size // 2 ** 4, self.img_size // 2 ** 4)\n        # Adversarial ground truths\n        valid = torch.ones((real_A.size(0), *patch), requires_grad=False).to(self.device)\n        fake = torch.zeros((real_A.size(0), *patch), requires_grad=False).to(self.device)\n        # ------------------\n        #  Train Generators\n        # ------------------\n        self.toggle_optimizer(optimizer_g)\n        # GAN loss\n        fake_B = self.generator(real_A)\n        pred_fake = self.discriminator(fake_B, real_A)\n        loss_GAN = self.criterion_GAN(pred_fake, valid)\n        # Pixel-wise loss\n        loss_pixel = self.criterion_pixelwise(fake_B, real_B)\n        # Total loss\n        loss_G = loss_GAN + self.lambda_pixel * loss_pixel\n        self.log('G loss', loss_G, prog_bar=True)\n        self.log('G pixel', loss_pixel, prog_bar=True)\n        self.log('G adv', loss_GAN, prog_bar=True)\n        self.manual_backward(loss_G)\n        optimizer_g.step()\n        optimizer_g.zero_grad()\n        self.untoggle_optimizer(optimizer_g)\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        self.toggle_optimizer(optimizer_d)\n        # Real loss\n        pred_real = self.discriminator(real_B, real_A)\n        loss_real = self.criterion_GAN(pred_real, valid)\n        # Fake loss\n        fake_B = self.generator(real_A)\n        pred_fake = self.discriminator(fake_B.detach(), real_A)\n        loss_fake = self.criterion_GAN(pred_fake, fake)\n        # Total loss\n        loss_D = 0.5 * (loss_real + loss_fake)\n        self.log('D loss', loss_D, prog_bar=True)\n        self.manual_backward(loss_D)\n        optimizer_d.step()\n        optimizer_d.zero_grad()\n        self.untoggle_optimizer(optimizer_d)\n\n    def validation_step(self, batch, batch_idx):\n        if batch_idx == 0:\n            real_A = batch[\"B\"]\n            real_B = batch[\"A\"]\n            fake_B = self.generator(real_A)\n            img_sample = torch.cat((real_B.data, real_A.data, fake_B.data), -2)\n            save_image(img_sample, os.path.join(self.out_dir, f'{self.current_epoch}-{self.global_step}.png'), nrow=5,\n                       normalize=True)\n            grid = make_grid(img_sample, nrow=5, normalize=True)\n            self.logger.experiment.add_image('images', grid, self.global_step)\n\n    def configure_optimizers(self):\n        optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n        optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n        return [optimizer_g, optimizer_d], []\n\n    def train_dataloader(self):\n        return DataLoader(\n            ImageDataset(os.path.join(self.data_dir, self.dataset_name), transforms_=self.transforms_),\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.n_cpu,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            ImageDataset(os.path.join(self.data_dir, self.dataset_name), transforms_=self.transforms_, mode=\"val\"),\n            batch_size=10,\n            shuffle=True,\n            num_workers=self.n_cpu,\n        )", ""]}
{"filename": "lib/transform/zero_padding_resize_transformer.py", "chunked_list": ["import numpy as np\nfrom PIL import Image\n\n\nclass ZeroPaddingResize:\n    \"\"\"\n    The ZeroPaddingResize transformer resizes the image into the given size and applies zero-padding to center\n    the output image.\n    \"\"\"\n\n    def __init__(self, size: int):\n        \"\"\"\n        @param size: The size the image will be resized to.\n        \"\"\"\n        self.size = size\n\n    def __call__(self, pic):\n        \"\"\"\n        @param pic (PIL Image or numpy.ndarray): Image to be converted to center zero-padded and resized image.\n        @return: numpy.ndarray: Converted image.\n        \"\"\"\n        if isinstance(pic, np.ndarray):\n            pic = Image.fromarray(pic)\n        if pic.width != self.size or pic.height != self.size:\n            ratio = min(self.size / pic.width, self.size / pic.height)\n            face_img = pic.resize((int(ratio * pic.width), int(ratio * pic.height)), Image.LANCZOS)\n            new_im = Image.new(\"RGB\", (self.size, self.size))\n            new_im.paste(face_img, ((self.size - face_img.width) // 2, (self.size - face_img.height) // 2))\n            pic = np.array(new_im)\n        return pic\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}\"", ""]}
{"filename": "lib/transform/facial_landmarks_478_transformer.py", "chunked_list": ["import cv2\nimport mediapipe\nimport numpy as np\nfrom mediapipe.python.solutions.drawing_utils import DrawingSpec, WHITE_COLOR\n\n\nclass FacialLandmarks478:\n    \"\"\"Extract 468 facial landmark points from the picture and return it in a 2-dimensional picture.\"\"\"\n\n    def __call__(self, pic):\n        \"\"\"\n        @param pic (PIL Image or numpy.ndarray): Image to be converted to a facial landmark image with 468 points.\n        @return: numpy.ndarray: Converted image.\n        \"\"\"\n        point_image = np.zeros(pic.shape, np.uint8)\n        with mediapipe.solutions.face_mesh.FaceMesh(\n                static_image_mode=True,\n                max_num_faces=1,\n                refine_landmarks=True,\n                min_detection_confidence=0.5) as face_mesh:\n            results = face_mesh.process(cv2.cvtColor(pic, cv2.COLOR_BGR2RGB))\n            if results.multi_face_landmarks is not None and len(results.multi_face_landmarks) > 0:\n                mediapipe.solutions.drawing_utils.draw_landmarks(\n                    image=point_image,\n                    landmark_list=results.multi_face_landmarks[0],\n                    landmark_drawing_spec=DrawingSpec(color=WHITE_COLOR, thickness=1, circle_radius=0))\n        return point_image\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}\"", ""]}
{"filename": "lib/transform/__init__.py", "chunked_list": ["import os\nimport pathlib\nimport shutil\nfrom typing import List, Optional\n\nimport cv2\nimport numpy as np\nfrom loguru import logger\nfrom p_tqdm import p_tqdm\nfrom tqdm import tqdm", "from p_tqdm import p_tqdm\nfrom tqdm import tqdm\n\nfrom lib.transform.face_crop_transformer import FaceCrop\nfrom lib.transform.face_segmentation_transformer import FaceSegmentation\nfrom lib.transform.facial_landmarks_478_transformer import FacialLandmarks478\nfrom lib.transform.zero_padding_resize_transformer import ZeroPaddingResize\nfrom lib.transform.pix2pix_transformer import Pix2PixTransformer\nfrom lib.utils import glob_dir\n", "from lib.utils import glob_dir\n\n\ndef exec(files: List[str], output_dir: str, input_dir: str, size: int, gallery: bool, transformer):\n    \"\"\"\n    Executes the augmentation.\n    @param files: The files to be augmented.\n    @param output_dir: The directory of the output.\n    @param input_dir: The directory of the input.\n    @param size: The image size.\n    @param gallery: Whether the image should be saved besides its original.\n    @param transformer: The transformer to be applied.\n    \"\"\"\n    name = str(transformer)\n    for image_file in files:\n        sub_path_image = os.path.dirname(image_file[len(input_dir):])\n        sub_output_dir = os.path.join(output_dir, *sub_path_image.split(os.sep))\n        img = cv2.imread(image_file)\n        if img is not None:\n            pred = transformer(img)\n            for idx, sub_pred in enumerate(pred):\n                sub_pred = ZeroPaddingResize(size)(sub_pred)\n                os.makedirs(sub_output_dir, exist_ok=True)\n                output_file = os.path.join(sub_output_dir, f'{name}_{idx}-{pathlib.Path(image_file).name}')\n                if gallery:\n                    cv2.imwrite(output_file, cv2.hconcat([img, sub_pred]))\n                else:\n                    cv2.imwrite(output_file, sub_pred)\n    return 0", "\n\ndef transform(input_dir: str, size: int, gallery: bool, transformer,\n              output_dir: Optional[str] = None, num_workers: int = 1) -> str:\n    \"\"\"\n    Transform all images found in the input directory.\n    @param input_dir: The input directory.\n    @param size: The size of the images afterwards.\n    @param gallery: Whether the image should be saved besides its original.\n    @param transformer: The transformer to be applied.\n    @param output_dir: The output directory.\n    @param num_workers: The number of parallel workers.\n    @return: The output path.\n    \"\"\"\n    name = str(transformer)\n    if output_dir is None:\n        output_dir = os.path.dirname(input_dir)\n    output_dir = os.path.join(output_dir, name)\n    logger.debug(f'Preparing output directory: {output_dir}')\n    os.makedirs(output_dir, exist_ok=True)\n    # Copy other files to destination\n    for f in os.listdir(input_dir):\n        file = os.path.join(input_dir, f)\n        if os.path.isfile(file):\n            shutil.copyfile(file, os.path.join(output_dir, f))\n    # Search for every image\n    files = glob_dir(input_dir)\n    logger.debug(f'Found {len(files)} files in source directory: {input_dir}')\n    # Search for already processed files\n    out_files = glob_dir(output_dir)\n    out_files = [pathlib.Path(f).name.split('-')[-1] for f in out_files]\n    logger.debug(f'Found {len(out_files)} files in destination directory: {output_dir}')\n    files_augment = []\n    for f in tqdm(files, desc='Skip Check'):\n        if pathlib.Path(f).name.split('-')[-1] not in out_files:\n            files_augment.append(f)\n    if len(files_augment) > 0:\n        logger.debug(f'Processing {len(files_augment)} files...')\n        list_chunks = np.array_split(files_augment, num_workers)\n        logger.debug(f'Distribute workload to {num_workers} workers with a chunk size of {len(list_chunks[0])} each')\n        p_tqdm.p_umap(exec, list_chunks, [output_dir] * len(list_chunks), [input_dir] * len(list_chunks),\n                      [size] * len(list_chunks), [gallery] * len(list_chunks), [transformer] * len(list_chunks),\n                      num_cpus=num_workers)\n    else:\n        logger.debug('Data has already been fully processed!')\n    return output_dir", ""]}
{"filename": "lib/transform/face_segmentation_transformer.py", "chunked_list": ["import cv2\nfrom head_segmentation import HumanHeadSegmentationPipeline\n\n\nclass FaceSegmentation:\n    \"\"\"\n    The FaceSegmentation transformer eliminates everything besides the face in the image.\n    \"\"\"\n\n    def __call__(self, pic):\n        \"\"\"\n        @param pic (PIL Image or numpy.ndarray): Image to be converted to a face segmentation.\n        @return: numpy.ndarray: Converted image.\n        \"\"\"\n        segmentation_pipeline = HumanHeadSegmentationPipeline()\n        face_mask = segmentation_pipeline.predict(pic)\n        segmented_region = pic * cv2.cvtColor(face_mask, cv2.COLOR_GRAY2RGB)\n        return segmented_region\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}\"", ""]}
{"filename": "lib/transform/pix2pix_transformer.py", "chunked_list": ["import torch\nfrom torchvision.transforms import transforms\n\nfrom lib.models.pix2pix import Pix2Pix\n\n\nclass Pix2PixTransformer:\n    \"\"\"\n    The GANonymization transformer synthesizes images based on facial landmarks images.\n    \"\"\"\n\n    def __init__(self, model_file: str, img_size: int, device: int):\n        self.model = Pix2Pix.load_from_checkpoint(model_file)\n        self.model.eval()\n        self.model.to(device)\n        self.device = device\n        self.transforms_ = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((img_size, img_size)),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ])\n\n    def __call__(self, pic):\n        \"\"\"\n        @param pic (PIL Image or numpy.ndarray): Image to be converted to a face image.\n        @return: Tensor: Converted image.\n        \"\"\"\n        pic_transformed = self.transforms_(pic)\n        pic_transformed_device = pic_transformed.to(self.device)\n        pic_transformed_device_batched = torch.unsqueeze(pic_transformed_device, dim=0)\n        return self.model(pic_transformed_device_batched)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}\"", ""]}
{"filename": "lib/transform/face_crop_transformer.py", "chunked_list": ["from retinaface import RetinaFace\n\n\nclass FaceCrop:\n    \"\"\"\n    The FaceCrop transformer is based on the RetinaFace library which extracts all available faces from the given image.\n    \"\"\"\n\n    def __init__(self, align: bool, multiple_faces: bool):\n        \"\"\"\n        @param align: Whether the face should be aligned.\n        @param multiple_faces: Whether multiple faces should be detected.\n        \"\"\"\n        self.align = align\n        self.multiple_faces = multiple_faces\n\n    def __call__(self, pic):\n        \"\"\"\n        @param pic (PIL Image or numpy.ndarray): Image to be converted to cropped faces.\n        @return: numpy.ndarray: Converted image.\n        \"\"\"\n        faces = [i[:, :, ::-1] for i in RetinaFace.extract_faces(pic, align=self.align)]\n        if not self.multiple_faces:\n            if len(faces) > 0:\n                return faces[0]\n            else:\n                return None\n        return faces\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}\"", ""]}
{"filename": "lib/datasets/labeled_dataset.py", "chunked_list": ["import json\nimport os\nfrom typing import List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom loguru import logger\nfrom sklearn.utils import compute_class_weight", "from loguru import logger\nfrom sklearn.utils import compute_class_weight\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose\n\nfrom lib.datasets import DatasetSplit\nfrom lib.utils import glob_dir\n\n\nclass LabeledDataset(torch.utils.data.Dataset):\n    \"\"\"\n    The LabeledDataset wraps a folder structure of a dataset with train, val, and test folders\n    and the corresponding labels.csv with the meta information.\n    \"\"\"\n\n    def __init__(self, data_dir: str, split: DatasetSplit, transforms: Compose):\n        \"\"\"\n        Create a new LabeledDataset.\n        @param data_dir: The root directory of the dataset.\n        @param split: Which dataset split should be considered.\n        @param transforms: The transformations that are applied to the images before returning.\n        \"\"\"\n        data_dir = os.path.join(data_dir, split.value['folder'])\n        logger.debug(f'Setup LabeledDataset on path: {data_dir}')\n        # Load meta-data from labels.csv and filter by the split\n        df = pd.read_csv(os.path.join(os.path.dirname(data_dir), 'labels.csv'))\n        self.meta_data = df[df['split'] == split.value['value']]\n        # Search for all files available in the filesystem\n        files_found = glob_dir(data_dir)\n        file_names_found = [os.path.basename(f).split('-')[-1] for f in files_found]\n        logger.debug(f'Files found: {len(file_names_found)}')\n        # Extract base name from image_path in meta-data\n        self.meta_data['image_path'] = self.meta_data['image_path'].apply(lambda p: p.split('\\\\')[-1].split('/')[-1])\n        # Filter for all available images in files_found\n        self.meta_data = self.meta_data[self.meta_data['image_path'].isin(file_names_found)]\n        logger.debug(f'Files not available: {len(file_names_found) - len(self.meta_data)}')\n        self.meta_data['image_path'] = self.meta_data['image_path'].apply(\n            lambda p: files_found[file_names_found.index(p)] if p in file_names_found else p)\n        # Extract labels from meta-data\n        self.labels = list(self.meta_data.drop(['image_path', 'split'], axis=1).columns.values)[1:]\n        self.transforms = transforms\n        logger.info(f'Finished \"{os.path.basename(data_dir)}\" Dataset (total={len(self.meta_data)}) Setup!')\n\n    def __len__(self) -> int:\n        return len(self.meta_data)\n\n    def __getitem__(self, idx) -> Tuple[Tensor, np.ndarray]:\n        df = self.meta_data.iloc[idx]\n        img = Image.open(df[\"image_path\"])\n        img = self.transforms(img)\n        labels = df[self.labels].astype(float).to_numpy()\n        return img, labels\n\n    @property\n    def classes(self) -> List[str]:\n        \"\"\"\n        @return: The class labels as list.\n        \"\"\"\n        return self.labels\n\n    @property\n    def num_classes(self) -> int:\n        \"\"\"\n        @return: The number of classes.\n        \"\"\"\n        return len(self.classes)\n\n    @property\n    def is_multi_label(self) -> bool:\n        \"\"\"\n        @return: If the dataset represents a multiple labels at once.\n        \"\"\"\n        return (self.meta_data[self.labels].sum(axis=1) > 1).any()\n\n    @property\n    def class_weight(self) -> Tensor:\n        \"\"\"\n        Compute the class weights for the dataset.\n        @return: The class weights as tensor.\n        \"\"\"\n        logger.debug(f'Compute class weights with {self.num_classes} classes: {\", \".join(self.classes)}')\n        logger.debug(f'Sum of samples per class:\\n{self.meta_data[self.labels].sum(axis=0)}')\n        if self.is_multi_label:\n            y = self.meta_data[self.labels].to_numpy()\n            class_weight = np.empty([self.num_classes, 2])\n            for idx in range(self.num_classes):\n                class_weight[idx] = compute_class_weight(class_weight='balanced', classes=[0., 1.], y=y[:, idx])\n            class_weight = np.mean(class_weight, axis=1)\n        else:\n            y = np.argmax(self.meta_data[self.labels].to_numpy(), axis=1)\n            class_weight = compute_class_weight(class_weight='balanced', classes=range(self.num_classes), y=y)\n        class_weights_per_label = {self.labels[idx]: class_weight[idx] for idx in range(len(self.labels))}\n        logger.debug(f'Class-Weights for each Label={json.dumps(class_weights_per_label, indent=3)}')\n        return torch.from_numpy(class_weight)", "\n\nclass LabeledDataset(torch.utils.data.Dataset):\n    \"\"\"\n    The LabeledDataset wraps a folder structure of a dataset with train, val, and test folders\n    and the corresponding labels.csv with the meta information.\n    \"\"\"\n\n    def __init__(self, data_dir: str, split: DatasetSplit, transforms: Compose):\n        \"\"\"\n        Create a new LabeledDataset.\n        @param data_dir: The root directory of the dataset.\n        @param split: Which dataset split should be considered.\n        @param transforms: The transformations that are applied to the images before returning.\n        \"\"\"\n        data_dir = os.path.join(data_dir, split.value['folder'])\n        logger.debug(f'Setup LabeledDataset on path: {data_dir}')\n        # Load meta-data from labels.csv and filter by the split\n        df = pd.read_csv(os.path.join(os.path.dirname(data_dir), 'labels.csv'))\n        self.meta_data = df[df['split'] == split.value['value']]\n        # Search for all files available in the filesystem\n        files_found = glob_dir(data_dir)\n        file_names_found = [os.path.basename(f).split('-')[-1] for f in files_found]\n        logger.debug(f'Files found: {len(file_names_found)}')\n        # Extract base name from image_path in meta-data\n        self.meta_data['image_path'] = self.meta_data['image_path'].apply(lambda p: p.split('\\\\')[-1].split('/')[-1])\n        # Filter for all available images in files_found\n        self.meta_data = self.meta_data[self.meta_data['image_path'].isin(file_names_found)]\n        logger.debug(f'Files not available: {len(file_names_found) - len(self.meta_data)}')\n        self.meta_data['image_path'] = self.meta_data['image_path'].apply(\n            lambda p: files_found[file_names_found.index(p)] if p in file_names_found else p)\n        # Extract labels from meta-data\n        self.labels = list(self.meta_data.drop(['image_path', 'split'], axis=1).columns.values)[1:]\n        self.transforms = transforms\n        logger.info(f'Finished \"{os.path.basename(data_dir)}\" Dataset (total={len(self.meta_data)}) Setup!')\n\n    def __len__(self) -> int:\n        return len(self.meta_data)\n\n    def __getitem__(self, idx) -> Tuple[Tensor, np.ndarray]:\n        df = self.meta_data.iloc[idx]\n        img = Image.open(df[\"image_path\"])\n        img = self.transforms(img)\n        labels = df[self.labels].astype(float).to_numpy()\n        return img, labels\n\n    @property\n    def classes(self) -> List[str]:\n        \"\"\"\n        @return: The class labels as list.\n        \"\"\"\n        return self.labels\n\n    @property\n    def num_classes(self) -> int:\n        \"\"\"\n        @return: The number of classes.\n        \"\"\"\n        return len(self.classes)\n\n    @property\n    def is_multi_label(self) -> bool:\n        \"\"\"\n        @return: If the dataset represents a multiple labels at once.\n        \"\"\"\n        return (self.meta_data[self.labels].sum(axis=1) > 1).any()\n\n    @property\n    def class_weight(self) -> Tensor:\n        \"\"\"\n        Compute the class weights for the dataset.\n        @return: The class weights as tensor.\n        \"\"\"\n        logger.debug(f'Compute class weights with {self.num_classes} classes: {\", \".join(self.classes)}')\n        logger.debug(f'Sum of samples per class:\\n{self.meta_data[self.labels].sum(axis=0)}')\n        if self.is_multi_label:\n            y = self.meta_data[self.labels].to_numpy()\n            class_weight = np.empty([self.num_classes, 2])\n            for idx in range(self.num_classes):\n                class_weight[idx] = compute_class_weight(class_weight='balanced', classes=[0., 1.], y=y[:, idx])\n            class_weight = np.mean(class_weight, axis=1)\n        else:\n            y = np.argmax(self.meta_data[self.labels].to_numpy(), axis=1)\n            class_weight = compute_class_weight(class_weight='balanced', classes=range(self.num_classes), y=y)\n        class_weights_per_label = {self.labels[idx]: class_weight[idx] for idx in range(len(self.labels))}\n        logger.debug(f'Class-Weights for each Label={json.dumps(class_weights_per_label, indent=3)}')\n        return torch.from_numpy(class_weight)", ""]}
{"filename": "lib/datasets/image_dataset.py", "chunked_list": ["import os\n\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nfrom lib.utils import glob_dir\n\n\nclass ImageDataset(Dataset):\n    \"\"\"\n    An ImageDataset manages the access to images in a folder structure of train, val, and test.\n    \"\"\"\n\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        \"\"\"\n        Create a new ImageDataset.\n        @param root: The root folder of the dataset.\n        @param transforms_: The transformers to be applied to the image before the images are handed over.\n        @param mode: The mode either \"train\", \"val\", or \"test\".\n        \"\"\"\n        # Set True if training data shall be mirrored for augmentation purposes:\n        self._MIRROR_IMAGES = False\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob_dir(os.path.join(root, mode)))\n        self.files.extend(sorted(glob_dir(os.path.join(root, mode))))\n        if mode == \"train\":\n            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n\n    def __getitem__(self, index):\n        file_path = self.files[index % len(self.files)]\n        img = Image.open(file_path)\n        w, h = img.size\n        img_A = img.crop((0, 0, int(w / 2), h))\n        img_B = img.crop((int(w / 2), 0, w, h))\n        if self._MIRROR_IMAGES:\n            if np.random.random() < 0.5:\n                img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n                img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n        return {\"A\": img_A, \"B\": img_B, \"Filepath\": file_path}\n\n    def __len__(self):\n        return len(self.files)", "\n\nclass ImageDataset(Dataset):\n    \"\"\"\n    An ImageDataset manages the access to images in a folder structure of train, val, and test.\n    \"\"\"\n\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        \"\"\"\n        Create a new ImageDataset.\n        @param root: The root folder of the dataset.\n        @param transforms_: The transformers to be applied to the image before the images are handed over.\n        @param mode: The mode either \"train\", \"val\", or \"test\".\n        \"\"\"\n        # Set True if training data shall be mirrored for augmentation purposes:\n        self._MIRROR_IMAGES = False\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob_dir(os.path.join(root, mode)))\n        self.files.extend(sorted(glob_dir(os.path.join(root, mode))))\n        if mode == \"train\":\n            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n\n    def __getitem__(self, index):\n        file_path = self.files[index % len(self.files)]\n        img = Image.open(file_path)\n        w, h = img.size\n        img_A = img.crop((0, 0, int(w / 2), h))\n        img_B = img.crop((int(w / 2), 0, w, h))\n        if self._MIRROR_IMAGES:\n            if np.random.random() < 0.5:\n                img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n                img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n        return {\"A\": img_A, \"B\": img_B, \"Filepath\": file_path}\n\n    def __len__(self):\n        return len(self.files)", ""]}
{"filename": "lib/datasets/__init__.py", "chunked_list": ["from enum import Enum\n\n\nclass DatasetSplit(Enum):\n    \"\"\"\n    The split for a dataset and their specific folders.\n    \"\"\"\n    TRAIN = {'value': 0, 'folder': 'train'}\n    VALIDATION = {'value': 1, 'folder': 'val'}\n    TEST = {'value': 2, 'folder': 'test'}", ""]}
{"filename": "lib/datasets/label_extractor.py", "chunked_list": ["import os\nimport pathlib\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport pandas as pd\nfrom loguru import logger\nfrom tqdm import tqdm\n\n# i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise).", "\n# i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise).\nfrom lib.utils import glob_dir\n\nSEED = 42\n\nlabel_map = {\n    # Neutral\n    0: 0,\n    # Angry", "    0: 0,\n    # Angry\n    1: 6,\n    # Contempt\n    2: 7,\n    # Disgust\n    3: 5,\n    # Fear\n    4: 4,\n    # Happy", "    4: 4,\n    # Happy\n    5: 1,\n    # Sad\n    6: 2,\n    # Surprise\n    7: 3\n}\n\n\nclass LabelExtractor(ABC):\n    def __init__(self, dataset_dir: str):\n        self.dataset_dir = dataset_dir\n\n    @abstractmethod\n    def __call__(self) -> pd.DataFrame:\n        pass", "\n\nclass LabelExtractor(ABC):\n    def __init__(self, dataset_dir: str):\n        self.dataset_dir = dataset_dir\n\n    @abstractmethod\n    def __call__(self) -> pd.DataFrame:\n        pass\n", "\n\nclass CelebALabelExtractor(LabelExtractor):\n    def __init__(self, dataset_dir: str):\n        super().__init__(os.path.join(dataset_dir, 'celeba'))\n\n    def __call__(self) -> pd.DataFrame:\n        df = pd.read_csv(os.path.join(self.dataset_dir, 'list_eval_partition.txt'), delimiter=' ',\n                         header=None)\n        df = df.rename(columns={0: 'image_path', 1: 'split'})\n        df = df.merge(\n            pd.read_csv(os.path.join(self.dataset_dir, 'list_attr_celeba.txt'), delimiter='\\s+',\n                        skiprows=1).rename_axis('image_path').reset_index(), on='image_path', how='left')\n        labels = list(df.keys())[2:]\n        df[labels] = (df[labels] + 1) / 2\n        return df", "\n\nclass CKPlusLabelExtractor(LabelExtractor):\n    def __init__(self, dataset_dir: str):\n        super().__init__(dataset_dir)\n\n    def __call__(self) -> pd.DataFrame:\n        label_base_dir = os.path.join(self.dataset_dir, 'Emotion')\n        image_base_dir = os.path.join(self.dataset_dir, 'cohn-kanade-images')\n        val_split = 0.2\n        result = {\n            'subject_id': [],\n            'image_path': [],\n            'anger': [],\n            'contempt': [],\n            'disgust': [],\n            'fear': [],\n            'happy': [],\n            'sadness': [],\n            'surprise': [],\n        }\n        labels = list(result.keys())[2:]\n        for file in tqdm(glob_dir(image_base_dir)):\n            folder_structure = file.replace(image_base_dir, '').split(os.sep)\n            img_path = os.path.join(*folder_structure[:-1], os.path.basename(file))\n            labels_path = os.path.join(label_base_dir, *folder_structure[:-1])\n            label_path = os.path.join(labels_path, f'{pathlib.Path(file).stem}_emotion.txt')\n            # creating on hot encoding\n            if os.path.exists(label_path):\n                with open(label_path, 'r') as label_file:\n                    mapped_idx = int(float(label_file.readline().strip()))\n                    sid = folder_structure[-3]\n                    result['subject_id'].append(sid)\n                    result['image_path'].append(img_path)\n                    for idx, key in enumerate(labels):\n                        result[key].append(1 if idx == mapped_idx - 1 else 0)\n        df = pd.DataFrame(result)\n        df_grouped = df.groupby(['subject_id'])[labels].sum().sample(frac=val_split, random_state=SEED)\n        df_train = df.loc[~df['subject_id'].isin(df_grouped.index)]\n        df_train['split'] = [0] * len(df_train)\n        df_val = df.loc[df['subject_id'].isin(df_grouped.index)]\n        df_val['split'] = [1] * len(df_val)\n        return pd.concat([df_train, df_val]).drop('subject_id', axis=1)", "\n\nclass AffectNetLabelExtractor(LabelExtractor):\n    def __init__(self, dataset_dir: str):\n        super().__init__(dataset_dir)\n\n    def __call__(self) -> pd.DataFrame:\n        label_base_dir = os.path.join(self.dataset_dir, 'Manually_Annotated_file_lists')\n        result = self.parse_csv_images(os.path.join(label_base_dir, 'training.csv'), 0)\n        result.update(self.parse_csv_images(os.path.join(label_base_dir, 'validation.csv'), 1))\n        df = pd.DataFrame(result)\n        return df\n\n    def parse_csv_images(self, csv_file: str, split: int):\n        result = {\n            'image_path': [],\n            'neutral': [],\n            'anger': [],\n            'contempt': [],\n            'disgust': [],\n            'fear': [],\n            'happy': [],\n            'sadness': [],\n            'surprise': [],\n            'split': [],\n        }\n        df = pd.read_csv(csv_file)\n        for index, row in tqdm(df.iterrows()):\n            label = row['expression']\n            if label > 7:\n                continue\n            result['image_path'].append(row['#subDirectory_filePath'])\n            for idx, key in enumerate(list(result.keys())[2:-1]):\n                result[key].append(1 if idx == label else 0)\n            result['split'].append(split)\n        return result", "\n\nclass FacesLabelExtractor(LabelExtractor):\n    def __init__(self, dataset_dir: str):\n        super().__init__(dataset_dir)\n\n    def __call__(self) -> pd.DataFrame:\n        emotion_map = {\n            'n': 0,  # Neutrality\n            'h': 1,  # Happy\n            's': 2,  # Sadness\n            'f': 3,  # Fear\n            'd': 4,  # Disgust\n            'a': 5,  # Anger\n        }\n        result = {\n            'image_path': [],\n            'subject_id': [],\n            'neutral': [],\n            'happy': [],\n            'sadness': [],\n            'fear': [],\n            'disgust': [],\n            'anger': [],\n        }\n        val_set = ['004', '066', '079', '116', '140', '168']  # Subject photos officially usable for publications\n        val_split = 0.2\n        labels = list(result.keys())[2:]\n        for file in tqdm(glob_dir(os.path.join(self.dataset_dir, 'bilder'))):\n            file_split = pathlib.Path(file).stem.split('_')\n            subject_id = file_split[0]\n            emotion = file_split[3]\n            result['image_path'].append(os.path.basename(file))\n            result['subject_id'].append(subject_id)\n            mapped_idx = emotion_map[emotion]\n            for idx, key in enumerate(labels):\n                result[key].append(1 if idx == mapped_idx else 0)\n        df = pd.DataFrame(result)\n        df_grouped = df.drop(val_set).groupby(['subject_id'])[list(result.keys())[2:]].sum().sample(frac=val_split)\n        df_train = df[~df['subject_id'].isin(df_grouped.index)]\n        df_train['split'] = [0] * len(df_train)\n        df_val = df[df['subject_id'].isin(df_grouped.index)]\n        df_val['split'] = [1] * len(df_val)\n        return pd.concat([df_train, df_val]).drop('subject_id', axis=1)", "\n\nlabel_extractors = {\n    'CelebA': CelebALabelExtractor,\n    'CK+': CKPlusLabelExtractor,\n    'AffectNet': AffectNetLabelExtractor,\n    'FACES': FacesLabelExtractor,\n}\n\n\ndef extract_labels(dataset_path: str, output_path: str) -> Optional[str]:\n    \"\"\"\n    Extract the labels from the given dataset and bring it in a uniform format.\n    @param dataset_path: The path to the dataset.\n    @param output_path: The path to save the results.\n    @return: The path to the csv file or None if no extractor could be found.\n    \"\"\"\n    path_parts = dataset_path.split(os.sep)\n    for dataset_name in path_parts:\n        if dataset_name in label_extractors:\n            dataset_path = dataset_path[:dataset_path.index(dataset_name) + len(dataset_name)]\n            os.makedirs(output_path, exist_ok=True)\n            logger.debug(f'Extracting labels for {dataset_name}...')\n            # csv_file =\n            extractor_class = label_extractors[dataset_name]\n            extractor = extractor_class(dataset_path)\n            df = extractor()\n            csv_file = os.path.join(output_path, 'labels.csv')\n            df.to_csv(csv_file)\n            logger.info(f'Extracted labels from {dataset_name} to {output_path}')\n            df_anaylser = df.drop(['image_path'], axis=1)\n            logger.debug(f'Meta-Data:\\n{df_anaylser.drop([\"split\"], axis=1).sum()}')\n            logger.debug(f'Train-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 0].drop([\"split\"], axis=1).sum()}')\n            logger.debug(f'Validation-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 1].drop([\"split\"], axis=1).sum()}')\n            return csv_file\n    logger.warning(f'No label extractor found for {dataset_path}')\n    return None", "\n\ndef extract_labels(dataset_path: str, output_path: str) -> Optional[str]:\n    \"\"\"\n    Extract the labels from the given dataset and bring it in a uniform format.\n    @param dataset_path: The path to the dataset.\n    @param output_path: The path to save the results.\n    @return: The path to the csv file or None if no extractor could be found.\n    \"\"\"\n    path_parts = dataset_path.split(os.sep)\n    for dataset_name in path_parts:\n        if dataset_name in label_extractors:\n            dataset_path = dataset_path[:dataset_path.index(dataset_name) + len(dataset_name)]\n            os.makedirs(output_path, exist_ok=True)\n            logger.debug(f'Extracting labels for {dataset_name}...')\n            # csv_file =\n            extractor_class = label_extractors[dataset_name]\n            extractor = extractor_class(dataset_path)\n            df = extractor()\n            csv_file = os.path.join(output_path, 'labels.csv')\n            df.to_csv(csv_file)\n            logger.info(f'Extracted labels from {dataset_name} to {output_path}')\n            df_anaylser = df.drop(['image_path'], axis=1)\n            logger.debug(f'Meta-Data:\\n{df_anaylser.drop([\"split\"], axis=1).sum()}')\n            logger.debug(f'Train-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 0].drop([\"split\"], axis=1).sum()}')\n            logger.debug(f'Validation-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 1].drop([\"split\"], axis=1).sum()}')\n            return csv_file\n    logger.warning(f'No label extractor found for {dataset_path}')\n    return None", ""]}
{"filename": "lib/datasets/unlabeled_dataset.py", "chunked_list": ["import os\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom loguru import logger\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose", "from torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose\n\nfrom lib.datasets import DatasetSplit\nfrom lib.utils import glob_dir\n\n\nclass UnlabeledDataset(torch.utils.data.Dataset):\n    \"\"\"\n    The LabeledDataset wraps a folder structure of a dataset with train, val, and test folders\n    and the corresponding labels.csv with the meta information.\n    \"\"\"\n\n    def __init__(self, data_dir: str, split: DatasetSplit, transforms: Compose):\n        \"\"\"\n        Create a new LabeledDataset.\n        @param data_dir: The root directory of the dataset.\n        @param split: Which dataset split should be considered.\n        @param transforms: The transformations that are applied to the images before returning.\n        \"\"\"\n        data_dir = os.path.join(data_dir, split.value['folder'])\n        logger.debug(f'Setup LabeledDataset on path: {data_dir}')\n        # Search for all files available in the filesystem\n        self.files_found = glob_dir(data_dir)\n        self.transforms = transforms\n        logger.info(f'Finished \"{os.path.basename(data_dir)}\" Dataset (total={len(self)}) Setup!')\n\n    def __len__(self) -> int:\n        return len(self.files_found)\n\n    def __getitem__(self, idx) -> Tuple[Tensor, np.ndarray]:\n        img = Image.open(self.files_found[idx])\n        img = self.transforms(img)\n        return img, np.asarray([-1])", ""]}
