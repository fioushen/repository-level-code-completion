{"filename": "run.py", "chunked_list": ["import argparse\nimport numpy as np\nimport os\nimport sys\nimport yaml\nimport envs.env_builder as env_builder\nimport learning.agent_builder as agent_builder\nimport util.util as util\n\ndef set_np_formatting():\n    np.set_printoptions(edgeitems=30, infstr='inf',\n                        linewidth=4000, nanstr='nan', precision=2,\n                        suppress=False, threshold=10000, formatter=None)\n    return", "\ndef set_np_formatting():\n    np.set_printoptions(edgeitems=30, infstr='inf',\n                        linewidth=4000, nanstr='nan', precision=2,\n                        suppress=False, threshold=10000, formatter=None)\n    return\n\ndef load_args(argv):\n    parser = argparse.ArgumentParser(description=\"Train or test control policies.\")\n    \n    parser.add_argument(\"--rand_seed\", dest=\"rand_seed\", type=int, default=None)\n    parser.add_argument(\"--mode\", dest=\"mode\", type=str, default=\"train\")\n    parser.add_argument(\"--visualize\", dest=\"visualize\", action=\"store_true\", default=False)\n    parser.add_argument(\"--env_config\", dest=\"env_config\")\n    parser.add_argument(\"--agent_config\", dest=\"agent_config\")\n    parser.add_argument(\"--device\", dest=\"device\", type=str, default=\"cpu\")\n    parser.add_argument(\"--log_file\", dest=\"log_file\", type=str, default=\"output/log.txt\")\n    parser.add_argument(\"--out_model_file\", dest=\"out_model_file\", type=str, default=\"output/model.pt\")\n    parser.add_argument(\"--int_output_dir\", dest=\"int_output_dir\", type=str, default=\"\")\n    parser.add_argument(\"--model_file\", dest=\"model_file\", type=str, default=\"\")\n    parser.add_argument(\"--max_samples\", dest=\"max_samples\", type=np.int64, default=np.iinfo(np.int64).max)\n    parser.add_argument(\"--test_episodes\", dest=\"test_episodes\", type=np.int64, default=np.iinfo(np.int64).max)\n    \n    args = parser.parse_args()\n\n    if (args.rand_seed is not None):\n        util.set_rand_seed(args.rand_seed)\n\n    return args", "\ndef build_env(args, device, visualize):\n    env_file = args.env_config\n    env = env_builder.build_env(env_file, device, visualize)\n    return env\n\ndef build_agent(args, env, device):\n    agent_file = args.agent_config\n    agent = agent_builder.build_agent(agent_file, env, device)\n    return agent", "\ndef train(agent, max_samples, out_model_file, int_output_dir, log_file):\n    agent.train_model(max_samples=max_samples, out_model_file=out_model_file, \n                      int_output_dir=int_output_dir, log_file=log_file)\n    return\n\ndef test(agent, test_episodes):\n    result = agent.test_model(num_episodes=test_episodes)\n    print(\"Mean Return: {}\".format(result[\"mean_return\"]))\n    print(\"Mean Episode Length: {}\".format(result[\"mean_ep_len\"]))\n    print(\"Episodes: {}\".format(result[\"episodes\"]))\n    return result", "\ndef create_output_dirs(out_model_file, int_output_dir):\n    output_dir = os.path.dirname(out_model_file)\n    if (output_dir != \"\" and (not os.path.exists(output_dir))):\n        os.makedirs(output_dir, exist_ok=True)\n        \n    if (int_output_dir != \"\" and (not os.path.exists(int_output_dir))):\n        os.makedirs(int_output_dir, exist_ok=True)\n    return\n\ndef main(argv):\n    set_np_formatting()\n\n    args = load_args(argv)\n\n    mode = args.mode\n    device = args.device\n    visualize = args.visualize\n    log_file = args.log_file\n    out_model_file = args.out_model_file\n    int_output_dir = args.int_output_dir\n    model_file = args.model_file\n\n    create_output_dirs(out_model_file, int_output_dir)\n\n    env = build_env(args, device, visualize)\n    agent = build_agent(args, env, device)\n\n    if (model_file != \"\"):\n        agent.load(model_file)\n\n    if (mode == \"train\"):\n        max_samples = args.max_samples\n        train(agent=agent, max_samples=max_samples, out_model_file=out_model_file, \n              int_output_dir=int_output_dir, log_file=log_file)\n    elif (mode == \"test\"):\n        test_episodes = args.test_episodes\n        test(agent=agent, test_episodes=test_episodes)\n    else:\n        assert(False), \"Unsupported mode: {}\".format(mode)\n    return", "\ndef main(argv):\n    set_np_formatting()\n\n    args = load_args(argv)\n\n    mode = args.mode\n    device = args.device\n    visualize = args.visualize\n    log_file = args.log_file\n    out_model_file = args.out_model_file\n    int_output_dir = args.int_output_dir\n    model_file = args.model_file\n\n    create_output_dirs(out_model_file, int_output_dir)\n\n    env = build_env(args, device, visualize)\n    agent = build_agent(args, env, device)\n\n    if (model_file != \"\"):\n        agent.load(model_file)\n\n    if (mode == \"train\"):\n        max_samples = args.max_samples\n        train(agent=agent, max_samples=max_samples, out_model_file=out_model_file, \n              int_output_dir=int_output_dir, log_file=log_file)\n    elif (mode == \"test\"):\n        test_episodes = args.test_episodes\n        test(agent=agent, test_episodes=test_episodes)\n    else:\n        assert(False), \"Unsupported mode: {}\".format(mode)\n    return", "\nif __name__ == \"__main__\":\n    main(sys.argv)\n"]}
{"filename": "tools/util/plot_util.py", "chunked_list": ["import numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import reduce\n\ndef plot_line(x_data, y_data, std_data=[], label='', line_style=None, color=None, draw_band=True):\n    x_data = x_data if isinstance(x_data, list) else [x_data] \n    y_data = y_data if isinstance(y_data, list) else [y_data] \n    std_data = std_data if isinstance(std_data, list) else [std_data] \n\n    min_x = np.inf\n    max_x = -np.inf\n    min_y = np.inf\n    max_y = -np.inf\n    min_len = int(reduce(lambda x, y: np.minimum(x, len(y)), x_data, np.inf))\n\n    if draw_band:\n        x_data = list(map(lambda x: x[:min_len], x_data))\n        y_data = list(map(lambda x: x[:min_len], y_data))\n        std_data = list(map(lambda x: x[:min_len], std_data))\n    \n        xs = np.mean(x_data, axis=0)\n        ys = np.mean(y_data, axis=0)\n        min_x = np.min(xs)\n        max_x = np.max(xs)\n        min_y = np.min(ys)\n        max_y = np.max(ys)\n\n        stds = None\n        if (len(y_data) > 1):\n            stds = np.std(y_data, axis=0)\n        elif (len(std_data) > 0):\n            stds = np.mean(std_data, axis=0)\n\n        curr_line = plt.plot(xs, ys, label=label, linestyle=line_style, color=color)\n\n        if stds is not None:\n            plt.fill_between(xs, ys - stds, ys + stds, alpha=0.25, linewidth=0,\n                             facecolor=curr_line[0].get_color(), label='_nolegend_')\n    else:\n        for i in range(len(x_data)):\n            alpha = 0.8 if (len(x_data) > 1) else 1.0\n            xs = x_data[i]\n            ys = y_data[i]\n            \n            min_x = np.minimum(np.min(xs), min_x)\n            max_x = np.maximum(np.max(xs), max_x)\n            min_y = np.minimum(np.min(ys), min_y)\n            max_y = np.maximum(np.max(ys), max_y)\n\n            if (i == 0):\n                curr_line = plt.plot(xs, ys, label=label, alpha=alpha, linestyle=line_style, color=color)\n            else:\n                curr_line = plt.plot(xs, ys, color=curr_line[0].get_color(), linestyle=line_style, label='_nolegend_', alpha=alpha)\n\n            if len(std_data) > 0:\n                stds = std_data[i]\n                plt.fill_between(xs, ys - stds, ys + stds, alpha=0.25, linewidth=0,\n                                 facecolor=curr_line[0].get_color(), label='_nolegend_')\n\n    return min_x, max_x, min_y, max_y"]}
{"filename": "tools/plot_log/plot_log.py", "chunked_list": ["import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom functools import reduce\n\nimport sys\nsys.path.append(os.getcwd())\n\nimport tools.util.plot_util as plot_util\n", "import tools.util.plot_util as plot_util\n\nfiles = [\n    \"output/log.txt\",\n]\n\ndraw_band = True\nx_key = \"Samples\"\ny_key = \"Test_Return\"\nplot_title = \"BC - Cheetah\"", "y_key = \"Test_Return\"\nplot_title = \"BC - Cheetah\"\nstd_key = None\n\ndef filter_data(x, window_size):\n    n = len(x)\n    filter_n = n // window_size\n    x = x[:filter_n * window_size]\n    x = np.reshape(x, [filter_n, window_size])\n    filter_x = np.mean(x, axis=-1)\n    return filter_x", "\nplt.figure(figsize=(5.5 * 0.8, 4 * 0.8))\n\nmin_x = np.inf\nmax_x = -np.inf\nfilter_window_size = 1\n\nfor f, file_group in enumerate(files):\n    if not isinstance(file_group, list):\n        if os.path.isdir(file_group):\n            files = os.listdir(file_group)\n            files = filter(lambda f: \"log\" in f, files)\n            file_group = list(map(lambda f: file_group + \"/\" + f, files))\n        else:\n            file_group = [file_group]\n\n    x_data = []\n    y_data = []\n    std_data = []\n    num_files = len(file_group)\n\n    for file in file_group:\n        with open(file, \"r\") as file_data:\n            clean_lines = [line.replace(\",\", \"\\t\") for line in file_data]\n            data = np.genfromtxt(clean_lines, delimiter=None, dtype=None, names=True)\n        \n        data_x_key = x_key\n        data_y_key = y_key\n        curr_window_size = filter_window_size\n\n        if data_x_key in data.dtype.names and data_y_key in data.dtype.names:\n            xs = data[data_x_key]\n            ys = data[data_y_key]\n            xs = filter_data(xs, curr_window_size)\n            ys = filter_data(ys, curr_window_size)\n\n            stds = None\n            if std_key is not None and std_key in data.dtype.names:\n                stds = data[std_key]\n                stds = filter_data(stds, curr_window_size)\n                std_data.append(stds)\n\n            x_data.append(xs)\n            y_data.append(ys)\n\n    label = os.path.basename(file_group[0])\n    label = os.path.splitext(label)[0]\n\n    line_col = None\n    curr_min_x, curr_max_x, _, _ = plot_util.plot_line(x_data, y_data, std_data, label, color=line_col,\n                                                      draw_band=draw_band)\n\n    min_len = int(reduce(lambda x, y: np.minimum(x, len(y)), x_data, np.inf))\n    x_final = x_data[0][min_len - 1]\n    y_final = np.array([y[min_len - 1] for y in y_data])\n    print(\"Final value: {:.2f}, {:.5f} +/- {:.5f}\".format(x_final, np.mean(y_final), np.std(y_final)))\n    \n    min_x = min(curr_min_x, min_x)\n    max_x = max(curr_max_x, max_x)", "\n\nax = plt.gca()\n\nplt.xlabel(x_key)\nplt.ylabel(y_key)\nplt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n\nplt.grid(linestyle='dotted')\nax.xaxis.grid(True)", "plt.grid(linestyle='dotted')\nax.xaxis.grid(True)\nax.yaxis.grid(True)\n\nplt.legend()\nplt.title(plot_title)\nplt.tight_layout()\n\nplt.show()", "plt.show()"]}
{"filename": "util/torch_util.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef add_torch_dict(in_dict, out_dict):\n    for k, v in in_dict.items():\n        if (v.requires_grad):\n            v = v.detach()\n\n        if (k in out_dict):\n            out_dict[k] += v\n        else:\n            out_dict[k] = v\n    return", "        \ndef scale_torch_dict(scale, out_dict):\n    for k in out_dict.keys():\n        out_dict[k] *= scale\n    return\n\ndef calc_layers_out_size(layers):\n    modules = list(layers.modules())\n    for m in reversed(modules):\n        if hasattr(m, \"out_features\"):\n            out_size = m.out_features\n            break\n    return out_size", "\n\ndef torch_dtype_to_numpy(torch_dtype):\n    if (torch_dtype == torch.float32):\n        numpy_dtype = np.float32\n    elif (torch_dtype == torch.float64):\n        numpy_dtype = np.float64\n    elif (torch_dtype == torch.uint8):\n        numpy_dtype = np.uint8\n    elif (torch_dtype == torch.int64):\n        numpy_dtype = np.int64\n    else:\n        assert(False), \"Unsupported type {}\".format(torch_dtype)\n    return numpy_dtype", "\ndef numpy_dtype_to_torch(numpy_dtype):\n    if (numpy_dtype == np.float32):\n        torch_dtype = torch.float32\n    elif (numpy_dtype == np.float64):\n        torch_dtype = torch.float64\n    elif (numpy_dtype == np.uint8):\n        torch_dtype = torch.uint8\n    elif (numpy_dtype == np.int64):\n        torch_dtype = torch.int64\n    else:\n        assert(False), \"Unsupported type {}\".format(numpy_dtype)\n    return torch_dtype", "\nclass UInt8ToFloat(torch.nn.Module):\n    def forward(self, x):\n        float_x = x.type(torch.float32) / 255.0\n        return float_x"]}
{"filename": "util/tb_logger.py", "chunked_list": ["import os\nimport subprocess\nimport tensorboardX\nimport time\n\nimport util.logger as logger\n\nclass TBLogger(logger.Logger):\n    MISC_TAG = \"Misc\"\n\n    def __init__(self, run_tb=False):\n        super().__init__()\n\n        self._writer = None\n        self._step_var_key = None\n        self._collections = dict()\n\n        self._run_tb = run_tb\n        self._tb_proc = None\n\n        return\n\n    def __del__(self):\n        if (self._tb_proc is not None):\n            self._tb_proc.kill()\n        return\n\n    def reset(self):\n        super().reset()\n        return\n\n    def configure_output_file(self, filename=None):\n        super().configure_output_file(filename)\n\n        output_dir = os.path.dirname(filename)\n        self._delete_event_files(output_dir)\n        self._writer = tensorboardX.SummaryWriter(output_dir)\n\n        if (self._run_tb):\n            self._run_tensorboard(output_dir)\n\n        return\n\n    def set_step_key(self, var_key):\n        self._step_key = var_key\n        return\n\n    def log(self, key, val, collection=None, quiet=False):\n        super().log(key, val, quiet)\n\n        if (collection is not None):\n            self._add_collection(collection, key)\n        return\n\n    def write_log(self):\n        row_count = self._row_count\n\n        super().write_log()\n\n        if (self._writer is not None):\n            if (row_count == 0):\n                self._key_tags = self._build_key_tags()\n\n            curr_time = time.time()\n            step_val = row_count\n            if (self._step_key is not None):\n                step_val = self.log_current_row.get(self._step_key, \"\").val\n\n            vals = []\n            for i, key in enumerate(self.log_headers):\n                if (key != self._step_key):\n                    entry = self.log_current_row.get(key, \"\")\n                    val = entry.val\n                    tag = self._key_tags[i]\n                    self._writer.add_scalar(tag, val, step_val)\n\n        return\n    \n    def _add_collection(self, name, key):\n        if (name not in self._collections):\n            self._collections[name] = []\n        self._collections[name].append(key)\n        return\n\n    def _delete_event_files(self, dir):\n        if (os.path.exists(dir)):\n            files = os.listdir(dir)\n            for file in files:\n                if (\"events.out.tfevents.\" in file):\n                    file_path = os.path.join(dir, file)\n                    print(\"Deleting event file: {:s}\".format(file_path))\n                    os.remove(file_path)\n        return\n\n    def _build_key_tags(self):\n        tags = []\n        for key in self.log_headers:\n            curr_tag = TBLogger.MISC_TAG\n            for col_tag, col_keys in self._collections.items():\n                if key in col_keys:\n                    curr_tag = col_tag\n\n            curr_tags = \"{:s}/{:s}\".format(curr_tag, key)\n            tags.append(curr_tags)\n\n        return tags\n\n    def _run_tensorboard(self, output_dir):\n        cmd = \"tensorboard --logdir {:s}\".format(output_dir)\n        self._tb_proc = subprocess.Popen(cmd, shell=True)\n        return"]}
{"filename": "util/logger.py", "chunked_list": ["import os.path as osp, shutil, time, atexit, os, subprocess\n\nclass Logger:\n    class Entry:\n        def __init__(self, val, quiet=False):\n            self.val = val\n            self.quiet = quiet\n            return\n\n    def __init__(self):\n        self.output_file = None\n        self.log_headers = []\n        self.log_current_row = {}\n        self._dump_str_template = \"\"\n        self._max_key_len = 0\n        self._row_count = 0\n        return\n\n    def reset(self):\n        self._row_count = 0\n        self.log_headers = []\n        self.log_current_row = {}\n        if self.output_file is not None:\n            self.output_file = open(output_path, 'w')\n        return\n\n    def configure_output_file(self, filename=None):\n        \"\"\"\n        Set output directory to d, or to /tmp/somerandomnumber if d is None\n        \"\"\"\n        self._row_count = 0\n        self.log_headers = []\n        self.log_current_row = {}\n\n        output_path = filename or \"output/log_%i.txt\"%int(time.time())\n        \n        out_dir = os.path.dirname(output_path)\n\n        if (not os.path.exists(out_dir)):\n            os.makedirs(out_dir, exist_ok=True)\n        \n        self.output_file = open(output_path, 'w')\n        assert osp.exists(output_path)\n        atexit.register(self.output_file.close)\n\n        print(\"Logging data to \" + self.output_file.name)\n\n        return\n\n    def log(self, key, val, quiet=False):\n        \"\"\"\n        Log a value of some diagnostic\n        Call this once for each diagnostic quantity, each iteration\n        \"\"\"\n        if (self._row_count == 0) and key not in self.log_headers:\n            self.log_headers.append(key)\n            self._max_key_len = max(self._max_key_len, len(key))\n        else:\n            assert key in self.log_headers, \"Trying to introduce a new key %s that you didn't include in the first iteration\"%key\n        self.log_current_row[key] = Logger.Entry(val, quiet)\n        return\n\n    def get_num_keys(self):\n        return len(self.log_headers)\n\n    def print_log(self):\n        \"\"\"\n        Print all of the diagnostics from the current iteration\n        \"\"\"\n        \n        key_spacing = self._max_key_len\n        format_str = \"| %\" + str(key_spacing) + \"s | %15s |\"\n\n        vals = []\n        print(\"-\" * (22 + key_spacing))\n        for key in self.log_headers:\n            entry = self.log_current_row.get(key, \"\")\n            if not (entry.quiet):\n                val = entry.val\n\n                if isinstance(val, float):\n                    valstr = \"%8.3g\"%val\n                elif isinstance(val, int):\n                    valstr = str(val)\n                else: \n                    valstr = val\n\n                print(format_str%(key, valstr))\n                vals.append(val)\n        print(\"-\" * (22 + key_spacing))\n        return\n\n    def write_log(self):\n        \"\"\"\n        Write all of the diagnostics from the current iteration\n        \"\"\"\n        if (self._row_count == 0):\n            self._dump_str_template = self._build_str_template()\n\n        vals = []\n        for key in self.log_headers:\n            entry = self.log_current_row.get(key, \"\")\n            val = entry.val\n            vals.append(val)\n            \n        if self.output_file is not None:\n            if (self._row_count == 0):\n                header_str = self._dump_str_template.format(*self.log_headers)\n                self.output_file.write(header_str + \"\\r\")\n\n            val_str = self._dump_str_template.format(*map(str,vals))\n            self.output_file.write(val_str + \"\\r\")\n            self.output_file.flush()\n\n        self._row_count += 1\n        return\n\n    def has_key(self, key):\n        return key in self.log_headers\n\n    def get_current_val(self, key):\n        val = None\n        if (key in self.log_current_row.keys()):\n            entry = self.log_current_row[key]\n            val = entry.val\n        return val\n\n    def _build_str_template(self):\n        num_keys = self.get_num_keys()\n        template = \"{:<25}\" * num_keys\n        return template"]}
{"filename": "util/util.py", "chunked_list": ["import random\nimport numpy as np\nimport torch\n\ndef set_rand_seed(seed):\n    print(\"Setting seed: {}\".format(seed))\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return"]}
{"filename": "util/math_util.py", "chunked_list": ["import numpy as np\n\nRAD_TO_DEG = 180.0 / np.pi\nDEG_TO_RAD = np.pi/ 180.0\nINVALID_IDX = -1\n\ndef lerp(x, y, t):\n    return (1 - t) * x + t * y\n\ndef log_lerp(x, y, t):\n    return np.exp(lerp(np.log(x), np.log(y), t))", "\ndef log_lerp(x, y, t):\n    return np.exp(lerp(np.log(x), np.log(y), t))\n\ndef flatten(arr_list):\n    return np.concatenate([np.reshape(a, [-1]) for a in arr_list], axis=0)\n\ndef flip_coin(p):\n    rand_num = np.random.binomial(1, p, 1)\n    return rand_num[0] == 1", "\ndef add_average(avg0, count0, avg1, count1):\n\ttotal = count0 + count1\n\treturn (float(count0) / total) * avg0 + (float(count1) / total) * avg1\n\ndef smooth_step(x):\n    smooth_x = x * x * x * (x * (x * 6 - 15) + 10)\n    return smooth_x"]}
{"filename": "a3/dqn_agent.py", "chunked_list": ["import gym\nimport numpy as np\nimport torch\n\nimport envs.base_env as base_env\nimport learning.base_agent as base_agent\nimport learning.dqn_model as dqn_model\nimport util.torch_util as torch_util\n\nclass DQNAgent(base_agent.BaseAgent):\n    NAME = \"DQN\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n\n        buffer_size = config[\"exp_buffer_size\"]\n        self._exp_buffer_length = int(buffer_size)\n        self._exp_buffer_length = max(self._exp_buffer_length, self._steps_per_iter)\n\n        self._updates_per_iter = config[\"updates_per_iter\"]\n        self._batch_size = config[\"batch_size\"]\n        self._init_samples = config[\"init_samples\"]\n        self._tar_net_update_iters = config[\"tar_net_update_iters\"]\n        \n        self._exp_anneal_samples = config.get(\"exp_anneal_samples\", np.inf)\n        self._exp_prob_beg = config.get(\"exp_prob_beg\", 1.0)\n        self._exp_prob_end = config.get(\"exp_prob_end\", 1.0)\n\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = dqn_model.DQNModel(model_config, self._env)\n\n        self._tar_model = dqn_model.DQNModel(model_config, self._env)\n        for param in self._tar_model.parameters():\n            param.requires_grad = False\n\n        self._sync_tar_model()\n        return\n    \n    def _get_exp_buffer_length(self):\n        return self._exp_buffer_length\n\n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        qs = self._model.eval_q(norm_obs)\n\n        if (self._mode == base_agent.AgentMode.TRAIN):\n            a = self._sample_action(qs)\n        elif (self._mode == base_agent.AgentMode.TEST):\n            a = torch.argmax(qs, dim=-1)\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n\n        a_info = {}\n        return a, a_info\n\n    def _init_train(self):\n        super()._init_train()\n        self._collect_init_samples(self._init_samples)\n        return\n\n    def _collect_init_samples(self, samples):\n        self.eval()\n        self.set_mode(base_agent.AgentMode.TRAIN)\n        self._rollout_train(samples)\n        return\n\n    def _update_model(self):\n        self.train()\n        \n        train_info = dict()\n\n        for i in range(self._updates_per_iter):\n            batch = self._exp_buffer.sample(self._batch_size)\n            loss_info = self._compute_loss(batch)\n                \n            self._optimizer.zero_grad()\n            loss = loss_info[\"loss\"]\n            loss.backward()\n            self._optimizer.step()\n            \n            torch_util.add_torch_dict(loss_info, train_info)\n        \n        torch_util.scale_torch_dict(1.0 / self._updates_per_iter, train_info)\n\n        if (self._iter % self._tar_net_update_iters == 0):\n            self._sync_tar_model()\n\n        return train_info\n    \n    def _log_train_info(self, train_info, test_info, start_time):\n        super()._log_train_info(train_info, test_info, start_time)\n        self._logger.log(\"Exp_Prob\", self._get_exp_prob())\n        return\n\n    def _compute_loss(self, batch):\n        r = batch[\"reward\"]\n        done = batch[\"done\"]\n        a = batch[\"action\"]\n        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n        norm_next_obs = self._obs_norm.normalize(batch[\"next_obs\"])\n        \n        tar_vals = self._compute_tar_vals(r, norm_next_obs, done)\n        q_loss = self._compute_q_loss(norm_obs, a, tar_vals)\n        \n        info = {\"loss\": q_loss}\n        return info\n    \n\n    \n    def _get_exp_prob(self):\n        '''\n        TODO 1.1: Calculate the epsilon-greedy exploration probability given the current sample\n        count. The exploration probability should start with a value of self._exp_prob_beg, and\n        then linearly annealed to self._exp_prob_end over the course of self._exp_anneal_samples\n        timesteps.\n        '''\n\n        # placeholder\n        prob = 1.0\n\n        return prob\n\n    def _sample_action(self, qs):\n        '''\n        TODO 1.2: Sample actions according to the Q-values of each action (qs). Implement epsilon\n        greedy exploration, where the probability of sampling a random action (epsilon) is specified\n        by self._get_exp_prob(). With probability 1 - epsilon, greedily select the action with\n        the highest Q-value. With probability epsilon, select a random action uniformly from the\n        set of possible actions. The output (a) should be a tensor containing the index of the selected\n        action.\n        '''\n        exp_prob = self._get_exp_prob()\n\n        # placeholder\n        a = torch.zeros(qs.shape[0], device=self._device, dtype=torch.int64)\n        return a\n    \n    def _compute_tar_vals(self, r, norm_next_obs, done):\n        '''\n        TODO 1.3: Compute target values for updating the Q-function. The inputs consist of a tensor\n        of rewards (r), normalized observations at the next timestep (norm_next_obs), and done flags\n        (done) indicating if a timestep is the last timestep of an episode. The output (tar_vals)\n        should be a tensor containing the target values for each sample. The target values should\n        be calculated using the target model (self._tar_model), not the main model (self._model).\n        The Q-function can be queried by using the method eval_q(norm_obs).\n        '''\n        \n        # placeholder\n        tar_vals = torch.zeros_like(r)\n\n        return tar_vals\n\n    def _compute_q_loss(self, norm_obs, a, tar_vals):\n        '''\n        TODO 1.4: Compute a loss for updating the Q-function. The inputs consist of a tensor of\n        normalized observations (norm_obs), a tensor containing the indices of actions selected\n        at each timestep (a), and target values for each timestep (tar_vals). The output (loss)\n        should be a scalar tensor containing the loss for updating the Q-function.\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        \n        return loss\n    \n    def _sync_tar_model(self):\n        '''\n        TODO 1.5: Update the target model by copying the parameters from the main model. The\n        main model is given by self._model, and the target model is given by self._tar_model.\n        HINT: self._model.parameters() can be used to retrieve a list of tensors containing\n        the parameters of a model.\n        '''\n        \n        return", "\nclass DQNAgent(base_agent.BaseAgent):\n    NAME = \"DQN\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n\n        buffer_size = config[\"exp_buffer_size\"]\n        self._exp_buffer_length = int(buffer_size)\n        self._exp_buffer_length = max(self._exp_buffer_length, self._steps_per_iter)\n\n        self._updates_per_iter = config[\"updates_per_iter\"]\n        self._batch_size = config[\"batch_size\"]\n        self._init_samples = config[\"init_samples\"]\n        self._tar_net_update_iters = config[\"tar_net_update_iters\"]\n        \n        self._exp_anneal_samples = config.get(\"exp_anneal_samples\", np.inf)\n        self._exp_prob_beg = config.get(\"exp_prob_beg\", 1.0)\n        self._exp_prob_end = config.get(\"exp_prob_end\", 1.0)\n\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = dqn_model.DQNModel(model_config, self._env)\n\n        self._tar_model = dqn_model.DQNModel(model_config, self._env)\n        for param in self._tar_model.parameters():\n            param.requires_grad = False\n\n        self._sync_tar_model()\n        return\n    \n    def _get_exp_buffer_length(self):\n        return self._exp_buffer_length\n\n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        qs = self._model.eval_q(norm_obs)\n\n        if (self._mode == base_agent.AgentMode.TRAIN):\n            a = self._sample_action(qs)\n        elif (self._mode == base_agent.AgentMode.TEST):\n            a = torch.argmax(qs, dim=-1)\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n\n        a_info = {}\n        return a, a_info\n\n    def _init_train(self):\n        super()._init_train()\n        self._collect_init_samples(self._init_samples)\n        return\n\n    def _collect_init_samples(self, samples):\n        self.eval()\n        self.set_mode(base_agent.AgentMode.TRAIN)\n        self._rollout_train(samples)\n        return\n\n    def _update_model(self):\n        self.train()\n        \n        train_info = dict()\n\n        for i in range(self._updates_per_iter):\n            batch = self._exp_buffer.sample(self._batch_size)\n            loss_info = self._compute_loss(batch)\n                \n            self._optimizer.zero_grad()\n            loss = loss_info[\"loss\"]\n            loss.backward()\n            self._optimizer.step()\n            \n            torch_util.add_torch_dict(loss_info, train_info)\n        \n        torch_util.scale_torch_dict(1.0 / self._updates_per_iter, train_info)\n\n        if (self._iter % self._tar_net_update_iters == 0):\n            self._sync_tar_model()\n\n        return train_info\n    \n    def _log_train_info(self, train_info, test_info, start_time):\n        super()._log_train_info(train_info, test_info, start_time)\n        self._logger.log(\"Exp_Prob\", self._get_exp_prob())\n        return\n\n    def _compute_loss(self, batch):\n        r = batch[\"reward\"]\n        done = batch[\"done\"]\n        a = batch[\"action\"]\n        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n        norm_next_obs = self._obs_norm.normalize(batch[\"next_obs\"])\n        \n        tar_vals = self._compute_tar_vals(r, norm_next_obs, done)\n        q_loss = self._compute_q_loss(norm_obs, a, tar_vals)\n        \n        info = {\"loss\": q_loss}\n        return info\n    \n\n    \n    def _get_exp_prob(self):\n        '''\n        TODO 1.1: Calculate the epsilon-greedy exploration probability given the current sample\n        count. The exploration probability should start with a value of self._exp_prob_beg, and\n        then linearly annealed to self._exp_prob_end over the course of self._exp_anneal_samples\n        timesteps.\n        '''\n\n        # placeholder\n        prob = 1.0\n\n        return prob\n\n    def _sample_action(self, qs):\n        '''\n        TODO 1.2: Sample actions according to the Q-values of each action (qs). Implement epsilon\n        greedy exploration, where the probability of sampling a random action (epsilon) is specified\n        by self._get_exp_prob(). With probability 1 - epsilon, greedily select the action with\n        the highest Q-value. With probability epsilon, select a random action uniformly from the\n        set of possible actions. The output (a) should be a tensor containing the index of the selected\n        action.\n        '''\n        exp_prob = self._get_exp_prob()\n\n        # placeholder\n        a = torch.zeros(qs.shape[0], device=self._device, dtype=torch.int64)\n        return a\n    \n    def _compute_tar_vals(self, r, norm_next_obs, done):\n        '''\n        TODO 1.3: Compute target values for updating the Q-function. The inputs consist of a tensor\n        of rewards (r), normalized observations at the next timestep (norm_next_obs), and done flags\n        (done) indicating if a timestep is the last timestep of an episode. The output (tar_vals)\n        should be a tensor containing the target values for each sample. The target values should\n        be calculated using the target model (self._tar_model), not the main model (self._model).\n        The Q-function can be queried by using the method eval_q(norm_obs).\n        '''\n        \n        # placeholder\n        tar_vals = torch.zeros_like(r)\n\n        return tar_vals\n\n    def _compute_q_loss(self, norm_obs, a, tar_vals):\n        '''\n        TODO 1.4: Compute a loss for updating the Q-function. The inputs consist of a tensor of\n        normalized observations (norm_obs), a tensor containing the indices of actions selected\n        at each timestep (a), and target values for each timestep (tar_vals). The output (loss)\n        should be a scalar tensor containing the loss for updating the Q-function.\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        \n        return loss\n    \n    def _sync_tar_model(self):\n        '''\n        TODO 1.5: Update the target model by copying the parameters from the main model. The\n        main model is given by self._model, and the target model is given by self._tar_model.\n        HINT: self._model.parameters() can be used to retrieve a list of tensors containing\n        the parameters of a model.\n        '''\n        \n        return"]}
{"filename": "learning/bc_model.py", "chunked_list": ["import torch\n\nimport learning.base_model as base_model\nimport learning.nets.net_builder as net_builder\n\nclass BCModel(base_model.BaseModel):\n    def __init__(self, config, env):\n        super().__init__(config, env)\n\n        self._build_nets(config, env)\n        return\n    \n    def _build_nets(self, config, env):\n        self._build_actor(config, env)\n        return\n\n    def _build_actor(self, config, env):\n        net_name = config[\"actor_net\"]\n        input_dict = self._build_actor_input_dict(env)\n        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                activation=self._activation)\n        \n        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n        return\n    \n    \n    def _build_actor_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict\n    \n    def eval_actor(self, obs):\n        h = self._actor_layers(obs)\n        a_dist = self._action_dist(h)\n        return a_dist  "]}
{"filename": "learning/expert_agent.py", "chunked_list": ["import learning.base_agent as base_agent\nimport learning.expert_model as expert_model\n\nclass ExpertAgent(base_agent.BaseAgent):\n    NAME = \"Expert\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = expert_model.ExpertModel(model_config, self._env)\n        return\n    \n    def _get_exp_buffer_length(self):\n        return 1\n\n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        norm_action_dist = self._model.eval_actor(norm_obs)\n\n        if (self._mode == base_agent.AgentMode.TRAIN):\n            norm_a = norm_action_dist.sample()\n        elif (self._mode == base_agent.AgentMode.TEST):\n            norm_a = norm_action_dist.mode\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n            \n        norm_a_logp = norm_action_dist.log_prob(norm_a)\n\n        norm_a = norm_a.detach()\n        norm_a_logp = norm_a_logp.detach()\n        a = self._a_norm.unnormalize(norm_a)\n\n        a_info = {\n            \"a_logp\": norm_a_logp\n        }\n\n        return a, a_info"]}
{"filename": "learning/base_agent.py", "chunked_list": ["import abc\nimport enum\nimport gym\nimport numpy as np\nimport os\nimport time\nimport torch\n\nimport envs.base_env as base_env\nimport learning.experience_buffer as experience_buffer", "import envs.base_env as base_env\nimport learning.experience_buffer as experience_buffer\nimport learning.normalizer as normalizer\nimport learning.return_tracker as return_tracker\nimport util.tb_logger as tb_logger\nimport util.torch_util as torch_util\n\nimport learning.distribution_gaussian_diag as distribution_gaussian_diag\n\nclass AgentMode(enum.Enum):\n    TRAIN = 0\n    TEST = 1", "\nclass AgentMode(enum.Enum):\n    TRAIN = 0\n    TEST = 1\n\nclass BaseAgent(torch.nn.Module):\n    NAME = \"base\"\n\n    def __init__(self, config, env, device):\n        super().__init__()\n\n        self._env = env\n        self._device = device\n        self._iter = 0\n        self._sample_count = 0\n        self._load_params(config)\n\n        self._build_normalizers()\n        self._build_model(config)\n        self._build_optimizer(config)\n\n        self._build_exp_buffer(config)\n        self._build_return_tracker()\n        \n        self._mode = AgentMode.TRAIN\n        self._curr_obs = None\n        self._curr_info = None\n\n        self.to(self._device)\n\n        return\n\n    def train_model(self, max_samples, out_model_file, int_output_dir, log_file):\n        start_time = time.time()\n\n        self._curr_obs, self._curr_info = self._env.reset()\n        self._logger = self._build_logger(log_file)\n        self._init_train()\n\n        test_info = None\n        while self._sample_count < max_samples:\n            train_info = self._train_iter()\n            \n            output_iter = (self._iter % self._iters_per_output == 0)\n            if (output_iter):\n                test_info = self.test_model(self._test_episodes)\n            \n            self._sample_count = self._update_sample_count()\n            self._log_train_info(train_info, test_info, start_time) \n            self._logger.print_log()\n\n            if (output_iter):\n                self._logger.write_log()\n                self._output_train_model(self._iter, out_model_file, int_output_dir)\n\n                self._train_return_tracker.reset()\n                self._curr_obs, self._curr_info = self._env.reset()\n            \n            self._iter += 1\n\n        return\n\n    def test_model(self, num_episodes):\n        self.eval()\n        self.set_mode(AgentMode.TEST)\n\n        self._curr_obs, self._curr_info = self._env.reset()\n        test_info = self._rollout_test(num_episodes)\n\n        return test_info\n    \n    def get_action_size(self):\n        a_space = self._env.get_action_space()\n        if (isinstance(a_space, gym.spaces.Box)):\n            a_size = np.prod(a_space.shape)\n        elif (isinstance(a_space, gym.spaces.Discrete)):\n            a_size = 1\n        else:\n            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n        return a_size\n    \n    def set_mode(self, mode):\n        self._mode = mode\n        if (self._mode == AgentMode.TRAIN):\n            self._env.set_mode(base_env.EnvMode.TRAIN)\n        elif (self._mode == AgentMode.TEST):\n            self._env.set_mode(base_env.EnvMode.TEST)\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(mode)\n        return\n\n    def save(self, out_file):\n        state_dict = self.state_dict()\n        torch.save(state_dict, out_file)\n        return\n\n    def load(self, in_file):\n        state_dict = torch.load(in_file, map_location=self._device)\n        self.load_state_dict(state_dict)\n        return\n\n    def _load_params(self, config):\n        self._discount = config[\"discount\"]\n        self._steps_per_iter = config[\"steps_per_iter\"]\n        self._iters_per_output = config[\"iters_per_output\"]\n        self._test_episodes = config[\"test_episodes\"]\n        self._normalizer_samples = config.get(\"normalizer_samples\", np.inf)\n        return\n\n    @abc.abstractmethod\n    def _build_model(self, config):\n        return\n\n    def _build_normalizers(self):\n        obs_space = self._env.get_obs_space()\n        obs_dtype = torch_util.numpy_dtype_to_torch(obs_space.dtype)\n        self._obs_norm = normalizer.Normalizer(obs_space.shape, device=self._device, dtype=obs_dtype)\n        self._a_norm = self._build_action_normalizer()\n        return\n    \n    def _build_action_normalizer(self):\n        a_space = self._env.get_action_space()\n        a_dtype = torch_util.numpy_dtype_to_torch(a_space.dtype)\n\n        if (isinstance(a_space, gym.spaces.Box)):\n            a_mean = torch.tensor(0.5 * (a_space.high + a_space.low), device=self._device, dtype=a_dtype)\n            a_std = torch.tensor(0.5 * (a_space.high - a_space.low), device=self._device, dtype=a_dtype)\n            a_norm = normalizer.Normalizer(a_mean.shape, device=self._device, init_mean=a_mean, \n                                                 init_std=a_std, dtype=a_dtype)\n        elif (isinstance(a_space, gym.spaces.Discrete)):\n            a_mean = torch.tensor([0], device=self._device, dtype=a_dtype)\n            a_std = torch.tensor([1], device=self._device, dtype=a_dtype)\n            a_norm = normalizer.Normalizer(a_mean.shape, device=self._device, init_mean=a_mean, \n                                                 init_std=a_std, eps=0, dtype=a_dtype)\n        else:\n            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n        return a_norm\n\n    def _build_optimizer(self, config):\n        lr = float(config[\"learning_rate\"])\n        params = list(self.parameters())\n        params = [p for p in params if p.requires_grad]\n        self._optimizer = torch.optim.SGD(params, lr, momentum=0.9)\n        return\n    \n    def _build_exp_buffer(self, config):\n        buffer_length = self._get_exp_buffer_length()\n        self._exp_buffer = experience_buffer.ExperienceBuffer(buffer_length=buffer_length,\n                                                              device=self._device)\n        \n        obs_space = self._env.get_obs_space()\n        obs_dtype = torch_util.numpy_dtype_to_torch(obs_space.dtype)\n        obs_buffer = torch.zeros([buffer_length] + list(obs_space.shape), device=self._device, dtype=obs_dtype)\n        self._exp_buffer.add_buffer(\"obs\", obs_buffer)\n        \n        next_obs_buffer = torch.zeros_like(obs_buffer)\n        self._exp_buffer.add_buffer(\"next_obs\", next_obs_buffer)\n\n        a_space = self._env.get_action_space()\n        a_dtype = torch_util.numpy_dtype_to_torch(a_space.dtype)\n        action_buffer = torch.zeros([buffer_length] + list(a_space.shape), device=self._device, dtype=a_dtype)\n        self._exp_buffer.add_buffer(\"action\", action_buffer)\n        \n        reward_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n        self._exp_buffer.add_buffer(\"reward\", reward_buffer)\n        \n        done_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.int)\n        self._exp_buffer.add_buffer(\"done\", done_buffer)\n\n        return\n\n    def _build_return_tracker(self):\n        self._train_return_tracker = return_tracker.ReturnTracker(self._device)\n        return\n\n    @abc.abstractmethod\n    def _get_exp_buffer_length(self):\n        return 0\n    \n    def _build_logger(self, log_file):\n        log = tb_logger.TBLogger()\n        log.set_step_key(\"Samples\")\n        log.configure_output_file(log_file)\n        return log\n\n    def _update_sample_count(self):\n        return self._exp_buffer.get_total_samples()\n\n    def _init_train(self):\n        self._iter = 0\n        self._sample_count = 0\n        self._exp_buffer.clear()\n        self._train_return_tracker.reset()\n        return\n\n    def _train_iter(self):\n        self._init_iter()\n        \n        self.eval()\n        self.set_mode(AgentMode.TRAIN)\n        self._rollout_train(self._steps_per_iter)\n        \n        data_info = self._build_train_data()\n        train_info = self._update_model()\n        \n        if (self._need_normalizer_update()):\n            self._update_normalizers()\n        \n        info = {**train_info, **data_info}\n        \n        info[\"mean_return\"] = self._train_return_tracker.get_mean_return().item()\n        info[\"mean_ep_len\"] = self._train_return_tracker.get_mean_ep_len().item()\n        info[\"episodes\"] = self._train_return_tracker.get_episodes()\n        \n        return info\n\n    def _init_iter(self):\n        return\n\n    def _rollout_train(self, num_steps):\n        for i in range(num_steps):\n            action, action_info = self._decide_action(self._curr_obs, self._curr_info)\n            self._record_data_pre_step(self._curr_obs, self._curr_info, action, action_info)\n\n            next_obs, r, done, next_info = self._step_env(action)\n            self._train_return_tracker.update(r, done)\n            self._record_data_post_step(next_obs, r, done, next_info)\n            \n            self._curr_obs = next_obs\n\n            if done != base_env.DoneFlags.NULL.value:  \n                self._curr_obs, self._curr_info = self._env.reset()\n            self._exp_buffer.inc()\n\n        return\n    \n    def _rollout_test(self, num_episodes):\n        sum_rets = 0.0\n        sum_ep_lens = 0\n        \n        self._curr_obs, self._curr_info = self._env.reset()\n\n        for e in range(num_episodes):\n            curr_ret = 0.0\n            curr_ep_len = 0\n            while True:\n                action, action_info = self._decide_action(self._curr_obs, self._curr_info)\n                self._curr_obs, r, done, self._curr_info = self._step_env(action)\n\n                curr_ret += r.item()\n                curr_ep_len += 1\n\n                if done != base_env.DoneFlags.NULL.value:\n                    sum_rets += curr_ret\n                    sum_ep_lens += curr_ep_len\n                    self._curr_obs, self._curr_info = self._env.reset()\n                    break\n\n        \n        mean_return = sum_rets / max(1, num_episodes)\n        mean_ep_len = sum_ep_lens / max(1, num_episodes)\n\n        test_info = {\n            \"mean_return\": mean_return,\n            \"mean_ep_len\": mean_ep_len,\n            \"episodes\": num_episodes\n        }\n        return test_info\n\n    @abc.abstractmethod\n    def _decide_action(self, obs, info):\n        a = None\n        a_info = dict()\n        return a, a_info\n\n    def _step_env(self, action):\n        obs, r, done, info = self._env.step(action)\n        return obs, r, done, info\n\n    def _record_data_pre_step(self, obs, info, action, action_info):\n        self._exp_buffer.record(\"obs\", obs)\n        self._exp_buffer.record(\"action\", action)\n        \n        if (self._need_normalizer_update()):\n            self._obs_norm.record(obs.unsqueeze(0))\n        return\n\n    def _record_data_post_step(self, next_obs, r, done, next_info):\n        self._exp_buffer.record(\"next_obs\", next_obs)\n        self._exp_buffer.record(\"reward\", r)\n        self._exp_buffer.record(\"done\", done)\n        return\n\n    def _reset_done_envs(self, done):\n        done_indices = (done != base_env.DoneFlags.NULL.value).nonzero(as_tuple=False)\n        done_indices = torch.flatten(done_indices)\n        obs, info = self._env.reset(done_indices)\n        return obs, info\n\n    def _need_normalizer_update(self):\n        return self._sample_count < self._normalizer_samples\n\n    def _update_normalizers(self):\n        self._obs_norm.update()\n        return\n\n    def _build_train_data(self):\n        return dict()\n\n    @abc.abstractmethod\n    def _update_model(self):\n        return\n\n    def _compute_succ_val(self):\n        r_succ = self._env.get_reward_succ()\n        val_succ = r_succ / (1.0 - self._discount)\n        return val_succ\n    \n    def _compute_fail_val(self):\n        r_fail = self._env.get_reward_fail()\n        val_fail = r_fail / (1.0 - self._discount)\n        return val_fail\n\n    def _compute_val_bound(self):\n        r_min, r_max = self._env.get_reward_bounds()\n        val_min = r_min / (1.0 - self._discount)\n        val_max = r_max / (1.0 - self._discount)\n        return val_min, val_max\n\n    def _log_train_info(self, train_info, test_info, start_time):\n        wall_time = (time.time() - start_time) / (60 * 60) # store time in hours\n        self._logger.log(\"Iteration\", self._iter, collection=\"1_Info\")\n        self._logger.log(\"Wall_Time\", wall_time, collection=\"1_Info\")\n        self._logger.log(\"Samples\", self._sample_count, collection=\"1_Info\")\n        \n        test_return = test_info[\"mean_return\"]\n        test_ep_len = test_info[\"mean_ep_len\"]\n        test_eps = test_info[\"episodes\"]\n        self._logger.log(\"Test_Return\", test_return, collection=\"0_Main\")\n        self._logger.log(\"Test_Episode_Length\", test_ep_len, collection=\"0_Main\", quiet=True)\n        self._logger.log(\"Test_Episodes\", test_eps, collection=\"1_Info\", quiet=True)\n\n        train_return = train_info.pop(\"mean_return\")\n        train_ep_len = train_info.pop(\"mean_ep_len\")\n        train_eps = train_info.pop(\"episodes\")\n        self._logger.log(\"Train_Return\", train_return, collection=\"0_Main\")\n        self._logger.log(\"Train_Episode_Length\", train_ep_len, collection=\"0_Main\", quiet=True)\n        self._logger.log(\"Train_Episodes\", train_eps, collection=\"1_Info\", quiet=True)\n\n        for k, v in train_info.items():\n            val_name = k.title()\n            if torch.is_tensor(v):\n                v = v.item()\n            self._logger.log(val_name, v)\n            \n        return\n    \n    def _compute_action_bound_loss(self, norm_a_dist):\n        loss = None\n        action_space = self._env.get_action_space()\n        if (isinstance(action_space, gym.spaces.Box)):\n            a_low = action_space.low\n            a_high = action_space.high\n            valid_bounds = np.all(np.isfinite(a_low)) and np.all(np.isfinite(a_high))\n\n            if (valid_bounds):\n                assert(isinstance(norm_a_dist, distribution_gaussian_diag.DistributionGaussianDiag))\n                # assume that actions have been normalized between [-1, 1]\n                bound_min = -1\n                bound_max = 1\n                violation_min = torch.clamp_max(norm_a_dist.mode - bound_min, 0.0)\n                violation_max = torch.clamp_min(norm_a_dist.mode - bound_max, 0)\n                violation = torch.sum(torch.square(violation_min), dim=-1) \\\n                            + torch.sum(torch.square(violation_max), dim=-1)\n                loss = violation\n\n        return loss\n\n    def _output_train_model(self, iter, out_model_file, int_output_dir):\n        self.save(out_model_file)\n\n        if (int_output_dir != \"\"):\n            int_model_file = os.path.join(int_output_dir, \"model_{:010d}.pt\".format(iter))\n            self.save(int_model_file)\n        return", ""]}
{"filename": "learning/experience_buffer.py", "chunked_list": ["import torch\n\nclass ExperienceBuffer():\n    def __init__(self, buffer_length, device):\n        self._buffer_length = buffer_length\n        self._device = device\n\n        self._buffer_head = 0\n        self._total_samples = 0\n\n        self._buffers = dict()\n        self._sample_buf = torch.randperm(self._buffer_length, device=self._device,\n                                          dtype=torch.long)\n        self._sample_buf_head = 0\n        self._reset_sample_buf()\n\n        return\n\n    def add_buffer(self, name, buffer):\n        assert(len(buffer.shape) >= 1)\n        assert(buffer.shape[0] == self._buffer_length)\n        assert(name not in self._buffers)\n\n        self._buffers[name] = buffer\n\n        return\n\n    def reset(self):\n        self._buffer_head = 0\n        self._reset_sample_buf()\n        return\n\n    def clear(self):\n        self.reset()\n        self._total_samples = 0\n        return\n\n    def inc(self):\n        self._buffer_head = (self._buffer_head + 1) % self._buffer_length\n        self._total_samples += 1\n        return\n\n    def get_total_samples(self):\n        return self._total_samples\n\n    def get_sample_count(self):\n        sample_count = min(self._total_samples, self._buffer_length)\n        return sample_count\n\n    def record(self, name, data):\n        data_buf = self._buffers[name]\n        data_buf[self._buffer_head] = data\n        return\n\n    def set_data(self, name, data):\n        data_buf = self._buffers[name]\n        assert(data_buf.shape[0] == data.shape[0])\n        data_buf[:] = data\n        return\n    \n    def get_data(self, name):\n        return self._buffers[name]\n\n    def sample(self, n):\n        output = dict()\n        rand_idx = self._sample_rand_idx(n)\n\n        for key, data in self._buffers.items():\n            batch_data = data[rand_idx]\n            output[key] = batch_data\n\n        return output\n\n    def _reset_sample_buf(self):\n        self._sample_buf[:] = torch.randperm(self._buffer_length, device=self._device, dtype=torch.long)\n        self._sample_buf_head = 0\n        return\n\n    def _sample_rand_idx(self, n):\n        buffer_len = self._sample_buf.shape[0]\n        assert(n <= buffer_len)\n\n        if (self._sample_buf_head + n <= buffer_len):\n            rand_idx = self._sample_buf[self._sample_buf_head:self._sample_buf_head + n]\n            self._sample_buf_head += n\n        else:\n            rand_idx0 = self._sample_buf[self._sample_buf_head:]\n            remainder = n - (buffer_len - self._sample_buf_head)\n\n            self._reset_sample_buf()\n            rand_idx1 = self._sample_buf[:remainder]\n            rand_idx = torch.cat([rand_idx0, rand_idx1], dim=0)\n\n            self._sample_buf_head = remainder\n\n        sample_count = self.get_sample_count()\n        rand_idx = torch.remainder(rand_idx, sample_count)\n        return rand_idx"]}
{"filename": "learning/pg_model.py", "chunked_list": ["import torch\n\nimport learning.base_model as base_model\nimport learning.nets.net_builder as net_builder\nimport util.torch_util as torch_util\n\nclass PGModel(base_model.BaseModel):\n    def __init__(self, config, env):\n        super().__init__(config, env)\n\n        self._build_nets(config, env)\n        return\n\n    def eval_actor(self, obs):\n        h = self._actor_layers(obs)\n        a_dist = self._action_dist(h)\n        return a_dist\n    \n    def eval_critic(self, obs):\n        h = self._critic_layers(obs)\n        val = self._critic_out(h)\n        return val\n    \n    def _build_nets(self, config, env):\n        self._build_actor(config, env)\n        self._build_critic(config, env)\n        return\n\n    def _build_actor(self, config, env):\n        net_name = config[\"actor_net\"]\n        input_dict = self._build_actor_input_dict(env)\n        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                activation=self._activation)\n        \n        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n        return\n    \n    def _build_critic(self, config, env):\n        net_name = config[\"critic_net\"]\n        input_dict = self._build_critic_input_dict(env)\n        self._critic_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                 activation=self._activation)\n\n        layers_out_size = torch_util.calc_layers_out_size(self._critic_layers)\n        self._critic_out = torch.nn.Linear(layers_out_size, 1)\n        torch.nn.init.zeros_(self._critic_out.bias)\n        return\n\n    def _build_actor_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict\n    \n    def _build_critic_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict", ""]}
{"filename": "learning/normalizer.py", "chunked_list": ["import numpy as np\nimport torch\n\nclass Normalizer(torch.nn.Module):\n    def __init__(self, shape, device, init_mean=None, init_std=None, eps=1e-4, clip=np.inf, dtype=torch.float):\n        super().__init__()\n        \n        self._eps = eps\n        self._clip = clip\n        self.dtype = dtype\n        self._build_params(shape, device, init_mean, init_std)\n        return\n\n    def record(self, x):\n        shape = self.get_shape()\n        assert len(x.shape) > len(shape)\n\n        x = x.flatten(start_dim=0, end_dim=len(x.shape) - len(shape) - 1)\n\n        self._new_count += x.shape[0]\n        self._new_sum += torch.sum(x, axis=0)\n        self._new_sum_sq += torch.sum(torch.square(x), axis=0)\n        return\n\n    def update(self):\n        if (self._mean_sq is None):\n            self._mean_sq = self._calc_mean_sq(self._mean, self._std)\n\n        new_count = self._new_count\n        new_mean = self._new_sum / new_count\n        new_mean_sq = self._new_sum_sq / new_count\n        \n        new_total = self._count + new_count\n        w_old = self._count.type(torch.float) / new_total.type(torch.float)\n        w_new = float(new_count) / new_total.type(torch.float)\n\n        self._mean[:] = w_old * self._mean + w_new * new_mean\n        self._mean_sq[:] = w_old * self._mean_sq + w_new * new_mean_sq\n        self._count[:] = new_total\n\n        self._std[:] = self._calc_std(self._mean, self._mean_sq)\n\n        self._new_count = 0\n        self._new_sum[:] = 0\n        self._new_sum_sq[:] = 0\n        return\n\n    def get_shape(self):\n        return self._mean.shape\n\n    def get_count(self):\n        return self._count\n\n    def get_mean(self):\n        return self._mean\n\n    def get_std(self):\n        return self._std\n\n    def set_mean_std(self, mean, std):\n        shape = self.get_shape()\n        is_array = isinstance(mean, np.ndarray) and isinstance(std, np.ndarray)\n        \n        assert mean.shape == shape and std.shape == shape, \\\n            print(\"Normalizer shape mismatch, expecting size {:d}, but got {:d} and {:d}\".format(shape, mean.shape, std.shape))\n        \n        self._mean[:] = mean\n        self._std[:] = std\n        self._mean_sq[:] = self._calc_mean_sq(self._mean, self._std)\n        return\n\n    def normalize(self, x):\n        norm_x = (x - self._mean) / (self._std + self._eps)\n        norm_x = torch.clamp(norm_x, -self._clip, self._clip)\n        return norm_x.type(self.dtype)\n\n    def unnormalize(self, norm_x):\n        x = norm_x * self._std + self._mean\n        return x.type(self.dtype)\n    \n    def _calc_std(self, mean, mean_sq):\n        var = mean_sq - torch.square(mean)\n        # some time floating point errors can lead to small negative numbers\n        var = torch.clamp_min(var, 0.0)\n        std = torch.sqrt(var)\n        std = std.type(self.dtype)\n        return std\n\n    def _calc_mean_sq(self, mean, std):\n        mean_sq = torch.square(std) + torch.square(self._mean)\n        mean_sq = mean_sq.type(self.dtype)\n        return mean_sq\n\n    def _build_params(self, shape, device, init_mean, init_std):\n        self._count = torch.nn.Parameter(torch.zeros([1], device=device, requires_grad=False, dtype=torch.long), requires_grad=False)\n        self._mean = torch.nn.Parameter(torch.zeros(shape, device=device, requires_grad=False, dtype=self.dtype), requires_grad=False)\n        self._std = torch.nn.Parameter(torch.ones(shape, device=device, requires_grad=False, dtype=self.dtype), requires_grad=False)\n\n        if init_mean is not None:\n            assert init_mean.shape == shape, \\\n            print('Normalizer init mean shape mismatch, expecting {:d}, but got {:d}'.shape(size, init_mean.shape))\n            self._mean[:] = init_mean\n\n        if init_std is not None:\n            assert init_std.shape == shape, \\\n            print('Normalizer init std shape mismatch, expecting {:d}, but got {:d}'.format(shape, init_std.shape))\n            self._std[:] = init_std\n\n        self._mean_sq = None\n\n        self._new_count = 0\n        self._new_sum = torch.zeros_like(self._mean)\n        self._new_sum_sq = torch.zeros_like(self._mean)\n        return"]}
{"filename": "learning/agent_builder.py", "chunked_list": ["import yaml\n\nimport learning.expert_agent as expert_agent\n\nimport a1.bc_agent as bc_agent\nimport a2.cem_agent as cem_agent\nimport a2.pg_agent as pg_agent\nimport a3.dqn_agent as dqn_agent\n\ndef build_agent(agent_file, env, device):\n    agent_config = load_agent_file(agent_file)\n    \n    agent_name = agent_config[\"agent_name\"]\n    print(\"Building {} agent\".format(agent_name))\n\n    if (agent_name == bc_agent.BCAgent.NAME):\n        agent = bc_agent.BCAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == expert_agent.ExpertAgent.NAME):\n        agent = expert_agent.ExpertAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == cem_agent.CEMAgent.NAME):\n        agent = cem_agent.CEMAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == pg_agent.PGAgent.NAME):\n        agent = pg_agent.PGAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == dqn_agent.DQNAgent.NAME):\n        agent = dqn_agent.DQNAgent(config=agent_config, env=env, device=device) \n    else:\n        assert(False), \"Unsupported agent: {}\".format(agent_name)\n\n    return agent", "\ndef build_agent(agent_file, env, device):\n    agent_config = load_agent_file(agent_file)\n    \n    agent_name = agent_config[\"agent_name\"]\n    print(\"Building {} agent\".format(agent_name))\n\n    if (agent_name == bc_agent.BCAgent.NAME):\n        agent = bc_agent.BCAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == expert_agent.ExpertAgent.NAME):\n        agent = expert_agent.ExpertAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == cem_agent.CEMAgent.NAME):\n        agent = cem_agent.CEMAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == pg_agent.PGAgent.NAME):\n        agent = pg_agent.PGAgent(config=agent_config, env=env, device=device)\n    elif (agent_name == dqn_agent.DQNAgent.NAME):\n        agent = dqn_agent.DQNAgent(config=agent_config, env=env, device=device) \n    else:\n        assert(False), \"Unsupported agent: {}\".format(agent_name)\n\n    return agent", "\ndef load_agent_file(file):\n    with open(file, \"r\") as stream:\n        agent_config = yaml.safe_load(stream)\n    return agent_config\n"]}
{"filename": "learning/return_tracker.py", "chunked_list": ["import torch\n\nimport envs.base_env as base_env\n\nclass ReturnTracker():\n    def __init__(self, device):\n        num_envs = 1\n        self._device = device\n        self._episodes = 0\n        self._mean_return = torch.zeros([1], device=device, dtype=torch.float32)\n        self._mean_ep_len = torch.zeros([1], device=device, dtype=torch.float32)\n\n        self._return_buf = torch.zeros([num_envs], device=device, dtype=torch.float32)\n        self._ep_len_buf = torch.zeros([num_envs], device=device, dtype=torch.long)\n        self._eps_per_env_buf = torch.zeros([num_envs], device=device, dtype=torch.long)\n        return\n\n    def get_mean_return(self):\n        return self._mean_return\n\n    def get_mean_ep_len(self):\n        return self._mean_ep_len\n\n    def get_episodes(self):\n        return self._episodes\n\n    def get_eps_per_env(self):\n        return self._eps_per_env_buf\n\n    def reset(self):\n        self._episodes = 0\n        self._eps_per_env_buf[:] = 0\n\n        self._mean_return[:] = 0.0\n        self._mean_ep_len[:] = 0.0\n\n        self._return_buf[:] = 0.0\n        self._ep_len_buf[:] = 0\n        return\n\n    def update(self, reward, done):\n        assert(reward.shape == self._return_buf.shape)\n        assert(done.shape == self._return_buf.shape)\n\n        self._return_buf += reward\n        self._ep_len_buf += 1\n\n        reset_mask = done != base_env.DoneFlags.NULL.value\n        reset_ids = reset_mask.nonzero(as_tuple=False).flatten()\n        num_resets = len(reset_ids)\n\n        if (num_resets > 0):\n            new_mean_return = torch.mean(self._return_buf[reset_ids])\n            new_mean_ep_len = torch.mean(self._ep_len_buf[reset_ids].type(torch.float))\n\n            new_count = self._episodes + num_resets\n            w_new = float(num_resets) / new_count\n            w_old = float(self._episodes) / new_count\n\n            self._mean_return = w_new * new_mean_return + w_old * self._mean_return\n            self._mean_ep_len = w_new * new_mean_ep_len + w_old * self._mean_ep_len\n            self._episodes += num_resets\n\n            self._return_buf[reset_ids] = 0.0\n            self._ep_len_buf[reset_ids] = 0\n            self._eps_per_env_buf[reset_ids] += 1\n\n        return"]}
{"filename": "learning/expert_model.py", "chunked_list": ["import torch\n\nimport learning.base_model as base_model\nimport learning.nets.net_builder as net_builder\nimport util.torch_util as torch_util\n\nclass ExpertModel(base_model.BaseModel):\n    def __init__(self, config, env):\n        super().__init__(config, env)\n\n        self._build_nets(config, env)\n        return\n\n    def eval_actor(self, obs):\n        h = self._actor_layers(obs)\n        a_dist = self._action_dist(h)\n        return a_dist\n    \n    def eval_critic(self, obs):\n        h = self._critic_layers(obs)\n        val = self._critic_out(h)\n        return val\n    \n    def _build_nets(self, config, env):\n        self._build_actor(config, env)\n        self._build_critic(config, env)\n        return\n\n    def _build_actor(self, config, env):\n        net_name = config[\"actor_net\"]\n        input_dict = self._build_actor_input_dict(env)\n        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                activation=self._activation)\n        \n        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n        return\n    \n    def _build_critic(self, config, env):\n        net_name = config[\"critic_net\"]\n        input_dict = self._build_critic_input_dict(env)\n        self._critic_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                 activation=self._activation)\n\n        layers_out_size = torch_util.calc_layers_out_size(self._critic_layers)\n        self._critic_out = torch.nn.Linear(layers_out_size, 1)\n        torch.nn.init.zeros_(self._critic_out.bias)\n        return\n\n    def _build_actor_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict\n    \n    def _build_critic_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict", ""]}
{"filename": "learning/cem_model.py", "chunked_list": ["import torch\n\nimport learning.base_model as base_model\nimport learning.nets.net_builder as net_builder\n\nclass CEMModel(base_model.BaseModel):\n    def __init__(self, config, env):\n        super().__init__(config, env)\n\n        self._build_nets(config, env)\n        return\n\n    def eval_actor(self, obs):\n        h = self._actor_layers(obs)\n        a_dist = self._action_dist(h)\n        return a_dist\n    \n    def _build_nets(self, config, env):\n        self._build_actor(config, env)\n        return\n\n    def _build_actor(self, config, env):\n        net_name = config[\"actor_net\"]\n        input_dict = self._build_actor_input_dict(env)\n        self._actor_layers, layers_info = net_builder.build_net(net_name, input_dict,\n                                                                activation=self._activation)\n        \n        self._action_dist = self._build_action_distribution(config, env, self._actor_layers)\n        return\n\n    def _build_actor_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict"]}
{"filename": "learning/distribution_gaussian_diag.py", "chunked_list": ["import enum\nimport numpy as np\nimport torch\n\nclass StdType(enum.Enum):\n    FIXED = 0\n    CONSTANT = 1\n    VARIABLE = 2\n\nclass DistributionGaussianDiagBuilder(torch.nn.Module):\n    def __init__(self, in_size, out_size, std_type, init_std, init_output_scale=0.01):\n        super().__init__()\n        self._std_type = std_type\n\n        self._build_params(in_size, out_size, init_std, init_output_scale)\n        return\n\n    def _build_params(self, in_size, out_size, init_std, init_output_scale):\n        self._mean_net = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.uniform_(self._mean_net.weight, -init_output_scale, init_output_scale)\n        torch.nn.init.zeros_(self._mean_net.bias)\n\n        logstd = np.log(init_std)\n        if (self._std_type == StdType.FIXED):\n            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=False, dtype=torch.float32), requires_grad=False)\n            torch.nn.init.constant_(self._logstd_net, logstd)\n        elif (self._std_type == StdType.CONSTANT):\n            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=True, dtype=torch.float32), requires_grad=True)\n            torch.nn.init.constant_(self._logstd_net, logstd)\n        elif (self._std_type == StdType.VARIABLE):\n            self._logstd_net = torch.nn.Linear(in_size, out_size)\n            torch.nn.init.uniform_(self._logstd_net.weight, -init_output_scale, init_output_scale) \n            torch.nn.init.constant_(self._logstd_net.bias, logstd)\n        else:\n            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n\n        return\n\n    def forward(self, input):\n        mean = self._mean_net(input)\n\n        if (self._std_type == StdType.FIXED or self._std_type == StdType.CONSTANT):\n            logstd = torch.broadcast_to(self._logstd_net, mean.shape)\n        elif (self._std_type == StdType.VARIABLE):\n            logstd = self._logstd_net(input)\n        else:\n            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n\n        dist = DistributionGaussianDiag(mean=mean, logstd=logstd)\n        return dist", "\nclass DistributionGaussianDiagBuilder(torch.nn.Module):\n    def __init__(self, in_size, out_size, std_type, init_std, init_output_scale=0.01):\n        super().__init__()\n        self._std_type = std_type\n\n        self._build_params(in_size, out_size, init_std, init_output_scale)\n        return\n\n    def _build_params(self, in_size, out_size, init_std, init_output_scale):\n        self._mean_net = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.uniform_(self._mean_net.weight, -init_output_scale, init_output_scale)\n        torch.nn.init.zeros_(self._mean_net.bias)\n\n        logstd = np.log(init_std)\n        if (self._std_type == StdType.FIXED):\n            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=False, dtype=torch.float32), requires_grad=False)\n            torch.nn.init.constant_(self._logstd_net, logstd)\n        elif (self._std_type == StdType.CONSTANT):\n            self._logstd_net = torch.nn.Parameter(torch.zeros(out_size, requires_grad=True, dtype=torch.float32), requires_grad=True)\n            torch.nn.init.constant_(self._logstd_net, logstd)\n        elif (self._std_type == StdType.VARIABLE):\n            self._logstd_net = torch.nn.Linear(in_size, out_size)\n            torch.nn.init.uniform_(self._logstd_net.weight, -init_output_scale, init_output_scale) \n            torch.nn.init.constant_(self._logstd_net.bias, logstd)\n        else:\n            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n\n        return\n\n    def forward(self, input):\n        mean = self._mean_net(input)\n\n        if (self._std_type == StdType.FIXED or self._std_type == StdType.CONSTANT):\n            logstd = torch.broadcast_to(self._logstd_net, mean.shape)\n        elif (self._std_type == StdType.VARIABLE):\n            logstd = self._logstd_net(input)\n        else:\n            assert(False), \"Unsupported StdType: {}\".format(self._std_type)\n\n        dist = DistributionGaussianDiag(mean=mean, logstd=logstd)\n        return dist", "\nclass DistributionGaussianDiag():\n    def __init__(self, mean, logstd):\n        self._mean = mean\n        self._logstd = logstd\n        self._std = torch.exp(self._logstd)\n        self._dim = self._mean.shape[-1]\n        return\n\n    @property\n    def stddev(self):\n        return self._std\n        \n    @property\n    def logstd(self):\n        return self._logstd\n        \n    @property\n    def mean(self):\n        return self._mean\n        \n    @property\n    def mode(self):\n        return self._mean\n\n    def sample(self):\n        noise = torch.normal(torch.zeros_like(self._mean), torch.ones_like(self._std))\n        x = self._mean + self._std * noise\n        return x\n\n    def log_prob(self, x):\n        diff = x - self._mean\n        logp = -0.5 * torch.sum(torch.square(diff / self._std), dim=-1)\n        logp += -0.5 * self._dim * np.log(2.0 * np.pi) - torch.sum(self._logstd, dim=-1)\n        return logp\n\n    def entropy(self):\n        ent = torch.sum(self._logstd, dim=-1)\n        ent += 0.5 * self._dim * np.log(2.0 * np.pi * np.e)\n        return ent\n        \n    def kl(self, other):\n        assert(isinstance(other, GaussianDiagDist))\n        other_var = torch.square(other.stddev)\n        res = torch.sum(other.logstd - self._logstd + (torch.square(self._std) + torch.square(self._mean - other.mean)) / (2.0 * other_var), dim=-1)\n        res += -0.5 * self._dim\n        return res\n\n    def param_reg(self):\n        # only regularize mean, covariance is regularized via entropy reg\n        reg = torch.sum(torch.square(self._mean), dim=-1)\n        return reg"]}
{"filename": "learning/base_model.py", "chunked_list": ["import gym\nimport numpy as np\nimport torch\n\nimport learning.distribution_gaussian_diag as distribution_gaussian_diag\nimport learning.distribution_categorical as distribution_categorical\nimport util.torch_util as torch_util\n\nclass BaseModel(torch.nn.Module):\n    def __init__(self, config, env):\n        super().__init__()\n        self._activation = torch.nn.ReLU\n        return\n\n    def _build_action_distribution(self, config, env, input):\n        a_space = env.get_action_space()\n        in_size = torch_util.calc_layers_out_size(input)\n\n        if (isinstance(a_space, gym.spaces.Box)):\n            a_size = np.prod(a_space.shape)\n            a_init_output_scale = config[\"actor_init_output_scale\"]\n            a_std_type = distribution_gaussian_diag.StdType[config[\"actor_std_type\"]]\n            a_std = config[\"action_std\"]\n            a_dist = distribution_gaussian_diag.DistributionGaussianDiagBuilder(in_size, a_size, std_type=a_std_type,\n                                                                            init_std=a_std, init_output_scale=a_init_output_scale)\n        elif (isinstance(a_space, gym.spaces.Discrete)):\n            num_actions = a_space.n\n            a_init_output_scale = config[\"actor_init_output_scale\"]\n            a_dist = distribution_categorical.DistributionCategoricalBuilder(in_size, num_actions, \n                                                                             init_output_scale=a_init_output_scale)\n        else:\n            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n\n        return a_dist", "class BaseModel(torch.nn.Module):\n    def __init__(self, config, env):\n        super().__init__()\n        self._activation = torch.nn.ReLU\n        return\n\n    def _build_action_distribution(self, config, env, input):\n        a_space = env.get_action_space()\n        in_size = torch_util.calc_layers_out_size(input)\n\n        if (isinstance(a_space, gym.spaces.Box)):\n            a_size = np.prod(a_space.shape)\n            a_init_output_scale = config[\"actor_init_output_scale\"]\n            a_std_type = distribution_gaussian_diag.StdType[config[\"actor_std_type\"]]\n            a_std = config[\"action_std\"]\n            a_dist = distribution_gaussian_diag.DistributionGaussianDiagBuilder(in_size, a_size, std_type=a_std_type,\n                                                                            init_std=a_std, init_output_scale=a_init_output_scale)\n        elif (isinstance(a_space, gym.spaces.Discrete)):\n            num_actions = a_space.n\n            a_init_output_scale = config[\"actor_init_output_scale\"]\n            a_dist = distribution_categorical.DistributionCategoricalBuilder(in_size, num_actions, \n                                                                             init_output_scale=a_init_output_scale)\n        else:\n            assert(False), \"Unsuppoted action space: {}\".format(a_space)\n\n        return a_dist"]}
{"filename": "learning/dqn_model.py", "chunked_list": ["import gym\nimport torch\n\nimport learning.base_model as base_model\nimport learning.nets.net_builder as net_builder\nimport util.torch_util as torch_util\n\nclass DQNModel(base_model.BaseModel):\n    def __init__(self, config, env):\n        super().__init__(config, env)\n        self._build_nets(config, env)\n        return\n\n    def eval_q(self, obs):\n        h = self._q_layers(obs)\n        q = self._q_out(h)\n        return q\n    \n    def _build_nets(self, config, env):\n        net_name = config[\"q_net\"]\n        q_init_output_scale = config[\"q_init_output_scale\"]\n        input_dict = self._build_q_input_dict(env)\n        \n        a_space = env.get_action_space()\n        assert(isinstance(a_space, gym.spaces.Discrete))\n        num_actions = a_space.n\n\n        self._q_layers, _ = net_builder.build_net(net_name, input_dict,\n                                                  activation=self._activation)\n\n        layers_out_size = torch_util.calc_layers_out_size(self._q_layers)\n        \n        self._q_out = torch.nn.Linear(layers_out_size, num_actions)\n        torch.nn.init.uniform_(self._q_out.weight, -q_init_output_scale, q_init_output_scale)\n        torch.nn.init.zeros_(self._q_out.bias)\n        \n        return\n    \n    def _build_q_input_dict(self, env):\n        obs_space = env.get_obs_space()\n        input_dict = {\"obs\": obs_space}\n        return input_dict"]}
{"filename": "learning/distribution_categorical.py", "chunked_list": ["import enum\nimport numpy as np\nimport torch\n\nclass DistributionCategoricalBuilder(torch.nn.Module):\n    def __init__(self, in_size, out_size, init_output_scale=0.01):\n        super().__init__()\n        self._build_params(in_size, out_size, init_output_scale)\n        return\n\n    def _build_params(self, in_size, out_size, init_output_scale):\n        self._logit_net = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.uniform_(self._logit_net.weight, -init_output_scale, init_output_scale)\n        return\n\n    def forward(self, input):\n        logits = self._logit_net(input)\n        dist = DistributionCategorical(logits=logits)\n        return dist", "\n\nclass DistributionCategorical(torch.distributions.Categorical):\n    def __init__(self, logits):\n        logits = logits.unsqueeze(-2)\n        super().__init__(logits=logits)\n        return\n    \n    @property\n    def mode(self):\n        arg_max = torch.argmax(self.logits, dim=-1)\n        return arg_max\n\n    def sample(self):\n        x = super().sample()\n        return x\n\n    def log_prob(self, x):\n        logp = super().log_prob(x)\n        logp = logp.squeeze(-1)\n        return logp\n    \n    def entropy(self):\n        ent = super().entropy()\n        ent = ent.squeeze(-1)\n        return ent\n\n    def param_reg(self):\n        # only regularize mean, covariance is regularized via entropy reg\n        reg = torch.sum(torch.square(self.logits), dim=-1)\n        reg = reg.squeeze(-1)\n        return reg", ""]}
{"filename": "learning/nets/fc_2layers_1024units.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef build_net(input_dict, activation):\n    layer_sizes = [1024, 512]\n    \n    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n    \n    in_size = input_dim\n    layers = []\n    for out_size in layer_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "learning/nets/cnn_3conv_1fc_0.py", "chunked_list": ["import numpy as np\nimport torch\n\nimport util.torch_util as torch_util\n\ndef build_net(input_dict, activation):\n    conv_kernel_size = [8, 4, 3]\n    conv_channels= [32, 64, 64]\n    conv_stride = [4, 2, 1]\n\n    fc_sizes = [512]\n\n    assert(len(input_dict) == 1)\n    obs_space = input_dict[\"obs\"]\n    in_channels = obs_space.shape[-3]\n    in_size = np.array(obs_space.shape[-2:], dtype=np.float32)\n\n    layers = []\n    \n    if (obs_space.dtype == np.uint8):\n        to_float_layer = torch_util.UInt8ToFloat()\n        layers.append(to_float_layer)\n\n    for i in range(len(conv_kernel_size)):\n        kernel_size = conv_kernel_size[i]\n        channels = conv_channels[i]\n        stride = conv_stride[i]\n\n        curr_layer = torch.nn.Conv2d(in_channels=in_channels, \n                                     out_channels=channels, \n                                     kernel_size=kernel_size,\n                                     stride=stride)\n        torch.nn.init.zeros_(curr_layer.bias)\n        \n        layers.append(curr_layer)\n        layers.append(activation())\n\n        in_channels = channels\n        in_size = np.ceil((in_size - kernel_size + 1) / stride)\n\n    layers.append(torch.nn.Flatten(start_dim=-3, end_dim=-1))\n\n    in_size = int(np.prod(in_size) * channels)\n    for out_size in fc_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "learning/nets/fc_2layers_128units.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef build_net(input_dict, activation):\n    layer_sizes = [128, 64]\n    \n    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n    \n    in_size = input_dim\n    layers = []\n    for out_size in layer_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "learning/nets/__init__.py", "chunked_list": ["from os.path import dirname, basename, isfile, join\nimport glob\nmodules = glob.glob(join(dirname(__file__), \"*.py\"))\n__all__ = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(\"__init__.py\") and not f.endswith(\"net_builder.py\")]\n"]}
{"filename": "learning/nets/net_builder.py", "chunked_list": ["import torch\n\nfrom learning.nets import *\n\ndef build_net(net_name, input_dict, activation=torch.nn.ReLU):\n    if (net_name in globals()):\n        net_func = globals()[net_name]\n        net, info = net_func.build_net(input_dict, activation)\n    else:\n        assert(False), \"Unsupported net: {}\".format(net_name)\n    return net, info", ""]}
{"filename": "learning/nets/fc_1layers_16units.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef build_net(input_dict, activation):\n    layer_sizes = [16]\n\n    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n    \n    in_size = input_dim\n    layers = []\n    for out_size in layer_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "learning/nets/fc_1layers_32units.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef build_net(input_dict, activation):\n    layer_sizes = [32]\n    \n    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n    \n    in_size = input_dim\n    layers = []\n    for out_size in layer_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "learning/nets/fc_2layers_64units.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef build_net(input_dict, activation):\n    layer_sizes = [64, 32]\n    \n    input_dim = np.sum([np.prod(curr_input.shape) for curr_input in input_dict.values()])\n    \n    in_size = input_dim\n    layers = []\n    for out_size in layer_sizes:\n        curr_layer = torch.nn.Linear(in_size, out_size)\n        torch.nn.init.zeros_(curr_layer.bias)\n\n        layers.append(curr_layer)\n        layers.append(activation())\n        in_size = out_size\n\n    net = torch.nn.Sequential(*layers)\n    info = dict()\n\n    return net, info", ""]}
{"filename": "envs/atari_env.py", "chunked_list": ["import envs.base_env as base_env\nimport envs.atari_wrappers as atari_wrappers\n\nimport gym\nimport numpy as np\nimport torch\n\nclass AtariEnv(base_env.BaseEnv):\n    def __init__(self, config, device, visualize):\n        super().__init__(visualize)\n    \n        self._device = device\n        self._curr_obs = None\n        self._curr_info = None\n        \n        self._env = self._build_atari_env(config, visualize)\n\n        self.reset()\n\n        return\n\n    def reset(self):\n        obs, info = self._env.reset()\n        self._curr_obs = self._convert_obs(obs)\n        self._curr_info = info\n        return self._curr_obs, self._curr_info\n\n    def step(self, action):\n        a = action.cpu().numpy()[0]\n        obs, reward, terminated, truncated, info = self._env.step(a)\n        self._curr_obs = self._convert_obs(obs)\n        self._curr_info = info\n\n        # train with clipped reward and then test with unclipped rewards\n        if (self._mode is base_env.EnvMode.TRAIN):\n            reward = reward[1] # clipped reward\n        else:\n            reward = reward[0] # unclipped reward\n\n        reward = torch.tensor([reward], device=self._device)\n        done = self._compute_done(terminated, truncated) \n        \n        return self._curr_obs, reward, done, self._curr_info \n\n    def _build_atari_env(self, config, visualize):\n        env_name = config[\"env_name\"]\n        max_timesteps = config.get(\"max_timesteps\", None)\n        assert(env_name.startswith(\"atari_\"))\n\n        env_name = env_name[6:]\n        env_name = env_name + \"NoFrameskip-v4\"\n\n        atari_env = atari_wrappers.make_atari(env_name, \n                                              max_episode_steps=max_timesteps,\n                                              visualize=visualize)\n        atari_env = atari_wrappers.wrap_deepmind(atari_env,\n                                                 warp_frame=True,\n                                                 frame_stack=True)\n        \n        return atari_env\n \n    def _convert_obs(self, obs):\n        num_frames = obs.count()\n        frames = [np.expand_dims(obs.frame(i), axis=-3) for i in range(num_frames)]\n        frames = np.concatenate(frames, axis=-3)\n        \n        obs = torch.tensor(frames, device=self._device)\n        obs = obs.unsqueeze(0)\n\n        return  obs\n\n    def _compute_done(self, terminated, truncated):\n        if (terminated): \n            done = torch.tensor([base_env.DoneFlags.FAIL.value], device=self._device)\n        elif (truncated): \n            done = torch.tensor([base_env.DoneFlags.TIME.value], device=self._device)\n        else:\n            done = torch.tensor([base_env.DoneFlags.NULL.value], device=self._device)\n        return done\n\n    def get_obs_space(self):\n        obs_space = self._env.observation_space\n        obs_shape = list(obs_space.shape)\n        obs_shape = obs_shape[-1:] + obs_shape[:-1]\n        obs_space = gym.spaces.Box(\n            low=obs_space.low.min(),\n            high=obs_space.high.max(),\n            shape=obs_shape,\n            dtype=obs_space.dtype,\n        )\n        return obs_space\n\n    def get_action_space(self):\n        a_space = self._env.action_space\n        a_space._shape = (1,)\n        return a_space\n  \n    def get_reward_bounds(self):\n        return (-1.0, 1.0)"]}
{"filename": "envs/env_builder.py", "chunked_list": ["import yaml\n\nimport envs.atari_env as atari_env\nimport envs.env_dm as env_dm\n\ndef build_env(env_file, device, visualize):\n    env_config = load_env_file(env_file)\n\n    env_name = env_config[\"env_name\"]\n    print(\"Building {} env\".format(env_name))\n    \n    if (env_name.startswith(\"dm_\")):\n        env = env_dm.DMEnv(config=env_config, device=device, visualize=visualize)\n    elif (env_name.startswith(\"atari_\")):\n        env = atari_env.AtariEnv(config=env_config, device=device, visualize=visualize)\n    else:\n        assert(False), \"Unsupported env: {}\".format(env_name)\n\n    return env", "\ndef load_env_file(file):\n    with open(file, \"r\") as stream:\n        env_config = yaml.safe_load(stream)\n    return env_config\n"]}
{"filename": "envs/env_dm.py", "chunked_list": ["import envs.base_env as base_env\n\nfrom dm_control import suite\nimport dm_env\nimport gym\nimport multiprocessing\nimport numpy as np\nimport pygame\nimport torch\n", "import torch\n\nSCREEN_WIDTH = 640\nSCREEN_HEIGHT = 360\n\nclass DMEnv(base_env.BaseEnv):\n    def __init__(self, config, device, visualize):\n        super().__init__(visualize)\n    \n        self._device = device\n        self._visualize = visualize\n        self._time_limit = config['time_limit']\n        self._mode = None\n        \n        self._env = self._build_dm_env(config)\n\n        if (self._visualize):\n            self._init_render()\n\n        self.reset()\n\n        return\n\n\n    def reset(self):\n        time_step = self._env.reset()\n        obs = self._convert_obs(time_step.observation)\n        return obs, {}\n\n\n    def step(self, action):\n        time_step = self._env.step(action.cpu().numpy())\n    \n        obs = self._convert_obs(time_step.observation)\n        reward = 0 if time_step.reward == None else time_step.reward\n        reward = torch.tensor([reward], device=self._device)\n        done = self._compute_done(time_step.last()) \n        info = dict()\n        \n        if self._visualize:\n            self._render()\n\n        return obs, reward, done, info \n\n \n    def _build_dm_env(self, config):\n        env_name = config['env_name']\n        task_name = config['task']\n        assert(env_name.startswith(\"dm_\"))\n        env_name = env_name[3:]\n        dm_env = suite.load(env_name, task_name)\n        return dm_env\n    \n    def _convert_obs(self, obs):\n        obs_list = []\n        for v in obs.values():\n            if not isinstance(v, np.ndarray): \n                obs_list.append(np.array([v]))\n            else:  \n                obs_list.append(v)\n\n        flat_obs = np.concatenate(obs_list, axis=-1)\n        flat_obs = flat_obs.astype(np.float32)\n        flat_obs = torch.from_numpy(flat_obs)\n        flat_obs = flat_obs.to(device=self._device)\n\n        return  flat_obs\n\n    def _compute_done(self, last):\n        if self._env._physics.time() >= self._time_limit: \n            done = torch.tensor([base_env.DoneFlags.TIME.value], device=self._device)\n        elif last: \n            done = torch.tensor([base_env.DoneFlags.FAIL.value], device=self._device)\n        else:\n            done = torch.tensor([base_env.DoneFlags.NULL.value], device=self._device)\n        return done\n\n    def get_action_space(self):\n        action_spec = self._env.action_spec()\n        if (isinstance(action_spec, dm_env.specs.BoundedArray)):\n            low = action_spec.minimum\n            high = action_spec.maximum\n            action_space = gym.spaces.Box(low=low, high=high, shape=low.shape)\n        else:\n            assert(False), \"Unsupported DM action type\"\n\n        return action_space\n  \n    def get_reward_bounds(self):\n        return (0.0, 1.0)\n\n    def _init_render(self):\n        self._clock = pygame.time.Clock()\n\n        self._render_queue = multiprocessing.Queue()\n        self._render_proc = multiprocessing.Process(target=render_worker, args=(SCREEN_WIDTH, SCREEN_HEIGHT, self._render_queue))\n        self._render_proc.start()\n        return\n\n    def _render(self):\n        im = self._env.physics.render(SCREEN_HEIGHT, SCREEN_WIDTH, camera_id=0)\n        im = np.transpose(im, axes=[1, 0, 2])\n        self._render_queue.put(im)\n\n        fps = 1.0 / self._env.control_timestep()\n        self._clock.tick(fps)\n        return", "\n   \n\n\ndef render_worker(screen_w, screen_h, render_queue):\n    pygame.init()     \n    window = pygame.display.set_mode((screen_w, screen_h))\n    pygame.display.set_caption(\"DeepMind Control Suite\")\n\n    done = False\n    while not done:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                done = True\n\n        im = render_queue.get()\n        surface = pygame.pixelcopy.make_surface(im)\n        window.blit(surface, (0, 0))\n        pygame.display.update()\n\n    pygame.quit()\n    return", ""]}
{"filename": "envs/base_env.py", "chunked_list": ["import abc\nimport enum\nimport gym\nimport numpy as np\n\nimport util.torch_util as torch_util\n\nclass EnvMode(enum.Enum):\n    TRAIN = 0\n    TEST = 1", "\nclass DoneFlags(enum.Enum):\n    NULL = 0\n    FAIL = 1\n    SUCC = 2\n    TIME = 3\n\nclass BaseEnv(abc.ABC):\n    NAME = \"base\"\n\n    def __init__(self, visualize):\n        self._mode = EnvMode.TRAIN\n        self._visualize = visualize\n        self._action_space = None\n        return\n    \n    @abc.abstractmethod\n    def reset(self, env_ids=None):\n        return\n    \n    @abc.abstractmethod\n    def step(self, action):\n        return\n    \n    def get_obs_space(self):\n        obs, _ = self.reset()\n        obs_shape = list(obs.shape)\n        \n        obs_dtype = torch_util.torch_dtype_to_numpy(obs.dtype)\n        obs_space = gym.spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=obs_shape,\n            dtype=obs_dtype,\n        )\n        return obs_space\n    \n    def get_action_space(self):\n        return self._action_space\n\n    def set_mode(self, mode):\n        self._mode = mode\n        return\n\n    def get_num_envs(self):\n        return int(1)\n\n    def get_reward_bounds(self):\n        return (-np.inf, np.inf)\n\n    def get_reward_fail(self):\n        return 0.0\n\n    def get_reward_succ(self):\n        return 0.0\n\n    def get_visualize(self):\n        return self._visualize", ""]}
{"filename": "envs/atari_wrappers.py", "chunked_list": ["import numpy as np\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\ncv2.ocl.setUseOpenCL(False)\nfrom gym.wrappers import TimeLimit\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\n    def reset(self, **kwargs):\n        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        info = None\n        for _ in range(noops):\n            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n            done = terminated or truncated\n            if done:\n                obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, ac):\n        return self.env.step(ac)", "class NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\n    def reset(self, **kwargs):\n        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        info = None\n        for _ in range(noops):\n            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n            done = terminated or truncated\n            if done:\n                obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, ac):\n        return self.env.step(ac)", "\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, terminated, truncated, info = self.env.step(1)\n        done = terminated or truncated\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, terminated, truncated, info = self.env.step(1)\n        done = terminated or truncated\n        if done:\n            self.env.reset(**kwargs)\n        return obs, info\n\n    def step(self, ac):\n        return self.env.step(ac)", "\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        done = terminated or truncated\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs, info = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _, info = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs, info", "\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)", "\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n        return\n\n    def reward(self, reward):\n        unclipped_reward = reward\n        clipped_reward = np.clip(reward, -1, 1)\n        return (unclipped_reward, clipped_reward)", "\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n        \"\"\"\n        Warp frames to 84x84 as done in the Nature paper and later work.\n\n        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n        observation should be warped.\n        \"\"\"\n        super().__init__(env)\n        self._width = width\n        self._height = height\n        self._grayscale = grayscale\n        self._key = dict_space_key\n        if self._grayscale:\n            num_colors = 1\n        else:\n            num_colors = 3\n\n        new_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(self._height, self._width, num_colors),\n            dtype=np.uint8,\n        )\n        if self._key is None:\n            original_space = self.observation_space\n            self.observation_space = new_space\n        else:\n            original_space = self.observation_space.spaces[self._key]\n            self.observation_space.spaces[self._key] = new_space\n        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n\n    def observation(self, obs):\n        if self._key is None:\n            frame = obs\n        else:\n            frame = obs[self._key]\n\n        if self._grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n        )\n        if self._grayscale:\n            frame = np.expand_dims(frame, -1)\n\n        if self._key is None:\n            obs = frame\n        else:\n            obs = obs.copy()\n            obs[self._key] = frame\n        return obs", "\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        \"\"\"Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob, info = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob(), info\n\n    def step(self, action):\n        ob, reward, terminated, truncated, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, terminated, truncated, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))", "\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0", "\nclass LazyFrames(object):\n    def __init__(self, frames):\n        \"\"\"This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You'd not believe how complex the previous solution was.\"\"\"\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n    def count(self):\n        frames = self._force()\n        return frames.shape[frames.ndim - 1]\n\n    def frame(self, i):\n        return self._force()[..., i]", "\ndef make_atari(env_id, max_episode_steps=None, visualize=False):\n    if (visualize):\n        render_mode = \"human\"\n    else:\n        render_mode = None\n\n    env = gym.make(env_id, render_mode=render_mode)\n    assert 'NoFrameskip' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env", "\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, warp_frame=True, scale=False):\n    \"\"\"Configure environment for DeepMind-style Atari.\n    \"\"\"\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if 'FIRE' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    if warp_frame:\n        env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env", "\n"]}
{"filename": "a2/pg_agent.py", "chunked_list": ["import numpy as np\nimport torch\n\nimport envs.base_env as base_env\nimport learning.base_agent as base_agent\nimport learning.pg_model as pg_model\nimport util.torch_util as torch_util\n\nclass PGAgent(base_agent.BaseAgent):\n    NAME = \"PG\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n        \n        self._batch_size = config[\"batch_size\"]\n        self._critic_update_epoch = config[\"critic_update_epoch\"]\n        self._norm_adv_clip = config[\"norm_adv_clip\"]\n        self._action_bound_weight = config[\"action_bound_weight\"]\n\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = pg_model.PGModel(model_config, self._env)\n        return\n    \n    def _get_exp_buffer_length(self):\n        return self._steps_per_iter\n\n    def _build_exp_buffer(self, config):\n        super()._build_exp_buffer(config)\n        \n        buffer_length = self._get_exp_buffer_length()\n        \n        tar_val_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n        self._exp_buffer.add_buffer(\"tar_val\", tar_val_buffer)\n        \n        adv_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n        self._exp_buffer.add_buffer(\"adv\", adv_buffer)\n\n        norm_obs_buffer = torch.zeros_like(self._exp_buffer.get_data(\"obs\"))\n        self._exp_buffer.add_buffer(\"norm_obs\", norm_obs_buffer)\n\n        norm_action_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n        self._exp_buffer.add_buffer(\"norm_action\", norm_action_buffer)\n\n        return\n    \n    def _init_iter(self):\n        super()._init_iter()\n        self._exp_buffer.reset()\n        return\n\n    def _build_optimizer(self, config):\n        actor_lr = float(config[\"actor_learning_rate\"])\n        critic_lr = float(config[\"critic_learning_rate\"])\n\n        actor_params = list(self._model._actor_layers.parameters())+list(self._model._action_dist.parameters())\n        actor_params_grad = [p for p in actor_params if p.requires_grad]\n        self._actor_optimizer = torch.optim.SGD(actor_params_grad, actor_lr, momentum=0.9)\n\n        critic_params = list(self._model._critic_layers.parameters())+list(self._model._critic_out.parameters())\n        critic_params_grad = [p for p in critic_params if p.requires_grad]\n        self._critic_optimizer = torch.optim.SGD(critic_params_grad, critic_lr, momentum=0.9)\n\n        return\n    \n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        norm_action_dist = self._model.eval_actor(norm_obs)\n\n        if (self._mode == base_agent.AgentMode.TRAIN):\n            norm_a = norm_action_dist.sample()\n        elif (self._mode == base_agent.AgentMode.TEST):\n            norm_a = norm_action_dist.mode\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n        \n        norm_a = norm_a.detach()\n        a = self._a_norm.unnormalize(norm_a)\n\n        info = dict()\n\n        return a, info\n        \n    \n    def _build_train_data(self):\n        self.eval()\n        \n        obs = self._exp_buffer.get_data(\"obs\")\n        r = self._exp_buffer.get_data(\"reward\")\n        done = self._exp_buffer.get_data(\"done\")\n        action = self._exp_buffer.get_data(\"action\")\n        norm_action = self._a_norm.normalize(action)\n        norm_obs = self._obs_norm.normalize(obs)\n\n        ret = self._calc_return(r, done)\n        adv = self._calc_adv(norm_obs, ret)\n\n        adv_std, adv_mean = torch.std_mean(adv)\n        norm_adv = (adv - adv_mean) / torch.clamp_min(adv_std, 1e-5)\n        norm_adv = torch.clamp(norm_adv, -self._norm_adv_clip, self._norm_adv_clip)\n\n        self._exp_buffer.set_data(\"tar_val\", ret)\n        self._exp_buffer.set_data(\"adv\", norm_adv)\n        self._exp_buffer.set_data(\"norm_obs\", norm_obs)\n        self._exp_buffer.set_data(\"norm_action\", norm_action)\n        \n        info = {\n            \"adv_mean\": adv_mean,\n            \"adv_std\": adv_std\n        }\n        return info\n\n    def _update_model(self):\n        self.train()\n\n        num_samples = self._exp_buffer.get_sample_count()\n        batch_size = self._batch_size \n        num_batches = int(np.ceil(float(num_samples) / batch_size))\n        train_info = dict()\n\n        for e in range(self._critic_update_epoch):   \n            for b in range(num_batches):\n                batch = self._exp_buffer.sample(batch_size)\n                critic_info = self._update_critic(batch)              \n\n                torch_util.add_torch_dict(critic_info, train_info)\n\n        torch_util.scale_torch_dict(1.0 / num_batches, train_info)\n\n        actor_batch = {\n            \"norm_obs\": self._exp_buffer.get_data(\"norm_obs\"),\n            \"norm_action\": self._exp_buffer.get_data(\"norm_action\"),\n            \"adv\": self._exp_buffer.get_data(\"adv\")\n        }\n        actor_info = self._update_actor(actor_batch)\n\n        for key, data in actor_info.items():\n            train_info[key] = data\n\n        return train_info\n\n\n    def _update_critic(self, batch):\n        norm_obs = batch[\"norm_obs\"]\n        tar_val = batch[\"tar_val\"]\n\n        loss = self._calc_critic_loss(norm_obs, tar_val)\n\n        self._critic_optimizer.zero_grad()\n        loss.backward()\n        self._critic_optimizer.step()\n\n        info = {\n            \"critic_loss\": loss\n        }\n        return info\n\n    def _update_actor(self, batch):\n        norm_obs = batch[\"norm_obs\"]\n        norm_a = batch[\"norm_action\"]\n        adv = batch[\"adv\"]\n        \n        loss = self._calc_actor_loss(norm_obs, norm_a, adv)\n        \n        info = {\n            \"actor_loss\": loss\n        }\n        \n        if (self._action_bound_weight != 0):\n            a_dist = self._model.eval_actor(norm_obs)\n            action_bound_loss = self._compute_action_bound_loss(a_dist)\n            if (action_bound_loss is not None):\n                action_bound_loss = torch.mean(action_bound_loss)\n                loss += self._action_bound_weight * action_bound_loss\n                info[\"action_bound_loss\"] = action_bound_loss.detach()\n\n        self._actor_optimizer.zero_grad()\n        loss.backward()\n        self._actor_optimizer.step()\n        \n        return info\n    \n    def _calc_return(self, r, done):\n        '''\n        TODO 2.1: Given a tensor of per-timestep rewards (r), and a tensor (done)\n        indicating if a timestep is the last timestep of an episode, Output a\n        tensor (return_t) containing the return (i.e. reward-to-go) at each timestep.\n        '''\n        \n        # placeholder\n        return_t = torch.zeros_like(r)\n        return return_t\n\n    def _calc_adv(self, norm_obs, ret):\n        '''\n        TODO 2.2: Given the normalized observations (norm_obs) and the return at\n        every timestep (ret), output the advantage at each timestep (adv).\n        '''\n        \n        # placeholder\n        adv = torch.zeros_like(ret)\n        return adv\n\n    def _calc_critic_loss(self, norm_obs, tar_val):\n        '''\n        TODO 2.3: Given the normalized observations (norm_obs) and the returns at\n        every timestep (tar_val), compute a loss for updating the value\n        function (critic).\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        return loss\n\n    def _calc_actor_loss(self, norm_obs, norm_a, adv):\n        '''\n        TODO 2.4: Given the normalized observations (norm_obs), normalized\n        actions (norm_a), and the advantage at every timestep (adv), compute\n        a loss for updating the policy (actor).\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        return loss", "class PGAgent(base_agent.BaseAgent):\n    NAME = \"PG\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n        \n        self._batch_size = config[\"batch_size\"]\n        self._critic_update_epoch = config[\"critic_update_epoch\"]\n        self._norm_adv_clip = config[\"norm_adv_clip\"]\n        self._action_bound_weight = config[\"action_bound_weight\"]\n\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = pg_model.PGModel(model_config, self._env)\n        return\n    \n    def _get_exp_buffer_length(self):\n        return self._steps_per_iter\n\n    def _build_exp_buffer(self, config):\n        super()._build_exp_buffer(config)\n        \n        buffer_length = self._get_exp_buffer_length()\n        \n        tar_val_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n        self._exp_buffer.add_buffer(\"tar_val\", tar_val_buffer)\n        \n        adv_buffer = torch.zeros([buffer_length], device=self._device, dtype=torch.float)\n        self._exp_buffer.add_buffer(\"adv\", adv_buffer)\n\n        norm_obs_buffer = torch.zeros_like(self._exp_buffer.get_data(\"obs\"))\n        self._exp_buffer.add_buffer(\"norm_obs\", norm_obs_buffer)\n\n        norm_action_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n        self._exp_buffer.add_buffer(\"norm_action\", norm_action_buffer)\n\n        return\n    \n    def _init_iter(self):\n        super()._init_iter()\n        self._exp_buffer.reset()\n        return\n\n    def _build_optimizer(self, config):\n        actor_lr = float(config[\"actor_learning_rate\"])\n        critic_lr = float(config[\"critic_learning_rate\"])\n\n        actor_params = list(self._model._actor_layers.parameters())+list(self._model._action_dist.parameters())\n        actor_params_grad = [p for p in actor_params if p.requires_grad]\n        self._actor_optimizer = torch.optim.SGD(actor_params_grad, actor_lr, momentum=0.9)\n\n        critic_params = list(self._model._critic_layers.parameters())+list(self._model._critic_out.parameters())\n        critic_params_grad = [p for p in critic_params if p.requires_grad]\n        self._critic_optimizer = torch.optim.SGD(critic_params_grad, critic_lr, momentum=0.9)\n\n        return\n    \n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        norm_action_dist = self._model.eval_actor(norm_obs)\n\n        if (self._mode == base_agent.AgentMode.TRAIN):\n            norm_a = norm_action_dist.sample()\n        elif (self._mode == base_agent.AgentMode.TEST):\n            norm_a = norm_action_dist.mode\n        else:\n            assert(False), \"Unsupported agent mode: {}\".format(self._mode)\n        \n        norm_a = norm_a.detach()\n        a = self._a_norm.unnormalize(norm_a)\n\n        info = dict()\n\n        return a, info\n        \n    \n    def _build_train_data(self):\n        self.eval()\n        \n        obs = self._exp_buffer.get_data(\"obs\")\n        r = self._exp_buffer.get_data(\"reward\")\n        done = self._exp_buffer.get_data(\"done\")\n        action = self._exp_buffer.get_data(\"action\")\n        norm_action = self._a_norm.normalize(action)\n        norm_obs = self._obs_norm.normalize(obs)\n\n        ret = self._calc_return(r, done)\n        adv = self._calc_adv(norm_obs, ret)\n\n        adv_std, adv_mean = torch.std_mean(adv)\n        norm_adv = (adv - adv_mean) / torch.clamp_min(adv_std, 1e-5)\n        norm_adv = torch.clamp(norm_adv, -self._norm_adv_clip, self._norm_adv_clip)\n\n        self._exp_buffer.set_data(\"tar_val\", ret)\n        self._exp_buffer.set_data(\"adv\", norm_adv)\n        self._exp_buffer.set_data(\"norm_obs\", norm_obs)\n        self._exp_buffer.set_data(\"norm_action\", norm_action)\n        \n        info = {\n            \"adv_mean\": adv_mean,\n            \"adv_std\": adv_std\n        }\n        return info\n\n    def _update_model(self):\n        self.train()\n\n        num_samples = self._exp_buffer.get_sample_count()\n        batch_size = self._batch_size \n        num_batches = int(np.ceil(float(num_samples) / batch_size))\n        train_info = dict()\n\n        for e in range(self._critic_update_epoch):   \n            for b in range(num_batches):\n                batch = self._exp_buffer.sample(batch_size)\n                critic_info = self._update_critic(batch)              \n\n                torch_util.add_torch_dict(critic_info, train_info)\n\n        torch_util.scale_torch_dict(1.0 / num_batches, train_info)\n\n        actor_batch = {\n            \"norm_obs\": self._exp_buffer.get_data(\"norm_obs\"),\n            \"norm_action\": self._exp_buffer.get_data(\"norm_action\"),\n            \"adv\": self._exp_buffer.get_data(\"adv\")\n        }\n        actor_info = self._update_actor(actor_batch)\n\n        for key, data in actor_info.items():\n            train_info[key] = data\n\n        return train_info\n\n\n    def _update_critic(self, batch):\n        norm_obs = batch[\"norm_obs\"]\n        tar_val = batch[\"tar_val\"]\n\n        loss = self._calc_critic_loss(norm_obs, tar_val)\n\n        self._critic_optimizer.zero_grad()\n        loss.backward()\n        self._critic_optimizer.step()\n\n        info = {\n            \"critic_loss\": loss\n        }\n        return info\n\n    def _update_actor(self, batch):\n        norm_obs = batch[\"norm_obs\"]\n        norm_a = batch[\"norm_action\"]\n        adv = batch[\"adv\"]\n        \n        loss = self._calc_actor_loss(norm_obs, norm_a, adv)\n        \n        info = {\n            \"actor_loss\": loss\n        }\n        \n        if (self._action_bound_weight != 0):\n            a_dist = self._model.eval_actor(norm_obs)\n            action_bound_loss = self._compute_action_bound_loss(a_dist)\n            if (action_bound_loss is not None):\n                action_bound_loss = torch.mean(action_bound_loss)\n                loss += self._action_bound_weight * action_bound_loss\n                info[\"action_bound_loss\"] = action_bound_loss.detach()\n\n        self._actor_optimizer.zero_grad()\n        loss.backward()\n        self._actor_optimizer.step()\n        \n        return info\n    \n    def _calc_return(self, r, done):\n        '''\n        TODO 2.1: Given a tensor of per-timestep rewards (r), and a tensor (done)\n        indicating if a timestep is the last timestep of an episode, Output a\n        tensor (return_t) containing the return (i.e. reward-to-go) at each timestep.\n        '''\n        \n        # placeholder\n        return_t = torch.zeros_like(r)\n        return return_t\n\n    def _calc_adv(self, norm_obs, ret):\n        '''\n        TODO 2.2: Given the normalized observations (norm_obs) and the return at\n        every timestep (ret), output the advantage at each timestep (adv).\n        '''\n        \n        # placeholder\n        adv = torch.zeros_like(ret)\n        return adv\n\n    def _calc_critic_loss(self, norm_obs, tar_val):\n        '''\n        TODO 2.3: Given the normalized observations (norm_obs) and the returns at\n        every timestep (tar_val), compute a loss for updating the value\n        function (critic).\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        return loss\n\n    def _calc_actor_loss(self, norm_obs, norm_a, adv):\n        '''\n        TODO 2.4: Given the normalized observations (norm_obs), normalized\n        actions (norm_a), and the advantage at every timestep (adv), compute\n        a loss for updating the policy (actor).\n        '''\n        \n        # placeholder\n        loss = torch.zeros(1)\n        return loss"]}
{"filename": "a2/cem_agent.py", "chunked_list": ["import numpy as np\nimport torch\n\nimport learning.base_agent as base_agent\nimport learning.cem_model as cem_model\n\nclass CEMAgent(base_agent.BaseAgent):\n    NAME = \"CEM\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n\n        self._param_mean = None\n        self._param_std = None\n        self._best_return = None\n        self._best_params = None\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n        \n        self._population_size = config[\"population_size\"]\n        self._elite_ratio = config[\"elite_ratio\"]\n        self._eps_per_candidate = config[\"eps_per_candidate\"]\n        self._min_param_std = config[\"min_param_std\"]\n        return\n\n    def _build_optimizer(self, config):\n        return\n\n    def _init_train(self):\n        super()._init_train()\n\n        self._param_mean = torch.nn.utils.parameters_to_vector(self._model.parameters())\n        self._param_std = 0.5 * torch.ones_like(self._param_mean)\n        self._best_return = None\n        self._best_params = None\n        return\n\n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = cem_model.CEMModel(model_config, self._env)\n        return\n\n    def _decide_action(self, obs, info):\n        norm_obs = self._obs_norm.normalize(obs)\n        norm_action_dist = self._model.eval_actor(norm_obs)\n\n        norm_a = norm_action_dist.mode\n        norm_a = norm_a.detach()\n        a = self._a_norm.unnormalize(norm_a)\n\n        info = dict()\n        return a, info\n    \n    def _update_sample_count(self):\n        return self._sample_count\n    \n    def _train_iter(self):\n        candidates = self._sample_candidates(self._population_size)\n\n        rets, ep_lens = self._eval_candidates(candidates)\n        \n        curr_best_idx = np.argmax(rets)\n        curr_best_ret = rets[curr_best_idx]\n\n        if ((self._best_params is None) or (curr_best_ret > self._best_return)):\n            self._best_params = torch.clone(candidates[curr_best_idx])\n            self._best_return = curr_best_ret\n\n        num_samples = self._eps_per_candidate * np.sum(ep_lens)\n        self._sample_count += num_samples\n\n        new_mean, new_std = self._compute_new_params(candidates, rets)\n        self._param_mean[:] = new_mean\n        self._param_std[:] = new_std\n\n        torch.nn.utils.vector_to_parameters(self._best_params, self._model.parameters())\n\n        train_return = np.mean(rets)\n        train_ep_len = np.mean(ep_lens)\n        num_eps = self._population_size * self._eps_per_candidate\n        mean_param_std = torch.mean(new_std)\n\n        train_info = {\n            \"mean_return\": train_return,\n            \"mean_ep_len\": train_ep_len,\n            \"episodes\": num_eps,\n            \"param_std\": mean_param_std\n        }\n        return train_info\n\n\n    def _sample_candidates(self, n):\n        '''\n        TODO 1.1: Sample n set of candidate parameters from the current search\n        distribution. The search distribution is a guassian distribution with mean\n        self._param_mean and standard deviation self._param_std. Output a tensor\n        containing parameters for each candidate. The tensor should have dimensions\n        [n, param_size].\n        '''\n        param_size = self._param_mean.shape[0]\n\n        # placeholder\n        candidates = torch.zeros([n, param_size], device=self._device)\n\n        return candidates\n\n    def _eval_candidates(self, candidates):\n        '''\n        TODO 1.2: Evaluate the performance of a set of candidate parameters.\n        You can use torch.nn.utils.vector_to_parameters to copy a candidate's\n        parameters to the model for the policy (self._model). self._rollout_test\n        can then be used to evaluate the performance of that set of parameters.\n        Record the average return and average episode length of each candidate\n        in the output variables rets and ep_lens.\n        '''\n        n = candidates.shape[0]\n\n        # placeholder\n        rets = np.zeros(n)\n        ep_lens = np.zeros(n)\n\n        return rets, ep_lens\n\n    def _compute_new_params(self, params, rets):\n        '''\n        TODO 1.3: Update the search distribution given a set of candidate\n        parameters (params) and their corresponding performance (rets).\n        Return the mean (new_mean) and standard deviation (new_std) of\n        the new search distribution.\n        '''\n        param_size = self._param_mean.shape[0]\n\n        # placeholder\n        new_mean = torch.zeros(param_size, device=self._device)\n        new_std = torch.ones(param_size, device=self._device)\n\n        return new_mean, new_std", ""]}
{"filename": "a1/bc_agent.py", "chunked_list": ["import numpy as np\nimport torch\n\nimport learning.agent_builder as agent_builder\nimport learning.base_agent as base_agent\nimport learning.bc_model as bc_model\nimport util.torch_util as torch_util\n\nclass BCAgent(base_agent.BaseAgent):\n    NAME = \"BC\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n\n        buffer_size = config[\"exp_buffer_size\"]\n        self._exp_buffer_length = max(buffer_size, self._steps_per_iter)\n\n        self._batch_size = config[\"batch_size\"]\n        self._update_epochs = config[\"update_epochs\"]\n        return\n    \n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = bc_model.BCModel(model_config, self._env)\n        \n        self._build_expert(config)\n        self._sync_normalizers()\n        return    \n    \n    def _build_expert(self, config):\n        expert_config = config[\"expert_config\"]\n        expert = agent_builder.build_agent(expert_config, self._env, self._device)\n        \n        expert_model_file = config[\"expert_model_file\"]\n        assert(expert_model_file is not None)\n        expert.load(expert_model_file)\n        expert.set_mode(base_agent.AgentMode.TEST)\n\n        # putting the expert in a list makes the expert's parameters invisible to the pytorch module\n        self._experts = [expert]\n        return\n\n    def _sync_normalizers(self):\n        expert = self._experts[0]\n        self._obs_norm.load_state_dict(expert._obs_norm.state_dict())\n        self._a_norm.load_state_dict(expert._a_norm.state_dict())\n        return\n\n    def _get_exp_buffer_length(self):\n        return self._exp_buffer_length\n    \n    def _need_normalizer_update(self):\n        return False\n\n    def _build_exp_buffer(self, config):\n        super()._build_exp_buffer(config)\n        \n        expert_a_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n        self._exp_buffer.add_buffer(\"expert_a\", expert_a_buffer)\n        return\n    \n    def _record_data_pre_step(self, obs, info, action, action_info):\n        super()._record_data_pre_step(obs, info, action, action_info)\n        self._exp_buffer.record(\"expert_a\", action_info[\"expert_a\"])\n        return\n    \n    def _update_model(self):\n        self.train()\n\n        num_samples = self._exp_buffer.get_sample_count()\n        batch_size = self._batch_size \n        num_batches = int(np.ceil(float(num_samples) / batch_size))\n        train_info = dict()\n\n        for i in range(self._update_epochs):\n            for b in range(num_batches):\n                batch = self._exp_buffer.sample(batch_size)\n                loss_info = self._compute_loss(batch)\n                \n                self._optimizer.zero_grad()\n                loss = loss_info[\"loss\"]\n                loss.backward()\n                self._optimizer.step()\n\n                torch_util.add_torch_dict(loss_info, train_info)\n        \n        num_steps = self._update_epochs * num_batches\n        torch_util.scale_torch_dict(1.0 / num_steps, train_info)\n\n        return train_info        \n    \n    def _compute_loss(self, batch):\n        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n        norm_expert_a = self._a_norm.normalize(batch[\"expert_a\"])\n\n        actor_loss = self._compute_actor_loss(norm_obs, norm_expert_a)\n\n        info = {\n            \"loss\": actor_loss\n        }\n        return info\n\n    def _eval_expert(self, obs):\n        info = None\n        expert = self._experts[0]\n        expert_a, _ = expert._decide_action(obs, info)\n        return expert_a\n    \n    def _decide_action(self, obs, info):\n        '''\n        TODO 1.1: Implement code for sampling from the policy and\n        querying the expert policy for the expert actions.\n        '''\n\n        ## a) sample an action from the policy\n        # placeholder\n        a_space = self._env.get_action_space()\n        a = torch.zeros(a_space.shape, device=self._device)\n        \n        ## b) query the expert for an action\n        # placeholder\n        a_space = self._env.get_action_space()\n        expert_a = torch.zeros(a_space.shape, device=self._device)\n\n        a_info = {\n            \"expert_a\": expert_a\n        }\n        return a, a_info\n    \n    def _compute_actor_loss(self, norm_obs, norm_expert_a):\n        '''\n        TODO 1.2: Implement code to calculate the loss for training the policy.\n        '''\n        # placeholder\n        loss = torch.zeros(1)\n        return loss", "class BCAgent(base_agent.BaseAgent):\n    NAME = \"BC\"\n\n    def __init__(self, config, env, device):\n        super().__init__(config, env, device)\n        return\n\n    def _load_params(self, config):\n        super()._load_params(config)\n\n        buffer_size = config[\"exp_buffer_size\"]\n        self._exp_buffer_length = max(buffer_size, self._steps_per_iter)\n\n        self._batch_size = config[\"batch_size\"]\n        self._update_epochs = config[\"update_epochs\"]\n        return\n    \n    def _build_model(self, config):\n        model_config = config[\"model\"]\n        self._model = bc_model.BCModel(model_config, self._env)\n        \n        self._build_expert(config)\n        self._sync_normalizers()\n        return    \n    \n    def _build_expert(self, config):\n        expert_config = config[\"expert_config\"]\n        expert = agent_builder.build_agent(expert_config, self._env, self._device)\n        \n        expert_model_file = config[\"expert_model_file\"]\n        assert(expert_model_file is not None)\n        expert.load(expert_model_file)\n        expert.set_mode(base_agent.AgentMode.TEST)\n\n        # putting the expert in a list makes the expert's parameters invisible to the pytorch module\n        self._experts = [expert]\n        return\n\n    def _sync_normalizers(self):\n        expert = self._experts[0]\n        self._obs_norm.load_state_dict(expert._obs_norm.state_dict())\n        self._a_norm.load_state_dict(expert._a_norm.state_dict())\n        return\n\n    def _get_exp_buffer_length(self):\n        return self._exp_buffer_length\n    \n    def _need_normalizer_update(self):\n        return False\n\n    def _build_exp_buffer(self, config):\n        super()._build_exp_buffer(config)\n        \n        expert_a_buffer = torch.zeros_like(self._exp_buffer.get_data(\"action\"))\n        self._exp_buffer.add_buffer(\"expert_a\", expert_a_buffer)\n        return\n    \n    def _record_data_pre_step(self, obs, info, action, action_info):\n        super()._record_data_pre_step(obs, info, action, action_info)\n        self._exp_buffer.record(\"expert_a\", action_info[\"expert_a\"])\n        return\n    \n    def _update_model(self):\n        self.train()\n\n        num_samples = self._exp_buffer.get_sample_count()\n        batch_size = self._batch_size \n        num_batches = int(np.ceil(float(num_samples) / batch_size))\n        train_info = dict()\n\n        for i in range(self._update_epochs):\n            for b in range(num_batches):\n                batch = self._exp_buffer.sample(batch_size)\n                loss_info = self._compute_loss(batch)\n                \n                self._optimizer.zero_grad()\n                loss = loss_info[\"loss\"]\n                loss.backward()\n                self._optimizer.step()\n\n                torch_util.add_torch_dict(loss_info, train_info)\n        \n        num_steps = self._update_epochs * num_batches\n        torch_util.scale_torch_dict(1.0 / num_steps, train_info)\n\n        return train_info        \n    \n    def _compute_loss(self, batch):\n        norm_obs = self._obs_norm.normalize(batch[\"obs\"])\n        norm_expert_a = self._a_norm.normalize(batch[\"expert_a\"])\n\n        actor_loss = self._compute_actor_loss(norm_obs, norm_expert_a)\n\n        info = {\n            \"loss\": actor_loss\n        }\n        return info\n\n    def _eval_expert(self, obs):\n        info = None\n        expert = self._experts[0]\n        expert_a, _ = expert._decide_action(obs, info)\n        return expert_a\n    \n    def _decide_action(self, obs, info):\n        '''\n        TODO 1.1: Implement code for sampling from the policy and\n        querying the expert policy for the expert actions.\n        '''\n\n        ## a) sample an action from the policy\n        # placeholder\n        a_space = self._env.get_action_space()\n        a = torch.zeros(a_space.shape, device=self._device)\n        \n        ## b) query the expert for an action\n        # placeholder\n        a_space = self._env.get_action_space()\n        expert_a = torch.zeros(a_space.shape, device=self._device)\n\n        a_info = {\n            \"expert_a\": expert_a\n        }\n        return a, a_info\n    \n    def _compute_actor_loss(self, norm_obs, norm_expert_a):\n        '''\n        TODO 1.2: Implement code to calculate the loss for training the policy.\n        '''\n        # placeholder\n        loss = torch.zeros(1)\n        return loss", ""]}
