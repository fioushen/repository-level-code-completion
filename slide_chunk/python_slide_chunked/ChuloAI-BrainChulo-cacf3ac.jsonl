{"filename": "download-model.py", "chunked_list": ["'''\nDownloads models from Hugging Face to models/model-name.\nSource: https://github.com/oobabooga/text-generation-webui\n\nExample:\npython download-model.py facebook/opt-1.3b\n\n'''\n\nimport argparse", "\nimport argparse\nimport base64\nimport datetime\nimport hashlib\nimport json\nimport re\nimport sys\nfrom pathlib import Path\n", "from pathlib import Path\n\nimport requests\nimport tqdm\nfrom tqdm.contrib.concurrent import thread_map\n\n\ndef select_model_from_default_options():\n    models = {\n        \"Vicuna-7B-1.1-GPTQ-4bit-128g\": (\"TheBloke\", \"vicuna-7B-1.1-GPTQ-4bit-128g\", \"main\"),\n    }\n    choices = {}\n\n    print(\"Select the model that you want to download:\\n\")\n    for i, name in enumerate(models):\n        char = chr(ord('A') + i)\n        choices[char] = name\n        print(f\"{char}) {name}\")\n    char = chr(ord('A') + len(models))\n    print(f\"{char}) None of the above\")\n\n    print()\n    print(\"Input> \", end='')\n    choice = input()[0].strip().upper()\n    if choice == char:\n        print(\n            \"\"\"\\nThen type the name of your desired Hugging Face model in the format organization/name.\n\nExamples:\nfacebook/opt-1.3b\nEleutherAI/pythia-1.4b-deduped\n\"\"\")\n\n        print(\"Input> \", end='')\n        model = input()\n        branch = \"main\"\n    else:\n        arr = models[choices[choice]]\n        model = f\"{arr[0]}/{arr[1]}\"\n        branch = arr[2]\n\n    return model, branch", "\n\ndef sanitize_model_and_branch_names(model, branch):\n    if model[-1] == '/':\n        model = model[:-1]\n    if branch is None:\n        branch = \"main\"\n    else:\n        pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n        if not pattern.match(branch):\n            raise ValueError(\n                \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n\n    return model, branch", "\n\ndef get_download_links_from_huggingface(model, branch, text_only=False):\n    base = \"https://huggingface.co\"\n    page = f\"/api/models/{model}/tree/{branch}?cursor=\"\n    cursor = b\"\"\n\n    links = []\n    sha256 = []\n    classifications = []\n    has_pytorch = False\n    has_pt = False\n    has_ggml = False\n    has_safetensors = False\n    is_lora = False\n    while True:\n        if len(cursor) > 0:\n            page = f\"/api/models/{model}/tree/{branch}?cursor=\"\n            content = requests.get(f\"{base}{page}{cursor.decode()}\").content\n        else:\n            page = f\"/api/models/{model}/tree/{branch}\"\n            content = requests.get(f\"{base}{page}\").content\n\n        dict = json.loads(content)\n        if len(dict) == 0:\n            break\n\n        for i in range(len(dict)):\n            fname = dict[i]['path']\n            if not is_lora and fname.endswith(\n                    ('adapter_config.json', 'adapter_model.bin')):\n                is_lora = True\n\n            is_pytorch = re.match(\"(pytorch|adapter)_model.*\\\\.bin\", fname)\n            is_safetensors = re.match(\".*\\\\.safetensors\", fname)\n            is_pt = re.match(\".*\\\\.pt\", fname)\n            is_ggml = re.match(\"ggml.*\\\\.bin\", fname)\n            is_tokenizer = re.match(\"tokenizer.*\\\\.model\", fname)\n            is_text = re.match(\".*\\\\.(txt|json|py|md)\", fname) or is_tokenizer\n\n            if any(\n                (is_pytorch,\n                 is_safetensors,\n                 is_pt,\n                 is_ggml,\n                 is_tokenizer,\n                 is_text)):\n                if 'lfs' in dict[i]:\n                    sha256.append([fname, dict[i]['lfs']['oid']])\n                if is_text:\n                    links.append(\n                        f\"https://huggingface.co/{model}/resolve/{branch}/{fname}\")\n                    classifications.append('text')\n                    continue\n                if not text_only:\n                    links.append(\n                        f\"https://huggingface.co/{model}/resolve/{branch}/{fname}\")\n                    if is_safetensors:\n                        has_safetensors = True\n                        classifications.append('safetensors')\n                    elif is_pytorch:\n                        has_pytorch = True\n                        classifications.append('pytorch')\n                    elif is_pt:\n                        has_pt = True\n                        classifications.append('pt')\n                    elif is_ggml:\n                        has_ggml = True\n                        classifications.append('ggml')\n\n        cursor = base64.b64encode(\n            f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n        cursor = base64.b64encode(cursor)\n        cursor = cursor.replace(b'=', b'%3D')\n\n    # If both pytorch and safetensors are available, download safetensors only\n    if (has_pytorch or has_pt) and has_safetensors:\n        for i in range(len(classifications) - 1, -1, -1):\n            if classifications[i] in ['pytorch', 'pt']:\n                links.pop(i)\n\n    return links, sha256, is_lora", "\n\ndef get_output_folder(model, branch, is_lora, base_folder=None):\n    if base_folder is None:\n        base_folder = 'models' if not is_lora else 'loras'\n\n    output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n    if branch != 'main':\n        output_folder += f'_{branch}'\n    output_folder = Path(base_folder) / output_folder\n    return output_folder", "\n\ndef get_single_file(url, output_folder, start_from_scratch=False):\n    filename = Path(url.rsplit('/', 1)[1])\n    output_path = output_folder / filename\n    if output_path.exists() and not start_from_scratch:\n        # Check if the file has already been downloaded completely\n        r = requests.get(url, stream=True)\n        total_size = int(r.headers.get('content-length', 0))\n        if output_path.stat().st_size >= total_size:\n            return\n        # Otherwise, resume the download from where it left off\n        headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n        mode = 'ab'\n    else:\n        headers = {}\n        mode = 'wb'\n\n    r = requests.get(url, stream=True, headers=headers)\n    with open(output_path, mode) as f:\n        total_size = int(r.headers.get('content-length', 0))\n        block_size = 1024\n        with tqdm.tqdm(total=total_size, unit='iB', unit_scale=True, bar_format='{l_bar}{bar}| {n_fmt:6}/{total_fmt:6} {rate_fmt:6}') as t:\n            for data in r.iter_content(block_size):\n                t.update(len(data))\n                f.write(data)", "\n\ndef start_download_threads(\n        file_list,\n        output_folder,\n        start_from_scratch=False,\n        threads=1):\n    thread_map(\n        lambda url: get_single_file(\n            url,\n            output_folder,\n            start_from_scratch=start_from_scratch),\n        file_list,\n        max_workers=threads,\n        disable=True)", "\n\ndef download_model_files(\n        model,\n        branch,\n        links,\n        sha256,\n        output_folder,\n        start_from_scratch=False,\n        threads=1):\n    # Creating the folder and writing the metadata\n    if not output_folder.exists():\n        output_folder.mkdir()\n    with open(output_folder / 'huggingface-metadata.txt', 'w') as f:\n        f.write(f'url: https://huggingface.co/{model}\\n')\n        f.write(f'branch: {branch}\\n')\n        f.write(\n            f'download date: {str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))}\\n')\n        sha256_str = ''\n        for i in range(len(sha256)):\n            sha256_str += f'    {sha256[i][1]} {sha256[i][0]}\\n'\n        if sha256_str != '':\n            f.write(f'sha256sum:\\n{sha256_str}')\n\n    # Downloading the files\n    print(f\"Downloading the model to {output_folder}\")\n    start_download_threads(\n        links,\n        output_folder,\n        start_from_scratch=start_from_scratch,\n        threads=threads)", "\n\ndef check_model_files(model, branch, links, sha256, output_folder):\n    # Validate the checksums\n    validated = True\n    for i in range(len(sha256)):\n        fpath = (output_folder / sha256[i][0])\n\n        if not fpath.exists():\n            print(f\"The following file is missing: {fpath}\")\n            validated = False\n            continue\n\n        with open(output_folder / sha256[i][0], \"rb\") as f:\n            bytes = f.read()\n            file_hash = hashlib.sha256(bytes).hexdigest()\n            if file_hash != sha256[i][1]:\n                print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n                validated = False\n            else:\n                print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n\n    if validated:\n        print('[+] Validated checksums of all model files!')\n    else:\n        print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')", "\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('MODEL', type=str, default=None, nargs='?')\n    parser.add_argument('--branch', type=str, default='main',\n                        help='Name of the Git branch to download from.')\n    parser.add_argument('--threads', type=int, default=1,\n                        help='Number of files to download simultaneously.')\n    parser.add_argument('--text-only', action='store_true',\n                        help='Only download text files (txt/json).')\n    parser.add_argument('--output', type=str, default=None,\n                        help='The folder where the model should be saved.')\n    parser.add_argument('--clean', action='store_true',\n                        help='Does not resume the previous download.')\n    parser.add_argument('--check', action='store_true',\n                        help='Validates the checksums of model files.')\n    args = parser.parse_args()\n\n    branch = args.branch\n    model = args.MODEL\n    if model is None:\n        model, branch = select_model_from_default_options()\n\n    # Cleaning up the model/branch names\n    try:\n        model, branch = sanitize_model_and_branch_names(model, branch)\n    except ValueError as err_branch:\n        print(f\"Error: {err_branch}\")\n        sys.exit()\n\n    # Getting the download links from Hugging Face\n    links, sha256, is_lora = get_download_links_from_huggingface(\n        model, branch, text_only=args.text_only)\n\n    # Getting the output folder\n    output_folder = get_output_folder(\n        model, branch, is_lora, base_folder=args.output)\n\n    if args.check:\n        # Check previously downloaded files\n        check_model_files(model, branch, links, sha256, output_folder)\n    else:\n        # Download files\n        download_model_files(model, branch, links, sha256,\n                             output_folder, threads=args.threads)", ""]}
{"filename": "bootstrap_models.py", "chunked_list": ["import sys\nimport os\nimport subprocess\n\nDEFAULT_EMBEDDINGS_MODEL =\"sentence-transformers/all-MiniLM-L6-v2\"\nDEFAULT_MODEL = \"openlm-research/open_llama_3b\"\nMODEL_DIR = \"./models\"\n\n\ndef _download_if_not_exists(model):\n    models_dir = os.path.join(MODEL_DIR, model)\n    if os.path.exists(models_dir):\n        print(f\"Directory {models_dir} already exists! Skpping download!\")\n\n    else:\n        print(f\"Downloading model {model} to {models_dir}\")\n        print(\"Please note that if model is large this may take a while.\")\n        process = subprocess.run([\"python3\",  \"download-model.py\", model, \"--output\", MODEL_DIR], capture_output=True)\n        process.check_returncode()", "\ndef _download_if_not_exists(model):\n    models_dir = os.path.join(MODEL_DIR, model)\n    if os.path.exists(models_dir):\n        print(f\"Directory {models_dir} already exists! Skpping download!\")\n\n    else:\n        print(f\"Downloading model {model} to {models_dir}\")\n        print(\"Please note that if model is large this may take a while.\")\n        process = subprocess.run([\"python3\",  \"download-model.py\", model, \"--output\", MODEL_DIR], capture_output=True)\n        process.check_returncode()", "\n\ndef main(model, embeddings_model):\n    print(f\"\"\"Your choices:\nMODEL: {model}\nEMBEDDINGS MODEL: {embeddings_model}\n\"\"\")\n    try:\n        os.mkdir(MODEL_DIR)\n    except FileExistsError:\n        pass\n\n    _download_if_not_exists(embeddings_model)\n    _download_if_not_exists(model)\n    print(\"Success!\")", "\nif __name__ == \"__main__\":\n    model = None\n    embeddings = None\n    \n    if len(sys.argv) > 2:\n        embeddings = sys.argv[2]\n\n    if len(sys.argv) > 1:\n        model = sys.argv[1]\n\n    if len(sys.argv) == 1:\n        print(\n            \"NOTE: You can change the default downloaded model by passing an additional argument:\"\n            + f\"{sys.argv[0]} [hugging-face-llm-model-name] [hugging-face-embeddings-model-name]\"\n        )\n\n    if not embeddings:\n        embeddings = DEFAULT_EMBEDDINGS_MODEL\n    \n    if not model:\n        model = DEFAULT_MODEL\n\n    main(model, embeddings)"]}
{"filename": "app/settings.py", "chunked_list": ["import os\nfrom dotenv import load_dotenv\nimport logging\nfrom langchain.embeddings import (\n    HuggingFaceEmbeddings,\n    HuggingFaceInstructEmbeddings,\n)\n\n\nclass CustomFormatter(logging.Formatter):\n    grey = \"\\x1b[38;20m\"\n    yellow = \"\\x1b[33;20m\"\n    red = \"\\x1b[31;20m\"\n    bold_red = \"\\x1b[31;1m\"\n    reset = \"\\x1b[0m\"\n    format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n\n    FORMATS = {\n        logging.DEBUG: grey + format + reset,\n        logging.INFO: grey + format + reset,\n        logging.WARNING: yellow + format + reset,\n        logging.ERROR: red + format + reset,\n        logging.CRITICAL: bold_red + format + reset,\n    }\n\n    def format(self, record):\n        log_fmt = self.FORMATS.get(record.levelno)\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)", "\nclass CustomFormatter(logging.Formatter):\n    grey = \"\\x1b[38;20m\"\n    yellow = \"\\x1b[33;20m\"\n    red = \"\\x1b[31;20m\"\n    bold_red = \"\\x1b[31;1m\"\n    reset = \"\\x1b[0m\"\n    format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n\n    FORMATS = {\n        logging.DEBUG: grey + format + reset,\n        logging.INFO: grey + format + reset,\n        logging.WARNING: yellow + format + reset,\n        logging.ERROR: red + format + reset,\n        logging.CRITICAL: bold_red + format + reset,\n    }\n\n    def format(self, record):\n        log_fmt = self.FORMATS.get(record.levelno)\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)", "\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\n", "ch.setLevel(logging.DEBUG)\n\nch.setFormatter(CustomFormatter())\n\nlogger.addHandler(ch)\n\n\nclass Settings:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        load_dotenv()\n\n        # Chat API - By default, we are assuming Oobabooga's Text Generation\n        # WebUI is running\n        self.root_path = os.getcwd()\n\n        if self.root_path.endswith(\"app\"):\n            self.root_path = self.root_path[:-4]\n\n        self.model_root_path = os.path.join(self.root_path, \"models\")\n        self.backend_root_path = os.path.join(self.root_path, \"app\")\n\n        # let's ensure the models and backend path is accurate when mounting docker volumes\n        if self.root_path.startswith(\"/code\"):\n            self.model_root_path = \"/models\"\n            self.backend_root_path = self.root_path\n\n        # Guidance new settings\n        self.test_file = os.getenv(\"TEST_FILE\", \"/data/uploads/the_trial.txt\")\n        self.embeddings_map = {\n            **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n            **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]},\n        }\n        self.persist_directory = os.getenv(\"PERSIST_DIRECTORY\", \"./persist_directory\")\n\n        self.embeddings_model = os.getenv(\"EMBEDDINGS_MODEL\", f\"{self.model_root_path}/all-MiniLM-L6-v2\")\n\n        self.chat_api_url = os.getenv(\"CHAT_API_URL\", \"http://0.0.0.0:5000/api/v1/generate\")\n        self.model_path = self.model_root_path + os.getenv(\"MODEL_PATH\")\n        self.guidance_reasoning_model_path = self.model_root_path  + os.getenv(\"GUIDANCE_REASONING_MODEL_PATH\")\n        self.guidance_extraction_model_path = self.model_root_path  + os.getenv(\"GUIDANCE_EXTRACTION_MODEL_PATH\")\n        \n\n        # Where all data is stored\n        self.data_path = os.getenv(\"DATA_PATH\", f\"{self.backend_root_path}/data\")\n\n        # Where short-term memory is stored\n        self.memories_path = os.getenv(\"MEMORIES_PATH\", f\"{self.data_path}/memories\")\n\n        # Where uploads are saved\n        self.upload_path = os.getenv(\"UPLOAD_PATH\", f\"{self.data_path}/uploads\")\n\n        # Where conversation history is stored\n        self.conversation_history_path = os.getenv(\n            \"CONVERSATION_HISTORY_PATH\",\n            f\"{self.data_path}/conversation_history/\",\n        )\n\n        # Document store name\n        self.document_store_name = os.getenv(\"DOCUMENT_STORE_NAME\", \"brainchulo_docs\")\n        self.conversation_store_name = os.getenv(\"CONVERSATION_STORE_NAME\", \"brainchulo_convos\")\n\n        # Default objective - If we go objective-based, this is the default\n        self.default_objective = os.getenv(\"DEFAULT_OBJECTIVE\", \"Be a CEO.\")\n\n        # Database URL\n        self.database_url = os.getenv(\"DATABASE_URL\", \"sqlite:///data/brainchulo.db\")\n\n        self.andromeda_url = os.getenv(\"ANDROMEDA_URL\", \"http://0.0.0.0:9000\")\n\n        self.use_flow_agents = os.getenv(\"USE_FLOW_AGENTS\", \"false\") == \"true\"", "\n\ndef load_config():\n    return Settings()\n"]}
{"filename": "app/main.py", "chunked_list": ["import os\nimport shutil\nfrom fastapi import FastAPI, Depends, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom sqlmodel import SQLModel, create_engine, Session, desc\nfrom models.all import Conversation, Message, ConversationWithMessages\nfrom typing import List\nfrom settings import load_config, logger\nfrom plugins import load_plugins\nfrom alembic import command", "from plugins import load_plugins\nfrom alembic import command\nfrom alembic.config import Config\nfrom configparser import ConfigParser\n\nconfig = load_config()\n\nsqlite_database_url = config.database_url\nconnect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_database_url, echo=True, connect_args=connect_args)", "connect_args = {\"check_same_thread\": False}\nengine = create_engine(sqlite_database_url, echo=True, connect_args=connect_args)\n\n# Introducing a new feature flag\n# So GuidanceLLaMAcpp can coexist with FlowAgents\n\nif config.use_flow_agents:\n    logger.info(\"Using (experimental) flow agents\")\n    from conversations.document_based_flow import DocumentBasedConversationFlowAgent\n\n    convo = DocumentBasedConversationFlowAgent()\nelse:\n    logger.info(\"Using experimental Guidance LLaMA cpp implementation.\")\n    from conversations.document_based import DocumentBasedConversation\n\n    convo = DocumentBasedConversation()", "\n\ndef create_db_and_tables():\n    confparser = ConfigParser()\n    confparser.read(f\"{config.backend_root_path}/alembic.ini\")\n    confparser.set('alembic', 'script_location', f\"{config.backend_root_path}/migrations\")\n    confparser.set('alembic', 'prepend_sys_path', config.backend_root_path)\n\n    migrations_config_path = os.path.join(config.backend_root_path, \"generated_alembic.ini\")\n\n    with open(migrations_config_path, 'w') as config_file:\n        confparser.write(config_file)\n\n    migrations_config = Config(migrations_config_path)\n    command.upgrade(migrations_config, \"head\")", "\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n\n\napp = FastAPI()\n\n", "\n\n# Load the plugins\nload_plugins(app=app)\n\norigins = [\n    \"http://127.0.0.1:5173\",\n    \"http://localhost:5173\",\n    \"http://0.0.0.0:5173\",\n]", "    \"http://0.0.0.0:5173\",\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)", "    allow_headers=[\"*\"],\n)\n\n\n@app.on_event(\"startup\")\ndef on_startup():\n    create_db_and_tables()\n\n\n@app.post('/llm/query/', response_model=str)\ndef llm_query(*, query: str, session: Session = Depends(get_session)):\n    \"\"\"\n    Query the LLM\n    \"\"\"\n    return convo.predict(query, [])", "\n@app.post('/llm/query/', response_model=str)\ndef llm_query(*, query: str, session: Session = Depends(get_session)):\n    \"\"\"\n    Query the LLM\n    \"\"\"\n    return convo.predict(query, [])\n\n\n@app.post(\"/conversations\", response_model=Conversation)\ndef create_conversation(*, session: Session = Depends(get_session), conversation: Conversation):\n    \"\"\"\n    Create a new conversation.\n    \"\"\"\n    conversation = Conversation.from_orm(conversation)\n    session.add(conversation)\n    session.commit()\n    session.refresh(conversation)\n    print(str(conversation))\n    return conversation", "\n@app.post(\"/conversations\", response_model=Conversation)\ndef create_conversation(*, session: Session = Depends(get_session), conversation: Conversation):\n    \"\"\"\n    Create a new conversation.\n    \"\"\"\n    conversation = Conversation.from_orm(conversation)\n    session.add(conversation)\n    session.commit()\n    session.refresh(conversation)\n    print(str(conversation))\n    return conversation", "\n\n@app.put('/conversations/{conversation_id}', response_model=Conversation)\ndef update_conversation(*, session: Session = Depends(get_session), conversation_id: int, payload: dict):\n    \"\"\"\n    Update the title of a conversation.\n    \"\"\"\n    conversation = session.get(Conversation, conversation_id)\n    conversation.title = payload[\"title\"]\n    session.add(conversation)\n    session.commit()\n    session.refresh(conversation)\n\n    return conversation", "\n\n@app.delete(\"/conversations/{conversation_id}\")\ndef delete_conversation(*, session: Session = Depends(get_session), conversation_id: int):\n    \"\"\"\n    Delete a conversation.\n    \"\"\"\n    conversation = session.get(Conversation, conversation_id)\n    session.delete(conversation)\n    session.commit()\n\n    return conversation", "\n\n@app.get(\"/conversations\", response_model=List[Conversation])\ndef get_conversations(session: Session = Depends(get_session)):\n    \"\"\"\n    Get all conversations.\n    \"\"\"\n    return session.query(Conversation).order_by(desc(Conversation.id)).all()\n\n", "\n\n@app.get(\"/conversations/{conversation_id}\", response_model=ConversationWithMessages)\ndef get_conversation(conversation_id: int, session: Session = Depends(get_session)):\n    \"\"\"\n    Get a conversation by id.\n    \"\"\"\n    conversation = session.get(Conversation, conversation_id)\n\n    return conversation", "\n\n@app.post(\"/conversations/{conversation_id}/messages\", response_model=Message)\ndef create_message(*, session: Session = Depends(get_session), conversation_id: int, message: Message):\n    \"\"\"\n    Create a new message.\n    \"\"\"\n    message = Message.from_orm(message)\n    session.add(message)\n    session.commit()\n    session.refresh(message)\n\n    return message", "\n\n@app.post(\"/conversations/{conversation_id}/files\", response_model=dict)\ndef upload_file(*, conversation_id: int, file: UploadFile):\n    \"\"\"\n    Upload a file.\n    \"\"\"\n    try:\n        uploaded_file_name = file.filename\n        filepath = os.path.join(os.getcwd(), \"data\", config.upload_path, uploaded_file_name)\n\n        os.makedirs(os.path.dirname(filepath), mode=0o777, exist_ok=True)\n\n        with open(filepath, \"wb\") as f:\n            shutil.copyfileobj(file.file, f)\n\n        convo.load_document(filepath, conversation_id)\n\n        return {\"text\": f\"{uploaded_file_name} has been loaded into memory for this conversation.\"}\n    except Exception as e:\n        logger.error(f\"Error adding file to history: {e}\")\n        return f\"Error adding file to history: {e}\"", "\n\n@app.post('/llm/{conversation_id}/', response_model=str)\ndef llm(*, conversation_id: str, query: str, session: Session = Depends(get_session)):\n    \"\"\"\n    Query the LLM\n    \"\"\"\n    conversation_data = get_conversation(conversation_id, session)\n    history = conversation_data.messages\n\n    return convo.predict(query, conversation_id)", "\n    # we could also work from history only\n    # return convo.predict(query, history)\n\n\n@app.post(\"/conversations/{conversation_id}/messages/{message_id}/upvote\", response_model=Message)\ndef upvote_message(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n    \"\"\"\n    Upvote a message.\n    \"\"\"\n    message = session.get(Message, message_id)\n    message.rating = 1\n    session.add(message)\n    session.commit()\n    session.refresh(message)\n\n    return message", "\n\n@app.post(\"/conversations/{conversation_id}/messages/{message_id}/downvote\", response_model=Message)\ndef downvote_message(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n    \"\"\"\n    Downvote a message.\n    \"\"\"\n    message = session.get(Message, message_id)\n    message.rating = -1\n    session.add(message)\n    session.commit()\n    session.refresh(message)\n\n    return message", "\n\n@app.post(\"/conversations/{conversation_id}/messages/{message_id}/resetVote\", response_model=Message)\ndef reset_message_vote(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n    \"\"\"\n    Reset a message vote.\n    \"\"\"\n    message = session.get(Message, message_id)\n    message.rating = 0\n    session.add(message)\n    session.commit()\n    session.refresh(message)\n\n    return message", "\n\n@app.post(\"/reset\", response_model=dict)\ndef reset_all():\n    \"\"\"\n    Reset the database.\n    \"\"\"\n    SQLModel.metadata.drop_all(engine)\n    print(\"Database has been reset.\")\n    SQLModel.metadata.create_all(engine)\n\n    return {\"text\": \"Database has been reset.\"}", "\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=7865, reload=False)\n"]}
{"filename": "app/__init__.py", "chunked_list": [""]}
{"filename": "app/plugins/__init__.py", "chunked_list": ["import importlib\nimport pkgutil\nimport os\n\ndef load_plugins(app):\n    # Get the current directory\n    plugins_dir = os.path.dirname(__file__)\n\n    # Iterate over the files in the plugins directory\n    for _, plugin_name, is_package in pkgutil.iter_modules([plugins_dir]):\n        if is_package:\n            # Dynamically import the routes module from each plugin\n            plugin_module = importlib.import_module(f\"plugins.{plugin_name}.routes\")\n\n            # Get the router from the plugin's routes module\n            router = plugin_module.router\n\n            # Include the plugin's routes in the main FastAPI app\n            app.include_router(router, prefix=f\"/plugins/{plugin_name}\")", "\n"]}
{"filename": "app/plugins/sample_plugin/database.py", "chunked_list": ["from datetime import datetime\nfrom typing import Optional, List\nfrom sqlmodel import SQLModel, Field, Relationship\nfrom models.all import Conversation\n\n\nclass SamplePluginModelBase(SQLModel):\n    \"\"\"A base model for SamplePluginModel\"\"\"\n\n    # __tablename__: str = 'sample_plugin_model'\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    title: Optional[str]\n    conversation_id: Optional[int] = Field(default=None, foreign_key=\"conversation.id\")", "\n\nclass SamplePluginModel(SamplePluginModelBase, table=True):\n    \"\"\"A model for SamplePlugin\"\"\"\n\n    id: Optional[int] = Field(default=None, primary_key=True)\n\n\nclass SamplePluginModelRead(SamplePluginModelBase):\n    \"\"\"A read model for SamplePlugin\"\"\"\n\n    id: int", "class SamplePluginModelRead(SamplePluginModelBase):\n    \"\"\"A read model for SamplePlugin\"\"\"\n\n    id: int\n"]}
{"filename": "app/plugins/sample_plugin/__init__.py", "chunked_list": [""]}
{"filename": "app/plugins/sample_plugin/routes.py", "chunked_list": ["from fastapi import APIRouter\n\nrouter = APIRouter()\n\n\n@router.get(\"/\")\ndef sample_plugin_route():\n    return {\"message\": \"Sample plugin route\"}\n\n# Additional routes and functionality for the sample plugin", "\n# Additional routes and functionality for the sample plugin\n"]}
{"filename": "app/tools/base.py", "chunked_list": ["from pyparsing import abstractmethod\nfrom typing import Dict, List, Any\n\n\nclass ToolFactory():\n    \"\"\"Instantiates tools with a reference to the current conversation_id\"\"\"\n    def __init__(self, list_of_tool_classes: List[\"BaseTool\"]) -> None:\n        self._tool_class_references = list_of_tool_classes\n\n    def build_tools(self, conversation_id, context: Dict[str, Any]) -> Dict[str, \"BaseTool\"]:\n        resolved_tools = {}\n        for tool_class in self._tool_class_references:\n            tool = tool_class(conversation_id, context)\n            resolved_tools[tool.name] = tool\n        return resolved_tools", "\n\nclass BaseTool:\n    \"\"\"Base interface expected of tools\"\"\"\n    def __init__(self, conversation_id: str, name: str, tool_context: Dict[str, Any], required_context_keys: List[str]):\n        \"\"\"Stores a reference to the conversation ID.\n        \n        Avoiding injecting expensive operations in the __init__ method of the subclasses.\n        If you need to do something expensive, use a Singleton or redesign this Tool interface.\n\n        The reason being is that these Tools are instantied **per processed message**, so the\n        constructor must be cheap to execute.\n        \"\"\"\n        self.conversation_id = conversation_id\n        self.name = name\n        self.tool_context = tool_context\n        self._validate_context_keys(required_context_keys, tool_context)\n\n    def _validate_context_keys(self, keys, context):\n        for key in keys:\n            if key not in context:\n                raise TypeError(f\"This instance of {self.__class__.__name__} requires variable {key} in context.\")\n\n\n    @abstractmethod\n    def short_description(self) -> str:\n        \"\"\"Returns a short description of the tool.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def few_shot_examples(self) -> str:\n        \"\"\"Returns few \"\"\"\n        raise NotImplementedError()\n    \n    @abstractmethod\n    def __call__(self, variables: Dict[str, str]) -> str:\n        \"\"\"Executes the tool\"\"\"\n        raise NotImplementedError()", ""]}
{"filename": "app/tools/conversation_memory.py", "chunked_list": ["from tools.base import BaseTool\nfrom tools.utils import _build_conversation_filter\nfrom typing import Dict, Any\n\nclass ConversationSearchTool(BaseTool):\n    def __init__(self, conversation_id: str, tool_context: Dict[str, Any], name: str = \"Conversation Search\"):\n        required_context_keys = [\"vector_store_convs\", \"k\"]\n        super().__init__(conversation_id, name, tool_context=tool_context, required_context_keys=required_context_keys)\n\n    def short_description(self) -> str:\n        return \"A tool to search in your memory previous conversations with this user.\"\n    \n    def few_shot_examples(self) -> str:\n        return \"\"\"Question: What's your name?\nThought: I should search my name in the previous conversations.\nAction: Conversation Search\nAction Input:\nWhat's my name?\nObservation:\nUser: I'd like to give you a better name.\nBot: How would you like to call me?\nUser: I'd like to call you Joseph.\nBot: Alright, you may call me Joseph from now on.\n\nThought: The user wants to call me Joseph.\nFinal Answer: As I recall from a previous conversation, you call me Joseph.\"\"\"\n\n\n    def __call__(self, search_input):\n        \"\"\"\n        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n        This function is used as a helper function for the SearchLongTermMemory tool\n\n        Args:\n          search_input (str): The input to search for in the vector store.\n\n        Returns:\n          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n        \"\"\"\n\n        k = self.tool_context[\"k\"]\n        filter_= _build_conversation_filter(conversation_id=self.conversation_id)\n        docs = self.tool_context[\"vector_store_convs\"].similarity_search_with_score(\n            search_input, k=5, filter=filter_\n        )\n        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]", ""]}
{"filename": "app/tools/document_memory.py", "chunked_list": ["from tools.base import BaseTool\nfrom tools.utils import _build_conversation_filter\nfrom typing import Dict, Any\n\nclass DocumentSearchTool(BaseTool):\n    def __init__(self, conversation_id: str, tool_context: Dict[str, Any], name: str = \"Document Search\"):\n        required_context_keys = [\"vector_store_docs\", \"k\"]\n        super().__init__(conversation_id, name, tool_context=tool_context, required_context_keys=required_context_keys)\n\n    def short_description(self) -> str:\n        return \"Useful for when you need to answer questions about documents that were uploaded by the user.\"\n\n    def few_shot_examples(self) -> str:\n        return \"\"\"Question: What's your name?\nThought: I should search my name in the documents.\nAction: Document Search\nAction Input:\nWhat's my name?\nObservation: You're an AI. You don't have a name.\nThought: I should answer that I don't have a name.\nFinal Answer: As an AI, I don't have a name, at least not in the human sense.\"\"\"\n        \n\n    def __call__(self, search_input: Dict[str, str]) -> str:\n        \"\"\"Executes the tool\n        Search for the given input in the vector store and return the top k most similar documents with their scores.\n        This function is used as a helper function for the SearchLongTermMemory tool\n\n        Args:\n          search_input (str): The input to search for in the vector store.\n\n        Returns:\n          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n        \"\"\"\n        k = self.tool_context[\"k\"]\n        filter_= _build_conversation_filter(conversation_id=self.conversation_id)\n        docs = self.tool_context[\"vector_store_docs\"].similarity_search_with_score(\n            search_input, k=k, filter=filter_\n        )\n\n        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]", "\n\n"]}
{"filename": "app/tools/utils.py", "chunked_list": ["from typing import Optional\n\ndef _build_conversation_filter(conversation_id: Optional[str]):\n    if conversation_id is not None:\n        return {\"conversation_id\": conversation_id}\n    else:\n        return {}"]}
{"filename": "app/tools/web_access.py", "chunked_list": ["from settings import Settings\nfrom tools.base import BaseTool\n\nconfig = Settings.load_config()\n\n\nclass WebAccess(BaseTool):\n    url: str\n\n    def commands(self):\n        return {}", ""]}
{"filename": "app/agents/base.py", "chunked_list": ["# import the necessary libraries\nfrom abc import abstractmethod\nfrom colorama import Style, Fore\nfrom typing import List\nfrom andromeda_chain import AndromedaChain\nfrom tools.base import BaseTool\n\ndef color_print(msg, color):\n    print(color + Style.BRIGHT + msg  + Style.RESET_ALL, flush=True)\n", "\n\nclass BaseAgent:\n    \"\"\"Base Agent.\n\n    Nothing too exciting here.\n    \"\"\"\n    def __init__(self, andromeda: AndromedaChain, tools: List[BaseTool]):\n        self.andromeda = andromeda\n        self.tools = tools\n    \n    @abstractmethod\n    def run(self, query: str) -> str:\n        raise NotImplementedError()\n    \n    def do_tool(self, tool_name, act_input):\n        color_print(f\"Using tool: {tool_name}\", Fore.GREEN)\n        result = self.tools[tool_name](act_input)\n        color_print(f\"Tool result: {result}\", Fore.BLUE)\n        return result\n\n    def _build_tool_description_line(self, tool: BaseTool):\n        return f\"{tool.name}: {tool.short_description()}\"\n\n    def prepare_start_prompt(self, prompt_start_template):\n        tools = [item for _, item in self.tools.items()]\n        tools_descriptions = \"\\n\".join([self._build_tool_description_line(tool) for tool in tools])\n        action_list = str([tool.name for tool in tools]).replace('\"', \"\")\n        few_shot_examples = \"\\n\".join(\n            [\n                f\"Example {idx}:\\n{tool.few_shot_examples()}\"\n                for idx, tool in enumerate(tools)\n            ]\n        )\n        self.valid_tools = [tool.name for tool in tools]\n        self.prepared_prompt = prompt_start_template.format(\n            tools_descriptions=tools_descriptions,\n            action_list=action_list,\n            few_shot_examples=few_shot_examples\n        )", ""]}
{"filename": "app/agents/flow_cot.py", "chunked_list": ["from typing import Dict, Callable\nfrom andromeda_chain import AndromedaChain\nfrom agents.flow_based import BaseFlowAgent\nfrom flow.flow import Flow, PromptNode, ToolNode, ChoiceNode, StartNode\nfrom prompts.flow_guidance_cot import FlowChainOfThoughts, PROMPT_START_STRING\n\n\nclass ChainOfThoughtsFlowAgent(BaseFlowAgent):\n    def __init__(self, andromeda: AndromedaChain, tools: Dict[str, Callable[[str], str]]):\n        def execute_tool(variables):\n            action_name = variables[\"tool_name\"]\n            action_input = variables[\"act_input\"]\n            return self.do_tool(action_name, action_input)\n\n        start = StartNode(\"start\", FlowChainOfThoughts.flow_prompt_start, {\n            \"Action\": \"choose_action\",\n            \"Final Answer\": \"final_prompt\"\n        })\n        thought = PromptNode(\"thought\", FlowChainOfThoughts.thought_gen)\n        choose_action = PromptNode(\"choose_action\", FlowChainOfThoughts.choose_action)\n        perform_action = PromptNode(\"perform_action\", FlowChainOfThoughts.action_input)\n        execute_tool_node = ToolNode(\"execute_tool\", execute_tool)\n        decide = ChoiceNode(\"decide\", [\"thought\", \"final_prompt\"], max_decisions=3, force_exit_on=\"final_prompt\")\n        final = PromptNode(\"final_prompt\", FlowChainOfThoughts.final_prompt)\n\n        thought.set_next(choose_action)\n        choose_action.set_next(perform_action)\n        perform_action.set_next(execute_tool_node)\n        execute_tool_node.set_next(decide)\n\n        flow = Flow(\n            [start, thought, choose_action, perform_action, execute_tool_node, decide, final]\n        )\n\n        super().__init__(andromeda, tools, flow)\n        self.valid_answers = [\"Action\", \"Final Answer\"]\n        self.prepare_start_prompt(PROMPT_START_STRING)\n\n\n    def run(self, query: str) -> str:\n        if not self.prepared_prompt:\n            raise TypeError(\"Your inherinted BaseFlowAgent class must call 'prepare_start_prompt' in it's constructor.\")\n\n        return super().run(query, variables={\n            \"prompt_start\": self.prepared_prompt,\n            \"question\": query,\n            \"valid_tools\": self.valid_tools,\n            \"valid_answers\": self.valid_answers,\n        })", ""]}
{"filename": "app/agents/chain_of_thoughts.py", "chunked_list": ["from agents.base import BaseAgent\nfrom guidance_tooling.guidance_programs.tools import ingest_file\nfrom guidance_tooling.guidance_programs.tools import clean_text\nfrom langchain.llms import LlamaCpp\nimport os\nimport time\nimport guidance\nfrom colorama import Fore, Style\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import LlamaCpp", "from langchain.chains import RetrievalQA\nfrom langchain.llms import LlamaCpp\nfrom prompt_templates.qa_agent import *\nfrom settings import load_config\n\nimport re \n\nconfig = load_config()\nllm = None\nvalid_answers = ['Action', 'Final Answer']", "llm = None\nvalid_answers = ['Action', 'Final Answer']\nvalid_tools = [\"Check Question\", \"Google Search\"]\nTEST_FILE = os.getenv(\"TEST_FILE\")\nTEST_MODE = os.getenv(\"TEST_MODE\")\n\nETHICS = os.getenv(\"ETHICS\")\nQA_MODEL = os.getenv(\"MODEL_PATH\")\nmodel_path = config.model_path\n", "model_path = config.model_path\n\n\nif ETHICS == \"ON\":\n    agent_template = QA_ETHICAL_AGENT\nelse: \n    agent_template = QA_AGENT\n\ndef get_llm():\n    global llm\n    if llm is None:\n        print(\"Loading qa model...\")\n        model_path =QA_MODEL\n        model_n_ctx =1000\n        n_gpu_layers = 500\n        use_mlock = 0\n        n_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n        callbacks = []\n        llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n    return llm", "def get_llm():\n    global llm\n    if llm is None:\n        print(\"Loading qa model...\")\n        model_path =QA_MODEL\n        model_n_ctx =1000\n        n_gpu_layers = 500\n        use_mlock = 0\n        n_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n        callbacks = []\n        llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n    return llm", " \nclass ChainOfThoughtsAgent(BaseAgent):\n  \n    def __init__(self, guidance, llama_model, llama_model2):\n        self.guidance = guidance\n         # We first load the model in charge of reasoning along the guidance program\n        self.llama_model = llama_model\n        # We then load the model in charge of correctly identifying the data within the context and provide an answer\n        self.llama_model2 = llama_model2\n\n    \n    def print_stage(self, stage_name, message):\n        print(Fore.CYAN + Style.BRIGHT + f\"Entering {stage_name} round\" + Style.RESET_ALL)\n        time.sleep(1)\n        print(Fore.GREEN + Style.BRIGHT + message + Style.RESET_ALL)\n    \n    def searchQA(self, t):    \n        return self.checkQuestion(self.question, self.context)\n\n    def checkQuestion(self, question: str, context):\n        context = context\n        if TEST_MODE == \"ON\":\n            print(Fore.GREEN + Style.BRIGHT + \"No document loaded in conversation. Falling back on test file.\" + Style.RESET_ALL)\n            question = question.replace(\"Action Input: \", \"\")\n            qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=self.retriever, return_source_documents=True)\n            answer_data = qa({\"query\": question})\n\n            if 'result' not in answer_data:\n                print(f\"\\033[1;31m{answer_data}\\033[0m\")\n                return \"Issue in retrieving the answer.\"\n            context_documents = answer_data['source_documents']\n            context = \" \".join([clean_text(doc.page_content) for doc in context_documents])\n            print(Fore.WHITE + Style.BRIGHT + \"Printing langchain context...\" + Style.RESET_ALL)\n            print(Fore.WHITE + Style.BRIGHT + context + Style.RESET_ALL)\n        return context\n    \n    def ethics_check(self, question, ethics_prompt):\n        ethics_program = self.guidance(ethics_prompt)\n        return ethics_program(question=question)\n\n    def query_identification(self, question, conversation_prompt):\n        guidance.llm = self.llama_model\n        conversation_program = self.guidance(conversation_prompt) \n        return conversation_program(question=question)\n\n    def phatic_answer(self, question, history, phatic_prompt):\n        phatic_program = self.guidance(phatic_prompt)\n        return phatic_program(question=question, history=history)\n\n    def data_retrieval(self, question):\n        if self.llama_model2 is not None:\n            guidance.llm = self.llama_model2\n        referential_program = self.guidance(referential_prompt)\n        referential_round = referential_program(question=question, search=self.searchQA)\n        return referential_round\n\n    def answer_question(self, question, answer_prompt):\n        if self.llama_model2 is not None:\n            guidance.llm = self.llama_model2\n        answer_program = self.guidance(answer_prompt)\n        answer_round = answer_program(question=question, search=self.searchQA)\n        return answer_round[\"final_answer\"] \n\n    def run(self, query: str, context, history) -> str:\n\n        self.question = query \n        self.context = context\n        self.history = history\n        print(Fore.GREEN + Style.BRIGHT + \"Starting guidance agent...\" + Style.RESET_ALL)\n        conversation_round= self.query_identification(self.question , conversation_prompt)\n\n        if conversation_round[\"query_type\"] == \"Phatic\": \n            self.print_stage(\"answering\", \"User query identified as phatic\")\n            phatic_round = self.phatic_answer(self.question , history, phatic_prompt)\n            return phatic_round[\"phatic_answer\"]  \n\n        self.print_stage(\"data retrieval\", \"User query identified as referential\")\n        referential_round = self.data_retrieval(self.question )\n\n        if referential_round[\"answerable\"] == \"Yes\":\n            self.print_stage(\"answering\", \"Matching information found\")\n            return self.answer_question(self.question, answer_prompt)\n        else:\n            return \"I don't have enough information to answer.\"", "\n\n\n  "]}
{"filename": "app/agents/__init__.py", "chunked_list": ["from agents.chain_of_thoughts import ChainOfThoughtsAgent\nfrom agents.flow_cot import ChainOfThoughtsFlowAgent"]}
{"filename": "app/agents/flow_based.py", "chunked_list": ["# import the necessary libraries\nfrom typing import Dict, Callable\nfrom agents.base import BaseAgent\nfrom colorama import Style\nfrom andromeda_chain import AndromedaChain\nfrom flow.flow import Flow\n\ndef color_print(msg, color):\n    print(color + Style.BRIGHT + msg  + Style.RESET_ALL)\n", "\n\nclass BaseFlowAgent(BaseAgent):\n    \"\"\"Base Flow Agent.\n\n    Implements a graph that the agents execute.\n    \"\"\"\n    def __init__(self, andromeda: AndromedaChain, tools: Dict[str, Callable[[str], str]], flow: Flow):\n        super().__init__(andromeda, tools)\n        self.flow = flow\n\n    def run(self, query: str, variables=None) -> str:\n        if not variables:\n            variables = {}\n        return self.flow.execute(self.andromeda, query, variables)"]}
{"filename": "app/prompts/guidance_choice.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\n\nCHOICE_PROMPT =  AndromedaPrompt(\n    name=\"choice_prompt\",\n    prompt_template = \"\"\"{{history}}\nYou must now choose an option out of the {{valid_choices}}.\nRemember that it must be coherent with your last thought.\n{{select 'choice' logprobs='logprobs' options=valid_choices}}: \"\"\",\n    input_vars=[\"history\", \"valid_choices\"],", "{{select 'choice' logprobs='logprobs' options=valid_choices}}: \"\"\",\n    input_vars=[\"history\", \"valid_choices\"],\n    output_vars=[\"choice\"],\n)"]}
{"filename": "app/prompts/guidance_check_question.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\n\nPROMPT_CHECK_QUESTION = AndromedaPrompt(\n    name=\"start-prompt\",\n    prompt_template=\"\"\"You MUST answer with 'yes' or 'no'. Given the following pieces of context, determine if there are any elements related to the question in the context.\nDon't forget you MUST answer with 'yes' or 'no'.\nContext: {{context}}\nQuestion: Are there any elements related to \"\"{{question}}\"\" in the context?\n{{select 'answer' options=['yes', 'no']}}", "Question: Are there any elements related to \"\"{{question}}\"\" in the context?\n{{select 'answer' options=['yes', 'no']}}\n\"\"\",\n    guidance_kwargs={},\n    input_vars=[\"context\", \"question\"],\n    output_vars=[\"answer\"],\n)\n"]}
{"filename": "app/prompts/__init__.py", "chunked_list": [""]}
{"filename": "app/prompts/flow_guidance_cot.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\nPROMPT_START_STRING = \"\"\"You're an AI assistant with access to tools.\nYou're nice and friendly, and try to answer questions to the best of your ability.\nYou have access to the following tools.\n\n{tools_descriptions}\n\nStrictly use the following format:\n", "Strictly use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of {action_list}\nAction Input: the input to the action, should be a question.\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question", "Thought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nWhen chatting with the user, you can search information using your tools.\n{few_shot_examples}\n\nNow your turn.\nQuestion:\"\"\"\n\n\nclass FlowChainOfThoughts:\n    choose_action = AndromedaPrompt(\n        name=\"cot_choose_action\",\n        prompt_template=\"\"\"{{history}}\nAction: {{select 'tool_name' options=valid_tools}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\", \"valid_tools\"],\n        output_vars=[\"tool_name\"],\n    )\n\n    action_input = AndromedaPrompt(\n        name=\"cot_action_input\",\n        prompt_template=\"\"\"{{history}}{{gen 'act_input' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\"],\n        output_vars=[\"act_input\"],\n    )\n\n    thought_gen = AndromedaPrompt(\n        name=\"cot_thought_gen\",\n        prompt_template=\"\"\"{{history}}\nObservation: {{observation}}\nThought: {{gen 'thought' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\", \"observation\"],\n        output_vars=[\"thought\"],\n    )\n\n    final_prompt = AndromedaPrompt(\n        name=\"cot_final\",\n        prompt_template=\"\"\"{{history}}\nThought: I should now reply the user with what I thought and gathered.\nFinal Answer: {{gen 'final_answer' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\"],\n        output_vars=[\"final_answer\"],\n    )\n\n    flow_prompt_start = AndromedaPrompt(\n        name=\"cot_flow_prompt_start\",\n        prompt_template=\"\"\"{{prompt_start}} {{question}}\nThink carefully about what you should do next. Take an action or provide a final answer to the user.\nThought: {{gen 'thought' stop='\\\\n'}}{{#block hidden=True}}\n{{select 'choice' logprobs='logprobs' options=valid_answers}}\n:{{/block}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"prompt_start\", \"question\", \"valid_answers\"],\n        output_vars=[\"thought\", \"choice\"],\n    )", "\n\nclass FlowChainOfThoughts:\n    choose_action = AndromedaPrompt(\n        name=\"cot_choose_action\",\n        prompt_template=\"\"\"{{history}}\nAction: {{select 'tool_name' options=valid_tools}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\", \"valid_tools\"],\n        output_vars=[\"tool_name\"],\n    )\n\n    action_input = AndromedaPrompt(\n        name=\"cot_action_input\",\n        prompt_template=\"\"\"{{history}}{{gen 'act_input' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\"],\n        output_vars=[\"act_input\"],\n    )\n\n    thought_gen = AndromedaPrompt(\n        name=\"cot_thought_gen\",\n        prompt_template=\"\"\"{{history}}\nObservation: {{observation}}\nThought: {{gen 'thought' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\", \"observation\"],\n        output_vars=[\"thought\"],\n    )\n\n    final_prompt = AndromedaPrompt(\n        name=\"cot_final\",\n        prompt_template=\"\"\"{{history}}\nThought: I should now reply the user with what I thought and gathered.\nFinal Answer: {{gen 'final_answer' stop='\\\\n'}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"history\"],\n        output_vars=[\"final_answer\"],\n    )\n\n    flow_prompt_start = AndromedaPrompt(\n        name=\"cot_flow_prompt_start\",\n        prompt_template=\"\"\"{{prompt_start}} {{question}}\nThink carefully about what you should do next. Take an action or provide a final answer to the user.\nThought: {{gen 'thought' stop='\\\\n'}}{{#block hidden=True}}\n{{select 'choice' logprobs='logprobs' options=valid_answers}}\n:{{/block}}\"\"\",\n        guidance_kwargs={},\n        input_vars=[\"prompt_start\", \"question\", \"valid_answers\"],\n        output_vars=[\"thought\", \"choice\"],\n    )", ""]}
{"filename": "app/models/all.py", "chunked_list": ["from datetime import datetime\nfrom typing import Optional, List\nfrom sqlmodel import SQLModel, Field, Relationship\nfrom importlib import import_module\n\nNAMING_CONVENTION = {\n    \"ix\": \"ix_%(column_0_label)s\",\n    \"uq\": \"uq_%(table_name)s_%(column_0_name)s\",\n    \"ck\": \"ck_%(table_name)s_%(constraint_name)s\",\n    \"fk\": \"fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s\",", "    \"ck\": \"ck_%(table_name)s_%(constraint_name)s\",\n    \"fk\": \"fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s\",\n    \"pk\": \"pk_%(table_name)s\",\n}\n\nmetadata = SQLModel.metadata\nmetadata.naming_convention = NAMING_CONVENTION\n\ndef load_plugin_tables():\n    \"\"\"\n    Loads all plugins in the plugins directory that have a database.py file and merge their metadata.\n\n    :return: None\n    \"\"\"\n    import os\n    plugins_dir = os.path.dirname(__file__) + '/../plugins'\n\n    for plugin_name in os.listdir(plugins_dir):\n        if plugin_name.startswith('_'):\n            continue  # Skip hidden files\n\n        plugin_dir = os.path.join(plugins_dir, plugin_name)\n        if not os.path.exists(os.path.join(plugin_dir, 'database.py')):\n            continue  # Skip plugins without a database.py file\n\n        import_module(f'plugins.{plugin_name}.database')", "def load_plugin_tables():\n    \"\"\"\n    Loads all plugins in the plugins directory that have a database.py file and merge their metadata.\n\n    :return: None\n    \"\"\"\n    import os\n    plugins_dir = os.path.dirname(__file__) + '/../plugins'\n\n    for plugin_name in os.listdir(plugins_dir):\n        if plugin_name.startswith('_'):\n            continue  # Skip hidden files\n\n        plugin_dir = os.path.join(plugins_dir, plugin_name)\n        if not os.path.exists(os.path.join(plugin_dir, 'database.py')):\n            continue  # Skip plugins without a database.py file\n\n        import_module(f'plugins.{plugin_name}.database')", "\n\n\nclass ConversationBase(SQLModel):\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    title: Optional[str]\n\nclass Conversation(ConversationBase, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    messages: List[\"Message\"] = Relationship(back_populates=\"conversation\")", "\nclass ConversationRead(ConversationBase):\n    id: int\n\nclass MessageBase(SQLModel):\n    text: str\n    is_user: bool\n    conversation_id: Optional[int] = Field(default=None, foreign_key=\"conversation.id\")\n    rating: int = 0 # -1, 0, 1\n    created_at: datetime = Field(default_factory=datetime.utcnow, nullable=False)", "\nclass Message(MessageBase, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    conversation: Optional[Conversation] = Relationship(back_populates=\"messages\")\n\nclass MessageRead(MessageBase):\n    id: int\n\nclass ConversationWithMessages(ConversationRead):\n    messages: List[MessageRead] = []", "class ConversationWithMessages(ConversationRead):\n    messages: List[MessageRead] = []\n\nload_plugin_tables()"]}
{"filename": "app/prompt_templates/qa_agent.py", "chunked_list": ["QA_ETHICAL_AGENT=\"\"\"\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n\n{{#block hidden=True}}\n{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}", "{{#block hidden=True}}\n{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n{{#user~}}\nUtilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query: {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n{{~/user}}\n\n{{#assistant~}}\nObservation: Let's see if the query is offensive.\nDecision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}", "Decision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}\n{{/block}}\n\n{{#if (equal offensive 'Yes')~}}\nFinal Answer: I'm sorry, but I can't answer that.\n{{else}}\n\n{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n{{#user~}}", "{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n{{#user~}}\nQuestion: {{question}}\nNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n{{~/user}}\n\n{{#assistant~}}\nThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\nDecision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n{{~/assistant}}", "Decision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n{{~/assistant}}\n\n{{#if (equal query_type \"Phatic\")~}}\nObservation: The user's query is conversational. I need to answer as a helpful assistant while taking into account our chat history;\nChat history: {{history}}\nLatest user message: {{question}}\nThought: I need to stay in my role of a helpful assistant and make casual conversation.\nFinal Answer: {{gen 'phatic answer' temperature=0.7 max_tokens=50}}\n{{else}}", "Final Answer: {{gen 'phatic answer' temperature=0.7 max_tokens=50}}\n{{else}}\n\n{{#user~}}\nHere are the relevant documents from our database:{{set 'documents' (search question)}}\nUsing the concept of deixis, please evaluate if the answer to {{question}} is in :{{documents}}. Note that your response MUST contain either 'yes' or 'no'.\n{{~/user}}\n\n{{#assistant~}}\nThought: I need to determine if the answer to {{question}} is in: {{documents}}.", "{{#assistant~}}\nThought: I need to determine if the answer to {{question}} is in: {{documents}}.\nDecision:{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\n{{#if (equal answerable \"Yes\")~}}\nObservation: I believe I can answer {{question}} based on the information contained in my analysis.\nThought: Now that I know that I can answer, I should provide the information to the user.\nFinal Answer: {{gen 'answer' temperature=0 max_tokens=100}}\n{{else}}\nThought: I don't think I can answer the question based on the information contained in the returned documents.", "{{else}}\nThought: I don't think I can answer the question based on the information contained in the returned documents.\nFinal Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n\n{{/if}}\n{{~/assistant}}\n\n{{/if}}\n\n{{/if}}", "\n{{/if}}\n\n\n\n\"\"\"\n\n\nQA_AGENT= \"\"\"\n{{#system~}}", "QA_AGENT= \"\"\"\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nAnswer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n\n{{#user~}}\nQuestion: {{question}}", "{{#user~}}\nQuestion: {{question}}\n{{~/user}}\n\n{{#assistant~}}\nThought: Let's first check our database.\nAction: Check Question\nAction Input: {{question}}\n{{~/assistant}}\n", "{{~/assistant}}\n\n{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n{{#user~}}\nQuestion: {{question}}\nNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n{{~/user}}\n\n{{#assistant~}}\nThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?", "{{#assistant~}}\nThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\nDecision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n{{~/assistant}}\n\n{{#if (equal query_type \"Phatic\")~}}\nObservation: The user's query is conversational. I need to answer as a helpful assistant while taking into account our chat history;\nChat history: {{history}}\nLatest user message: {{question}}\nThought: I need to stay in my role of a helpful assistant and make casual conversation.", "Latest user message: {{question}}\nThought: I need to stay in my role of a helpful assistant and make casual conversation.\nFinal Answer: {{gen 'phatic answer' temperature=0.7 max_tokens=50}}\n{{else}}\n\n{{#user~}}\nHere are the relevant documents from our database:{{set 'documents' (search question)}}\nUsing the concept of deixis, please evaluate if the answer to {{question}} is in :{{documents}}. Note that your response MUST contain either 'yes' or 'no'.\n{{~/user}}\n", "{{~/user}}\n\n{{#assistant~}}\nThought: I need to determine if the answer to {{question}} is in: {{documents}}.\nDecision:{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\n{{#if (equal answerable \"Yes\")~}}\nObservation: I believe I can answer {{question}} based on the information contained in my analysis.\nThought: Now that I know that I can answer, I should provide the information to the user.\nFinal Answer: {{gen 'answer' temperature=0 max_tokens=100}}", "Thought: Now that I know that I can answer, I should provide the information to the user.\nFinal Answer: {{gen 'answer' temperature=0 max_tokens=100}}\n{{else}}\nThought: I don't think I can answer the question based on the information contained in the returned documents.\nFinal Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n\n{{/if}}\n{{~/assistant}}\n\n{{/if}}", "\n{{/if}}\n\n\"\"\"\nETHICS_PROMPT= '''\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nAnswer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.", "Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n\n{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n{{#user~}}\nUtilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n{{~/user}}\n\n{{#assistant~}}", "\n{{#assistant~}}\nObservation: Let's see if the query is offensive.\nDecision:{{#select 'ethic' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}\n\n'''\n\n\n", "\n\nethics_prompt= '''\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n", "{{~/system}}\n\n{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n{{#user~}}\nUtilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query: {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n{{~/user}}\n{{#assistant~}}\nObservation: Let's see if the query is inherently offensive.\nDecision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}", "Decision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}\n'''\n\nconversation_prompt=\"\"\"\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.", "You are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n{{#user~}}\nQuestion: {{question}}\nNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n{{~/user}}\n\n{{#assistant~}}", "\n{{#assistant~}}\nThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\nDecision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n{{~/assistant}}\n\"\"\"\n\nphatic_prompt= '''\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.", "{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n\nObservation: The user's query is conversational. I need to answer him as a helpful assistant while taking into account our chat history;\nChat history: {{history}}\nLatest user message: {{question}}", "Chat history: {{history}}\nLatest user message: {{question}}\nFinal Answer: {{gen 'phatic_answer' temperature=0 max_tokens=50}}'''\n\nreferential_prompt = '''\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Follow user queries as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about factual information. The input is the question to search relevant information.", "You are a helpful assistant. Follow user queries as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about factual information. The input is the question to search relevant information.\n{{~/system}}\n\n{{#user~}}\nUsing the concept of deixis, please evaluate if the answer to {{question}} is in the documents retrieved from your database. Note that your response MUST contain either 'yes' or 'no'.\n{{~/user}}\n\n{{#assistant~}}\nObservation: I need to determine if the answer to {{question}}  is in:", "{{#assistant~}}\nObservation: I need to determine if the answer to {{question}}  is in:\n{{#each (search question)}}\n{{this.document_content}}\n{{/each}}\n{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n{{~/assistant}}\n'''\n\nanswer_prompt = \"\"\"", "\nanswer_prompt = \"\"\"\n{{#system~}}\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\nSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n{{~/system}}\n\n{{#user~}}", "\n{{#user~}}\nPlease answer {{question}} based on the information contained in:\n{{#each (search question)}}\n{{this.document_content}}\n{{/each}}\n{{~/user}}\n\n{{#assistant~}}\nThought: Now that I know that I can answer, I should provide the information to the user. ", "{{#assistant~}}\nThought: Now that I know that I can answer, I should provide the information to the user. \nFinal Answer: {{gen 'final_answer' temperature=0.7 max_tokens=50}}\n{{~/assistant}}\"\"\"\n"]}
{"filename": "app/flow/__init__.py", "chunked_list": [""]}
{"filename": "app/flow/flow.py", "chunked_list": ["from abc import abstractmethod\nfrom colorama import Fore\nfrom typing import Tuple, List, Union, Dict\nfrom andromeda_chain import AndromedaChain, AndromedaPrompt, AndromedaResponse\nfrom agents.base import color_print\nfrom prompts.guidance_choice import CHOICE_PROMPT\nfrom copy import deepcopy\n\nclass Node:\n    def __init__(self, name) -> None:\n        self.name = name\n\n    @abstractmethod\n    def run(self, variables) -> Union[AndromedaResponse, Dict[str, str]]:\n        raise NotImplementedError()", "class Node:\n    def __init__(self, name) -> None:\n        self.name = name\n\n    @abstractmethod\n    def run(self, variables) -> Union[AndromedaResponse, Dict[str, str]]:\n        raise NotImplementedError()\n\n\nclass HiddenNode(Node):\n    \"\"\"Classes of nodes that can be executed in the background,\n    without expanding the context history for the agent.\n    \"\"\"", "\nclass HiddenNode(Node):\n    \"\"\"Classes of nodes that can be executed in the background,\n    without expanding the context history for the agent.\n    \"\"\"\n\nclass ChoiceNode(HiddenNode):\n    def __init__(self, name, choices: List[str], max_decisions, force_exit_on) -> None:\n        super().__init__(name)\n        self.choices = choices\n        self.max_decisions = max_decisions\n        self.force_exit_on=force_exit_on\n        self.decisions_made = 0\n\n    def run(self, chain: AndromedaChain, variables) -> str:        \n        if self.decisions_made >= self.max_decisions:\n            return self.force_exit_on\n\n        history = \"\"\n        if \"history\" in variables:\n            history = variables[\"history\"]\n\n        result = chain.run_guidance_prompt(\n            CHOICE_PROMPT,\n            input_vars={\n                \"history\": history,\n                \"valid_choices\": self.choices\n            }\n        )\n        self.decisions_made += 1\n        return result.result_vars[\"choice\"]\n\n    def set_next(self, next_):\n        self._next = next_\n\n\n    def next(self):\n        return self._next", "    \n\nclass ToolNode(Node):\n    def __init__(self, name, tool_callback, variable_name = \"observation\") -> None:\n        super().__init__(name)\n        self.tool_callback = tool_callback\n        self.variable_name = variable_name\n\n    def run(self, variables) -> Dict[str, str]:        \n        return {self.variable_name: self.tool_callback(variables)}\n\n    def set_next(self, next_):\n        self._next = next_\n\n    def next(self):\n        return self._next", "\n\n\nclass PromptNode(Node):\n    def __init__(self, name, prompt: AndromedaPrompt) -> None:\n        super().__init__(name)\n        self.prompt = prompt\n        self._next = None\n\n\n    def run(self, chain: AndromedaChain, variables) -> AndromedaResponse:        \n        input_dict = {}\n        for var_ in self.prompt.input_vars:\n            value = variables[var_]\n            input_dict[var_] = value\n\n        return chain.run_guidance_prompt(\n            self.prompt,\n            input_vars=input_dict\n        )\n\n    def set_next(self, next_):\n        self._next = next_\n\n\n    def next(self):\n        return self._next", "\n\nclass StartNode(PromptNode):\n    def __init__(self, name, prompt: AndromedaPrompt, choices: List[str]) -> None:\n        super().__init__(name, prompt)\n        self.choices = choices\n\n    def run(self, chain: AndromedaChain, variables) -> Tuple[str, AndromedaResponse]:\n        response = super().run(chain, variables)\n        choice = response.result_vars.pop(\"choice\")\n        return self.choices[choice], response", "\nclass Flow:\n    def __init__(self, nodes: List[PromptNode]) -> None:\n        assert len(nodes) > 0\n        self.nodes = nodes\n    \n    def execute(self, chain, query: str, variables: Dict[str, str], return_key=\"final_answer\"):\n        node = self.nodes[0]\n        variables[\"query\"] = query\n        history = \"\"\n        while node:\n            color_print(f\"---> On node {node.name}\", Fore.RED)\n            debug_vars = deepcopy(variables)\n            if \"history\" in debug_vars:\n                debug_vars.pop(\"history\")\n            if \"prompt_start\" in debug_vars:\n                debug_vars.pop(\"prompt_start\")\n\n            if isinstance(node, StartNode):\n                color_print(f\"Executing start node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n                choice, result = node.run(chain, variables)\n                color_print(f\"Node result: {result.result_vars}\", Fore.GREEN)\n                history = result.expanded_generation\n                variables.update(result.result_vars)\n                variables[\"history\"] = history\n                node = _find_node_by_name(choice, self.nodes, node)\n                color_print(f\"Choice decided to jump to node {node.name}\", Fore.RED)\n\n            elif isinstance(node, PromptNode):\n                color_print(f\"Executing node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n                result = node.run(chain, variables)\n                color_print(f\"Node result: {result.result_vars}\", Fore.GREEN)\n                history = result.expanded_generation\n                # Merge contexts\n                variables.update(result.result_vars)\n                variables[\"history\"] = history\n                node = node.next()\n\n            elif isinstance(node, ToolNode):\n                color_print(f\"Executing tool node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n                tool_result = node.run(variables)\n                variables.update(tool_result)\n                node = node.next()\n\n            elif isinstance(node, ChoiceNode):\n                color_print(f\"Executing choice node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n                choice = node.run(chain, variables)\n                node = _find_node_by_name(choice, self.nodes, node)\n                color_print(f\"Choice decided to jump to node {node.name}\", Fore.RED)\n            else:\n                raise ValueError(f\"Invalid node class: {type(node)}\")\n\n            color_print(f\"History: {history}\", Fore.CYAN)\n\n\n        color_print(f\"Flow ended, returning variable '{return_key}'.\", Fore.GREEN)\n        return variables[return_key] ", "\n\n\ndef _find_node_by_name(choice, nodes, current_node):\n    new_node = None\n    for n in nodes:\n        if n.name == choice:\n            new_node = n\n            break\n    if not new_node:\n        raise ValueError(f\"Choice {choice} led to limbo! Please choose the name of another node in the flow.\")\n    if new_node == current_node: \n        raise ValueError(f\"Choice {choice} led to an infinite loop on itself! Make sure choice node hop to itself.\")\n    return new_node"]}
{"filename": "app/migrations/env.py", "chunked_list": ["from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\n\nfrom alembic import context\nfrom models import all\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.", "# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here", "\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = all.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")", "# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()", "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata, render_as_batch=True, user_module_prefix=\"sqlmodel.sql.sqltypes.\"\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()", "\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"]}
{"filename": "app/migrations/versions/51a9f1ab4b8b_add_sample_plugin_demo_model.py", "chunked_list": ["\"\"\"Add Sample Plugin Demo Model\n\nRevision ID: 51a9f1ab4b8b\nRevises: 804b20c9599c\nCreate Date: 2023-06-04 21:30:37.777274\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nimport sqlmodel.sql.sqltypes", "import sqlalchemy as sa\nimport sqlmodel.sql.sqltypes\n\n\n# revision identifiers, used by Alembic.\nrevision = '51a9f1ab4b8b'\ndown_revision = '804b20c9599c'\nbranch_labels = None\ndepends_on = None\n", "depends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('samplepluginmodel',\n    sa.Column('created_at', sa.DateTime(), nullable=False),\n    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.Column('conversation_id', sa.Integer(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.ForeignKeyConstraint(['conversation_id'], ['conversation.id'], name=op.f('fk_samplepluginmodel_conversation_id_conversation')),\n    sa.PrimaryKeyConstraint('id', name=op.f('pk_samplepluginmodel'))\n    )", "    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('samplepluginmodel')\n    # ### end Alembic commands ###\n"]}
{"filename": "app/migrations/versions/804b20c9599c_initial.py", "chunked_list": ["\"\"\"initial\n\nRevision ID: 804b20c9599c\nRevises: \nCreate Date: 2023-05-31 20:06:36.912608\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nimport sqlmodel.sql.sqltypes", "import sqlalchemy as sa\nimport sqlmodel.sql.sqltypes\n\n\n# revision identifiers, used by Alembic.\nrevision = '804b20c9599c'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n", "depends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('conversation',\n    sa.Column('created_at', sa.DateTime(), nullable=False),\n    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('id', name=op.f('pk_conversation'))\n    )\n    op.create_table('message',\n    sa.Column('text', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('is_user', sa.Boolean(), nullable=False),\n    sa.Column('conversation_id', sa.Integer(), nullable=True),\n    sa.Column('rating', sa.Integer(), nullable=False),\n    sa.Column('created_at', sa.DateTime(), nullable=False),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.ForeignKeyConstraint(['conversation_id'], ['conversation.id'], name=op.f('fk_message_conversation_id_conversation')),\n    sa.PrimaryKeyConstraint('id', name=op.f('pk_message'))\n    )", "    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('message')\n    op.drop_table('conversation')\n    # ### end Alembic commands ###\n", ""]}
{"filename": "app/guidance_tooling/guidance_programs/tools.py", "chunked_list": ["from dotenv import load_dotenv\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\nfrom langchain import HuggingFacePipeline\nfrom colorama import Fore, Style\nimport re\nfrom langchain.vectorstores import Chroma\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\nimport os", "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\nimport os\nfrom langchain.llms import LlamaCpp\n\nload_dotenv()\n\nTEST_FILE = os.getenv(\"TEST_FILE\")\nEMBEDDINGS_MODEL = os.getenv(\"EMBEDDINGS_MODEL\")\n\nEMBEDDINGS_MAP = {", "\nEMBEDDINGS_MAP = {\n    **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n    **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]}\n}\n\nmodel_type = os.environ.get('MODEL_TYPE')\nmodel_path = os.environ.get('MODEL_PATH')\nmodel_n_ctx =1000\ntarget_source_chunks = os.environ.get('TARGET_SOURCE_CHUNKS')", "model_n_ctx =1000\ntarget_source_chunks = os.environ.get('TARGET_SOURCE_CHUNKS')\nn_gpu_layers = os.environ.get('N_GPU_LAYERS')\nuse_mlock = os.environ.get('USE_MLOCK')\nn_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\ncallbacks = []\nqa_prompt = \"\"\n\nCHROMA_SETTINGS = {}  # Set your Chroma settings here\n\ndef clean_text(text):\n    # Remove line breaksRetrievalQA\n    text = text.replace('\\n', ' ')\n\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    return text", "CHROMA_SETTINGS = {}  # Set your Chroma settings here\n\ndef clean_text(text):\n    # Remove line breaksRetrievalQA\n    text = text.replace('\\n', ' ')\n\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    return text", "\ndef load_unstructured_document(document: str) -> list[Document]:\n    with open(document, 'r') as file:\n        text = file.read()\n    title = os.path.basename(document)\n    return [Document(page_content=text, metadata={\"title\": title})]\n\ndef split_documents(documents: list[Document], chunk_size: int = 250, chunk_overlap: int = 20) -> list[Document]:\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    return text_splitter.split_documents(documents)", "\n \n\ndef ingest_file(file_path):\n        # Load unstructured document\n        documents = load_unstructured_document(file_path)\n\n        # Split documents into chunks\n        documents = split_documents(documents, chunk_size=250, chunk_overlap=100)\n\n        # Determine the embedding model to use\n        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n        if EmbeddingsModel is None:\n            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n\n        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n\n        # Store embeddings from the chunked documents\n        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n\n        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n\n        print(file_path)\n        print(retriever)\n\n        return retriever", "\n\n\ndef load_tools():  \n    #llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n\n    def ingest_file(file_path):\n        # Load unstructured document\n        documents = load_unstructured_document(file_path)\n\n        # Split documents into chunks\n        documents = split_documents(documents, chunk_size=120, chunk_overlap=20)\n\n        # Determine the embedding model to use\n        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n        if EmbeddingsModel is None:\n            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n\n        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n\n        # Store embeddings from the chunked documents\n        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n\n        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n\n        print(file_path)\n        print(retriever)\n\n        return retriever, file_path\n\n    file_path = TEST_FILE\n    retriever, title = ingest_file(file_path)\n\n\n    dict_tools = {\n        'File Ingestion': ingest_file,\n    }\n\n    return dict_tools", ""]}
{"filename": "app/memory/base.py", "chunked_list": ["from typing import Any, Iterable, List, Optional, Dict, Tuple, Type\nfrom pydantic import BaseModel\nfrom pyparsing import abstractmethod\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores import VectorStore\n\n\nclass BaseMemory(BaseModel):\n    collection_name: Optional[str]\n    vector_store: Optional[Type[VectorStore]]\n\n    def __init__(self, collection_name: str = \"default_collection\"):\n        # init super class\n        super().__init__()\n        self.collection_name = collection_name\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        response = self.vector_store.add_texts(texts, metadatas, ids, **kwargs)\n        self.vector_store._client.persist()\n\n        return response\n\n    def add_documents(self, documents: list[Document]):\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n\n        return self.add_texts(texts, metadatas)\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        return self.vector_store.similarity_search(query, k=k, **kwargs)\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        return self.vector_store.similarity_search_by_vector(\n            embedding, k=k, **kwargs)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        return self.vector_store.similarity_search_with_score(\n            query, k=k, **kwargs)\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        return self.vector_store.max_marginal_relevance_search_by_vector(\n            embedding, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs)\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        return self.vector_store.max_marginal_relevance_search(\n            query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs)\n\n    def delete_collection(self) -> None:\n        return self.vector_store.delete_collection()\n\n    def persist(self) -> None:\n        return self.vector_store.persist()\n\n    def update_document(self, document_id: str, document: Document) -> None:\n        return self.vector_store.update_document(document_id, document)\n\n    def get_store(self) -> Document:\n        return self.vector_store", ""]}
{"filename": "app/memory/chroma_memory.py", "chunked_list": ["import os\nfrom typing import Any, Iterable, List, Optional, Type\nfrom memory.base import BaseMemory\nfrom langchain import vectorstores\nfrom langchain.embeddings import (\n    HuggingFaceInstructEmbeddings,\n    HuggingFaceEmbeddings,\n)\nfrom langchain.docstore.document import Document\nfrom settings import load_config, logger", "from langchain.docstore.document import Document\nfrom settings import load_config, logger\n\nconfig = load_config()\n\n\nclass Chroma(BaseMemory):\n    vector_store: Optional[Type[vectorstores.Chroma]]\n    collection_name: Optional[str]\n\n    def __init__(self, **kwargs: Any):\n        super().__init__(**kwargs)\n        embeddings_model_name = config.embeddings_model.split(\"/\")[-1]\n        EmbeddingsModel = config.embeddings_map.get(embeddings_model_name)\n        if EmbeddingsModel is None:\n            raise ValueError(f\"Invalid embeddings model: {config.embeddings_model}\")\n\n        kwargs = {\"model_name\": config.embeddings_model}\n        if EmbeddingsModel == HuggingFaceInstructEmbeddings:\n            kwargs[\"model_kwargs\"] = {\"device\": \"cuda\"}\n\n        embeddings = EmbeddingsModel(**kwargs)\n\n        persist_directory = os.path.join(os.getcwd(), \"data\", config.memories_path)\n        # Create directory if it doesn't exist\n        os.makedirs(persist_directory, exist_ok=True, mode=0o777)\n\n        self.vector_store = vectorstores.Chroma(\n            collection_name=self.collection_name,\n            embedding_function=embeddings,\n            persist_directory=persist_directory,\n        )\n\n        self.setup_index()\n\n    def setup_index(self):\n        collection = self.vector_store._client.get_collection(self.collection_name)\n        if len(collection.get()['ids']) < 6:\n            self.add_texts(\n                [\"Hello\", \"world!\", \"this\", \"is\", \"a\", \"test\"],\n                ids=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"],\n            )", ""]}
{"filename": "app/conversations/document_based.py", "chunked_list": ["from langchain.document_loaders import TextLoader\nfrom memory.chroma_memory import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom colorama import Fore, Style\nfrom guidance_tooling.guidance_programs.tools import clean_text\n\nfrom andromeda_chain import AndromedaChain\nfrom agents import ChainOfThoughtsAgent\nfrom settings import logger, load_config\nimport guidance ", "from settings import logger, load_config\nimport guidance \nimport os\n\nconfig = load_config()\ndict_tools = None\nllama_model = None\nllama_model2 = None\nguidance_reasoning_model_path = config.guidance_reasoning_model_path\nguidance_extraction_model_path = config.guidance_extraction_model_path", "guidance_reasoning_model_path = config.guidance_reasoning_model_path\nguidance_extraction_model_path = config.guidance_extraction_model_path\n\nTEST_MODE = os.getenv(\"TEST_MODE\")\nGUIDANCE_MODEL = os.getenv(\"GUIDANCE_MODEL_PATH\")\n\ndef get_llama_model():\n    global llama_model\n    if llama_model is None:\n        print(\"Loading main guidance model...\")\n        llama_model = guidance.llms.LlamaCpp(\n            model = guidance_reasoning_model_path,\n            tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n            before_role = \"<|\",\n            after_role = \"|>\",\n            n_gpu_layers=300,\n            n_threads=12,\n            caching=False, )\n        print(\"Loading main guidance model...\")\n        guidance.llm = llama_model\n    return llama_model", "\ndef get_llama_model2():\n    global llama_model2\n    if llama_model2 is None and guidance_extraction_model_path is not None: \n        print(\"Loading guidance model...\")\n        llama_model2 = guidance.llms.LlamaCpp(\n            model = guidance_extraction_model_path,\n            tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n            before_role = \"<|\",\n            after_role = \"|>\",\n            n_gpu_layers=300,\n            n_threads=12,\n            caching=False, )\n        print(\"Loading second guidance model...\")\n    return llama_model2", "\nclass DocumentBasedConversation:\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the class. It sets up LLM, text splitter, vector store, prompt template, retriever,\n        conversation chain, tools, and conversation agent if USE_AGENT is True.\n        \"\"\"\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500, chunk_overlap=20, length_function=len)\n        self.llama_model = get_llama_model()\n        if llama_model2 is not None:\n            self.llama_model2 = get_llama_model2()\n        guidance.llm = self.llama_model\n        self.vector_store_docs = Chroma(collection_name=\"docs_collection\")\n        self.vector_store_convs = Chroma(collection_name=\"convos_collection\")\n        tools = {\n            \"Search Documents\": self.search_documents,\n            \"Search Conversations\": self.search_conversations,\n        }\n        self.andromeda = AndromedaChain(config.andromeda_url)\n        self.document_qa_agent = ChainOfThoughtsAgent(guidance, llama_model,llama_model2)\n\n\n    def load_document(self, document_path, conversation_id=None):\n        \"\"\"\n        Load a document from a file and add its contents to the vector store.\n\n        Args:\n          document_path: A string representing the path to the document file.\n\n        Returns:\n          None.\n        \"\"\"\n        text_loader = TextLoader(document_path, encoding=\"utf8\")\n        documents = text_loader.load()\n        documents = self.text_splitter.split_documents(documents)\n\n        if conversation_id is not None:\n            for doc in documents:\n                doc.metadata[\"conversation_id\"] = conversation_id\n\n        self.vector_store_docs.add_documents(documents)\n\n    def search_documents(self, search_input, conversation_id=None):\n        \"\"\"\n        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n        This function is used as a helper function for the SearchLongTermMemory tool\n\n        Args:\n          search_input (str): The input to search for in the vector store.\n\n        Returns:\n          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n        \"\"\"\n\n        logger.info(f\"Searching for: {search_input} in LTM\")\n        docs = self.vector_store_docs.similarity_search_with_score(\n            search_input, k=5, filter=filter\n        )\n        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n        \n    def search_conversations(self, search_input, conversation_id=None):\n        \"\"\"\n        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n        This function is used as a helper function for the SearchLongTermMemory tool\n\n        Args:\n          search_input (str): The input to search for in the vector store.\n\n        Returns:\n          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n        \"\"\"\n        if conversation_id is not None:\n            filter = {\"conversation_id\": conversation_id}\n        else:\n            filter = {}\n\n        logger.info(f\"Searching for: {search_input} in LTM\")\n        docs = self.vector_store_convs.similarity_search_with_score(\n            search_input, k=5, filter=filter\n        )\n        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n\n    def predict(self, input, history):\n      global dict_tools\n      \"\"\"\n      Predicts a response based on the given input.\n\n      Args:\n        input (str): The input string to generate a response for.\n\n      Returns:\n        str: The generated response string.\n\n      Raises:\n        OutputParserException: If the response from the conversation agent could not be parsed.\n      \"\"\"\n      context = self.search_documents(input)\n      \n      str_context = str(context)\n      print(Fore.GREEN + Style.BRIGHT + \"Printing vector search context...\" + Style.RESET_ALL)\n      print(Fore.GREEN + Style.BRIGHT + str_context + Style.RESET_ALL)\n      final_answer = self.document_qa_agent.run(input, context, history)\n\n      print(Fore.CYAN + Style.BRIGHT + \"Printing full thought process...\" + Style.RESET_ALL)\n      print(Fore.CYAN + Style.BRIGHT + str(final_answer) + Style.RESET_ALL)\n\n      if isinstance(final_answer, dict):\n          final_answer = {'answer': str(final_answer), 'function': str(final_answer['fn'])}\n      else:\n          # Handle the case when final_answer is not a dictionary.\n          final_answer = {'answer': str(final_answer)}\n\n          # Check if 'Final Answer:' key exists in the dictionary\n      if 'Final Answer:' in final_answer['answer']:\n          # Find the index of 'Final Answer:' and extract everything after it\n          answer_start_index = final_answer['answer'].index('Final Answer:') + len('Final Answer:')\n          final_answer_text = final_answer['answer'][answer_start_index:]\n          return final_answer_text.strip()\n      else:\n          return final_answer[\"answer\"]", "\n\n"]}
{"filename": "app/conversations/document_based_flow.py", "chunked_list": ["from langchain.document_loaders import TextLoader\nfrom memory.chroma_memory import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nfrom andromeda_chain import AndromedaChain\nfrom agents import ChainOfThoughtsFlowAgent\nfrom tools.base import ToolFactory\nfrom tools.document_memory import DocumentSearchTool\nfrom tools.conversation_memory import ConversationSearchTool\n", "from tools.conversation_memory import ConversationSearchTool\n\nfrom settings import load_config, logger\n\nconfig = load_config()\n\n\nclass DocumentBasedConversationFlowAgent:\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the class. It sets up LLM, text splitter, vector store, prompt template, retriever,\n        conversation chain, tools, and conversation agent if USE_AGENT is True.\n        \"\"\"\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500, chunk_overlap=20, length_function=len)\n\n        self.vector_store_docs = Chroma(collection_name=\"docs_collection\")\n        self.vector_store_convs = Chroma(collection_name=\"convos_collection\")\n        self.tools = [DocumentSearchTool, ConversationSearchTool]\n        self.andromeda = AndromedaChain(config.andromeda_url)\n\n        self.active_agent_class = ChainOfThoughtsFlowAgent\n        self.tool_context = {\n            \"vector_store_docs\": self.vector_store_docs,\n            \"vector_store_convs\": self.vector_store_convs,\n            \"k\": 5,\n        }\n\n\n    def load_document(self, document_path, conversation_id=None):\n        \"\"\"\n        Load a document from a file and add its contents to the vector store.\n\n        Args:\n          document_path: A string representing the path to the document file.\n\n        Returns:\n          None.\n        \"\"\"\n        text_loader = TextLoader(document_path, encoding=\"utf8\")\n        documents = text_loader.load()\n        documents = self.text_splitter.split_documents(documents)\n\n        if conversation_id is not None:\n            for doc in documents:\n                doc.metadata[\"conversation_id\"] = conversation_id\n\n        self.vector_store_docs.add_documents(documents)\n\n\n    def predict(self, input: str, conversation_id: str):\n        \"\"\"\n        Predicts a response based on the given input.\n\n        Args:\n          input (str): The input string to generate a response for.\n\n        Returns:\n          str: The generated response string.\n\n        Raises:\n          OutputParserException: If the response from the conversation agent could not be parsed.\n        \"\"\"\n        logger.info(\"Defined tools: %s\", self.tools)\n        loaded_tools = ToolFactory(self.tools).build_tools(conversation_id, self.tool_context)\n\n        logger.info(\"Loaded tools: %s\", loaded_tools)\n        loaded_agent = self.active_agent_class(self.andromeda, loaded_tools)\n\n        final_answer = loaded_agent.run(input)\n        if isinstance(final_answer, dict):\n            final_answer = {'answer': str(final_answer), 'function': str(final_answer['fn'])}\n        else:\n            # Handle the case when final_answer is not a dictionary.\n            final_answer = {'answer': str(final_answer)}\n\n        return final_answer[\"answer\"]", ""]}
