{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\n# This is a shim to allow GitHub to detect the package, build is done with poetry\n# Taken from https://github.com/Textualize/rich\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup(name=\"boost-loss\")\n", ""]}
{"filename": "tests/test_sklearn.py", "chunked_list": ["from unittest import TestCase\n\nfrom catboost import CatBoostClassifier, CatboostError, CatBoostRegressor\nfrom parameterized import parameterized\nfrom sklearn.datasets import load_diabetes, load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nfrom boost_loss.sklearn import patch, patch_catboost\n", "from boost_loss.sklearn import patch, patch_catboost\n\n\nclass TestPatchCatboost(TestCase):\n    @parameterized.expand([\"RMSE\", \"RMSEWithUncertainty\"])\n    def test_patch_catboost_reg(self, objective: str) -> None:\n        estimator = patch_catboost(\n            CatBoostRegressor(iterations=100, objective=objective)\n        )\n        X, y = load_diabetes(return_X_y=True)\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n        estimator.fit(X_train, y_train)\n        y_pred = estimator.predict(X_test)\n        y_pred_var = estimator.predict_var(X_test)\n        self.assertEqual(y_test.shape, y_pred.shape)\n        self.assertEqual(y_test.shape, y_pred_var.shape)\n\n        self.assertFalse(hasattr(CatBoostRegressor(), \"predict_var\"))\n\n        # assert method properly created for each object\n        with self.assertRaises(CatboostError):\n            patch_catboost(\n                CatBoostRegressor(iterations=100, objective=objective)\n            ).predict_var(X_test)\n\n    def test_patch_catboost_clf(self) -> None:\n        estimator = patch_catboost(\n            CatBoostClassifier(iterations=100, loss_function=\"MultiClass\")\n        )\n        X, y = load_digits(return_X_y=True)\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n        estimator.fit(X_train, y_train)\n        y_pred = estimator.predict(X_test)\n        y_pred_var = estimator.predict_var(X_test)\n        self.assertEqual(y_test.shape, y_pred.shape)\n        self.assertEqual(y_test.shape, y_pred_var.shape)\n\n        self.assertFalse(hasattr(CatBoostClassifier(), \"predict_var\"))\n\n        with self.assertRaises(CatboostError):\n            patch_catboost(\n                CatBoostClassifier(iterations=100, loss_function=\"MultiClass\")\n            ).predict_var(X_test)\n\n    def test_patch(self) -> None:\n        estimator = patch(\n            Pipeline(\n                [\n                    (\n                        \"catboost\",\n                        CatBoostClassifier(iterations=100, loss_function=\"MultiClass\"),\n                    )\n                ]\n            )\n        )\n        X, y = load_digits(return_X_y=True)\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n        estimator.fit(X_train, y_train)\n        y_pred = estimator.predict(X_test)\n        y_pred_var = estimator[-1].predict_var(X_test)\n        self.assertEqual(y_test.shape, y_pred.shape)\n        self.assertEqual(y_test.shape, y_pred_var.shape)", "\n\ntry:\n    from ngboost import NGBRegressor\n\n    from boost_loss.sklearn import patch_ngboost\n\n    class TestPatchNGBoost(TestCase):\n        def setUp(self) -> None:\n            X, y = load_diabetes(return_X_y=True)\n            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n                X, y\n            )\n\n        def test_patch_ngboost(self) -> None:\n            estimator = patch_ngboost(NGBRegressor(n_estimators=100))\n            estimator.fit(self.X_train, self.y_train)\n            y_pred = estimator.predict(self.X_test)\n            y_pred_var = estimator.predict_var(self.X_test)\n            self.assertEqual(self.y_test.shape, y_pred.shape)\n            self.assertEqual(self.y_test.shape, y_pred_var.shape)\n\n            self.assertFalse(hasattr(NGBRegressor(), \"predict_var\"))\n\n            with self.assertRaises(BaseException):\n                patch_ngboost(NGBRegressor(n_estimators=100)).predict_var(self.X_test)\n\nexcept ImportError:\n    pass", ""]}
{"filename": "tests/test_torch.py", "chunked_list": ["import warnings\nfrom unittest import SkipTest, TestCase\n\nimport numpy as np\nfrom parameterized import parameterized_class\n\nfrom boost_loss.base import LossBase\nfrom boost_loss.regression.regression import L2Loss\n\ntry:\n    from boost_loss.torch import TorchLossBase, _LNLossTorch, _LNLossTorch_\nexcept ValueError as e:\n    raise SkipTest(f\"Failed to import TorchLossBase from PyTorch. Skip the test. {e}\")\nelse:\n    PARAMETERS = [\n        (_LNLossTorch_(n=2, divide_n_loss=False), L2Loss(divide_n_grad=False)),\n        (_LNLossTorch(n=2, divide_n_grad=True), L2Loss(divide_n_grad=True)),\n    ]", "\ntry:\n    from boost_loss.torch import TorchLossBase, _LNLossTorch, _LNLossTorch_\nexcept ValueError as e:\n    raise SkipTest(f\"Failed to import TorchLossBase from PyTorch. Skip the test. {e}\")\nelse:\n    PARAMETERS = [\n        (_LNLossTorch_(n=2, divide_n_loss=False), L2Loss(divide_n_grad=False)),\n        (_LNLossTorch(n=2, divide_n_grad=True), L2Loss(divide_n_grad=True)),\n    ]", "\nfrom .test_base import assert_array_almost_equal\n\ntry:\n    from torch.nn.modules.loss import MSELoss\n\n    PARAMETERS.append(\n        (TorchLossBase.from_callable_torch(MSELoss())(), L2Loss(divide_n_grad=False))\n    )\nexcept ValueError as e:\n    warnings.warn(f\"Failed to import MSELoss from PyTorch. Skip the test. {e}\")", "\n\n@parameterized_class((\"loss_torch\", \"loss_base\"), PARAMETERS)\nclass TestLossTorch(TestCase):\n    loss_torch: TorchLossBase\n    loss_base: LossBase\n\n    def setUp(self) -> None:\n        self.y_pred = np.random.randn(10)\n        self.y_true = np.random.randn(10)\n\n    def test_consistent(self) -> None:\n        loss_torch = self.loss_torch\n        loss_base = self.loss_base\n        assert_array_almost_equal(\n            np.mean(loss_torch.loss(self.y_true, self.y_pred)),\n            np.mean(loss_base.loss(self.y_true, self.y_pred)),\n        )\n        assert_array_almost_equal(\n            loss_torch.grad_hess(self.y_true, self.y_pred)[0],\n            loss_base.grad_hess(self.y_true, self.y_pred)[0],\n        )\n        assert_array_almost_equal(\n            loss_torch.grad_hess(self.y_true, self.y_pred)[1],\n            loss_base.grad_hess(self.y_true, self.y_pred)[1],\n        )", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_base.py", "chunked_list": ["from __future__ import annotations\n\nfrom functools import lru_cache\nfrom unittest import SkipTest, TestCase\n\nimport catboost as cb\nimport lightgbm as lgb\nimport numpy as np\nimport xgboost as xgb\nfrom numpy.testing import assert_allclose", "import xgboost as xgb\nfrom numpy.testing import assert_allclose\nfrom parameterized import parameterized_class\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom boost_loss.base import LossBase\nfrom boost_loss.regression.regression import L2Loss, LogCoshLoss\nfrom boost_loss.sklearn import apply_custom_loss", "from boost_loss.regression.regression import L2Loss, LogCoshLoss\nfrom boost_loss.sklearn import apply_custom_loss\n\n\ndef assert_array_almost_equal(a, b):\n    try:\n        assert_allclose(a, b, rtol=1e-2, atol=1e-7)\n    except Exception as e:\n        raise AssertionError(f\"{a[:3]}... != {b[:3]}...\") from e\n", "\n\nclass TestBase(TestCase):\n    loss: LossBase\n    loss_name: str\n    loss_names: dict[str, dict[str, str | None]] = {\n        \"catboost\": {\n            \"l2\": \"RMSE\",\n            \"l1\": \"MAE\",\n            \"poisson\": \"Poisson\",\n            \"logcosh\": \"LogCosh\",\n        },\n        \"lightgbm\": {\n            \"l2\": \"regression\",\n            \"l1\": \"regression_l1\",\n            \"poisson\": \"poisson\",\n            \"logcosh\": None,\n        },\n        \"xgboost\": {\n            \"l2\": \"reg:squarederror\",\n            \"l1\": \"reg:linear\",\n            \"poisson\": \"count:poisson\",\n            \"logcosh\": None,\n        },\n    }\n\n    def setUp(self) -> None:\n        self.X, self.y = load_diabetes(return_X_y=True)\n        self.seed = 0\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, random_state=0\n        )\n        x_scaler, y_scaler = StandardScaler(), StandardScaler()\n        self.X_train = x_scaler.fit_transform(self.X_train)\n        self.X_test = x_scaler.transform(self.X_test)\n        self.y_train = y_scaler.fit_transform(self.y_train.reshape(-1, 1)).ravel()\n        self.y_test = y_scaler.transform(self.y_test.reshape(-1, 1)).ravel()\n\n    def tearDown(self) -> None:\n        if hasattr(self, \"y_pred\"):\n            score = self.loss.loss(self.y_test, self.y_pred)\n            print(f\"Score: {score}\")\n\n    @lru_cache\n    def catboost_baseline(self):\n        if self.loss_names[\"catboost\"][self.loss_name] is None:\n            raise SkipTest(f\"CatBoost does not support {self.loss_name} loss.\")\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss_names[\"catboost\"][self.loss_name],\n            eval_metric=self.loss_names[\"catboost\"][self.loss_name],\n            iterations=100,\n        )\n        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n        return model.predict(self.X_test)\n\n    @lru_cache\n    def lightgbm_baseline(self):\n        if self.loss_names[\"lightgbm\"][self.loss_name] is None:\n            raise SkipTest(f\"LightGBM does not support {self.loss_name} loss.\")\n        model = lgb.LGBMRegressor(\n            objective=self.loss_names[\"lightgbm\"][self.loss_name],\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        return model.predict(self.X_test)\n\n    @lru_cache\n    def xgboost_baseline(self):\n        if self.loss_names[\"xgboost\"][self.loss_name] is None:\n            raise SkipTest(f\"XGBoost does not support {self.loss_name} loss.\")\n        model = xgb.XGBRegressor(\n            objective=self.loss_names[\"xgboost\"][self.loss_name],\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        return model.predict(self.X_test)", "\n\n@parameterized_class(\n    (\"loss\", \"loss_name\"),\n    [\n        # (L1Loss(), \"l1\"),\n        (L2Loss(), \"l2\"),\n        (LogCoshLoss(), \"logcosh\"),\n    ],\n)\nclass TestBasic(TestBase):\n    def test_catboost_sklearn(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n        self.y_pred = model.predict(self.X_test)\n        y_pred_baseline = self.catboost_baseline()\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n\n    def test_catboost_native(self):\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        train_pool = cb.Pool(self.X_train, self.y_train)\n        test_pool = cb.Pool(self.X_test, self.y_test)\n        model.fit(train_pool, eval_set=test_pool)\n        self.y_pred = model.predict(test_pool)\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, self.catboost_baseline()),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, self.catboost_baseline())  # type: ignore\n\n    def test_catboost_sklearn_apply(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n        self.y_pred = model.predict(self.X_test)\n        y_pred_baseline = self.catboost_baseline()\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n\n    def test_lightgbm_sklearn(self):\n        model = lgb.LGBMRegressor(objective=self.loss)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_lightgbm_native(self):\n        train_set = lgb.Dataset(self.X_train, self.y_train)\n        test_set = lgb.Dataset(self.X_test, self.y_test)\n        booster = lgb.train(\n            {\"seed\": self.seed},\n            train_set=train_set,\n            valid_sets=[train_set, test_set],\n            fobj=self.loss,\n            feval=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = booster.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_lightgbm_sklearn_apply(self):\n        model = lgb.LGBMRegressor()\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_xgboost_sklearn(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor(\n            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_xgboost_native(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n        train_set = xgb.DMatrix(self.X_train, self.y_train)\n        test_set = xgb.DMatrix(self.X_test, self.y_test)\n        booster = xgb.train(\n            {\"seed\": self.seed},\n            train_set,\n            num_boost_round=100,  # it is fucking that default value is different\n            evals=[(train_set, \"train\"), (test_set, \"test\")],\n            obj=self.loss,\n            custom_metric=self.loss.eval_metric_xgb_native,\n        )\n        self.y_pred = booster.predict(test_set)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_xgboost_sklearn_apply(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor()\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_sklearn_sklearn(self):\n        raise SkipTest(\"Not implemented yet\")", "    ],\n)\nclass TestBasic(TestBase):\n    def test_catboost_sklearn(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n        self.y_pred = model.predict(self.X_test)\n        y_pred_baseline = self.catboost_baseline()\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n\n    def test_catboost_native(self):\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        train_pool = cb.Pool(self.X_train, self.y_train)\n        test_pool = cb.Pool(self.X_test, self.y_test)\n        model.fit(train_pool, eval_set=test_pool)\n        self.y_pred = model.predict(test_pool)\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, self.catboost_baseline()),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, self.catboost_baseline())  # type: ignore\n\n    def test_catboost_sklearn_apply(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n        self.y_pred = model.predict(self.X_test)\n        y_pred_baseline = self.catboost_baseline()\n        self.assertAlmostEqual(\n            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n            1,\n            places=0,\n        )\n        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n\n    def test_lightgbm_sklearn(self):\n        model = lgb.LGBMRegressor(objective=self.loss)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_lightgbm_native(self):\n        train_set = lgb.Dataset(self.X_train, self.y_train)\n        test_set = lgb.Dataset(self.X_test, self.y_test)\n        booster = lgb.train(\n            {\"seed\": self.seed},\n            train_set=train_set,\n            valid_sets=[train_set, test_set],\n            fobj=self.loss,\n            feval=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = booster.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_lightgbm_sklearn_apply(self):\n        model = lgb.LGBMRegressor()\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\n    def test_xgboost_sklearn(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor(\n            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_xgboost_native(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n        train_set = xgb.DMatrix(self.X_train, self.y_train)\n        test_set = xgb.DMatrix(self.X_test, self.y_test)\n        booster = xgb.train(\n            {\"seed\": self.seed},\n            train_set,\n            num_boost_round=100,  # it is fucking that default value is different\n            evals=[(train_set, \"train\"), (test_set, \"test\")],\n            obj=self.loss,\n            custom_metric=self.loss.eval_metric_xgb_native,\n        )\n        self.y_pred = booster.predict(test_set)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_xgboost_sklearn_apply(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor()\n        model = apply_custom_loss(model, self.loss, target_transformer=None)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\n    def test_sklearn_sklearn(self):\n        raise SkipTest(\"Not implemented yet\")", "\n\n@parameterized_class(\n    (\"loss\", \"loss_name\"),\n    [\n        (L2Loss(), \"l2\"),\n    ],\n)\nclass TestWeighted(TestBase):\n    def setUp(self) -> None:\n        super().setUp()\n        self.w_train = np.random.rand(len(self.y_train))\n        self.w_test = np.random.rand(len(self.y_test))\n\n    def test_catboost_sklearn(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            eval_set=(self.X_test, self.y_test),\n        )\n        # no api to pass sample_weight to eval_set\n        self.y_pred = model.predict(self.X_test)\n\n    def test_catboost_native(self):\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        train_pool = cb.Pool(self.X_train, self.y_train, weight=self.w_train)\n        test_pool = cb.Pool(self.X_test, self.y_test, weight=self.w_test)\n        model.fit(train_pool, eval_set=test_pool)\n        self.y_pred = model.predict(test_pool)\n\n    def test_lightgbm_sklearn(self):\n        model = lgb.LGBMRegressor(objective=self.loss)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n            eval_sample_weight=[self.w_train, self.w_test],\n        )\n        self.y_pred = model.predict(self.X_test)\n\n    def test_lightgbm_native(self):\n        train_set = lgb.Dataset(self.X_train, self.y_train, weight=self.w_train)\n        test_set = lgb.Dataset(self.X_test, self.y_test, weight=self.w_test)\n        booster = lgb.train(\n            {\"seed\": self.seed},\n            train_set=train_set,\n            valid_sets=[train_set, test_set],\n            fobj=self.loss,\n            feval=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = booster.predict(self.X_test)\n\n    def test_xgboost_sklearn(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor(\n            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            sample_weight_eval_set=[self.w_train, self.w_test],\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n\n    def test_xgboost_native(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n        train_set = xgb.DMatrix(self.X_train, self.y_train, weight=self.w_train)\n        test_set = xgb.DMatrix(self.X_test, self.y_test, weight=self.w_test)\n        booster = xgb.train(\n            {\"seed\": self.seed},\n            train_set,\n            num_boost_round=100,  # it is fucking that default value is different\n            evals=[(train_set, \"train\"), (test_set, \"test\")],\n            obj=self.loss,\n            custom_metric=self.loss.eval_metric_xgb_native,\n        )\n        self.y_pred = booster.predict(test_set)\n\n    def test_sklearn_sklearn(self):\n        raise SkipTest(\"Not implemented yet\")", "class TestWeighted(TestBase):\n    def setUp(self) -> None:\n        super().setUp()\n        self.w_train = np.random.rand(len(self.y_train))\n        self.w_test = np.random.rand(len(self.y_test))\n\n    def test_catboost_sklearn(self):\n        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            eval_set=(self.X_test, self.y_test),\n        )\n        # no api to pass sample_weight to eval_set\n        self.y_pred = model.predict(self.X_test)\n\n    def test_catboost_native(self):\n        model = cb.CatBoostRegressor(\n            loss_function=self.loss,\n            eval_metric=self.loss,\n            iterations=100,\n            learning_rate=0.1,\n        )\n        train_pool = cb.Pool(self.X_train, self.y_train, weight=self.w_train)\n        test_pool = cb.Pool(self.X_test, self.y_test, weight=self.w_test)\n        model.fit(train_pool, eval_set=test_pool)\n        self.y_pred = model.predict(test_pool)\n\n    def test_lightgbm_sklearn(self):\n        model = lgb.LGBMRegressor(objective=self.loss)\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n            eval_metric=self.loss.eval_metric_lgb,\n            eval_sample_weight=[self.w_train, self.w_test],\n        )\n        self.y_pred = model.predict(self.X_test)\n\n    def test_lightgbm_native(self):\n        train_set = lgb.Dataset(self.X_train, self.y_train, weight=self.w_train)\n        test_set = lgb.Dataset(self.X_test, self.y_test, weight=self.w_test)\n        booster = lgb.train(\n            {\"seed\": self.seed},\n            train_set=train_set,\n            valid_sets=[train_set, test_set],\n            fobj=self.loss,\n            feval=self.loss.eval_metric_lgb,\n        )\n        self.y_pred = booster.predict(self.X_test)\n\n    def test_xgboost_sklearn(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n        model = xgb.XGBRegressor(\n            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n        )\n        model.fit(\n            self.X_train,\n            self.y_train,\n            sample_weight=self.w_train,\n            sample_weight_eval_set=[self.w_train, self.w_test],\n            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n        )\n        self.y_pred = model.predict(self.X_test)\n\n    def test_xgboost_native(self):\n        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n        train_set = xgb.DMatrix(self.X_train, self.y_train, weight=self.w_train)\n        test_set = xgb.DMatrix(self.X_test, self.y_test, weight=self.w_test)\n        booster = xgb.train(\n            {\"seed\": self.seed},\n            train_set,\n            num_boost_round=100,  # it is fucking that default value is different\n            evals=[(train_set, \"train\"), (test_set, \"test\")],\n            obj=self.loss,\n            custom_metric=self.loss.eval_metric_xgb_native,\n        )\n        self.y_pred = booster.predict(test_set)\n\n    def test_sklearn_sklearn(self):\n        raise SkipTest(\"Not implemented yet\")", ""]}
{"filename": "tests/regression/test_regression.py", "chunked_list": ["from unittest import TestCase\n\nimport numpy as np\n\nfrom boost_loss.regression.regression import L1Loss, L2Loss\n\n\nclass TestRegression(TestCase):\n    def test_l2(self):\n        l2 = L2Loss()\n        y_pred = np.random.rand(10)\n        y_true = np.random.rand(10)\n        loss = l2.loss(y_true=y_true, y_pred=y_pred)\n        grad = l2.grad(y_true=y_true, y_pred=y_pred)\n        hess = l2.hess(y_true=y_true, y_pred=y_pred)\n        self.assertEqual(grad.tolist(), (y_pred - y_true).tolist())\n        self.assertEqual(hess.tolist(), np.ones_like(y_pred).tolist())\n        self.assertEqual(loss.tolist(), ((y_pred - y_true) ** 2).tolist())\n\n    def test_l1(self):\n        l1 = L1Loss()\n        y_pred = np.random.rand(10)\n        y_true = np.random.rand(10)\n        loss = l1.loss(y_true=y_true, y_pred=y_pred)\n        grad = l1.grad(y_true=y_true, y_pred=y_pred)\n        hess = l1.hess(y_true=y_true, y_pred=y_pred)\n        self.assertEqual(grad.tolist(), np.sign(y_pred - y_true).tolist())\n        # NOTE: not np.zeros_like because it does not work\n        self.assertEqual(hess.tolist(), np.ones_like(y_pred).tolist())\n        self.assertEqual(loss.tolist(), np.abs(y_pred - y_true).tolist())", ""]}
{"filename": "tests/regression/test_sklearn.py", "chunked_list": [""]}
{"filename": "tests/regression/test_asymmetric.py", "chunked_list": [""]}
{"filename": "tests/regression/__init__.py", "chunked_list": [""]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\nfrom pathlib import Path\nfrom typing import Any\n\nfrom sphinx.application import Sphinx\nfrom sphinx.ext import apidoc", "from sphinx.application import Sphinx\nfrom sphinx.ext import apidoc\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"Boost Loss\"\ncopyright = \"2023, 34j\"\nauthor = \"34j\"\nrelease = \"0.0.0\"", "author = \"34j\"\nrelease = \"0.0.0\"\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [", "# ones.\nextensions = [\n    \"myst_parser\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.viewcode\",\n]\nnapoleon_google_docstring = False\n\n# The suffix of source filenames.", "\n# The suffix of source filenames.\nsource_suffix = [\".rst\", \".md\"]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.", "# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns: list[str] = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#", "# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n", "\n\n# -- Automatically run sphinx-apidoc -----------------------------------------\n\n\ndef run_apidoc(_: Any) -> None:\n    docs_path = Path(__file__).parent\n    module_path = docs_path.parent / \"src\" / \"boost_loss\"\n\n    apidoc.main(\n        [\n            \"--force\",\n            \"--module-first\",\n            \"-o\",\n            docs_path.as_posix(),\n            module_path.as_posix(),\n        ]\n    )", "\n\ndef setup(app: Sphinx) -> None:\n    app.connect(\"builder-inited\", run_apidoc)\n"]}
{"filename": "src/boost_loss/base.py", "chunked_list": ["from __future__ import annotations\n\nimport warnings\nfrom abc import ABCMeta\nfrom logging import getLogger\nfrom numbers import Real\nfrom typing import Any, Callable, Sequence, final\n\nimport attrs\nimport humps", "import attrs\nimport humps\nimport lightgbm as lgb\nimport numpy as np\nimport xgboost as xgb\nfrom numpy.typing import NDArray\nfrom typing_extensions import Self\n\nLOG = getLogger(__name__)\n", "LOG = getLogger(__name__)\n\n\ndef _dataset_to_ndarray(\n    y: NDArray | lgb.Dataset | xgb.DMatrix,\n) -> tuple[NDArray, NDArray]:\n    if isinstance(y, lgb.Dataset):\n        y_ = y.get_label()\n        if y_ is None:\n            raise ValueError(\"y is None\")\n        weight = y.get_weight()\n        if weight is None:\n            weight = np.ones_like(y_)\n        return y_, weight\n    if isinstance(y, xgb.DMatrix):\n        y_ = y.get_label()\n        return y_, np.ones_like(y_)\n    return y, np.ones_like(y)", "\n\ndef _get_name_from_callable(obj: Callable[..., Any]) -> str:\n    if hasattr(obj, \"__name__\"):\n        return getattr(obj, \"__name__\")\n    if hasattr(obj, \"__class__\") and hasattr(getattr(obj, \"__class__\"), \"__name__\"):\n        return getattr(getattr(obj, \"__class__\"), \"__name__\")\n    raise ValueError(f\"Could not get name from callable {obj}\")\n\n\nclass LossBase(metaclass=ABCMeta):\n    \"\"\"Base class for loss functions.\n    Inherit this class to implement custom loss function.\n\n    See Also\n    --------\n    Catboost:\n    https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\n    LightGBM:\n    https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#custom-objective-function\n\n    XGBoost:\n    https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html\n\n    Example\n    -------\n    >>> from boost_loss.base import LossBase\n    >>> import numpy as np\n    >>> from numpy.typing import NDArray\n    >>>\n    >>> class L2Loss(LossBase):\n    >>>     def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n    >>>         return (y_true - y_pred) ** 2\n    >>>     def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # dL/dy_pred\n    >>>         return -2 * (y_true - y_pred) # or (y_pred - y_true)\n    >>>     def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # d2L/dy_pred2\n    >>>         return 2 * np.ones_like(y_true) # or np.ones_like(y_true)\n    >>>\n    >>> from boost.sklearn import apply_custom_loss\n    >>> import lightgbm as lgb\n    >>> apply_custom_loss(lgb.LGBMRegressor(), L2Loss()).fit(X, y)\n    \"\"\"\n\n    is_higher_better: bool = False\n    \"\"\"Whether the result of loss function is better when it is higher.\"\"\"\n\n    @classmethod\n    @final\n    def from_callable(\n        cls,\n        loss: Callable[[NDArray, NDArray], NDArray | float],\n        grad: Callable[[NDArray, NDArray], NDArray],\n        hess: Callable[[NDArray, NDArray], NDArray],\n        name: str | None = None,\n        is_higher_better: bool = False,\n    ) -> type[Self]:\n        \"\"\"Create this class from loss, grad, and hess callables.\n\n        Parameters\n        ----------\n        loss : Callable[[NDArray, NDArray], NDArray | float]\n            The loss function. If 1-D array is returned,\n            the mean of array is calculated.\n            Return 1-D array if possible in order to utilize weights in the dataset\n            if available.\n            (y_true, y_pred) -> loss\n        grad : Callable[[NDArray, NDArray], NDArray]\n            The 1st order derivative (gradient) of loss w.r.t. y_pred.\n            (y_true, y_pred) -> grad\n        hess : Callable[[NDArray, NDArray], NDArray]\n            The 2nd order derivative (Hessian) of loss w.r.t. y_pred.\n            (y_true, y_pred) -> hess\n        name : str | None, optional\n            The name of loss function.\n            If None, it tries to infer from loss function, by default None\n        is_higher_better : bool, optional\n            Whether the result of loss function is better when it is higher,\n            by default False\n\n        Returns\n        -------\n        type[Self]\n            The subclass of this class.\n\n        Raises\n        ------\n        ValueError\n            If name is None and it can't infer from loss function.\n        \"\"\"\n        if name is None:\n            try:\n                name = _get_name_from_callable(loss)\n            except ValueError as e:\n                raise ValueError(\n                    \"Could not infer name from loss function. Please specify name.\"\n                ) from e\n        return type(\n            name,\n            (cls,),\n            dict(\n                loss=staticmethod(loss),\n                grad=staticmethod(grad),\n                hess=staticmethod(hess),\n                is_higher_better=is_higher_better,\n            ),\n        )\n\n    @property\n    def _grad_hess_sign(self) -> int:\n        return -1 if self.is_higher_better else 1\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        grad_inherited = cls.grad is not LossBase.grad\n        hess_inherited = cls.hess is not LossBase.hess\n        grad_hess_inherited = cls.grad_hess is not LossBase.grad_hess\n        if grad_inherited and hess_inherited:\n            pass\n        elif grad_hess_inherited:\n            pass\n        else:\n            raise TypeError(\n                f\"Can't instantiate abstract class {cls.__name__} \"\n                \"with grad_hess or both grad and hess not implemented\"\n            )\n        super().__init_subclass__(**kwargs)\n\n    @property\n    def name(self) -> str:\n        \"\"\"Name of loss function.\n\n        Returns\n        -------\n        str\n            Snake case of class name. e.g. `LogCoshLoss` -> `log_cosh_loss`.\n        \"\"\"\n        return humps.decamelize(self.__class__.__name__.replace(\"Loss\", \"\"))\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        \"\"\"The 1st order derivative (gradient) of loss w.r.t. y_pred.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray\n            The gradient of loss function. 1-D array with shape (n_samples,).\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        \"\"\"The 2nd order derivative (hessian) of loss w.r.t. y_pred.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray\n            The hessian of loss function. 1-D array with shape (n_samples,).\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        \"\"\"Loss function. If 1-D array is returned, the mean of array is calculated.\n        Return 1-D array if possible in order to utilize weights in the dataset\n        if available.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray | float\n            The loss function. 1-D array with shape (n_samples,) or float.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        \"\"\"Gradient and hessian of loss function. Override this method if you want to\n        calculate both gradient and hessian at the same time.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        tuple[NDArray, NDArray]\n            The gradient and hessian of loss function.\n            1-D array with shape (n_samples,).\n        \"\"\"\n        return self.grad(y_true=y_true, y_pred=y_pred), self.hess(\n            y_true=y_true, y_pred=y_pred\n        )\n\n    def _grad_hess_weighted(\n        self, y_true: NDArray, y_pred: NDArray, weight: NDArray\n    ) -> tuple[NDArray, NDArray]:\n        grad, hess = self.grad_hess(y_true=y_true, y_pred=y_pred)\n        if np.any(hess < 0):\n            negative_rate = np.mean(hess < 0)\n            warnings.warn(\n                f\"Found negative hessian in {negative_rate:.2%} samples.\"\n                \"This may cause convergence issue and cause CatBoostError in CatBoost.\"\n                \"If LightGBM or XGBoost is used, the estimator will return \"\n                \"nonsense values (like all 0s if 100%).\",\n                RuntimeWarning,\n            )\n        grad, hess = grad * weight, hess * weight\n        grad, hess = grad * self._grad_hess_sign, hess * self._grad_hess_sign\n        return grad, hess\n\n    @final\n    def __call__(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[NDArray, NDArray]:\n        \"\"\"Sklearn-compatible interface (Sklearn, LightGBM, XGBoost)\"\"\"\n        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n            # NOTE: swap (it is so fucking that the order is inconsistent)\n            y_true, y_pred = y_pred, y_true\n        y_true, weight = _dataset_to_ndarray(y=y_true)\n        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n        return self._grad_hess_weighted(y_true=y_true, y_pred=y_pred, weight=weight)\n\n    @final\n    def calc_ders_range(\n        self,\n        preds: Sequence[float],\n        targets: Sequence[float],\n        weights: Sequence[float] | None = None,\n    ) -> list[tuple[float, float]]:\n        \"\"\"Catboost-compatible interface\"\"\"\n        y_pred = np.array(preds)\n        y_true = np.array(targets)\n        weight = np.array(weights) if weights is not None else np.ones_like(y_pred)\n        grad, hess = self._grad_hess_weighted(\n            y_true=y_true, y_pred=y_pred, weight=weight\n        )\n        # NOTE: in catboost, the definition of loss is the inverse\n        grad, hess = -grad, -hess\n        return list(zip(grad, hess))\n\n    @final\n    def is_max_optimal(self) -> bool:\n        \"\"\"Catboost-compatible interface\"\"\"\n        return self.is_higher_better\n\n    @final\n    def evaluate(\n        self,\n        approxes: Sequence[float],\n        target: Sequence[float],\n        weight: Sequence[float] | None = None,\n    ) -> tuple[float, float]:\n        \"\"\"Catboost-compatible interface\"\"\"\n        approxes_ = np.array(approxes[0])\n        targets_ = np.array(target)\n        weights_ = np.array(weight) if weight is not None else np.ones_like(approxes_)\n        loss = self.loss(y_true=targets_, y_pred=approxes_)\n        if isinstance(loss, float) and not np.allclose(weights_, 1.0):\n            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n            return loss, np.nan\n        return float(np.sum(loss * weights_)), float(np.sum(weights_))\n\n    @final\n    def get_final_error(self, error: float, weight: float | None = None) -> float:\n        \"\"\"Catboost-compatible interface\"\"\"\n        return error / (weight + 1e-38) if weight is not None else error\n\n    @final\n    def eval_metric_lgb(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[str, float, bool]:\n        \"\"\"LightGBM-compatible interface\"\"\"\n        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n            # NOTE: swap (it is so fucking that the order is inconsistent)\n            y_true, y_pred = y_pred, y_true\n        y_true, weight = _dataset_to_ndarray(y=y_true)\n        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n        loss = self.loss(y_true=y_true, y_pred=y_pred)\n        if isinstance(loss, float) and not np.allclose(weight, 1.0):\n            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n            return self.name, loss, self.is_higher_better\n        return (\n            self.name,\n            float(np.sum(loss * weight) / (np.sum(weight) + 1e-38)),\n            self.is_higher_better,\n        )\n\n    @final\n    def eval_metric_xgb_native(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[str, float]:\n        \"\"\"XGBoost-native-api-compatible interface\"\"\"\n        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n        return result[0], result[1]\n\n    @final\n    def eval_metric_xgb_sklearn(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> float:\n        \"\"\"XGBoost-sklearn-api-compatible interface\"\"\"\n        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n        return result[1]\n\n    def __add__(self, other: LossBase) -> LossBase:\n        if not isinstance(other, LossBase):\n            return NotImplemented  # type: ignore\n        return _LossSum(self, other)\n\n    def __sub__(self, other: LossBase) -> LossBase:\n        return self.__add__(-other)\n\n    def __mul__(self, other: float | int | Real) -> LossBase:\n        if not isinstance(other, Real):\n            return NotImplemented\n        return _LossMul(self, other)\n\n    def __div__(self, other: float | int | Real) -> LossBase:\n        return self.__mul__(1.0 / other)\n\n    def __radd__(self, other: LossBase) -> LossBase:\n        return self.__add__(other)\n\n    def __rsub__(self, other: LossBase) -> LossBase:\n        return self.__sub__(other)\n\n    def __rmul__(self, other: float | int | Real) -> LossBase:\n        return self.__mul__(other)\n\n    def __rdiv__(self, other: float | int | Real) -> LossBase:\n        return self.__div__(other)\n\n    def __neg__(self) -> LossBase:\n        return self.__mul__(-1.0)\n\n    def __pos__(self) -> Self:\n        return self", "\n\nclass LossBase(metaclass=ABCMeta):\n    \"\"\"Base class for loss functions.\n    Inherit this class to implement custom loss function.\n\n    See Also\n    --------\n    Catboost:\n    https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\n    LightGBM:\n    https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#custom-objective-function\n\n    XGBoost:\n    https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html\n\n    Example\n    -------\n    >>> from boost_loss.base import LossBase\n    >>> import numpy as np\n    >>> from numpy.typing import NDArray\n    >>>\n    >>> class L2Loss(LossBase):\n    >>>     def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n    >>>         return (y_true - y_pred) ** 2\n    >>>     def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # dL/dy_pred\n    >>>         return -2 * (y_true - y_pred) # or (y_pred - y_true)\n    >>>     def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # d2L/dy_pred2\n    >>>         return 2 * np.ones_like(y_true) # or np.ones_like(y_true)\n    >>>\n    >>> from boost.sklearn import apply_custom_loss\n    >>> import lightgbm as lgb\n    >>> apply_custom_loss(lgb.LGBMRegressor(), L2Loss()).fit(X, y)\n    \"\"\"\n\n    is_higher_better: bool = False\n    \"\"\"Whether the result of loss function is better when it is higher.\"\"\"\n\n    @classmethod\n    @final\n    def from_callable(\n        cls,\n        loss: Callable[[NDArray, NDArray], NDArray | float],\n        grad: Callable[[NDArray, NDArray], NDArray],\n        hess: Callable[[NDArray, NDArray], NDArray],\n        name: str | None = None,\n        is_higher_better: bool = False,\n    ) -> type[Self]:\n        \"\"\"Create this class from loss, grad, and hess callables.\n\n        Parameters\n        ----------\n        loss : Callable[[NDArray, NDArray], NDArray | float]\n            The loss function. If 1-D array is returned,\n            the mean of array is calculated.\n            Return 1-D array if possible in order to utilize weights in the dataset\n            if available.\n            (y_true, y_pred) -> loss\n        grad : Callable[[NDArray, NDArray], NDArray]\n            The 1st order derivative (gradient) of loss w.r.t. y_pred.\n            (y_true, y_pred) -> grad\n        hess : Callable[[NDArray, NDArray], NDArray]\n            The 2nd order derivative (Hessian) of loss w.r.t. y_pred.\n            (y_true, y_pred) -> hess\n        name : str | None, optional\n            The name of loss function.\n            If None, it tries to infer from loss function, by default None\n        is_higher_better : bool, optional\n            Whether the result of loss function is better when it is higher,\n            by default False\n\n        Returns\n        -------\n        type[Self]\n            The subclass of this class.\n\n        Raises\n        ------\n        ValueError\n            If name is None and it can't infer from loss function.\n        \"\"\"\n        if name is None:\n            try:\n                name = _get_name_from_callable(loss)\n            except ValueError as e:\n                raise ValueError(\n                    \"Could not infer name from loss function. Please specify name.\"\n                ) from e\n        return type(\n            name,\n            (cls,),\n            dict(\n                loss=staticmethod(loss),\n                grad=staticmethod(grad),\n                hess=staticmethod(hess),\n                is_higher_better=is_higher_better,\n            ),\n        )\n\n    @property\n    def _grad_hess_sign(self) -> int:\n        return -1 if self.is_higher_better else 1\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        grad_inherited = cls.grad is not LossBase.grad\n        hess_inherited = cls.hess is not LossBase.hess\n        grad_hess_inherited = cls.grad_hess is not LossBase.grad_hess\n        if grad_inherited and hess_inherited:\n            pass\n        elif grad_hess_inherited:\n            pass\n        else:\n            raise TypeError(\n                f\"Can't instantiate abstract class {cls.__name__} \"\n                \"with grad_hess or both grad and hess not implemented\"\n            )\n        super().__init_subclass__(**kwargs)\n\n    @property\n    def name(self) -> str:\n        \"\"\"Name of loss function.\n\n        Returns\n        -------\n        str\n            Snake case of class name. e.g. `LogCoshLoss` -> `log_cosh_loss`.\n        \"\"\"\n        return humps.decamelize(self.__class__.__name__.replace(\"Loss\", \"\"))\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        \"\"\"The 1st order derivative (gradient) of loss w.r.t. y_pred.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray\n            The gradient of loss function. 1-D array with shape (n_samples,).\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        \"\"\"The 2nd order derivative (hessian) of loss w.r.t. y_pred.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray\n            The hessian of loss function. 1-D array with shape (n_samples,).\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        \"\"\"Loss function. If 1-D array is returned, the mean of array is calculated.\n        Return 1-D array if possible in order to utilize weights in the dataset\n        if available.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        NDArray | float\n            The loss function. 1-D array with shape (n_samples,) or float.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        \"\"\"Gradient and hessian of loss function. Override this method if you want to\n        calculate both gradient and hessian at the same time.\n\n        Parameters\n        ----------\n        y_true : NDArray\n            The true target values.\n        y_pred : NDArray\n            The predicted target values.\n\n        Returns\n        -------\n        tuple[NDArray, NDArray]\n            The gradient and hessian of loss function.\n            1-D array with shape (n_samples,).\n        \"\"\"\n        return self.grad(y_true=y_true, y_pred=y_pred), self.hess(\n            y_true=y_true, y_pred=y_pred\n        )\n\n    def _grad_hess_weighted(\n        self, y_true: NDArray, y_pred: NDArray, weight: NDArray\n    ) -> tuple[NDArray, NDArray]:\n        grad, hess = self.grad_hess(y_true=y_true, y_pred=y_pred)\n        if np.any(hess < 0):\n            negative_rate = np.mean(hess < 0)\n            warnings.warn(\n                f\"Found negative hessian in {negative_rate:.2%} samples.\"\n                \"This may cause convergence issue and cause CatBoostError in CatBoost.\"\n                \"If LightGBM or XGBoost is used, the estimator will return \"\n                \"nonsense values (like all 0s if 100%).\",\n                RuntimeWarning,\n            )\n        grad, hess = grad * weight, hess * weight\n        grad, hess = grad * self._grad_hess_sign, hess * self._grad_hess_sign\n        return grad, hess\n\n    @final\n    def __call__(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[NDArray, NDArray]:\n        \"\"\"Sklearn-compatible interface (Sklearn, LightGBM, XGBoost)\"\"\"\n        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n            # NOTE: swap (it is so fucking that the order is inconsistent)\n            y_true, y_pred = y_pred, y_true\n        y_true, weight = _dataset_to_ndarray(y=y_true)\n        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n        return self._grad_hess_weighted(y_true=y_true, y_pred=y_pred, weight=weight)\n\n    @final\n    def calc_ders_range(\n        self,\n        preds: Sequence[float],\n        targets: Sequence[float],\n        weights: Sequence[float] | None = None,\n    ) -> list[tuple[float, float]]:\n        \"\"\"Catboost-compatible interface\"\"\"\n        y_pred = np.array(preds)\n        y_true = np.array(targets)\n        weight = np.array(weights) if weights is not None else np.ones_like(y_pred)\n        grad, hess = self._grad_hess_weighted(\n            y_true=y_true, y_pred=y_pred, weight=weight\n        )\n        # NOTE: in catboost, the definition of loss is the inverse\n        grad, hess = -grad, -hess\n        return list(zip(grad, hess))\n\n    @final\n    def is_max_optimal(self) -> bool:\n        \"\"\"Catboost-compatible interface\"\"\"\n        return self.is_higher_better\n\n    @final\n    def evaluate(\n        self,\n        approxes: Sequence[float],\n        target: Sequence[float],\n        weight: Sequence[float] | None = None,\n    ) -> tuple[float, float]:\n        \"\"\"Catboost-compatible interface\"\"\"\n        approxes_ = np.array(approxes[0])\n        targets_ = np.array(target)\n        weights_ = np.array(weight) if weight is not None else np.ones_like(approxes_)\n        loss = self.loss(y_true=targets_, y_pred=approxes_)\n        if isinstance(loss, float) and not np.allclose(weights_, 1.0):\n            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n            return loss, np.nan\n        return float(np.sum(loss * weights_)), float(np.sum(weights_))\n\n    @final\n    def get_final_error(self, error: float, weight: float | None = None) -> float:\n        \"\"\"Catboost-compatible interface\"\"\"\n        return error / (weight + 1e-38) if weight is not None else error\n\n    @final\n    def eval_metric_lgb(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[str, float, bool]:\n        \"\"\"LightGBM-compatible interface\"\"\"\n        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n            # NOTE: swap (it is so fucking that the order is inconsistent)\n            y_true, y_pred = y_pred, y_true\n        y_true, weight = _dataset_to_ndarray(y=y_true)\n        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n        loss = self.loss(y_true=y_true, y_pred=y_pred)\n        if isinstance(loss, float) and not np.allclose(weight, 1.0):\n            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n            return self.name, loss, self.is_higher_better\n        return (\n            self.name,\n            float(np.sum(loss * weight) / (np.sum(weight) + 1e-38)),\n            self.is_higher_better,\n        )\n\n    @final\n    def eval_metric_xgb_native(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> tuple[str, float]:\n        \"\"\"XGBoost-native-api-compatible interface\"\"\"\n        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n        return result[0], result[1]\n\n    @final\n    def eval_metric_xgb_sklearn(\n        self,\n        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n    ) -> float:\n        \"\"\"XGBoost-sklearn-api-compatible interface\"\"\"\n        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n        return result[1]\n\n    def __add__(self, other: LossBase) -> LossBase:\n        if not isinstance(other, LossBase):\n            return NotImplemented  # type: ignore\n        return _LossSum(self, other)\n\n    def __sub__(self, other: LossBase) -> LossBase:\n        return self.__add__(-other)\n\n    def __mul__(self, other: float | int | Real) -> LossBase:\n        if not isinstance(other, Real):\n            return NotImplemented\n        return _LossMul(self, other)\n\n    def __div__(self, other: float | int | Real) -> LossBase:\n        return self.__mul__(1.0 / other)\n\n    def __radd__(self, other: LossBase) -> LossBase:\n        return self.__add__(other)\n\n    def __rsub__(self, other: LossBase) -> LossBase:\n        return self.__sub__(other)\n\n    def __rmul__(self, other: float | int | Real) -> LossBase:\n        return self.__mul__(other)\n\n    def __rdiv__(self, other: float | int | Real) -> LossBase:\n        return self.__div__(other)\n\n    def __neg__(self) -> LossBase:\n        return self.__mul__(-1.0)\n\n    def __pos__(self) -> Self:\n        return self", "\n\n@attrs.define()\nclass _LossSum(LossBase):\n    loss1: LossBase\n    loss2: LossBase\n\n    @property\n    def name(self) -> str:\n        return self.loss1.name + \"+\" + self.loss2.name\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        return self.loss1.loss(y_true=y_true, y_pred=y_pred) + self.loss2.loss(\n            y_true=y_true, y_pred=y_pred\n        )\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return self.loss1.grad(y_true=y_true, y_pred=y_pred) + self.loss2.grad(\n            y_true=y_true, y_pred=y_pred\n        )\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return self.loss1.hess(y_true=y_true, y_pred=y_pred) + self.loss2.hess(\n            y_true=y_true, y_pred=y_pred\n        )\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        grad1, hess1 = self.loss1.grad_hess(y_true=y_true, y_pred=y_pred)\n        grad2, hess2 = self.loss2.grad_hess(y_true=y_true, y_pred=y_pred)\n        return grad1 + grad2, hess1 + hess2", "\n\n@attrs.define()\nclass _LossMul(LossBase):\n    loss_: LossBase\n    factor: float | int | Real\n\n    @property\n    def name(self) -> str:\n        return f\"{self.factor}*{self.loss_.name}\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        return self.factor * self.loss_.loss(y_true=y_true, y_pred=y_pred)\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return self.factor * self.loss_.grad(y_true=y_true, y_pred=y_pred)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return self.factor * self.loss_.hess(y_true=y_true, y_pred=y_pred)\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n        return self.factor * grad, self.factor * hess", ""]}
{"filename": "src/boost_loss/torch.py", "chunked_list": ["from __future__ import annotations\n\nfrom abc import ABCMeta\nfrom typing import Any, Callable, final\n\nimport attrs\nimport numpy as np\nimport torch\nfrom numpy.typing import NDArray\nfrom typing_extensions import Self", "from numpy.typing import NDArray\nfrom typing_extensions import Self\n\nfrom .base import LossBase, _get_name_from_callable\n\n\nclass TorchLossBase(LossBase, metaclass=ABCMeta):\n    \"\"\"Calculate gradient and hessian using `torch.autograd.grad`.\n    One of `loss_torch` and `grad_torch` must be implemented.\n\n    Inspired by\n    https://github.com/TomerRonen34/treeboost_autograd/blob/main/treeboost_autograd/pytorch_objective.py # noqa: E501\n    \"\"\"\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        loss_inherited = cls.loss_torch is not TorchLossBase.loss_torch\n        grad_inherited = cls.grad_torch is not TorchLossBase.grad_torch\n        if loss_inherited or grad_inherited:\n            pass\n        else:\n            raise TypeError(\n                f\"Can't instantiate abstract class {cls.__name__} \"\n                \"with loss_torch or grad_torch not implemented\"\n            )\n        return super().__init_subclass__(**kwargs)\n\n    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        \"\"\"The loss function.  If 1-D array is returned,\n        the mean of array is calculated.\n        Return 1-D array if possible in order to utilize weights in the dataset\n        if available.\n\n        Parameters\n        ----------\n        y_true : torch.Tensor\n            The true target values.\n        y_pred : torch.Tensor\n            The predicted target values.\n\n        Returns\n        -------\n        torch.Tensor\n            0-dim or 1-dim tensor of shape (n_samples,). Return 1-dim tensor if possible\n            to utilize weights in the dataset if available.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    def grad_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        \"\"\"The 1st order derivative of loss w.r.t. y_pred.\n\n        Parameters\n        ----------\n        y_true : torch.Tensor\n            The true target values.\n        y_pred : torch.Tensor\n            The predicted target values.\n\n        Returns\n        -------\n        torch.Tensor\n            The gradient of loss w.r.t. y_pred. 1-dim tensor of shape (n_samples,).\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented.\n        \"\"\"\n        raise NotImplementedError()\n\n    @final\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        loss = self.loss_torch(\n            torch.from_numpy(y_true), torch.from_numpy(y_pred)\n        ).detach()\n        if loss.ndim == 0:\n            return loss.item()\n        else:\n            return loss.numpy()\n\n    @final\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        y_true_ = torch.from_numpy(y_true)\n        y_pred_ = torch.from_numpy(y_pred).requires_grad_(True)\n\n        try:\n            grad = self.grad_torch(y_true_, y_pred_)\n        except NotImplementedError:\n            loss = torch.mean(self.loss_torch(y_true_, y_pred_)) * y_true_.size(0)\n            grad = torch.autograd.grad(loss, y_pred_, create_graph=True)[0]\n            hess = np.array(\n                [\n                    torch.autograd.grad(grad_, y_pred_, retain_graph=True)[0][i].item()\n                    for i, grad_ in enumerate(grad)\n                ]\n            )\n        else:\n            hess = np.array(\n                [\n                    torch.autograd.grad(grad_, y_pred_, create_graph=True)[0][i].item()\n                    for i, grad_ in enumerate(grad)\n                ]\n            )\n        return grad.detach().numpy(), hess\n\n    @classmethod\n    @final\n    def from_callable_torch(\n        cls,\n        loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n        name: str | None = None,\n        is_higher_better: bool = False,\n    ) -> type[Self]:\n        \"\"\"Create a loss class from a callable.\n\n        Parameters\n        ----------\n        loss : Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n            The loss function.  If 1-D array is returned,\n            the mean of array is calculated.\n            Return 1-D array if possible in order to utilize weights in the dataset\n            if available.\n            (y_true, y_pred) -> loss\n        name : str | None, optional\n            The name of loss function.\n            If None, it tries to infer from loss function, by default None\n        is_higher_better : bool, optional\n            Whether the result of loss function is better when it is higher,\n            by default False\n\n        Returns\n        -------\n        type[Self]\n            The subclass of this class.\n\n        Raises\n        ------\n        ValueError\n            If name is None and it can't infer from loss function.\n        \"\"\"\n        if name is None:\n            try:\n                name = _get_name_from_callable(loss)\n            except ValueError as e:\n                raise ValueError(\n                    \"Could not infer name from loss function. Please specify name.\"\n                ) from e\n        return type(\n            name,\n            (cls,),\n            dict(loss_torch=staticmethod(loss), is_higher_better=is_higher_better),\n        )", "\n\n@attrs.define(kw_only=True)\nclass _LNLossTorch_(TorchLossBase):\n    \"\"\"L^n loss for PyTorch.\"\"\"\n\n    n: float\n    divide_n_loss: bool = False\n    divide_n_grad: bool = False\n\n    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        if self.divide_n_grad != self.divide_n_loss:\n            raise ValueError(\n                \"divide_n_grad and divide_n_loss must be the same, \"\n                f\"but got {self.divide_n_grad} and {self.divide_n_loss}\"\n            )\n        return torch.abs(y_true - y_pred) ** self.n / (\n            self.n if self.divide_n_loss else 1\n        )", "\n\n@attrs.define(kw_only=True)\nclass _LNLossTorch(TorchLossBase):\n    \"\"\"L^n loss for PyTorch.\"\"\"\n\n    n: float\n    divide_n_loss: bool = False\n    divide_n_grad: bool = True\n\n    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        return torch.abs(y_pred - y_true) ** self.n / (\n            self.n if self.divide_n_loss else 1\n        )\n\n    def grad_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        return (\n            torch.sign(y_pred - y_true)\n            * torch.abs(y_pred - y_true) ** (self.n - 1)\n            * self.n\n            / (self.n if self.divide_n_grad else 1)\n        )", ""]}
{"filename": "src/boost_loss/__init__.py", "chunked_list": ["__version__ = \"0.2.0\"\nfrom .base import LossBase\nfrom .debug import DebugLoss, PrintLoss\nfrom .resuming import ResumingLoss\nfrom .sklearn import apply_custom_loss, patch_catboost\n\ntry:\n    from .sklearn import patch_ngboost\nexcept ImportError:\n    pass", "__all__ = [\n    \"LossBase\",\n    \"DebugLoss\",\n    \"PrintLoss\",\n    \"ResumingLoss\",\n    \"apply_custom_loss\",\n    \"patch_catboost\",\n    \"patch_ngboost\",\n]\n", "]\n"]}
{"filename": "src/boost_loss/sklearn.py", "chunked_list": ["from __future__ import annotations\n\nimport functools\nimport importlib.util\nfrom typing import Any, Literal, TypeVar, overload\n\nimport catboost as cb\nimport lightgbm as lgb\nimport numpy as np\nimport xgboost as xgb", "import numpy as np\nimport xgboost as xgb\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nfrom .base import LossBase\n\nTEstimator = TypeVar(\"TEstimator\", cb.CatBoost, lgb.LGBMModel, xgb.XGBModel)\n", "TEstimator = TypeVar(\"TEstimator\", cb.CatBoost, lgb.LGBMModel, xgb.XGBModel)\n\n\n@overload\ndef apply_custom_loss(\n    estimator: TEstimator,\n    loss: LossBase,\n    *,\n    copy: bool = ...,\n    target_transformer: None = ...,\n    recursive: bool = ...,\n) -> TEstimator:\n    ...", "\n\n@overload\ndef apply_custom_loss(\n    estimator: TEstimator,\n    loss: LossBase,\n    *,\n    copy: bool = ...,\n    target_transformer: BaseEstimator = ...,\n    recursive: bool = ...,\n) -> TransformedTargetRegressor:\n    ...", "\n\ndef apply_custom_loss(\n    estimator: TEstimator,\n    loss: LossBase,\n    *,\n    copy: bool = True,\n    target_transformer: BaseEstimator | Any | None = StandardScaler(),\n    recursive: bool = True,\n) -> TEstimator | TransformedTargetRegressor:\n    \"\"\"Apply custom loss to the estimator.\n\n    Parameters\n    ----------\n    estimator : TEstimator\n        CatBoost, LGBMModel, or XGBModel\n    loss : LossBase\n        The custom loss to apply\n    copy : bool, optional\n        Whether to copy the estimator using `sklearn.base.clone`, by default True\n    target_transformer : BaseEstimator | Any | None, optional\n        The target transformer to use, by default StandardScaler()\n        (This option exists because some loss functions require the target\n        to be normalized (i.e. `LogCoshLoss`))\n    recursive : bool, optional\n        Whether to recursively search for estimators inside the estimator\n        and apply the custom loss to all of them, by default True\n\n    Returns\n    -------\n    TEstimator | TransformedTargetRegressor\n        The estimator with the custom loss applied\n    \"\"\"\n    if copy:\n        estimator = clone(estimator)\n    if isinstance(estimator, cb.CatBoost):\n        estimator.set_params(loss_function=loss, eval_metric=loss)\n    if isinstance(estimator, lgb.LGBMModel):\n        estimator.set_params(objective=loss)\n        estimator_fit = estimator.fit\n\n        @functools.wraps(estimator_fit)\n        def fit(X: Any, y: Any, **fit_params: Any) -> Any:\n            fit_params[\"eval_metric\"] = loss.eval_metric_lgb\n            return estimator_fit(X, y, **fit_params)\n\n        setattr(estimator, \"fit\", fit)\n    if isinstance(estimator, xgb.XGBModel):\n        estimator.set_params(objective=loss, eval_metric=loss.eval_metric_xgb_sklearn)\n\n    if recursive:\n        for key, value in estimator.get_params(deep=True).items():\n            if hasattr(value, \"fit\"):\n                estimator.set_params(\n                    **{\n                        key: apply_custom_loss(\n                            value, loss, copy=False, target_transformer=None\n                        )\n                    }\n                )\n\n    if target_transformer is None:\n        return estimator\n    return TransformedTargetRegressor(estimator, transformer=clone(target_transformer))", "\n\nif importlib.util.find_spec(\"ngboost\") is not None:\n    from ngboost import NGBoost\n    from ngboost.distns import Normal\n    from numpy.typing import NDArray\n\n    def patch_ngboost(estimator: NGBoost) -> NGBoost:\n        \"\"\"Patch NGBoost to return only the mean prediction in `predict`\n        and the variance in `predict_var` to be consistent with other models.\n        The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n        and requires re-patching.\n\n        Parameters\n        ----------\n        estimator : NGBoost\n            The NGBoost estimator to patch.\n\n        Returns\n        -------\n        NGBoost\n            The patched NGBoost estimator.\n        \"\"\"\n\n        self = estimator\n\n        def predict_var(X: Any, **predict_params: Any) -> NDArray[Any]:\n            dist = self.pred_dist(X, **predict_params)\n            if not isinstance(dist, Normal):\n                raise NotImplementedError\n            return dist.var\n\n        setattr(estimator, \"predict_var\", predict_var)\n\n        def predict_std(X: Any, **predict_params: Any) -> NDArray[Any]:\n            dist = self.pred_dist(X, **predict_params)\n            if not isinstance(dist, Normal):\n                raise NotImplementedError\n            return dist.scale\n\n        setattr(estimator, \"predict_std\", predict_std)\n        return estimator", "\n\ndef patch_catboost(estimator: cb.CatBoost) -> cb.CatBoost:\n    \"\"\"Patch CatBoost to return only the mean prediction in `predict`\n    and the variance in `predict_var` to be consistent with other models.\n    The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n    and requires re-patching.\n\n    Parameters\n    ----------\n    estimator : cb.CatBoost\n        The CatBoost estimator to patch.\n\n    Returns\n    -------\n    cb.CatBoost\n        The patched CatBoost estimator.\n    \"\"\"\n    original_predict = estimator.predict\n\n    @functools.wraps(original_predict)\n    def predict(\n        data: Any,\n        prediction_type: Literal[\n            \"Probability\", \"Class\", \"RawFormulaVal\", \"Exponent\", \"LogProbability\"\n        ] = \"RawFormulaVal\",\n        ntree_start: int = 0,\n        ntree_end: int = 0,\n        thread_count: int = -1,\n        verbose: bool | None = None,\n        task_type: str = \"CPU\",\n    ) -> NDArray[Any]:\n        prediction = original_predict(\n            data,\n            prediction_type,\n            ntree_start,\n            ntree_end,\n            thread_count,\n            verbose,\n            task_type,\n        )\n        if prediction.ndim == 2:\n            return prediction[:, 0]\n        return prediction\n\n    setattr(estimator, \"predict\", predict)\n\n    self = estimator\n\n    def predict_var(\n        X: Any,\n        prediction_type: Literal[\"knowledge\", \"data\", \"total\"] = \"total\",\n        **predict_params: Any,\n    ) -> NDArray[Any]:\n        uncertainty = self.virtual_ensembles_predict(\n            X, prediction_type=\"TotalUncertainty\", **predict_params\n        )\n\n        loss_function = self.get_params()[\"loss_function\"]\n        knowledge_uncertainty = np.zeros_like(uncertainty[..., 0])\n        data_uncertainty = np.zeros_like(uncertainty[..., 0])\n        if loss_function == \"RMSEWithUncertainty\":\n            knowledge_uncertainty = uncertainty[..., 1]\n            data_uncertainty = uncertainty[..., 2]\n        elif isinstance(estimator, cb.CatBoostClassifier):\n            knowledge_uncertainty = uncertainty[..., 1]\n        else:\n            data_uncertainty = uncertainty[..., 0]\n            knowledge_uncertainty = uncertainty[..., 1] - data_uncertainty\n\n        if prediction_type == \"knowledge\":\n            return knowledge_uncertainty\n        elif prediction_type == \"data\":\n            return data_uncertainty\n        elif prediction_type == \"total\":\n            return knowledge_uncertainty + data_uncertainty\n        else:\n            raise ValueError(\n                \"prediction_type must be one of ['knowledge', 'data', 'total'], \"\n                f\"but got {prediction_type}\"\n            )\n\n    setattr(estimator, \"predict_var\", predict_var)\n    return estimator", "\n\nTAny = TypeVar(\"TAny\")\n\n\ndef patch(estimator: TAny, *, copy: bool = True, recursive: bool = True) -> TAny:\n    \"\"\"Patch estimator if it is supported. (`patch_ngboost` and `patch_catboost`.)\n    The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n    and requires re-patching.\n\n    Parameters\n    ----------\n    estimator : TAny\n        The estimator to patch.\n    copy : bool, optional\n        Whether to copy the estimator before patching, by default True\n    recursive : bool, optional\n        Whether to recursively patch the estimator, by default True\n\n    Returns\n    -------\n    TAny\n        The patched estimator.\n    \"\"\"\n    if copy:\n        estimator = clone(estimator)\n    if importlib.util.find_spec(\"ngboost\") is not None:\n        if isinstance(estimator, NGBoost):\n            return patch_ngboost(estimator)\n    if isinstance(estimator, cb.CatBoost):\n        return patch_catboost(estimator)\n\n    if recursive and hasattr(estimator, \"get_params\"):\n        for _, value in estimator.get_params(deep=True).items():\n            patch(value, copy=False, recursive=False)\n\n    return estimator", ""]}
{"filename": "src/boost_loss/debug.py", "chunked_list": ["from __future__ import annotations\n\nfrom logging import getLogger\n\nimport attrs\nfrom numpy.typing import NDArray\n\nfrom .base import LossBase\n\nLOG = getLogger(__name__)", "\nLOG = getLogger(__name__)\n\n\n@attrs.define()\nclass DebugLoss(LossBase):\n    \"\"\"Calls LOG.debug() every time loss() or grad_hess() is called.\"\"\"\n\n    loss_: LossBase\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n        loss = self.loss_.loss(y_true=y_true, y_pred=y_pred)\n        LOG.debug(f\"y_true: {y_true}, y_pred: {y_pred}, loss: {loss}\")\n        return loss\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n        LOG.debug(f\"y_true: {y_true}, y_pred: {y_pred}, grad: {grad}, hess: {hess}\")\n        return grad, hess", "\n\n@attrs.define()\nclass PrintLoss(LossBase):\n    \"\"\"Prints every time loss() or grad_hess() is called.\"\"\"\n\n    loss_: LossBase\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n        loss = self.loss_.loss(y_true=y_true, y_pred=y_pred)\n        print(f\"y_true: {y_true}, y_pred: {y_pred}, loss: {loss}\")\n        return loss\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n        print(f\"y_true: {y_true}, y_pred: {y_pred}, grad: {grad}, hess: {hess}\")\n        return grad, hess", ""]}
{"filename": "src/boost_loss/resuming.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import Sequence\n\nimport numpy as np\nfrom numpy.typing import NDArray\n\nfrom .base import LossBase\n\n\nclass ResumingLoss(LossBase):\n    def __init__(\n        self,\n        losses: Sequence[LossBase],\n        *,\n        weights: Sequence[float] | None = None,\n        interval: int = 1,\n        random_state: int | None = None,\n    ) -> None:\n        self.losses = losses\n        if weights is None:\n            self.weights = np.ones_like(losses)\n        else:\n            self.weights = np.array(weights)\n        self.interval = interval\n        self.random_state = random_state\n        if self.random_state is None:\n            if weights is not None:\n                raise ValueError(\"weights must be None when random_state is None\")\n        else:\n            self.random = np.random.RandomState(self.random_state)\n        self._count = 0\n        self._idx = 0\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        if self._count % self.interval == 0:\n            if self.random_state is None:\n                self._idx = self.random.choice(len(self.losses), p=self.weights)\n            else:\n                self._idx = (self._count // self.interval) % len(self.losses)\n        self._count += 1\n        return self.losses[self._idx].grad_hess(y_true=y_true, y_pred=y_pred)\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        return self.losses[self._idx].loss(y_true=y_true, y_pred=y_pred)", "\n\nclass ResumingLoss(LossBase):\n    def __init__(\n        self,\n        losses: Sequence[LossBase],\n        *,\n        weights: Sequence[float] | None = None,\n        interval: int = 1,\n        random_state: int | None = None,\n    ) -> None:\n        self.losses = losses\n        if weights is None:\n            self.weights = np.ones_like(losses)\n        else:\n            self.weights = np.array(weights)\n        self.interval = interval\n        self.random_state = random_state\n        if self.random_state is None:\n            if weights is not None:\n                raise ValueError(\"weights must be None when random_state is None\")\n        else:\n            self.random = np.random.RandomState(self.random_state)\n        self._count = 0\n        self._idx = 0\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        if self._count % self.interval == 0:\n            if self.random_state is None:\n                self._idx = self.random.choice(len(self.losses), p=self.weights)\n            else:\n                self._idx = (self._count // self.interval) % len(self.losses)\n        self._count += 1\n        return self.losses[self._idx].grad_hess(y_true=y_true, y_pred=y_pred)\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n        return self.losses[self._idx].loss(y_true=y_true, y_pred=y_pred)", ""]}
{"filename": "src/boost_loss/regression/regression.py", "chunked_list": ["from __future__ import annotations\n\nfrom logging import getLogger\n\nimport attrs\nimport numpy as np\nfrom numpy.typing import NDArray\n\nfrom ..base import LossBase\n", "from ..base import LossBase\n\nLOG = getLogger(__name__)\n\n\n# cannot freeze due to FrozenInstanceError in catboost\n@attrs.define()\nclass LNLoss(LossBase):\n    \"\"\"LNLoss = |y_true - y_pred|^n\n    - x < 1 is not recommended because the loss is not convex.\n    - x >> 2 is not recommended because the gradient is too steep.\"\"\"\n\n    n: float\n    \"\"\"The exponent of the loss.\"\"\"\n    divide_n_loss: bool = False\n    \"\"\"Whether to divide the loss by n. Generally False is used.\"\"\"\n    divide_n_grad: bool = True\n    \"\"\"Whether to divide the gradient by n. Generally True is used.\"\"\"\n\n    @property\n    def name(self) -> str:\n        return f\"l{self.n:.2f}\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.abs(y_pred - y_true) ** self.n / (self.n if self.divide_n_loss else 1)\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        y_diff = y_pred - y_true\n        return (\n            np.abs(y_diff) ** (self.n - 1)\n            * np.sign(y_diff)\n            * self.n\n            / (self.n if self.divide_n_grad else 1)\n        )\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        y_diff = y_pred - y_true\n        return (\n            (self.n - 1)\n            * np.abs(y_diff) ** (self.n - 2)\n            * self.n\n            / (self.n if self.divide_n_grad else 1)\n        )", "\n\nclass L1Loss(LNLoss):\n    \"\"\"L1 loss = |y_true - y_pred|.\"\"\"\n\n    def __init__(\n        self, *, divide_n_loss: bool = False, divide_n_grad: bool = True\n    ) -> None:\n        \"\"\"L1 loss.\n\n        Parameters\n        ----------\n        divide_n_loss : bool, optional\n            Whether to divide the loss by n, by default False\n        divide_n_grad : bool, optional\n            Whether to divide the gradient by n, by default True\n        \"\"\"\n        super().__init__(n=1, divide_n_loss=divide_n_loss, divide_n_grad=divide_n_grad)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.ones_like(y_pred - y_true)  # zero hess is not allowed", "\n\nclass L2Loss(LNLoss):\n    \"\"\"L2 loss = |y_true - y_pred|^2\"\"\"\n\n    def __init__(\n        self, *, divide_n_loss: bool = False, divide_n_grad: bool = True\n    ) -> None:\n        \"\"\"L2 loss.\n\n        Parameters\n        ----------\n        divide_n_loss : bool, optional\n            Whether to divide the loss by n, by default False\n        divide_n_grad : bool, optional\n            Whether to divide the gradient by n, by default True\n        \"\"\"\n        super().__init__(n=2, divide_n_loss=divide_n_loss, divide_n_grad=divide_n_grad)", "\n\nclass LogCoshLoss(LossBase):\n    \"\"\"LogCosh loss = log(cosh(y_true - y_pred))\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.log(np.cosh(y_pred - y_true))\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.tanh(y_pred - y_true)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.cosh(y_pred - y_true) ** -2", "\n\nclass HuberLoss(LossBase):\n    \"\"\"Huber loss = 0.5 (y_true - y_pred)^2 if |y_true - y_pred| <= delta\n    else delta * (|y_true - y_pred| - 0.5 * delta)\"\"\"\n\n    def __init__(self, delta: float = 1.0) -> None:\n        \"\"\"Huber loss.\n\n        Parameters\n        ----------\n        delta : float, optional\n            The parameter delta, by default 1.0\n        \"\"\"\n        self.delta = delta\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        y_diff = y_pred - y_true\n        return np.where(\n            np.abs(y_diff) <= self.delta,\n            0.5 * y_diff**2,\n            self.delta * (np.abs(y_diff) - 0.5 * self.delta),\n        )\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        y_diff = y_pred - y_true\n        return np.where(\n            np.abs(y_diff) <= self.delta,\n            y_diff,\n            np.sign(y_diff) * self.delta,\n        )\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.ones_like(y_pred - y_true)  # zero hess is not allowed", "        # return np.where(\n        #     np.abs(y_pred - y_true) <= self.delta,\n        #     np.ones_like(y_true),\n        #     np.zeros_like(y_true),\n        # )\n\n\nclass FairLoss(LossBase):\n    \"\"\"Fair loss = c^2/2 * (abs(y_true - y_pred) -\n    c * log(1 + abs(y_true - y_pred)/c))\"\"\"\n\n    def __init__(self, c: float = 1.0) -> None:\n        \"\"\"Fair loss.\n\n        Parameters\n        ----------\n        c : float, optional\n            The parameter c, by default 1.0\n        \"\"\"\n        self.c = c\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return (\n            self.c**2\n            / 2\n            * (\n                np.abs(y_true - y_pred)\n                - self.c * np.log(1 + np.abs(y_true - y_pred) / self.c)\n            )\n        )\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return (\n            self.c**2\n            / 2\n            * (np.sign(y_pred - y_true) - self.c / (self.c + np.abs(y_pred - y_true)))\n        )\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return (\n            self.c**2\n            / 2\n            * (np.zeros_like(y_true) + self.c / (self.c + np.abs(y_pred - y_true)) ** 2)\n        )", "\n\nclass PoissonLoss(LossBase):\n    \"\"\"Poisson loss = y_pred - y_true * log(y_pred)\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return y_pred - y_true * np.log(y_pred)\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return 1 - y_true / y_pred\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return y_true / y_pred**2", "\n\nclass LogLoss(LossBase):\n    \"\"\"Log loss = log(1 + exp(-y_true * y_pred))\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.log(1 + np.exp(-y_true * y_pred))\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return -y_true / (1 + np.exp(y_true * y_pred))\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return (\n            y_true**2 * np.exp(y_true * y_pred) / (1 + np.exp(y_true * y_pred)) ** 2\n        )", "\n\nclass MSLELoss(LossBase):\n    \"\"\"Mean squared logarithmic error loss = (log(1 + y_true) - log(1 + y_pred))^2\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.square(np.log1p(y_true) - np.log1p(y_pred))\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return -2 * (np.log1p(y_true) - np.log1p(y_pred)) / (1 + y_pred)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return 2 * (np.log1p(y_true) - np.log1p(y_pred)) / (1 + y_pred) ** 2", "\n\nclass MAPELoss(LossBase):\n    \"\"\"Mean absolute percentage error loss = abs(y_true - y_pred) / y_true\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.abs(y_true - y_pred) / y_true\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.sign(y_pred - y_true) / y_true\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.zeros_like(y_true)", "\n\nclass SMAPELoss(LossBase):\n    \"\"\"Symmetric mean absolute percentage error loss =\n    abs(y_true - y_pred) / ((abs(y_true) + abs(y_pred))/2)\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.sign(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return np.zeros_like(y_true)", "\n\nclass TweedieLoss(LossBase):\n    \"\"\"Tweedie loss = -y_true * y_pred^(2-p) / (2-p) + y_pred^(1-p) / (1-p)\"\"\"\n\n    def __init__(self, p: float = 1.5) -> None:\n        \"\"\"Tweedie loss.\n\n        Parameters\n        ----------\n        p : float, optional\n            The parameter p, by default 1.5\n        \"\"\"\n        self.p = p\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return -y_true * y_pred ** (2 - self.p) / (2 - self.p) + y_pred ** (\n            1 - self.p\n        ) / (1 - self.p)\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return -y_true * (2 - self.p) * y_pred ** (1 - self.p) + y_pred ** (-self.p)\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return y_true * (2 - self.p) * (1 - self.p) * y_pred ** (\n            -self.p - 1\n        ) - self.p * y_pred ** (-self.p - 1)", "\n\nclass GammaLoss(LossBase):\n    \"\"\"Gamma loss = y_true / y_pred - log(y_true / y_pred) - 1\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return y_true / y_pred - np.log(y_true / y_pred) - 1\n\n    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return -y_true / y_pred**2 + 1 / y_pred\n\n    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n        return 2 * y_true / y_pred**3 - 1 / y_pred**2", ""]}
{"filename": "src/boost_loss/regression/__init__.py", "chunked_list": ["from .asymmetric import (\n    AsymmetricCompositeLoss,\n    AsymmetricLoss,\n    ExpectileLoss,\n    QuantileLoss,\n)\nfrom .regression import (\n    FairLoss,\n    GammaLoss,\n    HuberLoss,", "    GammaLoss,\n    HuberLoss,\n    L1Loss,\n    L2Loss,\n    LNLoss,\n    LogCoshLoss,\n    LogLoss,\n    MAPELoss,\n    MSLELoss,\n    PoissonLoss,", "    MSLELoss,\n    PoissonLoss,\n    SMAPELoss,\n    TweedieLoss,\n)\nfrom .sklearn import VarianceEstimator\n\n__all__ = [\n    \"AsymmetricLoss\",\n    \"AsymmetricCompositeLoss\",", "    \"AsymmetricLoss\",\n    \"AsymmetricCompositeLoss\",\n    \"QuantileLoss\",\n    \"ExpectileLoss\",\n    \"L1Loss\",\n    \"L2Loss\",\n    \"LNLoss\",\n    \"LogCoshLoss\",\n    \"HuberLoss\",\n    \"FairLoss\",", "    \"HuberLoss\",\n    \"FairLoss\",\n    \"PoissonLoss\",\n    \"TweedieLoss\",\n    \"GammaLoss\",\n    \"LogLoss\",\n    \"MAPELoss\",\n    \"MSLELoss\",\n    \"SMAPELoss\",\n    \"VarianceEstimator\",", "    \"SMAPELoss\",\n    \"VarianceEstimator\",\n]\n"]}
{"filename": "src/boost_loss/regression/asymmetric.py", "chunked_list": ["from __future__ import annotations\n\nimport attrs\nimport numpy as np\nfrom numpy.typing import NDArray\n\nfrom ..base import LossBase\nfrom .regression import L1Loss, L2Loss\n\n", "\n\n@attrs.define()\nclass AsymmetricCompositeLoss(LossBase):\n    \"\"\"Asymmetric composite loss function.\n    The loss function is `loss_pred_less` if `y_true < y_pred`,\n    otherwise `loss_pred_greater`.\n    \"\"\"\n\n    loss_pred_less: LossBase\n    \"\"\"The loss function if `y_true < y_pred`.\"\"\"\n    loss_pred_greater: LossBase\n    \"\"\"The loss function if `y_true >= y_pred`.\"\"\"\n\n    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n        loss = np.where(\n            y_true < y_pred,\n            self.loss_pred_less.loss(y_true=y_true, y_pred=y_pred),\n            self.loss_pred_greater.loss(y_true=y_true, y_pred=y_pred),\n        )\n        return loss\n\n    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n        grad, hess = np.where(\n            y_true < y_pred,\n            self.loss_pred_less.grad_hess(y_true=y_true, y_pred=y_pred),\n            self.loss_pred_greater.grad_hess(y_true=y_true, y_pred=y_pred),\n        )\n        return grad, hess", "\n\nclass AsymmetricLoss(AsymmetricCompositeLoss):\n    \"\"\"Asymmetric loss function.\n    The loss function is `loss * (1 - t)` if `y_true < y_pred`, otherwise `loss * t`.\n    Generalized from quantile loss (pinball loss, check loss, etc.) and expectile loss.\n    \"\"\"\n\n    def __init__(self, loss: LossBase, t: float = 0.5) -> None:\n        super().__init__(loss * (1 - t), loss * t)", "\n\nclass QuantileLoss(AsymmetricLoss):\n    \"\"\"[Quantile](https://en.wikipedia.org/wiki/Quantile) loss function.\"\"\"\n\n    def __init__(self, t: float = 0.5) -> None:\n        super().__init__(L1Loss(), t)\n\n\nclass ExpectileLoss(AsymmetricLoss):\n    r\"\"\"[Expectile](https://sites.google.com/site/csphilipps/expectiles) loss function.\n    - Expectile is a conditional mean if observations in [\u03bc_\u03c4, \u221e)\n    are \u03c4/(1 - \u03c4) times more likely than the original distribution.\n    - Expectiles are not always quantiles of F.\n    - Expectile loss is more smooth than quantile loss.\n\n    .. math::\n        \\tau \\int_{-\\infty}^{\\mu_\\tau} (y - \\mu_\\tau) dF(y)\n        = (1 - \\tau) \\int_{\\mu_\\tau}^{\\infty} (y - \\mu_\\tau) dF(y)\"\"\"\n\n    def __init__(self, t: float = 0.5) -> None:\n        super().__init__(L2Loss(), t)", "\nclass ExpectileLoss(AsymmetricLoss):\n    r\"\"\"[Expectile](https://sites.google.com/site/csphilipps/expectiles) loss function.\n    - Expectile is a conditional mean if observations in [\u03bc_\u03c4, \u221e)\n    are \u03c4/(1 - \u03c4) times more likely than the original distribution.\n    - Expectiles are not always quantiles of F.\n    - Expectile loss is more smooth than quantile loss.\n\n    .. math::\n        \\tau \\int_{-\\infty}^{\\mu_\\tau} (y - \\mu_\\tau) dF(y)\n        = (1 - \\tau) \\int_{\\mu_\\tau}^{\\infty} (y - \\mu_\\tau) dF(y)\"\"\"\n\n    def __init__(self, t: float = 0.5) -> None:\n        super().__init__(L2Loss(), t)", ""]}
{"filename": "src/boost_loss/regression/sklearn.py", "chunked_list": ["from __future__ import annotations\n\nfrom copy import copy\nfrom typing import Any, Literal, Sequence\n\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom numpy.typing import NDArray\nfrom sklearn.base import BaseEstimator\nfrom typing_extensions import Self", "from sklearn.base import BaseEstimator\nfrom typing_extensions import Self\n\nfrom ..base import LossBase\nfrom ..sklearn import apply_custom_loss\nfrom .asymmetric import AsymmetricLoss\n\n\ndef _recursively_set_random_state(estimator: BaseEstimator, random_state: int) -> None:\n    if hasattr(estimator, \"random_state\") and hasattr(estimator, \"set_params\"):\n        estimator.set_params(random_state=random_state)\n    for _, v in copy(estimator.get_params(deep=False)).items():\n        if hasattr(v, \"get_params\"):\n            _recursively_set_random_state(v, random_state)", "def _recursively_set_random_state(estimator: BaseEstimator, random_state: int) -> None:\n    if hasattr(estimator, \"random_state\") and hasattr(estimator, \"set_params\"):\n        estimator.set_params(random_state=random_state)\n    for _, v in copy(estimator.get_params(deep=False)).items():\n        if hasattr(v, \"get_params\"):\n            _recursively_set_random_state(v, random_state)\n\n\nclass VarianceEstimator(BaseEstimator):\n    \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n    with different `t`.\n    Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n    [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n    this estimator is much slower and does not support \"natural gradient\",\n    but does not require any assumption on the distribution.\n\n    Note that NGBoost supports\n    [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n    but it has to be defined beforehand.\n\n    NGBoost requires mean estimator and log standard deviation estimator\n    to be trained simultaneously, which is very difficult to implement\n    in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n    Consider change `Base` parameter in NGBoost.\n    (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n    \"\"\"\n\n    ts_: Sequence[float]\n    m_type: Literal[\"mean\", \"median\"]\n    var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"]\n\n    def __init__(\n        self,\n        estimator: Any,\n        loss: LossBase,\n        *,\n        ts: int | Sequence[float],\n        n_jobs: int | None = 1,\n        verbose: int = 0,\n        random_state: int | None = None,\n        m_type: Literal[\"mean\", \"median\"] = \"median\",\n        var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] = \"var\",\n        target_transformer: BaseEstimator | Any | None = None,\n    ) -> None:\n        \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n        with different `t`.\n        Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n        [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n        this estimator is much slower and does not support \"natural gradient\",\n        but does not require any assumption on the distribution.\n\n        Note that NGBoost supports\n        [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n        but it has to be defined beforehand.\n\n        NGBoost requires mean estimator and log standard deviation estimator\n        to be trained simultaneously, which is very difficult to implement\n        in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n        Consider change `Base` parameter in NGBoost.\n        (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n\n        Parameters\n        ----------\n        estimator : Any\n            The base estimator to use for fitting the data.\n        loss : LossBase\n            The loss function to use for fitting the data.\n            Generally, `loss` should not be `AsymmetricLoss`.\n        ts : int | Sequence[float]\n            The list of `t` to use for fitting the data or the number of `t` to use.\n            If `ts` is an integer, `np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)` is used.\n        n_jobs : int | None, optional\n            The number of jobs to run in parallel for `fit`. `None` means 1.\n        verbose : int, optional\n            The verbosity level.\n        random_state : int | None, optional\n            The random state to use for fitting the data. If `None`, the random state is not set.\n            If not `None`, new random state generated from `random_state` is set to each estimator.\n        m_type : Literal[&quot;mean&quot;, &quot;median&quot;], optional\n            M-statistics type to return from `predict` by default, by default \"median\"\n        var_type : Literal[&quot;var&quot;, &quot;std&quot;, &quot;range&quot;, &quot;mae&quot;, &quot;mse&quot;], optional\n            Variance type to return from `predict` by default, by default \"var\"\n        target_transformer : BaseEstimator | Any | None, optional\n            The transformer to use for transforming the target, by default None\n            If `None`, no `TransformedTargetRegressor` is used.\n\n        Raises\n        ------\n        TypeError\n            Raises if `estimator` does not have `fit` method or `predict` method.\n        \"\"\"\n        if not hasattr(estimator, \"fit\"):\n            raise TypeError(f\"{estimator} does not have fit method\")\n        if not hasattr(estimator, \"predict\"):\n            raise TypeError(f\"{estimator} does not have predict method\")\n        self.estimator = estimator\n        self.loss = loss\n        self.ts = ts\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.random_state = random_state\n        self.m_type = m_type\n        self.var_type = var_type\n        self.target_transformer = target_transformer\n        self.random = np.random.RandomState(random_state)\n\n    def fit(self, X: Any, y: Any, **fit_params: Any) -> Self:\n        \"\"\"Fit each estimator with different `t`.\n\n        Parameters\n        ----------\n        X : Any\n            The training input samples.\n        y : Any\n            The target values.\n\n        Returns\n        -------\n        Self\n            Fitted estimator.\n\n        Raises\n        ------\n        RuntimeError\n            Raises if joblib fails to return the results.\n        \"\"\"\n        ts = self.ts\n        if isinstance(ts, int):\n            ts = np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)\n        self.ts_ = ts  # type: ignore\n        estimators_ = [\n            apply_custom_loss(\n                self.estimator,\n                AsymmetricLoss(self.loss, t=t),\n                target_transformer=self.target_transformer,\n            )\n            for t in self.ts_\n        ]\n        if self.random is not None:\n            for estimator in estimators_:\n                _recursively_set_random_state(\n                    estimator, self.random.randint(0, np.iinfo(np.int32).max)\n                )\n        parallel_result = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n            [delayed(estimator.fit)(X, y, **fit_params) for estimator in estimators_]\n        )\n        if parallel_result is None:\n            raise RuntimeError(\"joblib.Parallel returned None\")\n        self.estimators_ = parallel_result\n        return self\n\n    def predict_raw(self, X: Any, **predict_params: Any) -> NDArray[Any]:\n        \"\"\"Returns raw predictions of each estimator.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Raw predictions of each estimator with shape (n_estimators, n_samples)\n        \"\"\"\n        return np.array(\n            [estimator.predict(X, **predict_params) for estimator in self.estimators_]\n        )\n\n    def predict(\n        self,\n        X: Any,\n        type_: Literal[\"mean\", \"median\", \"var\", \"std\", \"range\", \"mae\", \"mse\"]\n        | None = None,\n        **predict_params: Any,\n    ) -> NDArray[Any]:\n        \"\"\"Returns predictions of the ensemble.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        type_ : Literal['mean', 'median', 'var', 'std', 'range', 'mae', 'mse'], optional\n            Type of the prediction, by default None\n            If None, self.m_type is used.\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Predictions of the ensemble with shape (n_samples,)\n\n        Raises\n        ------\n        ValueError\n            When type_ is not supported.\n        \"\"\"\n        type_ = type_ or self.m_type\n        if type_ == \"mean\":\n            return self.predict_raw(X, **predict_params).mean(axis=0)\n        elif type_ == \"median\":\n            return np.median(self.predict_raw(X, **predict_params), axis=0)\n        elif type_ == \"var\":\n            return self.predict_raw(X, **predict_params).var(axis=0)\n        elif type_ == \"std\":\n            return self.predict_raw(X, **predict_params).std(axis=0)\n        elif type_ == \"range\":\n            return self.predict_raw(X, **predict_params).max(axis=0) - self.predict_raw(\n                X, **predict_params\n            ).min(axis=0)\n        elif type_ == \"mae\":\n            return np.abs(\n                self.predict_raw(X, **predict_params)\n                - self.predict_raw(X, **predict_params).mean(axis=0)\n            ).mean(axis=0)\n        elif type_ == \"mse\":\n            return (\n                (\n                    self.predict_raw(X, **predict_params)\n                    - self.predict_raw(X, **predict_params).mean(axis=0)\n                )\n                ** 2\n            ).mean(axis=0)\n        else:\n            raise ValueError(f\"Unknown type_: {type_}\")\n\n    def predict_var(\n        self,\n        X: Any,\n        type_: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] | None = None,\n        **predict_params: Any,\n    ) -> NDArray[Any]:\n        \"\"\"Returns variance of the ensemble.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        type_ : Literal['var', 'std', 'range', 'mae', 'mse'], optional\n            Type of the variance, by default None\n            If None, self.var_type is used.\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Variance of the ensemble with shape (n_samples,)\n        \"\"\"\n        type_ = type_ or self.var_type\n        return self.predict(X, type_=type_, **predict_params)", "class VarianceEstimator(BaseEstimator):\n    \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n    with different `t`.\n    Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n    [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n    this estimator is much slower and does not support \"natural gradient\",\n    but does not require any assumption on the distribution.\n\n    Note that NGBoost supports\n    [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n    but it has to be defined beforehand.\n\n    NGBoost requires mean estimator and log standard deviation estimator\n    to be trained simultaneously, which is very difficult to implement\n    in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n    Consider change `Base` parameter in NGBoost.\n    (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n    \"\"\"\n\n    ts_: Sequence[float]\n    m_type: Literal[\"mean\", \"median\"]\n    var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"]\n\n    def __init__(\n        self,\n        estimator: Any,\n        loss: LossBase,\n        *,\n        ts: int | Sequence[float],\n        n_jobs: int | None = 1,\n        verbose: int = 0,\n        random_state: int | None = None,\n        m_type: Literal[\"mean\", \"median\"] = \"median\",\n        var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] = \"var\",\n        target_transformer: BaseEstimator | Any | None = None,\n    ) -> None:\n        \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n        with different `t`.\n        Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n        [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n        this estimator is much slower and does not support \"natural gradient\",\n        but does not require any assumption on the distribution.\n\n        Note that NGBoost supports\n        [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n        but it has to be defined beforehand.\n\n        NGBoost requires mean estimator and log standard deviation estimator\n        to be trained simultaneously, which is very difficult to implement\n        in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n        Consider change `Base` parameter in NGBoost.\n        (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n\n        Parameters\n        ----------\n        estimator : Any\n            The base estimator to use for fitting the data.\n        loss : LossBase\n            The loss function to use for fitting the data.\n            Generally, `loss` should not be `AsymmetricLoss`.\n        ts : int | Sequence[float]\n            The list of `t` to use for fitting the data or the number of `t` to use.\n            If `ts` is an integer, `np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)` is used.\n        n_jobs : int | None, optional\n            The number of jobs to run in parallel for `fit`. `None` means 1.\n        verbose : int, optional\n            The verbosity level.\n        random_state : int | None, optional\n            The random state to use for fitting the data. If `None`, the random state is not set.\n            If not `None`, new random state generated from `random_state` is set to each estimator.\n        m_type : Literal[&quot;mean&quot;, &quot;median&quot;], optional\n            M-statistics type to return from `predict` by default, by default \"median\"\n        var_type : Literal[&quot;var&quot;, &quot;std&quot;, &quot;range&quot;, &quot;mae&quot;, &quot;mse&quot;], optional\n            Variance type to return from `predict` by default, by default \"var\"\n        target_transformer : BaseEstimator | Any | None, optional\n            The transformer to use for transforming the target, by default None\n            If `None`, no `TransformedTargetRegressor` is used.\n\n        Raises\n        ------\n        TypeError\n            Raises if `estimator` does not have `fit` method or `predict` method.\n        \"\"\"\n        if not hasattr(estimator, \"fit\"):\n            raise TypeError(f\"{estimator} does not have fit method\")\n        if not hasattr(estimator, \"predict\"):\n            raise TypeError(f\"{estimator} does not have predict method\")\n        self.estimator = estimator\n        self.loss = loss\n        self.ts = ts\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.random_state = random_state\n        self.m_type = m_type\n        self.var_type = var_type\n        self.target_transformer = target_transformer\n        self.random = np.random.RandomState(random_state)\n\n    def fit(self, X: Any, y: Any, **fit_params: Any) -> Self:\n        \"\"\"Fit each estimator with different `t`.\n\n        Parameters\n        ----------\n        X : Any\n            The training input samples.\n        y : Any\n            The target values.\n\n        Returns\n        -------\n        Self\n            Fitted estimator.\n\n        Raises\n        ------\n        RuntimeError\n            Raises if joblib fails to return the results.\n        \"\"\"\n        ts = self.ts\n        if isinstance(ts, int):\n            ts = np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)\n        self.ts_ = ts  # type: ignore\n        estimators_ = [\n            apply_custom_loss(\n                self.estimator,\n                AsymmetricLoss(self.loss, t=t),\n                target_transformer=self.target_transformer,\n            )\n            for t in self.ts_\n        ]\n        if self.random is not None:\n            for estimator in estimators_:\n                _recursively_set_random_state(\n                    estimator, self.random.randint(0, np.iinfo(np.int32).max)\n                )\n        parallel_result = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n            [delayed(estimator.fit)(X, y, **fit_params) for estimator in estimators_]\n        )\n        if parallel_result is None:\n            raise RuntimeError(\"joblib.Parallel returned None\")\n        self.estimators_ = parallel_result\n        return self\n\n    def predict_raw(self, X: Any, **predict_params: Any) -> NDArray[Any]:\n        \"\"\"Returns raw predictions of each estimator.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Raw predictions of each estimator with shape (n_estimators, n_samples)\n        \"\"\"\n        return np.array(\n            [estimator.predict(X, **predict_params) for estimator in self.estimators_]\n        )\n\n    def predict(\n        self,\n        X: Any,\n        type_: Literal[\"mean\", \"median\", \"var\", \"std\", \"range\", \"mae\", \"mse\"]\n        | None = None,\n        **predict_params: Any,\n    ) -> NDArray[Any]:\n        \"\"\"Returns predictions of the ensemble.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        type_ : Literal['mean', 'median', 'var', 'std', 'range', 'mae', 'mse'], optional\n            Type of the prediction, by default None\n            If None, self.m_type is used.\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Predictions of the ensemble with shape (n_samples,)\n\n        Raises\n        ------\n        ValueError\n            When type_ is not supported.\n        \"\"\"\n        type_ = type_ or self.m_type\n        if type_ == \"mean\":\n            return self.predict_raw(X, **predict_params).mean(axis=0)\n        elif type_ == \"median\":\n            return np.median(self.predict_raw(X, **predict_params), axis=0)\n        elif type_ == \"var\":\n            return self.predict_raw(X, **predict_params).var(axis=0)\n        elif type_ == \"std\":\n            return self.predict_raw(X, **predict_params).std(axis=0)\n        elif type_ == \"range\":\n            return self.predict_raw(X, **predict_params).max(axis=0) - self.predict_raw(\n                X, **predict_params\n            ).min(axis=0)\n        elif type_ == \"mae\":\n            return np.abs(\n                self.predict_raw(X, **predict_params)\n                - self.predict_raw(X, **predict_params).mean(axis=0)\n            ).mean(axis=0)\n        elif type_ == \"mse\":\n            return (\n                (\n                    self.predict_raw(X, **predict_params)\n                    - self.predict_raw(X, **predict_params).mean(axis=0)\n                )\n                ** 2\n            ).mean(axis=0)\n        else:\n            raise ValueError(f\"Unknown type_: {type_}\")\n\n    def predict_var(\n        self,\n        X: Any,\n        type_: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] | None = None,\n        **predict_params: Any,\n    ) -> NDArray[Any]:\n        \"\"\"Returns variance of the ensemble.\n\n        Parameters\n        ----------\n        X : Any\n            X\n        type_ : Literal['var', 'std', 'range', 'mae', 'mse'], optional\n            Type of the variance, by default None\n            If None, self.var_type is used.\n        **predict_params : Any\n            The parameters to be passed to `predict` method of each estimator.\n\n        Returns\n        -------\n        NDArray[Any]\n            Variance of the ensemble with shape (n_samples,)\n        \"\"\"\n        type_ = type_ or self.var_type\n        return self.predict(X, type_=type_, **predict_params)", ""]}
