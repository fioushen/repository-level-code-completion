{"filename": "main.py", "chunked_list": ["import os.path\nimport torch\nimport random\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import Trainer\nfrom vocsep.models.vocsep import HeteroVoiceLinkPredictionModel, VoiceLinkPredictionModel\nfrom vocsep.data.datamodules.mix_vs import GraphMixVSDataModule\nfrom pytorch_lightning.plugins import DDPPlugin\nfrom vocsep.utils.visualization import show_voice_pr", "from pytorch_lightning.plugins import DDPPlugin\nfrom vocsep.utils.visualization import show_voice_pr\nimport argparse\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--collection', type=str, default=\"inventions\",\n                    choices=[\"inventions\", \"the_well-tempered_clavier_book_I\", \"the_well-tempered_clavier_book_II\", \"sinfonias\", \"haydn\"],\n                    help=\"Collections of pieces to use for the test set.\")\nparser.add_argument('--wandb_entity', type=str, default=None,", "                    help=\"Collections of pieces to use for the test set.\")\nparser.add_argument('--wandb_entity', type=str, default=None,\n                    help=\"Your WandB user name. If not provided, it will be taken from the WANDB_ENTITY environment variable.\")\nparser.add_argument('--gpus', type=str, default=\"-1\", help=\"GPU ids to use (you can use multiple by writting 0,1,2, etc). If -1, it will use CPU.\")\nparser.add_argument('--n_layers', type=int, default=2, help=\"Number of layers for the Graph Convolutional Network.\")\nparser.add_argument('--n_hidden', type=int, default=256, help=\"Number of hidden units for the Graph Convolutional Network.\")\nparser.add_argument('--dropout', type=float, default=0.5, help=\"Dropout rate of the Model.\")\nparser.add_argument('--lr', type=float, default=0.001, help=\"Learning rate of the Model.\")\nparser.add_argument('--weight_decay', type=float, default=5e-4, help=\"Weight decay of the Model.\")\nparser.add_argument(\"--pot_edges_max_dist\", type=int, default=2, help=\"Maximum distance between nodes to consider them as potential edges in bars.\")", "parser.add_argument('--weight_decay', type=float, default=5e-4, help=\"Weight decay of the Model.\")\nparser.add_argument(\"--pot_edges_max_dist\", type=int, default=2, help=\"Maximum distance between nodes to consider them as potential edges in bars.\")\nparser.add_argument(\"--load_from_checkpoint\", action=\"store_true\", help=\"Load model from WANDB checkpoint\")\nparser.add_argument(\"--linear_assignment\", action=\"store_true\", help=\"Use linear assignment Hungarian algorithm for val and test predictions.\")\nparser.add_argument(\"--force_reload\", action=\"store_true\", help=\"Force reload of the data\")\nparser.add_argument(\"--model\", type=str, default=\"ResConv\", help=\"Block Convolution Model to use\")\nparser.add_argument(\"--reg_loss_weight\", type=str, default=\"auto\", help=\"Weight of the regularization loss. If 'auto', it augments every epoch end.\", choices=[\"auto\", \"none\", \"fixed\"])\nparser.add_argument(\"--use_jk\", action=\"store_true\", help=\"Use Jumping Knowledge\")\nparser.add_argument(\"--tags\", type=str, default=\"\", help=\"Tags to add to the WandB run api\")\nparser.add_argument(\"--homogeneous\", action=\"store_true\", help=\"Use homogeneous graph convolution.\")", "parser.add_argument(\"--tags\", type=str, default=\"\", help=\"Tags to add to the WandB run api\")\nparser.add_argument(\"--homogeneous\", action=\"store_true\", help=\"Use homogeneous graph convolution.\")\nparser.add_argument(\"--reg_loss_type\", type=str, default=\"la\", help=\"Use different regularization loss\")\n\n\n\n# for reproducibility\ntorch.manual_seed(0)\nrandom.seed(0)\ntorch.use_deterministic_algorithms(True)", "random.seed(0)\ntorch.use_deterministic_algorithms(True)\n\n\nargs = parser.parse_args()\nif args.gpus == \"-1\":\n    devices = None\nelse:\n    devices = [eval(gpu) for gpu in args.gpus.split(\",\")]\nrev_edges = \"new_type\"", "rev_edges = \"new_type\"\ncollections = args.collection.split(\",\")\nn_layers = args.n_layers\nn_hidden = args.n_hidden\nlinear_assignment = args.linear_assignment\npot_edges_max_dist = args.pot_edges_max_dist\ntags = args.tags.split(\",\")\nforce_reload = False\nnum_workers = 20\n", "num_workers = 20\n\n\nname = \"{}GLAN-{}x{}-{}-lr={}-wd={}-dr={}-rl={}-jk={}\".format(args.model,\n    n_layers, n_hidden, \"wLN\" if args.linear_assignment else \"woLN\", args.lr,\n    args.weight_decay, args.dropout, args.reg_loss_weight, args.use_jk)\n\n\nwandb_logger = WandbLogger(\n    log_model=True,", "wandb_logger = WandbLogger(\n    log_model=True,\n    entity=args.wandb_entity if args.wandb_entity is not None else os.getenv(\"WANDB_ENTITY\"),\n    project=\"Voice Separation\",\n    group=f\"MixVS-{collections[0]}\",\n    job_type=\"Homogeneous-mkGLAN\" if args.homogeneous else \"Heterogeneous-mkGLAN\",\n    tags=tags,\n    name=name)\n\n", "\n\ndatamodule = GraphMixVSDataModule(\n    batch_size=1, num_workers=num_workers,\n    force_reload=force_reload, test_collections=collections,\n    pot_edges_max_dist=pot_edges_max_dist)\ndatamodule.setup()\nif args.homogeneous:\n    model = VoiceLinkPredictionModel(\n        datamodule.features, n_hidden,\n        n_layers=n_layers, dropout=args.dropout, lr=args.lr,\n        weight_decay=args.weight_decay, reg_loss_weight=args.reg_loss_weight,\n        jk=args.use_jk, model=args.model, linear_assignment=linear_assignment)\nelse:\n    model = HeteroVoiceLinkPredictionModel(\n        datamodule.features, n_hidden,\n        n_layers=n_layers, lr=args.lr, dropout=args.dropout,\n        weight_decay=args.weight_decay, linear_assignment=linear_assignment,\n        model=args.model, jk=args.use_jk, reg_loss_weight=args.reg_loss_weight,\n        reg_loss_type=args.reg_loss_type)", "\nif args.load_from_checkpoint:\n    # download checkpoint locally (if not already cached)\n    import wandb\n    run = wandb.init(project=\"Voice Separation\", entity=\"vocsep\", job_type=\"Heterogeneous-mkSAGE\", group=f\"MCMA-{collections[0]}\", name=f\"Sage-{n_layers}x{n_hidden}\")\n    artifact = run.use_artifact('vocsep/Voice Separation/model-1eii6k4w:v0', type='model')\n    artifact_dir = artifact.download()\n\n    print(\"Only monophonic:\", model.linear_assignment)\n    trainer = Trainer(\n        max_epochs=50, accelerator=\"auto\", devices=devices,\n        num_sanity_val_steps=1,\n        logger=wandb_logger,\n    )\n    trainer.test(model, datamodule, ckpt_path=os.path.join(os.path.normpath(artifact_dir), \"model.ckpt\"))\n\n    prediction = trainer.predict(model, dataloaders=datamodule.test_dataloader())\n    # Show target Voice\n    pred_dict = prediction[0]\n    show_voice_pr(\n        pitches=pred_dict[\"note_array\"][0], onsets=pred_dict[\"note_array\"][3],\n        durations=pred_dict[\"note_array\"][4], voices=pred_dict[\"target_voices\"])\n    # Show predicted Voice\n    show_voice_pr(\n        pitches=pred_dict[\"note_array\"][0], onsets=pred_dict[\"note_array\"][3],\n        durations=pred_dict[\"note_array\"][4], voices=pred_dict[\"pred_voices\"])\nelse:\n\n    print(\"Only monophonic:\", model.linear_assignment)\n    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_fscore\", mode=\"max\")\n    trainer = Trainer(\n        max_epochs=50, accelerator=\"auto\", devices=devices,\n        num_sanity_val_steps=1,\n        logger=wandb_logger,\n        callbacks=[checkpoint_callback],\n        )\n\n    #training\n    trainer.fit(model, datamodule)\n\n    # Testing with best model\n    trainer.test(model, datamodule, ckpt_path=checkpoint_callback.best_model_path)", ""]}
{"filename": "vocsep/descriptors/general.py", "chunked_list": ["import numpy as np\nimport numpy.lib.recfunctions as rfn\nimport partitura\nimport partitura.score\n\nfrom .utils import *\nfrom typing import Union, Tuple, List\n# If imported should contain the name of descriptors\nfrom vocsep.utils.globals import *\nimport types", "from vocsep.utils.globals import *\nimport types\nimport sys\n\n\ndef notewise(part: Union[List, Union[partitura.score.Part, partitura.score.PartGroup]], dnames:Union[str, List] = \"all\", include_rests=False) -> Tuple[np.ndarray, List]:\n    \"\"\"\n\n    Parameters\n    ----------\n    part\n        A partitura part or PartGroup or List of parts.\n    dnames\n        The names of functions of discriptors\n    include_rests : bool\n        include_rests to feature computation.\n\n    Returns\n    -------\n    out_feats : np.ndarray\n        An array of feature values\n    out_dnames : list\n        A list of feature names\n    \"\"\"\n    if dnames == \"all\":\n        descriptors = list(map(lambda x: getattr(sys.modules[__name__], x), DESCRIPTORS.values()))\n    elif isinstance(dnames, str):\n        descriptors = [getattr(sys.modules[__name__], dnames)]\n    elif isinstance(dnames, types.FunctionType):\n        descriptors = [dnames]\n    else:\n        descriptors = list(map(lambda x: getattr(sys.modules[__name__], dnames)))\n\n    if isinstance(part, list) or isinstance(part, partitura.score.PartGroup):\n        part = partitura.score.merge_parts(part)\n\n    note_array = part.note_array(\n        include_time_signature=True,\n        include_staff=True,\n        include_grace_notes=True,\n        include_metrical_position=True\n        )\n\n    if include_rests:\n        rest_array = part.rest_array(\n            include_time_signature=True,\n            include_staff=True,\n            include_grace_notes=True,\n            include_metrical_position=True\n        )\n        note_array = rfn.merge_arrays((note_array, rest_array))\n\n    out_feats = list()\n    out_names = list()\n    for fdes in descriptors:\n        x, fname = fdes(part, note_array)\n\n        out_feats.append(x)\n\n        if isinstance(fname, str):\n            out_names.append(fname)\n        elif isinstance(fname, list):\n            out_names += fname\n\n        if len(x.shape) == 1:\n            x = np.expand_dims(x, axis=-1)\n        assert len(x.shape) == 2\n\n    out_feats = np.hstack(out_feats)\n\n    return out_feats, out_names", "\n\ndef onsetwise(part: Union[List, Union[partitura.score.Part, partitura.score.PartGroup]], dnames:Union[str, List]=\"all\") -> Tuple[np.ndarray, List]:\n    pass\n\n\ndef measurewise(part: Union[partitura.score.Part, partitura.score.PartGroup], dnames:Union[str, List]=\"all\") -> Tuple[np.ndarray, List]:\n    pass\n\n\ndef pitch_spelling_features(part: Union[Union[partitura.score.Part, partitura.score.PartGroup], partitura.performance.PerformedPart]) -> Tuple[np.ndarray, List]:\n    features, fnames = get_input_irrelevant_features(part)\n    return features, fnames", "\n\ndef pitch_spelling_features(part: Union[Union[partitura.score.Part, partitura.score.PartGroup], partitura.performance.PerformedPart]) -> Tuple[np.ndarray, List]:\n    features, fnames = get_input_irrelevant_features(part)\n    return features, fnames\n\n\ndef voice_separation_features(part: Union[Union[partitura.score.Part, partitura.score.PartGroup], partitura.performance.PerformedPart]) -> Tuple[np.ndarray, List]:\n    features, fnames = get_voice_separation_features(part)\n    return features, fnames", "\n\ndef chord_analysis_features(part: Union[Union[partitura.score.Part, partitura.score.PartGroup], partitura.performance.PerformedPart]) -> Tuple[np.ndarray, List]:\n    features, fnames = get_chord_analysis_features(part)\n    return features, fnames\n\n\ndef panalysis_features(part: Union[Union[partitura.score.Part, partitura.score.PartGroup], partitura.performance.PerformedPart]) -> Tuple[np.ndarray, List]:\n    features, fnames = get_panalysis_features(part)\n    return features, fnames", "\n\ndef select_features(part, features):\n    if features == \"voice\":\n        note_features, _ = voice_separation_features(part)\n    elif features == \"chord\":\n        note_features, _ = chord_analysis_features(part)\n    elif features == \"panalysis\":\n        note_features, _ = panalysis_features(part)\n    else:\n        note_features, _ = get_input_irrelevant_features(part)\n    return note_features", ""]}
{"filename": "vocsep/descriptors/__init__.py", "chunked_list": ["from .utils import *\nfrom .general import *"]}
{"filename": "vocsep/descriptors/utils/int_vec.py", "chunked_list": ["import numpy as np\nfrom itertools import groupby\nfrom vocsep.utils.chord_representations import chord_to_intervalVector\n\n\nint_vec_cadence_dict = { \n    \"V/I maj\" : [1, 2, 2, 2, 3, 0],\n    \"V/I min\": [2, 1, 2, 3, 2, 0],\n    \"V7/I maj\" : [2, 3, 3, 2, 4, 1],\n    \"V7/I min\" : [2, 3, 3, 3, 3, 1],", "    \"V7/I maj\" : [2, 3, 3, 2, 4, 1],\n    \"V7/I min\" : [2, 3, 3, 3, 3, 1],\n    \"V9/I min\" : [3, 3, 5, 4, 4, 2],\n    \"IV/I maj\" : [1, 2, 2, 2, 3, 0],\n    \"IV/I min\" : [1, 2, 2, 2, 3, 0],\n    \"IV/I picard\" : [2, 1, 2, 3, 2, 0],\n    \"IV/I dorian\" : [0, 3, 2, 2, 2, 1],\n    \"V/VI\" : [2, 3, 3, 3, 3, 1],\n}\n", "}\n\n\n\ndef get_cadences(note_array, step, window_size):\n    '''\n    A definition for match score analysis \n    \n    Parameters\n    ----------\n    note_array : array(tuples)\n        The note_array from the partitura analysis\n    step : float\n        The step for the analysis window.\n    window_size : int\n        The window size to draw samples from, usually a bar measure size.\n    \n    Returns\n    -------\n    The bar number where pivot cadences are found.\n        \n    '''\n    # standard forward lim\n    durations = [n['duration_beat'] for n in note_array if n['duration_beat']!=0]\n    min_duration = min(durations)\n    max_duration = max(durations)\n    max_polyphony = max([len(list(item[1])) for item in groupby(note_array, key=lambda x: x[0])])\n    forward_step_lim = int(max_duration / min_duration + max_polyphony)\n    duration = note_array[-1]['onset_beat'] + max_duration - step\n    \n    # normalize duration\n    duration = duration / step\n    step_unit = 1\n    dim = int(round((duration - (window_size * step_unit)) / step_unit) + 1)\n    # 5 is the attributes number\n    X = np.zeros((dim - 1, 6))\n    Y = np.zeros(dim - 1)\n    index = 0\n    for window in range(1, dim - 1, step_unit):\n        fix_start = window * step\n        ind_list = list()\n        note_list = list()\n        if len(note_array[index:]) > forward_step_lim*window_size:\n            look_in = forward_step_lim * window_size + index\n        else:\n            look_in = len(note_array)\n        for ind, note in enumerate(note_array[index:look_in]):\n            note_start = note['onset_beat'] #onset\n            note_end = note['onset_beat'] + note['duration_beat'] #onset + duration\n            fix_end = fix_start + (window_size * step)\n            # check if note is in window\n            if (fix_start <= note_start <= fix_end) or (note_start <= fix_start and note_end >= fix_start):\n                    ind_list.append(ind)\n                    note_list.append(note_array[index+ind][\"pitch\"])\n        if ind_list != []:\n            # compute interval Vector\n            X[window] = np.array(chord_to_intervalVector(note_list))\n            note_list = sorted(list(set(note_list)))\n            bass_notes = list(map(lambda x: x%12, note_list[:2])) if len(note_list) >= 2 else None\n            bass_int = abs(bass_notes[0]-bass_notes[1]) if bass_notes != None else 0\n            Y[window] = True if bass_int == 5 or bass_int == 7 else False\n            # update index\n            index += min(ind_list)\n    \n    pivot_points = [ind+window_size/step for ind, value in enumerate(list(X))\n                if list(value) in int_vec_cadence_dict.values() and Y[ind]]\n    # have to automate x/6 to bar\n    return pivot_points", "\n\n\n\n\nif __name__ == \"__main__\":    \n    import matplotlib.pyplot as plt\n    import partitura\n    import os\n    dirname = os.path.dirname(__file__)\n    par = lambda x: os.path.abspath(os.path.join(x, os.pardir))\n    my_musicxml_file = os.path.join(par(par(dirname)), \"samples\", \"xml\", \"k080-02.musicxml\")\n    step = 1\n    bar = 4\n    k = bar/step\n    \n    note_array = partitura.utils.ensure_notearray(partitura.load_musicxml(my_musicxml_file))\n    cad = list(set(map(lambda x: int(x/k)+1, get_cadences(note_array, 1, 4))))\n    print(cad)"]}
{"filename": "vocsep/descriptors/utils/note_density.py", "chunked_list": ["from itertools import groupby\nimport numpy as np\nfrom scipy.signal import argrelextrema\nfrom vocsep.descriptors.utils.harmonic_ssm import apply_ssm_analysis\n\n\ndef note_array2chords(note_array):\n    '''Group note_array list by time, effectively to chords.\n    \n    Parameters\n    ----------\n    note_array : array(N, 5)\n        The partitura note_array object. Every entry has 5 attributes, i.e. onset_time, note duration, note velocity, voice, id.\n        \n    Returns\n    -------\n    chords : list(list(tuples))\n    '''\n    note_array = [(n[\"onset_beat\"], n[\"duration_beat\"], n[\"pitch\"]) for n in note_array]\n    chords = [list(item[1]) for item in groupby(sorted(note_array), key=lambda x: x[0])]\n    for i in range(1, len(chords)):\n        temp = []\n        # \n        for index, chordTriplePrev in enumerate(chords[i-1]):\n            for chordTripleNext in chords[i]:\n                if chordTripleNext[0] < chordTriplePrev[1]+chordTriplePrev[0] and (chordTripleNext[0], chordTriplePrev[1], chordTriplePrev[2]) not in temp:            \n                    temp.append((chordTripleNext[0], chordTriplePrev[1], chordTriplePrev[2]))                 \n                    chords[i-1][index] = (chordTriplePrev[0], chordTripleNext[0] - chordTriplePrev[0], chordTriplePrev[2])\n        chords[i] = chords[i] + temp\n    return chords", "\n\ndef chord_cardinalites(chords):\n\tchords_card = np.zeros(len(chords), dtype=[('onset', 'f4'), ('note_end', 'f4'), ('cardinality', 'i4')])\n\tfor index, clist in enumerate(chords):\n\t\ton, dur, pitch = zip(*clist)\n\t\tchords_card[index] = (min(on), min(dur)+min(on), len(pitch))\n\treturn chords_card\n\n\ndef get_note_density(note_array, window_size):\n\t\"\"\"Vertical and Horrizontal Normalized Note Density.\n\t\n\tParameters\n\t----------\n\tnote_array : structured array\n\t\tA structured array containing onsets, durations and pitch of notes extracted with partitura from a xml score.\n\twindow_size : int\n\t\tUsually the bar size relative to beats\n\n\tReturns\n\t-------\n\thorrizontal : numpy array\n\t\tAn array containing the normalized note density of different note onsets\n\tvertical : numpy array\n\n\tExamples\n\t--------\n\timport partitura\n\tmy_musicxml_file = partitura.EXAMPLE_MUSICXML\n\n\tnote_array = partitura.musicxml_to_notearray(my_musicxml_file)\n\tprint(note_array)\n\thorrizontal, vertical = get_note_density(note_array, 2, 1)\n\tprint(vertical, horrizontal)\n\t\"\"\"\n\n\tcardinalities = chord_cardinalites(note_array2chords(note_array))\n\tlength = int((cardinalities[-1][\"note_end\"] - window_size))\n\n\tvertical = np.zeros(length)\n\thorrizontal = np.zeros(length)\n\tfor index, window in enumerate(range(0, length, window_size)):\n\t\ttemp1 = np.intersect1d(cardinalities[ np.where( cardinalities[\"onset\"] < window + window_size )], cardinalities[ np.where( cardinalities[\"onset\"] >= window )])\n\t\ttemp2 = np.intersect1d(cardinalities[ np.where( cardinalities[\"onset\"] < window)], cardinalities[ np.where( cardinalities[\"note_end\"] >= window )])\n\t\tnumber_of_dif_onsets = len(list(map(np.unique, temp1[\"onset\"]))) + len(list(map(np.unique, temp2[\"onset\"])))\n\t\thorrizontal[index] = (len(temp1) +len(temp2))\n\t\tvertical[index] = (sum(temp1[\"cardinality\"]) + sum(temp2[\"cardinality\"]))/number_of_dif_onsets if number_of_dif_onsets !=0 else 0 \n\treturn horrizontal, vertical", "\n\ndef get_note_density(note_array, window_size):\n\t\"\"\"Vertical and Horrizontal Normalized Note Density.\n\t\n\tParameters\n\t----------\n\tnote_array : structured array\n\t\tA structured array containing onsets, durations and pitch of notes extracted with partitura from a xml score.\n\twindow_size : int\n\t\tUsually the bar size relative to beats\n\n\tReturns\n\t-------\n\thorrizontal : numpy array\n\t\tAn array containing the normalized note density of different note onsets\n\tvertical : numpy array\n\n\tExamples\n\t--------\n\timport partitura\n\tmy_musicxml_file = partitura.EXAMPLE_MUSICXML\n\n\tnote_array = partitura.musicxml_to_notearray(my_musicxml_file)\n\tprint(note_array)\n\thorrizontal, vertical = get_note_density(note_array, 2, 1)\n\tprint(vertical, horrizontal)\n\t\"\"\"\n\n\tcardinalities = chord_cardinalites(note_array2chords(note_array))\n\tlength = int((cardinalities[-1][\"note_end\"] - window_size))\n\n\tvertical = np.zeros(length)\n\thorrizontal = np.zeros(length)\n\tfor index, window in enumerate(range(0, length, window_size)):\n\t\ttemp1 = np.intersect1d(cardinalities[ np.where( cardinalities[\"onset\"] < window + window_size )], cardinalities[ np.where( cardinalities[\"onset\"] >= window )])\n\t\ttemp2 = np.intersect1d(cardinalities[ np.where( cardinalities[\"onset\"] < window)], cardinalities[ np.where( cardinalities[\"note_end\"] >= window )])\n\t\tnumber_of_dif_onsets = len(list(map(np.unique, temp1[\"onset\"]))) + len(list(map(np.unique, temp2[\"onset\"])))\n\t\thorrizontal[index] = (len(temp1) +len(temp2))\n\t\tvertical[index] = (sum(temp1[\"cardinality\"]) + sum(temp2[\"cardinality\"]))/number_of_dif_onsets if number_of_dif_onsets !=0 else 0 \n\treturn horrizontal, vertical", "\n\ndef get_dssm(part, note_array, window_size=1, median_filter_size = (3, 21)):\n\t\"\"\"Perform the Density analysis and output density SSM.\n\n\tParameters\n\t----------\n\tpart : partitura.score.Part\n\t\tDummy attribute the partitura part.\n\tnote_array : structured array\n\t\tThe note array ordered on onsets.\n\tbar_in_beats : int\n\t\tThe duration of a bar in beats relevant to the time signature.\n\n\tReturns:\n\t--------\n\td_array : np.darray\n\t\tThe density array per note in the note array.\n\t\"\"\"\n\twindow = max(np.unique(note_array[\"ts_beats\"]))*window_size\n\th, v = get_note_density(note_array, window)\n\tfeatures = [\"dssm_novelty_curve\", \"normalize_horizontal_density\", \"normalize_vertical_density\", \"horizontal_density\", \"vertical_density\"]\n\tX = np.vstack((h, v)).T\n\trow_sums = X.sum(axis=1)\n\tX = X / (row_sums[:, np.newaxis] + 1e-6)\n\tnov = apply_ssm_analysis(X, gaussian_filter_size=2*window+1, median_filter_size = median_filter_size)\n\td_array = np.zeros((len(note_array), len(features)))\n\tfor i, value in enumerate(nov):\n\t\tw_start, w_end = i*window, (i+1)*window\n\t\td_array[np.where((note_array[\"onset_beat\"] >= w_start) & (note_array[\"onset_beat\"] < w_end))] = [value, X[i][0], X[i][1], h[i], v[i]]\n\treturn d_array, features", "\n\ndef get_bars_from_density(note_array, window_size):\n\t\"\"\"From Note Density get bar number where Horrizontal density is locally min and Vertical Density is locally max.\n\t\n\tParameters\n\t----------\n\tnote_array : structured array\n\t\tA structured array containing onsets, durations and pitch of notes extracted with partitura from a xml score.\n\twindow_size : int\n\t\tUsually the bar size relative to beats\n\tstep : int\n\t\tUsually a beat.\n\n\tReturns\n\t-------\n\thorrizontal : numpy array\n\t\tAn array containing the normalized note density of different note onsets\n\tvertical : numpy array\n\n\tExamples\n\t--------\n\timport partitura\n\tmy_musicxml_file = partitura.EXAMPLE_MUSICXML\n\n\tnote_array = partitura.musicxml_to_notearray(my_musicxml_file)\n\tprint(note_array)\n\thorrizontal, vertical = get_note_density(note_array, 2, 1)\n\tprint(vertical, horrizontal)\n\t\"\"\"\n\thorrizontal, vertical = get_note_density(note_array, window_size)\n\txn = argrelextrema(horrizontal, np.less)[0].tolist()\n\tyn = argrelextrema(vertical, np.greater)[0].tolist()\n\tx = list()\n\ty = list()\n\t# for n in range(-1, 2):\n\t# \tprint(n)\n\t# \tx += list(map(lambda i : i+n if i>n else 0, xn))\n\t# \ty += list(map(lambda i : i+n if i>n else 0, yn))\n\t# print(xn, yn)\n\tpivot_bars = np.intersect1d(xn, yn).tolist()\n\treturn list(set(pivot_bars))", "\n\n\n\n\nif __name__ == \"__main__\":\n\timport partitura\n\tmy_musicxml_file = partitura.EXAMPLE_MUSICXML\n\n\tnote_array = partitura.utils.ensure_notearray(partitura.load_musicxml(my_musicxml_file))\n\t# print(note_array)\n\t# horrizontal, vertical = get_note_density(note_array, 2, 1)\n\t# print(vertical, horrizontal)\n\t\n\timport os\n\timport matplotlib.pyplot as plt\n\tdirname = os.path.dirname(__file__)\n\tpar = lambda x: os.path.abspath(os.path.join(x, os.pardir))\n\tmy_musicxml_file = os.path.join(par(par(dirname)), \"samples\", \"xml\", \"mozart_piano_sonatas\", \"K279-1.musicxml\")\n\t\n\tpart = partitura.load_musicxml(my_musicxml_file)\n\tnote_array = partitura.utils.ensure_notearray(part)\n\td_array = get_dssm(part, note_array)", ""]}
{"filename": "vocsep/descriptors/utils/rests.py", "chunked_list": ["import numpy as np\n\ndef hammer(note_array, bar_in_beats):\n\t'''Get Onset vertical chords followed by a rest.'''    \n\thammer_pos = list()\n\tmax_onset = np.max(note_array[\"onset_beat\"])\n\tlength = max_onset + np.max(note_array[np.where(note_array[\"onset_beat\"] == max_onset)[0]][\"duration_beat\"]) - bar_in_beats\n\tstep = 1 \n\twindow_size = bar_in_beats\n\tvl_beat_pos = list()\n\tfor index in range(0, int(length)+1, window_size): \n\t\tchords = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index + window_size )], note_array[ np.where( note_array[\"onset_beat\"] >= index )]) \n\t\tif chords.size > 1:\n\t\t\tif np.all(chords['onset_beat']%1 == 0):\n\t\t\t\ton = np.max(chords[\"onset_beat\"])\n\t\t\t\tif on%bar_in_beats < bar_in_beats - 1:\n\t\t\t\t\thammer_pos.append(on)\n\treturn hammer_pos", "\n\ndef rest_after_onset(note_array, bar_in_beats):\n\t'''Get Onset of Rest at the last beat.'''    \n\trests = list()\n\tmax_onset = np.max(note_array[\"onset_beat\"])\n\tlength = max_onset + np.max(note_array[np.where(note_array[\"onset_beat\"] == max_onset)[0]][\"duration_beat\"]) - bar_in_beats\n\tstep = 1 \n\twindow_size = bar_in_beats\n\tvl_beat_pos = list()\n\tfor index in range(0, int(length)+1, window_size): \n\t\tchords = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index + window_size )], note_array[ np.where( note_array[\"onset_beat\"] >= index )]) \n\t\tonsets = chords[np.where(chords['onset_beat']%1 == 0)]\n\t\tif chords.size > 1 and onsets.size >1:\n\t\t\tif np.max(chords[\"onset_beat\"]) == np.max(onsets[\"onset_beat\"]):\n\t\t\t\ton = onsets[np.argmax(onsets[\"onset_beat\"])]\n\t\t\t\tif on[\"onset_beat\"]%bar_in_beats < bar_in_beats-1 and on[\"duration_beat\"] <= bar_in_beats-1:\n\t\t\t\t\trests.append(on[\"onset_beat\"])\n\treturn rests\t", "\n\n\n\nif __name__ == \"__main__\":\n\timport partitura\n\timport os\n\n\tdirname = os.path.dirname(__file__)\n\tpar = lambda x: os.path.abspath(os.path.join(x, os.pardir))\n\tmy_musicxml_file = os.path.join(par(par(dirname)), \"samples\", \"xml\", \"mozart_piano_sonatas\", \"K280-1.musicxml\")\n\t\n\tnote_array = partitura.utils.ensure_notearray(partitura.load_musicxml(my_musicxml_file))\n\tbar_in_beats = 3\n\tprint(list(map(lambda x: (int(x / bar_in_beats) + 1, x%bar_in_beats), rest_after_onset(note_array, bar_in_beats))))"]}
{"filename": "vocsep/descriptors/utils/score_meta.py", "chunked_list": ["from partitura import score\nimport itertools\n\nSCORE_META = {\n\t# \"Systems\" : score.System(number=1),\n\t\"DaCapo\" : score.DaCapo,\n\t\"Fine\" : score.Fine,\n\t\"Repeat\" : score.Repeat,\n\t\"Barline\" : score.Barline,\n\t\"Fermata\" : score.Fermata,", "\t\"Barline\" : score.Barline,\n\t\"Fermata\" : score.Fermata,\n\t\"TimeSignature\" : score.TimeSignature,\n\t# \"Tempo\" : score.Tempo,\n\t\"KeySignature\" : score.KeySignature,\n\t# \"Words\" : score.Words\n}\n\n\ndef iter_all_meta(part, meta):\n\treturn [part.beat_map(at.start.t).tolist() for at in part.iter_all(meta) if at.start.t !=0]", "\ndef iter_all_meta(part, meta):\n\treturn [part.beat_map(at.start.t).tolist() for at in part.iter_all(meta) if at.start.t !=0]\n\ndef get_score_meta(part):\n\t\"\"\"\n\tExport positions where score events (double lines, time signature changes, etc) occur.\n\n\tParameters\n\t----------\n\tpart : obj\n\t\tA partitura part object\n\tReturns\n\t-------\n\tpivatal_points : list()\n\t\tBars or beats in the score where changes in score meta data happen.\n\t\"\"\"\n\t# for key, meta in SCORE_META.items():\n\t# \tprint(\"{} is {} : \".format(key, iter_all_meta(part, meta)))\n\tif isinstance(part, list):\n\t\tpart = part[0]\n\tpivotal_points = sorted(list(set(\n\t\tlist(itertools.chain.from_iterable(\n\t\t\t[iter_all_meta(part, meta) for meta in SCORE_META.values()]\n\t\t\t)\n\t\t)))\n\t)\n\n\tpivotal_points = sorted(list(set(pivotal_points)))\n\treturn pivotal_points", "\n\n\nif __name__ == '__main__':\n    import partitura\n    import os\n\n    dirname = os.path.dirname(__file__)\n    par = lambda x: os.path.abspath(os.path.join(x, os.pardir))\n    my_kern_score = os.path.join(par(par(dirname)), \"samples\", \"xml\", \"k080-04.musicxml\")\n    part = partitura.load_musicxml(my_kern_score)[0]\n\n    print(get_score_meta(part, 4))"]}
{"filename": "vocsep/descriptors/utils/__init__.py", "chunked_list": ["from .int_vec import get_cadences\nfrom .harmonic_ssm import get_hssm\nfrom .note_density import get_dssm\nfrom .piano_roll import get_pssm\nfrom .score_meta import get_score_meta\nfrom .voice_leading import get_voice_leading\nfrom .rests import hammer, rest_after_onset\nfrom .note_features import get_general_features, get_pc_one_hot, get_input_irrelevant_features, get_voice_separation_features, get_panalysis_features, get_chord_analysis_features\nfrom .cadence_features import get_cad_features\nfrom .pitchdiff import get_pitchdiff", "from .cadence_features import get_cad_features\nfrom .pitchdiff import get_pitchdiff"]}
{"filename": "vocsep/descriptors/utils/note_features.py", "chunked_list": ["import numpy\nimport numpy as np\nfrom vocsep.utils.chord_representations import chord_to_intervalVector\nimport partitura as pt\nfrom typing import List, Tuple\n\n\n\n\nCHORDS = {", "\nCHORDS = {\n    \"M/m\": [0, 0, 1, 1, 1, 0],\n    \"sus4\": [0, 1, 0, 0, 2, 0],\n\t\"M7\": [0, 1, 2, 1, 1, 1],\n\t\"M7wo5\": [0, 1, 0, 1, 0, 1],\n\t\"Mmaj7\": [1, 0, 1, 2, 2, 0],\n\t\"Mmaj7maj9\" : [1, 2, 2, 2, 3, 0],\n\t\"M9\": [1, 1, 4, 1, 1, 2],\n\t\"M9wo5\": [1, 1, 2, 1, 0, 1],", "\t\"M9\": [1, 1, 4, 1, 1, 2],\n\t\"M9wo5\": [1, 1, 2, 1, 0, 1],\n\t\"m7\": [0, 1, 2, 1, 2, 0],\n\t\"m7wo5\": [0, 1, 1, 0, 1, 0],\n\t\"m9\": [1, 2, 2, 2, 3, 0],\n\t\"m9wo5\": [1, 2, 1, 1, 1, 0],\n\t\"m9wo7\": [1, 1, 1, 1, 2, 0],\n\t\"mmaj7\": [1, 0, 1, 3, 1, 0],\n\t\"Maug\": [0, 0, 0, 3, 0, 0],\n\t\"Maug7\": [1, 0, 1, 3, 1, 0],", "\t\"Maug\": [0, 0, 0, 3, 0, 0],\n\t\"Maug7\": [1, 0, 1, 3, 1, 0],\n\t\"mdim\": [0, 0, 2, 0, 0, 1],\n\t\"mdim7\": [0, 0, 4, 0, 0, 2]\n}\n\nNOTE_FEATURES = [\"int_vec1\", \"int_vec2\", \"int_vec3\", \"int_vec4\", \"int_vec5\", \"int_vec6\"] + \\\n    [\"interval\"+str(i) for i in range(13)] + list(CHORDS.keys()) + \\\n    [\"is_maj_triad\", \"is_pmaj_triad\", \"is_min_triad\", 'ped_note',\n     'hv_7', \"hv_5\", \"hv_3\", \"hv_1\", \"chord_has_2m\", \"chord_has_2M\"]", "    [\"is_maj_triad\", \"is_pmaj_triad\", \"is_min_triad\", 'ped_note',\n     'hv_7', \"hv_5\", \"hv_3\", \"hv_1\", \"chord_has_2m\", \"chord_has_2M\"]\n\n\ndef get_general_features(part, note_array):\n    \"\"\"\n    Create general features on the note level.\n\n    Parameters\n    ----------\n    note_array : numpy structured array\n        A part note array. Attention part must contain time signature information.\n\n    Returns\n    -------\n    feat_array : numpy structured array\n        A structured array of features. Each line corresponds to a note in the note array.\n    feature_fn : list\n        A list of the feature names.\n    \"\"\"\n    ca = np.zeros((len(note_array), len(NOTE_FEATURES)))\n    for i, n in enumerate(note_array):\n        n_onset = note_array[note_array[\"onset_beat\"] == n[\"onset_beat\"]]\n        n_dur = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (note_array[\"onset_beat\"] + note_array[\"duration_beat\"] > n[\"onset_beat\"]))]\n        n_cons = note_array[note_array[\"onset_beat\"] + note_array[\"duration_beat\"] == n[\"onset_beat\"]]\n        chord_pitch = np.hstack((n_onset[\"pitch\"], n_dur[\"pitch\"]))\n        int_vec, pc_class = chord_to_intervalVector(chord_pitch.tolist(), return_pc_class=True)\n        chords_features = {k: (1 if int_vec == v else 0) for k,v in CHORDS.items()}\n        pc_class_recentered = sorted(list(map(lambda x : x - min(pc_class), pc_class)))\n        is_maj_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 4, 7], [0, 5, 9], [0, 3, 8]] else 0\n        is_min_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 3, 7], [0, 5, 8], [0, 4, 9]] else 0\n        is_pmaj_triad = 1 if is_maj_triad and 4 in (chord_pitch - chord_pitch.min())%12 and 7 in (chord_pitch - chord_pitch.min())%12 else 0\n        ped_note = 1 if n[\"duration_beat\"] > n[\"ts_beats\"] else 0\n        hv_7 = 1 if (chord_pitch.max() - chord_pitch.min())%12 == 10 else 0\n        hv_5 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 7 else 0\n        hv_3 = 1 if (chord_pitch.max() - chord_pitch.min())%12 in [3, 4] else 0\n        hv_1 = 1 if (chord_pitch.max() - chord_pitch.min())%12 == 0 and chord_pitch.max() != chord_pitch.min() else 0\n        chord_has_2m = 1 if n[\"pitch\"] - chord_pitch.min() in [1, -1] else 0\n        chord_has_2M = 1 if n[\"pitch\"] - chord_pitch.min() in [2, -2] else 0\n        intervals = {\"interval\" + str(i): (1 if i in (n_cons[\"pitch\"] - n[\"pitch\"]) or -i in (\n                n_cons[\"pitch\"] - n[\"pitch\"]) else 0) for i in range(13)} if n_cons.size else {\"interval\" + str(i): 0 for i in range(13)}\n        ca[i] = np.array(int_vec + list(intervals.values()) + list(chords_features.values()) +\n                         [is_maj_triad, is_pmaj_triad, is_min_triad, ped_note, hv_7, hv_5, hv_3, hv_1, chord_has_2m,\n                          chord_has_2M])\n    fa, fnames = pt.musicanalysis.make_note_feats(part, \"all\")\n    out = np.hstack((fa, ca))\n    feature_fn = NOTE_FEATURES + fnames\n    return out, feature_fn", "\n\ndef get_pc_one_hot(part, note_array):\n    one_hot = np.zeros((len(note_array), 12))\n    idx = (np.arange(len(note_array)),np.remainder(note_array[\"pitch\"], 12))\n    one_hot[idx] = 1\n    return one_hot, [\"pc_{:02d}\".format(i) for i in range(12)]\n\n\ndef get_full_pitch_one_hot(part, note_array, piano_range = True):\n    one_hot = np.zeros((len(note_array), 127))\n    idx = (np.arange(len(note_array)),note_array[\"pitch\"])\n    one_hot[idx] = 1\n    if piano_range:\n        one_hot = one_hot[:, 21:109]\n    return one_hot, [\"pc_{:02d}\".format(i) for i in range(one_hot.shape[1])]", "\ndef get_full_pitch_one_hot(part, note_array, piano_range = True):\n    one_hot = np.zeros((len(note_array), 127))\n    idx = (np.arange(len(note_array)),note_array[\"pitch\"])\n    one_hot[idx] = 1\n    if piano_range:\n        one_hot = one_hot[:, 21:109]\n    return one_hot, [\"pc_{:02d}\".format(i) for i in range(one_hot.shape[1])]\n\n\ndef get_octave_one_hot(part, note_array):\n    one_hot = np.zeros((len(note_array), 10))\n    idx = (np.arange(len(note_array)), np.floor_divide(note_array[\"pitch\"], 12))\n    one_hot[idx] = 1\n    return one_hot, [\"octave_{:02d}\".format(i) for i in range(10)]", "\n\ndef get_octave_one_hot(part, note_array):\n    one_hot = np.zeros((len(note_array), 10))\n    idx = (np.arange(len(note_array)), np.floor_divide(note_array[\"pitch\"], 12))\n    one_hot[idx] = 1\n    return one_hot, [\"octave_{:02d}\".format(i) for i in range(10)]\n\n\ndef get_spelling_features(note_array):\n    step_onehot = np.zeros((len(note_array), 7))\n    alter_onehot = np.zeros((len(note_array), 7))\n    step_vocabulary = {\"C\": 0, \"D\": 1, \"E\": 2, \"F\": 3, \"G\": 4, \"A\": 5, \"B\": 6}\n    alter_vocabulary = {-3: 0, -2: 1, -1: 2, 0: 3, 1: 4, 2: 5, 3: 6}\n    step_idx = (np.arange(len(note_array)), np.vectorize(step_vocabulary.get)(note_array[\"step\"]))\n    alter_idx = (np.arange(len(note_array)), np.vectorize(alter_vocabulary.get)(note_array[\"alter\"]))\n    step_onehot[step_idx] = 1\n    alter_onehot[alter_idx] = 1\n    return np.hstack((step_onehot, alter_onehot)), [\"step_\" + s for s in step_vocabulary.keys()] + [\n        \"alter_\" + str(a) for a in alter_vocabulary.keys()]", "\ndef get_spelling_features(note_array):\n    step_onehot = np.zeros((len(note_array), 7))\n    alter_onehot = np.zeros((len(note_array), 7))\n    step_vocabulary = {\"C\": 0, \"D\": 1, \"E\": 2, \"F\": 3, \"G\": 4, \"A\": 5, \"B\": 6}\n    alter_vocabulary = {-3: 0, -2: 1, -1: 2, 0: 3, 1: 4, 2: 5, 3: 6}\n    step_idx = (np.arange(len(note_array)), np.vectorize(step_vocabulary.get)(note_array[\"step\"]))\n    alter_idx = (np.arange(len(note_array)), np.vectorize(alter_vocabulary.get)(note_array[\"alter\"]))\n    step_onehot[step_idx] = 1\n    alter_onehot[alter_idx] = 1\n    return np.hstack((step_onehot, alter_onehot)), [\"step_\" + s for s in step_vocabulary.keys()] + [\n        \"alter_\" + str(a) for a in alter_vocabulary.keys()]", "\n\n\ndef get_input_irrelevant_features(part) -> Tuple[np.ndarray, List]:\n    \"\"\"\n    Returns features irrelevant of the input being a score or a performance part.\n\n    Parameters\n    ----------\n    part: Part, PartGroup or PerformedPart\n\n    Returns\n    -------\n    out : np.ndarray\n    feature_fn : List\n    \"\"\"\n    if isinstance(part, pt.performance.PerformedPart):\n        perf_array = part.note_array()\n        x = perf_array[[\"onset_sec\", \"duration_sec\"]].astype([(\"onset_beat\", \"f4\"), (\"duration_beat\", \"f4\")])\n        note_array = np.lib.recfunctions.merge_arrays((perf_array, x))\n    else:\n        note_array = part.note_array()\n\n    ca = np.zeros((len(note_array), len(NOTE_FEATURES)))\n    for i, n in enumerate(note_array):\n        n_onset = note_array[note_array[\"onset_beat\"] == n[\"onset_beat\"]]\n        n_dur = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (\n                    note_array[\"onset_beat\"] + note_array[\"duration_beat\"] > n[\"onset_beat\"]))]\n        n_cons = note_array[note_array[\"onset_beat\"] + note_array[\"duration_beat\"] == n[\"onset_beat\"]]\n        chord_pitch = np.hstack((n_onset[\"pitch\"], n_dur[\"pitch\"]))\n        int_vec, pc_class = chord_to_intervalVector(chord_pitch.tolist(), return_pc_class=True)\n        chords_features = {k: (1 if int_vec == v else 0) for k, v in CHORDS.items()}\n        pc_class_recentered = sorted(list(map(lambda x: x - min(pc_class), pc_class)))\n        is_maj_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 4, 7], [0, 5, 9], [0, 3, 8]] else 0\n        is_min_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 3, 7], [0, 5, 8], [0, 4, 9]] else 0\n        is_pmaj_triad = 1 if is_maj_triad and 4 in (chord_pitch - chord_pitch.min()) % 12 and 7 in (\n                    chord_pitch - chord_pitch.min()) % 12 else 0\n        ped_note = 1 if n[\"duration_beat\"] > 4 else 0\n        hv_7 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 10 else 0\n        hv_5 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 7 else 0\n        hv_3 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 in [3, 4] else 0\n        hv_1 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 0 and chord_pitch.max() != chord_pitch.min() else 0\n        chord_has_2m = 1 if n[\"pitch\"] - chord_pitch.min() in [1, -1] else 0\n        chord_has_2M = 1 if n[\"pitch\"] - chord_pitch.min() in [2, -2] else 0\n        octave = int(n[\"pitch\"]/12)\n        intervals = {\"interval\" + str(i): (1 if i in (n_cons[\"pitch\"] - n[\"pitch\"]) or -i in (\n                n_cons[\"pitch\"] - n[\"pitch\"]) else 0) for i in range(13)} if n_cons.size else {\"interval\" + str(i): 0\n                                                                                               for i in range(13)}\n        ca[i] = np.array(int_vec + list(intervals.values()) + list(chords_features.values()) +\n                         [is_maj_triad, is_pmaj_triad, is_min_triad, ped_note, hv_7, hv_5, hv_3, hv_1, chord_has_2m,\n                          chord_has_2M, octave])\n    pc, pc_names = get_pc_one_hot(part, note_array)\n\n    dur_feats, dur_names = pt.musicanalysis.note_features.duration_feature(note_array, part)\n    on_feats, on_names = pt.musicanalysis.note_features.onset_feature(note_array, part)\n    pitch_feats, pitch_names = pt.musicanalysis.note_features.polynomial_pitch_feature(note_array, part)\n    out = np.hstack((on_feats, dur_feats, pitch_feats, ca, pc))\n    names = on_names + dur_names + pitch_names + NOTE_FEATURES + pc_names\n    return out, names", "\n\ndef get_voice_separation_features(part) -> Tuple[np.ndarray, List]:\n    \"\"\"\n    Returns features Voice Detection features.\n\n    Parameters\n    ----------\n    part: Part, PartGroup or PerformedPart\n\n    Returns\n    -------\n    out : np.ndarray\n    feature_fn : List\n    \"\"\"\n    if isinstance(part, pt.performance.PerformedPart):\n        perf_array = part.note_array()\n        x = perf_array[[\"onset_sec\", \"duration_sec\"]].astype([(\"onset_beat\", \"f4\"), (\"duration_beat\", \"f4\")])\n        note_array = np.lib.recfunctions.merge_arrays((perf_array, x))\n    elif isinstance(part, np.ndarray):\n        note_array = part\n        part = None\n    else:\n        note_array = part.note_array(include_time_signature=True)\n\n    # octave_oh, octave_names = get_octave_one_hot(part, note_array)\n    # pc_oh, pc_names = get_pc_one_hot(part, note_array)\n    # onset_feature = np.expand_dims(np.remainder(note_array[\"onset_beat\"], note_array[\"ts_beats\"]) / note_array[\"ts_beats\"], 1)\n    # on_feats, _ = pt.musicanalysis.note_features.onset_feature(note_array, part)\n    # duration_feature = np.expand_dims(np.remainder(note_array[\"duration_beat\"], note_array[\"ts_beats\"]) / note_array[\"ts_beats\"], 1)\n    # # new attempt! To delete in case\n    # # duration_feature = np.expand_dims(1- (1/(1+np.exp(-3*(note_array[\"duration_beat\"]/note_array[\"ts_beats\"])))-0.5)*2, 1)\n    # pitch_norm = np.expand_dims(note_array[\"pitch\"] / 127., 1)\n    # on_names = [\"barnorm_onset\", \"piecenorm_onset\"]\n    # dur_names = [\"barnorm_duration\"]\n    # pitch_names = [\"pitchnorm\"]\n    # names = on_names + dur_names + pitch_names + pc_names + octave_names\n    # out = np.hstack((onset_feature, np.expand_dims(on_feats[:, 1], 1), duration_feature, pitch_norm, pc_oh, octave_oh))\n\n    # octave_oh, octave_names = get_octave_one_hot(part, note_array)\n    # pitch_oh, pitch_names = get_full_pitch_one_hot(part, note_array)\n    # onset_feature = np.expand_dims(np.remainder(note_array[\"onset_beat\"], note_array[\"ts_beats\"]) / note_array[\"ts_beats\"], 1)\n    # on_feats, _ = pt.musicanalysis.note_features.onset_feature(note_array, part)\n    octave_oh, octave_names = get_octave_one_hot(part, note_array)\n    pc_oh, pc_names = get_pc_one_hot(part, note_array)\n    # duration_feature = np.expand_dims(1- (1/(1+np.exp(-3*(note_array[\"duration_beat\"]/note_array[\"ts_beats\"])))-0.5)*2, 1)\n    duration_feature = np.expand_dims(1 - np.tanh(note_array[\"duration_beat\"]/note_array[\"ts_beats\"]), 1)\n    dur_names = [\"bar_exp_duration\"]\n    # on_names = [\"barnorm_onset\", \"piecenorm_onset\"]\n    names = dur_names + pc_names + octave_names \n    out = np.hstack((duration_feature, pc_oh, octave_oh))\n    return out, names", "\n\ndef get_chord_analysis_features(part, one_hot=False) -> Tuple[np.ndarray, List]:\n    \"\"\"\n    Returns features for chord analysis.\n\n    Returns\n    -------\n    out : np.ndarray\n    feature_fn : List\n    \"\"\"\n    if isinstance(part, pt.performance.PerformedPart):\n        perf_array = part.note_array()\n        x = perf_array[[\"onset_sec\", \"duration_sec\"]].astype([(\"onset_beat\", \"f4\"), (\"duration_beat\", \"f4\")])\n        note_array = np.lib.recfunctions.merge_arrays((perf_array, x))\n    elif isinstance(part, np.ndarray):\n        note_array = part\n        part = None\n    else:\n        note_array = part.note_array(include_time_signature=True)\n    spelling_features, snames = get_spelling_features(note_array)\n    if one_hot:\n        octave_oh, octave_names = get_octave_one_hot(part, note_array)\n        pc_oh, pc_names = get_pc_one_hot(part, note_array)\n        pitch_features = np.hstack((pc_oh, octave_oh))\n\n        pitch_names = pc_names + octave_names\n    else:\n        pitch_features = np.expand_dims(note_array[\"pitch\"], 1)\n        pname = spelling_features[:, :7].argmax(axis=1)\n        alter = spelling_features[:, 7:].argmax(axis=1)\n        pitch_names = [\"pitch\"]\n    duration_feature = np.expand_dims(1 - np.tanh(note_array[\"duration_beat\"] / note_array[\"ts_beats\"]), 1)\n    dur_names = [\"bar_exp_duration\"]\n\n    # find min pitch per unique onset and set to one\n    min_max_pitch = np.zeros((len(note_array), 2))\n    min_max_pitch_names = [\"min_pitch\", \"max_pitch\"]\n    metrical_features = np.zeros((len(note_array), 4))\n    # is first note of bar\n    metrical_features[:, 0] = np.remainder(note_array[\"onset_beat\"], note_array[\"ts_beats\"]) == 0\n    # is integer beat\n    metrical_features[:, 1] = np.remainder(note_array[\"onset_beat\"], 1) == 0\n    # is half the ts_beats if ts_beats is even\n    metrical_features[:, 2] = np.remainder(note_array[\"onset_beat\"], note_array[\"ts_beats\"] / 2) == 0\n    metrical_names = [\"is_first_note_of_bar\", \"is_downbeat\", \"is_half_ts_beat\", \"time_until_next_onset\"]\n    # time until next onset\n    unique_onsets = np.unique(note_array[\"onset_beat\"])\n    time_until_next_onset = np.r_[np.diff(unique_onsets), 0.0]\n    time_until_next_onset = time_until_next_onset[np.searchsorted(unique_onsets, note_array[\"onset_beat\"])]\n    metrical_features[:, 3] = 1 - np.tanh(time_until_next_onset/note_array[\"ts_beats\"])\n    ca = np.zeros((len(note_array), len(NOTE_FEATURES)))\n    for i, n in enumerate(note_array):\n        n_onset = note_array[note_array[\"onset_beat\"] == n[\"onset_beat\"]]\n        n_dur = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (\n                    note_array[\"onset_beat\"] + note_array[\"duration_beat\"] > n[\"onset_beat\"]))]\n        n_cons = note_array[note_array[\"onset_beat\"] + note_array[\"duration_beat\"] == n[\"onset_beat\"]]\n        chord_pitch = np.hstack((n_onset[\"pitch\"], n_dur[\"pitch\"]))\n        min_max_pitch[i, 0] = np.min(chord_pitch) == n[\"pitch\"]\n        min_max_pitch[i, 1] = np.max(chord_pitch) == n[\"pitch\"]\n        int_vec, pc_class = chord_to_intervalVector(chord_pitch.tolist(), return_pc_class=True)\n        chords_features = {k: (1 if int_vec == v else 0) for k, v in CHORDS.items()}\n        pc_class_recentered = sorted(list(map(lambda x: x - min(pc_class), pc_class)))\n        is_maj_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 4, 7], [0, 5, 9], [0, 3, 8]] else 0\n        is_min_triad = 1 if chords_features[\"M/m\"] and pc_class_recentered in [[0, 3, 7], [0, 5, 8], [0, 4, 9]] else 0\n        is_pmaj_triad = 1 if is_maj_triad and 4 in (chord_pitch - chord_pitch.min()) % 12 and 7 in (\n                    chord_pitch - chord_pitch.min()) % 12 else 0\n        ped_note = 1 if n[\"duration_beat\"] > n[\"ts_beats\"] else 0\n        hv_7 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 10 else 0\n        hv_5 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 7 else 0\n        hv_3 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 in [3, 4] else 0\n        hv_1 = 1 if (chord_pitch.max() - chord_pitch.min()) % 12 == 0 and chord_pitch.max() != chord_pitch.min() else 0\n        chord_has_2m = 1 if n[\"pitch\"] - chord_pitch.min() in [1, -1] else 0\n        chord_has_2M = 1 if n[\"pitch\"] - chord_pitch.min() in [2, -2] else 0\n        intervals = {\"interval\" + str(i): (1 if i in (n_cons[\"pitch\"] - n[\"pitch\"]) or -i in (\n                n_cons[\"pitch\"] - n[\"pitch\"]) else 0) for i in range(13)} if n_cons.size else {\"interval\" + str(i): 0\n                                                                                               for i in range(13)}\n        ca[i] = np.array(int_vec + list(intervals.values()) + list(chords_features.values()) +\n                         [is_maj_triad, is_pmaj_triad, is_min_triad, ped_note, hv_7, hv_5, hv_3, hv_1, chord_has_2m,\n                          chord_has_2M])\n    out = np.hstack((pitch_features, spelling_features, duration_feature, ca, spelling_features, min_max_pitch, metrical_features))\n    names = pitch_names + snames + dur_names + NOTE_FEATURES + snames + min_max_pitch_names + metrical_names\n    return out, names", "\n\ndef get_panalysis_features(part) -> Tuple[np.ndarray, List]:\n    if isinstance(part, pt.performance.PerformedPart):\n        raise TypeError(\"PerformedPart is not supported\")\n    elif isinstance(part, np.ndarray):\n        note_array = part\n        if not np.all(np.isin([\"is_downbeat\", \"ts_beats\", \"ts_beat_type\"], note_array.dtype.names)):\n            raise ValueError(\"Not all field names are given\")\n    else:\n        note_array = part.note_array(include_time_signature=True, include_metrical_position=True)\n    octave_oh, octave_names = get_octave_one_hot(part, note_array)\n    pc_oh, pc_names = get_pc_one_hot(part, note_array)\n    # duration_feature = np.expand_dims(1- (1/(1+np.exp(-3*(note_array[\"duration_beat\"]/note_array[\"ts_beats\"])))-0.5)*2, 1)\n    duration_feature = np.expand_dims(1 - np.tanh(note_array[\"duration_beat\"] / note_array[\"ts_beats\"]), 1)\n    dur_names = [\"bar_exp_duration\"]\n    voice = np.expand_dims(note_array[\"voice\"], 1)\n    voice_names = [\"voice\"]\n    metrical = np.expand_dims(note_array[\"is_downbeat\"], 1)\n    metrical_names = [\"is_downbeat\"]\n\n    names = dur_names + pc_names + octave_names + voice_names + metrical_names\n    out = np.hstack((duration_feature, pc_oh, octave_oh, voice, metrical))\n    return out, names", ""]}
{"filename": "vocsep/descriptors/utils/harmonic_ssm.py", "chunked_list": ["from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom scipy import ndimage\nfrom scipy.signal import argrelextrema\nfrom vocsep.utils.chord_representations import chord_to_intervalVector\n\n\ndef intervalic_analysis(note_array, window_size=1):\n\t\"\"\"\n\tNote array 2 interval vectors with moving window.\n\n\tParameters\n\t----------\n\tnote_array : structured array.\n\t\tPartitura Part note array.\n\twindow_size: float\n\t\tExpressed in portion of the time signature nominator.\n\t\ti.e. ts 4/4 and window_size=0.5 means a window of 2 beats.\n\n\tReturns\n\t-------\n\tX : np.array\n\t\tAn array of interval vectors per window.\n\t\"\"\"\n\tX = np.zeros((len(note_array), 6))\n\tfor i, note in enumerate(note_array):\n\t\tindex = note[\"onset_beat\"]\n\t\twindow_length = window_size * note[\"ts_beats\"]\n\t\ttemp1 = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index + window_length )], note_array[ np.where( note_array[\"onset_beat\"] >= index )])\n\t\ttemp2 = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index)], note_array[ np.where( note_array[\"onset_beat\"]+note_array[\"duration_beat\"] >= index )])\n\t\ttnotes = np.union1d(temp1, temp2)\n\t\tX[i] = chord_to_intervalVector(tnotes[\"pitch\"])\n\treturn X", "\n\ndef analysis_to_SSM(note_array, window_size=1, return_analysis_matrix=False):\n\t\"\"\"\n\tPerform the intervalic analysis and output structure SSM.\n\n\n\tParameters\n\t----------\n\tnote_array : structured array\n\t\tThe note array ordered on onsets\n\twindow_size: float\n\t\tExpressed in portion of the time signature nominator.\n\t\ti.e. ts 4/4 and window_size=0.5 means a window of 2 beats.\n\treturn_analysis_matrix : bool\n\t\tIf True returns the interval analysis of the note array.\n\n\tReturns:\n\t--------\n\tS : np.array\n\t\tThe self_similarity matrix, i.e. cosine similarity of the PCA of X, \n\t\twhere X is the intervalic analysis of the note_array.\n\tX : np.array (optional)\n\t\tThe intervalic analysis of the note_array.\n\n\tExamples\n\t--------\n\timport partitura as pt\n\tnote_array = pt.load_score(pt.EXAMPLE_MUSICXML).note_array(include_time_signature=True)\n\tS = analysis_to_SSM(note_array)\n\t\"\"\"\n\n\tX = intervalic_analysis(note_array, window_size)\n\tS = cosine_similarity(X)\n\tif return_analysis_matrix:\n\t\treturn S, X\n\telse:\n\t\treturn S", "\n\ndef filter_diag_sm(S, L=30):\n\t\"\"\"Path smoothing of similarity matrix by forward filtering along main diagonal\n\n\tNotebook: C4/C4S2_SSM-PathEnhancement.ipynb\n\n\tArgs:\n\t\tS: Similarity matrix (SM)\n\t\tL: Length of filter\n\n\tReturns:\n\t\tS_L: Smoothed SM\n\t\"\"\"\n\tN = S.shape[0]\n\tM = S.shape[1]\n\tS_L = np.zeros((N, M))\n\tS_extend_L = np.zeros((N + L, M + L))\n\tS_extend_L[0:N, 0:M] = S\n\tfor pos in range(0, L):\n\t\tS_L = S_L + S_extend_L[pos:(N + pos), pos:(M + pos)]\n\tS_L = S_L / L\n\treturn S_L", "\n\ndef filter_diag_mult_sm(S, L=1, tempo_rel_set=np.asarray([1]), direction=0):\n\t\"\"\"Path smoothing of similarity matrix by filtering in forward or backward direction\n\talong various directions around main diagonal\n\tNote: Directions are simulated by resampling one axis using relative tempo values\n\n\tNotebook: C4/C4S2_SSM-PathEnhancement.ipynb\n\n\tArgs:\n\t\tS: Self-similarity matrix (SSM)\n\t\tL: Length of filter\n\t\ttempo_rel_set: Set of relative tempo values\n\t\tdirection: Direction of smoothing (0: forward; 1: backward)\n\n\tReturns:\n\t\tS_L_final: Smoothed SM\n\t\"\"\"\n\tN = S.shape[0]\n\tM = S.shape[1]\n\tnum = len(tempo_rel_set)\n\tS_L_final = np.zeros((N, M))\n\n\tfor s in range(0, num):\n\t\tM_ceil = int(np.ceil(M/tempo_rel_set[s]))\n\t\tresample = np.multiply(np.divide(np.arange(1, M_ceil+1), M_ceil), M)\n\t\tnp.around(resample, 0, resample)\n\t\tresample = resample - 1\n\t\tindex_resample = np.maximum(resample, np.zeros(len(resample))).astype(np.int64)\n\t\tS_resample = S[:, index_resample]\n\n\t\tS_L = np.zeros((N, M_ceil))\n\t\tS_extend_L = np.zeros((N + L, M_ceil + L))\n\n\t\t# Forward direction\n\t\tif direction == 0:\n\t\t\tS_extend_L[0:N, 0:M_ceil] = S_resample\n\t\t\tfor pos in range(0, L):\n\t\t\t\tS_L = S_L + S_extend_L[pos:(N + pos), pos:(M_ceil + pos)]\n\n\t\t# Backward direction\n\t\tif direction == 1:\n\t\t\tS_extend_L[L:(N+L), L:(M_ceil+L)] = S_resample\n\t\t\tfor pos in range(0, L):\n\t\t\t\tS_L = S_L + S_extend_L[(L-pos):(N + L - pos), (L-pos):(M_ceil + L - pos)]\n\n\t\tS_L = S_L / L\n\t\tresample = np.multiply(np.divide(np.arange(1, M+1), M), M_ceil)\n\t\tnp.around(resample, 0, resample)\n\t\tresample = resample - 1\n\t\tindex_resample = np.maximum(resample, np.zeros(len(resample))).astype(np.int64)\n\n\t\tS_resample_inv = S_L[:, index_resample]\n\t\tS_L_final = np.maximum(S_L_final, S_resample_inv)\n\n\treturn S_L_final", "\n\n\ndef forw_back_smoothing(S, L=20, tempo_rel_set=np.asarray([1])):\n\tS_forward = filter_diag_mult_sm(S, L, tempo_rel_set, direction=0)\n\tS_backward = filter_diag_mult_sm(S, L, tempo_rel_set, direction=1)\n\tS_final = np.maximum(S_forward, S_backward)\n\treturn S_final\n\n\ndef threshold_matrix(S, thresh, strategy='absolute', scale=False, penalty=0, binarize=False):\n\t\"\"\"Treshold matrix in a relative fashion\n\n\tNotebook: C4/C4/C4S2_SSM-Thresholding.ipynb\n\n\tArgs:\n\t\tS: Input matrix\n\t\tthresh: Treshold (meaning depends on strategy)\n\t\tstrategy: Thresholding strategy ('absolute', 'relative', 'local')\n\t\tscale: If scale=True, then scaling of positive values to range [0,1]\n\t\tpenalty: Set values below treshold to value specified\n\t\tbinarize: Binarizes final matrix (positive: 1; otherwise: 0)\n\t\tNote: Binarization is applied last (overriding other settings)\n\n\n\tReturns:\n\t\tS_thresh: Thresholded matrix\n\t\"\"\"\n\tif np.min(S) < 0:\n\t\traise Exception('All entries of the input matrix must be nonnegative')\n\n\tS_thresh = np.copy(S)\n\tN, M = S.shape\n\tnum_cells = N * M\n\n\tif strategy == 'absolute':\n\t\tthresh_abs = thresh\n\t\tS_thresh[S_thresh < thresh] = 0\n\n\tif strategy == 'relative':\n\t\tthresh_rel = thresh\n\t\tnum_cells_below_thresh = int(np.round(S_thresh.size*(1-thresh_rel)))\n\t\tif num_cells_below_thresh < num_cells:\n\t\t\tvalues_sorted = np.sort(S_thresh.flatten('F'))\n\t\t\tthresh_abs = values_sorted[num_cells_below_thresh]\n\t\t\tS_thresh[S_thresh < thresh_abs] = 0\n\t\telse:\n\t\t\tS_thresh = np.zeros([N, M])\n\n\tif scale:\n\t\tcell_val_zero = np.where(S_thresh == 0)\n\t\tcell_val_pos = np.where(S_thresh > 0)\n\t\tif len(cell_val_pos[0]) == 0:\n\t\t\tmin_value = 0\n\t\telse:\n\t\t\tmin_value = np.min(S_thresh[cell_val_pos])\n\t\tmax_value = np.max(S_thresh)\n\t\t# print('min_value = ', min_value, ', max_value = ', max_value)\n\t\tif max_value > min_value:\n\t\t\tS_thresh = np.divide((S_thresh - min_value), (max_value - min_value))\n\t\t\tif len(cell_val_zero[0]) > 0:\n\t\t\t\tS_thresh[cell_val_zero] = penalty\n\t\telse:\n\t\t\tprint('Condition max_value > min_value is voliated: output zero matrix')\n\n\tif binarize:\n\t\tS_thresh[S_thresh > 0] = 1\n\t\tS_thresh[S_thresh < 0] = 0\n\treturn S_thresh", "\n\ndef threshold_matrix(S, thresh, strategy='absolute', scale=False, penalty=0, binarize=False):\n\t\"\"\"Treshold matrix in a relative fashion\n\n\tNotebook: C4/C4/C4S2_SSM-Thresholding.ipynb\n\n\tArgs:\n\t\tS: Input matrix\n\t\tthresh: Treshold (meaning depends on strategy)\n\t\tstrategy: Thresholding strategy ('absolute', 'relative', 'local')\n\t\tscale: If scale=True, then scaling of positive values to range [0,1]\n\t\tpenalty: Set values below treshold to value specified\n\t\tbinarize: Binarizes final matrix (positive: 1; otherwise: 0)\n\t\tNote: Binarization is applied last (overriding other settings)\n\n\n\tReturns:\n\t\tS_thresh: Thresholded matrix\n\t\"\"\"\n\tif np.min(S) < 0:\n\t\traise Exception('All entries of the input matrix must be nonnegative')\n\n\tS_thresh = np.copy(S)\n\tN, M = S.shape\n\tnum_cells = N * M\n\n\tif strategy == 'absolute':\n\t\tthresh_abs = thresh\n\t\tS_thresh[S_thresh < thresh] = 0\n\n\tif strategy == 'relative':\n\t\tthresh_rel = thresh\n\t\tnum_cells_below_thresh = int(np.round(S_thresh.size*(1-thresh_rel)))\n\t\tif num_cells_below_thresh < num_cells:\n\t\t\tvalues_sorted = np.sort(S_thresh.flatten('F'))\n\t\t\tthresh_abs = values_sorted[num_cells_below_thresh]\n\t\t\tS_thresh[S_thresh < thresh_abs] = 0\n\t\telse:\n\t\t\tS_thresh = np.zeros([N, M])\n\n\tif scale:\n\t\tcell_val_zero = np.where(S_thresh == 0)\n\t\tcell_val_pos = np.where(S_thresh > 0)\n\t\tif len(cell_val_pos[0]) == 0:\n\t\t\tmin_value = 0\n\t\telse:\n\t\t\tmin_value = np.min(S_thresh[cell_val_pos])\n\t\tmax_value = np.max(S_thresh)\n\t\t# print('min_value = ', min_value, ', max_value = ', max_value)\n\t\tif max_value > min_value:\n\t\t\tS_thresh = np.divide((S_thresh - min_value), (max_value - min_value))\n\t\t\tif len(cell_val_zero[0]) > 0:\n\t\t\t\tS_thresh[cell_val_zero] = penalty\n\t\telse:\n\t\t\tprint('Condition max_value > min_value is voliated: output zero matrix')\n\n\tif binarize:\n\t\tS_thresh[S_thresh > 0] = 1\n\t\tS_thresh[S_thresh < 0] = 0\n\treturn S_thresh", "\n\ndef compute_kernel_checkerboard_gaussian(L, var=1, normalize=True):\n\t\"\"\"Compute Guassian-like checkerboard kernel [FMP, Section 4.4.1]\n\tSee also: https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/\n\n\tNotebook: C4/C4S4_NoveltySegmentation.ipynb\n\n\tArgs:\n\t\tL: Parameter specifying the kernel size M=2*L+1\n\t\tvar: Variance parameter determing the tapering (epsilon)\n\n\tReturns:\n\t\tkernel: Kernel matrix of size M x M\n\t\"\"\"\n\ttaper = np.sqrt(1/2) / (L * var)\n\taxis = np.arange(-L, L+1)\n\tgaussian1D = np.exp(-taper**2 * (axis**2))\n\tgaussian2D = np.outer(gaussian1D, gaussian1D)\n\tkernel_box = np.outer(np.sign(axis), np.sign(axis))\n\tkernel = kernel_box * gaussian2D\n\tif normalize:\n\t\tkernel = kernel / np.sum(np.abs(kernel))\n\treturn kernel", "\n\ndef compute_novelty_ssm(S, kernel=None, L=10, var=0.5, exclude=False):\n\t\"\"\"Compute novelty function from SSM [FMP, Section 4.4.1]\n\n\tNotebook: C4/C4S4_NoveltySegmentation.ipynb\n\n\tArgs:\n\t\tS: SSM\n\t\tkernel: Checkerboard kernel (if kernel==None, it will be computed)\n\t\tL: Parameter specifying the kernel size M=2*L+1\n\t\tvar: Variance parameter determing the tapering (epsilon)\n\t\texclude: Sets the first L and last L values of novelty function to zero\n\n\tReturns:\n\t\tnov: Novelty function\n\t\"\"\"\n\tif kernel is None:\n\t\tkernel = compute_kernel_checkerboard_gaussian(L=L, var=var)\n\tN = S.shape[0]\n\tM = 2*L + 1\n\tnov = np.zeros(N)\n\t# np.pad does not work with numba/jit\n\tS_padded = np.pad(S, L, mode='constant')\n\n\tfor n in range(N):\n\t\t# Does not work with numba/jit\n\t\tnov[n] = np.sum(S_padded[n:n+M, n:n+M] * kernel)\n\tif exclude:\n\t\tright = np.min([L, N])\n\t\tleft = np.max([0, N-L])\n\t\tnov[0:right] = 0\n\t\tnov[left:N] = 0\n\n\treturn nov", "\n\ndef compute_time_lag_representation(S, circular=True):\n\t\"\"\"Computation of (circular) time-lag representation\n\n\tNotebook: C4/C4S4_StructureFeature.ipynb\n\n\tArgs:\n\t\tS: Self-similarity matrix\n\t\tcircular: computes circular version\n\n\tReturns:\n\t\tL: (Circular) time-lag representation of S\n\t\"\"\"\n\tN = S.shape[0]\n\tif circular:\n\t\tL = np.zeros((N, N))\n\t\tfor n in range(N):\n\t\t\tL[:, n] = np.roll(S[:, n], -n)\n\telse:\n\t\tL = np.zeros((2*N-1, N))\n\t\tfor n in range(N):\n\t\t\tL[((N-1)-n):((2*N)-1-n), n] = S[:, n]\n\treturn L", "\ndef novelty_structure_feature(L, padding=True):\n\t\"\"\"Computation of the novelty function from a circular time-lag representation\n\n\tNotebook: C4/C4S4_StructureFeature.ipynb\n\n\tArgs:\n\t\tL: Circular time-lag representation\n\t\tpadding: Padding the result with the value zero\n\n\tReturns:\n\t\tnov: Novelty function\n\t\"\"\"\n\tN = L.shape[0]\n\tif padding:\n\t\tnov = np.zeros(N)\n\telse:\n\t\tnov = np.zeros(N-1)\n\tfor n in range(N-1):\n\t\tnov[n] = np.linalg.norm(L[:, n+1] - L[:, n])\n\n\tnov = (nov - np.min(nov)) / (np.max(nov) - np.min(nov) + 1e-6)\n\treturn nov", "\n\ndef peak_detection(nov, thresh = 0.3, window=10):\n\tpeaks = list()\n\tnov = np.where(nov > thresh, nov, 0)\n\tfor i in range(nov.shape[0]-window):\n\t\tmatrix = nov[i : i+window]\n\t\tlocal = argrelextrema(matrix, np.greater)[0]\n\t\tif local.size > 1:\n\t\t\tpeaks.append(np.argmax(nov[peaks]) + i )\n\t\telif local.size == 1:\n\t\t\tpeaks.append(local[0]+i)\n\t\telse:\n\t\t\tpass\n\treturn list(set(peaks))", "\n\ndef get_hssm(part, note_array, window_size=1, thresh=0.15, median_filter_size = (3, 21), gaussian_filter_size=8):\n\t\"\"\"Return the harmonic (Int_Vec) SSM and a set of lines within it.\n\n\tParameters\n\t----------\n\tpart : partitura.score.Part\n\t\tA partitura part, in this function it is a dummy variable can also be given empty.\n\tnote_array : structured array\n\t\tThe notes of the score with their corresponding onset, duration and pitch.\n\tstep : int\n\t\tThe step for the interval vector window, also reflects the information of the SSM pixels.\n\tthresh : float\n\t\tThe threshold for smoothing and thresholding of the cosine SSM.\n\tline_length : int\n\t\tDetect lines from above this pixel length (propotional to the step, i.e. 1 beat )\n\tline_gap : int\n\t\tDetect lines that have above this distance between them.\n\treg_E : float\n\t\tParameter of tensor PCA.\n\tlearning_rate : float\n\t\tParameter of tensor PCA.\n\tn_iter_max : float\n\t\tParameter of tensor PCA.    \n\n\tReturns\n\t-------\n\tS_smooth : int\n\t\tThe SSM after smoothing and thresholding.\n\tline : list\n\t\tThe set of lines detected in the SSM with probabilistic hough transform.\n\n\tExample\n\t-------\n\timport partitura as pt\n\n\tnote_array = pt.load_score(pt.EXAMPLE_MUSICXML).note_array(include_time_signature=True)\n\thssm = get_hssm(note_array)\n\t\"\"\"\n\tS_cos = analysis_to_SSM(note_array, window_size)\n\tS_smooth = threshold_matrix(forw_back_smoothing(S_cos), thresh=thresh, strategy='relative')\n\tL = compute_time_lag_representation(S_smooth)\n\tL_filter = ndimage.median_filter(L, median_filter_size)\n\tL_filter = ndimage.gaussian_filter(L_filter, gaussian_filter_size)\n\tnov = novelty_structure_feature(L_filter)\n\treturn nov", "\n\ndef apply_ssm_analysis(X, median_filter_size = (3, 21), gaussian_filter_size=6):\n\tS_cos = cosine_similarity(X)\n\tS_smooth = threshold_matrix(forw_back_smoothing(S_cos), thresh=0.05, strategy='relative')\n\tL = compute_time_lag_representation(S_smooth)\n\tL_filter = ndimage.median_filter(L, median_filter_size)\n\tL_filter = ndimage.gaussian_filter(L_filter, gaussian_filter_size)\n\tnov = novelty_structure_feature(L_filter)\n\treturn nov", "\n\n\n\n"]}
{"filename": "vocsep/descriptors/utils/pitchdiff.py", "chunked_list": ["import numpy as np\nfrom partitura.musicanalysis import estimate_voices\n\n\ndef get_pitchdiff(part, note_array):\n    features = [\"estimated_voices\", \"pitch_diff_left\", \"pitch_diff_right\"]\n    voices = estimate_voices(note_array)\n    note_array[\"voice\"] = voices\n    pitchdiff = np.zeros(len(note_array))\n\n    for voice in np.unique(voices):\n        idx = np.where(note_array[\"voice\"] == voice)\n        notes = note_array[idx][\"pitch\"]\n        pitchdiff[idx] = np.r_[notes[1:], notes[-1]] - notes\n    pitchdiff = np.expand_dims(pitchdiff, axis=1)\n    out = np.hstack((np.expand_dims(voices, axis=1), pitchdiff, -pitchdiff))\n    return out, features", ""]}
{"filename": "vocsep/descriptors/utils/cadence_features.py", "chunked_list": ["from vocsep.utils.chord_representations import chord_to_intervalVector\nimport numpy as np\n\n\ndef get_cad_features(part, note_array):\n    \"\"\"\n    Create cadence relevant features on the note level.\n\n    Parameters\n    ----------\n    part : partitura.score.Part\n        In this function a dummy variable. It can be given empty.\n    note_array : numpy structured array\n        A part note array. Attention part must contain time signature information.\n\n    Returns\n    -------\n    feat_array : numpy structured array\n        A structured array of features. Each line corresponds to a note in the note array.\n    feature_fn : list\n        A list of the feature names.\n    \"\"\"\n\n    features = list()\n    bass_voice = note_array[\"voice\"].max() if note_array[\"voice\" == note_array[\"voice\"].max()][\"pitch\"].mean() < note_array[\"voice\" == note_array[\"voice\"].min()][\"pitch\"].mean() else note_array[\"voice\"].min()\n    high_voice = note_array[\"voice\"].min() if note_array[\"voice\" == note_array[\"voice\"].min()][\"pitch\"].mean() > \\\n                                              note_array[\"voice\" == note_array[\"voice\"].max()][\"pitch\"].mean() else note_array[\"voice\"].max()\n    for i, n in enumerate(note_array):\n        d = {}\n        n_onset = note_array[note_array[\"onset_beat\"] == n[\"onset_beat\"]]\n        n_dur = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (note_array[\"onset_beat\"] + note_array[\"duration_beat\"] > n[\"onset_beat\"]))]\n        chord_pitch = np.hstack((n_onset[\"pitch\"], n_dur[\"pitch\"]))\n        int_vec, pc_class = chord_to_intervalVector(chord_pitch.tolist(), return_pc_class=True)\n        pc_class_recentered = sorted(list(map(lambda x: x - min(pc_class), pc_class)))\n        maj_int_vecs = [[0, 0, 1, 1, 1, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0]]\n        prev_4beats = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (note_array[\"onset_beat\"] > n[\"onset_beat\"] - 4))][\n                          \"pitch\"] % 12\n        prev_8beats = note_array[np.where((note_array[\"onset_beat\"] < n[\"onset_beat\"]) & (note_array[\"onset_beat\"] > n[\"onset_beat\"] - 8))][\n                          \"pitch\"] % 12\n        maj_pcs = [[0, 4, 7], [0, 5, 9], [0, 3, 8], [0, 4], [0, 8], [0, 7], [0, 5]]\n        scale = [2, 3, 5, 7, 8, 11] if (n[\"pitch\"] + 3) in chord_pitch % 12 else [2, 4, 5, 7, 9, 11]\n        v7 = [[0, 1, 2, 1, 1, 1], [0, 1, 0, 1, 0, 1], [0, 1, 0, 0, 0, 0]]\n        next_voice_notes = note_array[np.where((note_array[\"voice\"] == n[\"voice\"]) & (note_array[\"onset_beat\"] > n[\"onset_beat\"]))]\n        prev_voice_notes = note_array[np.where((note_array[\"voice\"] == n[\"voice\"]) & (note_array[\"onset_beat\"] < n[\"onset_beat\"]))]\n        prev_voice_pitch = prev_voice_notes[prev_voice_notes[\"onset_beat\"] == prev_voice_notes[\"onset_beat\"].max()][\"pitch\"] if prev_voice_notes.size else 0\n        # start Z features\n        d[\"perfect_triad\"] = int_vec in maj_int_vecs\n        d[\"perfect_major_triad\"] = d[\"perfect_triad\"] and pc_class_recentered in maj_pcs\n        d[\"is_sus4\"] = int_vec == [0, 1, 0, 0, 2, 0] or pc_class_recentered == [0, 5]\n        d[\"in_perfect_triad_or_sus4\"] = d[\"perfect_triad\"] or d[\"is_sus4\"]\n        d[\"highest_is_3\"] = (chord_pitch.max() - chord_pitch.min()) % 12 in [3, 4]\n        d[\"highest_is_1\"] = (chord_pitch.max() - chord_pitch.min()) % 12 == 0 and chord_pitch.max() != chord_pitch.min()\n\n        d[\"bass_compatible_with_I\"] = (n[\"pitch\"] + 5) % 12 in prev_4beats and (n[\"pitch\"] + 11) % 12 in prev_4beats if prev_4beats.size else False\n        d[\"bass_compatible_with_I_scale\"] = all([(n[\"pitch\"] + ni) % 12 in prev_8beats for ni in scale]) if prev_8beats.size else False\n        d[\"one_comes_from_7\"] = 11 in (prev_voice_pitch - chord_pitch.min())%12 and (\n                n[\"pitch\"] - chord_pitch.min())%12 == 0 if prev_voice_notes.size and len(chord_pitch)>1 else False\n        d[\"one_comes_from_1\"] = 0 in (prev_voice_pitch - chord_pitch.min())%12 and (\n                    n[\"pitch\"] - chord_pitch.min())%12 == 0 if prev_voice_notes.size and len(chord_pitch)>1 else False\n        d[\"one_comes_from_2\"] = 2 in (prev_voice_pitch - chord_pitch.min()) % 12 and (\n                n[\"pitch\"] - chord_pitch.min())%12 == 0 if prev_voice_notes.size and len(chord_pitch)>1 else False\n        d[\"three_comes_from_4\"] = 5 in (prev_voice_pitch - chord_pitch.min()) % 12 and (\n                n[\"pitch\"] - chord_pitch.min())%12 in [3, 4] if prev_voice_notes.size else False\n        d[\"five_comes_from_5\"] = 7 in (prev_voice_pitch - chord_pitch.min()) % 12 and (\n                n[\"pitch\"] - chord_pitch.min()) % 12 == 7 if prev_voice_notes.size else False\n\n        # Make R features\n        d[\"strong_beat\"] = (n[\"ts_beats\"] == 4 and n[\"onset_beat\"] % 2 == 0) or (n[\"onset_beat\"] % n['ts_beats'] == 0) # to debug\n        d[\"sustained_note\"] = n_dur.size > 0\n        if next_voice_notes.size:\n            d[\"rest_highest\"] = n[\"voice\"] == high_voice and next_voice_notes[\"onset_beat\"].min() > n[\"onset_beat\"] + n[\"duration_beat\"]\n            d[\"rest_lowest\"] = n[\"voice\"] == bass_voice and next_voice_notes[\"onset_beat\"].min() > n[\"onset_beat\"] + n[\"duration_beat\"]\n            d[\"rest_middle\"] = n[\"voice\"] != high_voice and n[\"voice\"] != bass_voice and next_voice_notes[\"onset_beat\"].min() > n[\n                \"onset_beat\"] + n[\"duration_beat\"]\n            d[\"voice_ends\"] = False\n        else:\n            d[\"rest_highest\"] = False\n            d[\"rest_lowest\"] = False\n            d[\"rest_middle\"] = False\n            d[\"voice_ends\"] = True\n\n        # start Y features\n        d[\"v7\"] = int_vec in v7\n        d[\"v7-3\"] = int_vec in v7 and 4 in pc_class_recentered\n        d[\"has_7\"] = 10 in pc_class_recentered\n        d[\"has_9\"] = 1 in pc_class_recentered or 2 in pc_class_recentered\n        d[\"bass_voice\"] = n[\"voice\"] == bass_voice\n        if prev_voice_notes.size:\n            x = prev_voice_notes[prev_voice_notes[\"onset_beat\"] == prev_voice_notes[\"onset_beat\"].max()][\"pitch\"]\n            d[\"bass_moves_chromatic\"] = n[\"voice\"] == bass_voice and (1 in x - n[\"pitch\"] or -1 in x-n[\"pitch\"])\n            d[\"bass_moves_octave\"] = n[\"voice\"] == bass_voice and (12 in x - n[\"pitch\"] or -12 in x - n[\"pitch\"])\n            d[\"bass_compatible_v-i\"] = n[\"voice\"] == bass_voice and (7 in x - n[\"pitch\"] or -5 in x - n[\"pitch\"])\n            d[\"bass_compatible_i-v\"] = n[\"voice\"] == bass_voice and (-7 in x - n[\"pitch\"] or 5 in x - n[\"pitch\"])\n        # X features\n            d[\"bass_moves_2M\"] = n[\"voice\"] == bass_voice and (2 in x - n[\"pitch\"] or -2 in x - n[\"pitch\"])\n        else:\n            d[\"bass_moves_chromatic\"] = d[\"bass_moves_octave\"] = d[\"bass_compatible_v-i\"] = d[\"bass_compatible_i-v\"] = d[\"bass_moves_2M\"] = False\n        features.append(tuple(d.values()))\n    feat_array = np.array(features)\n    feature_fn = list(d.keys())\n    return feat_array, feature_fn", "\n"]}
{"filename": "vocsep/descriptors/utils/voice_leading.py", "chunked_list": ["import numpy as np\nfrom vocsep.utils.chord_representations import chord_to_intervalVector\n\n\nINTVEC_DICT = { \n    \"V/I maj\" : [1, 2, 2, 2, 3, 0],\n    \"V/I min\": [2, 1, 2, 3, 2, 0],\n    \"V7/I maj\" : [2, 3, 3, 2, 4, 1],\n    \"V7/I min\" : [2, 3, 3, 3, 3, 1],\n    \"V9/I min\" : [3, 3, 5, 4, 4, 2]", "    \"V7/I min\" : [2, 3, 3, 3, 3, 1],\n    \"V9/I min\" : [3, 3, 5, 4, 4, 2]\n}\n\nINTVEC_DOM = { \n    \"V\" : [0, 1, 1, 1, 0, 0],\n    \"V7\": [0, 1, 2, 1, 1, 1],\n    \"V9\" : [1, 1, 4, 1, 1, 2]\n}\n", "}\n\n\n\ndef h_cad_onset(tnotes, bar_in_beats, index):\n\tonset = index + bar_in_beats\n\tind = np.where((tnotes[\"onset_beat\"] <= onset ) & (tnotes[\"onset_beat\"] + tnotes[\"duration_beat\"] > onset))[0]\n\tif ind.size > 1:\n\t\tcp = np.argsort(tnotes[ind][\"pitch\"])[::-1]\n\t\tif chord_to_intervalVector(cp) in INTVEC_DOM.values():\n\t\t\trp = [p for p in cp[:-1] if (p - cp[-1])%12 in [4, 7]]\n\t\telse:\n\t\t\treturn None\n\t\tif len(rp)==2:\n\t\t\tfor pp in rp:\n\t\t\t\tnon = cp[np.where(cp[\"pitch\"] == pp)][\"onset\"]\t\n\t\t\t\tpitches = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], non) == True)][\"pitch\"]\n\t\t\t\tif 1 in list(map(lambda x : x - pp, pitches)):\n\t\t\t\t\treturn onset\n\treturn None", "\n\ndef p_cad_bass(tnotes, bar_in_beats, index):\n\tif index%bar_in_beats != 0:\n\t\tonset = index - index%bar_in_beats + bar_in_beats\n\telse:\n\t\treturn None\n\tind = np.where(tnotes[\"onset_beat\"] == onset)[0]\n\tif ind.size > 0:\n\t\tbp = np.min(tnotes[ind][\"pitch\"])\n\t\tpnotes = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], onset) == True)]\n\t\tif pnotes.size > 0:\n\t\t\t# Search for V - I in first onset of measure.\n\t\t\tlow = np.min(pnotes[\"pitch\"])\n\t\t\tif low - bp in [7, -5]:\t\n\t\t\t\t# Now we search for possible voice leading patterns.\n\t\t\t\tpreonset = pnotes[np.argmin(pnotes[\"pitch\"])][\"onset_beat\"]\n\t\t\t\tvl = tnotes[np.where((tnotes[\"onset_beat\"] >= preonset) & (tnotes[\"onset_beat\"] < onset))]\n\t\t\t\tvl = vl[np.where(np.isclose(vl[\"onset_beat\"], np.max(vl[\"onset_beat\"])) == True)]\n\t\t\t\tif (np.max(vl[\"pitch\"]) - low)%12 in [4, 7, 10]:\t\t\t\t\t\n\t\t\t\t\tif vl.size > 1:\n\t\t\t\t\t\ti = np.argmax(vl[\"pitch\"])\n\t\t\t\t\t\trr = vl[\"onset_beat\"][i] + vl[\"duration_beat\"][i]\n\t\t\t\t\telif vl.size == 1:\n\t\t\t\t\t\trr = vl[\"onset_beat\"] + vl[\"duration_beat\"]\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn None\n\t\t\t\t\tif tnotes[np.where(np.isclose(tnotes[\"onset_beat\"], rr) == True)].size > 0:\n\t\t\t\t\t\tif np.max(vl[\"pitch\"]) - np.max(tnotes[np.where(np.isclose(tnotes[\"onset_beat\"], rr) == True)][\"pitch\"]) in [1, 2, -1, 0]:\n\t\t\t\t\t\t\treturn onset\n\treturn None\t", "\n\ndef vl3cons(tnotes, bar_in_beats, index):\n\tonset = index + bar_in_beats\n\tVOICE_LEADING_INT = [[-3, 1], [3, -2], [-2, 1]]\n\tind = np.where(tnotes[\"onset_beat\"] == onset)[0]\n\tif ind.size > 1:\n\t\tcmin = np.min(tnotes[ind][\"pitch\"])\n\t\tcmax = np.max(tnotes[ind][\"pitch\"])\n\t\tpnotes = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], onset) == True)]\n\t\t# pnotes = tnotes[np.where(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] == onset)]\n\t\tif pnotes.size > 0:\n\t\t\tpmax = np.max(pnotes[\"pitch\"])\n\t\t\tponset = pnotes[np.where(pnotes[\"pitch\"] == pmax)][\"onset_beat\"][0]\n\t\telse:\n\t\t\treturn None\n\t\tppnotes = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], ponset) == True)]\n\t\tif ppnotes.size > 0:\n\t\t\tppmax = np.max(ppnotes[\"pitch\"])\n\t\t\tdc = [pmax - ppmax, cmax - pmax]\t\t\n\t\t\tif dc in VOICE_LEADING_INT and cmin%12==cmax%12:\n\t\t\t\treturn onset\n\treturn None", "\n\n\ndef p_cad_delay(tnotes, bar_in_beats, index):\n\tonset = index + bar_in_beats\n\tind = np.where(tnotes[\"onset_beat\"] == onset)[0]\n\tif ind.size > 1:\n\t\tcp = np.argsort(tnotes[ind][\"pitch\"])[::-1]\n\t\t# Search for particular voicing\n\t\tif cp[0] - cp[1] in [3, 4, 15, 16] and cp[1]%12 == cp[-1]%12:\n\t\t\tpnotes = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], onset) == True)]\n\t\t\tif pnotes.size > 1:\n\t\t\t\tpp = np.argsort(pnotes[\"pitch\"])[::-1]\n\t\t\t\t# Search for Voice Leading schema\n\t\t\t\tif pp[0] - cp[0] in [1, 2] and pp[1] - cp[1] == 2:\n\t\t\t\t\tminonset = np.min(pnotes[\"onset\"])\n\t\t\t\t\t# Search for \n\t\t\t\t\tif (cp[1] - np.min(tnotes[np.where(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] == minonset)][\"pitch\"]))%12 in [7, 5]:\n\t\t\t\t\t\treturn onset\n\treturn None", "\n\ndef cad_delay(tnotes, bar_in_beats, index):\n\tonset = index + bar_in_beats\n\tind = np.where(tnotes[\"onset_beat\"] == onset)[0]\n\tif ind.size > 0:\n\t\tmax_pitch = np.max(tnotes[ind][\"pitch\"])  \n\telse: \n\t\treturn None\n\tbnotes = tnotes[np.where((tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] > onset) & (tnotes[\"onset_beat\"] < onset))] \n\tif bnotes.size != 0 :\n\t\tlow_note = bnotes[np.argmin(bnotes[\"pitch\"])]\n\t\tmin_pitch = low_note[\"pitch\"]\n\t\tif max_pitch%12 - min_pitch%12 == 0:\n\t\t\thp = tnotes[np.where(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] == onset)]\n\t\t\tlp = tnotes[np.where(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] == low_note[\"onset_beat\"])]\n\t\t\tif hp.size > 0 and lp.size > 0:\n\t\t\t\thp = np.max(hp[\"pitch\"])\n\t\t\t\tlp = np.min(lp[\"pitch\"])\n\t\t\t\tif hp - max_pitch in [2, -1] and lp - min_pitch in [7, -5]:\n\t\t\t\t\treturn onset\n\treturn None", "\n\ndef cad_onset(tnotes, bar_in_beats, index):\n\tonset = index + bar_in_beats - 1\n\tind = np.where(tnotes[\"onset_beat\"] == onset)[0]\n\tif ind.size > 1:\n\t\tlast_onset_notes = tnotes[ind]\n\t\tmax_pitch = np.max(last_onset_notes[\"pitch\"])\n\t\tmin_pitch = np.min(last_onset_notes[\"pitch\"])\n\t\tif max_pitch%12 - min_pitch%12 == 0:\n\t\t\tp = tnotes[np.where(np.isclose(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"], onset) == True)]\n\t\t\tif p.size > 0:\n\t\t\t\thp = np.max(p[\"pitch\"])\n\t\t\t\tlp = np.min(p[\"pitch\"])\n\t\t\t\tif hp - max_pitch == 2 and lp - min_pitch in [7, -5] and p[np.where(p[\"pitch\"] == lp)][\"onset_beat\"][0]%1==0:\t\t\t\t\t\n\t\t\t\t\treturn onset\n\t\t\t\telif hp - max_pitch in [7, -5] and p[np.where(p[\"pitch\"] == hp)][\"onset_beat\"][0]%1==0:\n\t\t\t\t\ttemp1 = np.intersect1d(tnotes[ np.where( tnotes[\"onset_beat\"] <= onset )], tnotes[ np.where( tnotes[\"onset_beat\"] >= onset - 1 )]) \n\t\t\t\t\ttemp2 = np.intersect1d(tnotes[ np.where( tnotes[\"onset_beat\"] < onset - 1)], tnotes[ np.where( tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] >= onset-1 )])\n\t\t\t\t\ttnotes = np.union1d(temp1, temp2)\n\t\t\t\t\tif chord_to_intervalVector in INTVEC_DICT.values():\t\t\t\t\t\t\n\t\t\t\t\t\treturn onset\n\t\t\t\telif vl4cons(tnotes, p, hp, max_pitch):\t\t\t\t\t\n\t\t\t\t\treturn onset\n\t\t\t\telse:\n\t\t\t\t\tpass\n\treturn None", "\n\ndef loop_cons_notes(tnotes, p, hp, max_pitch, rel_int):\n\tif hp - max_pitch == rel_int:\n\t\tonset = p[np.where(p[\"pitch\"] == hp)][\"onset_beat\"]\n\t\tp = tnotes[np.where(tnotes[\"onset_beat\"]+tnotes[\"duration_beat\"] == onset)]\n\t\tif p.size > 0:\n\t\t\tmax_pitch = hp\n\t\t\thp = np.max(p[\"pitch\"])\n\t\t\treturn p, hp, max_pitch\n\telse :\n\t\treturn None", "\ndef vl4cons(tnotes, p, hp, max_pitch, rel = [-1, 1, 2]):\n\tfor rel_int in rel:\n\t\tx = loop_cons_notes(tnotes, p, hp, max_pitch, rel_int)\n\t\tif x != None:\n\t\t\tp, hp, max_pitch = x\n\t\telse:\n\t\t\treturn False\n\treturn True\t\t\t\n", "\n\n\n\n\ndef get_voice_leading(note_array, bar_in_beats):\n\t'''\n\tDoes the intervalic analysis of a piece.\n\t\n\tParameters:\n\t-----------\n\tnote_array : Structured array\n\t\tA partitura note array from one or more parts\n\tbar_in_beats : float\n\t\tThe bar duration in beats\n\t\n\tReturns:\n\t--------\n\tvl_beat_pos : list\n\t\tOnsets in Beats of Possible Voice Leading Occurences\n\t'''    \n\t# standard forward lim\n\tmax_onset = np.max(note_array[\"onset_beat\"])\n\tlength = max_onset + np.max(note_array[np.where(note_array[\"onset_beat\"] == max_onset)[0]][\"duration_beat\"]) - bar_in_beats\n\tstep = 1 \n\twindow_size = bar_in_beats + 1\n\tvl_beat_pos = list()\n\tfor index in range(0, int(length), step): \n\t\ttemp1 = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index + window_size )], note_array[ np.where( note_array[\"onset_beat\"] >= index )]) \n\t\ttemp2 = np.intersect1d(note_array[ np.where( note_array[\"onset_beat\"] < index)], note_array[ np.where( note_array[\"onset_beat\"]+note_array[\"duration_beat\"] >= index )])\n\t\ttnotes = np.union1d(temp1, temp2)\n\t\tvl_beat_pos += filter(None, [\n\t\t\tp_cad_bass(tnotes, bar_in_beats, index),\n\t\t\tcad_delay(tnotes, bar_in_beats, index), \n\t\t\tp_cad_delay(tnotes, bar_in_beats, index), \n\t\t\tcad_onset(tnotes, bar_in_beats, index),\n\t\t\th_cad_onset(tnotes, bar_in_beats, index),\n\t\t\tvl3cons(tnotes, bar_in_beats, index)\n\t\t\t])\n\treturn sorted(list(set(vl_beat_pos)))", "\n\n\nif __name__ == \"__main__\":\n\timport partitura\n\timport os\n\n\tdirname = os.path.dirname(__file__)\n\tpar = lambda x: os.path.abspath(os.path.join(x, os.pardir))\n\tmy_musicxml_file = os.path.join(par(par(dirname)), \"samples\", \"xml\", \"mozart_piano_sonatas\", \"K284-2.musicxml\")\n\t\n\tnote_array = partitura.utils.ensure_notearray(partitura.load_musicxml(my_musicxml_file))\n\tbar_in_beats = 3\n\tprint(list(map(lambda x: (int(x / bar_in_beats) + 1, x%bar_in_beats), get_voice_leading(note_array, bar_in_beats))))"]}
{"filename": "vocsep/descriptors/utils/piano_roll.py", "chunked_list": ["import numpy as np\nfrom scipy.signal import argrelextrema\nimport partitura as pt\nfrom scipy import linalg as la\nfrom vocsep.descriptors.utils.harmonic_ssm import apply_ssm_analysis\n\n\ndef part_to_pianoroll_window_size(part):\n\tif isinstance(part, list):\n\t\tpart = part[0]\n\t\tts = [(ts.start.t, ts.end, (ts.beats, ts.beat_type)) for ts in part.iter_all(pt.score.TimeSignature)]\n\telse:\n\t\tts = [(ts.start.t, ts.end, (ts.beats, ts.beat_type)) for ts in part.iter_all(pt.score.TimeSignature)]\n\tbar_size = list()\n\tfor time_sig in ts:\n\t\tx = time_sig[2][0]\n\t\tbeat = part.inv_beat_map(1.)\n\t\tbar_size.append(beat*x )\n\tif len(bar_size) == 0:\n\t\traise ValueError(\"Part doesn't have a time signature\")\n\telif len(bar_size) == 1:\n\t\treturn bar_size[0]\n\telse:\n\t\treturn bar_size", "\n\ndef pianoroll_window_analysis(pianoroll, note_array, window_size, reg_jump=6):\n\tX = np.zeros((len(note_array), int(pianoroll.shape[1]/reg_jump)))\n\tfor i, note in enumerate(note_array):\n\t\tX[i] = np.count_nonzero(pianoroll[note[\"onset_div\"]:note[\"onset_div\"]+window_size, i*reg_jump:(i+2)*reg_jump])\n\treturn X\n\n\ndef pca(data, dims_rescaled_data=2):\n    \"\"\"\n    returns: data transformed in 2 dims/columns + regenerated original data\n    pass in: data as 2D NumPy array\n    \"\"\"\n\n    m, n = data.shape\n    # mean center the data\n    data -= data.mean(axis=0)\n    # calculate the covariance matrix\n    R = np.cov(data, rowvar=False)\n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    # use 'eigh' rather than 'eig' since R is symmetric,\n    # the performance gain is substantial\n    evals, evecs = la.eigh(R)\n    # sort eigenvalue in decreasing order\n    idx = np.argsort(evals)[::-1]\n    evecs = evecs[:,idx]\n    # sort eigenvectors according to same index\n    evals = evals[idx]\n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or dims_rescaled_data)\n    evecs = evecs[:, :dims_rescaled_data]\n    # carry out the transformation on the data using eigenvectors\n    # and return the re-scaled data, eigenvalues, and eigenvectors\n    return np.dot(evecs.T, data.T).T", "\ndef pca(data, dims_rescaled_data=2):\n    \"\"\"\n    returns: data transformed in 2 dims/columns + regenerated original data\n    pass in: data as 2D NumPy array\n    \"\"\"\n\n    m, n = data.shape\n    # mean center the data\n    data -= data.mean(axis=0)\n    # calculate the covariance matrix\n    R = np.cov(data, rowvar=False)\n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    # use 'eigh' rather than 'eig' since R is symmetric,\n    # the performance gain is substantial\n    evals, evecs = la.eigh(R)\n    # sort eigenvalue in decreasing order\n    idx = np.argsort(evals)[::-1]\n    evecs = evecs[:,idx]\n    # sort eigenvectors according to same index\n    evals = evals[idx]\n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or dims_rescaled_data)\n    evecs = evecs[:, :dims_rescaled_data]\n    # carry out the transformation on the data using eigenvectors\n    # and return the re-scaled data, eigenvalues, and eigenvectors\n    return np.dot(evecs.T, data.T).T", "\n\ndef get_pssm(part, note_array, window_size=1):\n\t\"\"\"Perform the pianoroll analysis and output register SSM.\n\n\tParameters\n\t----------\n\tnote_array : structured array\n\t\tThe note array ordered on onsets\n\n\n\tReturns:\n\t--------\n\tpeaks : numpy array\n\t\tA list of the beats where SSM segments are detected centered.\n\t\"\"\"\n\tfeatures = [\"pssm_novelty_curve\", \"dssm_pca1\", \"dssm_pca2\"]\n\tpianoroll = pt.utils.compute_pianoroll(part).todense().T\n\twindow = int(window_size * part.inv_beat_map(max(np.unique(note_array[\"ts_beats\"]))))\n\tX = pianoroll_window_analysis(pianoroll, note_array, window)\n\tX_scaled = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-6)\n\tnov = apply_ssm_analysis(X_scaled, gaussian_filter_size=2*window+1)\n\tX_pca = pca(X_scaled)\n\td_array = np.hstack((np.expand_dims(nov, axis=1), X_pca))\n\treturn d_array, features", "\n\n\ndef vpeak(X, hpeaks_dict):\n\t\"\"\"Builds a dictionary where keys are the window index and values are octave index of peaks in note density.\n\t\n\tParameters\n\t----------\n\tX : numpy array\n\t\tA sparse array where entrys are the number of black pixels for the pianoroll\n\thpeaks_dict : dictionary\n\t\ta dictionary of horrizontal peaks, we use this to filter X and reduce computation.\n\tReturns\n\t-------\n\tvpeaks : dictionary\n\t\tA dictionary where keys are the number of the octave and values are lists of peaks in note density.\n\t\tv stands for vertically, we can think this as a vertically sliding window on the pianoroll representation \n\t\twhich measures for different time positions on which octave black pixels peaks.\n\t\"\"\"\n\tvpeaks = dict()\n\t# Now find the critical bar where peaks are located.\n\tpotential_peaks = list(set([el for v in hpeaks_dict.values() for el in v]))\n\tfor i in potential_peaks:\n\t\tx = argrelextrema(X[:, i], np.greater)[0]\n\t\tif any(x):\n\t\t\tvpeaks[i] = x.tolist()\n\treturn vpeaks", "\ndef hpeak(X):\n\t\"\"\"Builds a dictionary where keys are the number of the octave and values are lists of peaks in note density.\n\t\n\tParameters\n\t----------\n\tX : numpy array\n\t\tA sparse array where entrys are the number of black pixels for the pianoroll\n\tReturns\n\t-------\n\thpeaks : dictionary\n\t\tA dictionary where keys are the number of the octave and values are lists of peaks in note density.\n\t\tH stands for horizontally, we can think this as a horrizontally sliding window on the pianoroll representation \n\t\twhich measures for different height positions where the number of black pixels peaks.\n\t\"\"\"\n\thpeaks = dict()\n\tfor i in range(X.shape[0]):\n\t\tx = argrelextrema(X[i], np.greater)[0]\n\t\tif any(x):\n\t\t\thpeaks[i] = x.tolist()\n\treturn hpeaks", "\ndef find_init_peak(X):\n\t\"\"\"Builds a list of octave index where peaks occur in note density for the first appearance of notes.\n\t\n\tParameters\n\t----------\n\tX : numpy array\n\t\tA sparse array where entrys are the number of black pixels for the pianoroll\n\tReturns\n\t-------\n\tvpeaks : list\n\t\tA list with octave indexes where peaks in note density occur, for the beginning of the score.\n\ti-1 : int\n\t\tThe index of the window where the score actually begins (typically 0 but it might have some frames of silence)\n\t\"\"\"\n\ti=0\n\tvpeaks = list()\n\twhile not any(vpeaks):\n\t\tvpeaks = argrelextrema(X[:, i], np.greater)[0]\n\t\ti+=1\n\treturn vpeaks, i-1", "\ndef get_peak_finding(part):\n\t\"\"\"Finds the number of the bar where there are potential Register changes.\n\t\n\tParameters\n\t----------\n\tpianoroll : numpy_array\n\t\tEach pianoroll entry contains 3 values an note start a note end and a midi pitch\n\tbar_in_pianoroll : int\n\t\tThis value represents the how many of the smallest pianoroll values are contained in a bar.\n\tReturns\n\t-------\n\tpeaks : list\n\t\tThe bar number where potential peaks occur.\n\t\"\"\"\n\tif isinstance(part, list):\n\t\tpianoroll = np.array([(n.start.t, n.end.t, n.midi_pitch) for p in part for n in p.notes])\t\t\n\t\tpianoroll = pianoroll[pianoroll[:, 0].argsort()]\n\t\tpart = part[0]\n\telse :\n\t\tpianoroll = np.array([(n.start.t, n.end.t, n.midi_pitch) for n in part.notes])\n\tbar_size = part_to_pianoroll_window_size(part)\n\tbar_in_beats = part.beat_map(bar_size)\n\twindow_size = int(bar_size/2)\n\tX = pianoroll_window_analysis(pianoroll, window_size)\n\tpeaks = list()\n\thpeaks_dict = hpeak(X) \n\t# first filter the range of the octaves which contain crital peaks that is the keys of hpeaks.\n\tvpeaks_dict = vpeak(X, hpeaks_dict) \n\tinit = find_init_peak(X)[0]\n\tfor key in vpeaks_dict.keys():\n\t\tfor octave in vpeaks_dict[key]:\n\t\t\tif octave not in init and octave-1 not in init and octave+1 not in init:\n\t\t\t\tinit = vpeaks_dict[key]\n\t\t\t\tpeaks.append(int(key/2)+1)\n\t\tif len(vpeaks_dict[key]) != len(init):\n\t\t\tinit = vpeaks_dict[key]\n\t\t\tpeaks.append(int(key/2)+1)\n\treturn sorted(list(set(map(lambda x: x*bar_in_beats, peaks))))", "\n\n\n\t"]}
{"filename": "vocsep/utils/hgraph.py", "chunked_list": ["import os\nimport random, string\nimport pickle\nfrom .general import exit_after\nfrom vocsep.descriptors.general import *\nimport torch\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\n\nclass HeteroScoreGraph(object):\n    def __init__(self, note_features, edges, etypes=[\"onset\", \"consecutive\", \"during\", \"rest\"], name=None, note_array=None, edge_weights=None, labels=None):\n        self.node_features = note_features.dtype.names if note_features.dtype.names else []\n        self.features = note_features\n        # Filter out string fields of structured array.\n        if self.node_features:\n            self.node_features = [feat for feat in self.node_features if note_features.dtype.fields[feat][0] != np.dtype('U256')]\n            self.features = self.features[self.node_features]\n        self.x = torch.from_numpy(np.asarray(rfn.structured_to_unstructured(self.features) if self.node_features else self.features, dtype=np.float32))\n        assert etypes is not None\n        self.etypes = {t: i for i, t in enumerate(etypes)}\n        self.note_array = note_array\n        self.edge_type = torch.from_numpy(edges[-1]).long()\n        self.edge_index = torch.from_numpy(edges[:2]).long()\n        self.edge_weights = torch.ones(len(self.edge_index[0])) if edge_weights is None else torch.from_numpy(edge_weights)\n        self.name = name\n        self.y = labels if labels is None else torch.from_numpy(labels)\n\n    def adj(self, weighted=False):\n        if weighted:\n            return torch.sparse_coo_tensor(self.edge_index, self.edge_weights, (len(self.x), len(self.x)))\n        ones = torch.ones(len(self.edge_index[0]))\n        matrix = torch.sparse_coo_tensor(self.edge_index, ones, (len(self.x), len(self.x)))\n        return matrix\n\n    def _add_measure_nodes(self, measures):\n        \"\"\"Add virtual nodes for every measure\"\"\"\n        assert \"onset_div\" in self.note_array.dtype.names, \"Note array must have 'onset_div' field to add measure nodes.\"\n        # if not hasattr(self, \"beat_nodes\"):\n        #     self._add_beat_nodes()\n        nodes = np.arange(len(measures))\n        # Add new attribute to hg\n        edges = []\n        for i, m in enumerate(measures):\n            idx = np.where((self.note_array[\"onset_div\"] >= m.start.t) & (self.note_array[\"onset_div\"] < m.end.t))[0]\n            if idx.size:\n                edges.append(np.vstack((idx, np.full(idx.size, i))))\n        self.measure_nodes = nodes\n        self.measure_edges = np.hstack(edges)\n\n    def _add_beat_nodes(self):\n        \"\"\"Add virtual nodes for every beat\"\"\"\n        assert \"onset_beat\" in self.note_array.dtype.names, \"Note array must have 'onset_beat' field to add measure nodes.\"\n        nodes = np.arange(int(self.note_array[\"onset_beat\"].max()))\n        # Add new attribute to hg\n\n        edges = []\n        for b in nodes:\n            idx = np.where((self.note_array[\"onset_beat\"] >= b) & (self.note_array[\"onset_beat\"] < b + 1))[0]\n            if idx.size:\n                edges.append(np.vstack((idx, np.full(idx.size, b))))\n        self.beat_nodes = nodes\n        self.beat_edges = np.hstack(edges)\n\n    def assign_typed_weight(self, weight_dict:dict):\n        assert weight_dict.keys() == self.etypes.keys()\n        for k, v in weight_dict.items():\n            etype = self.etypes[k]\n            self.edge_weights[self.edge_type == etype] = v\n\n    def get_edges_of_type(self, etype):\n        assert etype in self.etypes.keys()\n        etype = self.etypes[etype]\n        return self.edge_index[:, self.edge_type == etype]\n\n    def save(self, save_dir):\n        save_name = self.name if self.name else ''.join(random.choice(string.ascii_letters) for i in range(10))\n        (os.makedirs(os.path.join(save_dir, save_name)) if not os.path.exists(os.path.join(save_dir, save_name)) else None)\n        with open(os.path.join(save_dir, save_name, \"x.npy\"), \"wb\") as f:\n            np.save(f, self.x.numpy())\n        with open(os.path.join(save_dir, save_name, \"edge_index.npy\"), \"wb\") as f:\n            np.save(f, torch.cat((self.edge_index, self.edge_type.unsqueeze(0))).numpy())\n        if isinstance(self.y, torch.Tensor):\n            with open(os.path.join(save_dir, save_name, \"y.npy\"), \"wb\") as f:\n                np.save(f, self.y.numpy())\n        if isinstance(self.edge_weights, torch.Tensor):\n            np.save(open(os.path.join(save_dir, save_name, \"edge_weights.npy\"), \"wb\"), self.edge_weights.numpy())\n        if isinstance(self.note_array, np.ndarray):\n            np.save(open(os.path.join(save_dir, save_name, \"note_array.npy\"), \"wb\"), self.note_array)\n        with open(os.path.join(save_dir, save_name, 'graph_info.pkl'), 'wb') as handle:\n            object_properties = vars(self)\n            del object_properties['x']\n            del object_properties['edge_index']\n            del object_properties['edge_type']\n            del object_properties['edge_weights']\n            del object_properties['y']\n            del object_properties['note_array']\n            pickle.dump(object_properties, handle, protocol=pickle.HIGHEST_PROTOCOL)", "\n\nclass HeteroScoreGraph(object):\n    def __init__(self, note_features, edges, etypes=[\"onset\", \"consecutive\", \"during\", \"rest\"], name=None, note_array=None, edge_weights=None, labels=None):\n        self.node_features = note_features.dtype.names if note_features.dtype.names else []\n        self.features = note_features\n        # Filter out string fields of structured array.\n        if self.node_features:\n            self.node_features = [feat for feat in self.node_features if note_features.dtype.fields[feat][0] != np.dtype('U256')]\n            self.features = self.features[self.node_features]\n        self.x = torch.from_numpy(np.asarray(rfn.structured_to_unstructured(self.features) if self.node_features else self.features, dtype=np.float32))\n        assert etypes is not None\n        self.etypes = {t: i for i, t in enumerate(etypes)}\n        self.note_array = note_array\n        self.edge_type = torch.from_numpy(edges[-1]).long()\n        self.edge_index = torch.from_numpy(edges[:2]).long()\n        self.edge_weights = torch.ones(len(self.edge_index[0])) if edge_weights is None else torch.from_numpy(edge_weights)\n        self.name = name\n        self.y = labels if labels is None else torch.from_numpy(labels)\n\n    def adj(self, weighted=False):\n        if weighted:\n            return torch.sparse_coo_tensor(self.edge_index, self.edge_weights, (len(self.x), len(self.x)))\n        ones = torch.ones(len(self.edge_index[0]))\n        matrix = torch.sparse_coo_tensor(self.edge_index, ones, (len(self.x), len(self.x)))\n        return matrix\n\n    def _add_measure_nodes(self, measures):\n        \"\"\"Add virtual nodes for every measure\"\"\"\n        assert \"onset_div\" in self.note_array.dtype.names, \"Note array must have 'onset_div' field to add measure nodes.\"\n        # if not hasattr(self, \"beat_nodes\"):\n        #     self._add_beat_nodes()\n        nodes = np.arange(len(measures))\n        # Add new attribute to hg\n        edges = []\n        for i, m in enumerate(measures):\n            idx = np.where((self.note_array[\"onset_div\"] >= m.start.t) & (self.note_array[\"onset_div\"] < m.end.t))[0]\n            if idx.size:\n                edges.append(np.vstack((idx, np.full(idx.size, i))))\n        self.measure_nodes = nodes\n        self.measure_edges = np.hstack(edges)\n\n    def _add_beat_nodes(self):\n        \"\"\"Add virtual nodes for every beat\"\"\"\n        assert \"onset_beat\" in self.note_array.dtype.names, \"Note array must have 'onset_beat' field to add measure nodes.\"\n        nodes = np.arange(int(self.note_array[\"onset_beat\"].max()))\n        # Add new attribute to hg\n\n        edges = []\n        for b in nodes:\n            idx = np.where((self.note_array[\"onset_beat\"] >= b) & (self.note_array[\"onset_beat\"] < b + 1))[0]\n            if idx.size:\n                edges.append(np.vstack((idx, np.full(idx.size, b))))\n        self.beat_nodes = nodes\n        self.beat_edges = np.hstack(edges)\n\n    def assign_typed_weight(self, weight_dict:dict):\n        assert weight_dict.keys() == self.etypes.keys()\n        for k, v in weight_dict.items():\n            etype = self.etypes[k]\n            self.edge_weights[self.edge_type == etype] = v\n\n    def get_edges_of_type(self, etype):\n        assert etype in self.etypes.keys()\n        etype = self.etypes[etype]\n        return self.edge_index[:, self.edge_type == etype]\n\n    def save(self, save_dir):\n        save_name = self.name if self.name else ''.join(random.choice(string.ascii_letters) for i in range(10))\n        (os.makedirs(os.path.join(save_dir, save_name)) if not os.path.exists(os.path.join(save_dir, save_name)) else None)\n        with open(os.path.join(save_dir, save_name, \"x.npy\"), \"wb\") as f:\n            np.save(f, self.x.numpy())\n        with open(os.path.join(save_dir, save_name, \"edge_index.npy\"), \"wb\") as f:\n            np.save(f, torch.cat((self.edge_index, self.edge_type.unsqueeze(0))).numpy())\n        if isinstance(self.y, torch.Tensor):\n            with open(os.path.join(save_dir, save_name, \"y.npy\"), \"wb\") as f:\n                np.save(f, self.y.numpy())\n        if isinstance(self.edge_weights, torch.Tensor):\n            np.save(open(os.path.join(save_dir, save_name, \"edge_weights.npy\"), \"wb\"), self.edge_weights.numpy())\n        if isinstance(self.note_array, np.ndarray):\n            np.save(open(os.path.join(save_dir, save_name, \"note_array.npy\"), \"wb\"), self.note_array)\n        with open(os.path.join(save_dir, save_name, 'graph_info.pkl'), 'wb') as handle:\n            object_properties = vars(self)\n            del object_properties['x']\n            del object_properties['edge_index']\n            del object_properties['edge_type']\n            del object_properties['edge_weights']\n            del object_properties['y']\n            del object_properties['note_array']\n            pickle.dump(object_properties, handle, protocol=pickle.HIGHEST_PROTOCOL)", "\n\n@exit_after(10)\ndef load_score_hgraph(load_dir, name=None):\n    path = os.path.join(load_dir, name) if os.path.basename(load_dir) != name else load_dir\n    if not os.path.exists(path) or not os.path.isdir(path):\n        raise ValueError(\"The directory is not recognized.\")\n    x = np.load(open(os.path.join(path, \"x.npy\"), \"rb\"))\n    edge_index = np.load(open(os.path.join(path, \"edge_index.npy\"), \"rb\"))\n    graph_info = pickle.load(open(os.path.join(path, \"graph_info.pkl\"), \"rb\"))\n    y = np.load(open(os.path.join(path, \"y.npy\"), \"rb\")) if os.path.exists(os.path.join(path, \"y.npy\")) else None\n    edge_weights = np.load(open(os.path.join(path, \"edge_weights.npy\"), \"rb\")) if os.path.exists(os.path.join(path, \"edge_weights.npy\")) else None\n    note_array = np.load(open(os.path.join(path, \"note_array.npy\"), \"rb\")) if os.path.exists(\n        os.path.join(path, \"note_array.npy\")) else None\n    name = name if name else os.path.basename(path)\n    hg = HeteroScoreGraph(note_features=x, edges=edge_index, name=name, labels=y, edge_weights=edge_weights, note_array=note_array)\n    for k, v in graph_info.items():\n        setattr(hg, k, v)\n    return hg", "\n\ndef check_note_array(na):\n    dtypes = na.dtype.names\n    if not all([x in dtypes for x in [\"onset_beat\", \"duration_beat\", \"ts_beats\", \"ts_beat_type\"]]):\n        raise(TypeError(\"The given Note array is missing necessary fields.\"))\n\n\n# def hetero_graph_from_note_array(note_array, rest_array=None, norm2bar=True):\n#     '''Turn note_array to homogeneous graph dictionary.", "# def hetero_graph_from_note_array(note_array, rest_array=None, norm2bar=True):\n#     '''Turn note_array to homogeneous graph dictionary.\n\n#     Parameters\n#     ----------\n#     note_array : structured array\n#         The partitura note_array object. Every entry has 5 attributes, i.e. onset_time, note duration, note velocity, voice, id.\n#     rest_array : structured array\n#         A structured rest array similar to the note array but for rests.\n#     t_sig : list", "#         A structured rest array similar to the note array but for rests.\n#     t_sig : list\n#         A list of time signature in the piece.\n#     '''\n\n#     edg_src = list()\n#     edg_dst = list()\n#     etype = list()\n#     start_rest_index = len(note_array)\n#     for i, x in enumerate(note_array):", "#     start_rest_index = len(note_array)\n#     for i, x in enumerate(note_array):\n#         for j in np.where((np.isclose(note_array[\"onset_beat\"], x[\"onset_beat\"], rtol=1e-04, atol=1e-04) == True) & (note_array[\"pitch\"] != x[\"pitch\"]))[0]:\n#             edg_src.append(i)\n#             edg_dst.append(j)\n#             etype.append(0)\n\n#         for j in np.where(np.isclose(note_array[\"onset_beat\"], x[\"onset_beat\"] + x[\"duration_beat\"], rtol=1e-04, atol=1e-04) == True)[0]:\n#             edg_src.append(i)\n#             edg_dst.append(j)", "#             edg_src.append(i)\n#             edg_dst.append(j)\n#             etype.append(1)\n\n#         if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n#             for j in np.where(np.isclose(rest_array[\"onset_beat\"], x[\"onset_beat\"] + x[\"duration_beat\"], rtol=1e-04, atol=1e-04) == True)[0]:\n#                 edg_src.append(i)\n#                 edg_dst.append(j + start_rest_index)\n#                 etype.append(1)\n", "#                 etype.append(1)\n\n#         for j in np.where(\n#                 (x[\"onset_beat\"] < note_array[\"onset_beat\"]) & (x[\"onset_beat\"] + x[\"duration_beat\"] > note_array[\"onset_beat\"]))[0]:\n#             edg_src.append(i)\n#             edg_dst.append(j)\n#             etype.append(2)\n\n#     if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n#         for i, r in enumerate(rest_array):", "#     if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n#         for i, r in enumerate(rest_array):\n#             for j in np.where(np.isclose(note_array[\"onset_beat\"], r[\"onset_beat\"] + r[\"duration_beat\"], rtol=1e-04, atol=1e-04) == True)[0]:\n#                 edg_src.append(start_rest_index + i)\n#                 edg_dst.append(j)\n#                 etype.append(1)\n\n#         feature_fn = [dname for dname in note_array.dtype.names if dname not in rest_array.dtype.names]\n#         if feature_fn:\n#             rest_feature_zeros = np.zeros((len(rest_array), len(feature_fn)))", "#         if feature_fn:\n#             rest_feature_zeros = np.zeros((len(rest_array), len(feature_fn)))\n#             rest_feature_zeros = rfn.unstructured_to_structured(rest_feature_zeros, dtype=list(map(lambda x: (x, '<4f'), feature_fn)))\n#             rest_array = rfn.merge_arrays((rest_array, rest_feature_zeros))\n#     else:\n#         end_times = note_array[\"onset_beat\"] + note_array[\"duration_beat\"]\n#         for et in np.sort(np.unique(end_times))[:-1]:\n#             if et not in note_array[\"onset_beat\"]:\n#                 scr = np.where(end_times == et)[0]\n#                 diffs = note_array[\"onset_beat\"] - et", "#                 scr = np.where(end_times == et)[0]\n#                 diffs = note_array[\"onset_beat\"] - et\n#                 tmp = np.where(diffs > 0, diffs, np.inf)\n#                 dst = np.where(tmp == tmp.min())[0]\n#                 for i in scr:\n#                     for j in dst:\n#                         edg_src.append(i)\n#                         edg_dst.append(j)\n#                         etype.append(1)\n", "#                         etype.append(1)\n\n\n#     edges = np.array([edg_src, edg_dst, etype])\n#     # Resize Onset Beat to bar\n#     if norm2bar:\n#         note_array[\"onset_beat\"] = np.mod(note_array[\"onset_beat\"], note_array[\"ts_beats\"])\n#         if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n#             rest_array[\"onset_beat\"] = np.mod(rest_array[\"onset_beat\"], rest_array[\"ts_beats\"])\n", "#             rest_array[\"onset_beat\"] = np.mod(rest_array[\"onset_beat\"], rest_array[\"ts_beats\"])\n\n#     nodes = np.hstack((note_array, rest_array))\n#     return nodes, edges\n\ndef hetero_graph_from_note_array(note_array, rest_array=None, norm2bar=False, pot_edge_dist=0):\n    '''Turn note_array to homogeneous graph dictionary.\n\n    Parameters\n    ----------\n    note_array : structured array\n        The partitura note_array object. Every entry has 5 attributes, i.e. onset_time, note duration, note velocity, voice, id.\n    rest_array : structured array\n        A structured rest array similar to the note array but for rests.\n    t_sig : list\n        A list of time signature in the piece.\n    '''\n\n    edg_src = list()\n    edg_dst = list()\n    etype = list()\n    pot_edges = list()\n    start_rest_index = len(note_array)\n    for i, x in enumerate(note_array):\n        for j in np.where(np.isclose(note_array[\"onset_div\"], x[\"onset_div\"], rtol=1e-04, atol=1e-04) == True)[0]:\n            if i != j:\n                edg_src.append(i)\n                edg_dst.append(j)\n                etype.append(0)\n        if pot_edge_dist:\n            for j in np.where(\n                    (note_array[\"onset_div\"] > x[\"onset_div\"]+x[\"duration_div\"]) &\n                    (note_array[\"onset_beat\"] <= x[\"onset_beat\"] + x[\"duration_beat\"] + pot_edge_dist*x[\"ts_beats\"])\n            )[0]:\n                pot_edges.append([i, j])\n        for j in np.where(np.isclose(note_array[\"onset_div\"], x[\"onset_div\"] + x[\"duration_div\"], rtol=1e-04, atol=1e-04) == True)[0]:\n            edg_src.append(i)\n            edg_dst.append(j)\n            etype.append(1)\n\n        if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n            for j in np.where(np.isclose(rest_array[\"onset_div\"], x[\"onset_div\"] + x[\"duration_div\"], rtol=1e-04, atol=1e-04) == True)[0]:\n                edg_src.append(i)\n                edg_dst.append(j + start_rest_index)\n                etype.append(1)\n\n        for j in np.where(\n                (x[\"onset_div\"] < note_array[\"onset_div\"]) & (x[\"onset_div\"] + x[\"duration_div\"] > note_array[\"onset_div\"]))[0]:\n            edg_src.append(i)\n            edg_dst.append(j)\n            etype.append(2)\n\n    if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n        for i, r in enumerate(rest_array):\n            for j in np.where(np.isclose(note_array[\"onset_div\"], r[\"onset_div\"] + r[\"duration_div\"], rtol=1e-04, atol=1e-04) == True)[0]:\n                edg_src.append(start_rest_index + i)\n                edg_dst.append(j)\n                etype.append(1)\n\n        feature_fn = [dname for dname in note_array.dtype.names if dname not in rest_array.dtype.names]\n        if feature_fn:\n            rest_feature_zeros = np.zeros((len(rest_array), len(feature_fn)))\n            rest_feature_zeros = rfn.unstructured_to_structured(rest_feature_zeros, dtype=list(map(lambda x: (x, '<4f'), feature_fn)))\n            rest_array = rfn.merge_arrays((rest_array, rest_feature_zeros))\n    else:\n        end_times = note_array[\"onset_div\"] + note_array[\"duration_div\"]\n        for et in np.sort(np.unique(end_times))[:-1]:\n            if et not in note_array[\"onset_div\"]:\n                scr = np.where(end_times == et)[0]\n                diffs = note_array[\"onset_div\"] - et\n                tmp = np.where(diffs > 0, diffs, np.inf)\n                dst = np.where(tmp == tmp.min())[0]\n                for i in scr:\n                    for j in dst:\n                        edg_src.append(i)\n                        edg_dst.append(j)\n                        etype.append(3)\n\n\n    edges = np.array([edg_src, edg_dst, etype])\n\n    # Resize Onset Beat to bar\n    if norm2bar:\n        note_array[\"onset_beat\"] = np.mod(note_array[\"onset_beat\"], note_array[\"ts_beats\"])\n        if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n            rest_array[\"onset_beat\"] = np.mod(rest_array[\"onset_beat\"], rest_array[\"ts_beats\"])\n\n    nodes = np.hstack((note_array, rest_array))\n    if pot_edge_dist:\n        pot_edges = np.hstack((np.array(pot_edges).T, edges[:, edges[2] == 1][:2]))\n        return nodes, edges, pot_edges\n    return nodes, edges", "\n\n@exit_after(120)\ndef hetero_graph_from_part(x : Union[Union[partitura.score.Part, partitura.score.PartGroup], np.ndarray], features=None, name=None, norm2bar=True, include_rests=False, labels=None) -> HeteroScoreGraph:\n    if isinstance(x, partitura.score.Score) or isinstance(x, partitura.score.Part) or isinstance(x, partitura.score.PartGroup) or isinstance(x, list):\n        part = x\n        part = partitura.score.merge_parts(part)\n        part = partitura.score.unfold_part_maximal(part)\n        note_array = part.note_array(include_time_signature=True, include_grace_notes=True, include_staff=True)\n        note_features = select_features(part, features)\n\n        if include_rests:\n            rest_array = part.rest_array(include_time_signature=True, include_grace_notes=True, collapse=True, include_staff=True)\n            if labels is None:\n                labels = np.vstack((rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]]),\n                                    rfn.structured_to_unstructured(rest_array[[\"voice\", \"staff\"]])))\n        else:\n            rest_array = None\n            if labels is None:\n                labels = rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]])\n    elif isinstance(x, str):\n        return hetero_graph_from_part(partitura.load_score(x), features=features, name=name, norm2bar=norm2bar, include_rests=include_rests)\n    else:\n        check_note_array(x)\n        note_array = note_features = x\n        rest_array = None\n        if labels is None:\n            labels = rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]])\n    nodes, edges = hetero_graph_from_note_array(note_array, rest_array, norm2bar=norm2bar)\n    return HeteroScoreGraph(note_features, edges, name=name, labels=labels, note_array=note_array)", "\n\ndef voice_from_edges(edges, number_of_nodes):\n    \"\"\"\n        Assign to each disconnected node cluster an unique voice number.\n    \"\"\"\n    data = np.ones(edges.shape[1])\n    row, col = edges\n    scipy_graph = csr_matrix((data, (row.cpu(), col.cpu())), shape=(number_of_nodes, number_of_nodes))       \n    number_of_voices, voices = connected_components(csgraph=scipy_graph, directed=False, return_labels=True)\n    return voices+1, number_of_voices", "\n\ndef adj_matrix_from_edges(edges, number_of_nodes):\n    \"\"\"\n        Create adjacency matrix from edges.\n    \"\"\"\n    data = np.ones(edges.shape[1])\n    row, col = edges\n    scipy_graph = csr_matrix((data, (row.cpu(), col.cpu())), shape=(number_of_nodes, number_of_nodes))       \n    return scipy_graph", "        \n\ndef add_reverse_edges(graph, mode):\n    if mode == \"new_type\":\n        # Add reverse During Edges\n        graph.edge_index = torch.cat((graph.edge_index, graph.get_edges_of_type(\"during\").flip(0)), dim=1)\n        graph.edge_type = torch.cat((graph.edge_type, 2 + torch.zeros(graph.edge_index.shape[1] - graph.edge_type.shape[0],dtype=torch.long)), dim=0)\n        # Add reverse Consecutive Edges\n        graph.edge_index = torch.cat((graph.edge_index, graph.get_edges_of_type(\"consecutive\").flip(0)), dim=1)\n        graph.edge_type = torch.cat((graph.edge_type, 4+torch.zeros(graph.edge_index.shape[1] - graph.edge_type.shape[0], dtype=torch.long)), dim=0)\n        graph.etypes[\"consecutive_rev\"] = 4\n    else:\n        graph.edge_index = torch.cat((graph.edge_index, graph.edge_index.flip(0)), dim=1)\n        raise NotImplementedError(\"To undirected is not Implemented for HeteroScoreGraph.\")\n    return graph", "\n\ndef add_reverse_edges_from_edge_index(edge_index, edge_type, mode=\"new_type\"):\n    if mode == \"new_type\":\n        unique_edge_types = torch.unique(edge_type)\n        for type in unique_edge_types:\n            if type == 0:\n                continue\n            edge_index = torch.cat((edge_index, edge_index[:, edge_type == type].flip(0)), dim=1)\n            edge_type = torch.cat((edge_type, torch.max(edge_type) + torch.zeros(edge_index.shape[1] - edge_type.shape[0], dtype=torch.long).to(edge_type.device)), dim=0)\n    else:\n        edge_index = torch.cat((edge_index, edge_index.flip(0)), dim=1)\n        edge_type = torch.cat((edge_type, edge_type), dim=0)\n    return edge_index, edge_type"]}
{"filename": "vocsep/utils/graph.py", "chunked_list": ["import os\nimport random, string\nimport pickle\nimport warnings\nimport partitura.io.exportparangonada\nfrom vocsep.utils.general import exit_after\nfrom vocsep.descriptors.general import *\nimport torch\nfrom typing import Union\n", "from typing import Union\n\n\nclass ScoreGraph(object):\n    def __init__(\n        self,\n        note_features,\n        edges,\n        name=None,\n        note_array=None,\n        edge_weights=None,\n        labels=None,\n        mask=None,\n        info={},\n    ):\n        self.node_features = note_array.dtype.names if note_array.dtype.names else []\n        self.features = note_features\n        # Filter out string fields of structured array.\n        if self.node_features:\n            self.node_features = [\n                feat\n                for feat in self.node_features\n                if note_features.dtype.fields[feat][0] != np.dtype(\"U256\")\n            ]\n            self.features = self.features[self.node_features]\n        self.x = torch.from_numpy(\n            np.asarray(\n                rfn.structured_to_unstructured(self.features)\n                if self.node_features\n                else self.features\n            )\n        )\n        self.note_array = note_array\n        self.edge_index = torch.from_numpy(edges).long()\n        self.edge_weights = (\n            edge_weights if edge_weights is None else torch.from_numpy(edge_weights)\n        )\n        self.name = name\n        self.mask = mask\n        self.info = info\n        self.y = labels if labels is None else torch.from_numpy(labels)\n\n    def adj(self):\n        # ones = np.ones(len(self.edge_index[0]), np.uint32)\n        # matrix = sp.coo_matrix((ones, (self.edge_index[0], self.edge_index[1])))\n        ones = torch.ones(len(self.edge_index[0]))\n        matrix = torch.sparse_coo_tensor(\n            self.edge_index, ones, (len(self.x), len(self.x))\n        )\n        return matrix\n\n    def save(self, save_dir):\n        save_name = (\n            self.name\n            if self.name\n            else \"\".join(random.choice(string.ascii_letters) for i in range(10))\n        )\n        (\n            os.makedirs(os.path.join(save_dir, save_name))\n            if not os.path.exists(os.path.join(save_dir, save_name))\n            else None\n        )\n        with open(os.path.join(save_dir, save_name, \"x.npy\"), \"wb\") as f:\n            np.save(f, self.x.numpy())\n        with open(os.path.join(save_dir, save_name, \"edge_index.npy\"), \"wb\") as f:\n            np.save(f, self.edge_index.numpy())\n        if isinstance(self.y, torch.Tensor):\n            with open(os.path.join(save_dir, save_name, \"y.npy\"), \"wb\") as f:\n                np.save(f, self.y.numpy())\n        if isinstance(self.edge_weights, torch.Tensor):\n            np.save(\n                open(os.path.join(save_dir, save_name, \"edge_weights.npy\"), \"wb\"),\n                self.edge_weights.numpy(),\n            )\n        if isinstance(self.note_array, np.ndarray):\n            np.save(\n                open(os.path.join(save_dir, save_name, \"note_array.npy\"), \"wb\"),\n                self.note_array,\n            )\n        with open(os.path.join(save_dir, save_name, \"graph_info.pkl\"), \"wb\") as handle:\n            pickle.dump(\n                {\n                    \"node_features\": self.node_features,\n                    \"mask\": self.mask,\n                    \"info\": self.info,\n                },\n                handle,\n                protocol=pickle.HIGHEST_PROTOCOL,\n            )", "\n\n@exit_after(10)\ndef load_score_graph(load_dir, name=None):\n    path = (\n        os.path.join(load_dir, name) if os.path.basename(load_dir) != name else load_dir\n    )\n    if not os.path.exists(path) or not os.path.isdir(path):\n        raise ValueError(\"The directory is not recognized.\")\n    x = np.load(open(os.path.join(path, \"x.npy\"), \"rb\"))\n    edge_index = np.load(open(os.path.join(path, \"edge_index.npy\"), \"rb\"))\n    graph_info = pickle.load(open(os.path.join(path, \"graph_info.pkl\"), \"rb\"))\n    y = (\n        np.load(open(os.path.join(path, \"y.npy\"), \"rb\"))\n        if os.path.exists(os.path.join(path, \"y.npy\"))\n        else None\n    )\n    edge_weights = (\n        np.load(open(os.path.join(path, \"edge_weights.npy\"), \"rb\"))\n        if os.path.exists(os.path.join(path, \"edge_weights.npy\"))\n        else None\n    )\n    name = name if name else os.path.basename(path)\n    return ScoreGraph(\n        note_array=x,\n        edges=edge_index,\n        name=name,\n        labels=y,\n        edge_weights=edge_weights,\n        mask=graph_info[\"mask\"],\n        info=graph_info[\"info\"],\n    )", "\n\ndef check_note_array(na):\n    dtypes = na.dtype.names\n    if not all(\n        [\n            x in dtypes\n            for x in [\"onset_beat\", \"duration_beat\", \"ts_beats\", \"ts_beat_type\"]\n        ]\n    ):\n        raise (TypeError(\"The given Note array is missing necessary fields.\"))", "\n\ndef graph_from_note_array(note_array, rest_array=None, norm2bar=True):\n    \"\"\"Turn note_array to homogeneous graph dictionary.\n\n    Parameters\n    ----------\n    note_array : structured array\n        The partitura note_array object. Every entry has 5 attributes, i.e. onset_time, note duration, note velocity, voice, id.\n    rest_array : structured array\n        A structured rest array similar to the note array but for rests.\n    t_sig : list\n        A list of time signature in the piece.\n    \"\"\"\n\n    edg_src = list()\n    edg_dst = list()\n    start_rest_index = len(note_array)\n    for i, x in enumerate(note_array):\n        for j in np.where(\n            (\n                np.isclose(\n                    note_array[\"onset_beat\"], x[\"onset_beat\"], rtol=1e-04, atol=1e-04\n                )\n                == True\n            )\n            & (note_array[\"pitch\"] != x[\"pitch\"])\n        )[0]:\n            edg_src.append(i)\n            edg_dst.append(j)\n\n        for j in np.where(\n            np.isclose(\n                note_array[\"onset_beat\"],\n                x[\"onset_beat\"] + x[\"duration_beat\"],\n                rtol=1e-04,\n                atol=1e-04,\n            )\n            == True\n        )[0]:\n            edg_src.append(i)\n            edg_dst.append(j)\n\n        if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n            for j in np.where(\n                np.isclose(\n                    rest_array[\"onset_beat\"],\n                    x[\"onset_beat\"] + x[\"duration_beat\"],\n                    rtol=1e-04,\n                    atol=1e-04,\n                )\n                == True\n            )[0]:\n                edg_src.append(i)\n                edg_dst.append(j + start_rest_index)\n\n        for j in np.where(\n            (x[\"onset_beat\"] < note_array[\"onset_beat\"])\n            & (x[\"onset_beat\"] + x[\"duration_beat\"] > note_array[\"onset_beat\"])\n        )[0]:\n            edg_src.append(i)\n            edg_dst.append(j)\n\n    if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n        for i, r in enumerate(rest_array):\n            for j in np.where(\n                np.isclose(\n                    note_array[\"onset_beat\"],\n                    r[\"onset_beat\"] + r[\"duration_beat\"],\n                    rtol=1e-04,\n                    atol=1e-04,\n                )\n                == True\n            )[0]:\n                edg_src.append(start_rest_index + i)\n                edg_dst.append(j)\n\n        feature_fn = [\n            dname\n            for dname in note_array.dtype.names\n            if dname not in rest_array.dtype.names\n        ]\n        if feature_fn:\n            rest_feature_zeros = np.zeros((len(rest_array), len(feature_fn)))\n            rest_feature_zeros = rfn.unstructured_to_structured(\n                rest_feature_zeros, dtype=list(map(lambda x: (x, \"<4f\"), feature_fn))\n            )\n            rest_array = rfn.merge_arrays((rest_array, rest_feature_zeros))\n    else:\n        end_times = note_array[\"onset_beat\"] + note_array[\"duration_beat\"]\n        for et in np.sort(np.unique(end_times))[:-1]:\n            if et not in note_array[\"onset_beat\"]:\n                scr = np.where(end_times == et)[0]\n                diffs = note_array[\"onset_beat\"] - et\n                tmp = np.where(diffs > 0, diffs, np.inf)\n                dst = np.where(tmp == tmp.min())[0]\n                for i in scr:\n                    for j in dst:\n                        edg_src.append(i)\n                        edg_dst.append(j)\n\n    edges = np.array([edg_src, edg_dst])\n    # Resize Onset Beat to bar\n    if norm2bar:\n        note_array[\"onset_beat\"] = np.mod(\n            note_array[\"onset_beat\"], note_array[\"ts_beats\"]\n        )\n        if isinstance(rest_array, np.ndarray) and rest_array.size > 0:\n            rest_array[\"onset_beat\"] = np.mod(\n                rest_array[\"onset_beat\"], rest_array[\"ts_beats\"]\n            )\n\n    nodes = np.hstack((note_array, rest_array))\n    return nodes, edges", "\n\n@exit_after(60)\ndef graph_from_part(\n    x: Union[Union[partitura.score.Part, partitura.score.PartGroup], np.ndarray],\n    name=None,\n    norm2bar=True,\n    include_rests=False,\n    labels=None,\n) -> ScoreGraph:\n    if (\n        isinstance(x, partitura.score.Score)\n        or isinstance(x, partitura.score.Part)\n        or isinstance(x, partitura.score.PartGroup)\n        or isinstance(x, list)\n    ):\n        part = x\n        part = partitura.score.merge_parts(part)\n        # TODO: discuss, do we need to unfold?\n        part = partitura.score.unfold_part_maximal(part)\n        note_array = part.note_array(\n            include_time_signature=True, include_grace_notes=True, include_staff=True\n        )\n        note_features, _ = voice_separation_features(part)\n        if include_rests:\n            rest_array = part.rest_array(\n                include_time_signature=True,\n                include_grace_notes=True,\n                collapse=True,\n                include_staff=True,\n            )\n            if labels is None:\n                labels = np.vstack(\n                    (\n                        rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]]),\n                        rfn.structured_to_unstructured(rest_array[[\"voice\", \"staff\"]]),\n                    )\n                )\n        else:\n            rest_array = None\n            if labels is None:\n                labels = rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]])\n    elif isinstance(x, str):\n        return graph_from_part(partitura.load_score(x), name, norm2bar)\n    else:\n        check_note_array(x)\n        note_array = note_features = x\n        rest_array = None\n        if labels is None:\n            labels = rfn.structured_to_unstructured(note_array[[\"voice\", \"staff\"]])\n    nodes, edges = graph_from_note_array(note_array, rest_array, norm2bar=norm2bar)\n    return ScoreGraph(note_features, edges, name=name, labels=labels)", "\n\ndef get_matched_performance_idx(part, ppart, alignment):\n    # remove repetitions from aligment note ids\n    for a in alignment:\n        if a[\"label\"] == \"match\":\n            a[\"score_id\"] = str(a[\"score_id\"])\n\n    part_by_id = dict((n.id, n) for n in part.notes_tied)\n    ppart_by_id = dict((n[\"id\"], n) for n in ppart.notes)\n\n    # pair matched score and performance notes\n    note_pairs = [\n        (part_by_id[a[\"score_id\"]], ppart_by_id[a[\"performance_id\"]])\n        for a in alignment\n        if (a[\"label\"] == \"match\" and a[\"score_id\"] in part_by_id)\n    ]\n\n    # sort according to onset (primary) and pitch (secondary)\n    pitch_onset = [(sn.midi_pitch, sn.start.t) for sn, _ in note_pairs]\n    sort_order = np.lexsort(list(zip(*pitch_onset)))\n    pnote_ids = [note_pairs[i][1].id for i in sort_order]\n    return pnote_ids", "\n\n@exit_after(60)\ndef pgraph_from_part(\n    sfn, pfn, afn, features=None, name=None, norm2bar=True, include_rests=False\n) -> ScoreGraph:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        spart = partitura.load_score(sfn)\n        spart = partitura.score.merge_parts(spart)\n        spart = partitura.score.unfold_part_maximal(spart)\n        note_array = spart.note_array(\n            include_time_signature=True, include_grace_notes=True\n        )\n        nid_dict = dict((n, i) for i, n in enumerate(note_array[\"id\"]))\n        ppart = partitura.load_performance_midi(pfn)\n        alignment = partitura.io.exportparangonada.load_alignment_from_ASAP(afn)\n        perf_array, snote_idx = partitura.musicanalysis.encode_performance(\n            spart, ppart[0], alignment\n        )\n        matched_score_idx = np.array([nid_dict[nid] for nid in snote_idx])\n        note_features, _ = select_features(spart, features)\n        rest_array = None\n        labels = rfn.structured_to_unstructured(perf_array)\n        nodes, edges = graph_from_note_array(note_array, rest_array, norm2bar=norm2bar)\n        graph_info = {\"sfn\": sfn, \"pfn\": pfn}\n        return ScoreGraph(\n            note_features,\n            edges,\n            name=name,\n            labels=labels,\n            mask=matched_score_idx,\n            info=graph_info,\n        )", "\n\n@exit_after(60)\ndef agraph_from_part(\n    sfn, pfn, afn, features=None, name=None, norm2bar=True, include_rests=False\n) -> ScoreGraph:\n    spart = partitura.load_score(sfn)\n    spart = partitura.score.merge_parts(spart)\n    spart = partitura.score.unfold_part_maximal(spart)\n    note_array = spart.note_array(include_time_signature=True, include_grace_notes=True)\n    nid_dict = dict((n, i) for i, n in enumerate(note_array[\"id\"]))\n    ppart = partitura.load_performance_midi(pfn)\n    alignment = partitura.io.exportparangonada.load_alignment_from_ASAP(afn)\n    perf_array, snote_idx = partitura.musicanalysis.encode_performance(\n        spart, ppart, alignment\n    )\n    pnote_array = ppart.note_array()\n    pid_dict = dict((n, i) for i, n in enumerate(pnote_array[\"id\"]))\n    pnote_array = pnote_array[[\"onset_sec\", \"pitch\", \"velocity\"]]\n    matched_subset_idxs = np.array([nid_dict[nid] for nid in snote_idx])\n    pnote_idx = get_matched_performance_idx(spart, ppart, alignment)\n    matched_performance_idx = np.array([pid_dict[nid] for nid in pnote_idx])\n    note_features = select_features(spart, features)\n    rest_array = None\n    labels = rfn.structured_to_unstructured(pnote_array)\n    nodes, edges = graph_from_note_array(note_array, rest_array, norm2bar=norm2bar)\n    return ScoreGraph(\n        note_features,\n        edges,\n        name=name,\n        labels=labels,\n        mask=(matched_subset_idxs, matched_performance_idx),\n    )", "\n\ndef rec_dir_search(par_dir, doc_type, result=[]):\n    for cdir in os.listdir(par_dir):\n        path = os.path.join(par_dir, cdir)\n        if os.path.isdir(path):\n            result = rec_dir_search(path, doc_type, result)\n        else:\n            if path.endswith(doc_type):\n                try:\n                    name = (\n                        os.path.basename(os.path.dirname(os.path.dirname(par_dir)))\n                        + \"_\"\n                        + os.path.basename(os.path.dirname(par_dir))\n                        + \"_\"\n                        + os.path.basename(par_dir)\n                    )\n                    graph = graph_from_part(path, name)\n                    graph.save(\"/home/manos/Desktop/JKU/data/asap_graphs/\")\n                except:\n                    print(\"Graph Creation failed on {}\".format(path))\n    return result", "\n\n\n\nif __name__ == \"__main__\":\n    graphs = rec_dir_search(\"/home/manos/Desktop/JKU/codes/asap-dataset/\", \".musicxml\")\n    print(graphs)\n"]}
{"filename": "vocsep/utils/general.py", "chunked_list": ["from __future__ import print_function\nimport sys\nimport threading\ntry:\n    import thread\nexcept ImportError:\n    import _thread as thread\n\n\ndef quit_function(fn_name):\n    print('{} took too long'.format(fn_name), file=sys.stderr)\n    sys.stderr.flush() # Python 3 stderr is likely buffered.\n    thread.interrupt_main() # raises KeyboardInterrupt", "\ndef quit_function(fn_name):\n    print('{} took too long'.format(fn_name), file=sys.stderr)\n    sys.stderr.flush() # Python 3 stderr is likely buffered.\n    thread.interrupt_main() # raises KeyboardInterrupt\n\n\ndef exit_after(s):\n    '''\n    use as decorator to exit process if\n    function takes longer than s seconds\n    '''\n    def outer(fn):\n        def inner(*args, **kwargs):\n            timer = threading.Timer(s, quit_function, args=[fn.__name__])\n            timer.start()\n            try:\n                result = fn(*args, **kwargs)\n            finally:\n                # print(\"TimeoutException: \\n Skipping.\")\n                timer.cancel()\n            return result\n        return inner\n    return outer", "\n\n"]}
{"filename": "vocsep/utils/chord_representations.py", "chunked_list": ["from itertools import combinations\n\n\ndef chord_to_intervalVector(midi_pitches, return_pc_class=False):\n    '''Given a chord it calculates the Interval Vector.\n\n\n    Parameters\n    ----------\n    midi_pitches : list(int)\n        The midi_pitches, is a list of integers < 128.\n\n    Returns\n    -------\n    intervalVector : list(int)\n        The interval Vector is a list of six integer values.\n    '''\n    intervalVector = [0, 0, 0, 0, 0, 0]\n    PC = set([mp%12 for mp in midi_pitches])\n    for p1, p2 in combinations(PC, 2):\n        interval = int(abs(p1 - p2))\n        if interval <= 6:\n            index = interval\n        else:\n            index = 12 - interval\n        if index != 0:\n            index = index-1\n            intervalVector[index] += 1\n    if return_pc_class:\n        return intervalVector, list(PC)\n    else:\n        return intervalVector", ""]}
{"filename": "vocsep/utils/globals.py", "chunked_list": ["''' Should contain names of descriptors and globals.'''\n\n\nDESCRIPTORS = {\n    \"hssm\": \"get_hssm\",\n    \"cad\": \"make_cad_features\",\n}\n\n\nALTER = {", "\nALTER = {\n    \"---\": -3,\n    \"--\": -2,\n    \"-\": -1,\n    \"\": 0,\n    \"#\": 1,\n    \"##\": 2,\n    \"###\": 3\n}", "    \"###\": 3\n}\n"]}
{"filename": "vocsep/utils/__init__.py", "chunked_list": ["from .graph import *\nfrom .hgraph import *\nfrom .globals import *\nfrom .general import *\nfrom .chord_representations import chord_to_intervalVector\n"]}
{"filename": "vocsep/utils/visualization.py", "chunked_list": ["import partitura\nimport numpy as np\nimport plotly.express as px\n\n\ndef show_voice_pr(pitches, onsets, durations, voices, time_unit, time_div, return_figure= False, colors = None):\n    unique_voices = np.unique(voices)\n    # create the structured arrays\n    struct_array = np.zeros(len(voices), dtype={'names':('pitch', 'onset_beat', 'duration_beat'),\n                          'formats':('i4', 'f4', 'f4')})\n    struct_array[\"pitch\"] = pitches\n    struct_array[\"onset_beat\"] = onsets\n    struct_array[\"duration_beat\"] = durations\n\n    # create a pianoroll where each voice has a different value\n    piano_rolls = []\n    end_time = onsets[-1] + durations[-1]\n    for i,voice_n in enumerate(unique_voices):\n        pr = partitura.utils.compute_pianoroll(struct_array[voices==voice_n],piano_range =True, time_unit = time_unit, time_div=time_div, remove_silence = False, end_time = float(end_time))\n        piano_rolls.append(pr.multiply(voice_n).todense())\n    # this takes the maximum, meaning that if two voices share the same note, only the highest voice will be shown\n    mixed_pr = np.maximum.reduce(piano_rolls)\n    # sum_pr = np.sum(piano_rolls)\n    # different_bins = np.nonzero(mixed_pr!=sum_pr)\n    # if len(different_bins)!=0:\n    #     print(different_bins)\n    #     for e in different_bins:\n    #         print(\"Double voice in bins\", e)\n\n    # a list of accented colors in such an order to seems \n    # colors = [\"red\",\"green\",\"cyan\", \"blue\",  \"orange\", \"purple\", \"magenta\",\"yellow\"]\n    if colors is None:\n        colors = px.colors.sample_colorscale(\"turbo\", [voice_n/(np.max(unique_voices) -1) for voice_n in range(np.max(unique_voices))])\n\n    separators = [voice_n/np.max(unique_voices) for voice_n in unique_voices]\n    epsilon = 0.001\n    background_color = \"rgba(0,0,0,0.0)\"\n    color_scale = [(0.0,background_color),(0+epsilon,background_color)]\n    last_value = 0\n    for i,sep in enumerate(separators):\n        color_scale.append((last_value+epsilon,colors[i]))\n        color_scale.append((sep,colors[i]))\n        last_value = sep\n\n    fig = px.imshow( mixed_pr, origin=\"lower\", color_continuous_scale = color_scale)\n    fig.show()\n    if return_figure:\n        return fig", "\n"]}
{"filename": "vocsep/utils/pianoroll.py", "chunked_list": ["# TODO: move this in another file\nfrom functools import partial\nimport partitura\nfrom typing import Union, Dict\nfrom .general import exit_after\nimport numpy as np\nimport torch\nimport os\n\n", "\n\n@exit_after(60)\ndef pianorolls_from_part(\n    x: Union[Union[partitura.score.Part, partitura.score.PartGroup, str], np.ndarray],\n    time_unit: str = \"beat\",\n    time_div: int = 12,\n    musical_beat: bool = True,\n    path: str = \"\",\n) -> Dict:\n    if (\n        isinstance(x, partitura.score.Part)\n        or isinstance(x, partitura.score.PartGroup)\n        or isinstance(x, list)\n    ):\n        parts = list(partitura.score.iter_parts(x))\n        # set musical beat if requested\n        [part.use_musical_beat() for part in parts]\n        # get the maximum length of all parts to avoid shorter pianorolls\n        end_time = max([int(part.beat_map([part._points[-1].t])) for part in parts])\n        # define the parameters of the compute_pianoroll function\n        get_pianoroll = partial(\n            partitura.utils.compute_pianoroll,\n            time_unit=time_unit,\n            time_div=time_div,\n            piano_range=True,\n            remove_silence=False,\n            end_time=end_time,\n        )\n        # compute pianorolls for all separated voices\n        separated_prs = np.array([get_pianoroll(part) for part in parts])\n        if not all([pr.shape == separated_prs[0].shape for pr in separated_prs]):\n            raise Exception(f\"Pianorolls of different lenght in {path}\")\n        # compute mixed pianoroll\n        part = partitura.score.merge_parts(parts, reassign=\"voice\")\n        # mixed_pr = get_pianoroll(part)\n        # compute mixed note array\n        mixed_notearray = part.note_array()\n\n        ## compute the voice_pianoroll, that will have the number of the voice in the bin where there is a note, and -1 where there is not\n        dense_pianoroll = np.array([pr.todense() for pr in separated_prs])\n        negative_pianoroll = np.zeros(dense_pianoroll.shape[1:])\n        negative_pianoroll[np.sum(dense_pianoroll, axis=0) == 0] = 1\n        voice_pianoroll = (\n            np.concatenate(\n                [np.expand_dims(negative_pianoroll, axis=0), dense_pianoroll]\n            ).argmax(axis=0)\n            - 1\n        )\n        # return {\"separated_pianorolls\": separated_prs, \"mixed_pianoroll\": mixed_pr, \"mixed_notearray\": mixed_notearray, \"path\": path}\n        return {\n            \"voice_pianoroll\": voice_pianoroll,\n            \"notearray_pitch\": mixed_notearray[\"pitch\"].astype(int),\n            \"notearray_onset_beat\": mixed_notearray[\"onset_beat\"].astype(float),\n            \"notearray_duration_beat\": mixed_notearray[\"duration_beat\"].astype(float),\n            \"notearray_voice\": mixed_notearray[\"voice\"].astype(int),\n            \"path\": path,\n        }\n    elif isinstance(x, str):\n        # print(f\"Processing {x}\")\n        return pianorolls_from_part(\n            partitura.load_score(x),\n            time_unit,\n            time_div,\n            musical_beat,\n            x.split(os.path.sep)[-1],\n        )\n    else:\n        raise TypeError(f\"x must be a list of Parts, not {type(x)}\")", "\n\ndef pr_to_voice_pred(\n    pianoroll: np.ndarray,\n    onset_beat: np.ndarray,\n    duration_beat: np.ndarray,\n    pitch: np.ndarray,\n    piano_range: bool,\n    time_div: int,\n):\n    \"\"\"\n    Take the predicted voices from a pianoroll and map them into the note_array.\n    Returns a list with a voice for each note in the input note array.\n\n    The input pianoroll has dimension Tx88xV where V is the maximum number of voices.\n    For a fixed t and note, it should contains V log probabilities that the note belong to each voice.\n    WARNING: this does not work with normal or unnormalized probabilities. Only with log probabilities\n\n    Returns a voice array, one for each note in the same order as the input parameters.\n    Voices start from 1.\n    \"\"\"\n\n    # shift in case the first time position is negative (pickup measure)\n    positive_onset_beats = onset_beat\n    if onset_beat[0] < 0:\n        positive_onset_beats = positive_onset_beats - positive_onset_beats[0]\n    pr_onset_idxs = torch.round(time_div * positive_onset_beats).int()\n    pr_durations = torch.clip(\n        torch.round(time_div * duration_beat).int(), min=1, max=None\n    )\n    pr_offset_idxs = pr_onset_idxs + pr_durations\n\n    pitch_idxs = pitch\n    if piano_range:\n        pitch_idxs = pitch_idxs - 21  # pianorolls are with 88 only notes\n\n    pred_voice = torch.zeros(pitch.shape, dtype=torch.int64)\n    for i, (p, ons, offs) in enumerate(zip(pitch_idxs, pr_onset_idxs, pr_offset_idxs)):\n        # get predictions from the pianoroll\n        voice = pianoroll[:, ons : offs + 1, p]\n        # sum the log probs to get a unique probability for the entire note, and take the max\n        pred_voice[i] = torch.sum(voice, axis=1).argmax() + 1\n\n    return pred_voice", ""]}
{"filename": "vocsep/metrics/losses.py", "chunked_list": ["import torch.nn.functional as F\nimport torch\nfrom torch_scatter import scatter\n\n\nclass LinkPredictionLoss(torch.nn.Module):\n    def __init__(self):\n        super(LinkPredictionLoss, self).__init__()\n\n    def forward(self, pos_score: torch.Tensor, neg_score: torch.Tensor):\n        pos_loss = -torch.log(pos_score + 1e-15).mean()\n        neg_loss = -torch.log(1 - neg_score + 1e-15).mean()\n        loss = pos_loss + neg_loss\n        return loss", "\n\nclass LinearAssignmentLoss(torch.nn.Module):\n    def __init__(self):\n        super(LinearAssignmentLoss, self).__init__()\n\n    def forward(self, edge_index: torch.Tensor, score, target_edges, num_nodes):\n        add_row = scatter(score, edge_index[0], dim=0, dim_size=num_nodes, reduce=\"sum\")\n        add_col = scatter(score, edge_index[1], dim=0, dim_size=num_nodes, reduce=\"sum\")\n        norm_row = torch.sqrt(scatter(torch.pow(score, 2), edge_index[0], dim=0, dim_size=num_nodes, reduce=\"sum\"))\n        norm_col = torch.sqrt(scatter(torch.pow(score, 2), edge_index[1], dim=0, dim_size=num_nodes, reduce=\"sum\"))\n        ones_row = torch.zeros((num_nodes,), device=edge_index.device)\n        ones_col = torch.zeros((num_nodes,), device=edge_index.device)\n        ones_row[target_edges[0]] = 1\n        ones_col[target_edges[1]] = 1\n        l1 = torch.sqrt(torch.sum(torch.pow(add_row - ones_row, 2))/num_nodes) + torch.sqrt(torch.sum(torch.pow(add_col - ones_col, 2))/num_nodes)\n        l2 = torch.sqrt(torch.sum(torch.pow(norm_row - ones_row, 2))/num_nodes) + torch.sqrt(torch.sum(torch.pow(norm_col - ones_col, 2))/num_nodes)\n        mse_1 = torch.pow(ones_row - add_row, 2).sum() + torch.pow(ones_col - add_col, 2).sum() # removed sqrt\n        # mae_1 = torch.abs(ones_row - add_row).sum() + torch.abs(ones_col - add_col).sum()\n        mse_2 = torch.pow(ones_row - norm_row, 2).sum() + torch.pow(ones_col - norm_col, 2).sum() # removed sqrt\n        # mae_2 = torch.abs(ones_row - norm_row).sum() + torch.abs(ones_col - norm_col).sum()\n        # lc = (mse_1 + mse_2) / num_nodes\n        # lc = (mae_1 + mae_2) / num_nodes\n        lc = torch.sqrt((mse_1 + mse_2) / (4*num_nodes))\n        # lc = (l1 + l2) # / num_nodes\n        return lc", "\n\nclass LinearAssignmentLossCE(torch.nn.Module):\n    def __init__(self):\n        super(LinearAssignmentLossCE, self).__init__()\n\n    def forward(self, edge_index, score, target_edges, num_nodes):\n        \"\"\"Computes the loss for the linear assignment problem, using cross entropy.\n\n        Args:\n            edge_index (torch.Tensor): list of all potential edges\n            edge_pred_mask_logits (torch.Tensor): unnormalized logits predicted for the potential edges. Shape (pot_edges.shape[1]).\n            edge_target_mask (torch.Tensor): binary targets for the potential edges. Shape (pot_edges.shape[1]).\n\n        Returns:\n            float: the loss\n        \"\"\"\n        loss_sum = torch.tensor(0.0, device=edge_index.device)\n        # for each non-zero element, take all pot_edges that starts and ends there\n        for start, dst in target_edges.T:\n            # get all pot_edges that start at start and end at dst\n            mask = (edge_index[0] == start) | (edge_index[1] == dst)\n            edge_target_mask = (edge_index[0] == start) & (edge_index[1] == dst)\n            if torch.any(edge_target_mask):\n                # get the logits for the restricted pot_edges\n                pred_logits = score[mask]\n                # get the ground truth for the restricted pot_edges\n                target_logits = edge_target_mask[mask].long()\n                loss_sum += F.cross_entropy(pred_logits.unsqueeze(0), target_logits.unsqueeze(0).argmax(-1))/pred_logits.shape[0]\n        loss_sum /= target_edges.shape[1]\n        return loss_sum"]}
{"filename": "vocsep/metrics/slow_eval.py", "chunked_list": ["from torchmetrics import Metric\nfrom torchmetrics.classification.f_beta import F1Score\nimport torch\nimport itertools\nfrom typing import Any\n\n\nclass AverageVoiceConsistency(Metric):\n    \"\"\"Implementation of the evaluation metric proposed by the paper\n    'Chew, E., & Wu, X. (2005). Separating voices in polyphonic music:\n    A contig mapping approach. In U. Wiil (Ed.), Computer music modeling and retrieval,\n    Vol. 3310 (pp. 1-20). Berlin, Heidelberg: Springer.'\n    \"\"\"\n\n    higher_is_better = True\n    full_state_update: bool = False\n    correct: torch.Tensor\n    total: torch.Tensor\n\n    def __init__(self, allow_permutations=True):\n        super(AverageVoiceConsistency, self).__init__()\n        self.allow_permutations = allow_permutations\n        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, voice_pred: torch.Tensor, voice_truth: torch.Tensor):\n        if not self.allow_permutations:\n            local_correct = torch.sum(voice_pred == voice_truth)\n        else:\n            unique_voices = torch.unique(voice_truth)\n            permutations = list(itertools.permutations(unique_voices))\n            best_voice_perm_result = torch.zeros(\n                len(permutations), dtype=torch.int64\n            ).to(self.device)\n            # iterate over all possible permutations of the prediction\n            for i_perm, perm in enumerate(permutations):\n                perm_pred = torch.zeros_like(voice_pred, device=self.device)\n                for e in unique_voices:\n                    perm_pred[voice_pred == e] = perm[e]\n                best_voice_perm_result[i_perm] = torch.sum(perm_pred == voice_truth)\n            # now pick the best mapping\n            local_correct = torch.max(best_voice_perm_result)\n\n        self.correct += local_correct\n        self.total += len(voice_pred)\n\n    def compute(self):\n        return self.correct.float() / self.total", "\n\nclass MonophonicVoiceF1(F1Score):\n    \"\"\"Implementation of monophonic F1-score for voice separation\n    \"\"\"\n\n    def __init__(self, **kwargs: Any):\n        super(MonophonicVoiceF1, self).__init__(**kwargs)\n\n    # def update(self, voice_pred: torch.Tensor, voice_truth: torch.Tensor, onset, duration):\n    #     edge_matrix_pred = torch.zeros((len(voice_pred),len(voice_pred)), dtype= torch.int32).to(self.device)\n    #     edge_matrix_truth = torch.zeros((len(voice_truth),len(voice_truth)), dtype = torch.int32).to(self.device)\n    #     for i_start, (start_pred, start_truth) in enumerate(zip(voice_pred, voice_truth)):\n    #         for i_end, (end_pred, end_truth) in enumerate(zip(voice_pred,voice_truth)):\n    #             if (onset[i_start] + duration[i_start] == onset[i_end]):\n    #                 if (start_pred == end_pred):\n    #                     edge_matrix_pred[i_start, i_end] = 1\n    #                 if (start_truth == end_truth):\n    #                     edge_matrix_truth[i_start, i_end] = 1\n        \n    #     super(MonophonicVoiceF1, self).update(edge_matrix_pred.flatten(), edge_matrix_truth.flatten())\n\n    def update(self, voice_pred: torch.Tensor, voice_truth: torch.Tensor, onset, duration):\n        len_na = len(voice_pred)\n        # build a len_na x len_na matrix where consecutive notes are True\n        consecutive = onset == (onset+duration).expand(len_na,len_na).t()\n        # build a len_na x len_na matrix where same voice notes are True\n        same_voice_pred = voice_pred == voice_pred.expand(len_na,len_na).t()\n        same_voice_truth = voice_truth == voice_truth.expand(len_na,len_na).t()\n        # find consecutive notes on the same voice\n        edge_matrix_pred = torch.logical_and(consecutive,same_voice_pred)\n        edge_matrix_truth = torch.logical_and(consecutive, same_voice_truth)\n        \n        super(MonophonicVoiceF1, self).update(edge_matrix_pred.flatten(), edge_matrix_truth.flatten())"]}
{"filename": "vocsep/metrics/__init__.py", "chunked_list": ["from .eval import *\nfrom .slow_eval import *\nfrom .losses import *\n"]}
{"filename": "vocsep/metrics/eval.py", "chunked_list": ["from torchmetrics import Metric, Accuracy\nimport torch\nfrom sklearn.metrics import roc_auc_score\nfrom torch_scatter import scatter\n\n\nclass VoiceSeparationAUC(Metric):\n    def __init__(self):\n        super(VoiceSeparationAUC, self).__init__()\n        self.add_state(\"correct\", default=torch.tensor(0, dtype=float), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, pos_score: torch.Tensor, neg_score: torch.Tensor):\n        scores = torch.cat([pos_score, neg_score]).numpy()\n        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n        assert labels.shape == scores.shape\n\n        score = roc_auc_score(labels, scores)\n        self.correct += score\n        self.total += 1\n\n    def compute(self):\n        return self.correct.float() / self.total", "\n\nclass LinearAssignmentScore(Metric):\n    def __init__(self):\n        super(LinearAssignmentScore, self).__init__()\n        self.add_state(\"correct\", default=torch.tensor(0, dtype=float), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, edge_index: torch.Tensor, score, target_edges, num_nodes):\n        score = (score > 0.3).float()\n        add_row = scatter(score, edge_index[0], dim=0, dim_size=num_nodes, reduce=\"sum\")\n        add_col = scatter(score, edge_index[1], dim=0, dim_size=num_nodes, reduce=\"sum\")\n        ones = torch.zeros((num_nodes,), device=edge_index.device)\n        ones[torch.unique(torch.cat([target_edges[0], target_edges[1]], dim=0))] = 1\n        score = torch.sqrt(torch.pow(ones - add_row, 2).sum()) + torch.sqrt(torch.pow(ones - add_col, 2).sum())\n        self.correct += (score / num_nodes).item()\n        self.total += 1\n\n    def compute(self):\n        return self.correct.float() / self.total", ""]}
{"filename": "vocsep/data/dataset.py", "chunked_list": ["import os, abc, hashlib\nimport warnings\nimport errno\nimport requests\nfrom git import Repo\n\n\ndef check_sha1(filename, sha1_hash):\n    \"\"\"Check whether the sha1 hash of the file content matches the expected hash.\n    Codes borrowed from mxnet/gluon/utils.py\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    \"\"\"\n    sha1 = hashlib.sha1()\n    with open(filename, 'rb') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash", "\n\ndef makedirs(path):\n    try:\n        os.makedirs(os.path.expanduser(os.path.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and os.path.isdir(path):\n            raise\n\n\ndef makedirs(path):\n    try:\n        os.makedirs(os.path.expanduser(os.path.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and os.path.isdir(path):\n            raise", "\n\ndef makedirs(path):\n    try:\n        os.makedirs(os.path.expanduser(os.path.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and os.path.isdir(path):\n            raise\n\n\ndef extract_archive(file, target_dir, overwrite=False):\n    \"\"\"Extract archive file.\n\n    Parameters\n    ----------\n    file : str\n        Absolute path of the archive file.\n    target_dir : str\n        Target directory of the archive to be uncompressed.\n    overwrite : bool, default True\n        Whether to overwrite the contents inside the directory.\n        By default always overwrites.\n    \"\"\"\n    if os.path.exists(target_dir) and not overwrite:\n        return\n    print('Extracting file to {}'.format(target_dir))\n    if file.endswith('.tar.gz') or file.endswith('.tar') or file.endswith('.tgz'):\n        import tarfile\n        with tarfile.open(file, 'r') as archive:\n            archive.extractall(path=target_dir)\n    elif file.endswith('.gz'):\n        import gzip\n        import shutil\n        with gzip.open(file, 'rb') as f_in:\n            target_file = os.path.join(target_dir, os.path.basename(file)[:-3])\n            with open(target_file, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n    elif file.endswith('.zip'):\n        import zipfile\n        with zipfile.ZipFile(file, 'r') as archive:\n            archive.extractall(path=target_dir)\n    else:\n        raise Exception('Unrecognized file type: ' + file)", "\n\ndef extract_archive(file, target_dir, overwrite=False):\n    \"\"\"Extract archive file.\n\n    Parameters\n    ----------\n    file : str\n        Absolute path of the archive file.\n    target_dir : str\n        Target directory of the archive to be uncompressed.\n    overwrite : bool, default True\n        Whether to overwrite the contents inside the directory.\n        By default always overwrites.\n    \"\"\"\n    if os.path.exists(target_dir) and not overwrite:\n        return\n    print('Extracting file to {}'.format(target_dir))\n    if file.endswith('.tar.gz') or file.endswith('.tar') or file.endswith('.tgz'):\n        import tarfile\n        with tarfile.open(file, 'r') as archive:\n            archive.extractall(path=target_dir)\n    elif file.endswith('.gz'):\n        import gzip\n        import shutil\n        with gzip.open(file, 'rb') as f_in:\n            target_file = os.path.join(target_dir, os.path.basename(file)[:-3])\n            with open(target_file, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n    elif file.endswith('.zip'):\n        import zipfile\n        with zipfile.ZipFile(file, 'r') as archive:\n            archive.extractall(path=target_dir)\n    else:\n        raise Exception('Unrecognized file type: ' + file)", "\n\ndef get_download_dir():\n    \"\"\"Get the absolute path to the download directory.\n    Returns\n    -------\n    dirname : str\n        Path to the download directory\n    \"\"\"\n    default_dir = os.path.join(os.path.expanduser('~'), '.struttura')\n    dirname = os.environ.get('STRUTTURA_DOWNLOAD_DIR', default_dir)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n    return dirname", "\n\ndef download(url, path=None, overwrite=True, sha1_hash=None, retries=5, verify_ssl=True, log=True, extract=True):\n    \"\"\"Download a given URL.\n    Codes borrowed from mxnet/gluon/utils.py\n    Parameters\n    ----------\n    url : str\n        URL to download.\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with the same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite the destination file if it already exists.\n        By default always overwrites the downloaded file.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn't match.\n    retries : integer, default 5\n        The number of times to attempt downloading in case of failure or non 200 return codes.\n    verify_ssl : bool, default True\n        Verify SSL certificates.\n    log : bool, default True\n        Whether to print the progress for download\n    extract : bool, default True\n        Whether to extract the downloaded file.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    \"\"\"\n    if path is None:\n        fname = url.split('/')[-1]\n        # Empty filenames are invalid\n        assert fname, 'Can\\'t construct file-name from this URL. ' \\\n            'Please set the `path` option manually.'\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split('/')[-1])\n        else:\n            fname = path\n    assert retries >= 0, \"Number of retries should be at least 0\"\n\n    if not verify_ssl:\n        warnings.warn(\n            'Unverified HTTPS request is being made (verify_ssl=False). '\n            'Adding certificate verification is strongly advised.')\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        while retries+1 > 0:\n            # Disable pyling too broad Exception\n            # pylint: disable=W0703\n            try:\n                if log:\n                    print('Downloading %s from %s...' % (fname, url))\n                r = requests.get(url, stream=True, verify=verify_ssl)\n                if r.status_code != 200:\n                    raise RuntimeError(\"Failed downloading url %s\" % url)\n                with open(fname, 'wb') as f:\n                    for chunk in r.iter_content(chunk_size=1024):\n                        if chunk:  # filter out keep-alive new chunks\n                            f.write(chunk)\n                if sha1_hash and not check_sha1(fname, sha1_hash):\n                    raise UserWarning('File {} is downloaded but the content hash does not match.'\n                                      ' The repo may be outdated or download may be incomplete. '\n                                      'If the \"repo_url\" is overridden, consider switching to '\n                                      'the default repo.'.format(fname))\n                break\n            except Exception as e:\n                retries -= 1\n                if retries <= 0:\n                    raise e\n                else:\n                    if log:\n                        print(\"download failed, retrying, {} attempt{} left\"\n                              .format(retries, 's' if retries > 1 else ''))\n\n    if extract:\n        extract_archive(fname, os.path.dirname(fname), overwrite=overwrite)\n\n    return", "\n\nclass StrutturaDataset(object):\n    \"\"\"The basic Struttura Dataset for creating various datasets.\n    This class defines a basic template class for Struttura Dataset.\n    The following steps will are executed automatically:\n\n      1. Check whether there is a dataset cache on disk\n         (already processed and stored on the disk) by\n         invoking ``has_cache()``. If true, goto 5.\n      2. Call ``download()`` to download the data.\n      3. Call ``process()`` to process the data.\n      4. Call ``save()`` to save the processed dataset on disk and goto 6.\n      5. Call ``load()`` to load the processed dataset from disk.\n      6. Done.\n\n    Users can overwite these functions with their\n    own data processing logic.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset\n    url : str\n        Url to download the raw dataset\n    raw_dir : str\n        Specifying the directory that will store the\n        downloaded data or the directory that\n        already stores the input data.\n        Default: ~/.struttura/\n    save_dir : str\n        Directory to save the processed dataset.\n        Default: same as raw_dir\n    hash_key : tuple\n        A tuple of values as the input for the hash function.\n        Users can distinguish instances (and their caches on the disk)\n        from the same dataset class by comparing the hash values.\n        Default: (), the corresponding hash value is ``'f9065fa7'``.\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information\n\n    Attributes\n    ----------\n    url : str\n        The URL to download the dataset\n    name : str\n        The dataset name\n    raw_dir : str\n        Raw file directory contains the input data folder\n    raw_path : str\n        Directory contains the input data files.\n        Default : ``os.path.join(self.raw_dir, self.name)``\n    save_dir : str\n        Directory to save the processed dataset\n    save_path : str\n        File path to save the processed dataset\n    verbose : bool\n        Whether to print information\n    hash : str\n        Hash value for the dataset and the setting.\n    \"\"\"\n    def __init__(self, name, features=\"all\", url=None, raw_dir=None, save_dir=None,\n                 hash_key=(), force_reload=False, verbose=False):\n        self._name = name\n        self._url = url\n        self._features = features\n        self._force_reload = force_reload\n        self._verbose = verbose\n        self._hash_key = hash_key\n        self._hash = self._get_hash()\n\n        # if no dir is provided, the default struttura download dir is used.\n        if raw_dir is None:\n            self._raw_dir = get_download_dir()\n        else:\n            self._raw_dir = raw_dir\n\n        if save_dir is None:\n            self._save_dir = self._raw_dir\n        else:\n            self._save_dir = save_dir\n\n        self._load()\n\n    def download(self):\n        \"\"\"Overwite to realize your own logic of downloading data.\n\n        It is recommended to download the to the :obj:`self.raw_dir`\n        folder. Can be ignored if the dataset is\n        already in :obj:`self.raw_dir`.\n        \"\"\"\n        pass\n\n    def save(self):\n        \"\"\"Overwite to realize your own logic of\n        saving the processed dataset into files.\n\n        It is recommended to use ``struttura.graphs.save()``\n        to save struttura score graph into files and use\n        ``struttura.utils.save_info`` to save extra\n        information into files.\n        \"\"\"\n        pass\n\n    def load(self):\n        \"\"\"Overwite to realize your own logic of\n        loading the saved dataset from files.\n\n        It is recommended to use ``struttura.utils.load_graph_from_part``\n        to load struttura Score graph from files and use\n        ``struttura.utils.load_info`` to load extra information\n        into python dict object.\n        \"\"\"\n        pass\n\n    def process(self):\n        \"\"\"Overwrite to realize your own logic of processing the input data.\n        \"\"\"\n        raise NotImplementedError\n\n    def has_cache(self):\n        \"\"\"Overwrite to realize your own logic of\n        deciding whether there exists a cached dataset.\n\n        By default False.\n        \"\"\"\n        return False\n\n    def _download(self):\n        \"\"\"Download dataset by calling ``self.download()`` if the dataset does not exists under ``self.raw_path``.\n            By default ``self.raw_path = os.path.join(self.raw_dir, self.name)``\n            One can overwrite ``raw_path()`` function to change the path.\n        \"\"\"\n        if os.path.exists(self.raw_path):  # pragma: no cover\n            return\n\n        makedirs(self.raw_dir)\n        self.download()\n\n    def _load(self):\n        r\"\"\"Entry point from __init__ to load the dataset.\n            if the cache exists:\n                Load the dataset from saved struttura score graph and information files.\n                If loadin process fails, re-download and process the dataset.\n            else:\n                1. Download the dataset if needed.\n                2. Process the dataset and build the score graph.\n                3. Save the processed dataset into files.\n        \"\"\"\n        load_flag = not self._force_reload and self.has_cache()\n\n        if load_flag:\n            try:\n                self.load()\n                if self.verbose:\n                    print('Done loading data from cached files.')\n            except KeyboardInterrupt:\n                raise\n            except:\n                load_flag = False\n                print('Loading from cache failed, re-processing.')\n\n        if not load_flag:\n            self._download()\n            if self.verbose:\n                print('Preprocessing data...')\n            self.process()\n            if self.verbose:\n                print('Saving preprocessed data...')\n            self.save()\n            if self.verbose:\n                print('Done saving data into cached files.')\n\n    def _get_hash(self):\n        hash_func = hashlib.sha1()\n        hash_func.update(str(self._hash_key).encode('utf-8'))\n        return hash_func.hexdigest()[:8]\n\n    @property\n    def url(self):\n        \"\"\"Get url to download the raw dataset.\n        \"\"\"\n        return self._url\n\n    @property\n    def name(self):\n        r\"\"\"Name of the dataset.\n        \"\"\"\n        return self._name\n\n    @property\n    def raw_dir(self):\n        r\"\"\"Raw file directory contains the input data folder.\n        \"\"\"\n        return self._raw_dir\n\n    @property\n    def raw_path(self):\n        r\"\"\"Directory contains the input data files.\n            By default raw_path = os.path.join(self.raw_dir, self.name)\n        \"\"\"\n        return os.path.join(self.raw_dir, self.name)\n\n    @property\n    def save_dir(self):\n        r\"\"\"Directory to save the processed dataset.\n        \"\"\"\n        return self._save_dir\n\n    @property\n    def save_path(self):\n        r\"\"\"Path to save the processed dataset.\n        \"\"\"\n        return os.path.join(self._save_dir, self.name)\n\n    @property\n    def verbose(self):\n        r\"\"\"Whether to print information.\n        \"\"\"\n        return self._verbose\n\n    @property\n    def hash(self):\n        r\"\"\"Hash value for the dataset and the setting.\n        \"\"\"\n        return self._hash\n\n    @abc.abstractmethod\n    def __getitem__(self, idx):\n        r\"\"\"Gets the data object at index.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __len__(self):\n        r\"\"\"The number of examples in the dataset.\"\"\"\n        pass", "\n\nclass BuiltinDataset(StrutturaDataset):\n    \"\"\"The Basic Struttura Builtin Dataset.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset.\n    url : str\n        Url to download the raw dataset.\n    raw_dir : str\n        Specifying the directory that will store the\n        downloaded data or the directory that\n        already stores the input data.\n        Default: ~/.struttura/\n    hash_key : tuple\n        A tuple of values as the input for the hash function.\n        Users can distinguish instances (and their caches on the disk)\n        from the same dataset class by comparing the hash values.\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose: bool\n        Whether to print out progress information. Default: False\n    is_zip : bool\n    \"\"\"\n    def __init__(self, name, url, raw_dir=None, hash_key=(), force_reload=False, verbose=False, is_zip=False, clone=False, branch=None):\n        self.is_zip = is_zip\n        self.force_reload = force_reload\n        self.clone = clone if not is_zip else False\n        if self.clone:\n            self.branch = \"master\" if branch is None else branch\n        else:\n            self.branch = None\n        super(BuiltinDataset, self).__init__(\n            name,\n            url=url,\n            raw_dir=raw_dir,\n            save_dir=None,\n            hash_key=hash_key,\n            force_reload=force_reload,\n            verbose=verbose)\n\n    def download(self):\n        if self.is_zip or \".zip\" in self.url:\n            download(self.url, path=os.path.join(self.raw_path, \"raw.zip\"), extract=True)\n        elif \"https://github.com/\" in self.url or self.clone:\n            repo_path = os.path.join(self.raw_dir, self.name)\n            Repo.clone_from(self.url, repo_path, single_branch=True, b=self.branch, depth=1)\n        else:\n            raise ValueError(\"Unknown url: {}\".format(self.url))", "\n"]}
{"filename": "vocsep/data/__init__.py", "chunked_list": ["from .dataset import BuiltinDataset, StrutturaDataset\nfrom .datasets import *\nfrom .datamodules import *\n"]}
{"filename": "vocsep/data/vocsep.py", "chunked_list": ["from vocsep.data import StrutturaDataset\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nimport partitura\nimport os\nfrom vocsep.utils import hetero_graph_from_note_array, select_features, HeteroScoreGraph, load_score_hgraph\nfrom vocsep.models.core import positional_encoding\nimport torch\nimport partitura as pt\nimport gc", "import partitura as pt\nimport gc\nimport numpy as np\nfrom numpy.lib.recfunctions import structured_to_unstructured\n\n\n\nclass GraphVoiceSeparationDataset(StrutturaDataset):\n    r\"\"\"Parent class for Graph Voice Sepration Datasets.\n\n    Parameters\n    -----------\n    dataset_base : StrutturaDataset\n        The Base Dataset.\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n\n    def __init__(\n            self, dataset_base, is_pyg=False, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=2, include_measures=False\n    ):\n        self.dataset_base = dataset_base\n        self.dataset_base.process()\n        if verbose:\n            print(\"Loaded {} Successfully, now processing...\".format(dataset_base.name))\n        self.graphs = list()\n        self.n_jobs = nprocs\n        self.dropped_notes = 0\n        self.pot_edges_max_dist = pot_edges_dist\n        self.is_pyg = is_pyg\n        self._force_reload = force_reload\n        self.include_measures = include_measures\n        if self.is_pyg:\n            name = self.dataset_base.name.split(\"Dataset\")[0] + \"PGGraphVoiceSeparationDataset\"\n        else:\n            name = self.dataset_base.name.split(\"Dataset\")[0] + \"GraphVoiceSeparationDataset\"\n        print(\"pot_edges_max_dist\", self.pot_edges_max_dist)\n        super(GraphVoiceSeparationDataset, self).__init__(\n            name=name,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n        Parallel(n_jobs=self.n_jobs)(\n            delayed(self._process_score)(score, collection)\n            for score, collection in tqdm(\n                zip(self.dataset_base.scores, self.dataset_base.collections)\n            )\n        )\n        self.load()\n\n    def _process_score(self, score_fn, collection):\n        if self._force_reload or \\\n                not (os.path.exists(\n                os.path.join(\n                    self.save_path, os.path.splitext(os.path.basename(score_fn))[0]\n                )) or \\\n                os.path.exists(\n                os.path.join(\n                    self.save_path, os.path.splitext(os.path.basename(score_fn))[0] + \".pt\"\n                ))):\n            try:\n                score = partitura.load_score(score_fn)\n                if len(score.parts) ==0:\n                    print(\"Something is wrong with the score\", score_fn)\n                    return\n            except:\n                print(\"Something is wrong with the score\", score_fn)\n                return\n            note_array = score.note_array(\n                include_time_signature=True,\n                include_grace_notes=True,\n                include_staff=True,\n            )\n            note_array, num_dropped = preprocess_na_to_monophonic(note_array, score_fn)\n            self.dropped_notes += num_dropped\n            # Compute the truth edges\n            truth_edges = get_mcma_truth_edges(note_array).numpy()\n            note_features = select_features(note_array, \"voice\")\n            nodes, edges, pot_edges = hetero_graph_from_note_array(note_array, pot_edge_dist=self.pot_edges_max_dist)\n            hg = HeteroScoreGraph(\n                note_features,\n                edges,\n                name=os.path.splitext(os.path.basename(score_fn))[0],\n                labels=truth_edges,\n                note_array=note_array,\n            )\n            if self.include_measures:\n                measures = score[0].measures\n                hg._add_beat_nodes()\n                hg._add_measure_nodes(measures)\n            # Adding positional encoding to the graph features.\n            pos_enc = positional_encoding(hg.edge_index, len(hg.x), 20)\n            hg.x = torch.cat((hg.x, pos_enc), dim=1)\n            pot_edges = get_mcma_potential_edges(hg, max_dist=self.pot_edges_max_dist)\n            # compute the truth edges mask over potential edges\n            truth_edges_mask, dropped_truth_edges = get_edges_mask(truth_edges, pot_edges, check_strict_subset=True)\n            hg.y = truth_edges_mask\n            setattr(hg, \"truth_edges\", truth_edges)\n            # Save edges to use for prediction as a new attribute of the graph.\n            setattr(hg, \"pot_edges\", torch.tensor(pot_edges))\n            # Save collection as an attribute of the graph.\n            setattr(hg, \"collection\", collection)\n            setattr(hg, \"truth_edges_mask\", truth_edges_mask)\n            setattr(hg, \"dropped_truth_edges\", dropped_truth_edges)\n            hg.save(self.save_path)\n            del hg, note_array, truth_edges, nodes, edges, note_features, score\n            gc.collect()\n        return\n\n    def save(self):\n        \"\"\"save the graph list and the labels\"\"\"\n        pass\n\n    def has_cache(self):\n        if self.is_pyg:\n            if all(\n                    [os.path.exists(os.path.join(self.save_path, os.path.splitext(os.path.basename(path))[0] + \".pt\",))\n                     for path in self.dataset_base.scores]\n            ):\n                return True\n        else:\n            if all(\n                    [os.path.exists(os.path.join(self.save_path, os.path.splitext(os.path.basename(path))[0]))\n                     for path in self.dataset_base.scores]\n            ):\n                return True\n        return False\n\n    def load(self):\n        for fn in os.listdir(self.save_path):\n            path_graph = os.path.join(self.save_path, fn)\n            graph = load_score_hgraph(path_graph, fn)\n            self.graphs.append(graph)\n\n    def __getitem__(self, idx):\n        if self.is_pyg:\n            return [[self.graphs[i]] for i in idx]\n        else:\n            if self.include_measures:\n                return [\n                [\n                    self.graphs[i].x,\n                    self.graphs[i].edge_index,\n                    self.graphs[i].y,\n                    self.graphs[i].edge_type,\n                    self.graphs[i].pot_edges,\n                    self.graphs[i].truth_edges,\n                    structured_to_unstructured(\n                        self.graphs[i].note_array[[\"pitch\", \"onset_div\", \"duration_div\", \"onset_beat\", \"duration_beat\", \"ts_beats\"]]\n                    ),\n                    self.graphs[i].name,\n                    self.graphs[i].beat_nodes,\n                    self.graphs[i].beat_edges,\n                    self.graphs[i].measure_nodes,\n                    self.graphs[i].measure_edges,\n                ]\n                for i in idx\n            ]\n            return [\n                [\n                    self.graphs[i].x,\n                    self.graphs[i].edge_index,\n                    self.graphs[i].y,\n                    self.graphs[i].edge_type,\n                    self.graphs[i].pot_edges,\n                    self.graphs[i].truth_edges,\n                    structured_to_unstructured(\n                        self.graphs[i].note_array[[\"pitch\", \"onset_div\", \"duration_div\", \"onset_beat\", \"duration_beat\", \"ts_beats\"]]\n                    ),\n                    self.graphs[i].name,\n                ]\n                for i in idx\n            ]\n\n    def __len__(self):\n        return len(self.graphs)\n\n    @property\n    def features(self):\n        if self.is_pyg:\n            return self.graphs[0][\"note\"].x.shape[-1]\n        else:\n            if self.graphs[0].node_features:\n                return self.graphs[0].node_features\n            else:\n                return self.graphs[0].x.shape[-1]\n\n    @property\n    def metadata(self):\n        if self.is_pyg:\n            return self.graphs[0].metadata()\n        else:\n            return None\n\n    def num_dropped_truth_edges(self):\n        if self.is_pyg:\n            return sum([len(graph[\"dropped_truth_edges\"]) for graph in self.graphs])\n        else:\n            return None\n\n    def get_positive_weight(self):\n        if self.is_pyg:\n            return sum([len(g.truth_edges_mask)/torch.sum(g.truth_edges_mask) for g in self.graphs])/len(self.graphs)\n        else:\n            raise Exception(\"Get positive weight not supported for non pyg graphs\")", "\n\ndef get_mcma_potential_edges(hg, max_dist=16):\n    \"\"\"Get potential edges for the MCMADataset.\"\"\"\n    # Compute which edge to use for prediction.\n    onset_edges = hg.get_edges_of_type(\"onset\")\n    during_edges = hg.get_edges_of_type(\"during\")\n    consecutive_edges = hg.get_edges_of_type(\"consecutive\")\n    consecutive_dense = torch.sparse_coo_tensor(consecutive_edges, torch.ones(consecutive_edges.shape[1]),\n                                                size=(len(hg.x), len(hg.x))).to_dense()\n    predsub_edges = torch.cat((onset_edges, during_edges), dim=1)\n    trim_adj = torch.sparse_coo_tensor(predsub_edges, torch.ones(predsub_edges.shape[1]),\n                                       size=(len(hg.x), len(hg.x)))\n    trim_adj = trim_adj.to_dense()\n    # Remove onset and during edges from a full adjacency matrix.\n    trim_adj = torch.ones((len(hg.x), len(hg.x))) - trim_adj\n    # Take only the upper triangular part of the adjacency matrix.\n    # without the self loops (diagonal=1)\n    trim_adj = torch.triu(trim_adj, diagonal=1)\n    # remove indices that are further than x units apart.\n    trim_adj = trim_adj - torch.triu(trim_adj, diagonal=max_dist)\n    # readd consecutive edges if they were deleted\n    trim_adj[consecutive_dense == 1] = 1\n    # transform to edge index\n    pot_edges = trim_adj.nonzero().t()\n    return pot_edges", "\n\ndef get_mcma_truth_edges(note_array):\n    \"\"\"Get the ground truth edges for the MCMA dataset.\n    Parameters\n    ----------\n    note_array : np.array\n        The note array of the score.\n    Returns\n    -------\n    truth_edges : np.array\n        Ground truth edges.\n    \"\"\"\n    part_ids = np.char.partition(note_array[\"id\"], sep=\"_\")[:, 0]\n    truth_edges = list()\n    # Append edges for consecutive notes in the same voice.\n    for un in np.unique(part_ids):\n        # Sort indices which are in the same voice.\n        voc_inds = np.sort(np.where(part_ids == un)[0])\n        # edge indices between consecutive notes in the same voice.\n        truth_edges.append(np.vstack((voc_inds[:-1], voc_inds[1:])))\n    truth_edges = np.hstack(truth_edges)\n    return torch.from_numpy(truth_edges)", "\n\ndef get_edges_mask(subset_edges, total_edges, transpose=True, check_strict_subset=False):\n    \"\"\"Get a mask of edges to use for training.\n    Parameters\n    ----------\n    subset_edges : np.array\n        A subset of total_edges.\n    total_edges : np.array\n        Total edges.\n    transpose : bool, optional.\n        Whether to transpose the subset_edges, by default True.\n        This is necessary if the input arrays are (2, n) instead of (n, 2)\n    check_strict_subset : bool, optional\n        Whether to check that the subset_edges are a strict subset of total_edges.\n    Returns\n    -------\n    edges_mask : np.array\n        Mask that identifies subset edges from total_edges.\n    dropped_edges : np.array\n        Truth edges that are not in potential edges.\n        This is only returned if check_strict_subset is True.\n    \"\"\"\n    # convert to numpy, custom types are not supported by torch\n    total_edges = total_edges.numpy() if not isinstance(total_edges, np.ndarray) else total_edges\n    subset_edges = subset_edges.numpy() if not isinstance(subset_edges, np.ndarray) else subset_edges\n    # transpose if r; contiguous is required for the type conversion step later\n    if transpose:\n        total_edges = np.ascontiguousarray(total_edges.T)\n        subset_edges = np.ascontiguousarray(subset_edges.T)\n    # convert (n, 2) array to an n array of bytes, in order to use isin, that only works with 1d arrays\n    # view_total = total_edges.view(np.dtype((np.void, total_edges.dtype.itemsize * total_edges.shape[-1])))\n    # view_subset = subset_edges.view(np.dtype((np.void, subset_edges.dtype.itemsize * subset_edges.shape[-1])))\n    view_total = np.char.array(total_edges.astype(str))\n    view_subset = np.char.array(subset_edges.astype(str))\n    view_total = view_total[:, 0] + \"-\" + view_total[:, 1]\n    view_subset = view_subset[:, 0] + \"-\" + view_subset[:, 1]\n    if check_strict_subset:\n        dropped_edges = subset_edges[(~np.isin(view_subset, view_total))]\n        if dropped_edges.shape[0] > 0:\n            print(f\"{dropped_edges.shape[0]} truth edges are not part of potential edges\")\n        return torch.from_numpy(np.isin(view_total, view_subset)).squeeze(), dropped_edges\n    else:\n        return torch.from_numpy(np.isin(view_total, view_subset)).squeeze()", "\n\ndef preprocess_na_to_monophonic(note_array, score_fn, drop_extra_voices=True, drop_chords=True):\n    \"\"\"Preprocess the note array to remove polyphonic artifacts.\n    Parameters\n    ----------\n    note_array : np.array\n        The note array of the score.\n        score_fn : str\n        The score filename.\n    drop_extra_voices : bool, optional\n        Whether to drop extra voices in parts, by default True.\n    drop_chords : bool, optional\n        Whether to drop chords all notes in chords except the highest, by default True.\n        Returns\n        -------\n    note_array : np.array\n        The preprocessed note array.\n    \"\"\"\n    num_dropped = 0\n    if drop_chords and not drop_extra_voices:\n        raise ValueError(\"Drop chords work correctly only if drop_extra_voices is True.\")\n    if drop_extra_voices:\n        # Check how many voices per part:\n        num_voices_per_part = np.count_nonzero(note_array[\"voice\"] > 1)\n        if num_voices_per_part > 0:\n            print(\"More than one voice on part of score: {}\".format(score_fn))\n            print(\"Dropping {} notes\".format(num_voices_per_part))\n            num_dropped += num_voices_per_part\n            note_array = note_array[note_array[\"voice\"] == 1]\n    if drop_chords:\n        ids_to_drop = []\n        part_ids = np.char.partition(note_array[\"id\"], sep=\"_\")[:, 0]\n        for id in np.unique(part_ids):\n            part_na = note_array[part_ids == id]\n            for onset in np.unique(part_na[\"onset_div\"]):\n                if len(part_na[part_na[\"onset_div\"] == onset]) > 1:\n                    to_drop = list(part_na[part_na[\"onset_div\"] == onset][\"id\"][:-1])\n                    num_dropped += len(to_drop)\n                    ids_to_drop.extend(to_drop)\n                    print(\"Dropping {} notes from chord in score: {}\".format(len(to_drop), score_fn))\n        return note_array[~np.isin(note_array[\"id\"], ids_to_drop)], num_dropped", "\n\ndef create_polyphonic_groundtruth(part, note_array):\n    \"\"\"Create a groundtruth for polyphonic music.\n\n    The function creates edges between (consecutive) notes in the same voice.\n    The current version only works for monophonic parts.\n    \"\"\"\n\n    edges = []\n    if isinstance(part, pt.score.Score):\n        part = part.parts[0]\n    measures = part.measures\n    # split note_array to measures where every onset_div is within range of measure start and end\n    for m in measures:\n        indices = np.nonzero((note_array[\"onset_div\"] >= m.start.t) & (note_array[\"onset_div\"] < m.end.t))[0]\n        start_counting = indices.min()\n        na = note_array[indices]\n        for voice in np.unique(na[\"voice\"]):\n            voc_indices = np.nonzero(na[\"voice\"] == voice)[0] + start_counting\n            voc_edges = np.vstack((voc_indices[:-1], voc_indices[1:]))\n            edges.append(voc_edges)\n    return np.hstack(edges).T", ""]}
{"filename": "vocsep/data/datamodules/mix_vs.py", "chunked_list": ["from pytorch_lightning import LightningDataModule\nimport torch\nfrom torch.utils.data import ConcatDataset\nfrom ..datasets import (\n    MCMAGraphVoiceSeparationDataset,\n    Bach370ChoralesGraphVoiceSeparationDataset,\n    HaydnStringQuartetGraphVoiceSeparationDataset,\n    MozartStringQuartetGraphVoiceSeparationDataset,\n)\nfrom torch.nn import functional as F", ")\nfrom torch.nn import functional as F\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\n\n\ndef idx_tuple_to_dict(idx_tuple, datasets_map):\n    \"\"\"Transforms indices of a list of tuples of indices (dataset, piece_in_dataset) \n    into a dict {dataset: [piece_in_dataset,...,piece_in_dataset]}\"\"\"\n    result_dict = defaultdict(list)\n    for x in idx_tuple:\n        result_dict[datasets_map[x][0]].append(datasets_map[x][1])\n    return result_dict", "\n\nclass GraphMixVSDataModule(LightningDataModule):\n    def __init__(\n            self, batch_size=50, num_workers=4, force_reload=False, test_collections=None, pot_edges_max_dist=2, include_measures=False\n    ):\n        super(GraphMixVSDataModule, self).__init__()\n        self.batch_size = batch_size\n        self.bucket_boundaries = [200, 300, 400, 500, 700, 1000]\n        self.num_workers = num_workers\n        self.force_reload = force_reload\n        self.include_measures = include_measures\n        self.normalize_features = True\n        self.datasets = [\n            Bach370ChoralesGraphVoiceSeparationDataset(force_reload=self.force_reload, nprocs=self.num_workers, pot_edges_dist=pot_edges_max_dist, include_measures=self.include_measures),\n            MCMAGraphVoiceSeparationDataset(force_reload=self.force_reload, nprocs=self.num_workers, pot_edges_dist=pot_edges_max_dist, include_measures=self.include_measures),\n            HaydnStringQuartetGraphVoiceSeparationDataset(force_reload=self.force_reload, nprocs=self.num_workers, pot_edges_dist=pot_edges_max_dist, include_measures=self.include_measures),\n            MozartStringQuartetGraphVoiceSeparationDataset(force_reload=self.force_reload, nprocs=self.num_workers, pot_edges_dist=pot_edges_max_dist, include_measures=self.include_measures),\n        ]\n        if not (all([d.features == self.datasets[0].features for d in self.datasets])):\n            raise Exception(\"Input dataset has different features, Datasets {} with sizes: {}\".format(\n                \" \".join([d.name for d in self.datasets]), \" \".join([str(d.features) for d in self.datasets])))\n        self.features = self.datasets[0].features\n        self.test_collections = test_collections\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n        self.datasets_map = [(dataset_i, piece_i) for dataset_i, dataset in enumerate(self.datasets) for piece_i in\n                             range(len(dataset))]\n        if self.test_collections is None:\n            idxs = range(len(self.datasets_map))\n            collections = [self.datasets[self.datasets_map[i][0]].graphs[self.datasets_map[i][1]].collection for i\n                           in idxs]\n            trainval_idx, test_idx = train_test_split(idxs, test_size=0.3, stratify=collections, random_state=0)\n            trainval_collections = [collections[i] for i in trainval_idx]\n            train_idx, val_idx = train_test_split(trainval_idx, test_size=0.1, stratify=trainval_collections,\n                                                  random_state=0)\n\n            # structure the indices as dicts {dataset_i : [piece_i,...,piece_i]}\n            test_idx_dict = idx_tuple_to_dict(test_idx, self.datasets_map)\n            train_idx_dict = idx_tuple_to_dict(train_idx, self.datasets_map)\n            val_idx_dict = idx_tuple_to_dict(val_idx, self.datasets_map)\n\n            # create the datasets\n            self.dataset_train = ConcatDataset([self.datasets[k][train_idx_dict[k]] for k in train_idx_dict.keys()])\n            self.dataset_val = ConcatDataset([self.datasets[k][val_idx_dict[k]] for k in val_idx_dict.keys()])\n            self.dataset_test = ConcatDataset([self.datasets[k][test_idx_dict[k]] for k in test_idx_dict.keys()])\n            print(\"Running on all collections\")\n            print(\n                f\"Train size :{len(self.dataset_train)}, Val size :{len(self.dataset_val)}, Test size :{len(self.dataset_test)}\"\n            )\n        else:\n            # idxs = torch.randperm(len(self.datasets_map)).long()\n            idxs = range(len(self.datasets_map))\n            test_idx = [\n                i\n                for i in idxs\n                if self.datasets[self.datasets_map[i][0]].graphs[\n                       self.datasets_map[i][1]].collection in self.test_collections\n            ]\n            trainval_idx = [i for i in idxs if i not in test_idx]\n            trainval_collections = [\n                self.datasets[self.datasets_map[i][0]].graphs[self.datasets_map[i][1]].collection for i in\n                trainval_idx]\n            train_idx, val_idx = train_test_split(trainval_idx, test_size=0.1, stratify=trainval_collections,\n                                                  random_state=0)\n            # nidx = int(len(trainval_idx) * 0.9)\n            # train_idx = trainval_idx[:nidx]\n            # val_idx = trainval_idx[nidx:]\n\n            # structure the indices as dicts {dataset_i : [piece_i,...,piece_i]}\n            test_idx_dict = idx_tuple_to_dict(test_idx, self.datasets_map)\n            train_idx_dict = idx_tuple_to_dict(train_idx, self.datasets_map)\n            val_idx_dict = idx_tuple_to_dict(val_idx, self.datasets_map)\n\n            # create the datasets\n            self.dataset_train = ConcatDataset([self.datasets[k][train_idx_dict[k]] for k in train_idx_dict.keys()])\n            self.dataset_val = ConcatDataset([self.datasets[k][val_idx_dict[k]] for k in val_idx_dict.keys()])\n            self.dataset_test = ConcatDataset([self.datasets[k][test_idx_dict[k]] for k in test_idx_dict.keys()])\n            print(f\"Running evaluation on collections {self.test_collections}\")\n            print(\n                f\"Train size :{len(self.dataset_train)}, Val size :{len(self.dataset_val)}, Test size :{len(self.dataset_test)}\"\n            )\n\n    def collate_fn(self, batch):\n        if self.include_measures:\n            batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name, beat_nodes, beat_index, measure_nodes, measure_index = batch[0]\n            beat_nodes = torch.tensor(beat_nodes).long().squeeze()\n            beat_index = torch.tensor(beat_index).long().squeeze()\n            measure_nodes = torch.tensor(measure_nodes).long().squeeze()\n            measure_index = torch.tensor(measure_index).long().squeeze()\n        else:\n            batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name = batch[0]\n        batch_inputs = F.normalize(batch_inputs.squeeze(0).float()) if self.normalize_features else batch_inputs.squeeze(0).float()\n        batch_label = batch_label.squeeze(0)\n        edges = edges.squeeze(0)\n        edge_type = edge_type.squeeze(0)\n        pot_edges = pot_edges.squeeze(0)\n        truth_edges = torch.tensor(truth_edges.squeeze()).to(pot_edges.device)\n        na = torch.tensor(na).float()\n        if self.include_measures:\n            return batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name, beat_nodes, beat_index, measure_nodes, measure_index\n        else:\n            return batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name\n\n    # def collate_train_fn(self, examples):\n    #     lengths = list()\n    #     x = list()\n    #     edge_index = list()\n    #     edge_types = list()\n    #     y = list()\n    #     note_array = list()\n    #     potential_edges = list()\n    #     true_edges = list()\n    #     max_idx = []\n    #     beats = []\n    #     beat_eindex = []\n    #     measures = []\n    #     measure_eindex = []\n    #     for e in examples:\n    #         if self.include_measures:\n    #             batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name, beat_nodes, beat_index, measure_nodes, measure_index = e\n    #             beats.append(torch.tensor(beat_nodes).long())\n    #             beat_eindex.append(torch.tensor(beat_index).long())\n    #             measures.append(torch.tensor(measure_nodes).long())\n    #             measure_eindex.append(torch.tensor(measure_index).long())\n    #         else:\n    #             batch_inputs, edges, batch_label, edge_type, pot_edges, truth_edges, na, name = e\n    #         x.append(batch_inputs)\n    #         lengths.append(batch_inputs.shape[0])\n    #         edge_index.append(edges)\n    #         edge_types.append(edge_type)\n    #         y.append(batch_label)\n    #         note_array.append(torch.tensor(na))\n    #         max_idx.append(batch_inputs.shape[0])\n    #         potential_edges.append(pot_edges)\n    #         true_edges.append(torch.tensor(truth_edges).long())\n    #     lengths = torch.tensor(lengths).long()\n    #     lengths, perm_idx = lengths.sort(descending=True)\n    #     perm_idx = perm_idx.tolist()\n    #     max_idx = np.cumsum(np.array([0] + [max_idx[i] for i in perm_idx]))\n    #     x = torch.cat([x[i] for i in perm_idx], dim=0).float()\n    #     edge_index = torch.cat([edge_index[pi]+max_idx[i] for i, pi in enumerate(perm_idx)], dim=1).long()\n    #     potential_edges = torch.cat([potential_edges[pi]+max_idx[i] for i, pi in enumerate(perm_idx)], dim=1).long()\n    #     true_edges = torch.cat([true_edges[pi]+max_idx[i] for i, pi in enumerate(perm_idx)], dim=1).long()\n    #     edge_types = torch.cat([edge_types[i] for i in perm_idx], dim=0).long()\n    #     y = torch.cat([y[i] for i in perm_idx], dim=0).long()\n    #     note_array = torch.cat([note_array[i] for i in perm_idx], dim=0).float()\n    #     if self.include_measures:\n    #         max_beat_idx = np.cumsum(np.array([0] + [beats[i].shape[0] for i in perm_idx]))\n    #         beats = torch.cat([beats[i] + max_beat_idx[i] for i in perm_idx], dim=0).long()\n    #         beat_eindex = torch.cat([torch.vstack((beat_eindex[pi][0] + max_idx[i], beat_eindex[pi][1] + max_beat_idx[i])) for i, pi in enumerate(perm_idx)], dim=1).long()\n    #         max_measure_idx = np.cumsum(np.array([0] + [measures[i].shape[0] for i in perm_idx]))\n    #         measures = torch.cat([measures[i] + max_measure_idx[i] for i in perm_idx], dim=0).long()\n    #         measure_eindex = torch.cat([torch.vstack((measure_eindex[pi][0] + max_idx[i], measure_eindex[pi][1] + max_measure_idx[i])) for i, pi in enumerate(perm_idx)], dim=1).long()\n    #         data = x, edge_index, y, edge_types, potential_edges, true_edges, note_array, \"batch\", beats, beat_eindex, measures, measure_eindex\n    #         return data\n    #     else:\n    #         return x, edge_index, y, edge_types, potential_edges, true_edges, note_array, \"batch\"\n\n    def train_dataloader(self):\n        # sampler = BySequenceLengthSampler(self.dataset_train, self.bucket_boundaries, self.batch_size)\n        return torch.utils.data.DataLoader(\n            self.dataset_train, batch_size=1, num_workers=self.num_workers, collate_fn=self.collate_fn\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dataset_val, batch_size=1, num_workers=self.num_workers, collate_fn=self.collate_fn\n        )\n\n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dataset_test, batch_size=1, num_workers=self.num_workers, collate_fn=self.collate_fn\n        )", "\n"]}
{"filename": "vocsep/data/datamodules/__init__.py", "chunked_list": ["from .mix_vs import GraphMixVSDataModule\n"]}
{"filename": "vocsep/data/datasets/haydn_string_quartets.py", "chunked_list": ["import os\nfrom ..dataset import BuiltinDataset\nfrom ..vocsep import GraphVoiceSeparationDataset\n\n\nclass HaydnStringQuartetDataset(BuiltinDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True):\n        url = \"https://github.com/musedata/humdrum-haydn-quartets\"\n        super(HaydnStringQuartetDataset, self).__init__(\n            name=\"HaydnStringQuartets\",\n            url=url,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        root = os.path.join(self.raw_path, \"kern\")\n        self.scores = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(\".krn\")]\n        self.collections = [\"haydn\"]*len(self.scores)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            return True\n        return False", "\n\nclass HaydnStringQuartetGraphVoiceSeparationDataset(GraphVoiceSeparationDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=2, include_measures=False):\n        r\"\"\"The Haydn String Quartet Graph Voice Separation Dataset.\n\n    Four-part Haydn string quartets digital edition of the quartets composed by Joseph Haydn,\n    encoded in the Humdrum file format.\n\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if Haydn String Quartet Dataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n        dataset_base = HaydnStringQuartetDataset(raw_dir=raw_dir)\n        super(HaydnStringQuartetGraphVoiceSeparationDataset, self).__init__(\n            dataset_base=dataset_base,\n            raw_dir=raw_dir,\n            is_pyg=False,\n            force_reload=force_reload,\n            verbose=verbose,\n            nprocs=nprocs,\n            pot_edges_dist=pot_edges_dist,\n            include_measures=include_measures,\n        )", ""]}
{"filename": "vocsep/data/datasets/mozart_string_quartets.py", "chunked_list": ["import os\nfrom ..dataset import BuiltinDataset\nfrom ..vocsep import GraphVoiceSeparationDataset\n\n\nclass MozartStringQuartetDataset(BuiltinDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True):\n        url = \"https://github.com/musedata/humdrum-mozart-quartets\"\n        super(MozartStringQuartetDataset, self).__init__(\n            name=\"MozartStringQuartets\",\n            url=url,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        root = os.path.join(self.raw_path, \"kern\")\n        self.scores = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(\".krn\")]\n        self.collections = [\"mozart\"]*len(self.scores)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            return True\n        return False", "\n\nclass MozartStringQuartetGraphVoiceSeparationDataset(GraphVoiceSeparationDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=2, include_measures=False):\n        r\"\"\"The Mozart String Quartet Graph Voice Separation Dataset.\n\n    Four-part Mozart string quartets digital edition of the quartets composed by Wolfgang Amadeus Mozart,\n    encoded in the Humdrum file format.\n\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if Mozart String Quartet Dataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n        dataset_base = MozartStringQuartetDataset(raw_dir=raw_dir)\n        super(MozartStringQuartetGraphVoiceSeparationDataset, self).__init__(\n            dataset_base=dataset_base,\n            raw_dir=raw_dir,\n            is_pyg=False,\n            force_reload=force_reload,\n            verbose=verbose,\n            nprocs=nprocs,\n            pot_edges_dist=pot_edges_dist,\n            include_measures=include_measures,\n        )", ""]}
{"filename": "vocsep/data/datasets/mcma.py", "chunked_list": ["from vocsep.utils.hgraph import *\nimport os\nfrom ..dataset import BuiltinDataset\nfrom ..vocsep import GraphVoiceSeparationDataset\n\n\nclass MCMADataset(BuiltinDataset):\n    \"\"\"\n    The Multitrack Contrapuntal Music Archive (MCMA) is a symbolic dataset of pieces specifically\n    collated and edited to comprise, for any given polyphonic work, independent parts.\n\n    The dataset is available at https://gitlab.com/skalo/mcma\n    \"\"\"\n\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, presplit=False):\n        url = \"https://gitlab.com/skalo/mcma/-/archive/master/mcma-master.zip?path=mcma\"\n        self.is_processed = False\n        super(MCMADataset, self).__init__(\n            name=\"MCMADataset\",\n            url=url,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n            is_zip=True,\n        )\n\n    # def process(self):\n    #     import zipfile\n\n    #     self.scores = list()\n    #     if (not self.is_processed) or self._force_reload:\n    #         for root, dirs, files in os.walk(self.save_path):\n    #             for file in files:\n    #                 if file.endswith(\".mxl\"):\n    #                     # extract and delete the compressed\n    #                     compressed_path = os.path.join(root, file)\n    #                     uncompressed_path = compressed_path.replace(\".mxl\", \".xml\")\n    #                     with zipfile.ZipFile(compressed_path, \"r\") as zip_ref:\n    #                         zip_ref.extractall(root)\n    #                     os.remove(compressed_path)\n    #                     # append the uncompressed path\n    #                     self.scores.append(uncompressed_path)\n    #                 # elif file.endswith(\".xml\"):\n    #                 #     self.scores.append(os.path.join(root, file))\n\n    # # Old processing loop that was used to decompress mxl files.\n    def process(self):\n        import zipfile\n\n        # This function decompresses every .mxl file in the dataset and converts it to a .xml file\n        for root, dirs, files in os.walk(self.save_path):\n            for file in files:\n                if file.endswith(\".mxl\"):\n                    with zipfile.ZipFile(os.path.join(root, file), \"r\") as zip_ref:\n                        zip_ref.extractall(root)\n\n        # The extraction puts the extracted files in a folder called .musicxml\n        # It has created two .xml files with one dummy called container so we skip loading this.\n        self.scores = list()\n        self.collections = list()\n        for root, dirs, files in os.walk(self.save_path):\n            for file in files:\n                if file.endswith(\".xml\") and (not file.startswith(\"container\")):\n                    self.scores.append(os.path.join(root, file))\n                    # get the name of the collection\n                    collection = root.split(os.sep)[-1]\n                    if collection.startswith(\"the_well-tempered_clavier_book\"):\n                        if file.endswith(\"-2.xml\") or file.endswith(\"-F.xml\"):\n                            collection = root.split(os.sep)[-1]\n                        else:\n                            collection = \"preludes\"\n                    self.collections.append(collection)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            self.is_processed = True\n            return True\n\n        return False", "\n\nclass MCMAGraphVoiceSeparationDataset(GraphVoiceSeparationDataset):\n    r\"\"\"The MCMADataset Graph Voice Separation Dataset.\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if MCMADataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=1, include_measures=False):\n        dataset_base = MCMADataset(raw_dir=raw_dir)\n        super(MCMAGraphVoiceSeparationDataset, self).__init__(\n            dataset_base=dataset_base,\n            is_pyg=False,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n            nprocs=nprocs,\n            pot_edges_dist=pot_edges_dist,\n            include_measures=include_measures,\n        )", "\n\ndef get_mcma_potential_edges(hg, max_dist=16):\n    raise Exception(\"Not goood, you should not be here\")\n    \"\"\"Get potential edges for the MCMADataset.\"\"\"\n    # Compute which edge to use for prediction.\n    onset_edges = hg.get_edges_of_type(\"onset\")\n    during_edges = hg.get_edges_of_type(\"during\")\n    consecutive_edges = hg.get_edges_of_type(\"consecutive\")\n    consecutive_dense = torch.sparse_coo_tensor(consecutive_edges, torch.ones(consecutive_edges.shape[1]),\n                                    size=(len(hg.x), len(hg.x))).to_dense()\n    predsub_edges = torch.cat((onset_edges, during_edges), dim=1)\n    trim_adj = torch.sparse_coo_tensor(predsub_edges, torch.ones(predsub_edges.shape[1]),\n                                    size=(len(hg.x), len(hg.x)))\n    trim_adj = trim_adj.to_dense()\n    # Remove onset and during edges from a full adjacency matrix.\n    trim_adj = torch.ones((len(hg.x), len(hg.x))) - trim_adj\n    # Take only the upper triangular part of the adjacency matrix.\n    # without the self loops (diagonal=1)\n    trim_adj = torch.triu(trim_adj, diagonal=1)\n    # remove indices that are further than x units apart.\n    trim_adj = trim_adj - torch.triu(trim_adj, diagonal=max_dist)\n    # readd consecutive edges if they were deleted\n    trim_adj[consecutive_dense==1] = 1\n    # transform to edge index\n    pot_edges = pyg.utils.sparse.dense_to_sparse(trim_adj)[0]\n    return pot_edges", "\ndef get_mcma_truth_edges(note_array):\n    \"\"\"Get the ground truth edges for the MCMA dataset.\n    Parameters\n    ----------\n    note_array : np.array\n        The note array of the score.\n    Returns\n    -------\n    truth_edges : np.array\n        Ground truth edges.\n    \"\"\"\n    part_ids = np.char.partition(note_array[\"id\"], sep=\"_\")[:, 0]\n    truth_edges = list()\n    # Append edges for consecutive notes in the same voice.\n    for un in np.unique(part_ids):\n        # Sort indices which are in the same voice.\n        voc_inds = np.sort(np.where(part_ids == un)[0])\n        # edge indices between consecutive notes in the same voice.\n        truth_edges.append(np.vstack((voc_inds[:-1], voc_inds[1:])))\n    truth_edges = np.hstack(truth_edges)\n    return torch.from_numpy(truth_edges)", "\n\ndef get_edges_mask(subset_edges, total_edges, transpose=True, check_strict_subset=False):\n    \"\"\"Get a mask of edges to use for training.\n    Parameters\n    ----------\n    subset_edges : np.array\n        A subset of total_edges.\n    total_edges : np.array\n        Total edges.\n    transpose : bool, optional.\n        Whether to transpose the subset_edges, by default True.\n        This is necessary if the input arrays are (2, n) instead of (n, 2)\n    check_strict_subset : bool, optional\n        Whether to check that the subset_edges are a strict subset of total_edges.\n    Returns\n    -------\n    edges_mask : np.array\n        Mask that identifies subset edges from total_edges.\n    dropped_edges : np.array\n        Truth edges that are not in potential edges. \n        This is only returned if check_strict_subset is True.\n    \"\"\"\n    # convert to numpy, custom types are not supported by torch\n    total_edges = total_edges.numpy() if not isinstance(total_edges, np.ndarray) else total_edges\n    subset_edges = subset_edges.numpy() if not isinstance(subset_edges, np.ndarray) else subset_edges\n    # transpose if r; contiguous is required for the type conversion step later\n    if transpose:\n        total_edges = np.ascontiguousarray(total_edges.T)\n        subset_edges = np.ascontiguousarray(subset_edges.T)\n    # convert (n, 2) array to an n array of bytes, in order to use isin, that only works with 1d arrays\n    # view_total = total_edges.view(np.dtype((np.void, total_edges.dtype.itemsize * total_edges.shape[-1])))\n    # view_subset = subset_edges.view(np.dtype((np.void, subset_edges.dtype.itemsize * subset_edges.shape[-1])))\n    view_total = np.char.array(total_edges.astype(str))\n    view_subset = np.char.array(subset_edges.astype(str))\n    view_total = view_total[:,0] + \"-\" + view_total[:,1]\n    view_subset = view_subset[:,0] + \"-\" + view_subset[:,1]\n    if check_strict_subset:\n        dropped_edges = subset_edges[(~np.isin(view_subset, view_total))]\n        if dropped_edges.shape[0] > 0:\n            print(f\"{dropped_edges.shape[0]} truth edges are not part of potential edges\")\n        return torch.from_numpy(np.isin(view_total, view_subset)).squeeze(), dropped_edges\n    else:\n        return torch.from_numpy(np.isin(view_total, view_subset)).squeeze()", "\n\ndef preprocess_na_to_monophonic(note_array, score_fn, drop_extra_voices = True, drop_chords = True):\n    \"\"\"Preprocess the note array to remove polyphonic artifacts.\n    Parameters\n    ----------\n    note_array : np.array\n        The note array of the score.\n        score_fn : str\n        The score filename.\n    drop_extra_voices : bool, optional\n        Whether to drop extra voices in parts, by default True.\n    drop_chords : bool, optional\n        Whether to drop chords all notes in chords except the highest, by default True.\n        Returns\n        -------\n    note_array : np.array\n        The preprocessed note array.\n    \"\"\"\n    if drop_chords and not drop_extra_voices:\n        raise ValueError(\"Drop chords work correctly only if drop_extra_voices is True.\")\n    if drop_extra_voices:\n        # Check how many voices per part:\n        num_voices_per_part = np.count_nonzero(note_array[\"voice\"] > 1)\n        if num_voices_per_part > 0:\n            print(\"More than one voice on part of score: {}\".format(score_fn))\n            print(\"Dropping {} notes\".format(num_voices_per_part))\n            note_array = note_array[note_array[\"voice\"] == 1]\n    if drop_chords:\n        ids_to_drop = []\n        part_ids = np.char.partition(note_array[\"id\"], sep=\"_\")[:, 0]\n        for id in np.unique(part_ids):\n            part_na = note_array[part_ids == id]\n            for onset in np.unique(part_na[\"onset_div\"]):\n                if len(part_na[part_na[\"onset_div\"] == onset]) > 1:\n                    to_drop = list(part_na[part_na[\"onset_div\"] == onset][\"id\"][:-1])\n                    ids_to_drop.extend(to_drop)\n                    print(\"Dropping {} notes from chord in score: {}\".format(len(to_drop), score_fn))\n        return note_array[~np.isin(note_array[\"id\"], ids_to_drop)]", "\n"]}
{"filename": "vocsep/data/datasets/__init__.py", "chunked_list": ["from .bach_chorales import *\nfrom .mcma import *\nfrom .haydn_string_quartets import *\nfrom .mozart_string_quartets import *\n\n"]}
{"filename": "vocsep/data/datasets/bach_chorales.py", "chunked_list": ["from vocsep.utils.hgraph import *\nfrom vocsep.utils.pianoroll import pianorolls_from_part\nimport os\nfrom ..dataset import BuiltinDataset, StrutturaDataset\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom ..vocsep import GraphVoiceSeparationDataset\n\nclass Bach370ChoralesDataset(BuiltinDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True):\n        url = \"https://github.com/craigsapp/bach-370-chorales\"\n        super(Bach370ChoralesDataset, self).__init__(\n            name=\"Bach370ChoralesDataset\",\n            url=url,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        root = os.path.join(self.raw_path, \"kern\")\n        self.scores = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(\".krn\")]\n        self.collections = [\"chorales\"] * len(self.scores)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            return True\n\n        return False", "class Bach370ChoralesDataset(BuiltinDataset):\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True):\n        url = \"https://github.com/craigsapp/bach-370-chorales\"\n        super(Bach370ChoralesDataset, self).__init__(\n            name=\"Bach370ChoralesDataset\",\n            url=url,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        root = os.path.join(self.raw_path, \"kern\")\n        self.scores = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(\".krn\")]\n        self.collections = [\"chorales\"] * len(self.scores)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            return True\n\n        return False", "\n\n# TODO : to complete\nclass Bach370ChoralesPianorollVoiceSeparationDataset(StrutturaDataset):\n    r\"\"\"The Bach 370 Chorales Graph Voice Separation Dataset.\n\n    Four-part chorales collected after J.S. Bach's death by his son C.P.E. Bach\n    (and finished by Kirnberger, J.S. Bach's student, after C.P.E. Bach's death).\n    Ordered by Breitkopf & H\u00e4rtel numbers.\n\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if Bach 370 Chorales Dataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n\n    def __init__(\n        self,\n        raw_dir=None,\n        force_reload=False,\n        verbose=True,\n        time_unit=\"beat\",\n        time_div=12,\n        musical_beat=True,\n        nprocs=4,\n    ):\n        self.dataset_base = Bach370ChoralesDataset(raw_dir=raw_dir)\n        self.dataset_base.process()\n        if verbose:\n            print(\"Loaded 370 Bach Chorales Dataset Successfully, now processing...\")\n        self.pianorolls_dicts = list()\n        self.n_jobs = nprocs\n        self.time_unit = time_unit\n        self.time_div = time_div\n        self.musical_beat = musical_beat\n        super(Bach370ChoralesPianorollVoiceSeparationDataset, self).__init__(\n            name=\"Bach370ChoralesPianorollVoiceSeparationDataset\",\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n        )\n\n    def process(self):\n        # sys.setrecursionlimit(5000)\n        def gfunc(fn, path):\n            if fn.endswith(\".krn\"):\n                return pianorolls_from_part(\n                    os.path.join(path, fn),\n                    self.time_unit,\n                    self.time_div,\n                    self.musical_beat,\n                )\n\n        path = os.path.join(self.raw_dir, \"Bach370ChoralesDataset\", \"kern\")\n        self.pianorolls_dicts = Parallel(self.n_jobs)(\n            delayed(gfunc)(fn, path) for fn in tqdm(os.listdir(path))\n        )\n\n        # for fn in tqdm(os.listdir(path)):\n        #     out = pianorolls_from_part(os.path.join(path, fn), self.time_unit, self.time_div, self.musical_beat)\n        #     self.pianorolls_dicts.append(out)\n\n    def has_cache(self):\n        if os.path.exists(self.save_path):\n            return True\n\n        return False\n\n    # TODO save as numpy arrays for faster loading.\n    def save(self):\n        \"\"\"save the pianorolls dicts\"\"\"\n        if not os.path.exists(os.path.join(self.save_path)):\n            os.mkdir(os.path.join(self.save_path))\n        for i, prdicts in tqdm(enumerate(self.pianorolls_dicts)):\n            file_path = os.path.join(\n                self.save_path, os.path.splitext(prdicts[\"path\"])[0] + \".pkl\"\n            )\n            pickle.dump(prdicts, open(file_path, \"wb\"))\n\n    def load(self):\n        for fn in os.listdir(self.save_path):\n            path_prdict = os.path.join(self.save_path, fn)\n            prdict = pickle.load(open(path_prdict, \"rb\"))\n            self.pianorolls_dicts.append(prdict)\n\n    # Return opnly dense matrices to avoid conflicts with torch dataloader.\n    def __getitem__(self, idx):\n        return [[self.pianorolls_dicts[i]] for i in idx]\n\n    def __len__(self):\n        return len(self.pianorolls_dicts)\n\n    @property\n    def save_name(self):\n        return self.name", "\n    # @property\n    # def features(self):\n    #     if self.graphs[0].node_features:\n    #         return self.graphs[0].node_features\n    #     else:\n    #         return list(range(self.graphs[0].x.shape[-1]))\n\n\nclass Bach370ChoralesGraphVoiceSeparationDataset(GraphVoiceSeparationDataset):\n    r\"\"\"The Bach 370 Chorales Graph Voice Separation Dataset.\n\n    Four-part chorales collected after J.S. Bach's death by his son C.P.E. Bach\n    (and finished by Kirnberger, J.S. Bach's student, after C.P.E. Bach's death).\n    Ordered by Breitkopf & H\u00e4rtel numbers.\n\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if Bach 370 Chorales Dataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=1, include_measures=False):\n        dataset_base = Bach370ChoralesDataset(raw_dir=raw_dir)\n        super(Bach370ChoralesGraphVoiceSeparationDataset, self).__init__(\n            dataset_base=dataset_base,\n            is_pyg=False,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n            nprocs=nprocs,\n            pot_edges_dist=pot_edges_dist,\n            include_measures=include_measures\n        )", "\nclass Bach370ChoralesGraphVoiceSeparationDataset(GraphVoiceSeparationDataset):\n    r\"\"\"The Bach 370 Chorales Graph Voice Separation Dataset.\n\n    Four-part chorales collected after J.S. Bach's death by his son C.P.E. Bach\n    (and finished by Kirnberger, J.S. Bach's student, after C.P.E. Bach's death).\n    Ordered by Breitkopf & H\u00e4rtel numbers.\n\n    Parameters\n    -----------\n    raw_dir : str\n        Raw file directory to download/contains the input data directory.\n        Dataset will search if Bach 370 Chorales Dataset scores are already available otherwise it will download it.\n        Default: ~/.struttura/\n    force_reload : bool\n        Whether to reload the dataset. Default: False\n    verbose : bool\n        Whether to print out progress information. Default: True.\n    \"\"\"\n\n    def __init__(self, raw_dir=None, force_reload=False, verbose=True, nprocs=4, pot_edges_dist=1, include_measures=False):\n        dataset_base = Bach370ChoralesDataset(raw_dir=raw_dir)\n        super(Bach370ChoralesGraphVoiceSeparationDataset, self).__init__(\n            dataset_base=dataset_base,\n            is_pyg=False,\n            raw_dir=raw_dir,\n            force_reload=force_reload,\n            verbose=verbose,\n            nprocs=nprocs,\n            pot_edges_dist=pot_edges_dist,\n            include_measures=include_measures\n        )", ""]}
{"filename": "vocsep/models/__init__.py", "chunked_list": ["from .vocsep import GraphVoiceSeparationModel, GraphVoiceSeparationModel, VoiceLinkPredictionModel, HeteroVoiceLinkPredictionModel\nfrom .core import *"]}
{"filename": "vocsep/models/vocsep/VoicePred.py", "chunked_list": ["from torch.optim import Adam\nfrom ..core import *\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import F1Score, Accuracy\nimport torch\nfrom torch_scatter import scatter_add\nfrom random import randint\nimport random\nfrom torch import nn\nfrom torch.nn import functional as F", "from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SMOTE(object):\n    \"\"\"\n    Minority Sampling with SMOTE.\n    \"\"\"\n    def __init__(self, distance='euclidian', dims=512, k=2):\n        super(SMOTE, self).__init__()\n        self.newindex = 0\n        self.k = k\n        self.dims = dims\n        self.distance_measure = distance\n\n    def populate(self, N, i, nnarray, min_samples, k):\n        while N:\n            nn = randint(0, k - 2)\n\n            diff = min_samples[nnarray[nn]] - min_samples[i]\n            gap = random.uniform(0, 1)\n\n            self.synthetic_arr[self.newindex, :] = min_samples[i] + gap * diff\n\n            self.newindex += 1\n\n            N -= 1\n\n    def k_neighbors(self, euclid_distance, k):\n        nearest_idx = torch.zeros((euclid_distance.shape[0], euclid_distance.shape[0]), dtype=torch.int64)\n\n        idxs = torch.argsort(euclid_distance, dim=1)\n        nearest_idx[:, :] = idxs\n\n        return nearest_idx[:, 1:k]\n\n    def find_k(self, X, k):\n        z = F.normalize(X, p=2, dim=1)\n        distance = torch.mm(z, z.t())\n        return self.k_neighbors(distance, k)\n\n    # TODO: Need to find a matrix version of Euclid Distance and Cosine Distance.\n    def find_k_euc(self, X, k):\n        euclid_distance = torch.zeros((X.shape[0], X.shape[0]), dtype=torch.float32)\n\n        for i in range(len(X)):\n            dif = (X - X[i]) ** 2\n            dist = torch.sqrt(dif.sum(axis=1))\n            euclid_distance[i] = dist\n\n        return self.k_neighbors(euclid_distance, k)\n\n    def find_k_cos(self, X, k):\n        cosine_distance = torch.zeros((X.shape[0], X.shape[0]), dtype=torch.float32)\n\n        for i in range(len(X)):\n            dist = F.cosine_similarity(X, X[i].unsqueeze(0), dim=1)\n            cosine_distance[i] = dist\n\n        return self.k_neighbors(cosine_distance, k)\n\n    def generate(self, min_samples, N, k):\n        \"\"\"\n        Returns (N/100) * n_minority_samples synthetic minority samples.\n        Parameters\n        ----------\n        min_samples : Numpy_array-like, shape = [n_minority_samples, n_features]\n            Holds the minority samples\n        N : percetange of new synthetic samples:\n            n_synthetic_samples = N/100 * n_minority_samples. Can be < 100.\n        k : int. Number of nearest neighbours.\n        Returns\n        -------\n        S : Synthetic samples. array,\n            shape = [(N/100) * n_minority_samples, n_features].\n        \"\"\"\n        T = min_samples.shape[0]\n        self.synthetic_arr = torch.zeros(int(N / 100) * T, self.dims)\n        N = int(N / 100)\n        if self.distance_measure == 'euclidian':\n            indices = self.find_k_euc(min_samples, k)\n        elif self.distance_measure == 'cosine':\n            indices = self.find_k_cos(min_samples, k)\n        else:\n            indices = self.find_k(min_samples, k)\n        for i in range(indices.shape[0]):\n            self.populate(N, i, indices[i], min_samples, k)\n        self.newindex = 0\n        return self.synthetic_arr\n\n    def fit_generate(self, X, y):\n        \"\"\"\n        Over-samples using SMOTE. Returns synthetic samples concatenated at the end of the original samples.\n        Parameters\n        ----------\n        X : Numpy_array-like, shape = [n_samples, n_features]\n            The input features\n        y : Numpy_array-like, shape = [n_samples]\n            The target labels.\n\n        Returns\n        -------\n        X_resampled : Numpy_array, shape = [(n_samples + n_synthetic_samples), n_features]\n            The array containing the original and synthetic samples.\n        y_resampled : Numpy_array, shape = [(n_samples + n_synthetic_samples)]\n            The corresponding labels of `X_resampled`.\n        \"\"\"\n        # get occurence of each class\n        occ = torch.eye(int(y.max() + 1), int(y.max() + 1))[y].sum(axis=0)\n        # get the dominant class\n        dominant_class = torch.argmax(occ)\n        # get occurence of the dominant class\n        n_occ = int(occ[dominant_class].item())\n        for i in range(len(occ)):\n            # For Mini-Batch Training exclude examples with less than k occurances in the mini banch.\n            if i != dominant_class and occ[i] >= self.k:\n                # calculate the amount of synthetic data to generate\n                N = (n_occ - occ[i]) * 100 / occ[i]\n                if N != 0:\n                    candidates = X[y == i]\n                    xs = self.generate(candidates, N, self.k)\n                    # TODO Possibility to add Gaussian noise here for ADASYN approach, important for mini-batch training with respect to the max euclidian distance.\n                    X = torch.cat((X, xs.to(X.get_device()))) if X.get_device() >= 0 else torch.cat((X, xs))\n                    ys = torch.ones(xs.shape[0]) * i\n                    y = torch.cat((y, ys.to(y.get_device()))) if y.get_device() >= 0 else torch.cat((y, ys))\n        return X, y", "\nclass IdentityTuple(object):\n    def __init__(self):\n        super(IdentityTuple, self).__init__()\n\n    def fit_generate(self, X, y):\n        return X, y\n\n\nclass LinkPredictionModel(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5, alpha=3.1, smote=False, block=\"ResConv\", jk=True):\n        super(LinkPredictionModel, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = nn.LayerNorm(n_hidden)\n        self.activation = activation\n        self.use_jk = jk\n        self.block = block\n        self.alpha = alpha\n        self.dropout = nn.Dropout(dropout)\n        if block == \"ResConv\":\n            self.layers.append(ResGatedGraphConv(in_feats, n_hidden, bias=True))\n            for i in range(n_layers):\n                self.layers.append(ResGatedGraphConv(n_hidden, n_hidden, bias=True))\n        elif block == \"SageConv\" or block == \"Sage\":\n            self.layers.append(SageConvLayer(in_feats, n_hidden, bias=True))\n            for i in range(n_layers):\n                self.layers.append(SageConvLayer(n_hidden, n_hidden, bias=True))\n        else:\n            raise ValueError(\"Block type not supported\")\n        if self.use_jk:\n            self.jk = JumpingKnowledge(n_hidden, n_layers+1)\n        self.predictor = nn.Sequential(\n            nn.Linear(n_hidden*2+3, int(n_hidden)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden), int(n_hidden/2)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden/2), 1))\n\n    def reset_parameters(self):\n        nn.init.constant_(self.score_weight, 0)\n\n    def predict(self, h, edge_index, pitch_score, onset_score):\n        src, dst = edge_index\n        h_src, h_dst = h[src], h[dst]\n        score = self.predictor(torch.cat([h_src, h_dst, pitch_score, onset_score], dim=1))\n        return torch.sigmoid(score)\n\n    def embed(self, x, edge_index):\n        h = x\n        hs = list()\n        for i, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if i != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.normalize(h)\n                h = self.dropout(h)\n            hs.append(h)\n        if self.use_jk:\n            h = self.jk(hs)\n        return h\n\n    def forward(self, target_edge_index, x, embed_edge_index, pitch_score, onset_score):\n        h = self.embed(x, embed_edge_index)\n        pred = self.predict(h, target_edge_index, pitch_score, onset_score)\n        return pred", "\nclass LinkPredictionModel(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5, alpha=3.1, smote=False, block=\"ResConv\", jk=True):\n        super(LinkPredictionModel, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = nn.LayerNorm(n_hidden)\n        self.activation = activation\n        self.use_jk = jk\n        self.block = block\n        self.alpha = alpha\n        self.dropout = nn.Dropout(dropout)\n        if block == \"ResConv\":\n            self.layers.append(ResGatedGraphConv(in_feats, n_hidden, bias=True))\n            for i in range(n_layers):\n                self.layers.append(ResGatedGraphConv(n_hidden, n_hidden, bias=True))\n        elif block == \"SageConv\" or block == \"Sage\":\n            self.layers.append(SageConvLayer(in_feats, n_hidden, bias=True))\n            for i in range(n_layers):\n                self.layers.append(SageConvLayer(n_hidden, n_hidden, bias=True))\n        else:\n            raise ValueError(\"Block type not supported\")\n        if self.use_jk:\n            self.jk = JumpingKnowledge(n_hidden, n_layers+1)\n        self.predictor = nn.Sequential(\n            nn.Linear(n_hidden*2+3, int(n_hidden)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden), int(n_hidden/2)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden/2), 1))\n\n    def reset_parameters(self):\n        nn.init.constant_(self.score_weight, 0)\n\n    def predict(self, h, edge_index, pitch_score, onset_score):\n        src, dst = edge_index\n        h_src, h_dst = h[src], h[dst]\n        score = self.predictor(torch.cat([h_src, h_dst, pitch_score, onset_score], dim=1))\n        return torch.sigmoid(score)\n\n    def embed(self, x, edge_index):\n        h = x\n        hs = list()\n        for i, layer in enumerate(self.layers):\n            h = layer(h, edge_index)\n            if i != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.normalize(h)\n                h = self.dropout(h)\n            hs.append(h)\n        if self.use_jk:\n            h = self.jk(hs)\n        return h\n\n    def forward(self, target_edge_index, x, embed_edge_index, pitch_score, onset_score):\n        h = self.embed(x, embed_edge_index)\n        pred = self.predict(h, target_edge_index, pitch_score, onset_score)\n        return pred", "\n\nclass HeteroLinkPredictionModel(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5, alpha=3.1, etypes={\"onset\":0, \"consecutive\":1, \"during\":2, \"rests\":3, \"consecutive_rev\":4, \"during_rev\":5, \"rests_rev\":6}, smote=False, block=\"ResConv\", jk=True):\n        super(HeteroLinkPredictionModel, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = nn.LayerNorm(n_hidden)\n        self.activation = activation\n        self.use_jk = jk\n        self.block = block\n        self.alpha = alpha\n        self.dropout = nn.Dropout(dropout)\n        if block == \"ResConv\":\n            self.layers.append(HeteroResGatedGraphConvLayer(in_feats, n_hidden, etypes, bias=True))\n            for i in range(n_layers):\n                self.layers.append(HeteroResGatedGraphConvLayer(n_hidden, n_hidden, etypes, bias=True))\n        elif block == \"SageConv\" or block == \"Sage\":\n            self.layers.append(HeteroSageConvLayer(in_feats, n_hidden, etypes, bias=True))\n            for i in range(n_layers):\n                self.layers.append(HeteroSageConvLayer(n_hidden, n_hidden, etypes, bias=True))\n        else:\n            raise ValueError(\"Block type not supported\")\n        if self.use_jk:\n            self.jk = JumpingKnowledge(n_hidden, n_layers+1)\n        self.predictor = nn.Sequential(\n            nn.Linear(n_hidden*2+3, int(n_hidden)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden), int(n_hidden/2)),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(n_hidden/2), 1))\n\n    def reset_parameters(self):\n        nn.init.constant_(self.score_weight, 0)\n\n    def predict(self, h, edge_index, pitch_score, onset_score):\n        src, dst = edge_index\n        h_src, h_dst = h[src], h[dst]\n        score = self.predictor(torch.cat([h_src, h_dst, pitch_score, onset_score], dim=1))\n        return torch.sigmoid(score)\n\n    def embed(self, x, edge_index, edge_type):\n        h = x\n        hs = list()\n        for i, layer in enumerate(self.layers):\n            h = layer(h, edge_index, edge_type)\n            if i != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.normalize(h)\n                h = self.dropout(h)\n            hs.append(h)\n        if self.use_jk:\n            h = self.jk(hs)\n        return h\n\n    def forward(self, target_edge_index, x, embed_edge_index, edge_type, pitch_score, onset_score):\n        h = self.embed(x, embed_edge_index, edge_type)\n        pred = self.predict(h, target_edge_index, pitch_score, onset_score)\n        return pred", "\n\nclass GVocSep(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5):\n        super(GVocSep, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = nn.BatchNorm1d(n_hidden)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.smote = SMOTE(dims=n_hidden)\n        self.layers.append(SageConvLayer(in_feats, n_hidden))\n        for i in range(n_layers):\n            self.layers.append(SageConvLayer(n_hidden, n_hidden))\n        self.jk = JumpingKnowledge(n_hidden, n_layers+1)\n        self.predictor = nn.Sequential(\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, 2))\n\n    def predict(self, h_src, h_dst, y):\n        h, y = self.smote.fit_generate(h_src * h_dst, y)\n        return self.predictor(h), y.long()\n\n    def val_predict(self, h_src, h_dst):\n        return self.predictor(h_src * h_dst)\n\n    def forward(self, edge_index, adj, x, y):\n        h = x\n        hs = []\n        for i, layer in enumerate(self.layers):\n            h = layer(h, adj)\n            if i != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.normalize(h)\n                h = self.dropout(h)\n                hs.append(h)\n        h = self.jk(hs)\n        src_idx, dst_idx = edge_index\n        out, y = self.predict(h[src_idx], h[dst_idx], y)\n        return out, y\n\n    def val_forward(self, edge_index, adj, x):\n        h = x\n        hs = []\n        for i, layer in enumerate(self.layers):\n            h = layer(h, adj)\n            if i != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.normalize(h)\n                h = self.dropout(h)\n                hs.append(h)\n        h = self.jk(hs)\n        src_idx, dst_idx = edge_index\n        out = self.val_predict(h[src_idx], h[dst_idx])\n        return out", "\n\nclass NeoGNN(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5):\n        super(NeoGNN, self).__init__()\n        self.n_hidden = n_hidden\n        self.normalize = nn.BatchNorm1d(n_hidden)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.gnn = GCN(in_feats, n_hidden, n_hidden, n_layers, activation, dropout)\n        self.predictor = MLP(n_hidden, n_hidden, 1, n_layers, activation, dropout)\n        # NeoGNN args\n        self.alpha = torch.nn.Parameter(torch.FloatTensor([0, 0]))\n        self.f_edge = MLP(1, n_hidden, 1)\n        self.f_node = MLP(1, n_hidden, 1)\n        self.g_phi = MLP(1, n_hidden, 1)\n\n    def reset_parameters(self):\n        nn.init.constant_(self.alpha, 0)\n        nn.init.xavier_uniform_(self.f_edge.weight, gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self.f_node.weight, gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self.g_phi.weight, gain=nn.init.calculate_gain('relu'))\n\n    def forward(self, edge_index, x, adj):\n        h = self.gnn(adj, x)\n        out_feat = self.predictor(h[edge_index[0]] * h[edge_index[1]])\n        row_A, col_A = edge_index[0], edge_index[1]\n        edge_weight_A = torch.ones((len(edge_index[0])))\n        edge_weight_A = self.f_edge(edge_weight_A.unsqueeze(-1))\n        node_struct_feat = scatter_add(edge_weight_A, col_A, dim=0, dim_size=len(x))\n\n        edge_weight_src = adj[edge_index[0]].T * self.f_node(node_struct_feat[edge_index[0]]).squeeze()\n        edge_weight_dst = adj[edge_index[1]].T * self.f_node(node_struct_feat[edge_index[1]]).squeeze()\n\n        out_struct = torch.mm(edge_weight_src.t(), edge_weight_dst).diag()\n        out_struct = self.g_phi(out_struct.unsqueeze(-1))\n        out_struct_raw = out_struct\n        out_struct = torch.sigmoid(out_struct)\n        alpha = torch.softmax(self.alpha, dim=0)\n        out = alpha[0] * out_struct + alpha[1] * out_feat + 1e-15\n        return out, out_struct, out_feat, out_struct_raw", "\n\nclass GVAE(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_layers, activation=F.relu, dropout=0.5):\n        super(GVAE, self).__init__()\n        self.n_hidden = n_hidden\n        self.normalize = nn.BatchNorm1d(n_hidden)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.gnn = GCN(in_feats, n_hidden, n_hidden, n_layers, activation, dropout)\n        self.proj_1 = MLP(n_hidden, n_hidden, n_hidden, 1, activation, dropout)\n        self.proj_2 = MLP(n_hidden, n_hidden, n_hidden, 1, activation, dropout)\n\n    def decode(self, h):\n        z1 = self.proj_1(h)\n        z2 = self.proj_2(h)\n        z = torch.sigmoid(torch.mm(z1, z2.T))\n        return z\n\n    def encode(self, adj, x):\n        h = self.gnn(adj, x)\n        return h\n\n    def forward(self, adj, x):\n        h = self.encode(adj, x)\n        h = self.decode(h)\n        return h", "\n\nclass GaugLoss(nn.Module):\n    def __init__(self):\n        super(GaugLoss, self).__init__()\n\n    def forward(self, adj_rec, adj_tgt):\n        if adj_tgt.is_sparse:\n            shape = adj_tgt.size()\n            indices = adj_tgt._indices().T\n            adj_sum = torch.sparse.sum(adj_tgt)\n            bce_weight = (shape[0] * shape[1] - adj_sum) / adj_sum\n            norm_w = shape[0] * shape[1] / float((shape[0] * shape[1] - adj_sum) * 2)\n            bce_loss = norm_w * F.binary_cross_entropy_with_logits(torch.transpose(adj_rec[:shape[1], :shape[0]], 0, 1),\n                                                                   adj_tgt.to_dense(), pos_weight=bce_weight)\n        else:\n            shape = adj_tgt.shape\n            indices = adj_tgt.nonzero()\n            adj_sum = torch.sum(adj_tgt)\n            bce_weight = (shape[0]*shape[1] - adj_sum) / adj_sum\n            norm_w = shape[0]*shape[1] / float((shape[0]*shape[1] - adj_sum) * 2)\n            bce_loss = norm_w * F.binary_cross_entropy_with_logits(torch.transpose(adj_rec[:shape[1], :shape[0]], 0, 1), adj_tgt, pos_weight=bce_weight)\n        return bce_loss", "\n\nclass GraphVoiceSeparationModel(LightningModule):\n    def __init__(self,\n                 in_feats,\n                 n_hidden,\n                 n_layers,\n                 activation=F.relu,\n                 dropout=0.5,\n                 lr=0.001,\n                 weight_decay=5e-4,\n        ):\n        super(GraphVoiceSeparationModel, self).__init__()\n        self.save_hyperparameters()\n        self.module = GVocSep(in_feats, n_hidden, n_layers, activation, dropout)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.train_loss = nn.CrossEntropyLoss()\n        self.train_acc = Accuracy()\n        self.train_f1 = F1Score(num_classes=2, average=\"macro\")\n        self.val_acc_score = Accuracy()\n        self.val_f1_score = F1Score(num_classes=2, average=\"macro\")\n\n    def training_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_label = batch\n        batch_inputs = batch_inputs.squeeze()\n        edges = edges.squeeze().to(self.device)\n        batch_label = batch_label.squeeze()\n        batch_labels = torch.all(batch_label[edges[0]] == batch_label[edges[1]], dim=1).long().to(self.device)\n        batch_inputs = batch_inputs.to(self.device)\n        adj = torch.sparse_coo_tensor(\n            edges, torch.ones(len(edges[0])).to(self.device), (len(batch_inputs), len(batch_inputs))).to_dense().to(self.device)\n        batch_pred, batch_labels = self.module(edges, adj, batch_inputs, batch_labels)\n        loss = self.train_loss(batch_pred, batch_labels)\n        batch_acc = self.train_acc(batch_pred, batch_labels)\n        batch_f1 = self.train_f1(batch_pred, batch_labels)\n        self.log(\"train_loss\", loss.item(), prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", batch_acc.item(), prog_bar=True, on_epoch=True)\n        self.log(\"train_f1\", batch_f1.item(), prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_label = batch\n        batch_inputs = batch_inputs.squeeze()\n        edges = edges.squeeze()\n        edges = torch.cat((edges, torch.cat((edges[1].unsqueeze(0), edges[0].unsqueeze(0)))), dim=1).to(self.device)\n        batch_label = batch_label.squeeze()\n        batch_labels = torch.all(batch_label[edges[0]] == batch_label[edges[1]], dim=1).long().to(self.device)\n        batch_inputs = batch_inputs.to(self.device).to(self.device)\n        adj = torch.sparse_coo_tensor(\n            edges, torch.ones(len(edges[0])).to(self.device), (len(batch_inputs), len(batch_inputs))).to_dense().to(self.device)\n        batch_pred = self.module.val_forward(edges, adj, batch_inputs)\n\n        val_acc_score = self.val_acc_score(batch_pred, batch_labels)\n        val_f1_score = self.val_f1_score(batch_pred, batch_labels)\n        self.log(\"val_acc\", val_acc_score.item(), prog_bar=True, on_epoch=True)\n        self.log(\"val_f1\", val_f1_score.item(), prog_bar=True, on_epoch=True)\n        # return val_auc_score\n\n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        return {\n            \"optimizer\": optimizer,\n        }", "\n"]}
{"filename": "vocsep/models/vocsep/__init__.py", "chunked_list": ["from .VoicePred import GraphVoiceSeparationModel, GraphVoiceSeparationModel\nfrom .pl_models import HeteroVoiceLinkPredictionModel, VoiceLinkPredictionModel\n"]}
{"filename": "vocsep/models/vocsep/pl_models.py", "chunked_list": ["from .lightning_base import VocSepLightningModule, to_dense_adj\nfrom .VoicePred import LinkPredictionModel, HeteroLinkPredictionModel\nfrom torch.nn import functional as F\nimport torch\nfrom vocsep.utils import add_reverse_edges_from_edge_index\nfrom scipy.sparse.csgraph import connected_components\n\n\n\n\nclass VoiceLinkPredictionModel(VocSepLightningModule):\n    def __init__(\n        self,\n        in_feats,\n        n_hidden,\n        n_layers,\n        activation=F.relu,\n        dropout=0.5,\n        lr=0.001,\n        weight_decay=5e-4,\n        linear_assignment=True,\n        model=\"ResConv\",\n        jk=True,\n        reg_loss_weight=\"auto\"\n    ):\n        super(VoiceLinkPredictionModel, self).__init__(\n            in_feats,\n            n_hidden,\n            n_layers,\n            activation,\n            dropout,\n            lr,\n            weight_decay,\n            LinkPredictionModel,\n            linear_assignment=linear_assignment,\n            model_name=model,\n            jk=jk,\n            reg_loss_weight=reg_loss_weight\n        )\n\n    def training_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pos_edges = pot_edges[:, batch_labels.bool()]\n        neg_labels = torch.where(~batch_labels.bool())[0]\n        neg_edges = pot_edges[\n            :, neg_labels[torch.randperm(len(neg_labels))][: pos_edges.shape[1]]\n        ]\n        h = self.module.embed(batch_inputs, edges)\n        pos_pitch_score = self.pitch_score(pos_edges, na[:, 0])\n        pos_onset_score = self.onset_score(pos_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        neg_pitch_score = self.pitch_score(neg_edges, na[:, 0])\n        neg_onset_score = self.onset_score(neg_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pos_out = self.module.predict(h, pos_edges, pos_pitch_score, pos_onset_score)\n        neg_out = self.module.predict(h, neg_edges, neg_pitch_score, neg_onset_score)\n        reg_loss = self.reg_loss(\n            pot_edges, self.module.predict(h, pot_edges, pitch_score, onset_score), pos_edges, len(batch_inputs))\n        batch_pred = torch.cat((pos_out, neg_out), dim=0)\n        loss = self.train_loss(pos_out, neg_out)\n        batch_pred = torch.cat((1 - batch_pred, batch_pred), dim=1).squeeze()\n        targets = (\n            torch.cat(\n                (torch.ones(pos_out.shape[0]), torch.zeros(neg_out.shape[0])), dim=0\n            )\n            .long()\n            .to(self.device)\n        )\n        self.log(\"train_regloss\", reg_loss.item(), on_step=True, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weight\", self.reg_loss_weight, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weighted\", self.reg_loss_weight*reg_loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.train_metric_logging_step(loss, batch_pred, targets)\n        loss = loss + self.reg_loss_weight * reg_loss\n        self.log(\"train_joinloss\", loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, pitch_score, onset_score)\n        self.val_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def test_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, pitch_score, onset_score)\n        self.test_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def compute_linkpred_loss(self, pos_score, neg_score):\n        scores = torch.cat([pos_score, neg_score])\n        labels = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n        )\n        w_coef = pos_score.shape[0] / neg_score.shape[0]\n        weight = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.ones(neg_score.shape[0]) * w_coef]\n        )\n        return F.binary_cross_entropy(scores.squeeze(), labels, weight=weight)", "\n\nclass VoiceLinkPredictionModel(VocSepLightningModule):\n    def __init__(\n        self,\n        in_feats,\n        n_hidden,\n        n_layers,\n        activation=F.relu,\n        dropout=0.5,\n        lr=0.001,\n        weight_decay=5e-4,\n        linear_assignment=True,\n        model=\"ResConv\",\n        jk=True,\n        reg_loss_weight=\"auto\"\n    ):\n        super(VoiceLinkPredictionModel, self).__init__(\n            in_feats,\n            n_hidden,\n            n_layers,\n            activation,\n            dropout,\n            lr,\n            weight_decay,\n            LinkPredictionModel,\n            linear_assignment=linear_assignment,\n            model_name=model,\n            jk=jk,\n            reg_loss_weight=reg_loss_weight\n        )\n\n    def training_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pos_edges = pot_edges[:, batch_labels.bool()]\n        neg_labels = torch.where(~batch_labels.bool())[0]\n        neg_edges = pot_edges[\n            :, neg_labels[torch.randperm(len(neg_labels))][: pos_edges.shape[1]]\n        ]\n        h = self.module.embed(batch_inputs, edges)\n        pos_pitch_score = self.pitch_score(pos_edges, na[:, 0])\n        pos_onset_score = self.onset_score(pos_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        neg_pitch_score = self.pitch_score(neg_edges, na[:, 0])\n        neg_onset_score = self.onset_score(neg_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pos_out = self.module.predict(h, pos_edges, pos_pitch_score, pos_onset_score)\n        neg_out = self.module.predict(h, neg_edges, neg_pitch_score, neg_onset_score)\n        reg_loss = self.reg_loss(\n            pot_edges, self.module.predict(h, pot_edges, pitch_score, onset_score), pos_edges, len(batch_inputs))\n        batch_pred = torch.cat((pos_out, neg_out), dim=0)\n        loss = self.train_loss(pos_out, neg_out)\n        batch_pred = torch.cat((1 - batch_pred, batch_pred), dim=1).squeeze()\n        targets = (\n            torch.cat(\n                (torch.ones(pos_out.shape[0]), torch.zeros(neg_out.shape[0])), dim=0\n            )\n            .long()\n            .to(self.device)\n        )\n        self.log(\"train_regloss\", reg_loss.item(), on_step=True, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weight\", self.reg_loss_weight, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weighted\", self.reg_loss_weight*reg_loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.train_metric_logging_step(loss, batch_pred, targets)\n        loss = loss + self.reg_loss_weight * reg_loss\n        self.log(\"train_joinloss\", loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, pitch_score, onset_score)\n        self.val_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def test_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, pitch_score, onset_score)\n        self.test_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def compute_linkpred_loss(self, pos_score, neg_score):\n        scores = torch.cat([pos_score, neg_score])\n        labels = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n        )\n        w_coef = pos_score.shape[0] / neg_score.shape[0]\n        weight = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.ones(neg_score.shape[0]) * w_coef]\n        )\n        return F.binary_cross_entropy(scores.squeeze(), labels, weight=weight)", "\n\n\nclass HeteroVoiceLinkPredictionModel(VocSepLightningModule):\n    def __init__(\n        self,\n        in_feats,\n        n_hidden,\n        n_layers,\n        activation=F.relu,\n        dropout=0.5,\n        lr=0.001,\n        weight_decay=5e-4,\n        linear_assignment=True,\n        model=\"ResConv\",\n        jk=True,\n        reg_loss_weight=\"auto\",\n        reg_loss_type=\"la\",\n        tau=0.5\n    ):\n        super(HeteroVoiceLinkPredictionModel, self).__init__(\n            in_feats,\n            n_hidden,\n            n_layers,\n            activation,\n            dropout,\n            lr,\n            weight_decay,\n            HeteroLinkPredictionModel,\n            linear_assignment=linear_assignment,\n            model_name=model,\n            jk=jk,\n            reg_loss_weight=reg_loss_weight,\n            reg_loss_type=reg_loss_type,\n            tau=tau\n        )\n\n    def training_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pos_edges = pot_edges[:, batch_labels.bool()]\n        neg_labels = torch.where(~batch_labels.bool())[0]\n        neg_edges = pot_edges[\n            :, neg_labels[torch.randperm(len(neg_labels))][: pos_edges.shape[1]]\n        ]\n        h = self.module.embed(batch_inputs, edges, edge_types)\n        pos_pitch_score = self.pitch_score(pos_edges, na[:, 0])\n        pos_onset_score = self.onset_score(pos_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        neg_pitch_score = self.pitch_score(neg_edges, na[:, 0])\n        neg_onset_score = self.onset_score(neg_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        pos_out = self.module.predict(h, pos_edges, pos_pitch_score, pos_onset_score)\n        neg_out = self.module.predict(h, neg_edges, neg_pitch_score, neg_onset_score)\n        reg_loss = self.reg_loss(\n            pot_edges, self.module.predict(h, pot_edges, pitch_score, onset_score), pos_edges, len(batch_inputs))\n        batch_pred = torch.cat((pos_out, neg_out), dim=0)\n        loss = self.train_loss(pos_out, neg_out)\n        batch_pred = torch.cat((1 - batch_pred, batch_pred), dim=1).squeeze()\n        targets = (\n            torch.cat(\n                (torch.ones(pos_out.shape[0]), torch.zeros(neg_out.shape[0])), dim=0\n            )\n            .long()\n            .to(self.device)\n        )\n        self.log(\"train_regloss\", reg_loss.item(), on_step=True, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weight\", self.reg_loss_weight, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.log(\"regloss_weighted\", self.reg_loss_weight*reg_loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        self.train_metric_logging_step(loss, batch_pred, targets)\n        loss = loss + self.reg_loss_weight * reg_loss\n        self.log(\"train_joinloss\", loss.item(), on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, edge_types, pitch_score, onset_score)\n        self.val_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def test_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, edge_types, pitch_score, onset_score)\n        self.test_metric_logging_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n\n    def predict_step(self, batch, batch_idx):\n        batch_inputs, edges, batch_labels, edge_types, pot_edges, truth_edges, na, name = batch\n        edges, edge_types = add_reverse_edges_from_edge_index(edges, edge_types)\n        pitch_score = self.pitch_score(pot_edges, na[:, 0])\n        onset_score = self.onset_score(pot_edges, na[:, 1], na[:, 2], na[:, 3], na[:, 4], na[:, 5])\n        batch_pred = self.module(pot_edges, batch_inputs, edges, edge_types, pitch_score, onset_score)\n        adj_pred, fscore = self.predict_metric_step(\n            batch_pred, pot_edges, truth_edges, len(batch_inputs)\n        )\n        print(f\"Piece {name} F-score: {fscore}\")\n        nov_pred, voices_pred = connected_components(csgraph=adj_pred, directed=False, return_labels=True)\n        adj_target = to_dense_adj(truth_edges, max_num_nodes=len(batch_inputs)).squeeze().long().cpu()\n        nov_target, voices_target = connected_components(csgraph=adj_target, directed=False, return_labels=True)\n        return (\n            name,\n            voices_pred,\n            voices_target,\n            nov_pred,\n            nov_target,\n            na[:, 1],\n            na[:, 2],\n            na[:, 0],\n        )\n\n    def compute_linkpred_loss(self, pos_score, neg_score):\n        scores = torch.cat([pos_score, neg_score])\n        labels = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n        )\n        w_coef = pos_score.shape[0] / neg_score.shape[0]\n        weight = torch.cat(\n            [torch.ones(pos_score.shape[0]), torch.ones(neg_score.shape[0]) * w_coef]\n        )\n        return F.binary_cross_entropy(scores.squeeze(), labels, weight=weight)", "\n"]}
{"filename": "vocsep/models/vocsep/lightning_base.py", "chunked_list": ["import numpy as np\nfrom pytorch_lightning import LightningModule\nimport torch\nfrom vocsep.metrics.losses import LinkPredictionLoss, LinearAssignmentLoss, LinearAssignmentLossCE\nfrom torchmetrics import F1Score, Accuracy, Precision, Recall\nfrom vocsep.metrics.slow_eval import MonophonicVoiceF1\nfrom vocsep.metrics.eval import LinearAssignmentScore\nfrom torch.nn import functional as F\nfrom scipy.optimize import linear_sum_assignment\nfrom torch_scatter import scatter", "from scipy.optimize import linear_sum_assignment\nfrom torch_scatter import scatter\n\n\ndef to_dense_adj(edge_index, max_num_nodes):\n    num_nodes = int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    batch = edge_index.new_zeros(num_nodes)\n    batch_size = int(batch.max()) + 1 if batch.numel() > 0 else 1\n    one = batch.new_ones(batch.size(0))\n    num_nodes = scatter(one, batch, dim=0, dim_size=batch_size, reduce='sum')\n    cum_nodes = torch.cat([batch.new_zeros(1), num_nodes.cumsum(dim=0)])\n\n    idx0 = batch[edge_index[0]]\n    idx1 = edge_index[0] - cum_nodes[batch][edge_index[0]]\n    idx2 = edge_index[1] - cum_nodes[batch][edge_index[1]]\n\n    if ((idx1.numel() > 0 and idx1.max() >= max_num_nodes)\n          or (idx2.numel() > 0 and idx2.max() >= max_num_nodes)):\n        mask = (idx1 < max_num_nodes) & (idx2 < max_num_nodes)\n        idx0 = idx0[mask]\n        idx1 = idx1[mask]\n        idx2 = idx2[mask]\n        edge_attr = None\n\n    edge_attr = torch.ones(idx0.numel(), device=edge_index.device)\n    size = [batch_size, max_num_nodes, max_num_nodes]\n    size += list(edge_attr.size())[1:]\n    flattened_size = batch_size * max_num_nodes * max_num_nodes\n    idx = idx0 * max_num_nodes * max_num_nodes + idx1 * max_num_nodes + idx2\n    adj = scatter(edge_attr, idx, dim=0, dim_size=flattened_size, reduce='sum')\n    adj = adj.view(size)\n    return adj", "\n\nclass VocSepLightningModule(LightningModule):\n    \"\"\"\n    This is the Core Lightning Module for logging and computing Voice Separation with GNNs.\n    \"\"\"\n    def __init__(self, in_feats, n_hidden, n_layers, activation, dropout, lr, weight_decay, module, linear_assignment=True, model_name=\"ResConv\", jk=True, reg_loss_weight=\"auto\", reg_loss_type=\"la\", tau=0.5):\n\n        super(VocSepLightningModule, self).__init__()\n        self.save_hyperparameters()\n        if module is not None:\n            self.module = module(in_feats, n_hidden, n_layers, activation, dropout, block=model_name, jk=jk)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.threshold = tau\n        if reg_loss_weight == \"auto\":\n            self.reg_loss_type = \"auto\"\n            self.reg_loss_weight = 0.0\n        elif reg_loss_weight == \"fixed\":\n            self.reg_loss_type = \"fixed\"\n            self.reg_loss_weight = 1.0\n        elif reg_loss_weight == \"none\":\n            self.reg_loss_type = \"none\"\n            self.reg_loss_weight = 0.0\n        else:\n            self.reg_loss_weight = reg_loss_weight\n        # metrics\n        self.train_acc_score = Accuracy()\n        self.val_acc_score = Accuracy()\n        self.train_loss = LinkPredictionLoss()\n        if reg_loss_type == \"ca\":\n            self.reg_loss = LinearAssignmentLossCE()\n        else:\n            self.reg_loss = LinearAssignmentLoss()\n        self.val_loss = LinkPredictionLoss()\n\n        self.linear_assignment = linear_assignment\n        self.val_f1_score = F1Score(average=\"macro\", num_classes=1).cpu()\n        self.val_precision = Precision(average=\"macro\", num_classes=1).cpu()\n        self.val_recall = Recall(average=\"macro\", num_classes=1).cpu()\n        self.val_monvoicef1 = MonophonicVoiceF1(average=\"macro\", num_classes=2)\n        self.test_f1_score = F1Score(average=\"macro\", num_classes=1)\n        self.test_precision = Precision(average=\"macro\", num_classes=1)\n        self.test_recall = Recall(average=\"macro\", num_classes=1)\n        self.test_f1_allignment = F1Score(average=\"macro\", num_classes=1)\n        self.test_precision_allignment = Precision(average=\"macro\", num_classes=1)\n        self.test_recall_allignment = Recall(average=\"macro\", num_classes=1)\n        self.test_monvoicef1 = MonophonicVoiceF1(average=\"macro\", num_classes=2)\n        self.test_linear_assignment = LinearAssignmentScore()\n        self.val_linear_assignment = LinearAssignmentScore()\n        # Alpha and beta are hyperparams from reference for pitch and onset score.\n        self.alpha = 3.1\n        self.beta = 5\n\n    def training_step(self, *args, **kwargs):\n        \"\"\"To be re-written by child\"\"\"\n        pass\n\n    def training_epoch_end(self, *args, **kwargs):\n        if self.reg_loss_type == \"auto\":\n            self.reg_loss_weight += 0.02\n\n    def validation_step(self, *args, **kwargs):\n        \"\"\"To be re-written by child\"\"\"\n        pass\n\n    def test_step(self, *args, **kwargs):\n        \"\"\"To be re-written by child\"\"\"\n        pass\n\n    def predict_step(self, *args, **kwargs):\n        \"\"\"To be re-written by child\"\"\"\n        pass\n\n    def compute_linkpred_loss(self, pos_score, neg_score):\n        \"\"\"Standard Link Prediction loss with possitive and negative edges, no need for weighting.\"\"\"\n        scores = torch.cat([pos_score, neg_score])\n        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n        w_coef = pos_score.shape[0] / neg_score.shape[0]\n        weight = torch.cat([torch.ones(pos_score.shape[0]), torch.ones(neg_score.shape[0]) * w_coef])\n        return F.binary_cross_entropy(scores.squeeze(), labels, weight=weight)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\")\n        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50, verbose=False)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\"\n        }\n\n    def linear_assignment_step(self, batch_pred, pot_edges, target_edges, num_nodes):\n        adj_target = to_dense_adj(target_edges, max_num_nodes=num_nodes).squeeze().long().cpu()\n        if self.linear_assignment:\n            # Solve with Hungarian Algorithm\n            cost_matrix = torch.sparse_coo_tensor(\n                pot_edges, batch_pred.squeeze(), (num_nodes, num_nodes)).to_dense().cpu().numpy()\n            row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)\n            pot_edges = np.vstack((row_ind, col_ind))\n            trimmed_preds = torch.tensor(cost_matrix[row_ind, col_ind])\n            adj_pred = torch.sparse_coo_tensor(torch.tensor(pot_edges), trimmed_preds,\n                                               (num_nodes, num_nodes)).to_dense()\n            # Eliminate prediction that do not reach the threshold to fix zero nodes\n            adj_pred = (adj_pred > self.threshold).float()\n            # adj_pred = torch.round(adj_pred)\n        else:\n            pred_edges = pot_edges[:, torch.round(batch_pred).squeeze().long()]\n            # compute pred and ground truth adj matrices\n            if torch.sum(pred_edges) > 0:\n                adj_pred = to_dense_adj(pred_edges, max_num_nodes=num_nodes).squeeze().cpu()\n            else:  # to avoid exception in to_dense_adj when there is no predicted edge\n                adj_pred = torch.zeros((num_nodes, num_nodes)).squeeze().to(self.device).cpu()\n        return adj_pred, adj_target\n\n    def train_metric_logging_step(self, loss, batch_pred, targets):\n        \"\"\"Logging for the training step using the standard loss\"\"\"\n        acc = self.train_acc_score(batch_pred, targets.long())\n        self.log(\"train_loss\", loss.item(), prog_bar=True, on_step=True, on_epoch=True, batch_size=1)\n        self.log(\"train_acc\", acc.item(), prog_bar=True, on_step=False, on_epoch=True, batch_size=1)\n\n    def val_metric_logging_step(self, batch_pred, pot_edges, target_edges, num_nodes):\n        \"\"\"Logging for validation step\n        Args:\n            batch_pred: Predicted edges, 1d array of length (num_pot_edges). This is a binary mask over pot_edges.\n            pot_edges: Potential edges, shape (2, num_pot_edges). Each column is the start and destination of a potential edge.\n            target_edges: Target edges, shape (2, num_edges). Each column is the start and destination of a truth voice edge.\n            num_nodes: Number of nodes in the graph, i.e., notes in the score.\n        \"\"\"\n        adj_target = to_dense_adj(target_edges, max_num_nodes=num_nodes).squeeze().long().cpu()\n        if self.linear_assignment:\n            # Solve with Hungarian Algorithm\n            cost_matrix = torch.sparse_coo_tensor(\n                pot_edges, batch_pred.squeeze(), (num_nodes, num_nodes)).to_dense().cpu().numpy()\n            row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)\n            pot_edges = np.vstack((row_ind, col_ind))\n            trimmed_preds = torch.tensor(cost_matrix[row_ind, col_ind])\n            adj_pred = torch.sparse_coo_tensor(torch.tensor(pot_edges), trimmed_preds, (num_nodes, num_nodes)).to_dense()\n            # Eliminate prediction that do not reach the threshold to fix zero nodes\n            adj_pred = (adj_pred > self.threshold).float()\n            # adj_pred = torch.round(adj_pred)\n        else:\n            pred_edges = pot_edges[:, torch.round(batch_pred).squeeze().bool()]\n            # compute pred and ground truth adj matrices\n            if torch.sum(pred_edges) > 0:\n                adj_pred = to_dense_adj(pred_edges, max_num_nodes=num_nodes).squeeze().cpu()\n            else: # to avoid exception in to_dense_adj when there is no predicted edge\n                adj_pred = torch.zeros((num_nodes, num_nodes)).squeeze().to(self.device).cpu()\n        # compute f1 score on the adj matrices\n        loss = F.binary_cross_entropy(adj_pred.float(), adj_target.float())\n        val_fscore = self.val_f1_score.cpu()(adj_pred.flatten(), adj_target.flatten())\n        self.log(\"val_loss\", loss.item(), batch_size=1)\n        self.log(\"val_fscore\", val_fscore.item(), prog_bar=True, batch_size=1)\n        self.log(\"val_precision\", self.val_precision.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n        self.log(\"val_recall\", self.val_recall.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n        self.log(\"val_la_score\", self.val_linear_assignment.cpu()(pot_edges, batch_pred, target_edges, num_nodes).item(), batch_size=1)\n\n        # TODO compute monophonic voice f1 score\n        # y_pred, n_voc = voice_from_edges(pred_edges, num_nodes)\n        # val_monf1 = self.val_monvoicef1(torch.tensor(y_pred, device=self.device), graph[\"note\"].y, onset=torch.tensor(graph[\"note\"].onset_div[0], device=self.device), duration=torch.tensor(graph[\"note\"].duration_div[0], device=self.device))\n        # self.log(\"val_monvoicef1\", val_monf1.item(),prog_bar = True, on_epoch = True, batch_size=1)\n\n    def test_metric_logging_step(self, batch_pred, pot_edges, target_edges, num_nodes):\n        \"\"\"Test logging only done once, similar to validation.\n        See val_metric_logging_step for details.\"\"\"\n        batch_pred = batch_pred.cpu()\n        pot_edges = pot_edges.cpu()\n        target_edges = target_edges.cpu()\n\n        adj_target = to_dense_adj(target_edges, max_num_nodes=num_nodes).squeeze().long().cpu()\n\n        # Solve with Hungarian Algorithm\n        cost_matrix = torch.sparse_coo_tensor(\n            pot_edges, batch_pred.squeeze(), (num_nodes, num_nodes)).to_dense().cpu().numpy()\n        row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)\n        potential_edges = np.vstack((row_ind, col_ind))\n        trimmed_preds = torch.tensor(cost_matrix[row_ind, col_ind])\n        adj_pred = torch.sparse_coo_tensor(torch.tensor(potential_edges), trimmed_preds, (num_nodes, num_nodes)).to_dense()\n        # Eliminate prediction that do not reach the threshold to fix zero nodes\n        adj_pred = (adj_pred > self.threshold).float()\n        test_la = self.test_f1_allignment.cpu()(adj_pred.flatten(), adj_target.flatten())\n        self.log(\"test_fscore_la\", test_la.item(), prog_bar=True, batch_size=1)\n        self.log(\"test_precision_la\", self.test_precision_allignment.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n        self.log(\"test_recall_la\", self.test_recall_allignment.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n\n        pred_edges = pot_edges[:, batch_pred.squeeze() > self.threshold]\n        # compute pred and ground truth adj matrices\n        if torch.sum(pred_edges) > 0:\n            adj_pred = to_dense_adj(pred_edges, max_num_nodes=num_nodes).squeeze().cpu()\n        else: # to avoid exception in to_dense_adj when there is no predicted edge\n            adj_pred = torch.zeros((num_nodes, num_nodes)).squeeze().to(self.device).cpu()\n        # compute f1 score on the adj matrices\n        test_fscore = self.test_f1_score.cpu()(adj_pred.flatten(), adj_target.flatten())\n        self.log(\"test_fscore\", test_fscore.item(), prog_bar=True, batch_size=1)\n        self.log(\"test_precision\", self.test_precision.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n        self.log(\"test_recall\", self.test_recall.cpu()(adj_pred.flatten(), adj_target.flatten()).item(), batch_size=1)\n        self.log(\"test_la_score\", self.test_linear_assignment.cpu()(pot_edges, batch_pred, target_edges, num_nodes).item(), batch_size=1)\n\n    def predict_metric_step(self, batch_pred, pot_edges, target_edges, num_nodes):\n        \"\"\"Predict logging only done once, similar to validation.\n        See val_metric_logging_step for details.\"\"\"\n        batch_pred = batch_pred.cpu()\n        pot_edges = pot_edges.cpu()\n        target_edges = target_edges.cpu()\n\n        adj_target = to_dense_adj(target_edges, max_num_nodes=num_nodes).squeeze().long().cpu()\n\n        # Solve with Hungarian Algorithm\n        cost_matrix = torch.sparse_coo_tensor(\n            pot_edges, batch_pred.squeeze(), (num_nodes, num_nodes)).to_dense().cpu().numpy()\n        row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)\n        potential_edges = np.vstack((row_ind, col_ind))\n        trimmed_preds = torch.tensor(cost_matrix[row_ind, col_ind])\n        adj_pred = torch.sparse_coo_tensor(torch.tensor(potential_edges), trimmed_preds,\n                                           (num_nodes, num_nodes)).to_dense()\n        # Eliminate prediction that do not reach the threshold to fix zero nodes\n        adj_pred = (adj_pred > self.threshold).float()\n        fscore = self.test_f1_allignment.cpu()(adj_pred.flatten(), adj_target.flatten())\n        return adj_pred, fscore\n\n    def pitch_score(self, edge_index, mpitch):\n        \"\"\"Pitch score from midi to freq.\"\"\"\n        a = 440  # frequency of A (coomon value is 440Hz)\n        fpitch = (a / 32) * (2 ** ((mpitch - 9) / 12))\n        pscore = torch.pow(\n            torch.div(torch.min(fpitch[edge_index], dim=0)[0], torch.max(fpitch[edge_index], dim=0)[0]), self.alpha)\n        return pscore.unsqueeze(1)\n\n    def onset_score(self, edge_index, onset, duration, onset_beat, duration_beat, ts_beats):\n        offset = onset + duration\n        offset_beat = onset_beat + duration_beat\n        note_distance_beat = onset_beat[edge_index[1]] - offset_beat[edge_index[0]]\n        ts_beats_edges = ts_beats[edge_index[1]]\n        oscore = 1 - (1 / (1 + torch.exp(-2 * (note_distance_beat / ts_beats_edges))) - 0.5) * 2\n        one_hot_pitch_score = (onset[edge_index[1]] == offset[edge_index[0]]).float()\n        oscore = torch.cat((oscore.unsqueeze(1), one_hot_pitch_score.unsqueeze(1)), dim=1)\n        return oscore", ""]}
{"filename": "vocsep/models/core/gnn.py", "chunked_list": ["import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch_scatter import scatter\n\nclass SageConvLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=False):\n        super(SageConvLayer, self).__init__()\n        self.neigh_linear = nn.Linear(in_features, in_features, bias=bias)\n        self.linear = nn.Linear(in_features * 2, out_features, bias=bias)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_uniform_(self.linear.weight, gain=gain)\n        nn.init.xavier_uniform_(self.neigh_linear.weight, gain=gain)\n        if self.linear.bias is not None:\n            nn.init.constant_(self.linear.bias, 0.)\n        if self.neigh_linear.bias is not None:\n            nn.init.constant_(self.neigh_linear.bias, 0.)\n\n    def forward(self, features, adj, neigh_feats=None):\n        if neigh_feats is None:\n            neigh_feats = features\n        h = self.neigh_linear(neigh_feats)\n        if not isinstance(adj, torch.sparse.FloatTensor):\n            if len(adj.shape) == 3:\n                h = torch.bmm(adj, h) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1], -1)) + 1)\n            else:\n                h = torch.mm(adj, h) / (adj.sum(dim=1).reshape(adj.shape[0], -1) + 1)\n        else:\n            h = torch.mm(adj, h) / (adj.to_dense().sum(dim=1).reshape(adj.shape[0], -1) + 1)\n        z = self.linear(torch.cat([features, h], dim=-1))\n        return z", "\n\nclass SageConvScatter(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(SageConvScatter, self).__init__()\n        self.neigh_linear = nn.Linear(in_features, in_features, bias=bias)\n        self.linear = nn.Linear(in_features*2, out_features, bias=bias)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_uniform_(self.linear.weight, gain=gain)\n        nn.init.xavier_uniform_(self.neigh_linear.weight, gain=gain)\n        if self.linear.bias is not None:\n            nn.init.constant_(self.linear.bias, 0.)\n        if self.neigh_linear.bias is not None:\n            nn.init.constant_(self.neigh_linear.bias, 0.)\n\n    def forward(self, features, edge_index, neigh_feats=None):\n        if neigh_feats is None:\n            neigh_feats = features\n        h = self.neigh_linear(neigh_feats)\n        s = scatter(h[edge_index[1]], edge_index[0], 0, out=h.clone(), reduce='mean')\n        z = self.linear(torch.cat([features, s], dim=-1))\n        return z", "\n\nclass GATConvLayer(nn.Module):\n    def __init__(self, in_features, out_features, num_heads, bias=True, dropout=0.3, negative_slope=0.2):\n        super(GATConvLayer, self).__init__()\n        self.num_heads = num_heads\n        self.in_features = in_features\n        self.out_features = out_features\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.el = nn.Linear(in_features, in_features * num_heads, bias=bias)\n        self.er = nn.Linear(in_features, in_features * num_heads, bias=bias)\n        self.attnl = nn.Parameter(torch.FloatTensor(1, num_heads, in_features))\n        self.attnr = nn.Parameter(torch.FloatTensor(1, num_heads, in_features))\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        self.softmax = nn.Softmax(dim=1)\n        self.attndrop = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.linear.weight, gain=gain)\n        nn.init.xavier_normal_(self.el.weight, gain=gain)\n        nn.init.xavier_normal_(self.er.weight, gain=gain)\n        nn.init.xavier_normal_(self.attnl, gain=gain)\n        nn.init.xavier_normal_(self.attnr, gain=gain)\n        if self.linear.bias is not None:\n            nn.init.constant_(self.linear.bias, 0.)\n        if self.el.bias is not None:\n            nn.init.constant_(self.el.bias, 0.)\n        if self.er.bias is not None:\n            nn.init.constant_(self.er.bias, 0.)\n\n    def forward(self, features, edge_index):\n        prefix_shape = features.shape[:-1]\n        fc_src = self.el(features).view(*prefix_shape, self.num_heads, self.in_features)\n        fc_dst = self.er(features).view(*prefix_shape, self.num_heads, self.in_features)\n        el = (fc_src[edge_index[0]] * self.attnl).sum(dim=-1).unsqueeze(-1)\n        er = (fc_dst[edge_index[1]] * self.attnr).sum(dim=-1).unsqueeze(-1)\n        e = self.leaky_relu(el + er)\n        # Not Quite the same as the Softmax in the paper.\n        a = self.softmax(self.attndrop(e)).mean(dim=1)\n        h = self.linear(features)\n        out = scatter(a * h[edge_index[1]], edge_index[0], 0, out=h.clone(), reduce='add')\n        return out", "\n\nclass ResGatedGraphConv(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(ResGatedGraphConv, self).__init__()\n        self.W1 = nn.Linear(in_features, out_features, bias=bias)\n        self.W2 = nn.Linear(in_features, out_features, bias=bias)\n        self.W3 = nn.Linear(in_features, out_features, bias=bias)\n        self.W4 = nn.Linear(in_features, out_features, bias=bias)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_uniform_(self.W1.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W2.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W3.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W4.weight, gain=gain)\n        if self.W1.bias is not None:\n            nn.init.constant_(self.W1.bias, 0.)\n        if self.W2.bias is not None:\n            nn.init.constant_(self.W2.bias, 0.)\n        if self.W3.bias is not None:\n            nn.init.constant_(self.W3.bias, 0.)\n        if self.W4.bias is not None:\n            nn.init.constant_(self.W4.bias, 0.)\n    def forward(self, features, edge_index, neigh_feats=None):\n        if neigh_feats is None:\n            neigh_feats = features\n        h1 = self.W1(features)\n        h2 = self.W2(neigh_feats)\n        h3 = self.W3(features)[edge_index[0]]\n        h4 = self.W4(features)[edge_index[1]]\n        z1 = torch.sigmoid(h3 + h4)\n        h = z1 * h2[edge_index[1]]\n        s = scatter(h, edge_index[0], 0, out=h1.clone(), reduce='sum')\n        z = h1 + s\n        return z", "\n\nclass GCN(nn.Module):\n    def __init__(self, in_feats, n_hidden, out_feats, n_layers, activation=F.relu, dropout=0.5, jk=False):\n        super(GCN, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = F.normalize\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.layers.append(SageConvScatter(in_feats, n_hidden))\n        for i in range(n_layers - 1):\n            self.layers.append(SageConvScatter(n_hidden, n_hidden))\n        if jk:\n            self.use_knowledge = True\n            self.jk = JumpingKnowledge(n_hidden=n_hidden, n_layers=n_layers)\n        else:\n            self.use_knowledge = False\n        self.layers.append(SageConvScatter(n_hidden, out_feats))\n\n    def forward(self, x, edge_index):\n        h = x\n        hs = []\n        for conv in self.layers[:-1]:\n            h = conv(h, edge_index)\n            h = self.activation(h)\n            h = self.normalize(h)\n            h = self.dropout(h)\n            hs.append(h)\n        if self.use_knowledge:\n            h = self.jk(hs)\n        h = self.layers[-1](h, edge_index)\n        return h", "\n\nclass OnsetEmbedding(nn.Module):\n    def __init__(self, in_feats, out_feats, bias=True, add_self_loops=True):\n        super(OnsetEmbedding, self).__init__()\n        self.W = nn.Linear(in_feats, out_feats, bias=bias)\n        self.add_self_loops = add_self_loops\n\n    def forward(self, x, edge_index):\n        # add self-loops\n        if self.add_self_loops:\n            self_loops = torch.arange(0, x.size(0), dtype=torch.long, device=x.device)\n            self_loops = self_loops.unsqueeze(0).repeat(2, 1)\n            edge_index = torch.cat([edge_index, self_loops], dim=1)\n        src = x[edge_index[0]]\n        dst = x[edge_index[1]]\n        out = torch.abs(src - dst)\n        out = scatter(out, edge_index[0], 0, out=x.clone(), reduce='mean')\n        out = self.W(out)\n        return out", "\n\ndef compare_all_elements(u, v, max_val, data_split=1):\n    \"\"\"\n    Description.....\n\n    Parameters\n    ----------\n        u:         first array to be compared (1D torch.tensor of ints)\n        v:         second array to be compared (1D torch.tensor of ints)\n        max_val:         the largest element in either tensorA or tensorB (real number)\n        data_split:      the number of subsets to split the mask up into (int)\n    Returns\n    -------\n        compared_inds_a:  indices of tensorA that match elements in tensorB (1D torch.tensor of ints, type torch.long)\n        compared_inds_b:  indices of tensorB that match elements in tensorA (1D torch.tensor of ints, type torch.long)\n    \"\"\"\n    compared_inds_a, compared_inds_b, inc = torch.tensor([]).to(u.device), torch.tensor([]).to(u.device), int(\n        max_val // data_split) + 1\n    for iii in range(data_split):\n        inds_a, inds_b = (iii * inc <= u) * (u < (iii + 1) * inc), (iii * inc <= v) * (\n                v < (iii + 1) * inc)\n        tile_a, tile_b = u[inds_a], v[inds_b]\n        tile_a, tile_b = tile_a.unsqueeze(0).repeat(tile_b.size(0), 1), torch.transpose(tile_b.unsqueeze(0), 0, 1).repeat(1,\n                                                                                                                     tile_a.size(\n                                                                                                                         0))\n        nz_inds = torch.nonzero(tile_a == tile_b, as_tuple=False)\n        nz_inds_a, nz_inds_b = nz_inds[:, 1], nz_inds[:, 0]\n        compared_inds_a, compared_inds_b = torch.cat((compared_inds_a, inds_a.nonzero()[nz_inds_a]), 0), torch.cat(\n            (compared_inds_b, inds_b.nonzero()[nz_inds_b]), 0)\n    return compared_inds_a.squeeze().long(), compared_inds_b.squeeze().long()", "\n\nclass JumpingKnowledge(nn.Module):\n    \"\"\"\n    Combines information per GNN layer with a LSTM,\n    provided that all hidden representation are on the same dimension.\n    \"\"\"\n    def __init__(self, n_hidden, n_layers):\n        super(JumpingKnowledge, self).__init__()\n        self.lstm = nn.LSTM(n_hidden, (n_layers*n_hidden)//2, bidirectional=True, batch_first=True)\n        self.att = nn.Linear(2 * ((n_layers*n_hidden)//2), 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.lstm.reset_parameters()\n        nn.init.xavier_uniform_(self.att.weight, gain=nn.init.calculate_gain(\"relu\"))\n\n    def forward(self, xs):\n        x = torch.stack(xs, dim=1)  # [num_nodes, num_layers, num_channels]\n        alpha, _ = self.lstm(x)\n        alpha = self.att(alpha).squeeze(-1)  # [num_nodes, num_layers]\n        alpha = torch.softmax(alpha, dim=-1)\n        return (x * alpha.unsqueeze(-1)).sum(dim=1)", "\n\nclass JumpingKnowledge3D(nn.Module):\n    \"\"\"\n    Combines information per GNN layer with a LSTM,\n    provided that all hidden representation are on the same dimension.\n    \"\"\"\n    def __init__(self, n_hidden, n_layers):\n        super(JumpingKnowledge3D, self).__init__()\n        self.lstm = nn.LSTM(n_hidden, (n_layers*n_hidden)//2, bidirectional=True, batch_first=True)\n        self.att = nn.Linear(2 * ((n_layers*n_hidden)//2), 1)\n        self.n_hidden = n_hidden\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.lstm.reset_parameters()\n        nn.init.xavier_uniform_(self.att.weight, gain=nn.init.calculate_gain(\"relu\"))\n\n    def forward(self, xs):\n        x = torch.stack(xs, dim=2)  # [batch_size, num_nodes, num_layers, num_channels]\n        h = torch.reshape(x, (x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))\n        alpha, _ = self.lstm(h)\n        alpha = self.att(alpha).squeeze(-1)  # [num_nodes, num_layers]\n        alpha = torch.softmax(alpha, dim=-1)\n        return (h * alpha.unsqueeze(-1)).sum(dim=1).view(x.shape[0], x.shape[1], self.n_hidden)", "\n\nclass GGRU(nn.Module):\n    \"\"\"\n    A GRU inspired implementation of a GCN cell.\n\n    h(t-1) in this case is the neighbors of node t.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=False):\n        super(GGRU, self).__init__()\n        self.wr = SageConvLayer(in_features, in_features)\n        self.wz = SageConvLayer(in_features, out_features)\n        self.w_ni = nn.Linear(in_features*2, out_features)\n        self.w_nh = nn.Linear(in_features, in_features)\n        self.proj = nn.Linear(in_features, out_features)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain(\"relu\")\n        nn.init.xavier_uniform_(self.w_ni.weight, gain=gain)\n        nn.init.xavier_uniform_(self.w_nh.weight, gain=gain)\n        nn.init.xavier_uniform_(self.proj.weight, gain=gain)\n\n    def forward(self, x, adj):\n        h = x\n        r = F.sigmoid(self.wr(h, adj))\n        z = F.sigmoid(self.wz(h, adj))\n        h = torch.bmm(adj, self.w_nh(h)) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1], -1)) + 1)\n        n = self.w_ni(torch.cat([x, r*h], dim=-1))\n        n = F.tanh(n)\n        neigh = torch.bmm(adj, x) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1], -1)) + 1)\n        out = (1 - z)*n + z*self.proj(neigh)\n        return out", "\n\nclass GPSLayer(nn.Module):\n    def __init__(self, in_features, out_features, num_heads, activation, dropout=0.2, bias=True):\n        \"\"\"\n        General Powerful Scalable Graph Transformers Convolutional Layer\n\n        Parameters\n        ----------\n        in_features: int\n            Number of input features\n        out_features: int\n            Number of output features\n        num_heads: int\n            Number of attention heads\n        activation: nn.Module\n            Activation function\n        dropout: float\n            Dropout rate\n        bias: bool\n            Whether to use bias\n        \"\"\"\n        super(GPSLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_heads = num_heads\n        self.activation = activation\n        self.normalize_local = nn.LayerNorm(out_features)\n        self.normalize_attn = nn.LayerNorm(out_features)\n        self.dropout_ff = nn.Dropout(dropout)\n        self.dropout_attn = nn.Dropout(dropout)\n        self.dropout_local = nn.Dropout(dropout)\n        self.ff1 = nn.Linear(out_features, out_features*2, bias=bias)\n        self.ff2 = nn.Linear(out_features*2, out_features, bias=bias)\n        self.attn = nn.MultiheadAttention(out_features, num_heads, dropout=dropout, bias=bias, batch_first=True)\n        self.local = ResGatedGraphConv(in_features, out_features, bias=bias)\n\n    def forward(self, x, edge_index):\n        h_init = x\n        # Local embeddings\n        local_out = self.local(x, edge_index)\n        local_out = self.activation(local_out)\n        local_out = self.normalize_local(local_out)\n        local_out = self.dropout_local(local_out)\n        local_out = local_out + h_init\n\n        # Global embeddings\n        attn_out, _ = self.attn(x, x, x)\n        attn_out = self.activation(attn_out)\n        attn_out = self.normalize_attn(attn_out)\n        attn_out = self.dropout_attn(attn_out)\n        attn_out = attn_out + h_init\n\n        # Combine\n        out = local_out + attn_out\n        h = self.ff1(out)\n        h = self.activation(h)\n        h = self.dropout_ff(h)\n        h = self.ff2(h)\n        h - self.dropout_ff(h)\n        out = F.normalize(out + h)\n        return out", "\n"]}
{"filename": "vocsep/models/core/mlp.py", "chunked_list": ["import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_feats, n_hidden, out_feats=None, n_layers=1, activation=F.relu, dropout=0.5, bias=False):\n        super(MLP, self).__init__()\n        if out_feats is None:\n            out_feats = n_hidden\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = nn.BatchNorm1d(n_hidden)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.layers.append(nn.Linear(in_feats, n_hidden, bias=bias))\n        for i in range(n_layers - 1):\n            self.layers.append(nn.Linear(n_hidden, n_hidden, bias=bias))\n        self.layers.append(nn.Linear(n_hidden, out_feats, bias=bias))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for i in range(len(self.layers)):\n            nn.init.xavier_uniform_(self.layers[i].weight, gain=nn.init.calculate_gain('relu'))\n            if self.layers[i].bias is not None:\n                nn.init.constant_(self.layers[i].bias, 0.)\n\n    def forward(self, x):\n        h = x\n        for layer in self.layers[:-1]:\n            h = layer(h)\n            h = self.activation(h)\n            h = self.normalize(h)\n            h = self.dropout(h)\n        h = self.layers[-1](h)\n        return h", "\n\n"]}
{"filename": "vocsep/models/core/unet.py", "chunked_list": ["# Taken from https://github.com/milesial/Pytorch-UNet\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits", "\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)", "\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)", "\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)", "\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)"]}
{"filename": "vocsep/models/core/graph_sampling.py", "chunked_list": ["import numpy as np\n\n\nclass GraphSampler:\n    \"\"\"\n    This class is just to showcase how you can write the graph sampler in pure python.\n    The simplest and most basic sampler: just pick nodes uniformly at random and return the\n    node-induced subgraph.\n    \"\"\"\n    def __init__(self, adj_train, node_train, size_subgraph, args_preproc):\n        self.adj_train = adj_train\n        self.node_train = np.unique(node_train).astype(np.int32)\n        # size in terms of number of vertices in subgraph\n        self.size_subgraph = size_subgraph\n        self.name_sampler = 'None'\n        self.node_subgraph = None\n        self.preproc(**args_preproc)\n\n    def par_sample(self, stage, **kwargs):\n        node_ids = np.random.choice(self.node_train, self.size_subgraph)\n        ret = self._helper_extract_subgraph(node_ids)\n        ret = list(ret)\n        for i in range(len(ret)):\n            ret[i] = [ret[i]]\n        return ret\n\n    def preproc(self):\n        pass\n\n    def par_sample(self, stage, **kwargs):\n        pass\n\n    def _helper_extract_subgraph(self, node_ids):\n        \"\"\"\n        ONLY used for serial Python sampler (NOT for the parallel cython sampler).\n        Return adj of node-induced subgraph and other corresponding data struct.\n        Inputs:\n            node_ids        1D np array, each element is the ID in the original\n                            training graph.\n        Outputs:\n            indptr          np array, indptr of the subg adj CSR\n            indices         np array, indices of the subg adj CSR\n            data            np array, data of the subg adj CSR. Since we have aggregator\n                            normalization, we can simply set all data values to be 1\n            subg_nodes      np array, i-th element stores the node ID of the original graph\n                            for the i-th node in the subgraph. Used to index the full feats\n                            and label matrices.\n            subg_edge_index np array, i-th element stores the edge ID of the original graph\n                            for the i-th edge in the subgraph. Used to index the full array\n                            of aggregation normalization.\n        \"\"\"\n        node_ids = np.unique(node_ids)\n        node_ids.sort()\n        orig2subg = {n: i for i, n in enumerate(node_ids)}\n        n = node_ids.size\n        indptr = np.zeros(node_ids.size + 1)\n        indices = []\n        subg_edge_index = []\n        subg_nodes = node_ids\n        for nid in node_ids:\n            idx_s, idx_e = self.adj_train.indptr[nid], self.adj_train.indptr[nid + 1]\n            neighs = self.adj_train.indices[idx_s : idx_e]\n            for i_n, n in enumerate(neighs):\n                if n in orig2subg:\n                    indices.append(orig2subg[n])\n                    indptr[orig2subg[nid] + 1] += 1\n                    subg_edge_index.append(idx_s + i_n)\n        indptr = indptr.cumsum().astype(np.int64)\n        indices = np.array(indices)\n        subg_edge_index = np.array(subg_edge_index)\n        data = np.ones(indices.size)\n        assert indptr[-1] == indices.size == subg_edge_index.size\n        return indptr, indices, data, subg_nodes, subg_edge_index", "\n"]}
{"filename": "vocsep/models/core/hgnn.py", "chunked_list": ["import torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\nfrom .gnn import SageConvScatter as SageConv, ResGatedGraphConv, JumpingKnowledge\n\n\nclass HeteroAttention(nn.Module):\n    def __init__(self, n_hidden, n_layers):\n        super(HeteroAttention, self).__init__()\n        self.lstm = nn.LSTM(n_hidden, (n_layers*n_hidden)//2, bidirectional=True, batch_first=True)\n        self.att = nn.Linear(2 * ((n_layers*n_hidden)//2), 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.lstm.reset_parameters()\n        nn.init.xavier_uniform_(self.att.weight, gain=nn.init.calculate_gain(\"relu\"))\n\n    def forward(self, x):\n        alpha, _ = self.lstm(x)\n        alpha = self.att(alpha).squeeze(-1)  # [num_nodes, num_layers]\n        alpha = torch.softmax(alpha, dim=-1)\n        return (x * alpha.unsqueeze(-1)).sum(dim=0)", "\n\nclass HeteroResGatedGraphConvLayer(nn.Module):\n    def __init__(self, in_features, out_features, etypes, bias=True, reduction='mean'):\n        super(HeteroResGatedGraphConvLayer, self).__init__()\n        self.out_features = out_features\n        self.etypes = etypes\n        if reduction == 'mean':\n            self.reduction = lambda x: x.mean(dim=0)\n        elif reduction == 'sum':\n            self.reduction = lambda x: x.sum(dim=0)\n        elif reduction == 'max':\n            self.reduction = lambda x: x.max(dim=0)\n        elif reduction == 'min':\n            self.reduction = lambda x: x.min(dim=0)\n        elif reduction == 'concat':\n            self.reduction = lambda x: torch.cat(x, dim=0)\n        elif reduction == 'lstm':\n            self.reduction = HeteroAttention(out_features, len(etypes.keys()))\n        elif reduction == 'none':\n            self.reduction = lambda x: x\n        else:\n            raise NotImplementedError\n\n        conv_dict = dict()\n        for etype in etypes.keys():\n            conv_dict[etype] = ResGatedGraphConv(in_features, out_features, bias=bias)\n        self.conv = nn.ModuleDict(conv_dict)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for conv in self.conv.values():\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type):\n        out = torch.zeros((len(self.conv.keys()), x.shape[0], self.out_features))\n        for idx, (ekey, evalue) in enumerate(self.etypes.items()):\n            mask = edge_type == evalue\n            out[idx] = self.conv[ekey](x, edge_index[:, mask])\n        return self.reduction(out).to(x.device)", "\n\nclass HeteroSageConvLayer(nn.Module):\n    def __init__(self, in_features, out_features, etypes, bias=True, reduction='mean'):\n        super(HeteroSageConvLayer, self).__init__()\n        self.out_features = out_features\n        self.etypes = etypes\n        if reduction == 'mean':\n            self.reduction = lambda x: x.mean(dim=0)\n        elif reduction == 'sum':\n            self.reduction = lambda x: x.sum(dim=0)\n        elif reduction == 'max':\n            self.reduction = lambda x: x.max(dim=0)\n        elif reduction == 'min':\n            self.reduction = lambda x: x.min(dim=0)\n        elif reduction == 'concat':\n            self.reduction = lambda x: torch.cat(x, dim=0)\n        elif reduction == 'lstm':\n            self.reduction = HeteroAttention(out_features, len(etypes.keys()))\n        else:\n            raise NotImplementedError\n\n        conv_dict = dict()\n        for etype in etypes.keys():\n            conv_dict[etype] = SageConv(in_features, out_features, bias=bias)\n        self.conv = nn.ModuleDict(conv_dict)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for conv in self.conv.values():\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type):\n        out = torch.zeros((len(self.conv.keys()), x.shape[0], self.out_features))\n        for idx, (ekey, evalue) in enumerate(self.etypes.items()):\n            mask = edge_type == evalue\n            out[idx] = self.conv[ekey](x, edge_index[:, mask])\n        return self.reduction(out).to(x.device)", "\n\n\nclass HGCN(nn.Module):\n    def __init__(self, in_feats, n_hidden, out_feats, n_layers, etypes={\"onset\":0, \"consecutive\":1, \"during\":2, \"rests\":3, \"consecutive_rev\":4, \"during_rev\":5, \"rests_rev\":6}, activation=F.relu, dropout=0.5, jk=False):\n        super(HGCN, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = F.normalize\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.layers.append(HeteroSageConvLayer(in_feats, n_hidden, etypes=etypes))\n        for i in range(n_layers - 1):\n            self.layers.append(HeteroSageConvLayer(n_hidden, n_hidden, etypes=etypes))\n        if jk:\n            self.use_knowledge = True\n            self.jk = JumpingKnowledge(n_hidden=n_hidden, n_layers=n_layers)\n        else:\n            self.use_knowledge = False\n        self.layers.append(HeteroSageConvLayer(n_hidden, out_feats, etypes=etypes))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for conv in self.layers:\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type):\n        h = x\n        hs = []\n        for conv in self.layers[:-1]:\n            h = conv(h, edge_index, edge_type)\n            h = self.activation(h)\n            h = self.normalize(h)\n            h = self.dropout(h)\n            hs.append(h)\n        if self.use_knowledge:\n            h = self.jk(hs)\n        h = self.layers[-1](h, edge_index, edge_type)\n        return h", "\n\nclass HResGatedConv(nn.Module):\n    def __init__(self, in_feats, n_hidden, out_feats, n_layers, etypes={\"onset\":0, \"consecutive\":1, \"during\":2, \"rests\":3, \"consecutive_rev\":4, \"during_rev\":5, \"rests_rev\":6}, activation=F.relu, dropout=0.5, jk=False):\n        super(HResGatedConv, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = F.normalize\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.layers.append(HeteroResGatedGraphConvLayer(in_feats, n_hidden, etypes=etypes))\n        for i in range(n_layers - 1):\n            self.layers.append(HeteroResGatedGraphConvLayer(n_hidden, n_hidden, etypes=etypes))\n        if jk:\n            self.use_knowledge = True\n            self.jk = JumpingKnowledge(n_hidden=n_hidden, n_layers=n_layers)\n        else:\n            self.use_knowledge = False\n        self.layers.append(HeteroResGatedGraphConvLayer(n_hidden, out_feats, etypes=etypes))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for conv in self.layers:\n            conv.reset_parameters()\n\n    def forward(self, x, edge_index, edge_type):\n        h = x\n        hs = []\n        for conv in self.layers:\n            h = conv(h, edge_index, edge_type)\n            h = self.activation(h)\n            h = self.normalize(h)\n            h = self.dropout(h)\n            hs.append(h)\n        if self.use_knowledge:\n            h = self.jk(hs)\n        # h = self.layers[-1](h, edge_index, edge_type)\n        return h", "\n\nclass HGPSLayer(nn.Module):\n    def __init__(\n            self, in_features, out_features, num_heads,\n            etypes={\"onset\":0, \"consecutive\":1, \"during\":2, \"rests\":3, \"consecutive_rev\":4, \"during_rev\":5, \"rests_rev\":6},\n            activation=F.relu, dropout=0.2, bias=True):\n        \"\"\"\n        General Powerful Scalable Graph Transformers Convolutional Layer\n\n        Parameters\n        ----------\n        in_features: int\n            Number of input features\n        out_features: int\n            Number of output features\n        num_heads: int\n            Number of attention heads\n        etypes: dict\n            Edge types\n        activation: nn.Module\n            Activation function\n        dropout: float\n            Dropout rate\n        bias: bool\n            Whether to use bias\n        \"\"\"\n        super(HGPSLayer, self).__init__()\n        self.embedding = nn.Linear(in_features, out_features, bias=bias)\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_heads = num_heads\n        self.activation = activation\n        self.normalize_local = nn.LayerNorm(out_features)\n        self.normalize_attn = nn.LayerNorm(out_features)\n        self.dropout_ff = nn.Dropout(dropout)\n        self.dropout_attn = nn.Dropout(dropout)\n        self.dropout_local = nn.Dropout(dropout)\n        self.ff1 = nn.Linear(out_features, out_features*2, bias=bias)\n        self.ff2 = nn.Linear(out_features*2, out_features, bias=bias)\n        self.attn = nn.MultiheadAttention(out_features, num_heads, dropout=dropout, bias=bias, batch_first=True)\n        self.local = HeteroResGatedGraphConvLayer(out_features, out_features, bias=bias, etypes=etypes)\n\n    def forward(self, x, edge_index, edge_type):\n\n        h_init = self.embedding(x)\n        # Local embeddings\n        local_out = self.local(h_init, edge_index, edge_type)\n        local_out = self.activation(local_out)\n        local_out = self.normalize_local(local_out)\n        local_out = self.dropout_local(local_out)\n        local_out = local_out + h_init\n\n        # Global embeddings\n        h = h_init.unsqueeze(0)\n        attn_out, _ = self.attn(h, h, h)\n        attn_out = self.activation(attn_out)\n        attn_out = self.normalize_attn(attn_out)\n        attn_out = self.dropout_attn(attn_out)\n        attn_out = attn_out + h_init\n\n        # Combine\n        out = local_out + attn_out.squeeze()\n        h = self.ff1(out)\n        h = self.activation(h)\n        h = self.dropout_ff(h)\n        h = self.ff2(h)\n        h - self.dropout_ff(h)\n        out = F.normalize(out + h)\n        return out", "\n\nclass HGPS(nn.Module):\n    def __init__(self, in_feats, n_hidden, out_feats, n_layers, etypes={\"onset\":0, \"consecutive\":1, \"during\":2, \"rests\":3, \"consecutive_rev\":4, \"during_rev\":5, \"rests_rev\":6}, activation=F.relu, dropout=0.5, jk=False):\n        super(HGPS, self).__init__()\n        self.n_hidden = n_hidden\n        self.layers = nn.ModuleList()\n        self.normalize = F.normalize\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout)\n        self.layers.append(HGPSLayer(in_feats, n_hidden, etypes=etypes, num_heads=4))\n        for i in range(n_layers - 1):\n            self.layers.append(HGPSLayer(n_hidden, n_hidden, etypes=etypes, num_heads=4))\n        if jk:\n            self.use_knowledge = True\n            self.jk = JumpingKnowledge(n_hidden=n_hidden, n_layers=n_layers)\n        else:\n            self.use_knowledge = False\n        self.layers.append(HGPSLayer(n_hidden, out_feats, etypes=etypes, num_heads=4))\n\n    def forward(self, x, edge_index, edge_type):\n        h = x\n        hs = []\n        for conv in self.layers:\n            h = conv(h, edge_index, edge_type)\n            h = self.activation(h)\n            h = self.normalize(h)\n            h = self.dropout(h)\n            hs.append(h)\n        if self.use_knowledge:\n            h = self.jk(hs)\n        # h = self.layers[-1](h, edge_index, edge_type)\n        return h", ""]}
{"filename": "vocsep/models/core/__init__.py", "chunked_list": ["from .gnn import *\nfrom .mlp import *\nfrom .unet import *\nfrom .hgnn import HeteroSageConvLayer, HeteroResGatedGraphConvLayer, HGCN, HResGatedConv, HGPS, HeteroAttention\nfrom .graph_utils import positional_encoding\n"]}
{"filename": "vocsep/models/core/graph_utils.py", "chunked_list": ["from scipy import sparse as sp\nfrom scipy.sparse.linalg import eigs\nimport torch\nimport numpy as np\n\n\ndef degree(edge_index, num_nodes):\n    \"\"\"Computes the (unweighted) degree of a given one-dimensional index\n    tensor.\n\n    Parameters\n    ----------\n    index (LongTensor)\n        Index tensor.\n\n    num_nodes : (int)\n        The number of nodes of the graph.\n    \"\"\"\n    out = torch.zeros((num_nodes,), dtype=torch.long)\n    one = torch.ones((edge_index[1].size(0), ), dtype=out.dtype, device=out.device)\n    return out.scatter_add_(0, edge_index[1], one)", "\n\ndef positional_encoding(edge_index, num_nodes, pos_enc_dim: int) -> torch.Tensor:\n    \"\"\"\n    Graph positional encoding v/ Laplacian eigenvectors\n\n    Parameters\n    ----------\n    edge_index (LongTensor)\n        edge indices tensor.\n    num_nodes : (int)\n        The number of nodes of the graph.\n    pos_enc_dim : int\n        The Positional Encoding Dimension to be added to Nodes of the graph\n\n    Returns\n    -------\n    pos_enc : torch.Tensor\n        A tensor of shape (N, pos_enc_dim) where N is the number of nodes in the graph\n    \"\"\"\n    # Laplacian\n    A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (num_nodes, num_nodes)).to_dense()\n    in_degree = A.sum(axis=1).numpy()\n    N = sp.diags(in_degree.clip(1) ** -0.5, dtype=float)\n    L = sp.eye(num_nodes) - N * A * N\n\n    # Eigenvectors with scipy\n    EigVal, EigVec = eigs(L, k=pos_enc_dim+1, which='SR', tol=1e-2)\n    # increasing order\n    EigVec = EigVec[:, EigVal.argsort()]\n    pos_enc = torch.from_numpy(np.real(EigVec[:, 1:pos_enc_dim+1])).float()\n    return pos_enc", ""]}
