{"filename": "setup.py", "chunked_list": ["import setuptools\n\nsetuptools.setup(\n    name=\"vqtorch\",\n    packages=setuptools.find_packages(),\n    version=\"0.1.0\",\n    author=\"Minyoung Huh\",\n    author_email=\"minhuh@mit.edu\",\n    description=f\"vector-quantization for pytorch\",\n    url=\"git@github.com:minyoungg/vqtorch.git\",", "    description=f\"vector-quantization for pytorch\",\n    url=\"git@github.com:minyoungg/vqtorch.git\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    install_requires=[\n            \"torch>=1.13.0\",\n            \"string-color==1.2.3\",", "            \"torch>=1.13.0\",\n            \"string-color==1.2.3\",\n            \"torchpq==0.3.0.1\",\n    ],\n    python_requires='>=3.6', # developed on 3.9 / 3.10\n)\n"]}
{"filename": "vqtorch/dists.py", "chunked_list": ["import math\nimport time\nimport warnings\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef check_shape(tensor, codebook):\n\tif len(tensor.shape) != 3:\n\t\traise RuntimeError(f'expected 3d tensor but found {tensor.size()}')\n\n\tif tensor.size(2) != codebook.size(1):\n\t\traise RuntimeError(\n\t\t\t\tf'expected tensor and codebook to have the same feature ' + \\\n\t\t\t\tf'dimensions but found: {tensor.size()} vs {codebook.size()}'\n\t\t\t\t)\n\treturn", "\n\ndef check_shape(tensor, codebook):\n\tif len(tensor.shape) != 3:\n\t\traise RuntimeError(f'expected 3d tensor but found {tensor.size()}')\n\n\tif tensor.size(2) != codebook.size(1):\n\t\traise RuntimeError(\n\t\t\t\tf'expected tensor and codebook to have the same feature ' + \\\n\t\t\t\tf'dimensions but found: {tensor.size()} vs {codebook.size()}'\n\t\t\t\t)\n\treturn", "\n\ndef get_dist_fns(dist):\n\tif dist in ['euc', 'euclidean']:\n\t\tloss_fn = euclidean_distance\n\t\tdist_fn = euclidean_cdist_topk\n\telif dist in ['cos', 'cosine']:\n\t\tloss_fn = cosine_distance\n\t\tdist_fn = cosine_cdist_topk\n\telse:\n\t\traise ValueError(f'unknown distance method: {dist}')\n\treturn loss_fn, dist_fn", "\n\ndef cosine_distance(z, z_q):\n\t\"\"\"\n\tComputes element wise euclidean of z and z_q\n\n\tNOTE: the euclidean distance is not a true euclidean distance.\n\t\"\"\"\n\n\tz = F.normalize(z, p=2, dim=-1)\n\tz_q = F.normalize(z_q, p=2, dim=-1)\n\treturn euclidean_distance(z, z_q)", "\n\ndef euclidean_distance(z, z_q):\n\t\"\"\"\n\tComputes element wise euclidean of z and z_q\n\n\tNOTE: uses spatial averaging and no square root is applied. hence this is\n\tnot a true euclidean distance but makes no difference in practice.\n\t\"\"\"\n\tif z.size() != z_q.size():\n\t\traise RuntimeError(\n\t\t\t\t\tf'expected z and z_q to have the same shape but got ' + \\\n\t\t\t\t\tf'{z.size()} vs {z_q.size()}'\n\t\t\t\t\t)\n\n\tz, z_q = z.reshape(z.size(0), -1), z_q.reshape(z_q.size(0), -1)\n\treturn ((z_q - z) ** 2).mean(1) #.sqrt()", "\n\ndef euclidean_cdist_topk(tensor, codebook, compute_chunk_size=1024, topk=1,\n\t\t\t\t\t\t half_precision=False):\n\t\"\"\"\n\tCompute the euclidean distance between tensor and every element in the\n\tcodebook.\n\n\tArgs:\n\t\ttensor (Tensor): a 3D tensor of shape [batch x HWG x feats].\n\t\tcodebook (Tensor): a 2D tensor of shape [num_codes x feats].\n\t\tcompute_chunk_size (int): the chunk size to use when computing cdist.\n\t\ttopk (int): stores `topk` distance minimizers. If -1, topk is the\n\t\t\tsame length as the codebook\n\t\thalf_precision (bool): if True, matrix multiplication is computed\n\t\t\tusing half-precision to save memory.\n\tReturns:\n\t\td (Tensor): distance matrix of shape [batch x HWG x topk].\n\t\t\teach element is the distance of tensor[i] to every codebook.\n\t\tq (Tensor): code matrix of the same dimension as `d`. The index of the\n\t\t\tcorresponding topk distances.\n\n\tNOTE: Compute chunk only looks at tensor since optimal codebook size\n\tgenerally does not vary too much. In future versions, should consider\n\tcomputing chunk size while taking into consideration of codebook and\n\tfeature dimension size.\n\t\"\"\"\n\tcheck_shape(tensor, codebook)\n\n\tb, n, c = tensor.shape\n\ttensor_dtype = tensor.dtype\n\ttensor = tensor.reshape(-1, tensor.size(-1))\n\ttensor = tensor.split(compute_chunk_size)\n\tdq = []\n\n\tif topk == -1:\n\t\ttopk = codebook.size(0)\n\n\tfor i, tc in enumerate(tensor):\n\t\tcb = codebook\n\n\t\tif half_precision:\n\t\t\ttc = tc.half()\n\t\t\tcb = cb.half()\n\n\t\td = torch.cdist(tc, cb)\n\t\tdq.append(torch.topk(d, k=topk, largest=False, dim=-1))\n\n\td, q = torch.cat([_dq[0] for _dq in dq]), torch.cat([_dq[1] for _dq in dq])\n\n\treturn_dict = {'d': d.to(tensor_dtype).reshape(b, n, -1),\n\t\t\t\t   'q': q.long().reshape(b, n, -1)}\n\treturn return_dict", "\n\ndef cosine_cdist_topk(tensor, codebook, chunks=4, topk=1, half_precision=False):\n\t\"\"\" Computes cosine distance instead. see `euclidean_cdist_topk` \"\"\"\n\tcheck_shape(tensor, codebook, mask)\n\n\ttensor   = F.normalize(tensor,   p=2, dim=-1)\n\tcodebook = F.normalize(codebook, p=2, dim=-1)\n\n\td, q = euclidean_cdist_topk(tensor, codebook, chunks, topk, half_precision)\n\n\td = 0.5 * (d ** 2)\n\treturn d, q.long()", ""]}
{"filename": "vqtorch/__init__.py", "chunked_list": ["from vqtorch.nn import *\nfrom vqtorch.utils import *\nfrom vqtorch.math_fns import *\n"]}
{"filename": "vqtorch/utils.py", "chunked_list": ["import numpy as np\nimport torch\n\nfrom vqtorch.nn import _VQBaseLayer\n\n\ndef is_vq(m):\n\t\"\"\" checks if the module is a VQ layer \"\"\"\n\treturn issubclass(type(m), _VQBaseLayer)\n\ndef is_vqn(net):\n\t\"\"\" checks if the network contains a VQ layer \"\"\"\n\treturn np.any([is_vq(layer) for layer in net.modules()])", "\ndef is_vqn(net):\n\t\"\"\" checks if the network contains a VQ layer \"\"\"\n\treturn np.any([is_vq(layer) for layer in net.modules()])\n\ndef get_vq_layers(model):\n\t\"\"\" returns vq layers from a network \"\"\"\n\treturn [m for m in model.modules() if is_vq(m)]\n\n\nclass no_vq():\n    \"\"\"\n    Function to turn off VQ by setting all the VQ layers to identity function.\n\n    Examples::\n        >>> with vqtorch.no_vq():\n        ...     out = model(x)\n    \"\"\"\n\n    def __init__(self, modules):\n        if type(modules) is not list:\n            modules = [modules]\n\n        for module in modules:\n            if isinstance(module, torch.nn.DataParallel):\n                module = module.module\n\n            assert isinstance(module, torch.nn.Module), \\\n                f'expected input to be nn.Module or a list of nn.Module ' + \\\n                f'but found {type(module)}'\n\n            self.enable_vq(module, enable=False)\n\n        self.modules = modules\n        return\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        for module in self.modules:\n            if isinstance(module, torch.nn.DataParallel):\n                module = module.module\n            self.enable_vq(module, enable=True)\n        return\n\n    def enable_vq(self, module, enable):\n        for m in module.modules():\n            if is_vq(m):\n                m.enabled = enable\n        return", "\n\nclass no_vq():\n    \"\"\"\n    Function to turn off VQ by setting all the VQ layers to identity function.\n\n    Examples::\n        >>> with vqtorch.no_vq():\n        ...     out = model(x)\n    \"\"\"\n\n    def __init__(self, modules):\n        if type(modules) is not list:\n            modules = [modules]\n\n        for module in modules:\n            if isinstance(module, torch.nn.DataParallel):\n                module = module.module\n\n            assert isinstance(module, torch.nn.Module), \\\n                f'expected input to be nn.Module or a list of nn.Module ' + \\\n                f'but found {type(module)}'\n\n            self.enable_vq(module, enable=False)\n\n        self.modules = modules\n        return\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        for module in self.modules:\n            if isinstance(module, torch.nn.DataParallel):\n                module = module.module\n            self.enable_vq(module, enable=True)\n        return\n\n    def enable_vq(self, module, enable):\n        for m in module.modules():\n            if is_vq(m):\n                m.enabled = enable\n        return", ""]}
{"filename": "vqtorch/math_fns.py", "chunked_list": ["\n\ndef entropy(x, dim=-1, eps=1e-8, keepdim=False):\n\tassert x.min() >= 0., \\\n\t\tf'function takes non-negative values but found x.min(): {x.min()}'\n\tis_tensor = True\n\n\tif len(x.shape) == 1:\n\t\tis_tensor = False\n\t\tx = x.unsqueeze(0)\n\n\tx = x.moveaxis(dim, -1)\n\tx_shape = x.shape\n\tx = x.view(-1, x.size(-1)) + eps\n\tp = x / x.sum(dim=1, keepdim=True)\n\th = - (p * p.log()).sum(dim=1, keepdim=True)\n\th = h.view(*x_shape[:-1], 1).moveaxis(dim, -1)\n\n\tif not keepdim:\n\t\th = h.squeeze(dim)\n\n\tif not is_tensor:\n\t\th = h.squeeze(0)\n\treturn h", ""]}
{"filename": "vqtorch/norms.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nMAXNORM_CONSTRAINT_VALUE = 10\n\n\nclass Normalize(nn.Module):\n    \"\"\"\n    Simple vector normalization module. By default, vectors are normalizes\n    along the channel dimesion. Each vector associated to the spatial\n    location is normalized. Used along with cosine-distance VQ layer.\n    \"\"\"\n    def __init__(self, p=2, dim=1, eps=1e-6):\n        super().__init__()\n        self.p = p\n        self.dim = dim\n        self.eps = eps\n        return\n\n    def forward(self, x):\n        return F.normalize(x, p=self.p, dim=self.dim, eps=self.eps)", "class Normalize(nn.Module):\n    \"\"\"\n    Simple vector normalization module. By default, vectors are normalizes\n    along the channel dimesion. Each vector associated to the spatial\n    location is normalized. Used along with cosine-distance VQ layer.\n    \"\"\"\n    def __init__(self, p=2, dim=1, eps=1e-6):\n        super().__init__()\n        self.p = p\n        self.dim = dim\n        self.eps = eps\n        return\n\n    def forward(self, x):\n        return F.normalize(x, p=self.p, dim=self.dim, eps=self.eps)", "\n\ndef max_norm(w, p=2, dim=-1, max_norm=MAXNORM_CONSTRAINT_VALUE, eps=1e-8):\n    norm = w.norm(p=p, dim=dim, keepdim=True)\n    desired = torch.clamp(norm.data, max=max_norm)\n    # desired = torch.clamp(norm, max=max_norm)\n    return w * (desired / (norm + eps))\n\n\nclass MaxNormConstraint(nn.Module):\n    def __init__(self, max_norm=1, p=2, dim=-1, eps=1e-8):\n        super().__init__()\n        self.p = p\n        self.eps = eps\n        self.dim = dim\n        self.max_norm = max_norm\n\n    def forward(self, x):\n        return max_norm(x, self.p, self.dim, max_norm=self.max_norm)", "\nclass MaxNormConstraint(nn.Module):\n    def __init__(self, max_norm=1, p=2, dim=-1, eps=1e-8):\n        super().__init__()\n        self.p = p\n        self.eps = eps\n        self.dim = dim\n        self.max_norm = max_norm\n\n    def forward(self, x):\n        return max_norm(x, self.p, self.dim, max_norm=self.max_norm)", "\n\n@torch.no_grad()\ndef with_codebook_normalization(func):\n    def wrapper(*args):\n        self = args[0]\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Embedding):\n                if self.codebook_norm == 'l2':\n                    m.weight.data = max_norm(m.weight.data, p=2, dim=1, eps=1e-8)\n                elif self.codebook_norm == 'l2c':\n                    m.weight.data = F.normalize(m.weight.data, p=2, dim=1, eps=1e-8)\n        return func(*args)\n    return wrapper", "\n\ndef get_norm(norm, num_channels=None):\n    before_grouping = True\n    if norm == 'l2':\n        norm_layer = Normalize(p=2, dim=-1)\n        before_grouping = False\n    elif norm == 'l2c':\n        norm_layer = MaxNormConstraint(p=2, dim=-1, max_norm=MAXNORM_CONSTRAINT_VALUE)\n        before_grouping = False\n    elif norm == 'bn':\n        norm_layer = nn.BatchNorm2d(num_channels)\n    elif norm == 'gn':\n        norm_layer = GroupNorm(num_channels)\n    elif norm in ['none', None]:\n        norm_layer = nn.Identity()\n    elif norm == 'in':\n        norm_layer = nn.InstanceNorm2d(num_channels)\n    else:\n        raise ValueError(f'unknown norm {norm}')\n    return norm_layer, before_grouping", "\n\ndef GroupNorm(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\ndef match_norm(x, y, dim=-1, eps=1e-8):\n    \"\"\"\n    matches vector norm of x to that of y\n    Args:\n        x (Tensor): a tensor of any shape\n        y (Tensor): a tensor of the same shape as `x`.\n        dim (int): dimension to match the norm over\n        eps (float): epsilon to mitigate division by zero.\n    Returns:\n        `x` with the same norm as `y` across `dim`\n    \"\"\"\n    assert x.shape == y.shape, \\\n        f'expected `x` and `y` to have the same dim but found {x.shape} vs {y.shape}'\n\n    # move chosen dim to last dim\n    x = x.moveaxis(dim, -1).contiguous()\n    y = y.moveaxis(dim, -1).contiguous()\n    x_shape = x.shape\n\n    # unravel everything such that [GBHW X C]\n\n    # print(x.shape)\n    x = x.view(-1, x.size(-1))\n    y = y.view(-1, y.size(-1))\n\n    # compute norm on C\n    x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n    y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n\n    # clamp y_norm for division by 0\n    x_norm = torch.clamp(x_norm, min=eps)\n\n    # normalize (x now has same norm as y)\n    x = y_norm * (x / x_norm)\n    x = x.view(x_shape)\n    x = x.moveaxis(-1, dim).contiguous()\n    return x", ""]}
{"filename": "vqtorch/nn/gvq.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom stringcolor import cs\nfrom vqtorch.norms import with_codebook_normalization\nfrom .vq import VectorQuant\n\n\nclass GroupVectorQuant(VectorQuant):\n\t\"\"\"\n\tVector quantization layer.\n\n\tArgs:\n\t\tgroups (int): Number of groups for vector quantization. The vectors are divided\n\t\t\tinto group chunks. When groups=1, it is the same as VectorQuant.\n\t\tshare (bool): If True, codebook is shared for each sub-vector.\n\t\t*rest*: see VectorQuant()\n\t\"\"\"\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes : int,\n\t\t\tgroups : int = 1,\n\t\t\tshare : bool = True,\n\t\t\t**kwargs,\n\t\t\t):\n\n\t\tif not share and not feature_size % groups == 0:\n\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by groups {groups}.'\n\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\n\t\tnum_codebooks = 1 if share else groups\n\t\tin_dim  = self.group_size = num_codes // num_codebooks\n\t\tout_dim = feature_size // groups\n\n\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\n\t\tself.groups = groups\n\t\tself.share = share\n\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\treturn\n\t\n\n\tdef get_codebook_by_group(self, group):\n\t\tcb = self.codebook.weight\n\t\toffset = 0 if self.share else group * self.group_size\n\t\treturn cb[offset : offset + self.group_size], offset\n\n\n\t@with_codebook_normalization\n\tdef forward(self, z):\n\n\t\t######\n\t\t## (1) formatting data by groups and invariant to dim\n\t\t######\n\n\t\tz = self.prepare_inputs(z, self.groups)\n\n\t\tif not self.enabled:\n\t\t\tz = self.to_original_format(z)\n\t\t\treturn z, {}\n\n\t\t######\n\t\t## (2) quantize latent vector\n\t\t######\n\n\t\tz_q = torch.zeros_like(z)\n\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\n\t\tfor i in range(self.groups):\n\t\t\t# select group\n\t\t\t_z = z[..., i:i+1, :]\n\n\t\t\t# quantize\n\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\n\t\t\t# assign to tensor\n\t\t\tz_q[..., i:i+1, :] = _z_q\n\t\t\td[..., i:i+1] = _d\n\t\t\tq[..., i:i+1] = _q + offset\n\n\t\tto_return = {\n\t\t\t'z'   : z,               # each group input z_e\n\t\t\t'z_q' : z_q,             # quantized output z_q\n\t\t\t'd'   : d,               # distance function for each group\n\t\t\t'q'\t  : q,\t\t\t\t # codes using offsetted indices\n\t\t\t'loss': self.compute_loss(z, z_q),\n\t\t\t'perplexity': None,\n\t\t\t}\n\n\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\tz_q = self.to_original_format(z_q)\n\t\treturn z_q, to_return", "class GroupVectorQuant(VectorQuant):\n\t\"\"\"\n\tVector quantization layer.\n\n\tArgs:\n\t\tgroups (int): Number of groups for vector quantization. The vectors are divided\n\t\t\tinto group chunks. When groups=1, it is the same as VectorQuant.\n\t\tshare (bool): If True, codebook is shared for each sub-vector.\n\t\t*rest*: see VectorQuant()\n\t\"\"\"\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes : int,\n\t\t\tgroups : int = 1,\n\t\t\tshare : bool = True,\n\t\t\t**kwargs,\n\t\t\t):\n\n\t\tif not share and not feature_size % groups == 0:\n\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by groups {groups}.'\n\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\n\t\tnum_codebooks = 1 if share else groups\n\t\tin_dim  = self.group_size = num_codes // num_codebooks\n\t\tout_dim = feature_size // groups\n\n\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\n\t\tself.groups = groups\n\t\tself.share = share\n\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\treturn\n\t\n\n\tdef get_codebook_by_group(self, group):\n\t\tcb = self.codebook.weight\n\t\toffset = 0 if self.share else group * self.group_size\n\t\treturn cb[offset : offset + self.group_size], offset\n\n\n\t@with_codebook_normalization\n\tdef forward(self, z):\n\n\t\t######\n\t\t## (1) formatting data by groups and invariant to dim\n\t\t######\n\n\t\tz = self.prepare_inputs(z, self.groups)\n\n\t\tif not self.enabled:\n\t\t\tz = self.to_original_format(z)\n\t\t\treturn z, {}\n\n\t\t######\n\t\t## (2) quantize latent vector\n\t\t######\n\n\t\tz_q = torch.zeros_like(z)\n\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\n\t\tfor i in range(self.groups):\n\t\t\t# select group\n\t\t\t_z = z[..., i:i+1, :]\n\n\t\t\t# quantize\n\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\n\t\t\t# assign to tensor\n\t\t\tz_q[..., i:i+1, :] = _z_q\n\t\t\td[..., i:i+1] = _d\n\t\t\tq[..., i:i+1] = _q + offset\n\n\t\tto_return = {\n\t\t\t'z'   : z,               # each group input z_e\n\t\t\t'z_q' : z_q,             # quantized output z_q\n\t\t\t'd'   : d,               # distance function for each group\n\t\t\t'q'\t  : q,\t\t\t\t # codes using offsetted indices\n\t\t\t'loss': self.compute_loss(z, z_q),\n\t\t\t'perplexity': None,\n\t\t\t}\n\n\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\tz_q = self.to_original_format(z_q)\n\t\treturn z_q, to_return", ""]}
{"filename": "vqtorch/nn/rvq.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom stringcolor import cs\nfrom vqtorch.norms import with_codebook_normalization\nfrom .vq import VectorQuant\n\n\nclass ResidualVectorQuant(VectorQuant):\n\t\"\"\"\n\tArgs\n\t\tgroups (int): Number of residual VQ to apply. When num_residual=1, \n\t\t\tlayer acts will be equivalent to VectorQuant.\n\t\tshare (bool): If True, codebook is shared for every quantization.\n\t\t*rest*: see VectorQuant()\n\n\tNOTE: Don't use L2 normalization on the codebook. ResidualVQ is norm sensitive.\n\t\tFor norm invariant, consider using cosine distance variant.\n\t\"\"\"\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes : int,\n\t\t\tgroups : int = 1,\n\t\t\tshare : bool = True,\n\t\t\t**kwargs,\n\t\t\t):\n\n\t\tif not share and not self.feature_size % groups == 0:\n\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by num_residual {groups}.'\n\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\n\t\tself.groups = groups\n\t\tself.share = share\n\n\t\tnum_codebooks = 1 if share else groups\n\t\tin_dim  = self.group_size = num_codes // num_codebooks\n\t\tout_dim = feature_size\n\n\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\n\t\tself.groups = groups\n\t\tself.share = share\n\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\treturn\n\n\t\n\tdef get_codebook_by_group(self, group):\n\t\tcb = self.codebook.weight\n\t\toffset = 0 if self.share else group * self.group_size\n\t\treturn cb[offset : offset + self.group_size], offset\n\t\n\n\t@with_codebook_normalization\n\tdef forward(self, z):\n\n\t\t######\n\t\t## (1) formatting data by groups and invariant to dim\n\t\t######\n\n\t\tz = self.prepare_inputs(z, groups=1)\n\n\t\tif not self.enabled:\n\t\t\tz = self.to_original_format(z)\n\t\t\treturn z, {}\n\n\t\t######\n\t\t## (2) quantize latent vector\n\t\t######\n\n\t\tz_q = torch.zeros_like(z)\n\t\tz_res = torch.zeros(*z.shape[:-2], self.groups + 1, z.shape[-1]).to(z.device)\n\n\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\n\t\tfor i in range(self.groups):\n\t\t\t# select group\n\t\t\t_z = (z - z_q) # compute resiudal\n\t\t\tz_res[..., i:i+1, :] = _z\n\n\t\t\t# quantize\n\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\n\t\t\t# update estimate\n\t\t\tz_q = z_q + _z_q\n\n\t\t\t# assign to tensor\n\t\t\td[..., i:i+1] = _d\n\t\t\tq[..., i:i+1] = _q + offset\n\t\t\n\t\tz_res[..., -1:, :] = z - z_q\n\n\t\tto_return = {\n\t\t\t'z'    : z,               # each group input z_e\n\t\t\t'z_q'  : z_q,             # quantized output z_q\n\t\t\t'd'    : d,               # distance function for each group\n\t\t\t'q'\t  : q,\t\t\t\t  # codes using offsetted indices\n\t\t\t'z_res': z_res,\n\t\t\t'loss' : self.compute_loss(z, z_q),\n\t\t\t'perplexity': None,\n\t\t\t}\n\n\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\tz_q = self.to_original_format(z_q)\n\t\treturn z_q, to_return", "class ResidualVectorQuant(VectorQuant):\n\t\"\"\"\n\tArgs\n\t\tgroups (int): Number of residual VQ to apply. When num_residual=1, \n\t\t\tlayer acts will be equivalent to VectorQuant.\n\t\tshare (bool): If True, codebook is shared for every quantization.\n\t\t*rest*: see VectorQuant()\n\n\tNOTE: Don't use L2 normalization on the codebook. ResidualVQ is norm sensitive.\n\t\tFor norm invariant, consider using cosine distance variant.\n\t\"\"\"\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes : int,\n\t\t\tgroups : int = 1,\n\t\t\tshare : bool = True,\n\t\t\t**kwargs,\n\t\t\t):\n\n\t\tif not share and not self.feature_size % groups == 0:\n\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by num_residual {groups}.'\n\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\n\t\tself.groups = groups\n\t\tself.share = share\n\n\t\tnum_codebooks = 1 if share else groups\n\t\tin_dim  = self.group_size = num_codes // num_codebooks\n\t\tout_dim = feature_size\n\n\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\n\t\tself.groups = groups\n\t\tself.share = share\n\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\treturn\n\n\t\n\tdef get_codebook_by_group(self, group):\n\t\tcb = self.codebook.weight\n\t\toffset = 0 if self.share else group * self.group_size\n\t\treturn cb[offset : offset + self.group_size], offset\n\t\n\n\t@with_codebook_normalization\n\tdef forward(self, z):\n\n\t\t######\n\t\t## (1) formatting data by groups and invariant to dim\n\t\t######\n\n\t\tz = self.prepare_inputs(z, groups=1)\n\n\t\tif not self.enabled:\n\t\t\tz = self.to_original_format(z)\n\t\t\treturn z, {}\n\n\t\t######\n\t\t## (2) quantize latent vector\n\t\t######\n\n\t\tz_q = torch.zeros_like(z)\n\t\tz_res = torch.zeros(*z.shape[:-2], self.groups + 1, z.shape[-1]).to(z.device)\n\n\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\n\t\tfor i in range(self.groups):\n\t\t\t# select group\n\t\t\t_z = (z - z_q) # compute resiudal\n\t\t\tz_res[..., i:i+1, :] = _z\n\n\t\t\t# quantize\n\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\n\t\t\t# update estimate\n\t\t\tz_q = z_q + _z_q\n\n\t\t\t# assign to tensor\n\t\t\td[..., i:i+1] = _d\n\t\t\tq[..., i:i+1] = _q + offset\n\t\t\n\t\tz_res[..., -1:, :] = z - z_q\n\n\t\tto_return = {\n\t\t\t'z'    : z,               # each group input z_e\n\t\t\t'z_q'  : z_q,             # quantized output z_q\n\t\t\t'd'    : d,               # distance function for each group\n\t\t\t'q'\t  : q,\t\t\t\t  # codes using offsetted indices\n\t\t\t'z_res': z_res,\n\t\t\t'loss' : self.compute_loss(z, z_q),\n\t\t\t'perplexity': None,\n\t\t\t}\n\n\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\tz_q = self.to_original_format(z_q)\n\t\treturn z_q, to_return", ""]}
{"filename": "vqtorch/nn/__init__.py", "chunked_list": ["from .vq_base import _VQBaseLayer\n\nfrom .vq import VectorQuant\nfrom .gvq import GroupVectorQuant\nfrom .rvq import ResidualVectorQuant\n\nfrom .affine import AffineTransform\nfrom . import utils\n", ""]}
{"filename": "vqtorch/nn/affine.py", "chunked_list": ["import math\nimport time\nimport warnings\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n", "\n\n\nclass AffineTransform(nn.Module):\n\tdef __init__(\n\t\t\tself, \n\t\t\tfeature_size, \n\t\t\tuse_running_statistics=False, \n\t\t\tmomentum=0.1, \n\t\t\tlr_scale=1,\n\t\t\tnum_groups=1,\n\t\t\t):\n\t\tsuper().__init__()\n\n\t\tself.use_running_statistics = use_running_statistics\n\t\tself.num_groups = num_groups\n\n\t\tif use_running_statistics:\n\t\t\tself.momentum = momentum\n\t\t\tself.register_buffer('running_statistics_initialized', torch.zeros(1))\n\t\t\tself.register_buffer('running_ze_mean', torch.zeros(num_groups, feature_size))\n\t\t\tself.register_buffer('running_ze_var', torch.ones(num_groups, feature_size))\n\n\t\t\tself.register_buffer('running_c_mean', torch.zeros(num_groups, feature_size))\n\t\t\tself.register_buffer('running_c_var', torch.ones(num_groups, feature_size))\n\t\telse:\n\t\t\tself.scale = nn.parameter.Parameter(torch.zeros(num_groups, feature_size))\n\t\t\tself.bias = nn.parameter.Parameter(torch.zeros(num_groups, feature_size))\n\t\t\tself.lr_scale = lr_scale\n\t\treturn\n\n\t@torch.no_grad()\n\tdef update_running_statistics(self, z_e, c):\n\t\t# we find it helpful to often to make an under-estimation on the\n\t\t# z_e embedding statistics. Empirically we observe a slight\n\t\t# over-estimation of the statistics, causing the straight-through\n\t\t# estimation to grow indefinitely. While this is not an issue\n\t\t# for most model architecture, some model architectures that don't\n\t\t# have normalized bottlenecks, can cause it to eventually explode.\n        # placing the VQ layer in certain layers of ViT exhibits this behavior\n\n\n\t\tif self.training and self.use_running_statistics:\n\t\t\tunbiased = False\n\n\t\t\tze_mean = z_e.mean([0, 1]).unsqueeze(0)\n\t\t\tze_var = z_e.var([0, 1], unbiased=unbiased).unsqueeze(0)\n\n\t\t\tc_mean = c.mean([0]).unsqueeze(0)\n\t\t\tc_var = c.var([0], unbiased=unbiased).unsqueeze(0)\n\n\t\t\tif not self.running_statistics_initialized:\n\t\t\t\tself.running_ze_mean.data.copy_(ze_mean)\n\t\t\t\tself.running_ze_var.data.copy_(ze_var)\n\t\t\t\tself.running_c_mean.data.copy_(c_mean)\n\t\t\t\tself.running_c_var.data.copy_(c_var)\n\t\t\t\tself.running_statistics_initialized.fill_(1)\n\t\t\telse:\n\t\t\t\tself.running_ze_mean = (self.momentum * ze_mean) + (1 - self.momentum) * self.running_ze_mean\n\t\t\t\tself.running_ze_var = (self.momentum * ze_var) + (1 - self.momentum) * self.running_ze_var\n\t\t\t\tself.running_c_mean = (self.momentum * c_mean) + (1 - self.momentum) * self.running_c_mean\n\t\t\t\tself.running_c_var = (self.momentum * c_var) + (1 - self.momentum) * self.running_c_var\n\n\t\t# wd = 0.9998 # 0.995\n\t\t# self.running_ze_mean = wd * self.running_ze_mean\n\t\t# self.running_ze_var = wd * self.running_ze_var\n\t\treturn\n\n\n\tdef forward(self, codebook):\n\t\tscale, bias = self.get_affine_params()\n\t\tn, c = codebook.shape\n\t\tcodebook = codebook.view(self.num_groups, -1, codebook.shape[-1])\n\t\tcodebook = scale * codebook + bias\n\t\treturn codebook.reshape(n, c)\n\n\n\tdef get_affine_params(self):\n\t\tif self.use_running_statistics:\n\t\t\tscale = (self.running_ze_var / (self.running_c_var + 1e-8)).sqrt()\n\t\t\tbias = - scale * self.running_c_mean + self.running_ze_mean\n\t\telse:\n\t\t\tscale = (1. + self.lr_scale * self.scale)\n\t\t\tbias = self.lr_scale * self.bias\n\t\treturn scale.unsqueeze(1), bias.unsqueeze(1)", ""]}
{"filename": "vqtorch/nn/pool.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass _VecPool2d(nn.Module):\n    def __init__(self, weighting_fn, kernel_size=3, stride=2, padding=2, dilation=1):\n        super().__init__()\n\n        self.k = kernel_size\n        self.s = stride\n        self.p = padding\n        self.d = dilation\n\n        self.weighting_fn = weighting_fn\n\n        self.unfold = nn.Unfold(kernel_size=self.k, dilation=self.d, padding=self.p, stride=self.s)\n        return\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        out_h = (h - self.k + 2 * self.p) // self.s + 1\n        out_w = (w - self.k + 2 * self.p) // self.s + 1\n\n        with torch.no_grad():\n            n = x.norm(dim=1, p=2, keepdim=True)\n            n = self.unfold(n) # B x (K**2) x N\n            n = self.weighting_fn(n, dim=1).unsqueeze(1)\n\n        x = self.unfold(x)  # B x C * (K**2) x N\n        x = x.view(b, c, -1, x.size(-1))\n\n        x = n * x\n        x = x.sum(2).view(b, c, out_h, out_w)\n        return x", "\n\nclass MaxVecPool2d(_VecPool2d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(MaxVecPool2d.max_onehot, *args, **kwargs)\n\n    @staticmethod\n    def max_onehot(x, dim):\n        b, s, n = x.shape\n\n        x = x.argmax(dim=1)\n        x = x.view(-1)\n        x = F.one_hot(x, s).view(b, n, s).swapaxes(-1, dim)\n        return x", "\n\nclass SoftMaxVecPool2d(_VecPool2d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(torch.softmax, *args, **kwargs)\n"]}
{"filename": "vqtorch/nn/vq_base.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom vqtorch.norms import get_norm\nfrom vqtorch.nn.utils.init import data_dependent_init_forward_hook\n\n\n\nclass _VQBaseLayer(nn.Module):\n\t\"\"\"\n\tBase template code for vector quanitzation. All VQ layers will inherit\n\tfrom this class.\n\n\tArgs:\n\t\tfeature_size (int):\n\t\t\tThe size of the feature. this is the length of each\n\t\t\tcode vector. the dimensions must match the input feature size.\n\t\tnum_codes (int):\n\t\t\tNumber of codes to use in the codebook.\n\t\tdim (int): Dimension to quantize. by default quantization happens on\n\t\t\tthe channel dimension. For example, given an image tensor\n\t\t\t(B x C x H x W) and dim=1, the channels are treated as features \n\t\t\tand the resulting codes `q` has the shape (B x H x W). \n\t\t\tFor transformers (B x N x C), you should set dim=2 or -1.\n\t\tnorm (str): Feature normalization.\n\t\tcodebook_norm (str): Codebook normalization.\n\n\tReturns:\n\t\tQuantized vector z_q and return dict\n\n\tAttributes:\n\t\tcdist_chunk_size (int): chunk size for divide-and-conquer topk cdist.\n\t\tenabled (bool): If false, the model is not quantized and acts as an identity layer.\n\t\"\"\"\n\n\tcdist_chunk_size = 1024\n\tenabled = True\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes :\tint,\n\t\t\tdim : int = 1,\n\t\t\tnorm :\tstr = 'none',\n\t\t\tcb_norm : str = 'none',\n\t\t\tkmeans_init : bool = False,\n\t\t\tcode_vector_size : int = None,\n\t\t\t):\n\n\t\tsuper().__init__()\n\t\tself.feature_size = feature_size\n\t\tself.code_vector_size = feature_size if code_vector_size is None else code_vector_size\n\t\tself.num_codes = num_codes\n\t\tself.dim = dim\n\n\t\tself.groups = 1 # for group VQ\n\t\tself.topk = 1   # for probabilistic VQ\n\n\t\tself.norm = norm\n\t\tself.codebook_norm = cb_norm\n\t\tself.norm_layer, self.norm_before_grouping = get_norm(norm, feature_size)\n\n\t\tif kmeans_init:\n\t\t\tself.register_buffer('data_initialized', torch.zeros(1))\n\t\t\tself.register_forward_hook(data_dependent_init_forward_hook)\n\t\treturn\n\n\tdef quantize(self, codebook, z):\n\t\t\"\"\"\n\t\tQuantizes the latent codes z with the codebook\n\n\t\tArgs:\n\t\t\tcodebook (Tensor): B x C\n\t\t\tz (Tensor): B x ... x C\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\n\tdef compute_loss(self, z_e, z_q):\n\t\t\"\"\" computes error between z and z_q \"\"\"\n\t\traise NotImplementedError\n\n\n\tdef to_canonical_group_format(self, z, groups):\n\t\t\"\"\"\n\t\tConverts data into canonical group format\n\n\t\tThe quantization dim is sent to the last dimension.\n\t\tThe features are then resized such that C -> G x C'\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\tgroups (int): number of groups\n\t\tReturns:\n\t\t\tx of shape [B x ... x G x C']\n\t\t\"\"\"\n\n\t\tz = z.moveaxis(self.dim, -1).contiguous()\n\t\tz = z.unflatten(-1, (groups, -1))\n\t\treturn z\n\n\n\tdef to_original_format(self, x):\n\t\t\"\"\"\n\t\tMerges group and permutes dimension back\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x ... x G x C // G]\n\t\tReturns:\n\t\t\tmerged `x` of shape [B x ... x C] (assuming dim=1)\n\t\t\"\"\"\n\t\treturn x.flatten(-2, -1).moveaxis(-1, self.dim)\n\n\n\tdef prepare_inputs(self, z, groups):\n\t\t\"\"\"\n\t\tPrepare input with normalization and group format\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\tgroups (int): number of groups\n\t\t\"\"\"\n\n\t\tif len(z.shape) <= 1:\n\t\t\te_msg = f'expected a tensor of at least 2 dimensions but found {z.size()}'\n\t\t\traise ValueError(e_msg)\n\n\t\tif self.norm_before_grouping:\n\t\t\tz = self.norm_layer(z)\n\n\t\tz = self.to_canonical_group_format(z, groups)\n\n\t\tif not self.norm_before_grouping:\n\t\t\tz = self.norm_layer(z)\n\n\t\treturn z\n\n\n\t@property\n\tdef requires_grad(self):\n\t\treturn self.codebook[0].weight.requires_grad\n\n\n\tdef set_requires_grad(self, requires_grad):\n\t\tfor codebook in self.codebook:\n\t\t\tcodebook.weight.requires_grad = requires_grad\n\t\treturn\n\n\n\tdef extra_repr(self):\n\t\trepr = \"\\n\".join([\n\t\t\tf\"num_codes: {self.num_codes}\",\n\t\t\tf\"groups: {self.groups}\",\n\t\t\tf\"enabled: {self.enabled}\",\n\t\t])\n\t\treturn repr", "class _VQBaseLayer(nn.Module):\n\t\"\"\"\n\tBase template code for vector quanitzation. All VQ layers will inherit\n\tfrom this class.\n\n\tArgs:\n\t\tfeature_size (int):\n\t\t\tThe size of the feature. this is the length of each\n\t\t\tcode vector. the dimensions must match the input feature size.\n\t\tnum_codes (int):\n\t\t\tNumber of codes to use in the codebook.\n\t\tdim (int): Dimension to quantize. by default quantization happens on\n\t\t\tthe channel dimension. For example, given an image tensor\n\t\t\t(B x C x H x W) and dim=1, the channels are treated as features \n\t\t\tand the resulting codes `q` has the shape (B x H x W). \n\t\t\tFor transformers (B x N x C), you should set dim=2 or -1.\n\t\tnorm (str): Feature normalization.\n\t\tcodebook_norm (str): Codebook normalization.\n\n\tReturns:\n\t\tQuantized vector z_q and return dict\n\n\tAttributes:\n\t\tcdist_chunk_size (int): chunk size for divide-and-conquer topk cdist.\n\t\tenabled (bool): If false, the model is not quantized and acts as an identity layer.\n\t\"\"\"\n\n\tcdist_chunk_size = 1024\n\tenabled = True\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes :\tint,\n\t\t\tdim : int = 1,\n\t\t\tnorm :\tstr = 'none',\n\t\t\tcb_norm : str = 'none',\n\t\t\tkmeans_init : bool = False,\n\t\t\tcode_vector_size : int = None,\n\t\t\t):\n\n\t\tsuper().__init__()\n\t\tself.feature_size = feature_size\n\t\tself.code_vector_size = feature_size if code_vector_size is None else code_vector_size\n\t\tself.num_codes = num_codes\n\t\tself.dim = dim\n\n\t\tself.groups = 1 # for group VQ\n\t\tself.topk = 1   # for probabilistic VQ\n\n\t\tself.norm = norm\n\t\tself.codebook_norm = cb_norm\n\t\tself.norm_layer, self.norm_before_grouping = get_norm(norm, feature_size)\n\n\t\tif kmeans_init:\n\t\t\tself.register_buffer('data_initialized', torch.zeros(1))\n\t\t\tself.register_forward_hook(data_dependent_init_forward_hook)\n\t\treturn\n\n\tdef quantize(self, codebook, z):\n\t\t\"\"\"\n\t\tQuantizes the latent codes z with the codebook\n\n\t\tArgs:\n\t\t\tcodebook (Tensor): B x C\n\t\t\tz (Tensor): B x ... x C\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\n\tdef compute_loss(self, z_e, z_q):\n\t\t\"\"\" computes error between z and z_q \"\"\"\n\t\traise NotImplementedError\n\n\n\tdef to_canonical_group_format(self, z, groups):\n\t\t\"\"\"\n\t\tConverts data into canonical group format\n\n\t\tThe quantization dim is sent to the last dimension.\n\t\tThe features are then resized such that C -> G x C'\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\tgroups (int): number of groups\n\t\tReturns:\n\t\t\tx of shape [B x ... x G x C']\n\t\t\"\"\"\n\n\t\tz = z.moveaxis(self.dim, -1).contiguous()\n\t\tz = z.unflatten(-1, (groups, -1))\n\t\treturn z\n\n\n\tdef to_original_format(self, x):\n\t\t\"\"\"\n\t\tMerges group and permutes dimension back\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x ... x G x C // G]\n\t\tReturns:\n\t\t\tmerged `x` of shape [B x ... x C] (assuming dim=1)\n\t\t\"\"\"\n\t\treturn x.flatten(-2, -1).moveaxis(-1, self.dim)\n\n\n\tdef prepare_inputs(self, z, groups):\n\t\t\"\"\"\n\t\tPrepare input with normalization and group format\n\n\t\tArgs:\n\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\tgroups (int): number of groups\n\t\t\"\"\"\n\n\t\tif len(z.shape) <= 1:\n\t\t\te_msg = f'expected a tensor of at least 2 dimensions but found {z.size()}'\n\t\t\traise ValueError(e_msg)\n\n\t\tif self.norm_before_grouping:\n\t\t\tz = self.norm_layer(z)\n\n\t\tz = self.to_canonical_group_format(z, groups)\n\n\t\tif not self.norm_before_grouping:\n\t\t\tz = self.norm_layer(z)\n\n\t\treturn z\n\n\n\t@property\n\tdef requires_grad(self):\n\t\treturn self.codebook[0].weight.requires_grad\n\n\n\tdef set_requires_grad(self, requires_grad):\n\t\tfor codebook in self.codebook:\n\t\t\tcodebook.weight.requires_grad = requires_grad\n\t\treturn\n\n\n\tdef extra_repr(self):\n\t\trepr = \"\\n\".join([\n\t\t\tf\"num_codes: {self.num_codes}\",\n\t\t\tf\"groups: {self.groups}\",\n\t\t\tf\"enabled: {self.enabled}\",\n\t\t])\n\t\treturn repr", ""]}
{"filename": "vqtorch/nn/vq.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vqtorch.dists import get_dist_fns\nimport vqtorch\nfrom vqtorch.norms import with_codebook_normalization\nfrom .vq_base import _VQBaseLayer\nfrom .affine import AffineTransform\n", "from .affine import AffineTransform\n\n\nclass VectorQuant(_VQBaseLayer):\n\t\"\"\"\n\tVector quantization layer using straight-through estimation.\n\n\tArgs:\n\t\tfeature_size (int): feature dimension corresponding to the vectors\n\t\tnum_codes (int): number of vectors in the codebook\n\t\tbeta (float): commitment loss weighting\n\t\tsync_nu (float): sync loss weighting\n\t\taffine_lr (float): learning rate for affine transform\n\t\taffine_groups (int): number of affine parameter groups\n\t\treplace_freq (int): frequency to replace dead codes\n\t\tinplace_optimizer (Optimizer): optimizer for inplace codebook updates\n\t\t**kwargs: additional arguments for _VQBaseLayer\n\t\n\tReturns:\n\t\tQuantized vector z_q and return dict\n\t\"\"\"\n\n\n\tdef __init__(\n\t\t\tself,\n\t\t\tfeature_size : int,\n\t\t\tnum_codes : int,\n\t\t\tbeta : float = 0.95,\n\t\t\tsync_nu : float = 0.0,\n\t\t\taffine_lr:\tfloat = 0.0,\n\t\t\taffine_groups: int = 1,\n\t\t\treplace_freq: int = 0,\n\t\t\tinplace_optimizer: torch.optim.Optimizer = None,\n\t\t\t**kwargs,\n\t\t\t):\n\n\t\tsuper().__init__(feature_size, num_codes, **kwargs)\n\t\tself.loss_fn, self.dist_fn = get_dist_fns('euclidean')\n\n\t\tif beta < 0.0 or beta > 1.0:\n\t\t\traise ValueError(f'beta must be in [0, 1] but got {beta}')\n\t\t\t\n\t\tself.beta = beta\n\t\tself.nu = sync_nu\n\t\tself.affine_lr = affine_lr\n\t\tself.codebook = nn.Embedding(self.num_codes, self.feature_size)\n\n\t\tif inplace_optimizer is not None:\n\t\t\tif beta != 1.0:\n\t\t\t\traise ValueError('inplace_optimizer can only be used with beta=1.0')\n\t\t\tself.inplace_codebook_optimizer = inplace_optimizer(self.codebook.parameters())\t\t\t\n\n\t\tif affine_lr > 0:\n\t\t\t# defaults to using learnable affine parameters\n\t\t\tself.affine_transform = AffineTransform(\n\t\t\t\t\t\t\t\t\t\tself.code_vector_size,\n\t\t\t\t\t\t\t\t\t\tuse_running_statistics=False,\n\t\t\t\t\t\t\t\t\t\tlr_scale=affine_lr,\n\t\t\t\t\t\t\t\t\t\tnum_groups=affine_groups,\n\t\t\t\t\t\t\t\t\t\t)\n\t\tif replace_freq > 0:\n\t\t\tvqtorch.nn.utils.lru_replacement(self, rho=0.01, timeout=replace_freq)\n\t\treturn\n\n\n\tdef straight_through_approximation(self, z, z_q):\n\t\t\"\"\" passed gradient from z_q to z \"\"\"\n\t\tif self.nu > 0:\n\t\t\tz_q = z + (z_q - z).detach() + (self.nu * z_q) + (-self.nu * z_q).detach()\n\t\telse:\n\t\t\tz_q = z + (z_q - z).detach()\n\t\treturn z_q\n\n\n\tdef compute_loss(self, z_e, z_q):\n\t\t\"\"\" computes loss between z and z_q \"\"\"\n\t\treturn ((1.0 - self.beta) * self.loss_fn(z_e, z_q.detach()) + \\\n\t\t\t\t\t  (self.beta) * self.loss_fn(z_e.detach(), z_q))\n\n\n\tdef quantize(self, codebook, z):\n\t\t\"\"\"\n\t\tQuantizes the latent codes z with the codebook\n\n\t\tArgs:\n\t\t\tcodebook (Tensor): B x F\n\t\t\tz (Tensor): B x ... x F\n\t\t\"\"\"\n\n\t\t# reshape to (BHWG x F//G) and compute distance\n\t\tz_shape = z.shape[:-1]\n\t\tz_flat = z.view(z.size(0), -1, z.size(-1))\n\n\t\tif hasattr(self, 'affine_transform'):\n\t\t\tself.affine_transform.update_running_statistics(z_flat, codebook)\n\t\t\tcodebook = self.affine_transform(codebook)\n\n\t\twith torch.no_grad():\n\t\t\tdist_out = self.dist_fn(\n\t\t\t\t\t\t\ttensor=z_flat,\n\t\t\t\t\t\t\tcodebook=codebook,\n\t\t\t\t\t\t\ttopk=self.topk,\n\t\t\t\t\t\t\tcompute_chunk_size=self.cdist_chunk_size,\n\t\t\t\t\t\t\thalf_precision=(z.is_cuda),\n\t\t\t\t\t\t\t)\n\n\t\t\td = dist_out['d'].view(z_shape)\n\t\t\tq = dist_out['q'].view(z_shape).long()\n\n\t\tz_q = F.embedding(q, codebook)\n\n\t\tif self.training and hasattr(self, 'inplace_codebook_optimizer'):\n\t\t\t# update codebook inplace \n\t\t\t((z_q - z.detach()) ** 2).mean().backward()\n\t\t\tself.inplace_codebook_optimizer.step()\n\t\t\tself.inplace_codebook_optimizer.zero_grad()\n\n\t\t\t# forward pass again with the update codebook\n\t\t\tz_q = F.embedding(q, codebook)\n\n\t\t\t# NOTE to save compute, we assumed Q did not change.\n\n\t\treturn z_q, d, q\n\n\t@torch.no_grad()\n\tdef get_codebook(self):\n\t\tcb = self.codebook.weight\n\t\tif hasattr(self, 'affine_transform'):\n\t\t\tcb = self.affine_transform(cb)\n\t\treturn cb\n\n\tdef get_codebook_affine_params(self):\n\t\tif hasattr(self, 'affine_transform'):\n\t\t\treturn self.affine_transform.get_affine_params()\n\t\treturn None\n\n\t@with_codebook_normalization\n\tdef forward(self, z):\n\n\t\t######\n\t\t## (1) formatting data by groups and invariant to dim\n\t\t######\n\n\t\tz = self.prepare_inputs(z, self.groups)\n\n\t\tif not self.enabled:\n\t\t\tz = self.to_original_format(z)\n\t\t\treturn z, {}\n\n\t\t######\n\t\t## (2) quantize latent vector\n\t\t######\n\n\t\tz_q, d, q = self.quantize(self.codebook.weight, z)\n\n\t\t# e_mean = F.one_hot(q, num_classes=self.num_codes).view(-1, self.num_codes).float().mean(0)\n\t\t# perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n\t\tperplexity = None\n\n\t\tto_return = {\n\t\t\t'z'  : z,               # each group input z_e\n\t\t\t'z_q': z_q,             # quantized output z_q\n\t\t\t'd'  : d,               # distance function for each group\n\t\t\t'q'\t : q,\t\t\t\t# codes\n\t\t\t'loss': self.compute_loss(z, z_q).mean(),\n\t\t\t'perplexity': perplexity,\n\t\t\t}\n\n\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\tz_q = self.to_original_format(z_q)\n\n\t\treturn z_q, to_return", ""]}
{"filename": "vqtorch/nn/utils/__init__.py", "chunked_list": ["from .replace import lru_replacement\n"]}
{"filename": "vqtorch/nn/utils/replace.py", "chunked_list": ["import torch\n\n\n\nclass ReplaceLRU():\n\t\"\"\"\n\tAttributes:\n\t\trho (float): mutation noise\n\t\ttimeout (int): number of batch it has seen\n\t\"\"\"\n\tVALID_POLICIES = ['input_random', 'input_kmeans', 'self']\n\n\tdef __init__(self, rho=1e-4, timeout=100):\n\t\tassert timeout > 1\n\t\tassert rho > 0.0\n\t\tself.rho = rho\n\t\tself.timeout = timeout\n\n\t\tself.policy = 'input_random'\n\t\t# self.policy = 'input_kmeans'\n\t\t# self.policy = 'self'\n\t\tself.tau = 2.0\n\n\t\tassert self.policy in self.VALID_POLICIES\n\t\treturn\n\n\t@staticmethod\n\tdef apply(module, rho=0., timeout=100):\n\t\t\"\"\" register forward hook \"\"\"\n\t\tfn = ReplaceLRU(rho, timeout)\n\t\tdevice = next(module.parameters()).device\n\t\tmodule.register_forward_hook(fn)\n\t\tmodule.register_buffer('_counts', timeout * torch.ones(module.num_codes))\n\t\tmodule._counts = module._counts.to(device)\n\t\treturn fn\n\n\tdef __call__(self, module, inputs, outputs):\n\t\t\"\"\"\n\t\tThis function is triggered during forward pass\n\t\trecall: z_q, misc = vq(x)\n\n\t\tArgs\n\t\t\tmodule (nn.VectorQuant)\n\t\t\tinputs (tuple): A tuple with 1 element\n\t\t\t\tx (Tensor)\n\t\t\toutputs (tuple): A tuple with 2 elements\n\t\t\t\tz_q (Tensor), misc (dict)\n\t\t\"\"\"\n\t\tif not module.training:\n\t\t\treturn\n\n\t\t# count down all code by 1 and if used, reset timer to timeout value\n\t\tmodule._counts -= 1\n\n\t\t# --- computes most recent codebook usage --- #\n\t\tunique, counts = torch.unique(outputs[1]['q'], return_counts=True)\n\t\tmodule._counts.index_fill_(0, unique, self.timeout)\n\n\t\t# --- find how many needs to be replaced --- #\n\t\t# num_active = self.check_and_replace_dead_codes(module, outputs)\n\t\tinactive_indices = torch.argwhere(module._counts == 0).squeeze(-1)\n\t\tnum_inactive = inactive_indices.size(0)\n\n\t\tif num_inactive > 0:\n\n\t\t\tif self.policy == 'self':\n\t\t\t\t# exponential distance allows more recently used codes to be even more preferable\n\t\t\t\tp = torch.zeros_like(module._counts)\n\t\t\t\tp[unique] = counts.float()\n\t\t\t\tp = p / p.sum()\n\t\t\t\tp = torch.exp(self.tau * p) - 1 # the negative 1 is to drive p=0 to stay 0\n\n\t\t\t\tselected_indices = torch.multinomial(p, num_inactive, replacement=True)\n\t\t\t\tselected_values = module.codebook.weight.data[selected_indices].clone()\n\n\t\t\telif self.policy == 'input_random':\n\t\t\t\tz_e = outputs[1]['z'].flatten(0, -2)   # flatten to 2D\n\t\t\t\tz_e = z_e[torch.randperm(z_e.size(0))] # shuffle\n\t\t\t\tmult = num_inactive // z_e.size(0) + 1\n\t\t\t\tif mult > 1: # if theres not enough\n\t\t\t\t\tz_e = torch.cat(mult * [z_e])\n\t\t\t\tselected_values = z_e[:num_inactive]\n\n\t\t\telif self.policy == 'input_kmeans':\n\t\t\t\t# can be extremely slow\n\t\t\t\tfrom torchpq.clustering import KMeans\n\t\t\t\tz_e = outputs[1]['z'].flatten(0, -2)   # flatten to 2D\n\t\t\t\tz_e = z_e[torch.randperm(z_e.size(0))] # shuffle\n\t\t\t\tkmeans = KMeans(n_clusters=num_inactive, distance='euclidean', init_mode=\"kmeans++\")\n\t\t\t\tkmeans.fit(z_e.data.T.contiguous())\n\t\t\t\tselected_values = kmeans.centroids.T\n\n\t\t\tif self.rho > 0:\n\t\t\t\tnorm = selected_values.norm(p=2, dim=-1, keepdim=True)\n\t\t\t\tnoise = torch.randn_like(selected_values)\n\t\t\t\tselected_values = selected_values + self.rho * norm * noise\n\n\t\t\t# --- update dead codes with new codes --- #\n\t\t\tmodule.codebook.weight.data[inactive_indices] = selected_values\n\t\t\tmodule._counts[inactive_indices] += self.timeout\n\n\t\treturn outputs", "\n\n\ndef lru_replacement(vq_module, rho=1e-4, timeout=100):\n\t\"\"\"\n\tExample::\n\t\t>>> vq = VectorQuant(...)\n\t\t>>> vq = lru_replacement(vq)\n\t\"\"\"\n\tReplaceLRU.apply(vq_module, rho, timeout)\n\treturn vq_module", ""]}
{"filename": "vqtorch/nn/utils/init.py", "chunked_list": ["import torch\nfrom stringcolor import cs\nimport warnings\nimport vqtorch\n\n\n\n@torch.no_grad()\ndef data_dependent_init_forward_hook(self, inputs, outputs, use_kmeans=True, verbose=False):\n\t\"\"\" initializes codebook from data \"\"\"\n\n\tif (not self.training) or (self.data_initialized.item() == 1):\n\t\treturn\n\n\tif verbose:\n\t\tprint(cs('initializing codebook with k-means++', 'y'))\n\n\tdef sample_centroids(z_e, num_codes):\n\t\t\"\"\" replaces the data of the codebook with z_e randomly. \"\"\"\n\n\t\tz_e = z_e.reshape(-1, z_e.size(-1))\n\n\t\tif num_codes >= z_e.size(0):\n\t\t\te_msg = f'\\ncodebook size > warmup samples: {num_codes} vs {z_e.size(0)}. ' + \\\n\t\t\t\t\t 'recommended to decrease the codebook size or increase batch size.'\n\n\t\t\twarnings.warn(str(cs(e_msg, 'yellow')))\n\n\t\t\t# repeat until it fits and add noise\n\t\t\trepeat = num_codes // z_e.shape[0]\n\t\t\tnew_codes = z_e.data.tile([repeat, 1])[:num_codes]\n\t\t\tnew_codes += 1e-3 * torch.randn_like(new_codes.data)\n\n\t\telse:\n\t\t\t# you have more warmup samples than codebook. subsample data\n\t\t\tif use_kmeans:\n\t\t\t\tfrom torchpq.clustering import KMeans\n\t\t\t\tkmeans = KMeans(n_clusters=num_codes, distance='euclidean', init_mode=\"kmeans++\")\n\t\t\t\tkmeans.fit(z_e.data.T.contiguous())\n\t\t\t\tnew_codes = kmeans.centroids.T\n\t\t\telse:\n\t\t\t\tindices = torch.randint(low=0, high=num_codes, size=(num_codes,))\n\t\t\t\tindices = indices.to(z_e.device)\n\t\t\t\tnew_codes = torch.index_select(z_e, 0, indices).to(z_e.device).data\n\n\t\treturn new_codes\n\n\t_, misc = outputs\n\tz_e, z_q = misc['z'], misc['z_q']\n\n\tif type(self) is vqtorch.nn.VectorQuant:\n\t\tnum_codes = self.codebook.weight.shape[0]\n\t\tnew_codebook = sample_centroids(z_e, num_codes)\n\t\tself.codebook.weight.data = new_codebook\n\n\telif type(self) is vqtorch.nn.GroupVectorQuant:\n\t\tif self.share:\n\t\t\tprint(self.codebook.weight.shape)\n\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\tself.codebook.weight.data = new_codebook\n\t\telse:\n\t\t\tfor i in range(self.groups):\n\t\t\t\toffset = i * self.group_size\n\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n\t\t\t\t\n\telif type(self) is vqtorch.nn.ResidualVectorQuant:\n\t\tz_e = misc['z_res']\n\n\t\tif self.share:\n\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\tself.codebook.weight.data = new_codebook\n\t\telse:\n\t\t\tfor i in range(self.groups):\n\t\t\t\toffset = i * self.group_size\n\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n\t\t\t\t\n\n\tself.data_initialized.fill_(1)\n\treturn", "def data_dependent_init_forward_hook(self, inputs, outputs, use_kmeans=True, verbose=False):\n\t\"\"\" initializes codebook from data \"\"\"\n\n\tif (not self.training) or (self.data_initialized.item() == 1):\n\t\treturn\n\n\tif verbose:\n\t\tprint(cs('initializing codebook with k-means++', 'y'))\n\n\tdef sample_centroids(z_e, num_codes):\n\t\t\"\"\" replaces the data of the codebook with z_e randomly. \"\"\"\n\n\t\tz_e = z_e.reshape(-1, z_e.size(-1))\n\n\t\tif num_codes >= z_e.size(0):\n\t\t\te_msg = f'\\ncodebook size > warmup samples: {num_codes} vs {z_e.size(0)}. ' + \\\n\t\t\t\t\t 'recommended to decrease the codebook size or increase batch size.'\n\n\t\t\twarnings.warn(str(cs(e_msg, 'yellow')))\n\n\t\t\t# repeat until it fits and add noise\n\t\t\trepeat = num_codes // z_e.shape[0]\n\t\t\tnew_codes = z_e.data.tile([repeat, 1])[:num_codes]\n\t\t\tnew_codes += 1e-3 * torch.randn_like(new_codes.data)\n\n\t\telse:\n\t\t\t# you have more warmup samples than codebook. subsample data\n\t\t\tif use_kmeans:\n\t\t\t\tfrom torchpq.clustering import KMeans\n\t\t\t\tkmeans = KMeans(n_clusters=num_codes, distance='euclidean', init_mode=\"kmeans++\")\n\t\t\t\tkmeans.fit(z_e.data.T.contiguous())\n\t\t\t\tnew_codes = kmeans.centroids.T\n\t\t\telse:\n\t\t\t\tindices = torch.randint(low=0, high=num_codes, size=(num_codes,))\n\t\t\t\tindices = indices.to(z_e.device)\n\t\t\t\tnew_codes = torch.index_select(z_e, 0, indices).to(z_e.device).data\n\n\t\treturn new_codes\n\n\t_, misc = outputs\n\tz_e, z_q = misc['z'], misc['z_q']\n\n\tif type(self) is vqtorch.nn.VectorQuant:\n\t\tnum_codes = self.codebook.weight.shape[0]\n\t\tnew_codebook = sample_centroids(z_e, num_codes)\n\t\tself.codebook.weight.data = new_codebook\n\n\telif type(self) is vqtorch.nn.GroupVectorQuant:\n\t\tif self.share:\n\t\t\tprint(self.codebook.weight.shape)\n\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\tself.codebook.weight.data = new_codebook\n\t\telse:\n\t\t\tfor i in range(self.groups):\n\t\t\t\toffset = i * self.group_size\n\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n\t\t\t\t\n\telif type(self) is vqtorch.nn.ResidualVectorQuant:\n\t\tz_e = misc['z_res']\n\n\t\tif self.share:\n\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\tself.codebook.weight.data = new_codebook\n\t\telse:\n\t\t\tfor i in range(self.groups):\n\t\t\t\toffset = i * self.group_size\n\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n\t\t\t\t\n\n\tself.data_initialized.fill_(1)\n\treturn", ""]}
{"filename": "examples/experimental_inplace_update.py", "chunked_list": ["# mnist VQ experiment with various settings.\nimport torch \nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torchvision import datasets\n\nfrom vqtorch.nn import VectorQuant\nfrom tqdm.auto import trange\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms", "from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n\nlr = 3e-4\ntrain_iter = 300\nnum_codes = 256\nseed = 1234\n\nclass SimpleVQClassifier(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n                nn.AdaptiveMaxPool2d((1, 1)),\n                nn.Flatten(),\n                nn.Linear(64, 10),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x, vq_dict", "\nclass SimpleVQClassifier(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n                nn.AdaptiveMaxPool2d((1, 1)),\n                nn.Flatten(),\n                nn.Linear(64, 10),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x, vq_dict", "\n\ndef train(model, train_loader, train_iterations=1000, alpha=10, ignore_commitment_loss=False):\n    def iterate_dataset(data_loader):\n        data_iter = iter(data_loader)\n        while True:\n            try:\n                x, y = next(data_iter)\n            except StopIteration:\n                data_iter = iter(data_loader)\n                x, y = next(data_iter)\n            yield x.cuda(), y.cuda()\n    \n    criterion = nn.CrossEntropyLoss()\n\n    for _ in (pbar := trange(train_iterations)):\n        opt.zero_grad()\n        x, y = next(iterate_dataset(train_loader))\n        out, vq_out = model(x)\n        sce_loss = criterion(out, y)\n        cmt_loss = vq_out['loss']\n\n        if ignore_commitment_loss:\n            sce_loss.backward()\n        else:\n            (sce_loss + alpha * cmt_loss).backward()\n\n        acc = (out.argmax(dim=1) == y).float().mean()\n\n        opt.step()\n        pbar.set_description(f'sce loss: {sce_loss.item():.3f} | ' + \\\n                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n                             f'acc: {acc.item() * 100:.1f} | ' + \\\n                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n    return", "\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\n\nprint('baseline + kmeans init')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)", "model = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter, alpha=50)\n\n\nprint('+ inplace alt update')\ninplace_optimizer = lambda *args, **kwargs: torch.optim.SGD(*args, **kwargs, lr=50.0, momentum=0.9)\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, beta=1.0, inplace_optimizer=inplace_optimizer).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)", "model = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, beta=1.0, inplace_optimizer=inplace_optimizer).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter, ignore_commitment_loss=True)\n"]}
{"filename": "examples/classification.py", "chunked_list": ["# mnist VQ experiment with various settings.\nimport torch \nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torchvision import datasets\n\nfrom vqtorch.nn import VectorQuant\nfrom tqdm.auto import trange\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms", "from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n\nlr = 3e-4\ntrain_iter = 1000\nnum_codes = 256\nseed = 1234\n\nclass SimpleVQClassifier(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n                nn.AdaptiveMaxPool2d((1, 1)),\n                nn.Flatten(),\n                nn.Linear(64, 10),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x, vq_dict", "\nclass SimpleVQClassifier(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n                nn.AdaptiveMaxPool2d((1, 1)),\n                nn.Flatten(),\n                nn.Linear(64, 10),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x, vq_dict", "\n\ndef train(model, train_loader, train_iterations=1000, alpha=10):\n    def iterate_dataset(data_loader):\n        data_iter = iter(data_loader)\n        while True:\n            try:\n                x, y = next(data_iter)\n            except StopIteration:\n                data_iter = iter(data_loader)\n                x, y = next(data_iter)\n            yield x.cuda(), y.cuda()\n    \n    criterion = nn.CrossEntropyLoss()\n\n    for _ in (pbar := trange(train_iterations)):\n        opt.zero_grad()\n        x, y = next(iterate_dataset(train_loader))\n        out, vq_out = model(x)\n        sce_loss = criterion(out, y)\n        cmt_loss = vq_out['loss']\n        acc = (out.argmax(dim=1) == y).float().mean()\n        (sce_loss + alpha * cmt_loss).backward()\n\n        opt.step()\n        pbar.set_description(f'sce loss: {sce_loss.item():.3f} | ' + \\\n                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n                             f'acc: {acc.item() * 100:.1f} | ' + \\\n                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n    return", "\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\n\nprint('baseline')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)", "model = SimpleVQClassifier(num_codes=num_codes).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n\n\nprint('+ kmeans init')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)", "opt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n\n\nprint('+ synchronized update')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, sync_nu=1.0).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n", "train(model, train_dataset, train_iterations=train_iter)\n\n\nprint('+ affine parameterization')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, sync_nu=1.0, affine_lr=10).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)"]}
{"filename": "examples/autoencoder.py", "chunked_list": ["# mnist VQ experiment with various settings.\nimport torch \nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torchvision import datasets\n\nfrom vqtorch.nn import VectorQuant\nfrom tqdm.auto import trange\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms", "from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n\nlr = 3e-4\ntrain_iter = 1000\nnum_codes = 256\nseed = 1234\n\nclass SimpleVQAutoEncoder(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n                nn.GELU(),\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x.clamp(-1, 1), vq_dict", "\nclass SimpleVQAutoEncoder(nn.Module):\n    def __init__(self, **vq_kwargs):    \n        super().__init__()\n        self.layers = nn.ModuleList([\n                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                nn.GELU(),\n                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                VectorQuant(32, **vq_kwargs),\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n                nn.GELU(),\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n                ])\n        return\n    \n    def forward(self, x):\n        for layer in self.layers:\n            if isinstance(layer, VectorQuant):\n                x, vq_dict = layer(x)\n            else:\n                x = layer(x)\n        return x.clamp(-1, 1), vq_dict", "\n\ndef train(model, train_loader, train_iterations=1000, alpha=10):\n    def iterate_dataset(data_loader):\n        data_iter = iter(data_loader)\n        while True:\n            try:\n                x, y = next(data_iter)\n            except StopIteration:\n                data_iter = iter(data_loader)\n                x, y = next(data_iter)\n            yield x.cuda(), y.cuda()\n    \n    for _ in (pbar := trange(train_iterations)):\n        opt.zero_grad()\n        x, _ = next(iterate_dataset(train_loader))\n        out, vq_out = model(x)\n        rec_loss = (out - x).abs().mean()\n        cmt_loss = vq_out['loss']\n        (rec_loss + alpha * cmt_loss).backward()\n\n        opt.step()\n        pbar.set_description(f'rec loss: {rec_loss.item():.3f} | ' + \\\n                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n    return", "\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\nprint('baseline')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQAutoEncoder(num_codes=num_codes).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)", "opt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n\n\nprint('+ kmeans init')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n", "train(model, train_dataset, train_iterations=train_iter)\n\n\nprint('+ synchronized update')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True, sync_nu=2.0).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)\n\n", "\n\nprint('+ affine parameterization')\ntorch.random.manual_seed(seed)\nmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True, sync_nu=2.0, affine_lr=2.0).cuda()\nopt = torch.optim.AdamW(model.parameters(), lr=lr)\ntrain(model, train_dataset, train_iterations=train_iter)"]}
{"filename": "examples/test.py", "chunked_list": ["import torch\nfrom vqtorch.nn import VectorQuant, GroupVectorQuant, ResidualVectorQuant\n\n\nprint('Testing VectorQuant')\n# create VQ layer\nvq_layer = VectorQuant(\n                feature_size=32,     # feature dimension corresponding to the vectors\n                num_codes=1024,      # number of codebook vectors\n                beta=0.98,           # (default: 0.9) commitment trade-off", "                num_codes=1024,      # number of codebook vectors\n                beta=0.98,           # (default: 0.9) commitment trade-off\n                kmeans_init=True,    # (default: False) whether to use kmeans++ init\n                norm=None,           # (default: None) normalization for input vector\n                cb_norm=None,        # (default: None) normalization for codebook vectors\n                affine_lr=10.0,      # (default: 0.0) lr scale for affine parameters\n                sync_nu=0.2,         # (default: 0.0) codebook syncronization contribution\n                replace_freq=20,     # (default: None) frequency to replace dead codes\n                dim=-1,              # (default: -1) dimension to be quantized\n                ).cuda()", "                dim=-1,              # (default: -1) dimension to be quantized\n                ).cuda()\n\n# when using `kmeans_init`, we can warmup the codebook\nwith torch.no_grad():\n    z_e = torch.randn(128, 8, 8, 32).cuda()\n    vq_layer(z_e)\n\n# standard forward pass\nz_e = torch.randn(128, 8, 8, 32).cuda()", "# standard forward pass\nz_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above\nassert z_e.shape == z_q.shape\nerr = ((z_e - z_q) ** 2).mean().item()\nprint(f'>>> quantization error: {err:.3f}')\n\n\n\nprint('Testing GroupVectorQuant')", "\nprint('Testing GroupVectorQuant')\n# create VQ layer\nvq_layer = GroupVectorQuant(\n                feature_size=32,     \n                num_codes=1024,      \n                beta=0.98,    \n                kmeans_init=True,    \n                norm=None,         \n                cb_norm=None,        ", "                norm=None,         \n                cb_norm=None,        \n                affine_lr=10.0,   \n                sync_nu=0.2,         \n                replace_freq=20,     \n                dim=-1,             \n                groups=4,            # (default: 1) number of groups to divide the feature dimension\n                share=False,         # (default: True) when True, same codebook is used for each group\n                ).cuda()\n", "                ).cuda()\n\n# when using `kmeans_init`, we can warmup the codebook\nwith torch.no_grad():\n    z_e = torch.randn(128, 8, 8, 32).cuda()\n    vq_layer(z_e)\n\n# standard forward pass\nz_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above", "z_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above\nassert z_e.shape == z_q.shape\nerr = ((z_e - z_q) ** 2).mean().item()\nprint(f'>>> quantization error: {err:.3f}')\n\n\n\n\nprint('Testing ResidualVectorQuant')", "\nprint('Testing ResidualVectorQuant')\n# create VQ layer\nvq_layer = ResidualVectorQuant(\n                feature_size=32,     \n                num_codes=1024,      \n                beta=0.98,    \n                kmeans_init=True,    \n                norm=None,         \n                cb_norm=None,        ", "                norm=None,         \n                cb_norm=None,        \n                affine_lr=10.0,   \n                sync_nu=0.2,         \n                replace_freq=20,     \n                dim=-1,             \n                groups=4,            # (default: 1) number of groups to divide the feature dimension\n                share=True,          # (default: True) when True, same codebook is used for each group\n                ).cuda()\n", "                ).cuda()\n\n# when using `kmeans_init`, we can warmup the codebook\nwith torch.no_grad():\n    z_e = torch.randn(128, 8, 8, 32).cuda()\n    vq_layer(z_e)\n\n# standard forward pass\nz_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above", "z_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above\nassert z_e.shape == z_q.shape\nerr = ((z_e - z_q) ** 2).mean().item()\nprint(f'>>> quantization error: {err:.3f}')\n"]}
{"filename": "examples/experimental_group_affine.py", "chunked_list": ["import torch\nfrom vqtorch.nn import VectorQuant, GroupVectorQuant, ResidualVectorQuant\n\n\nprint('Testing VectorQuant')\n# create VQ layer\nvq_layer = VectorQuant(\n                feature_size=32,     # feature dimension corresponding to the vectors\n                num_codes=1024,      # number of codebook vectors\n                beta=0.98,           # (default: 0.9) commitment trade-off", "                num_codes=1024,      # number of codebook vectors\n                beta=0.98,           # (default: 0.9) commitment trade-off\n                kmeans_init=True,    # (default: False) whether to use kmeans++ init\n                norm=None,           # (default: None) normalization for input vector\n                cb_norm=None,        # (default: None) normalization for codebook vectors\n                affine_lr=10.0,      # (default: 0.0) lr scale for affine parameters\n                affine_groups=8,     # *** NEW *** (default: 1) number of affine parameter groups\n                sync_nu=0.2,         # (default: 0.0) codebook syncronization contribution\n                replace_freq=20,     # (default: None) frequency to replace dead codes\n                dim=-1,              # (default: -1) dimension to be quantized", "                replace_freq=20,     # (default: None) frequency to replace dead codes\n                dim=-1,              # (default: -1) dimension to be quantized\n                ).cuda()\n\n# when using `kmeans_init`, we can warmup the codebook\nwith torch.no_grad():\n    z_e = torch.randn(128, 8, 8, 32).cuda()\n    vq_layer(z_e)\n\n# standard forward pass", "\n# standard forward pass\nz_e = torch.randn(128, 8, 8, 32).cuda()\nz_q, vq_dict = vq_layer(z_e) # equivalent to above\nassert z_e.shape == z_q.shape\nerr = ((z_e - z_q) ** 2).mean().item()\nprint(f'>>> quantization error: {err:.3f}')"]}
