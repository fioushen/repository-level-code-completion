{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\nfrom setuptools import find_packages, setup\n\nif __name__ == '__main__':\n    setup(\n        name='cma',\n        version='1.0',\n        description='',\n        author='brdavid',\n        author_email='',\n        url='',\n        install_requires=['pytorch-lightning'],\n        packages=find_packages(),\n    )", ""]}
{"filename": "tools/run.py", "chunked_list": ["import sys\n\nimport pytorch_lightning as pl\nfrom helpers.pseudo_labels import generate_pseudo_labels\nfrom pytorch_lightning.cli import LightningCLI\n\n\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.add_optimizer_args(\n            nested_key=\"optimizer\", link_to=\"model.init_args.optimizer_init\")\n        parser.add_lr_scheduler_args(\n            nested_key=\"lr_scheduler\", link_to=\"model.init_args.lr_scheduler_init\")", "\n\ndef cli_main():\n\n    if sys.argv[1] == \"generate_pl\":\n        del sys.argv[1]\n        sys.argv.append('--data.init_args.generate_pseudo_labels')\n        sys.argv.append('True')\n        cli = MyLightningCLI(pl.LightningModule,\n                             pl.LightningDataModule,\n                             subclass_mode_model=True,\n                             subclass_mode_data=True,\n                             save_config_kwargs={'overwrite': True},\n                             seed_everything_default=2770466080,\n                             run=False)\n        generate_pseudo_labels(cli.model, cli.datamodule)\n    \n    else:\n        cli = MyLightningCLI(pl.LightningModule,\n                             pl.LightningDataModule,\n                             subclass_mode_model=True,\n                             subclass_mode_data=True,\n                             save_config_kwargs={'overwrite': True},\n                             seed_everything_default=2770466080)", "\n\nif __name__ == '__main__':\n    cli_main()\n"]}
{"filename": "data_modules/transforms.py", "chunked_list": ["import random\n\nimport torch\nimport torch.nn as nn\nimport torchvision\n\nIMNET_MEAN = (0.485, 0.456, 0.406)\nIMNET_STD = (0.229, 0.224, 0.225)\n\n\nclass ToTensor:\n    def __init__(self, apply_keys='all'):\n        self.apply_keys = apply_keys\n\n    def __call__(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        elif self.apply_keys == 'none':\n            apply_keys = []\n        else:\n            apply_keys = self.apply_keys\n\n        for key in apply_keys:\n            val = sample[key]\n            if key in ['image', 'image_ref']:\n                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n                    val)\n            elif key in ['semantic']:\n                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n                    val).squeeze(0)\n            elif key in ['filename']:\n                pass\n            else:\n                raise ValueError\n\n        return sample", "\n\nclass ToTensor:\n    def __init__(self, apply_keys='all'):\n        self.apply_keys = apply_keys\n\n    def __call__(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        elif self.apply_keys == 'none':\n            apply_keys = []\n        else:\n            apply_keys = self.apply_keys\n\n        for key in apply_keys:\n            val = sample[key]\n            if key in ['image', 'image_ref']:\n                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n                    val)\n            elif key in ['semantic']:\n                sample[key] = torchvision.transforms.functional.pil_to_tensor(\n                    val).squeeze(0)\n            elif key in ['filename']:\n                pass\n            else:\n                raise ValueError\n\n        return sample", "\n\nclass RandomCrop(nn.Module):\n    def __init__(self, apply_keys='all', size=None, ignore_index=255, cat_max_ratio=1.0):\n        super().__init__()\n        self.apply_keys = apply_keys\n        self.size = size\n        self.ignore_index = ignore_index\n        self.cat_max_ratio = cat_max_ratio\n\n    def forward(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        else:\n            apply_keys = self.apply_keys\n\n        for k in ['image', 'image_ref', 'semantic']:\n            if k in sample.keys():\n                h, w = sample[k].shape[-2:]\n        crop_params = self.get_params([h, w], self.size)\n        if self.cat_max_ratio < 1.:\n            # Repeat 10 times\n            for _ in range(10):\n                seg_tmp = self.crop(sample['semantic'], *crop_params)\n                labels, cnt = torch.unique(seg_tmp, return_counts=True)\n                cnt = cnt[labels != self.ignore_index]\n                if len(cnt) > 1 and cnt.max() / torch.sum(cnt).float() < self.cat_max_ratio:\n                    break\n                crop_params = self.get_params([h, w], self.size)\n        for key in apply_keys:\n            val = sample[key]\n            if key in ['image',\n                       'image_ref',\n                       'semantic']:\n                sample[key] = self.crop(val, *crop_params)\n            elif key in ['filename']:\n                pass\n            else:\n                raise ValueError('unknown key: {}'.format(key))\n        return sample\n\n    @staticmethod\n    def get_params(img_size, output_size):\n        \"\"\"Get parameters for ``crop`` for a random crop.\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        \"\"\"\n        h, w = img_size\n        th, tw = output_size\n\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, max(h - th, 0))\n        j = random.randint(0, max(w - tw, 0))\n        return i, j, min(th, h), min(tw, w)\n\n    def crop(self, img, top, left, height, width):\n        h, w = img.shape[-2:]\n        right = left + width\n        bottom = top + height\n\n        if left < 0 or top < 0 or right > w or bottom > h:\n            raise ValueError(\"Invalid crop parameters: {}, img size: {}\".format(\n                (top, left, height, width), (h, w)))\n        return img[..., top:bottom, left:right]", "\n\nclass RandomHorizontalFlip(nn.Module):\n    def __init__(self, apply_keys='all', p=0.5):\n        super().__init__()\n        self.apply_keys = apply_keys\n        self.p = p\n\n    def forward(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        else:\n            apply_keys = self.apply_keys\n\n        if random.random() < self.p:\n            for key in apply_keys:\n                val = sample[key]\n                if key in ['image',\n                           'image_ref',\n                           'semantic']:\n                    sample[key] = torchvision.transforms.functional.hflip(val)\n                elif key in ['filename']:\n                    pass\n                else:\n                    raise ValueError\n\n        return sample", "\n\nclass ConvertImageDtype(torchvision.transforms.ConvertImageDtype):\n    def __init__(self, apply_keys='all', **kwargs):\n        dtype = kwargs.pop('dtype', torch.float)\n        super().__init__(dtype=dtype, **kwargs)\n        self.apply_keys = apply_keys\n\n    def forward(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        else:\n            apply_keys = self.apply_keys\n        for key in apply_keys:\n            val = sample[key]\n            if key in ['image', 'image_ref']:\n                sample[key] = super().forward(val)\n            elif key in ['semantic']:\n                sample[key] = val.to(torch.long)  # from byte to long\n            elif key in ['filename']:\n                pass\n            else:\n                raise ValueError\n        return sample", "\n\nclass Normalize(torchvision.transforms.Normalize):\n    def __init__(self, apply_keys='all', **kwargs):\n        # set imagenet statistics as default\n        mean = kwargs.pop('mean', IMNET_MEAN)\n        std = kwargs.pop('std', IMNET_STD)\n        super().__init__(mean=mean, std=std, **kwargs)\n        self.apply_keys = apply_keys\n\n    def forward(self, sample):\n        if self.apply_keys == 'all':\n            apply_keys = list(sample)\n        else:\n            apply_keys = self.apply_keys\n\n        for key in apply_keys:\n            val = sample[key]\n            if key in ['image', 'image_ref']:\n                sample[key] = super().forward(val)\n            elif key in ['semantic', 'filename']:\n                pass\n            else:\n                raise ValueError\n        return sample", ""]}
{"filename": "data_modules/combined_data_module.py", "chunked_list": ["import os\nfrom itertools import chain\nfrom operator import itemgetter\nfrom typing import Optional\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom pytorch_lightning.cli import instantiate_class\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose", "from torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose\n\nfrom . import transforms as transform_lib\nfrom .datasets import *\n\nDATA_DIR = os.environ['DATA_DIR']\n\n\nclass CombinedDataModule(LightningDataModule):\n\n    def __init__(\n        self,\n        load_config: dict,\n        num_workers: int = 0,\n        batch_size: int = 8,\n        batch_size_divisor: int = 1,\n        generate_pseudo_labels: bool = False,\n        pin_memory: bool = True,\n    ) -> None:\n        super().__init__()\n        self.data_dirs = {\n            'ACDC': os.path.join(DATA_DIR, 'ACDC'),\n            'DarkZurich': os.path.join(DATA_DIR, 'DarkZurich'),\n            'RobotCar': os.path.join(DATA_DIR, 'RobotCar'),\n            'ACG': DATA_DIR,\n        }\n        self.num_workers = num_workers\n        assert batch_size % batch_size_divisor == 0\n        self.batch_size_divisor = batch_size_divisor\n        self.batch_size = batch_size // batch_size_divisor\n        self.pin_memory = pin_memory\n        self.generate_pseudo_labels = generate_pseudo_labels\n\n        self.train_on = []\n        self.train_config = []\n        self.val_on = []\n        self.val_config = []\n        self.test_on = []\n        self.test_config = []\n        self.predict_on = []\n        self.predict_config = []\n\n        # parse load_config\n        if 'train' in load_config:\n            for ds, conf in load_config['train'].items():\n                if isinstance(conf, dict):\n                    self.train_on.append(ds)\n                    self.train_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.train_on.append(ds)\n                        self.train_config.append(el)\n\n        if 'val' in load_config:\n            for ds, conf in load_config['val'].items():\n                if isinstance(conf, dict):\n                    self.val_on.append(ds)\n                    self.val_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.val_on.append(ds)\n                        self.val_config.append(el)\n\n        if 'test' in load_config:\n            for ds, conf in load_config['test'].items():\n                if isinstance(conf, dict):\n                    self.test_on.append(ds)\n                    self.test_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.test_on.append(ds)\n                        self.test_config.append(el)\n\n        if 'predict' in load_config:\n            for ds, conf in load_config['predict'].items():\n                if isinstance(conf, dict):\n                    if self.generate_pseudo_labels:\n                        conf['predict_on'] = 'train'\n                    self.predict_on.append(ds)\n                    self.predict_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        if self.generate_pseudo_labels:\n                            el['predict_on'] = 'train'\n                        self.predict_on.append(ds)\n                        self.predict_config.append(el)\n\n        self.idx_to_name = {'train': {}, 'val': {}, 'test': {}, 'predict': {}}\n        for idx, ds in enumerate(self.train_on):\n            self.idx_to_name['train'][idx] = ds\n        for idx, ds in enumerate(self.val_on):\n            self.idx_to_name['val'][idx] = ds\n        for idx, ds in enumerate(self.test_on):\n            self.idx_to_name['test'][idx] = ds\n        for idx, ds in enumerate(self.predict_on):\n            self.idx_to_name['predict'][idx] = ds\n\n        if len(self.train_on) > 0:\n            assert self.batch_size % len(\n                self.train_on) == 0, 'batch size should be divisible by number of train datasets'\n\n        # handle transformations\n        for idx, (ds, cfg) in enumerate(zip(self.train_on, self.train_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.train_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.train_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.val_on, self.val_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.val_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.val_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.test_on, self.test_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.test_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.test_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.predict_on, self.predict_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.predict_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.predict_config[idx]['transforms'] = transform_lib.ToTensor(\n                )\n\n        self.val_batch_size = 1\n        self.test_batch_size = 1\n\n    def setup(self, stage: Optional[str] = None):\n        if stage in (None, \"fit\"):\n            self.train_ds = []\n            for ds, cfg in zip(self.train_on, self.train_config):\n                self.train_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"train\",\n                    **cfg\n                ))\n\n        if stage in (None, \"fit\", \"validate\"):\n            self.val_ds = []\n            for ds, cfg in zip(self.val_on, self.val_config):\n                self.val_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"val\",\n                    **cfg\n                ))\n\n        if stage in (None, \"test\"):\n            self.test_ds = []\n            for ds, cfg in zip(self.test_on, self.test_config):\n                self.test_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"test\",\n                    **cfg\n                ))\n\n        if stage in (None, \"predict\"):\n            self.predict_ds = []\n            for ds, cfg in zip(self.predict_on, self.predict_config):\n                self.predict_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"predict\",\n                    **cfg\n                ))\n\n    def train_dataloader(self):\n        loader_list = []\n        for ds in self.train_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.batch_size // len(self.train_on),\n                shuffle=True,\n                num_workers=self.num_workers,\n                drop_last=True,\n                pin_memory=self.pin_memory,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def val_dataloader(self):\n        loader_list = []\n        for ds in self.val_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.val_batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def test_dataloader(self):\n        loader_list = []\n        for ds in self.test_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.test_batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def predict_dataloader(self, shuffle=False):\n        loader_list = []\n        for ds in self.predict_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.test_batch_size,\n                shuffle=shuffle,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def on_before_batch_transfer(self, batch, dataloader_idx):\n        if self.trainer.training:\n            tmp_batch = {k: list(map(itemgetter(k), batch)) for k in set(chain.from_iterable(batch))}\n            return {k: torch.cat(v, dim=0) if k != 'filename' else [\n                item for sublist in v for item in sublist] for k, v in tmp_batch.items()}\n        else:\n            return batch", "\nclass CombinedDataModule(LightningDataModule):\n\n    def __init__(\n        self,\n        load_config: dict,\n        num_workers: int = 0,\n        batch_size: int = 8,\n        batch_size_divisor: int = 1,\n        generate_pseudo_labels: bool = False,\n        pin_memory: bool = True,\n    ) -> None:\n        super().__init__()\n        self.data_dirs = {\n            'ACDC': os.path.join(DATA_DIR, 'ACDC'),\n            'DarkZurich': os.path.join(DATA_DIR, 'DarkZurich'),\n            'RobotCar': os.path.join(DATA_DIR, 'RobotCar'),\n            'ACG': DATA_DIR,\n        }\n        self.num_workers = num_workers\n        assert batch_size % batch_size_divisor == 0\n        self.batch_size_divisor = batch_size_divisor\n        self.batch_size = batch_size // batch_size_divisor\n        self.pin_memory = pin_memory\n        self.generate_pseudo_labels = generate_pseudo_labels\n\n        self.train_on = []\n        self.train_config = []\n        self.val_on = []\n        self.val_config = []\n        self.test_on = []\n        self.test_config = []\n        self.predict_on = []\n        self.predict_config = []\n\n        # parse load_config\n        if 'train' in load_config:\n            for ds, conf in load_config['train'].items():\n                if isinstance(conf, dict):\n                    self.train_on.append(ds)\n                    self.train_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.train_on.append(ds)\n                        self.train_config.append(el)\n\n        if 'val' in load_config:\n            for ds, conf in load_config['val'].items():\n                if isinstance(conf, dict):\n                    self.val_on.append(ds)\n                    self.val_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.val_on.append(ds)\n                        self.val_config.append(el)\n\n        if 'test' in load_config:\n            for ds, conf in load_config['test'].items():\n                if isinstance(conf, dict):\n                    self.test_on.append(ds)\n                    self.test_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        self.test_on.append(ds)\n                        self.test_config.append(el)\n\n        if 'predict' in load_config:\n            for ds, conf in load_config['predict'].items():\n                if isinstance(conf, dict):\n                    if self.generate_pseudo_labels:\n                        conf['predict_on'] = 'train'\n                    self.predict_on.append(ds)\n                    self.predict_config.append(conf)\n                elif isinstance(conf, list):\n                    for el in conf:\n                        if self.generate_pseudo_labels:\n                            el['predict_on'] = 'train'\n                        self.predict_on.append(ds)\n                        self.predict_config.append(el)\n\n        self.idx_to_name = {'train': {}, 'val': {}, 'test': {}, 'predict': {}}\n        for idx, ds in enumerate(self.train_on):\n            self.idx_to_name['train'][idx] = ds\n        for idx, ds in enumerate(self.val_on):\n            self.idx_to_name['val'][idx] = ds\n        for idx, ds in enumerate(self.test_on):\n            self.idx_to_name['test'][idx] = ds\n        for idx, ds in enumerate(self.predict_on):\n            self.idx_to_name['predict'][idx] = ds\n\n        if len(self.train_on) > 0:\n            assert self.batch_size % len(\n                self.train_on) == 0, 'batch size should be divisible by number of train datasets'\n\n        # handle transformations\n        for idx, (ds, cfg) in enumerate(zip(self.train_on, self.train_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.train_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.train_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.val_on, self.val_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.val_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.val_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.test_on, self.test_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.test_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.test_config[idx]['transforms'] = transform_lib.ToTensor()\n        for idx, (ds, cfg) in enumerate(zip(self.predict_on, self.predict_config)):\n            trafos = cfg.pop('transforms', None)\n            if trafos:\n                self.predict_config[idx]['transforms'] = Compose(\n                    [instantiate_class(tuple(), t) for t in trafos])\n            else:\n                self.predict_config[idx]['transforms'] = transform_lib.ToTensor(\n                )\n\n        self.val_batch_size = 1\n        self.test_batch_size = 1\n\n    def setup(self, stage: Optional[str] = None):\n        if stage in (None, \"fit\"):\n            self.train_ds = []\n            for ds, cfg in zip(self.train_on, self.train_config):\n                self.train_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"train\",\n                    **cfg\n                ))\n\n        if stage in (None, \"fit\", \"validate\"):\n            self.val_ds = []\n            for ds, cfg in zip(self.val_on, self.val_config):\n                self.val_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"val\",\n                    **cfg\n                ))\n\n        if stage in (None, \"test\"):\n            self.test_ds = []\n            for ds, cfg in zip(self.test_on, self.test_config):\n                self.test_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"test\",\n                    **cfg\n                ))\n\n        if stage in (None, \"predict\"):\n            self.predict_ds = []\n            for ds, cfg in zip(self.predict_on, self.predict_config):\n                self.predict_ds.append(globals()[ds](\n                    self.data_dirs[ds],\n                    stage=\"predict\",\n                    **cfg\n                ))\n\n    def train_dataloader(self):\n        loader_list = []\n        for ds in self.train_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.batch_size // len(self.train_on),\n                shuffle=True,\n                num_workers=self.num_workers,\n                drop_last=True,\n                pin_memory=self.pin_memory,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def val_dataloader(self):\n        loader_list = []\n        for ds in self.val_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.val_batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def test_dataloader(self):\n        loader_list = []\n        for ds in self.test_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.test_batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def predict_dataloader(self, shuffle=False):\n        loader_list = []\n        for ds in self.predict_ds:\n            loader = DataLoader(\n                dataset=ds,\n                batch_size=self.test_batch_size,\n                shuffle=shuffle,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                drop_last=False,\n            )\n            loader_list.append(loader)\n        return loader_list\n\n    def on_before_batch_transfer(self, batch, dataloader_idx):\n        if self.trainer.training:\n            tmp_batch = {k: list(map(itemgetter(k), batch)) for k in set(chain.from_iterable(batch))}\n            return {k: torch.cat(v, dim=0) if k != 'filename' else [\n                item for sublist in v for item in sublist] for k, v in tmp_batch.items()}\n        else:\n            return batch", ""]}
{"filename": "data_modules/__init__.py", "chunked_list": ["from .combined_data_module import CombinedDataModule\n"]}
{"filename": "data_modules/datasets/darkzurich.py", "chunked_list": ["import io\nimport os\nimport zipfile\nfrom typing import Any, Callable, List, Optional, Union\n\nimport requests\nimport torch\nfrom PIL import Image\n\nURLS = {", "\nURLS = {\n    \"pseudo_labels_train_DarkZurich_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_DarkZurich_cma_segformer.zip\",\n}\n\n\nclass DarkZurich(torch.utils.data.Dataset):\n\n    orig_dims = (1080, 1920)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"train\",\n            load_keys: Union[List[str], str] = [\"image_ref\", \"image\"],\n            transforms: Optional[Callable] = None,\n            predict_on: str = \"test\",\n            load_pseudo_labels: bool = False,\n            pseudo_label_dir: Optional[str] = None,\n            **kwargs\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.transforms = transforms\n        self.load_pseudo_labels = load_pseudo_labels\n        if self.load_pseudo_labels:\n            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n            os.makedirs(_dpath, exist_ok=True)\n            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n                print('Downloading and extracting pseudo-labels...')\n                r = requests.get(URLS[pseudo_label_dir])\n                z = zipfile.ZipFile(io.BytesIO(r.content))\n                z.extractall(_dpath)\n            self.pseudo_label_dir = os.path.join(\n                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\n        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n        self.stage = stage\n\n        # mapping from stage to splits\n        if self.stage == 'train':\n            self.split = 'train'\n        elif self.stage == 'val':\n            self.split = 'val'\n        elif self.stage == 'test':\n            self.split = 'val'  # test on val split\n        elif self.stage == 'predict':\n            self.split = predict_on\n\n        if isinstance(load_keys, str):\n            self.load_keys = [load_keys]\n        else:\n            self.load_keys = load_keys\n\n        self.paths = {k: []\n                      for k in ['image', 'image_ref', 'semantic']}\n\n        self.images_dir = os.path.join(self.root, 'rgb_anon')\n        self.semantic_dir = os.path.join(self.root, 'gt')\n        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.semantic_dir):\n            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n                               ' specified \"split\" and \"condition\" are inside the \"root\" directory')\n\n        if self.split == 'train':\n            img_ids = [i_id.strip() for i_id in open(os.path.join(\n                os.path.dirname(__file__), 'lists/zurich_dn_pair_train.csv'))]\n            for pair in img_ids:\n                night, day = pair.split(\",\")\n                for k in ['image', 'image_ref']:\n                    if k == 'image':\n                        file_path = os.path.join(\n                            self.root, 'rgb_anon', night + \"_rgb_anon.png\")\n                    elif k == 'image_ref':\n                        file_path = os.path.join(\n                            self.root, 'rgb_anon', day + \"_rgb_anon.png\")\n                    self.paths[k].append(file_path)\n\n        else:\n            img_parent_dir = os.path.join(self.images_dir, self.split, 'night')\n            semantic_parent_dir = os.path.join(\n                self.semantic_dir, self.split, 'night')\n            for recording in os.listdir(img_parent_dir):\n                img_dir = os.path.join(img_parent_dir, recording)\n                semantic_dir = os.path.join(semantic_parent_dir, recording)\n                for file_name in os.listdir(img_dir):\n                    for k in ['image', 'image_ref', 'semantic']:\n                        if k == 'image':\n                            file_path = os.path.join(img_dir, file_name)\n                        elif k == 'image_ref':\n                            if self.split == 'val':\n                                ref_img_dir = img_dir.replace(self.split, self.split + '_ref').replace(\n                                    'night', 'day').replace(recording, recording + '_ref')\n                                ref_file_name = file_name.replace(\n                                    'rgb_anon.png', 'ref_rgb_anon.png')\n                                file_path = os.path.join(\n                                    ref_img_dir, ref_file_name)\n                            elif self.split == 'test':\n                                ref_img_dir = img_dir.replace(self.split, self.split + '_ref').replace(\n                                    'night', 'day').replace(recording, recording + '_ref')\n                                ref_file_name_start = file_name.split('rgb_anon.png')[\n                                    0]\n                                for f in os.listdir(ref_img_dir):\n                                    if f.startswith(ref_file_name_start):\n                                        ref_file_name = f\n                                        break\n                                file_path = os.path.join(\n                                    ref_img_dir, ref_file_name)\n                        elif k == 'semantic':\n                            semantic_file_name = file_name.replace(\n                                'rgb_anon.png', 'gt_labelTrainIds.png')\n                            file_path = os.path.join(\n                                semantic_dir, semantic_file_name)\n                        self.paths[k].append(file_path)\n\n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n\n        filename = self.paths['image'][index].split('/')[-1]\n        sample['filename'] = filename\n\n        for k in self.load_keys:\n            if k in ['image', 'image_ref']:\n                data = Image.open(self.paths[k][index]).convert('RGB')\n            elif k == 'semantic':\n                if self.load_pseudo_labels:\n                    path = os.path.join(self.pseudo_label_dir, filename)\n                    data = Image.open(path)\n                else:\n                    data = Image.open(self.paths[k][index])\n            else:\n                raise ValueError('invalid load_key')\n            sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(next(iter(self.paths.values())))", ""]}
{"filename": "data_modules/datasets/robotcar.py", "chunked_list": ["import io\nimport os\nimport zipfile\nfrom typing import Any, Callable, List, Optional, Union\n\nimport h5py\nimport numpy as np\nimport requests\nimport torch\nfrom PIL import Image", "import torch\nfrom PIL import Image\n\nURLS = {\n    \"pseudo_labels_train_RobotCar_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_RobotCar_cma_segformer.zip\",\n}\n\n\nclass RobotCar(torch.utils.data.Dataset):\n\n    ignore_index = 255\n    id_to_trainid = {-1: ignore_index, 0: ignore_index, 1: ignore_index, 2: ignore_index,\n                     3: ignore_index, 4: ignore_index, 5: ignore_index, 6: ignore_index,\n                     7: 0, 8: 1, 9: ignore_index, 10: ignore_index, 11: 2, 12: 3, 13: 4,\n                     14: ignore_index, 15: ignore_index, 16: ignore_index, 17: 5,\n                     18: ignore_index, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n                     28: 15, 29: ignore_index, 30: ignore_index, 31: 16, 32: 17, 33: 18}\n    orig_dims = (1024, 1024)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"train\",\n            load_keys: Union[List[str], str] = [\n                \"image_ref\", \"image\", \"semantic\"],\n            transforms: Optional[Callable] = None,\n            predict_on: str = \"test\",\n            load_pseudo_labels: bool = False,\n            pseudo_label_dir: Optional[str] = None,\n            **kwargs\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.transforms = transforms\n        self.load_pseudo_labels = load_pseudo_labels\n        if self.load_pseudo_labels:\n            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n            os.makedirs(_dpath, exist_ok=True)\n            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n                print('Downloading and extracting pseudo-labels...')\n                r = requests.get(URLS[pseudo_label_dir])\n                z = zipfile.ZipFile(io.BytesIO(r.content))\n                z.extractall(_dpath)\n            self.pseudo_label_dir = os.path.join(\n                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\n        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n        self.stage = stage\n\n        # mapping from stage to splits\n        if self.stage == 'train':\n            self.split = 'train'\n        elif self.stage == 'val':\n            self.split = 'val'\n        elif self.stage == 'test':\n            self.split = 'test'\n        elif self.stage == 'predict':\n            self.split = predict_on\n\n        if isinstance(load_keys, str):\n            self.load_keys = [load_keys]\n        else:\n            self.load_keys = load_keys\n\n        if not os.path.isdir(self.root):\n            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n                               ' specified \"split\" are inside the \"root\" directory')\n\n        if self.split == 'train':\n            self.images_dir = os.path.join(self.root, 'images')\n            corr_dir = os.path.join(self.root, 'correspondence_data')\n            self.paths = {'corr_files': []}\n            f_name_list = [fn for fn in os.listdir(\n                corr_dir) if fn.endswith('mat')]\n            for f_name in f_name_list:\n                self.paths['corr_files'].append(\n                    os.path.join(corr_dir, f_name))\n        else:\n            assert not \"image_ref\" in self.load_keys\n            self.paths = {k: [] for k in self.load_keys}\n            splitdir_map = {'val': 'validation', 'test': 'testing'}\n            images_dir = os.path.join(\n                self.root, 'segmented_images', splitdir_map[self.split], 'imgs')\n            semantic_dir = os.path.join(\n                self.root, 'segmented_images', splitdir_map[self.split], 'annos')\n            for img_name in os.listdir(images_dir):\n                for k in self.load_keys:\n                    if k == 'image':\n                        file_path = os.path.join(images_dir, img_name)\n                    elif k == 'semantic':\n                        file_path = os.path.join(semantic_dir, img_name)\n                    self.paths[k].append(file_path)\n\n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n\n        if self.split == 'train':\n            # Load correspondence data\n            mat_content = {}\n            f = h5py.File(self.paths['corr_files'][index], 'r')\n            for k, v in f.items():\n                mat_content[k] = np.array(v)\n\n            im1name = ''.join(chr(a[0])\n                              for a in mat_content['im_i_path'])  # convert to string\n            im2name = ''.join(chr(a[0])\n                              for a in mat_content['im_j_path'])  # convert to string\n\n            filename = im2name.split('/')[-1]\n            sample['filename'] = filename\n\n            for k in self.load_keys:\n                if k == 'image_ref':\n                    data = Image.open(os.path.join(\n                        self.images_dir, im1name)).convert('RGB')\n                elif k == 'image':\n                    data = Image.open(os.path.join(\n                        self.images_dir, im2name)).convert('RGB')\n                elif k == 'semantic':\n                    assert self.load_pseudo_labels\n                    name = '.'.join([*filename.split('.')[:-1], 'png'])\n                    path = os.path.join(self.pseudo_label_dir, name)\n                    data = Image.open(path)\n                sample[k] = data\n\n        else:\n            sample['filename'] = self.paths['image'][index].split('/')[-1]\n            for k in self.load_keys:\n                if k == 'image':\n                    data = Image.open(self.paths[k][index]).convert('RGB')\n                elif k == 'semantic':\n                    data = Image.open(self.paths[k][index])\n                    data = self.encode_semantic_map(data)\n                sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(next(iter(self.paths.values())))\n\n    @classmethod\n    def encode_semantic_map(cls, semseg):\n        semseg_arr = np.array(semseg)\n        semseg_arr_copy = semseg_arr.copy()\n        for k, v in cls.id_to_trainid.items():\n            semseg_arr_copy[semseg_arr == k] = v\n        return Image.fromarray(semseg_arr_copy.astype(np.uint8))", "class RobotCar(torch.utils.data.Dataset):\n\n    ignore_index = 255\n    id_to_trainid = {-1: ignore_index, 0: ignore_index, 1: ignore_index, 2: ignore_index,\n                     3: ignore_index, 4: ignore_index, 5: ignore_index, 6: ignore_index,\n                     7: 0, 8: 1, 9: ignore_index, 10: ignore_index, 11: 2, 12: 3, 13: 4,\n                     14: ignore_index, 15: ignore_index, 16: ignore_index, 17: 5,\n                     18: ignore_index, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n                     28: 15, 29: ignore_index, 30: ignore_index, 31: 16, 32: 17, 33: 18}\n    orig_dims = (1024, 1024)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"train\",\n            load_keys: Union[List[str], str] = [\n                \"image_ref\", \"image\", \"semantic\"],\n            transforms: Optional[Callable] = None,\n            predict_on: str = \"test\",\n            load_pseudo_labels: bool = False,\n            pseudo_label_dir: Optional[str] = None,\n            **kwargs\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.transforms = transforms\n        self.load_pseudo_labels = load_pseudo_labels\n        if self.load_pseudo_labels:\n            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n            os.makedirs(_dpath, exist_ok=True)\n            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n                print('Downloading and extracting pseudo-labels...')\n                r = requests.get(URLS[pseudo_label_dir])\n                z = zipfile.ZipFile(io.BytesIO(r.content))\n                z.extractall(_dpath)\n            self.pseudo_label_dir = os.path.join(\n                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\n        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n        self.stage = stage\n\n        # mapping from stage to splits\n        if self.stage == 'train':\n            self.split = 'train'\n        elif self.stage == 'val':\n            self.split = 'val'\n        elif self.stage == 'test':\n            self.split = 'test'\n        elif self.stage == 'predict':\n            self.split = predict_on\n\n        if isinstance(load_keys, str):\n            self.load_keys = [load_keys]\n        else:\n            self.load_keys = load_keys\n\n        if not os.path.isdir(self.root):\n            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n                               ' specified \"split\" are inside the \"root\" directory')\n\n        if self.split == 'train':\n            self.images_dir = os.path.join(self.root, 'images')\n            corr_dir = os.path.join(self.root, 'correspondence_data')\n            self.paths = {'corr_files': []}\n            f_name_list = [fn for fn in os.listdir(\n                corr_dir) if fn.endswith('mat')]\n            for f_name in f_name_list:\n                self.paths['corr_files'].append(\n                    os.path.join(corr_dir, f_name))\n        else:\n            assert not \"image_ref\" in self.load_keys\n            self.paths = {k: [] for k in self.load_keys}\n            splitdir_map = {'val': 'validation', 'test': 'testing'}\n            images_dir = os.path.join(\n                self.root, 'segmented_images', splitdir_map[self.split], 'imgs')\n            semantic_dir = os.path.join(\n                self.root, 'segmented_images', splitdir_map[self.split], 'annos')\n            for img_name in os.listdir(images_dir):\n                for k in self.load_keys:\n                    if k == 'image':\n                        file_path = os.path.join(images_dir, img_name)\n                    elif k == 'semantic':\n                        file_path = os.path.join(semantic_dir, img_name)\n                    self.paths[k].append(file_path)\n\n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n\n        if self.split == 'train':\n            # Load correspondence data\n            mat_content = {}\n            f = h5py.File(self.paths['corr_files'][index], 'r')\n            for k, v in f.items():\n                mat_content[k] = np.array(v)\n\n            im1name = ''.join(chr(a[0])\n                              for a in mat_content['im_i_path'])  # convert to string\n            im2name = ''.join(chr(a[0])\n                              for a in mat_content['im_j_path'])  # convert to string\n\n            filename = im2name.split('/')[-1]\n            sample['filename'] = filename\n\n            for k in self.load_keys:\n                if k == 'image_ref':\n                    data = Image.open(os.path.join(\n                        self.images_dir, im1name)).convert('RGB')\n                elif k == 'image':\n                    data = Image.open(os.path.join(\n                        self.images_dir, im2name)).convert('RGB')\n                elif k == 'semantic':\n                    assert self.load_pseudo_labels\n                    name = '.'.join([*filename.split('.')[:-1], 'png'])\n                    path = os.path.join(self.pseudo_label_dir, name)\n                    data = Image.open(path)\n                sample[k] = data\n\n        else:\n            sample['filename'] = self.paths['image'][index].split('/')[-1]\n            for k in self.load_keys:\n                if k == 'image':\n                    data = Image.open(self.paths[k][index]).convert('RGB')\n                elif k == 'semantic':\n                    data = Image.open(self.paths[k][index])\n                    data = self.encode_semantic_map(data)\n                sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(next(iter(self.paths.values())))\n\n    @classmethod\n    def encode_semantic_map(cls, semseg):\n        semseg_arr = np.array(semseg)\n        semseg_arr_copy = semseg_arr.copy()\n        for k, v in cls.id_to_trainid.items():\n            semseg_arr_copy[semseg_arr == k] = v\n        return Image.fromarray(semseg_arr_copy.astype(np.uint8))", ""]}
{"filename": "data_modules/datasets/__init__.py", "chunked_list": ["from .acdc import ACDC\nfrom .acg import ACG\nfrom .darkzurich import DarkZurich\nfrom .robotcar import RobotCar\n"]}
{"filename": "data_modules/datasets/acg.py", "chunked_list": ["import json\nimport os\nfrom collections import namedtuple\nfrom typing import Any, Callable, List, Optional, Union\n\nimport numpy as np\nimport torch\nfrom PIL import Image\n\n\nclass ACG(torch.utils.data.Dataset):\n    \"\"\"ACG benchmark from the paper:\n\n        Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation\n\n    \"\"\"\n\n    WildDash2Class = namedtuple(\"WildDash2Class\", [\"name\", \"id\", \"train_id\"])\n    labels = [\n        #       name                             id    trainId  \n        WildDash2Class(  'unlabeled'            ,  0 ,      255  ),\n        WildDash2Class(  'ego vehicle'          ,  1 ,      255  ),\n        WildDash2Class(  'overlay'              ,  2 ,      255  ),\n        WildDash2Class(  'out of roi'           ,  3 ,      255  ),\n        WildDash2Class(  'static'               ,  4 ,      255  ),\n        WildDash2Class(  'dynamic'              ,  5 ,      255  ),\n        WildDash2Class(  'ground'               ,  6 ,      255  ),\n        WildDash2Class(  'road'                 ,  7 ,        0  ),\n        WildDash2Class(  'sidewalk'             ,  8 ,        1  ),\n        WildDash2Class(  'parking'              ,  9 ,      255  ),\n        WildDash2Class(  'rail track'           , 10 ,      255  ),\n        WildDash2Class(  'building'             , 11 ,        2  ),\n        WildDash2Class(  'wall'                 , 12 ,        3  ),\n        WildDash2Class(  'fence'                , 13 ,        4  ),\n        WildDash2Class(  'guard rail'           , 14 ,      255  ),\n        WildDash2Class(  'bridge'               , 15 ,      255  ),\n        WildDash2Class(  'tunnel'               , 16 ,      255  ),\n        WildDash2Class(  'pole'                 , 17 ,        5  ),\n        WildDash2Class(  'polegroup'            , 18 ,      255  ),\n        WildDash2Class(  'traffic light'        , 19 ,        6  ),\n        WildDash2Class(  'traffic sign front'   , 20 ,        7  ),\n        WildDash2Class(  'vegetation'           , 21 ,        8  ),\n        WildDash2Class(  'terrain'              , 22 ,        9  ),\n        WildDash2Class(  'sky'                  , 23 ,       10  ),\n        WildDash2Class(  'person'               , 24 ,       11  ),\n        WildDash2Class(  'rider'                , 25 ,       12  ),\n        WildDash2Class(  'car'                  , 26 ,       13  ),\n        WildDash2Class(  'truck'                , 27 ,       14  ),\n        WildDash2Class(  'bus'                  , 28 ,       15  ),\n        WildDash2Class(  'caravan'              , 29 ,      255  ),\n        WildDash2Class(  'trailer'              , 30 ,      255  ),\n        WildDash2Class(  'on rails'             , 31 ,       16  ),\n        WildDash2Class(  'motorcycle'           , 32 ,       17  ),\n        WildDash2Class(  'bicycle'              , 33 ,       18  ),\n        WildDash2Class(  'pickup'               , 34 ,       14  ),\n        WildDash2Class(  'van'                  , 35 ,       13  ),\n        WildDash2Class(  'billboard'            , 36 ,      255  ),\n        WildDash2Class(  'street light'         , 37 ,      255  ),\n        WildDash2Class(  'road marking'         , 38 ,        0  ),\n        WildDash2Class(  'junctionbox'          , 39 ,      255  ),\n        WildDash2Class(  'mailbox'              , 40 ,      255  ),\n        WildDash2Class(  'manhole'              , 41 ,        0  ),\n        WildDash2Class(  'phonebooth'           , 42 ,      255  ),\n        WildDash2Class(  'pothole'              , 43 ,        0  ),\n        WildDash2Class(  'bikerack'             , 44 ,      255  ),\n        WildDash2Class(  'traffic sign frame'   , 45 ,        5  ),\n        WildDash2Class(  'utility pole'         , 46 ,        5  ),\n        WildDash2Class(  'motorcyclist'         , 47 ,       12  ),\n        WildDash2Class(  'bicyclist'            , 48 ,       12  ),\n        WildDash2Class(  'other rider'          , 49 ,       12  ),\n        WildDash2Class(  'bird'                 , 50 ,      255  ),\n        WildDash2Class(  'ground animal'        , 51 ,      255  ),\n        WildDash2Class(  'curb'                 , 52 ,        1  ),\n        WildDash2Class(  'traffic sign any'     , 53 ,      255  ),\n        WildDash2Class(  'traffic sign back'    , 54 ,      255  ),\n        WildDash2Class(  'trashcan'             , 55 ,      255  ),\n        WildDash2Class(  'other barrier'        , 56 ,        3  ),\n        WildDash2Class(  'other vehicle'        , 57 ,      255  ),\n        WildDash2Class(  'auto rickshaw'        , 58 ,       17  ),\n        WildDash2Class(  'bench'                , 59 ,      255  ),\n        WildDash2Class(  'mountain'             , 60 ,      255  ),\n        WildDash2Class(  'tram track'           , 61 ,        0  ),\n        WildDash2Class(  'wheeled slow'         , 62 ,      255  ),\n        WildDash2Class(  'boat'                 , 63 ,      255  ),\n        WildDash2Class(  'bikelane'             , 64 ,        0  ),\n        WildDash2Class(  'bikelane sidewalk'    , 65 ,        1  ),\n        WildDash2Class(  'banner'               , 66 ,      255  ),\n        WildDash2Class(  'dashcam mount'        , 67 ,      255  ),\n        WildDash2Class(  'water'                , 68 ,      255  ),\n        WildDash2Class(  'sand'                 , 69 ,      255  ),\n        WildDash2Class(  'pedestrian area'      , 70 ,        0  ),\n        WildDash2Class(  'fire hydrant'         , 71 ,      255  ),\n        WildDash2Class(  'cctv camera'          , 72 ,      255  ),\n        WildDash2Class(  'snow'                 , 73 ,      255  ),\n        WildDash2Class(  'catch basin'          , 74 ,        0  ),\n        WildDash2Class(  'crosswalk plain'      , 75 ,        0  ),\n        WildDash2Class(  'crosswalk zebra'      , 76 ,        0  ),\n        WildDash2Class(  'manhole sidewalk'     , 77 ,        1  ),\n        WildDash2Class(  'curb terrain'         , 78 ,        9  ),\n        WildDash2Class(  'service lane'         , 79 ,        0  ),\n        WildDash2Class(  'curb cut'             , 80 ,        1  ),\n        WildDash2Class(  'license plate'        , -1 ,      255  ),\n    ]\n    id_to_train_id = np.array([c.train_id for c in labels], dtype=int)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"test\",\n            load_keys: Union[List[str], str] = [\"image\", \"semantic\"],\n            conditions: Union[List[str], str] = [\"fog\", \"night\", \"rain\", \"snow\"],\n            transforms: Optional[Callable] = None,\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.conditions = [conditions] if isinstance(conditions, str) else conditions\n        self.transforms = transforms\n\n        assert stage == 'test'\n        self.stage = stage\n        self.split = 'test'\n        if isinstance(load_keys, str):\n            self.load_keys = [load_keys]\n        else:\n            self.load_keys = load_keys\n\n        # load WildDash2 encodings\n        pan = json.load(open(os.path.join(self.root, 'WildDash2', 'panoptic.json')))\n        self.img_to_segments = {img_dict['file_name']: img_dict['segments_info'] for img_dict in pan['annotations']}\n\n        self.paths = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_night = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_rain = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_snow = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_fog = {k: [] for k in ['image', 'semantic', 'dataset']}\n\n        wilddash_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_fog'))]\n        for file_name in wilddash_img_ids_fog:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('wilddash')\n\n        wilddash_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_night'))]\n        for file_name in wilddash_img_ids_night:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_night['image'].append(file_path)\n            paths_night['semantic'].append(semantic_path)\n            paths_night['dataset'].append('wilddash')\n        \n        wilddash_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_rain'))]\n        for file_name in wilddash_img_ids_rain:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('wilddash')\n\n        wilddash_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_snow'))]\n        for file_name in wilddash_img_ids_snow:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_snow['image'].append(file_path)\n            paths_snow['semantic'].append(semantic_path)\n            paths_snow['dataset'].append('wilddash')\n\n        bdd100k_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_fog'))]\n        for file_name in bdd100k_img_ids_fog:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_night'))]\n        for file_name in bdd100k_img_ids_night:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_night['image'].append(file_path)\n            paths_night['semantic'].append(semantic_path)\n            paths_night['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_rain'))]\n        for file_name in bdd100k_img_ids_rain:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_snow'))]\n        for file_name in bdd100k_img_ids_snow:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_snow['image'].append(file_path)\n            paths_snow['semantic'].append(semantic_path)\n            paths_snow['dataset'].append('bdd100k')\n\n        foggydriving_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyDriving_fog'))]\n        for file_name in foggydriving_img_ids:\n            file_path = os.path.join(self.root, 'Foggy_Driving', file_name)\n            if 'test_extra' in file_name:\n                semantic_name = file_name.replace('leftImg8bit/test_extra/', 'gtCoarse/test_extra/')\n                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtCoarse_labelTrainIds.png')\n            else:\n                semantic_name = file_name.replace('leftImg8bit/test/', 'gtFine/test/')\n                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n            semantic_path = os.path.join(self.root, 'Foggy_Driving', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('foggydriving')\n\n        foggyzurich_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyZurich_fog'))]\n        for file_name in foggyzurich_img_ids:\n            file_path = os.path.join(self.root, 'Foggy_Zurich', file_name)\n            semantic_path = file_path.replace('/RGB/', '/gt_labelTrainIds/')\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('foggyzurich')\n\n        custom_img_ids = [\n            'train_000001.jpg',\n            'train_000002.jpg',\n            'train_000003.jpg',\n        ]\n        for file_name in custom_img_ids:\n            file_path = os.path.join(self.root, 'ACG', file_name)\n            semantic_path = file_path.replace('.jpg', '_mask.png')\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('custom')\n\n        self.len_fog = len(paths_fog['image'])\n        self.len_night = len(paths_night['image'])\n        self.len_rain = len(paths_rain['image'])\n        self.len_snow = len(paths_snow['image'])\n\n        for c in self.conditions:\n            if c == \"fog\":\n                self.paths['image'].extend(paths_fog['image'])\n                self.paths['semantic'].extend(paths_fog['semantic'])\n                self.paths['dataset'].extend(paths_fog['dataset'])\n            elif c == \"night\":\n                self.paths['image'].extend(paths_night['image'])\n                self.paths['semantic'].extend(paths_night['semantic'])\n                self.paths['dataset'].extend(paths_night['dataset'])\n            elif c == \"rain\":\n                self.paths['image'].extend(paths_rain['image'])\n                self.paths['semantic'].extend(paths_rain['semantic'])\n                self.paths['dataset'].extend(paths_rain['dataset'])\n            elif c == \"snow\":\n                self.paths['image'].extend(paths_snow['image'])\n                self.paths['semantic'].extend(paths_snow['semantic'])\n                self.paths['dataset'].extend(paths_snow['dataset'])\n                            \n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n\n        filename = self.paths['image'][index].split('/')[-1]\n        sample['filename'] = filename\n\n        dataset = self.paths['dataset'][index]\n        for k in self.load_keys:\n            if k == 'image':\n                data = Image.open(self.paths[k][index]).convert('RGB')\n            elif k == 'semantic':\n                data = Image.open(self.paths[k][index])\n                if dataset == 'wilddash':\n                    data = self.encode_semantic_map(\n                        data, filename.replace('.jpg', '.png'))\n            else:\n                raise ValueError('invalid load_key')\n            sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(self.paths['image'])\n\n    def encode_semantic_map(self, semseg, filename):\n        pan_format = np.array(semseg, dtype=np.uint32)\n        pan = self.rgb2id(pan_format)\n        semantic = np.zeros(pan.shape, dtype=np.uint8)\n        for segm_info in self.img_to_segments[filename]:\n            semantic[pan == segm_info['id']] = segm_info['category_id']\n        semantic = self.id_to_train_id[semantic.astype(int)]\n        return Image.fromarray(semantic.astype(np.uint8))\n\n    @staticmethod\n    def rgb2id(color):\n        if color.dtype == np.uint8:\n            color = color.astype(np.uint32)\n        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]", "\n\nclass ACG(torch.utils.data.Dataset):\n    \"\"\"ACG benchmark from the paper:\n\n        Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation\n\n    \"\"\"\n\n    WildDash2Class = namedtuple(\"WildDash2Class\", [\"name\", \"id\", \"train_id\"])\n    labels = [\n        #       name                             id    trainId  \n        WildDash2Class(  'unlabeled'            ,  0 ,      255  ),\n        WildDash2Class(  'ego vehicle'          ,  1 ,      255  ),\n        WildDash2Class(  'overlay'              ,  2 ,      255  ),\n        WildDash2Class(  'out of roi'           ,  3 ,      255  ),\n        WildDash2Class(  'static'               ,  4 ,      255  ),\n        WildDash2Class(  'dynamic'              ,  5 ,      255  ),\n        WildDash2Class(  'ground'               ,  6 ,      255  ),\n        WildDash2Class(  'road'                 ,  7 ,        0  ),\n        WildDash2Class(  'sidewalk'             ,  8 ,        1  ),\n        WildDash2Class(  'parking'              ,  9 ,      255  ),\n        WildDash2Class(  'rail track'           , 10 ,      255  ),\n        WildDash2Class(  'building'             , 11 ,        2  ),\n        WildDash2Class(  'wall'                 , 12 ,        3  ),\n        WildDash2Class(  'fence'                , 13 ,        4  ),\n        WildDash2Class(  'guard rail'           , 14 ,      255  ),\n        WildDash2Class(  'bridge'               , 15 ,      255  ),\n        WildDash2Class(  'tunnel'               , 16 ,      255  ),\n        WildDash2Class(  'pole'                 , 17 ,        5  ),\n        WildDash2Class(  'polegroup'            , 18 ,      255  ),\n        WildDash2Class(  'traffic light'        , 19 ,        6  ),\n        WildDash2Class(  'traffic sign front'   , 20 ,        7  ),\n        WildDash2Class(  'vegetation'           , 21 ,        8  ),\n        WildDash2Class(  'terrain'              , 22 ,        9  ),\n        WildDash2Class(  'sky'                  , 23 ,       10  ),\n        WildDash2Class(  'person'               , 24 ,       11  ),\n        WildDash2Class(  'rider'                , 25 ,       12  ),\n        WildDash2Class(  'car'                  , 26 ,       13  ),\n        WildDash2Class(  'truck'                , 27 ,       14  ),\n        WildDash2Class(  'bus'                  , 28 ,       15  ),\n        WildDash2Class(  'caravan'              , 29 ,      255  ),\n        WildDash2Class(  'trailer'              , 30 ,      255  ),\n        WildDash2Class(  'on rails'             , 31 ,       16  ),\n        WildDash2Class(  'motorcycle'           , 32 ,       17  ),\n        WildDash2Class(  'bicycle'              , 33 ,       18  ),\n        WildDash2Class(  'pickup'               , 34 ,       14  ),\n        WildDash2Class(  'van'                  , 35 ,       13  ),\n        WildDash2Class(  'billboard'            , 36 ,      255  ),\n        WildDash2Class(  'street light'         , 37 ,      255  ),\n        WildDash2Class(  'road marking'         , 38 ,        0  ),\n        WildDash2Class(  'junctionbox'          , 39 ,      255  ),\n        WildDash2Class(  'mailbox'              , 40 ,      255  ),\n        WildDash2Class(  'manhole'              , 41 ,        0  ),\n        WildDash2Class(  'phonebooth'           , 42 ,      255  ),\n        WildDash2Class(  'pothole'              , 43 ,        0  ),\n        WildDash2Class(  'bikerack'             , 44 ,      255  ),\n        WildDash2Class(  'traffic sign frame'   , 45 ,        5  ),\n        WildDash2Class(  'utility pole'         , 46 ,        5  ),\n        WildDash2Class(  'motorcyclist'         , 47 ,       12  ),\n        WildDash2Class(  'bicyclist'            , 48 ,       12  ),\n        WildDash2Class(  'other rider'          , 49 ,       12  ),\n        WildDash2Class(  'bird'                 , 50 ,      255  ),\n        WildDash2Class(  'ground animal'        , 51 ,      255  ),\n        WildDash2Class(  'curb'                 , 52 ,        1  ),\n        WildDash2Class(  'traffic sign any'     , 53 ,      255  ),\n        WildDash2Class(  'traffic sign back'    , 54 ,      255  ),\n        WildDash2Class(  'trashcan'             , 55 ,      255  ),\n        WildDash2Class(  'other barrier'        , 56 ,        3  ),\n        WildDash2Class(  'other vehicle'        , 57 ,      255  ),\n        WildDash2Class(  'auto rickshaw'        , 58 ,       17  ),\n        WildDash2Class(  'bench'                , 59 ,      255  ),\n        WildDash2Class(  'mountain'             , 60 ,      255  ),\n        WildDash2Class(  'tram track'           , 61 ,        0  ),\n        WildDash2Class(  'wheeled slow'         , 62 ,      255  ),\n        WildDash2Class(  'boat'                 , 63 ,      255  ),\n        WildDash2Class(  'bikelane'             , 64 ,        0  ),\n        WildDash2Class(  'bikelane sidewalk'    , 65 ,        1  ),\n        WildDash2Class(  'banner'               , 66 ,      255  ),\n        WildDash2Class(  'dashcam mount'        , 67 ,      255  ),\n        WildDash2Class(  'water'                , 68 ,      255  ),\n        WildDash2Class(  'sand'                 , 69 ,      255  ),\n        WildDash2Class(  'pedestrian area'      , 70 ,        0  ),\n        WildDash2Class(  'fire hydrant'         , 71 ,      255  ),\n        WildDash2Class(  'cctv camera'          , 72 ,      255  ),\n        WildDash2Class(  'snow'                 , 73 ,      255  ),\n        WildDash2Class(  'catch basin'          , 74 ,        0  ),\n        WildDash2Class(  'crosswalk plain'      , 75 ,        0  ),\n        WildDash2Class(  'crosswalk zebra'      , 76 ,        0  ),\n        WildDash2Class(  'manhole sidewalk'     , 77 ,        1  ),\n        WildDash2Class(  'curb terrain'         , 78 ,        9  ),\n        WildDash2Class(  'service lane'         , 79 ,        0  ),\n        WildDash2Class(  'curb cut'             , 80 ,        1  ),\n        WildDash2Class(  'license plate'        , -1 ,      255  ),\n    ]\n    id_to_train_id = np.array([c.train_id for c in labels], dtype=int)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"test\",\n            load_keys: Union[List[str], str] = [\"image\", \"semantic\"],\n            conditions: Union[List[str], str] = [\"fog\", \"night\", \"rain\", \"snow\"],\n            transforms: Optional[Callable] = None,\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.conditions = [conditions] if isinstance(conditions, str) else conditions\n        self.transforms = transforms\n\n        assert stage == 'test'\n        self.stage = stage\n        self.split = 'test'\n        if isinstance(load_keys, str):\n            self.load_keys = [load_keys]\n        else:\n            self.load_keys = load_keys\n\n        # load WildDash2 encodings\n        pan = json.load(open(os.path.join(self.root, 'WildDash2', 'panoptic.json')))\n        self.img_to_segments = {img_dict['file_name']: img_dict['segments_info'] for img_dict in pan['annotations']}\n\n        self.paths = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_night = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_rain = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_snow = {k: [] for k in ['image', 'semantic', 'dataset']}\n        paths_fog = {k: [] for k in ['image', 'semantic', 'dataset']}\n\n        wilddash_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_fog'))]\n        for file_name in wilddash_img_ids_fog:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('wilddash')\n\n        wilddash_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_night'))]\n        for file_name in wilddash_img_ids_night:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_night['image'].append(file_path)\n            paths_night['semantic'].append(semantic_path)\n            paths_night['dataset'].append('wilddash')\n        \n        wilddash_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_rain'))]\n        for file_name in wilddash_img_ids_rain:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('wilddash')\n\n        wilddash_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_WildDash_snow'))]\n        for file_name in wilddash_img_ids_snow:\n            file_path = os.path.join(self.root, 'WildDash2', file_name)\n            semantic_name = file_name.replace('images/', 'panoptic/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'WildDash2', semantic_name)\n            paths_snow['image'].append(file_path)\n            paths_snow['semantic'].append(semantic_path)\n            paths_snow['dataset'].append('wilddash')\n\n        bdd100k_img_ids_fog = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_fog'))]\n        for file_name in bdd100k_img_ids_fog:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_night = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_night'))]\n        for file_name in bdd100k_img_ids_night:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_night['image'].append(file_path)\n            paths_night['semantic'].append(semantic_path)\n            paths_night['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_rain = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_rain'))]\n        for file_name in bdd100k_img_ids_rain:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('bdd100k')\n\n        bdd100k_img_ids_snow = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_BDD100k_snow'))]\n        for file_name in bdd100k_img_ids_snow:\n            file_path = os.path.join(self.root, 'bdd100k', file_name)\n            semantic_name = file_name.replace('images/10k/', 'labels/sem_seg/masks/')\n            semantic_name = semantic_name.replace('.jpg', '.png')\n            semantic_path = os.path.join(self.root, 'bdd100k', semantic_name)\n            paths_snow['image'].append(file_path)\n            paths_snow['semantic'].append(semantic_path)\n            paths_snow['dataset'].append('bdd100k')\n\n        foggydriving_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyDriving_fog'))]\n        for file_name in foggydriving_img_ids:\n            file_path = os.path.join(self.root, 'Foggy_Driving', file_name)\n            if 'test_extra' in file_name:\n                semantic_name = file_name.replace('leftImg8bit/test_extra/', 'gtCoarse/test_extra/')\n                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtCoarse_labelTrainIds.png')\n            else:\n                semantic_name = file_name.replace('leftImg8bit/test/', 'gtFine/test/')\n                semantic_name = semantic_name.replace('_leftImg8bit.png', '_gtFine_labelTrainIds.png')\n            semantic_path = os.path.join(self.root, 'Foggy_Driving', semantic_name)\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('foggydriving')\n\n        foggyzurich_img_ids = [i_id.strip() for i_id in open(os.path.join(self.root, 'ACG', 'ACG_FoggyZurich_fog'))]\n        for file_name in foggyzurich_img_ids:\n            file_path = os.path.join(self.root, 'Foggy_Zurich', file_name)\n            semantic_path = file_path.replace('/RGB/', '/gt_labelTrainIds/')\n            paths_fog['image'].append(file_path)\n            paths_fog['semantic'].append(semantic_path)\n            paths_fog['dataset'].append('foggyzurich')\n\n        custom_img_ids = [\n            'train_000001.jpg',\n            'train_000002.jpg',\n            'train_000003.jpg',\n        ]\n        for file_name in custom_img_ids:\n            file_path = os.path.join(self.root, 'ACG', file_name)\n            semantic_path = file_path.replace('.jpg', '_mask.png')\n            paths_rain['image'].append(file_path)\n            paths_rain['semantic'].append(semantic_path)\n            paths_rain['dataset'].append('custom')\n\n        self.len_fog = len(paths_fog['image'])\n        self.len_night = len(paths_night['image'])\n        self.len_rain = len(paths_rain['image'])\n        self.len_snow = len(paths_snow['image'])\n\n        for c in self.conditions:\n            if c == \"fog\":\n                self.paths['image'].extend(paths_fog['image'])\n                self.paths['semantic'].extend(paths_fog['semantic'])\n                self.paths['dataset'].extend(paths_fog['dataset'])\n            elif c == \"night\":\n                self.paths['image'].extend(paths_night['image'])\n                self.paths['semantic'].extend(paths_night['semantic'])\n                self.paths['dataset'].extend(paths_night['dataset'])\n            elif c == \"rain\":\n                self.paths['image'].extend(paths_rain['image'])\n                self.paths['semantic'].extend(paths_rain['semantic'])\n                self.paths['dataset'].extend(paths_rain['dataset'])\n            elif c == \"snow\":\n                self.paths['image'].extend(paths_snow['image'])\n                self.paths['semantic'].extend(paths_snow['semantic'])\n                self.paths['dataset'].extend(paths_snow['dataset'])\n                            \n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n\n        filename = self.paths['image'][index].split('/')[-1]\n        sample['filename'] = filename\n\n        dataset = self.paths['dataset'][index]\n        for k in self.load_keys:\n            if k == 'image':\n                data = Image.open(self.paths[k][index]).convert('RGB')\n            elif k == 'semantic':\n                data = Image.open(self.paths[k][index])\n                if dataset == 'wilddash':\n                    data = self.encode_semantic_map(\n                        data, filename.replace('.jpg', '.png'))\n            else:\n                raise ValueError('invalid load_key')\n            sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(self.paths['image'])\n\n    def encode_semantic_map(self, semseg, filename):\n        pan_format = np.array(semseg, dtype=np.uint32)\n        pan = self.rgb2id(pan_format)\n        semantic = np.zeros(pan.shape, dtype=np.uint8)\n        for segm_info in self.img_to_segments[filename]:\n            semantic[pan == segm_info['id']] = segm_info['category_id']\n        semantic = self.id_to_train_id[semantic.astype(int)]\n        return Image.fromarray(semantic.astype(np.uint8))\n\n    @staticmethod\n    def rgb2id(color):\n        if color.dtype == np.uint8:\n            color = color.astype(np.uint32)\n        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]", ""]}
{"filename": "data_modules/datasets/acdc.py", "chunked_list": ["import io\nimport os\nimport zipfile\nfrom typing import Any, Callable, List, Optional, Union\n\nimport requests\nimport torch\nfrom PIL import Image\n\nURLS = {", "\nURLS = {\n    \"pseudo_labels_train_ACDC_cma_segformer\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_ACDC_cma_segformer.zip\",\n    \"pseudo_labels_train_ACDC_cma_deeplabv2\": \"https://data.vision.ee.ethz.ch/brdavid/cma/pseudo_labels_train_ACDC_cma_deeplabv2.zip\",\n}\n\n\nclass ACDC(torch.utils.data.Dataset):\n\n    orig_dims = (1080, 1920)\n\n    def __init__(\n            self,\n            root: str,\n            stage: str = \"train\",\n            condition: Union[List[str], str] = [\n                \"fog\", \"night\", \"rain\", \"snow\"],\n            load_keys: Union[List[str], str] = [\n                \"image_ref\", \"image\", \"semantic\"],\n            transforms: Optional[Callable] = None,\n            predict_on: str = \"test\",\n            load_pseudo_labels: bool = False,\n            pseudo_label_dir: Optional[str] = None,\n            **kwargs\n    ) -> None:\n        super().__init__()\n        self.root = root\n        self.transforms = transforms\n        self.load_pseudo_labels = load_pseudo_labels\n        if self.load_pseudo_labels:\n            assert pseudo_label_dir is not None\n            _dpath = os.path.join(os.path.dirname(self.root), 'pseudo_labels')\n            os.makedirs(_dpath, exist_ok=True)\n            if not os.path.exists(os.path.join(_dpath, pseudo_label_dir)):\n                print('Downloading and extracting pseudo-labels...')\n                r = requests.get(URLS[pseudo_label_dir])\n                z = zipfile.ZipFile(io.BytesIO(r.content))\n                z.extractall(_dpath)\n            self.pseudo_label_dir = os.path.join(\n                os.path.dirname(self.root), 'pseudo_labels', pseudo_label_dir)\n\n        assert stage in [\"train\", \"val\", \"test\", \"predict\"]\n        self.stage = stage\n\n        # mapping from stage to splits\n        if self.stage == 'train':\n            self.split = 'train'\n        elif self.stage == 'val':\n            self.split = 'val'\n        elif self.stage == 'test':\n            self.split = 'val'  # test on val split\n        elif self.stage == 'predict':\n            self.split = predict_on\n\n        self.condition = [condition] if isinstance(\n            condition, str) else condition\n\n        self.load_keys = [load_keys] if isinstance(\n            load_keys, str) else load_keys\n\n        self.paths = {k: []\n                      for k in ['image', 'image_ref', 'semantic']}\n\n        self.images_dir = os.path.join(self.root, 'rgb_anon')\n        self.semantic_dir = os.path.join(self.root, 'gt')\n        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.semantic_dir):\n            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n                               ' specified \"split\" and \"condition\" are inside the \"root\" directory')\n\n        for cond in self.condition:\n            img_parent_dir = os.path.join(self.images_dir, cond, self.split)\n            semantic_parent_dir = os.path.join(\n                self.semantic_dir, cond, self.split)\n            for recording in os.listdir(img_parent_dir):\n                img_dir = os.path.join(img_parent_dir, recording)\n                semantic_dir = os.path.join(semantic_parent_dir, recording)\n                for file_name in os.listdir(img_dir):\n                    for k in ['image', 'image_ref', 'semantic']:\n                        if k == 'image':\n                            file_path = os.path.join(img_dir, file_name)\n                        elif k == 'image_ref':\n                            ref_img_dir = img_dir.replace(\n                                self.split, self.split + '_ref')\n                            ref_file_name = file_name.replace(\n                                'rgb_anon', 'rgb_ref_anon')\n                            file_path = os.path.join(\n                                ref_img_dir, ref_file_name)\n                        elif k == 'semantic':\n                            semantic_file_name = file_name.replace(\n                                'rgb_anon.png', 'gt_labelTrainIds.png')\n                            file_path = os.path.join(\n                                semantic_dir, semantic_file_name)\n                        self.paths[k].append(file_path)\n\n    def __getitem__(self, index: int):\n\n        sample: Any = {}\n        if (not 'image' in self.load_keys) and ('image_ref' in self.load_keys):\n            filename = self.paths['image_ref'][index].split('/')[-1]\n        else:\n            filename = self.paths['image'][index].split('/')[-1]\n        sample['filename'] = filename\n\n        for k in self.load_keys:\n            if k in ['image', 'image_ref']:\n                data = Image.open(self.paths[k][index]).convert('RGB')\n            elif k == 'semantic':\n                if self.load_pseudo_labels:\n                    path = os.path.join(self.pseudo_label_dir, filename)\n                    data = Image.open(path)\n                else:\n                    data = Image.open(self.paths[k][index])\n            else:\n                raise ValueError('invalid load_key')\n            sample[k] = data\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(next(iter(self.paths.values())))", ""]}
{"filename": "helpers/metrics.py", "chunked_list": ["from typing import Any, Literal, Optional\n\nimport torch\nfrom torch import Tensor\nfrom torchmetrics.classification import MulticlassJaccardIndex\nfrom torchmetrics.utilities.compute import _safe_divide\n\n\ndef _my_jaccard_index_reduce(\n    confmat: Tensor,\n    average: Optional[Literal[\"macro\", \"none\"]],\n    over_present_classes: bool = False,\n) -> Tensor:\n    allowed_average = [\"macro\", \"none\", None]\n    if average not in allowed_average:\n        raise ValueError(f\"The `average` has to be one of {allowed_average}, got {average}.\")\n    confmat = confmat.float()\n    if confmat.ndim == 3:  # multilabel\n        raise NotImplementedError\n    else:  # multiclass\n        num = torch.diag(confmat)\n        denom = confmat.sum(0) + confmat.sum(1) - num\n\n        if over_present_classes:\n            present_classes = confmat.sum(1) != 0\n            num = torch.masked_select(num, present_classes)\n            denom = torch.masked_select(denom, present_classes)\n\n    jaccard = _safe_divide(num, denom)\n\n    if average is None or average == \"none\":\n        return jaccard\n    return (jaccard / jaccard.numel()).sum()", "def _my_jaccard_index_reduce(\n    confmat: Tensor,\n    average: Optional[Literal[\"macro\", \"none\"]],\n    over_present_classes: bool = False,\n) -> Tensor:\n    allowed_average = [\"macro\", \"none\", None]\n    if average not in allowed_average:\n        raise ValueError(f\"The `average` has to be one of {allowed_average}, got {average}.\")\n    confmat = confmat.float()\n    if confmat.ndim == 3:  # multilabel\n        raise NotImplementedError\n    else:  # multiclass\n        num = torch.diag(confmat)\n        denom = confmat.sum(0) + confmat.sum(1) - num\n\n        if over_present_classes:\n            present_classes = confmat.sum(1) != 0\n            num = torch.masked_select(num, present_classes)\n            denom = torch.masked_select(denom, present_classes)\n\n    jaccard = _safe_divide(num, denom)\n\n    if average is None or average == \"none\":\n        return jaccard\n    return (jaccard / jaccard.numel()).sum()", "\n\nclass IoU(MulticlassJaccardIndex):\n\n    def __init__(\n        self,\n        over_present_classes: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self.over_present_classes = over_present_classes\n        super().__init__(**kwargs)\n\n    def compute(self) -> Tensor:\n        return _my_jaccard_index_reduce(self.confmat, average=self.average, over_present_classes=self.over_present_classes)", ""]}
{"filename": "helpers/pseudo_labels.py", "chunked_list": ["import os\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom models.backbones import MixVisionTransformer, ResNet\nfrom models.heads import DeepLabV2Head, SegFormerHead\nfrom PIL import Image\nfrom tqdm import tqdm", "from PIL import Image\nfrom tqdm import tqdm\n\n\n@torch.inference_mode()\ndef generate_pseudo_labels(model, datamodule):\n    print(\"Generating pseudo labels...\")\n\n    datamodule.prepare_data()\n    datamodule.setup('predict')\n    assert len(datamodule.predict_ds) == 1\n\n    # iterate through all samples to collect statistics\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n\n    model.use_ensemble_inference = True\n    model.inference_ensemble_scale = [\n        0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n\n    print(\"Collecting statistics...\")\n    conf_dict = defaultdict(list)\n    for batch in tqdm(datamodule.predict_dataloader()[0]):\n        batch = model.transfer_batch_to_device(\n            batch, device, dataloader_idx=0)\n        orig_size = datamodule.predict_ds[0].orig_dims\n        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n        y_hat = model.forward(x, orig_size)\n        probs = y_hat if model.use_ensemble_inference else nn.functional.softmax(y_hat, dim=1)\n        pred_prob, pred_labels = probs.max(dim=1)\n        for idx_cls in range(model.head.num_classes):\n            idx_temp = pred_labels == idx_cls\n            if idx_temp.any():\n                conf_cls_temp = pred_prob[idx_temp][::10].cpu(\n                ).tolist()\n                conf_dict[idx_cls].extend(conf_cls_temp)\n\n    trg_portion = 0.2\n    cls_thresh = torch.ones(\n        model.head.num_classes, dtype=torch.float, device=device)\n    for idx_cls in range(model.head.num_classes):\n        cls_probs = sorted(conf_dict[idx_cls], reverse=True)\n        len_cls_sel = int(len(cls_probs) * trg_portion)\n        if len_cls_sel > 0:\n            cls_thresh[idx_cls] = cls_probs[len_cls_sel - 1]\n\n    print(\"Extract PL...\")\n    dataset_name = datamodule.predict_on[0]\n    if isinstance(model.backbone, MixVisionTransformer) and isinstance(model.head, SegFormerHead):\n        model_name = 'segformer'\n    elif isinstance(model.backbone, ResNet) and isinstance(model.head, DeepLabV2Head):\n        model_name = 'deeplabv2'\n    else:\n        raise RuntimeError('unknown model configuration')\n    save_dir = os.path.join(\n        os.environ['$DATA_DIR'], 'pseudo_labels', 'pseudo_labels_train_{}_cma_{}'.format(dataset_name, model_name))\n    os.makedirs(save_dir, exist_ok=True)\n\n    # iterate through all samples to generate pseudo label\n    for batch in tqdm(datamodule.predict_dataloader()[0]):\n        filename = batch['filename']\n        if all(os.path.isfile(os.path.join(\n                save_dir, '.'.join([*im_name.split('.')[:-1], 'png']))) for im_name in filename):\n            continue\n        batch = model.transfer_batch_to_device(\n            batch, device, dataloader_idx=0)\n        orig_size = datamodule.predict_ds[0].orig_dims\n        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n        y_hat = model.forward(x, orig_size)\n        probs = y_hat if model.use_ensemble_inference else nn.functional.softmax(y_hat, dim=1)\n        weighted_prob = probs / cls_thresh.view(1, -1, 1, 1)\n        weighted_conf, preds = weighted_prob.max(dim=1)\n        preds[weighted_conf < 1] = 255\n\n        for pred, im_name in zip(preds, filename):\n            arr = pred.cpu().numpy()\n            image = Image.fromarray(arr.astype(np.uint8))\n            image.save(os.path.join(\n                save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))", ""]}
{"filename": "helpers/__init__.py", "chunked_list": [""]}
{"filename": "helpers/lr_scheduler.py", "chunked_list": ["import torch\nfrom torch.optim import Optimizer\n\n\nclass LinearWarmupLinearLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,\n                 optimizer: Optimizer,\n                 max_steps: int = None,\n                 warmup_iters: int = 1500,\n                 warmup_ratio: float = 1e-6,\n                 min_lr=0.,\n                 last_epoch=-1):\n        self.max_updates = max_steps\n        self.warmup_iters = warmup_iters\n        self.warmup_ratio = warmup_ratio\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n\n        # warmup phase\n        if self.last_epoch < self.warmup_iters:\n            k = (1 - self.last_epoch / self.warmup_iters) * \\\n                (1 - self.warmup_ratio)\n            return [_lr * (1 - k) for _lr in self.base_lrs]\n\n        # poly phase\n        else:\n            coeff = 1 - (self.last_epoch - self.warmup_iters) / \\\n                     float(self.max_updates - self.warmup_iters)\n            return [(base_lr - self.min_lr) * coeff + self.min_lr for base_lr in self.base_lrs]", ""]}
{"filename": "models/model.py", "chunked_list": ["import copy\nimport itertools\nimport math\nimport os\nfrom typing import List, Optional\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torchmetrics\nfrom PIL import Image\nfrom pytorch_lightning.callbacks import Checkpoint\nfrom pytorch_lightning.cli import instantiate_class\n\nfrom .backbones.mix_transformer import DropPath\nfrom .heads.base import BaseHead\nfrom .utils import (colorize_mask, estimate_probability_of_confidence_interval,", "from .heads.base import BaseHead\nfrom .utils import (colorize_mask, estimate_probability_of_confidence_interval,\n                    warp)\n\n\nclass CMAModel(pl.LightningModule):\n    def __init__(self,\n                 optimizer_init: dict,\n                 lr_scheduler_init: dict,\n                 backbone: nn.Module,\n                 head: BaseHead,\n                 contrastive_head: Optional[nn.Module] = None,\n                 alignment_backbone: Optional[nn.Module] = None,\n                 alignment_head: Optional[BaseHead] = None,\n                 self_training_loss: Optional[nn.Module] = None,\n                 self_training_loss_weight: float = 1.0,\n                 entropy_loss: Optional[nn.Module] = None,\n                 entropy_loss_weight: float = 1.0,\n                 contrastive_loss: Optional[nn.Module] = None,\n                 contrastive_loss_weight: float = 1.0,\n                 use_slide_inference: bool = True,\n                 use_ensemble_inference: bool = False,\n                 inference_ensemble_scale: List[float] = [1.0],\n                 inference_ensemble_scale_divisor: Optional[int] = None,\n                 projection_head_lr_factor: float = 10.0,\n                 ema_momentum: float = 0.9999,\n                 freeze_decoder: bool = True,\n                 metrics: dict = {},\n                 ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\n            'backbone',\n            'head',\n            'contrastive_head',\n            'alignment_backbone',\n            'alignment_head',\n            'self_training_loss',\n            'entropy_loss',\n            'contrastive_loss'\n        ])\n\n        #### MODEL ####\n        self.backbone = backbone\n        self.head = head\n        self.contrastive_head = contrastive_head\n        self.m_backbone = copy.deepcopy(self.backbone)\n        self.m_contrastive_head = copy.deepcopy(self.contrastive_head)\n        for ema_m in filter(None, [self.m_backbone, self.m_contrastive_head]):\n            for param in ema_m.parameters():\n                param.requires_grad = False\n        self.alignment_backbone = alignment_backbone\n        self.alignment_head = alignment_head\n        for alignment_m in filter(None, [self.alignment_backbone, self.alignment_head]):\n            for param in alignment_m.parameters():\n                param.requires_grad = False\n\n        #### LOSSES ####\n        self.self_training_loss = self_training_loss\n        self.self_training_loss_weight = self_training_loss_weight\n        self.entropy_loss = entropy_loss\n        self.entropy_loss_weight = entropy_loss_weight\n        self.contrastive_loss = contrastive_loss\n        self.contrastive_loss_weight = contrastive_loss_weight\n\n        #### INFERENCE ####\n        self.use_slide_inference = use_slide_inference\n        self.use_ensemble_inference = use_ensemble_inference\n        self.inference_ensemble_scale = inference_ensemble_scale\n        self.inference_ensemble_scale_divisor = inference_ensemble_scale_divisor\n\n        #### METRICS ####\n        val_metrics = {'val_{}_{}'.format(ds, el['class_path'].split(\n            '.')[-1]): instantiate_class(tuple(), el) for ds, m in metrics.get('val', {}).items() for el in m}\n        test_metrics = {'test_{}_{}'.format(ds, el['class_path'].split(\n            '.')[-1]): instantiate_class(tuple(), el) for ds, m in metrics.get('test', {}).items() for el in m}\n        self.valid_metrics = torchmetrics.MetricCollection(val_metrics)\n        self.test_metrics = torchmetrics.MetricCollection(test_metrics)\n\n        #### OPTIMIZATION ####\n        self.optimizer_init = optimizer_init\n        self.lr_scheduler_init = lr_scheduler_init\n\n        #### OTHER STUFF ####\n        self.projection_head_lr_factor = projection_head_lr_factor\n        self.ema_momentum = ema_momentum\n        self.freeze_decoder = freeze_decoder\n        if self.freeze_decoder:\n            for param in self.head.parameters():\n                param.requires_grad = False\n\n    def training_step(self, batch, batch_idx):\n\n        #\n        # MODEL FORWARD\n        #\n        images_trg = batch['image']\n        pseudo_label_trg = batch['semantic']\n        images_ref = batch['image_ref']\n\n        feats_trg = self.backbone(images_trg)\n        logits_trg = self.head(feats_trg)\n        logits_trg = nn.functional.interpolate(\n            logits_trg, images_trg.shape[-2:], mode='bilinear', align_corners=False)\n\n        if self.contrastive_loss is not None:\n            if self.global_step < self.contrastive_loss.warm_up_steps:\n                if isinstance(feats_trg, (list, tuple)):\n                    contrastive_inp = [el.detach() for el in feats_trg]\n                else:\n                    contrastive_inp = feats_trg.detach()\n            else:\n                contrastive_inp = feats_trg\n            emb_anc = self.contrastive_head(contrastive_inp)\n            with torch.no_grad():\n                self.update_momentum_encoder()\n                m_input = torch.cat((images_trg, images_ref))\n                m_output = self.m_contrastive_head(self.m_backbone(m_input))\n                emb_neg, emb_pos = torch.tensor_split(m_output, 2)\n                # warp the reference embeddings\n                emb_pos, confidence = self.align(emb_pos, images_ref, images_trg)\n\n        #\n        # LOSSES\n        #\n        loss = torch.tensor(0.0, device=self.device)\n\n        # SELF-TRAINING / CONSISTENCY\n        if self.self_training_loss is not None:\n            self_training_loss = self.self_training_loss(logits_trg, pseudo_label_trg)\n            self_training_loss *= self.self_training_loss_weight\n            self.log(\"train_loss_selftraining\", self_training_loss)\n            loss += self_training_loss\n\n        # ENTROPY MINIMIZATION\n        if self.entropy_loss is not None:\n            entropy_loss = self.entropy_loss(logits_trg)\n            entropy_loss *= self.entropy_loss_weight\n            self.log(\"train_loss_entropy\", entropy_loss)\n            loss += entropy_loss\n\n        # CONTRASTIVE\n        if self.contrastive_loss is not None:\n            contrastive_loss = self.contrastive_loss(emb_anc, emb_pos, confidence)\n            contrastive_loss *= self.contrastive_loss_weight\n            self.log(\"train_loss_contrastive\", contrastive_loss)\n            loss += contrastive_loss\n\n            # update the queue\n            with torch.no_grad():\n                emb_neg = self.all_gather(emb_neg)\n                if emb_neg.dim() == 5:\n                    emb_neg = torch.flatten(emb_neg, start_dim=0, end_dim=1)\n                self.contrastive_loss.update_queue(emb_neg)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch['image'], batch['semantic']\n        y_hat = self.forward(x, out_size=y.shape[-2:])\n        src_name = self.trainer.datamodule.idx_to_name['val'][dataloader_idx]\n        if not self.trainer.sanity_checking:\n            for k, m in self.valid_metrics.items():\n                if src_name in k:\n                    m.update(y_hat, y)\n\n    def validation_epoch_end(self, outs):\n        if not self.trainer.sanity_checking:\n            out_dict = self.valid_metrics.compute()\n            for k, v in out_dict.items():\n                self.log(k, v)\n        self.valid_metrics.reset()\n\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch['image'], batch['semantic']\n        y_hat = self.forward(x, out_size=y.shape[-2:])\n        src_name = self.trainer.datamodule.idx_to_name['test'][dataloader_idx]\n        for k, m in self.test_metrics.items():\n            if src_name in k:\n                m.update(y_hat, y)\n\n    def test_epoch_end(self, outs):\n        out_dict = self.test_metrics.compute()\n        for k, v in out_dict.items():\n            self.log(k, v)\n        self.test_metrics.reset()\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        dataset_name = self.trainer.datamodule.predict_on[dataloader_idx]\n        save_dir = os.path.join(os.path.dirname(\n            self.ckpt_dir), 'preds', dataset_name)\n        col_save_dir = os.path.join(os.path.dirname(\n            self.ckpt_dir), 'color_preds', dataset_name)\n        if self.trainer.is_global_zero:\n            os.makedirs(save_dir, exist_ok=True)\n            os.makedirs(col_save_dir, exist_ok=True)\n        img_names = batch['filename']\n        x = batch['image'] if 'image' in batch.keys() else batch['image_ref']\n        orig_size = self.trainer.datamodule.predict_ds[dataloader_idx].orig_dims\n        y_hat = self.forward(x, orig_size)\n        preds = torch.argmax(y_hat, dim=1)\n        for pred, im_name in zip(preds, img_names):\n            arr = pred.cpu().numpy()\n            image = Image.fromarray(arr.astype(np.uint8))\n            image.save(os.path.join(\n                save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))\n            col_image = colorize_mask(image)\n            col_image.save(os.path.join(\n                col_save_dir, '.'.join([*im_name.split('.')[:-1], 'png'])))\n\n    def forward(self, x, out_size=None):\n        if self.use_ensemble_inference:\n            out = self.forward_ensemble(x, out_size)\n        else:\n            if self.use_slide_inference:\n                out = self.slide_inference(x)\n            else:\n                out = self.whole_inference(x)\n            if out_size is not None:\n                out = nn.functional.interpolate(\n                    out, size=out_size, mode='bilinear', align_corners=False)\n        return out\n\n    def forward_ensemble(self, x, out_size=None):\n        assert out_size is not None\n        h, w = x.shape[-2:]\n        # RETURNS SUM OF PROBABILITIES\n        out_stack = 0\n        cnt = 0\n        for flip, scale in itertools.product([False, True], self.inference_ensemble_scale):\n            new_h, new_w = int(h * scale + 0.5), int(w * scale + 0.5)\n            if self.inference_ensemble_scale_divisor is not None:\n                size_divisor = self.inference_ensemble_scale_divisor\n                new_h = int(math.ceil(new_h / size_divisor)) * size_divisor\n                new_w = int(math.ceil(new_w / size_divisor)) * size_divisor\n            # this resize should mimic PIL\n            x_inp = nn.functional.interpolate(\n                x, size=(new_h, new_w), mode='bilinear', align_corners=False, antialias=True)\n            x_inp = x_inp.flip(-1) if flip else x_inp\n            if self.use_slide_inference:\n                out = self.slide_inference(x_inp)\n            else:\n                out = self.whole_inference(x_inp)\n            out = nn.functional.interpolate(\n                out, size=out_size, mode='bilinear', align_corners=False)\n            out = out.flip(-1) if flip else out\n            out = nn.functional.softmax(out, dim=1)\n            out_stack += out\n            cnt += 1\n        return out_stack / cnt\n\n    def whole_inference(self, x):\n        logits = self.head(self.backbone(x))\n        logits = nn.functional.interpolate(\n            logits, x.shape[-2:], mode='bilinear', align_corners=False)\n        return logits\n\n    def slide_inference(self, img):\n        \"\"\"\n        ---------------------------------------------------------------------------\n        Copyright (c) OpenMMLab. All rights reserved.\n        \n        This source code is licensed under the license found in the\n        LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n        ---------------------------------------------------------------------------\n        \"\"\"\n        batch_size, _, h_img, w_img = img.size()\n        h_stride, w_stride, h_crop, w_crop = self.get_inference_slide_params(\n            h_img, w_img)\n        num_classes = self.head.num_classes\n        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n        preds = img.new_zeros((batch_size, num_classes, h_img, w_img))\n        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))\n        for h_idx in range(h_grids):\n            for w_idx in range(w_grids):\n                y1 = h_idx * h_stride\n                x1 = w_idx * w_stride\n                y2 = min(y1 + h_crop, h_img)\n                x2 = min(x1 + w_crop, w_img)\n                y1 = max(y2 - h_crop, 0)\n                x1 = max(x2 - w_crop, 0)\n                crop_img = img[:, :, y1:y2, x1:x2]\n                crop_seg_logit = self.whole_inference(crop_img)\n                preds += nn.functional.pad(crop_seg_logit,\n                                            (int(x1), int(preds.shape[3] - x2), int(y1),\n                                            int(preds.shape[2] - y2)))\n                count_mat[:, :, y1:y2, x1:x2] += 1\n        assert (count_mat == 0).sum() == 0\n        preds = preds / count_mat\n        return preds\n\n    def configure_optimizers(self):\n        optimizer = instantiate_class(\n            self.optimizer_parameters(), self.optimizer_init)\n        lr_scheduler = {\n            \"scheduler\": instantiate_class(optimizer, self.lr_scheduler_init),\n            \"interval\": \"step\"\n        }\n        return [optimizer], [lr_scheduler]\n\n    def optimizer_parameters(self):\n        proj_weight_params = []\n        proj_bias_params = []\n        other_weight_params = []\n        other_bias_params = []\n        for name, p in self.named_parameters():\n            if not p.requires_grad: \n                continue\n            if name.startswith('contrastive_head'):\n                proj_bias_params.append(p) if len(p.shape) == 1 else proj_weight_params.append(p)\n            else:\n                other_bias_params.append(p) if len(p.shape) == 1 else other_weight_params.append(p)\n        lr = self.optimizer_init['init_args']['lr']\n        weight_decay = self.optimizer_init['init_args']['weight_decay']\n        return [\n            {'name': 'other_weight', 'params': other_weight_params,\n                'lr': lr, 'weight_decay': weight_decay},\n            {'name': 'other_bias', 'params': other_bias_params,\n                'lr': lr, 'weight_decay': 0},\n            {'name': 'proj_weight', 'params': proj_weight_params,\n                'lr': self.projection_head_lr_factor * lr, 'weight_decay': weight_decay},\n            {'name': 'proj_bias', 'params': proj_bias_params,\n                'lr': self.projection_head_lr_factor * lr, 'weight_decay': 0}\n        ]\n\n    @torch.no_grad()\n    def align(self, logits_ref, images_ref, images_trg):\n        assert self.alignment_head is not None\n\n        h, w = logits_ref.shape[-2:]\n        images_trg = nn.functional.interpolate(images_trg, size=(h, w), mode='bilinear', align_corners=False, antialias=True)\n        images_ref = nn.functional.interpolate(images_ref, size=(h, w), mode='bilinear', align_corners=False, antialias=True)\n\n        images_trg_256 = nn.functional.interpolate(\n            images_trg, size=(256, 256), mode='area')\n        images_ref_256 = nn.functional.interpolate(\n            images_ref, size=(256, 256), mode='area')\n\n        x_backbone = self.alignment_backbone(\n            torch.cat([images_ref, images_trg]), extract_only_indices=[-3, -2])\n        unpacked_x = [torch.tensor_split(l, 2) for l in x_backbone]\n        pyr_ref, pyr_trg = zip(*unpacked_x)\n        x_backbone_256 = self.alignment_backbone(\n            torch.cat([images_ref_256, images_trg_256]), extract_only_indices=[-2, -1])\n        unpacked_x_256 = [torch.tensor_split(l, 2) for l in x_backbone_256]\n        pyr_ref_256, pyr_trg_256 = zip(*unpacked_x_256)\n\n        trg_ref_flow, trg_ref_uncert = self.alignment_head(\n            pyr_trg, pyr_ref, pyr_trg_256, pyr_ref_256, (h, w))[-1]\n        trg_ref_flow = nn.functional.interpolate(\n            trg_ref_flow, size=(h, w), mode='bilinear', align_corners=False)\n        trg_ref_uncert = nn.functional.interpolate(\n            trg_ref_uncert, size=(h, w), mode='bilinear', align_corners=False)\n\n        trg_ref_cert = estimate_probability_of_confidence_interval(\n            trg_ref_uncert, R=1.0)\n        warped_ref_logits, trg_ref_mask = warp(\n            logits_ref, trg_ref_flow, return_mask=True)\n        warp_confidence = trg_ref_mask.unsqueeze(1) * trg_ref_cert\n        return warped_ref_logits, warp_confidence\n\n    @staticmethod\n    def get_inference_slide_params(h, w):\n        if h == w:\n            return 1, 1, h, w\n        if min(h, w) == h:  # wide image\n            assert w <= 2 * h\n            h_crop, w_crop, h_stride = h, h, 1\n            w_stride = w // 2 - h // 2 if w > 1.5 * h else w - h\n        else:  # tall image\n            assert h <= 2 * w\n            h_crop, w_crop, w_stride = w, w, 1\n            h_stride = h // 2 - w // 2 if h > 1.5 * w else h - w\n        return h_stride, w_stride, h_crop, w_crop\n\n    @torch.no_grad()\n    def update_momentum_encoder(self):\n        m = min(1.0 - 1 / (float(self.global_step) + 1.0),\n                self.ema_momentum)  # limit momentum in the beginning\n        for param, param_m in zip(\n                itertools.chain(self.backbone.parameters(), self.contrastive_head.parameters()),\n                itertools.chain(self.m_backbone.parameters(), self.m_contrastive_head.parameters())):\n            if not param.data.shape:\n                param_m.data = param_m.data * m + param.data * (1. - m)\n            else:\n                param_m.data[:] = param_m[:].data[:] * \\\n                    m + param[:].data[:] * (1. - m)\n\n    def train(self, mode=True):\n        super().train(mode=mode)\n        for m in filter(None, [self.alignment_backbone, self.alignment_head]):\n            m.eval()\n        for m in filter(None, [self.m_backbone, self.m_contrastive_head]):\n            if isinstance(m, (nn.modules.dropout._DropoutNd, DropPath)):\n                m.training = False\n        if self.freeze_decoder:\n            self.head.eval()\n\n    @property\n    def ckpt_dir(self):\n        # mirroring https://github.com/Lightning-AI/lightning/blob/3bee81960a6f8979c8e1b5e747a17124feee652d/src/pytorch_lightning/callbacks/model_checkpoint.py#L571\n        for cb in self.trainer.callbacks:\n            if isinstance(cb, Checkpoint):\n                if cb.dirpath is not None:\n                    return cb.dirpath\n\n        if len(self.trainer.loggers) > 0:\n            if self.trainer.loggers[0].save_dir is not None:\n                save_dir = self.trainer.loggers[0].save_dir\n            else:\n                save_dir = self.trainer.default_root_dir\n            name = self.trainer.loggers[0].name\n            version = self.trainer.loggers[0].version\n            version = version if isinstance(version, str) else f\"version_{version}\"\n\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\n        else:\n            # if no loggers, use default_root_dir\n            ckpt_path = os.path.join(self.trainer.default_root_dir, \"checkpoints\")\n\n        return ckpt_path", ""]}
{"filename": "models/losses.py", "chunked_list": ["import math\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass NormalizedEntropyLoss(nn.Module):\n    def __init__(self, reduction: str = 'mean'):\n        super().__init__()\n        assert reduction in ['none', 'mean', 'sum'], 'invalid reduction'\n        self.reduction = reduction\n\n    def forward(self, logits: Tensor):\n        assert logits.dim() in [3, 4]\n        dim = logits.shape[1]\n        p_log_p = nn.functional.softmax(\n            logits, dim=1) * nn.functional.log_softmax(logits, dim=1)\n        ent = -1.0 * p_log_p.sum(dim=1)  # b x h x w OR b x n\n        loss = ent / math.log(dim)\n        if self.reduction == 'none':\n            return loss\n        elif self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()", "\n\nclass CDCLoss(nn.Module):\n    def __init__(self,\n                 feat_dim: int = 128,\n                 temperature: float = 0.3,\n                 num_grid: int = 7,\n                 queue_len: int = 65536,\n                 warm_up_steps: int = 2500,\n                 confidence_threshold: float = 0.2):\n        super().__init__()\n        self.feat_dim = feat_dim\n        self.temperature = temperature\n        self.num_grid = num_grid\n        self.queue_len = queue_len\n        self.warm_up_steps = warm_up_steps\n        self.confidence_threshold = confidence_threshold\n\n        self.register_buffer(\"queue\", torch.randn(feat_dim, queue_len))\n        self.queue = nn.functional.normalize(self.queue, p=2, dim=0)\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n\n    def forward(self, emb_anc, emb_pos, confidence):\n        # since the features are normalized afterwards, we don't need to worry about the\n        # normalization factor during pooling (for a weighted average)\n        emb_anc = emb_anc * confidence\n        emb_anc = nn.functional.adaptive_avg_pool2d(emb_anc, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n        emb_anc_zero_mask = torch.linalg.norm(emb_anc, dim=1) != 0\n\n        emb_pos = emb_pos * confidence\n        emb_pos = nn.functional.adaptive_avg_pool2d(emb_pos, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n        emb_pos_zero_mask = torch.linalg.norm(emb_pos, dim=1) != 0\n\n        avg_confidence = nn.functional.adaptive_avg_pool2d(confidence, self.num_grid).view(-1)\n        avg_confidence_mask = avg_confidence >= self.confidence_threshold\n\n        zero_mask = emb_anc_zero_mask & emb_pos_zero_mask & avg_confidence_mask\n\n        emb_anc = emb_anc[zero_mask]\n        emb_pos = emb_pos[zero_mask]\n        emb_anc = nn.functional.normalize(emb_anc, p=2, dim=1)\n        emb_pos = nn.functional.normalize(emb_pos, p=2, dim=1)\n\n        l_pos_dense = torch.einsum('nc,nc->n', [emb_anc, emb_pos]).unsqueeze(-1)\n        l_neg_dense = torch.einsum('nc,ck->nk', [emb_anc, self.queue.clone().detach()])\n\n        logits = torch.cat((l_pos_dense, l_neg_dense), dim=1) / self.temperature\n        labels = torch.zeros((logits.size(0), ), dtype=torch.long, device=logits.device)\n        loss = nn.functional.cross_entropy(logits, labels, reduction='mean')\n\n        return loss\n\n    @torch.no_grad()\n    def update_queue(self, emb_neg):\n        emb_neg = nn.functional.adaptive_avg_pool2d(emb_neg, self.num_grid).permute(0, 2, 3, 1).contiguous().view(-1, self.feat_dim)\n        emb_neg = nn.functional.normalize(emb_neg, p=2, dim=1)\n\n        batch_size = emb_neg.shape[0]\n\n        ptr = int(self.queue_ptr)\n        if ptr + batch_size > self.queue_len:\n            sec1, sec2 = self.queue_len - \\\n                ptr, ptr + batch_size - self.queue_len\n            emb_neg1, emb_neg2 = torch.split(emb_neg, [sec1, sec2], dim=0)\n            self.queue[:, -sec1:] = emb_neg1.transpose(0, 1)\n            self.queue[:, :sec2] = emb_neg2.transpose(0, 1)\n        else:\n            self.queue[:, ptr:ptr + batch_size] = emb_neg.transpose(0, 1)\n\n        ptr = (ptr + batch_size) % self.queue_len  # move pointer\n        self.queue_ptr[0] = ptr", ""]}
{"filename": "models/__init__.py", "chunked_list": ["from .model import CMAModel\n"]}
{"filename": "models/utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom PIL import Image\n\nPALETTE = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nfor i in range(256 * 3 - len(PALETTE)):\n    PALETTE.append(0)\n", "\n\ndef colorize_mask(mask):\n    assert isinstance(mask, Image.Image)\n    new_mask = mask.convert('P')\n    new_mask.putpalette(PALETTE)\n    return new_mask\n\n\ndef warp(x, flo, padding_mode='zeros', return_mask=False):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n    \n    warp an image/tensor (im2) back to im1, according to the optical flow\n    Args:\n        x: [B, C, H, W] (im2)\n        flo: [B, 2, H, W] flow\n    \"\"\"\n    B, C, H, W = x.size()\n    if torch.all(flo == 0):\n        if return_mask:\n            return x, torch.ones((B, H, W), dtype=torch.bool, device=x.device)\n        return x\n    # mesh grid\n    xx = torch.arange(0, W, dtype=flo.dtype,\n                      device=flo.device).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H, dtype=flo.dtype,\n                      device=flo.device).view(-1, 1).repeat(1, W)\n    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    grid = torch.cat((xx, yy), 1)\n    vgrid = grid + flo\n\n    # scale grid to [-1,1]\n    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :] / float(max(W-1, 1)) - 1.0\n    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :] / float(max(H-1, 1)) - 1.0\n\n    vgrid = vgrid.permute(0, 2, 3, 1)\n\n    output = nn.functional.grid_sample(\n        x, vgrid, align_corners=True, padding_mode=padding_mode)\n    if return_mask:\n        vgrid = vgrid.detach().clone().permute(0, 3, 1, 2)\n        mask = (vgrid[:, 0] > -1) & (vgrid[:, 1] > -\n                                     1) & (vgrid[:, 0] < 1) & (vgrid[:, 1] < 1)\n        return output, mask\n    return output", "\ndef warp(x, flo, padding_mode='zeros', return_mask=False):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n    \n    warp an image/tensor (im2) back to im1, according to the optical flow\n    Args:\n        x: [B, C, H, W] (im2)\n        flo: [B, 2, H, W] flow\n    \"\"\"\n    B, C, H, W = x.size()\n    if torch.all(flo == 0):\n        if return_mask:\n            return x, torch.ones((B, H, W), dtype=torch.bool, device=x.device)\n        return x\n    # mesh grid\n    xx = torch.arange(0, W, dtype=flo.dtype,\n                      device=flo.device).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H, dtype=flo.dtype,\n                      device=flo.device).view(-1, 1).repeat(1, W)\n    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    grid = torch.cat((xx, yy), 1)\n    vgrid = grid + flo\n\n    # scale grid to [-1,1]\n    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :] / float(max(W-1, 1)) - 1.0\n    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :] / float(max(H-1, 1)) - 1.0\n\n    vgrid = vgrid.permute(0, 2, 3, 1)\n\n    output = nn.functional.grid_sample(\n        x, vgrid, align_corners=True, padding_mode=padding_mode)\n    if return_mask:\n        vgrid = vgrid.detach().clone().permute(0, 3, 1, 2)\n        mask = (vgrid[:, 0] > -1) & (vgrid[:, 1] > -\n                                     1) & (vgrid[:, 0] < 1) & (vgrid[:, 1] < 1)\n        return output, mask\n    return output", "\n\ndef estimate_probability_of_confidence_interval(uncert_output, R=1.0):\n    assert uncert_output.shape[1] == 1\n    var = torch.exp(uncert_output)\n    p_r = 1.0 - torch.exp(-R ** 2 / (2 * var))\n    return p_r\n"]}
{"filename": "models/backbones/mix_transformer.py", "chunked_list": ["# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/NVlabs/SegFormer.\n#\nimport collections.abc\nimport math\nimport os\nimport warnings\nfrom functools import partial", "import warnings\nfrom functools import partial\nfrom itertools import repeat\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nmodel_urls = {\n    \"imagenet\": {", "model_urls = {\n    \"imagenet\": {\n        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/mit_b5.pth\",\n    },\n    \"cityscapes\": {\n        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/segformer.b5.1024x1024.city.160k.pth\",\n    }\n}", "    }\n}\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor", "\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n", "\n\nto_2tuple = _ntuple(2)\n\n\nclass Mlp(nn.Module):\n\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x", "\n\nclass Attention(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.,\n                 proj_drop=0.,\n                 sr_ratio=1):\n        super().__init__()\n        assert dim % num_heads == 0, f'dim {dim} should be divided by ' \\\n                                     f'num_heads {num_heads}.'\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(\n                dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads,\n                              C // self.num_heads).permute(0, 2, 1,\n                                                           3).contiguous()\n\n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).contiguous().reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1).contiguous()\n            x_ = self.norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads,\n                                     C // self.num_heads).permute(\n                                         2, 0, 3, 1, 4).contiguous()\n        else:\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads,\n                                    C // self.num_heads).permute(\n                                        2, 0, 3, 1, 4).contiguous()\n        k, v = kv[0], kv[1]\n\n        attn = (q @ k.transpose(-2, -1).contiguous()) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).contiguous().reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 num_heads,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 sr_ratio=1):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n            sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better\n        # than dropout here\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop)\n\n    def forward(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\n        return x", "\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\"\"\"\n\n    def __init__(self,\n                 img_size=224,\n                 patch_size=7,\n                 stride=4,\n                 in_chans=3,\n                 embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[\n            1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=stride,\n            padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2).contiguous()\n        x = self.norm(x)\n\n        return x, H, W", "\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n    @staticmethod\n    def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n        \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n        This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n        the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n        See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n        changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n        'survival rate' as the argument.\n        \"\"\"\n        if drop_prob == 0. or not training:\n            return x\n        keep_prob = 1 - drop_prob\n        # work with diff dim tensors, not just 2D ConvNets\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n        if keep_prob > 0.0 and scale_by_keep:\n            random_tensor.div_(keep_prob)\n        return x * random_tensor", "\n\nclass MixVisionTransformer(nn.Module):\n\n    arch_settings = {\n        'mit_b0': {\n            'patch_size': 4,\n            'embed_dims': [32, 64, 160, 256],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [2, 2, 2, 2],\n            'sr_ratios': [8, 4, 2, 1],\n        },\n        'mit_b1': {\n            'patch_size': 4,\n            'embed_dims': [64, 128, 320, 512],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [2, 2, 2, 2],\n            'sr_ratios': [8, 4, 2, 1],\n        },\n        'mit_b2': {\n            'patch_size': 4,\n            'embed_dims': [64, 128, 320, 512],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [3, 4, 6, 3],\n            'sr_ratios': [8, 4, 2, 1],\n        },\n        'mit_b3': {\n            'patch_size': 4,\n            'embed_dims': [64, 128, 320, 512],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [3, 4, 18, 3],\n            'sr_ratios': [8, 4, 2, 1],\n        },\n        'mit_b4': {\n            'patch_size': 4,\n            'embed_dims': [64, 128, 320, 512],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [3, 8, 27, 3],\n            'sr_ratios': [8, 4, 2, 1],\n        },\n        'mit_b5': {\n            'patch_size': 4,\n            'embed_dims': [64, 128, 320, 512],\n            'num_heads': [1, 2, 5, 8],\n            'mlp_ratios': [4, 4, 4, 4],\n            'qkv_bias': True,\n            'norm_layer': partial(nn.LayerNorm, eps=1e-6),\n            'depths': [3, 6, 40, 3],\n            'sr_ratios': [8, 4, 2, 1],\n        }\n    }\n\n    def __init__(self,\n                 model_type: str,\n                 pretrained: Optional[str] = None,\n                 img_size: int = 224,\n                 in_chans: int = 3,\n                 qk_scale: Optional[float] = None,\n                 drop_rate: float = 0.,\n                 attn_drop_rate: float = 0.,\n                 drop_path_rate: float = 0.1,\n                 freeze_patch_embed: bool = False):\n        super().__init__()\n\n        embed_dims = self.arch_settings[model_type]['embed_dims']\n        num_heads = self.arch_settings[model_type]['num_heads']\n        mlp_ratios = self.arch_settings[model_type]['mlp_ratios']\n        qkv_bias = self.arch_settings[model_type]['qkv_bias']\n        norm_layer = self.arch_settings[model_type]['norm_layer']\n        depths = self.arch_settings[model_type]['depths']\n        sr_ratios = self.arch_settings[model_type]['sr_ratios']\n        self.model_type = model_type\n        self.depths = depths\n\n        # patch_embed\n        self.patch_embed1 = OverlapPatchEmbed(\n            img_size=img_size,\n            patch_size=7,\n            stride=4,\n            in_chans=in_chans,\n            embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(\n            img_size=img_size // 4,\n            patch_size=3,\n            stride=2,\n            in_chans=embed_dims[0],\n            embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(\n            img_size=img_size // 8,\n            patch_size=3,\n            stride=2,\n            in_chans=embed_dims[1],\n            embed_dim=embed_dims[2])\n        self.patch_embed4 = OverlapPatchEmbed(\n            img_size=img_size // 16,\n            patch_size=3,\n            stride=2,\n            in_chans=embed_dims[2],\n            embed_dim=embed_dims[3])\n        if freeze_patch_embed:\n            self.freeze_patch_emb()\n\n        # transformer encoder\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n        cur = 0\n        self.block1 = nn.ModuleList([\n            Block(\n                dim=embed_dims[0],\n                num_heads=num_heads[0],\n                mlp_ratio=mlp_ratios[0],\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[cur + i],\n                norm_layer=norm_layer,\n                sr_ratio=sr_ratios[0]) for i in range(depths[0])\n        ])\n        self.norm1 = norm_layer(embed_dims[0])\n\n        cur += depths[0]\n        self.block2 = nn.ModuleList([\n            Block(\n                dim=embed_dims[1],\n                num_heads=num_heads[1],\n                mlp_ratio=mlp_ratios[1],\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[cur + i],\n                norm_layer=norm_layer,\n                sr_ratio=sr_ratios[1]) for i in range(depths[1])\n        ])\n        self.norm2 = norm_layer(embed_dims[1])\n\n        cur += depths[1]\n        self.block3 = nn.ModuleList([\n            Block(\n                dim=embed_dims[2],\n                num_heads=num_heads[2],\n                mlp_ratio=mlp_ratios[2],\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[cur + i],\n                norm_layer=norm_layer,\n                sr_ratio=sr_ratios[2]) for i in range(depths[2])\n        ])\n        self.norm3 = norm_layer(embed_dims[2])\n\n        cur += depths[2]\n        self.block4 = nn.ModuleList([\n            Block(\n                dim=embed_dims[3],\n                num_heads=num_heads[3],\n                mlp_ratio=mlp_ratios[3],\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[cur + i],\n                norm_layer=norm_layer,\n                sr_ratio=sr_ratios[3]) for i in range(depths[3])\n        ])\n        self.norm4 = norm_layer(embed_dims[3])\n\n        self.init_weights(pretrained=pretrained)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def init_weights(self, pretrained=None):\n        if pretrained is None:\n            for m in self.modules():\n                self._init_weights(m)\n        else:\n            if pretrained == 'imagenet':\n                pretrained = model_urls['imagenet'][self.model_type]\n            elif pretrained == 'cityscapes':\n                pretrained = model_urls['cityscapes'][self.model_type]\n\n            if os.path.exists(pretrained):\n                checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n            elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n                checkpoint = torch.load(os.path.join(os.environ.get(\n                    'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n            else:\n                checkpoint = torch.hub.load_state_dict_from_url(\n                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n            if 'state_dict' in checkpoint:\n                state_dict = checkpoint['state_dict']\n            else:\n                state_dict = checkpoint\n\n            if any(el.startswith('backbone.') for el in state_dict.keys()):\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith('backbone.'):\n                        new_k = k.replace('backbone.', '')\n                        new_state_dict[new_k] = v\n                state_dict = new_state_dict\n            state_dict = {k: v for k, v in state_dict.items()\n                          if not k.startswith('head.')}\n            self.load_state_dict(state_dict, strict=True)\n\n    def reset_drop_path(self, drop_path_rate):\n        dpr = [\n            x.item()\n            for x in torch.linspace(0, drop_path_rate, sum(self.depths))\n        ]\n        cur = 0\n        for i in range(self.depths[0]):\n            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[0]\n        for i in range(self.depths[1]):\n            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[1]\n        for i in range(self.depths[2]):\n            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[2]\n        for i in range(self.depths[3]):\n            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n\n    def freeze_patch_emb(self):\n        self.patch_embed1.requires_grad = False\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\n            'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'\n        }  # has pos_embed may be better\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        outs = []\n\n        # stage 1\n        x, H, W = self.patch_embed1(x)\n        for i, blk in enumerate(self.block1):\n            x = blk(x, H, W)\n        x = self.norm1(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 2\n        x, H, W = self.patch_embed2(x)\n        for i, blk in enumerate(self.block2):\n            x = blk(x, H, W)\n        x = self.norm2(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 3\n        x, H, W = self.patch_embed3(x)\n        for i, blk in enumerate(self.block3):\n            x = blk(x, H, W)\n        x = self.norm3(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 4\n        x, H, W = self.patch_embed4(x)\n        for i, blk in enumerate(self.block4):\n            x = blk(x, H, W)\n        x = self.norm4(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        return outs\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        # x = self.head(x)\n\n        return x", "\n\nclass DWConv(nn.Module):\n\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2).contiguous()\n\n        return x", ""]}
{"filename": "models/backbones/__init__.py", "chunked_list": ["from .mix_transformer import MixVisionTransformer\nfrom .resnet import ResNet\nfrom .vgg import VGG\n"]}
{"filename": "models/backbones/vgg.py", "chunked_list": ["# Copyright (c) Prune Truong. All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/PruneTruong/DenseMatching.\n#\nimport os\nfrom typing import Dict, List, Optional, Union, cast\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\n\nmodel_urls = {\n    \"vgg11\": \"https://download.pytorch.org/models/vgg11-8a719046.pth\",\n    \"vgg13\": \"https://download.pytorch.org/models/vgg13-19584684.pth\",\n    \"vgg16\": \"https://download.pytorch.org/models/vgg16-397923af.pth\",\n    \"vgg19\": \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\",\n    \"vgg11_bn\": \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\",\n    \"vgg13_bn\": \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\",", "    \"vgg11_bn\": \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\",\n    \"vgg13_bn\": \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\",\n    \"vgg16_bn\": \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\",\n    \"vgg19_bn\": \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\",\n}\n\n\ncfgs: Dict[str, List[Union[str, int]]] = {\n    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],", "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n    \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n    \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n}\n\n\nclass VGG(nn.Module):\n\n    arch_settings = {\n        'vgg11': {\n            'cfg': 'A',\n            'batch_norm': False,\n        },\n        'vgg11_bn': {\n            'cfg': 'A',\n            'batch_norm': True,\n        },\n        'vgg13': {\n            'cfg': 'B',\n            'batch_norm': False,\n        },\n        'vgg13_bn': {\n            'cfg': 'B',\n            'batch_norm': True,\n        },\n        'vgg16': {\n            'cfg': 'D',\n            'batch_norm': False,\n        },\n        'vgg16_bn': {\n            'cfg': 'D',\n            'batch_norm': True,\n        },\n        'vgg19': {\n            'cfg': 'E',\n            'batch_norm': False,\n        },\n        'vgg19_bn': {\n            'cfg': 'E',\n            'batch_norm': True,\n        }\n    }\n\n    def __init__(\n        self, model_type: str, out_indices: list = [0, 1, 2, 3, 4, 5], pretrained: Optional[str] = None\n    ) -> None:\n        super().__init__()\n        self.model_type = model_type\n        cfg = self.arch_settings[model_type]['cfg']\n        batch_norm = self.arch_settings[model_type]['batch_norm']\n        self.features, layer_indices = self._make_layers(\n            cfgs[cfg], batch_norm=batch_norm)\n        self.layer_indices = [layer_indices[i] for i in out_indices]\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if pretrained is not None:\n            if pretrained == 'imagenet':\n                pretrained = model_urls[self.model_type]\n\n            if os.path.exists(pretrained):\n                checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n            else:\n                checkpoint = torch.hub.load_state_dict_from_url(\n                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n            if 'state_dict' in checkpoint.keys():\n                state_dict = checkpoint['state_dict']\n            else:\n                state_dict = checkpoint\n            state_dict = {k: v for k, v in state_dict.items(\n            ) if not k.startswith('classifier.')}\n            self.load_state_dict(state_dict, strict=True)\n\n    def forward(self, x: torch.Tensor, extract_only_indices=None) -> torch.Tensor:\n        if extract_only_indices:\n            layer_indices = [self.layer_indices[i]\n                             for i in extract_only_indices]\n        else:\n            layer_indices = self.layer_indices\n        prev_i = 0\n        outs = []\n        for i in layer_indices:\n            x = self.features[prev_i:i](x)\n            outs.append(x)\n            prev_i = i\n        return outs\n\n    @staticmethod\n    def _make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n        layers: List[nn.Module] = []\n        in_channels = 3\n\n        current_idx = 0\n        layer_indices = []\n        first_relu = True\n        for v in cfg:\n            if v == \"M\":\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n                current_idx += 1\n                layer_indices.append(current_idx)\n            else:\n                v = cast(int, v)\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n                if batch_norm:\n                    layers += [conv2d,\n                               nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                    current_idx += 3\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                    current_idx += 2\n                in_channels = v\n                if first_relu:\n                    first_relu = False\n                    layer_indices.append(current_idx)\n        return nn.Sequential(*layers), layer_indices", ""]}
{"filename": "models/backbones/resnet.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n#\nimport os\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nmodel_urls = {\n    \"imagenet\": {\n        \"resnet18_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet18_v1c-b5776b93.pth\",\n        \"resnet50_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet50_v1c-2cccc1ad.pth\",\n        \"resnet101_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet101_v1c-e67eebb6.pth\",\n    },", "        \"resnet101_v1c\": \"https://download.openmmlab.com/pretrain/third_party/resnet101_v1c-e67eebb6.pth\",\n    },\n    \"cityscapes\": {\n        \"resnet101_v1c\": \"https://data.vision.ee.ethz.ch/brdavid/cma/deeplabv2_cityscapes.pth\"\n    }\n}\n\n\nclass ResLayer(nn.Sequential):\n    \"\"\"ResLayer to build ResNet style backbone.\n    Args:\n        block (nn.Module): Residual block used to build ResLayer.\n        num_blocks (int): Number of blocks.\n        in_channels (int): Input channels of this block.\n        out_channels (int): Output channels of this block.\n        expansion (int, optional): The expansion for BasicBlock/Bottleneck.\n            If not specified, it will firstly be obtained via\n            ``block.expansion``. If the block has no attribute \"expansion\",\n            the following default values will be used: 1 for BasicBlock and\n            4 for Bottleneck. Default: None.\n        stride (int): stride of the first block. Default: 1.\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck. Default: False\n        conv_cfg (dict, optional): dictionary to construct and config conv\n            layer. Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='BN')\n    \"\"\"\n\n    def __init__(self,\n                 block,\n                 num_blocks,\n                 in_channels,\n                 out_channels,\n                 stride=1,\n                 dilation=1,\n                 avg_down=False,\n                 norm_layer=nn.BatchNorm2d,\n                 contract_dilation=False,\n                 **kwargs):\n        self.block = block\n\n        downsample = None\n        if stride != 1 or in_channels != out_channels * block.expansion:\n            downsample = []\n            conv_stride = stride\n            if avg_down and stride != 1:\n                conv_stride = 1\n                downsample.append(\n                    nn.AvgPool2d(\n                        kernel_size=stride,\n                        stride=stride,\n                        ceil_mode=True,\n                        count_include_pad=False))\n            downsample.extend([\n                nn.Conv2d(\n                    in_channels,\n                    out_channels * block.expansion,\n                    kernel_size=1,\n                    stride=conv_stride,\n                    bias=False),\n                norm_layer(out_channels * block.expansion)\n            ])\n            downsample = nn.Sequential(*downsample)\n\n        layers = []\n        if dilation > 1 and contract_dilation:\n            first_dilation = dilation // 2\n        else:\n            first_dilation = dilation\n        layers.append(\n            block(\n                inplanes=in_channels,\n                planes=out_channels,\n                stride=stride,\n                dilation=first_dilation,\n                downsample=downsample,\n                norm_layer=norm_layer,\n                **kwargs))\n        in_channels = out_channels * block.expansion\n        for _ in range(1, num_blocks):\n            layers.append(\n                block(\n                    inplanes=in_channels,\n                    planes=out_channels,\n                    stride=1,\n                    dilation=dilation,\n                    norm_layer=norm_layer,\n                    **kwargs))\n        super(ResLayer, self).__init__(*layers)", "class ResLayer(nn.Sequential):\n    \"\"\"ResLayer to build ResNet style backbone.\n    Args:\n        block (nn.Module): Residual block used to build ResLayer.\n        num_blocks (int): Number of blocks.\n        in_channels (int): Input channels of this block.\n        out_channels (int): Output channels of this block.\n        expansion (int, optional): The expansion for BasicBlock/Bottleneck.\n            If not specified, it will firstly be obtained via\n            ``block.expansion``. If the block has no attribute \"expansion\",\n            the following default values will be used: 1 for BasicBlock and\n            4 for Bottleneck. Default: None.\n        stride (int): stride of the first block. Default: 1.\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck. Default: False\n        conv_cfg (dict, optional): dictionary to construct and config conv\n            layer. Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='BN')\n    \"\"\"\n\n    def __init__(self,\n                 block,\n                 num_blocks,\n                 in_channels,\n                 out_channels,\n                 stride=1,\n                 dilation=1,\n                 avg_down=False,\n                 norm_layer=nn.BatchNorm2d,\n                 contract_dilation=False,\n                 **kwargs):\n        self.block = block\n\n        downsample = None\n        if stride != 1 or in_channels != out_channels * block.expansion:\n            downsample = []\n            conv_stride = stride\n            if avg_down and stride != 1:\n                conv_stride = 1\n                downsample.append(\n                    nn.AvgPool2d(\n                        kernel_size=stride,\n                        stride=stride,\n                        ceil_mode=True,\n                        count_include_pad=False))\n            downsample.extend([\n                nn.Conv2d(\n                    in_channels,\n                    out_channels * block.expansion,\n                    kernel_size=1,\n                    stride=conv_stride,\n                    bias=False),\n                norm_layer(out_channels * block.expansion)\n            ])\n            downsample = nn.Sequential(*downsample)\n\n        layers = []\n        if dilation > 1 and contract_dilation:\n            first_dilation = dilation // 2\n        else:\n            first_dilation = dilation\n        layers.append(\n            block(\n                inplanes=in_channels,\n                planes=out_channels,\n                stride=stride,\n                dilation=first_dilation,\n                downsample=downsample,\n                norm_layer=norm_layer,\n                **kwargs))\n        in_channels = out_channels * block.expansion\n        for _ in range(1, num_blocks):\n            layers.append(\n                block(\n                    inplanes=in_channels,\n                    planes=out_channels,\n                    stride=1,\n                    dilation=dilation,\n                    norm_layer=norm_layer,\n                    **kwargs))\n        super(ResLayer, self).__init__(*layers)", "\n\nclass BasicBlock(nn.Module):\n    \"\"\"BasicBlock for ResNet.\n    Args:\n        in_channels (int): Input channels of this block.\n        out_channels (int): Output channels of this block.\n        expansion (int): The ratio of ``out_channels/mid_channels`` where\n            ``mid_channels`` is the output channels of conv1. This is a\n            reserved argument in BasicBlock and should always be 1. Default: 1.\n        stride (int): stride of the block. Default: 1\n        dilation (int): dilation of convolution. Default: 1\n        downsample (nn.Module, optional): downsample operation on identity\n            branch. Default: None.\n        style (str): `pytorch` or `caffe`. It is unused and reserved for\n            unified API with Bottleneck.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        conv_cfg (dict, optional): dictionary to construct and config conv\n            layer. Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='BN')\n    \"\"\"\n\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style='pytorch',\n                 with_cp=False,\n                 norm_layer=nn.BatchNorm2d,\n                 **kwargs):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(\n            planes,\n            planes,\n            3,\n            padding=1,\n            bias=False)\n        self.bn2 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.out_channels = planes\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.bn2(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out", "\n\nclass Bottleneck(nn.Module):\n    \"\"\"Bottleneck block for ResNet.\n    Args:\n        in_channels (int): Input channels of this block.\n        out_channels (int): Output channels of this block.\n        expansion (int): The ratio of ``out_channels/mid_channels`` where\n            ``mid_channels`` is the input/output channels of conv2. Default: 4.\n        stride (int): stride of the block. Default: 1\n        dilation (int): dilation of convolution. Default: 1\n        downsample (nn.Module, optional): downsample operation on identity\n            branch. Default: None.\n        style (str): ``\"pytorch\"`` or ``\"caffe\"``. If set to \"pytorch\", the\n            stride-two layer is the 3x3 conv layer, otherwise the stride-two\n            layer is the first 1x1 conv layer. Default: \"pytorch\".\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        conv_cfg (dict, optional): dictionary to construct and config conv\n            layer. Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='BN')\n    \"\"\"\n\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style='pytorch',\n                 with_cp=False,\n                 norm_layer=nn.BatchNorm2d,\n                 **kwargs):\n        super(Bottleneck, self).__init__()\n        assert style in ['pytorch', 'caffe']\n        self.stride = stride\n        self.dilation = dilation\n        self.style = style\n        self.with_cp = with_cp\n        self.out_channels = planes * self.expansion\n\n        if self.style == 'pytorch':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.bn1 = norm_layer(planes)\n        self.bn2 = norm_layer(planes)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.conv2 = nn.Conv2d(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=self.conv2_stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.conv3 = nn.Conv2d(\n            planes,\n            planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.bn3(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out", "\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet backbone.\n    Please refer to the `paper <https://arxiv.org/abs/1512.03385>`__ for\n    details.\n    Args:\n        depth (int): Network depth, from {18, 34, 50, 101, 152}.\n        in_channels (int): Number of input image channels. Default: 3.\n        stem_channels (int): Output channels of the stem layer. Default: 64.\n        base_channels (int): Middle channels of the first stage. Default: 64.\n        num_stages (int): Stages of the network. Default: 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n            Default: ``(1, 2, 2, 2)``.\n        dilations (Sequence[int]): Dilation of each stage.\n            Default: ``(1, 1, 1, 1)``.\n        out_indices (Sequence[int]): Output from which stages. If only one\n            stage is specified, a single tensor (feature map) is returned,\n            otherwise multiple stages are specified, a tuple of tensors will\n            be returned. Default: ``(3, )``.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv.\n            Default: False.\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters. Default: -1.\n        conv_cfg (dict | None): The config dict for conv layers. Default: None.\n        norm_cfg (dict): The config dict for norm layers.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only. Default: False.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed. Default: False.\n        zero_init_residual (bool): Whether to use zero init for last norm layer\n            in resblocks to let them behave as identity. Default: True.\n    Example:\n        >>> from mmcls.models import ResNet\n        >>> import torch\n        >>> self = ResNet(depth=18)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 64, 8, 8)\n        (1, 128, 4, 4)\n        (1, 256, 2, 2)\n        (1, 512, 1, 1)\n    \"\"\"\n\n    arch_settings = {\n        'resnet18_v1c': {\n            'depth': 18,\n            'block': BasicBlock,\n            'stage_blocks': (2, 2, 2, 2),\n            'deep_stem': True,\n            'avg_down': False,\n            'cifar_version': False\n        },\n        'resnet50_v1c': {\n            'depth': 50,\n            'block': Bottleneck,\n            'stage_blocks': (3, 4, 6, 3),\n            'deep_stem': True,\n            'avg_down': False,\n            'cifar_version': False\n        },\n        'resnet101_v1': {\n            'depth': 101,\n            'block': Bottleneck,\n            'stage_blocks': (3, 4, 23, 3),\n            'deep_stem': False,\n            'avg_down': False,\n            'cifar_version': False\n        },\n        'resnet101_v1c': {\n            'depth': 101,\n            'block': Bottleneck,\n            'stage_blocks': (3, 4, 23, 3),\n            'deep_stem': True,\n            'avg_down': False,\n            'cifar_version': False\n        },\n    }\n\n    def __init__(self,\n                 model_type: str,\n                 pretrained: Optional[str] = None,\n                 in_channels=3,\n                 stem_channels=64,\n                 base_channels=64,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style='pytorch',\n                 frozen_stages=-1,\n                 norm_eval=False,\n                 with_cp=False,\n                 zero_init_residual=True,\n                 contract_dilation=False,\n                 max_pool_ceil_mode=False):\n        norm_layer = nn.BatchNorm2d\n        super(ResNet, self).__init__()\n        self.model_type = model_type\n        self.depth = self.arch_settings[model_type]['depth']\n        self.stem_channels = stem_channels\n        self.base_channels = base_channels\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.deep_stem = self.arch_settings[model_type]['deep_stem']\n        self.avg_down = self.arch_settings[model_type]['avg_down']\n        self.frozen_stages = frozen_stages\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.zero_init_residual = zero_init_residual\n        self.block = self.arch_settings[model_type]['block']\n        stage_blocks = self.arch_settings[model_type]['stage_blocks']\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.contract_dilation = contract_dilation\n        self.max_pool_ceil_mode = max_pool_ceil_mode\n\n        self._make_stem_layer(in_channels, stem_channels, norm_layer)\n\n        self.res_layers = []\n        _in_channels = stem_channels\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            _out_channels = base_channels * 2**i\n            res_layer = self.make_res_layer(\n                block=self.block,\n                num_blocks=num_blocks,\n                in_channels=_in_channels,\n                out_channels=_out_channels,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                avg_down=self.avg_down,\n                with_cp=with_cp,\n                norm_layer=norm_layer,\n                contract_dilation=contract_dilation)\n            _in_channels = _out_channels * self.block.expansion\n            layer_name = f'layer{i + 1}'\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n        self.init_weights(pretrained=pretrained)\n\n    def make_res_layer(self, **kwargs):\n        return ResLayer(**kwargs)\n\n    def _make_stem_layer(self, in_channels, stem_channels, norm_layer):\n        if self.deep_stem:\n            self.stem = nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    bias=False),\n                norm_layer(stem_channels // 2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    stem_channels // 2,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False),\n                norm_layer(stem_channels // 2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    stem_channels // 2,\n                    stem_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False),\n                norm_layer(stem_channels),\n                nn.ReLU(inplace=True))\n        else:\n            self.conv1 = nn.Conv2d(\n                in_channels,\n                stem_channels,\n                kernel_size=7,\n                stride=2,\n                padding=3,\n                bias=False)\n            self.norm1 = norm_layer(stem_channels)\n            self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(\n            kernel_size=3, stride=2, padding=1, ceil_mode=self.max_pool_ceil_mode)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            if self.deep_stem:\n                self.stem.eval()\n                for param in self.stem.parameters():\n                    param.requires_grad = False\n            else:\n                self.norm1.eval()\n                for m in [self.conv1, self.norm1]:\n                    for param in m.parameters():\n                        param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, f'layer{i}')\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if self.zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, (Bottleneck)):\n                    # type: ignore[arg-type]\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, (BasicBlock)):\n                    # type: ignore[arg-type]\n                    nn.init.constant_(m.bn2.weight, 0)\n\n        if pretrained is not None:\n            if pretrained == 'imagenet':\n                pretrained = model_urls['imagenet'][self.model_type]\n            elif pretrained == 'cityscapes':\n                pretrained = model_urls['cityscapes'][self.model_type]\n\n            if os.path.exists(pretrained):\n                checkpoint = torch.load(\n                    pretrained, map_location=lambda storage, loc: storage)\n            elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n                checkpoint = torch.load(os.path.join(\n                    os.environ.get('TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n            else:\n                checkpoint = torch.hub.load_state_dict_from_url(\n                    pretrained, progress=True, map_location=lambda storage, loc: storage)\n            if 'state_dict' in checkpoint.keys():\n                state_dict = checkpoint['state_dict']\n            else:\n                state_dict = checkpoint\n\n            if any(el.startswith('backbone.') for el in state_dict.keys()):\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith('backbone.'):\n                        new_k = k.replace('backbone.', '')\n                        new_state_dict[new_k] = v\n                state_dict = new_state_dict\n            state_dict = {k: v for k, v in state_dict.items()\n                          if not k.startswith('fc.')}\n            self.load_state_dict(state_dict, strict=True)\n\n    def forward(self, x):\n        if self.deep_stem:\n            x = self.stem(x)\n        else:\n            x = self.conv1(x)\n            x = self.norm1(x)\n            x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n\n        return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()", ""]}
{"filename": "models/correlation_ops/__init__.py", "chunked_list": ["import os\n\nimport torch\nfrom torch.utils import cpp_extension\n\ncwd = os.path.dirname(os.path.realpath(__file__))\n\nsources = []\nsources.append(os.path.join(cwd, 'correlation.cpp'))\n\nif torch.cuda.is_available():\n    sources.append(os.path.join(cwd, 'correlation_sampler.cpp'))\n    sources.append(os.path.join(cwd, 'correlation_cuda_kernel.cu'))\n    correlation = cpp_extension.load('correlation',\n                                        sources=sources,\n                                        build_directory=cwd,\n                                        extra_cflags=['-fopenmp'],\n                                        extra_ldflags=['-lgomp'],\n                                        with_cuda=True,\n                                        verbose=False)\nelse:\n    # CPU only version\n    sources.append(os.path.join(cwd, 'correlation_sampler_cpu.cpp'))\n    correlation = cpp_extension.load('correlation',\n                                        sources=sources,\n                                        build_directory=cwd,\n                                        extra_cflags=['-fopenmp'],\n                                        extra_ldflags=['-lgomp'],\n                                        with_cuda=False,\n                                        verbose=False)", "sources.append(os.path.join(cwd, 'correlation.cpp'))\n\nif torch.cuda.is_available():\n    sources.append(os.path.join(cwd, 'correlation_sampler.cpp'))\n    sources.append(os.path.join(cwd, 'correlation_cuda_kernel.cu'))\n    correlation = cpp_extension.load('correlation',\n                                        sources=sources,\n                                        build_directory=cwd,\n                                        extra_cflags=['-fopenmp'],\n                                        extra_ldflags=['-lgomp'],\n                                        with_cuda=True,\n                                        verbose=False)\nelse:\n    # CPU only version\n    sources.append(os.path.join(cwd, 'correlation_sampler_cpu.cpp'))\n    correlation = cpp_extension.load('correlation',\n                                        sources=sources,\n                                        build_directory=cwd,\n                                        extra_cflags=['-fopenmp'],\n                                        extra_ldflags=['-lgomp'],\n                                        with_cuda=False,\n                                        verbose=False)", ""]}
{"filename": "models/correlation_ops/correlation_function.py", "chunked_list": ["# Copyright (c) Clemend Pinard. All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/ClementPinard/Pytorch-Correlation-extension.\n#\nimport torch\nfrom torch.autograd.function import once_differentiable\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom torch.nn.modules.utils import _pair\n", "from torch.nn.modules.utils import _pair\n\nfrom .. import correlation_ops\n\n\ndef spatial_correlation_sample(input1,\n                               input2,\n                               kernel_size=1,\n                               patch_size=1,\n                               stride=1,\n                               padding=0,\n                               dilation=1,\n                               dilation_patch=1):\n    \"\"\"Apply spatial correlation sampling on from input1 to input2,\n    Every parameter except input1 and input2 can be either single int\n    or a pair of int. For more information about Spatial Correlation\n    Sampling, see this page.\n    https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/\n    Args:\n        input1 : The first parameter.\n        input2 : The second parameter.\n        kernel_size : total size of your correlation kernel, in pixels\n        patch_size : total size of your patch, determining how many\n            different shifts will be applied\n        stride : stride of the spatial sampler, will modify output\n            height and width\n        padding : padding applied to input1 and input2 before applying\n            the correlation sampling, will modify output height and width\n        dilation_patch : step for every shift in patch\n    Returns:\n        Tensor: Result of correlation sampling\n    \"\"\"\n    return SpatialCorrelationSamplerFunction.apply(input1, input2,\n                                                   kernel_size, patch_size,\n                                                   stride, padding, dilation, dilation_patch)", "\n\nclass SpatialCorrelationSamplerFunction(torch.autograd.Function):\n    ''' For AMP, we need to cast to float32\n    '''\n\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx,\n                input1,\n                input2,\n                kernel_size=1,\n                patch_size=1,\n                stride=1,\n                padding=0,\n                dilation=1,\n                dilation_patch=1):\n        ctx.save_for_backward(input1, input2)\n        kH, kW = ctx.kernel_size = _pair(kernel_size)\n        patchH, patchW = ctx.patch_size = _pair(patch_size)\n        padH, padW = ctx.padding = _pair(padding)\n        dilationH, dilationW = ctx.dilation = _pair(dilation)\n        dilation_patchH, dilation_patchW = ctx.dilation_patch = _pair(\n            dilation_patch)\n        dH, dW = ctx.stride = _pair(stride)\n\n        output = correlation_ops.correlation.forward(input1, input2,\n                                                     kH, kW, patchH, patchW,\n                                                     padH, padW, dilationH, dilationW,\n                                                     dilation_patchH, dilation_patchW,\n                                                     dH, dW)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    @custom_bwd\n    def backward(ctx, grad_output):\n        input1, input2 = ctx.saved_tensors\n        kH, kW = ctx.kernel_size\n        patchH, patchW = ctx.patch_size\n        padH, padW = ctx.padding\n        dilationH, dilationW = ctx.dilation\n        dilation_patchH, dilation_patchW = ctx.dilation_patch\n        dH, dW = ctx.stride\n\n        grad_input1, grad_input2 = correlation_ops.correlation.backward(input1, input2, grad_output,\n                                                                        kH, kW, patchH, patchW,\n                                                                        padH, padW, dilationH, dilationW,\n                                                                        dilation_patchH, dilation_patchW,\n                                                                        dH, dW)\n        return grad_input1, grad_input2, None, None, None, None, None, None", ""]}
{"filename": "models/heads/projection.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\n# \n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/open-mmlab/mmsegmentation.\n#\nfrom typing import List, Optional, Sequence, Union\n\nimport torch.nn as nn\n\nfrom .base import BaseHead", "\nfrom .base import BaseHead\nfrom .modules import ConvBNReLU\n\n\nclass ProjectionHead(BaseHead):\n    \"\"\"Projection Head for feature dimension reduction in contrastive loss.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels: Union[int, List[int]],\n                 in_index: Union[List[int], int],\n                 input_transform: Optional[str] = None,\n                 channels: int = 128,\n                 kernel_size: int = 1):\n        super().__init__(-1, in_index, input_transform)\n        self.in_channels = in_channels\n        self.channels = channels\n        self.kernel_size = kernel_size\n\n        if self.input_transform == 'multiple_select':\n            assert isinstance(self.in_channels, Sequence)\n            self.convs = nn.ModuleList([])\n            for i in range(len(self.in_channels)):\n                self.convs.append(nn.Sequential(\n                    ConvBNReLU(\n                        self.in_channels[i],\n                        self.in_channels[i],\n                        kernel_size=kernel_size,\n                        norm_layer=None,\n                        activation_layer=nn.ReLU),\n                    ConvBNReLU(\n                        self.in_channels[i],\n                        self.channels,\n                        kernel_size=kernel_size,\n                        norm_layer=None,\n                        activation_layer=None)))\n        else:\n            if self.input_transform == 'resize_concat':\n                if isinstance(self.in_channels, Sequence):\n                    in_channels = sum(self.in_channels)\n                else:\n                    in_channels = self.in_channels\n            else:\n                in_channels = self.in_channels\n            self.convs = nn.Sequential(\n                ConvBNReLU(\n                    in_channels,\n                    in_channels,\n                    kernel_size=kernel_size,\n                    norm_layer=None,\n                    activation_layer=nn.ReLU),\n                ConvBNReLU(\n                    in_channels,\n                    self.channels,\n                    kernel_size=kernel_size,\n                    norm_layer=None,\n                    activation_layer=None))\n\n        # mmseg init\n        for m in self.modules():\n            # initialize ConvBNReLU as in mmsegmentation\n            if isinstance(m, ConvBNReLU) and not m.depthwise_separable:\n                nn.init.kaiming_normal_(\n                    m.conv.weight, a=0, mode='fan_out', nonlinearity='relu')\n                if hasattr(m.conv, 'bias') and m.conv.bias is not None:\n                    nn.init.constant_(m.conv.bias, 0)\n                if m.use_norm:\n                    if hasattr(m.bn, 'weight') and m.bn.weight is not None:\n                        nn.init.constant_(m.bn.weight, 1)\n                    if hasattr(m.bn, 'bias') and m.bn.bias is not None:\n                        nn.init.constant_(m.bn.bias, 0)\n        nn.init.normal_(\n            self.convs[-1].conv.weight, mean=0, std=0.01)\n        nn.init.constant_(self.convs[-1].conv.bias, 0)\n\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        x = self._transform_inputs(inputs)\n        if isinstance(x, list):\n            # multiple_select\n            output = [self.convs[i](x[i]) for i in range(len(x))]\n        else:\n            # resize_concat or single_select\n            output = self.convs(x)\n        return output", ""]}
{"filename": "models/heads/segformer.py", "chunked_list": ["# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in https://github.com/NVlabs/SegFormer.\n#\nimport os\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\n\nfrom .base import BaseHead\nfrom .modules import ConvBNReLU\n\nmodel_urls = {\n    \"cityscapes\": {\n        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/segformer.b5.1024x1024.city.160k.pth\",", "        # same weights as provided by official SegFormer repo: https://github.com/NVlabs/SegFormer\n        \"mit_b5\": \"https://data.vision.ee.ethz.ch/brdavid/refign/segformer.b5.1024x1024.city.160k.pth\",\n    }\n}\n\n\nclass MLP(nn.Module):\n    \"\"\"\n    Linear Embedding\n    \"\"\"\n    def __init__(self, input_dim=2048, embed_dim=768):\n        super().__init__()\n        self.proj = nn.Linear(input_dim, embed_dim)\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x", "\n\nclass SegFormerHead(BaseHead):\n    \"\"\"\n    SegFormer: Simple and Efficient Design for Semantic Segmentation with\n    Transformers\n    \"\"\"\n\n    def __init__(self,\n                 in_channels: List[int],\n                 in_index: Union[List[int], int],\n                 num_classes: int,\n                 input_transform: Optional[str] = None,\n                 channels: int = 256,\n                 dropout_ratio: float = 0.1,\n                 pretrained: Optional[str] = None,\n                 ):\n        super().__init__(num_classes, in_index, input_transform)\n\n        self.in_channels = in_channels\n\n        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n        embedding_dim = channels\n\n        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n\n        self.linear_fuse = ConvBNReLU(\n            in_channels=embedding_dim*4,\n            out_channels=embedding_dim,\n            kernel_size=1,\n            norm_layer=nn.BatchNorm2d,\n        )\n\n        self.linear_pred = nn.Conv2d(\n            embedding_dim, self.num_classes, kernel_size=1)\n\n        if dropout_ratio > 0:\n            self.dropout = nn.Dropout2d(dropout_ratio)\n        else:\n            self.dropout = None\n\n        self.init_weights()\n        self.load_pretrained(pretrained)\n\n    def init_weights(self):\n        # mmseg init\n        nn.init.normal_(self.linear_pred.weight, mean=0, std=0.01)\n        nn.init.constant_(self.linear_pred.bias, 0)\n        for m in self.modules():\n            # initialize ConvBNReLU as in mmsegmentation\n            if isinstance(m, ConvBNReLU) and not m.depthwise_separable:\n                nn.init.kaiming_normal_(\n                    m.conv.weight, a=0, mode='fan_out', nonlinearity='relu')\n                if hasattr(m.conv, 'bias') and m.conv.bias is not None:\n                    nn.init.constant_(m.conv.bias, 0)\n                if m.use_norm:\n                    if hasattr(m.bn, 'weight') and m.bn.weight is not None:\n                        nn.init.constant_(m.bn.weight, 1)\n                    if hasattr(m.bn, 'bias') and m.bn.bias is not None:\n                        nn.init.constant_(m.bn.bias, 0)\n\n    def forward(self, inputs):\n        # Receive 4 stage backbone feature map: 1/4, 1/8, 1/16, 1/32\n\n        c1, c2, c3, c4 = inputs\n\n        ############## MLP decoder on C1-C4 ###########\n        n, _, h, w = c4.shape\n\n        _c4 = self.linear_c4(c4).permute(0, 2, 1).reshape(\n            n, -1, c4.shape[2], c4.shape[3])\n        _c4 = nn.functional.interpolate(\n            _c4, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\n        _c3 = self.linear_c3(c3).permute(0, 2, 1).reshape(\n            n, -1, c3.shape[2], c3.shape[3])\n        _c3 = nn.functional.interpolate(\n            _c3, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\n        _c2 = self.linear_c2(c2).permute(0, 2, 1).reshape(\n            n, -1, c2.shape[2], c2.shape[3])\n        _c2 = nn.functional.interpolate(\n            _c2, size=c1.size()[2:], mode='bilinear', align_corners=False)\n\n        _c1 = self.linear_c1(c1).permute(0, 2, 1).reshape(\n            n, -1, c1.shape[2], c1.shape[3])\n\n        x = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n\n        if self.dropout is not None:\n            x = self.dropout(x)\n\n        x = self.linear_pred(x)\n\n        return x\n\n    def load_pretrained(self, pretrained):\n        if pretrained is None:\n            return\n\n        if pretrained == 'cityscapes':\n            pretrained = model_urls['cityscapes']['mit_b5']\n\n        if os.path.exists(pretrained):\n            checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n            checkpoint = torch.load(os.path.join(os.environ.get(\n                'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n        else:\n            checkpoint = torch.hub.load_state_dict_from_url(\n                pretrained, progress=True, map_location=lambda storage, loc: storage)\n        if 'state_dict' in checkpoint:\n            state_dict = checkpoint['state_dict']\n        else:\n            state_dict = checkpoint\n\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if k.startswith('decode_head.'):\n                new_k = k.replace('decode_head.', '')\n                new_state_dict[new_k] = v\n            elif k.startswith('head.'):\n                new_k = k.replace('head.', '')\n                new_state_dict[new_k] = v\n            else:\n                pass\n        new_state_dict = {k: v for k, v in new_state_dict.items()\n                      if not k.startswith('conv_seg.')}\n        self.load_state_dict(new_state_dict, strict=True)", ""]}
{"filename": "models/heads/base.py", "chunked_list": ["from typing import Iterable, List, Optional, Union\n\nimport torch\nimport torch.nn as nn\n\n\nclass BaseHead(nn.Module):\n    def __init__(self,\n                 num_classes: int,\n                 in_index: Union[List[int], int],\n                 input_transform: Optional[str] = None):\n        super().__init__()\n        self.input_transform = input_transform\n        if isinstance(in_index, Iterable) and len(in_index) == 1:\n            self.in_index = in_index[0]\n        else:\n            self.in_index = in_index\n        self.num_classes = num_classes\n\n    def forward(self, inp):\n        raise NotImplementedError\n\n    def _transform_inputs(self, inputs):\n        \"\"\"Transform inputs for decoder.\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"\n        if self.input_transform == 'resize_concat':\n            inputs = [inputs[i] for i in self.in_index]\n            upsampled_inputs = [\n                nn.functional.interpolate(\n                    input=x,\n                    size=inputs[0].shape[2:],\n                    mode='bilinear',\n                    align_corners=False) for x in inputs\n            ]\n            inputs = torch.cat(upsampled_inputs, dim=1)\n        elif self.input_transform == 'multiple_select':\n            inputs = [inputs[i] for i in self.in_index]\n        else:\n            inputs = inputs[self.in_index]\n        return inputs", ""]}
{"filename": "models/heads/__init__.py", "chunked_list": ["from .deeplabv2 import DeepLabV2Head\nfrom .projection import ProjectionHead\nfrom .segformer import SegFormerHead\nfrom .uawarpc import UAWarpCHead\n"]}
{"filename": "models/heads/modules.py", "chunked_list": ["import torch.nn as nn\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1, padding=None,\n                 norm_layer=nn.BatchNorm2d, activation_layer=nn.ReLU, bias='auto',\n                 depthwise_separable=False, inplace=True, affine=True):\n        super().__init__()\n        padding = dilation * \\\n            (kernel_size - 1) // 2 if padding is None else padding\n        self.use_norm = norm_layer is not None\n        self.use_activation = activation_layer is not None\n        self.depthwise_separable = depthwise_separable\n        if bias == 'auto':\n            bias = not self.use_norm\n        if depthwise_separable:\n            assert kernel_size > 1\n            assert groups == 1  # not sure how to handle this\n            self.depthwise_conv = ConvBNReLU(in_channels, in_channels, kernel_size, stride=stride,\n                                             padding=padding, dilation=dilation, groups=in_channels,\n                                             norm_layer=norm_layer, activation_layer=activation_layer)\n            self.pointwise_conv = ConvBNReLU(in_channels, out_channels, 1,\n                                             norm_layer=norm_layer, activation_layer=activation_layer)\n        else:\n            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n                                  dilation=dilation, groups=groups, bias=bias)\n            if self.use_norm:\n                self.bn = norm_layer(out_channels, affine=affine)\n\n        if self.use_activation:\n            self.activation = activation_layer(inplace=inplace)\n\n    def forward(self, x):\n        if self.depthwise_separable:\n            x = self.depthwise_conv(x)\n            x = self.pointwise_conv(x)\n        else:\n            x = self.conv(x)\n            if self.use_norm:\n                x = self.bn(x)\n            if self.use_activation:\n                x = self.activation(x)\n        return x", ""]}
{"filename": "models/heads/uawarpc.py", "chunked_list": ["import math\nimport os\nfrom functools import partial\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom ..utils import warp\nfrom .base import BaseHead", "from ..utils import warp\nfrom .base import BaseHead\nfrom .modules import ConvBNReLU\n\nmodel_urls = {\n    # checkpoint from Refign: https://github.com/brdav/refign\n    \"megadepth\": \"https://data.vision.ee.ethz.ch/brdavid/refign/uawarpc_megadepth.ckpt\"\n}\n\n\nclass LocalFeatureCorrelationLayer(nn.Module):\n    def __init__(self, patch_size=9):\n        super().__init__()\n\n        try:\n            # If the PyTorch correlation module is installed,\n            # we can avoid JIT compilation of the extension:\n            # https://github.com/ClementPinard/Pytorch-Correlation-extension\n            from spatial_correlation_sampler import spatial_correlation_sample\n\n        except ModuleNotFoundError:\n            from models.correlation_ops.correlation_function import \\\n                spatial_correlation_sample\n\n        self.local_correlation = spatial_correlation_sample\n\n        self.patch_size = patch_size\n\n    def forward(self, feature_source, feature_target):\n        b = feature_target.shape[0]\n        corr = self.local_correlation(feature_target,\n                                      feature_source,\n                                      patch_size=self.patch_size)\n        corr = corr.view(b, self.patch_size * self.patch_size,\n                         feature_target.shape[2], feature_target.shape[3])\n        corr = nn.functional.normalize(nn.functional.relu(corr), p=2, dim=1)\n        return corr", "\n\nclass LocalFeatureCorrelationLayer(nn.Module):\n    def __init__(self, patch_size=9):\n        super().__init__()\n\n        try:\n            # If the PyTorch correlation module is installed,\n            # we can avoid JIT compilation of the extension:\n            # https://github.com/ClementPinard/Pytorch-Correlation-extension\n            from spatial_correlation_sampler import spatial_correlation_sample\n\n        except ModuleNotFoundError:\n            from models.correlation_ops.correlation_function import \\\n                spatial_correlation_sample\n\n        self.local_correlation = spatial_correlation_sample\n\n        self.patch_size = patch_size\n\n    def forward(self, feature_source, feature_target):\n        b = feature_target.shape[0]\n        corr = self.local_correlation(feature_target,\n                                      feature_source,\n                                      patch_size=self.patch_size)\n        corr = corr.view(b, self.patch_size * self.patch_size,\n                         feature_target.shape[2], feature_target.shape[3])\n        corr = nn.functional.normalize(nn.functional.relu(corr), p=2, dim=1)\n        return corr", "\n\nclass GlobalFeatureCorrelationLayer(nn.Module):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n\n    Implementation of the global feature correlation layer\n    Source and query, as well as target and reference refer to the same images.\n    \"\"\"\n\n    def __init__(self, cyclic_consistency=True):\n        super().__init__()\n        self.cyclic_consistency = cyclic_consistency\n\n    def forward(self, feature_source, feature_target):\n        b = feature_target.shape[0]\n        # directly obtain the 3D correlation\n        corr = self.compute_global_correlation(feature_source,\n                                               feature_target)\n\n        if self.cyclic_consistency:\n            # to add on top of the correlation ! (already included in NC-Net)\n            corr4d = self.mutual_matching(corr.view(\n                b, feature_source.shape[2], feature_source.shape[3], feature_target.shape[2], feature_target.shape[3]).unsqueeze(1))\n            corr = corr4d.squeeze(1).view(\n                b, feature_source.shape[2] * feature_source.shape[3], feature_target.shape[2], feature_target.shape[3])\n\n        corr = nn.functional.normalize(nn.functional.relu(corr), p=2, dim=1)\n        return corr\n\n    @staticmethod\n    def mutual_matching(corr4d):\n        # mutual matching\n        batch_size, ch, fs1, fs2, fs3, fs4 = corr4d.size()\n\n        # [batch_idx,k_A,i_B,j_B] #correlation target\n        corr4d_B = corr4d.view(batch_size, fs1 * fs2, fs3, fs4)\n        corr4d_A = corr4d.view(batch_size, fs1, fs2, fs3 * fs4)\n\n        # get max\n        corr4d_B_max, _ = torch.max(corr4d_B, dim=1, keepdim=True)\n        corr4d_A_max, _ = torch.max(corr4d_A, dim=3, keepdim=True)\n\n        eps = 1e-5\n        corr4d_B = corr4d_B / (corr4d_B_max + eps)\n        corr4d_A = corr4d_A / (corr4d_A_max + eps)\n\n        corr4d_B = corr4d_B.view(batch_size, 1, fs1, fs2, fs3, fs4)\n        corr4d_A = corr4d_A.view(batch_size, 1, fs1, fs2, fs3, fs4)\n\n        # parenthesis are important for symmetric output\n        corr4d = corr4d * (corr4d_A * corr4d_B)\n\n        return corr4d\n\n    @staticmethod\n    def compute_global_correlation(feature_source, feature_target, shape='3D', put_W_first_in_channel_dimension=False):\n        if shape == '3D':\n            b, c, h_source, w_source = feature_source.size()\n            b, c, h_target, w_target = feature_target.size()\n\n            if put_W_first_in_channel_dimension:\n                # FOR SOME REASON, THIS IS THE DEFAULT\n                feature_source = feature_source.transpose(\n                    2, 3).contiguous().view(b, c, w_source * h_source)\n                # ATTENTION, here the w and h of the source features are inverted !!!\n                # shape (b,c, w_source * h_source)\n\n                feature_target = feature_target.view(\n                    b, c, h_target * w_target).transpose(1, 2)\n                # shape (b,h_target*w_target,c)\n\n                # perform matrix mult.\n                feature_mul = torch.bmm(feature_target, feature_source)\n                # shape (b,h_target*w_target, w_source*h_source)\n                # indexed [batch,idx_A=row_A+h*col_A,row_B,col_B]\n\n                correlation_tensor = feature_mul.view(b, h_target, w_target, w_source * h_source).transpose(2, 3) \\\n                    .transpose(1, 2)\n                # shape (b, w_source*h_source, h_target, w_target)\n                # ATTENTION, here in source dimension, W is first !! (channel dimension is W,H)\n            else:\n                feature_source = feature_source.contiguous().view(b, c, h_source * w_source)\n                # shape (b,c, h_source * w_source)\n\n                feature_target = feature_target.view(\n                    b, c, h_target * w_target).transpose(1, 2)\n                # shape (b,h_target*w_target,c)\n\n                # perform matrix mult.\n                feature_mul = torch.bmm(feature_target,\n                                        feature_source)  # shape (b,h_target*w_target, h_source*w_source)\n                correlation_tensor = feature_mul.view(b, h_target, w_target, h_source * w_source).transpose(2, 3) \\\n                    .transpose(1, 2)\n                # shape (b, h_source*w_source, h_target, w_target)\n                # ATTENTION, here H is first in channel dimension !\n        elif shape == '4D':\n            b, c, hsource, wsource = feature_source.size()\n            b, c, htarget, wtarget = feature_target.size()\n            # reshape features for matrix multiplication\n            feature_source = feature_source.view(\n                b, c, hsource * wsource).transpose(1, 2)  # size [b,hsource*wsource,c]\n            feature_target = feature_target.view(\n                b, c, htarget * wtarget)  # size [b,c,htarget*wtarget]\n            # perform matrix mult.\n            # size [b, hsource*wsource, htarget*wtarget]\n            feature_mul = torch.bmm(feature_source, feature_target)\n            correlation_tensor = feature_mul.view(\n                b, hsource, wsource, htarget, wtarget).unsqueeze(1)\n            # size is [b, 1, hsource, wsource, htarget, wtarget]\n        else:\n            raise ValueError('tensor should be 3D or 4D')\n        return correlation_tensor", "\n\nclass OpticalFlowEstimatorResidualConnection(nn.Module):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels=2, batch_norm=True, output_x=False, extra_bias='auto'):\n        super().__init__()\n        self.output_x = output_x\n        norm_layer = nn.BatchNorm2d if batch_norm else None\n        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n        self.leaky_relu = activation_layer()\n\n        self.conv_0 = ConvBNReLU(in_channels, 128, kernel_size=3,\n                                 norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n        self.conv0_skip = ConvBNReLU(\n            128, 96, kernel_size=1, norm_layer=norm_layer, activation_layer=None)\n        self.conv_1 = ConvBNReLU(128, 128, kernel_size=3, norm_layer=norm_layer,\n                                 activation_layer=activation_layer, bias=extra_bias)\n        self.conv_2 = ConvBNReLU(\n            128, 96, kernel_size=3, norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n        self.conv2_skip = ConvBNReLU(\n            96, 32, kernel_size=1, norm_layer=norm_layer, activation_layer=None)\n        self.conv_3 = ConvBNReLU(96, 64, kernel_size=3, norm_layer=norm_layer,\n                                 activation_layer=activation_layer, bias=extra_bias)\n        self.conv_4 = ConvBNReLU(\n            64, 32, kernel_size=3, norm_layer=norm_layer, activation_layer=None, bias=extra_bias)\n        self.predict_mapping = nn.Conv2d(\n            32, out_channels, kernel_size=3, padding=1, bias=True)\n\n    def forward(self, x):\n        x0 = self.conv_0(x)\n        x0_relu = self.leaky_relu(x0)\n        x2 = self.conv_2(self.conv_1(x0_relu))\n        x2_skip = x2 + self.conv0_skip(x0)\n        x2_skip_relu = self.leaky_relu(x2_skip)\n        x4 = self.conv_4(self.conv_3(x2_skip_relu))\n        x4_skip = x4 + self.conv2_skip(x2_skip)\n        x = self.leaky_relu(x4_skip)\n        mapping = self.predict_mapping(x)\n\n        if self.output_x:\n            return mapping, x\n        else:\n            return mapping", "\n\nclass RefinementModule(nn.Module):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels=2, batch_norm=True, extra_bias='auto'):\n        super().__init__()\n        norm_layer = nn.BatchNorm2d if batch_norm else None\n        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n        self.dc_convs = nn.Sequential(\n            ConvBNReLU(in_channels, 128, kernel_size=3, dilation=1, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            ConvBNReLU(128, 128, kernel_size=3, dilation=2, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            ConvBNReLU(128, 128, kernel_size=3, dilation=4, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            ConvBNReLU(128, 96, kernel_size=3, dilation=8, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            ConvBNReLU(96, 64, kernel_size=3, dilation=16, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            ConvBNReLU(64, 32, kernel_size=3, dilation=1, norm_layer=norm_layer,\n                       activation_layer=activation_layer, bias=extra_bias),\n            nn.Conv2d(32, out_channels, kernel_size=3, padding=1, bias=True)\n        )\n\n    def forward(self, x):\n        return self.dc_convs(x)", "\n\nclass UncertaintyModule(nn.Module):\n    \"\"\"\n    ---------------------------------------------------------------------------\n    Copyright (c) Prune Truong. All rights reserved.\n    \n    This source code is licensed under the license found in the\n    LICENSE file in https://github.com/PruneTruong/DenseMatching.\n    ---------------------------------------------------------------------------\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 feed_in_previous=False,\n                 out_channels=1,\n                 search_size=9,\n                 batch_norm=True,\n                 depthwise_separable=False):\n        super().__init__()\n        norm_layer = nn.BatchNorm2d if batch_norm else None\n        activation_layer = partial(nn.LeakyReLU, negative_slope=0.1)\n        self.search_size = search_size\n        self.feed_in_previous = feed_in_previous\n        if self.feed_in_previous:\n            add_channels = 2 + 1  # 2 flow channels and 1 sigma channel\n        else:\n            add_channels = 0\n        out_channels = 1\n        if self.search_size == 9:\n            self.conv_0 = ConvBNReLU(in_channels, 32, kernel_size=3, stride=1, padding=0,\n                                     norm_layer=norm_layer, activation_layer=activation_layer, depthwise_separable=False)\n            self.conv_1 = ConvBNReLU(32, 32, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n            self.conv_2 = ConvBNReLU(32, 16, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n            self.predict_uncertainty = nn.Conv2d(\n                16, 6, kernel_size=3, stride=1, padding=0, bias=True)\n        elif search_size == 16:\n            self.conv_0 = ConvBNReLU(in_channels, 32, kernel_size=3, stride=1, padding=0,\n                                     norm_layer=norm_layer, activation_layer=activation_layer, depthwise_separable=False)\n            self.maxpool = nn.MaxPool2d((2, 2))\n            self.conv_1 = ConvBNReLU(32, 32, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n            self.conv_2 = ConvBNReLU(32, 16, kernel_size=3, stride=1, padding=0, norm_layer=norm_layer,\n                                     activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n            self.predict_uncertainty = nn.Conv2d(\n                16, 6, kernel_size=3, stride=1, padding=0, bias=True)\n\n        self.pred_conv_0 = ConvBNReLU(6 + 32 + add_channels, 32, kernel_size=3, norm_layer=norm_layer,\n                                      activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n        self.pred_conv_1 = ConvBNReLU(32, 16, kernel_size=3, norm_layer=norm_layer,\n                                      activation_layer=activation_layer, depthwise_separable=depthwise_separable)\n        self.predict_uncertainty_final = nn.Conv2d(\n            16, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n\n    def forward(self, corr, feat, up_previous_uncertainty=None, up_previous_flow=None, logits_corr=None):\n        # x shape is b, s*s, h, w\n        b, _, h, w = corr.size()\n        x = corr.permute(0, 2, 3, 1).contiguous().view(\n            b * h * w, self.search_size, self.search_size).unsqueeze(1)\n        # x is now shape b*h*w, 1, s, s\n\n        if logits_corr is not None:\n            x_logits = logits_corr.permute(0, 2, 3, 1).contiguous().view(\n                b * h * w, self.search_size, self.search_size).unsqueeze(1)\n            x = torch.cat((x, x_logits), dim=1)\n\n        if self.search_size == 9:\n            x = self.conv_2(self.conv_1(self.conv_0(x)))\n            uncertainty_corr = self.predict_uncertainty(x)\n        elif self.search_size == 16:\n            x = self.conv_0(x)\n            x = self.maxpool(x)\n            x = self.conv_2(self.conv_1(x))\n            uncertainty_corr = self.predict_uncertainty(x)\n\n        uncertainty_corr = uncertainty_corr.squeeze().view(\n            b, h, w, -1).permute(0, 3, 1, 2)\n\n        if self.feed_in_previous:\n            uncertainty = torch.cat(\n                (uncertainty_corr, feat, up_previous_uncertainty, up_previous_flow), 1)\n        else:\n            uncertainty = torch.cat((uncertainty_corr, feat), 1)\n\n        uncertainty = self.pred_conv_1(self.pred_conv_0(uncertainty))\n        uncertainty = self.predict_uncertainty_final(uncertainty)\n        return uncertainty", "\nclass UAWarpCHead(BaseHead):\n\n    def __init__(self,\n                 in_index: Union[List[int], int],\n                 input_transform: Optional[str] = None,\n                 pretrained: Optional[str] = None,\n                 batch_norm: bool = True,\n                 refinement_at_adaptive_res: bool = True,\n                 refinement_at_finest_level: bool = True,\n                 estimate_uncertainty: bool = False,\n                 uncertainty_mixture: bool = False,\n                 iterative_refinement: bool = False):\n        super().__init__(None, in_index, input_transform)\n\n        self.estimate_uncertainty = estimate_uncertainty\n        self.uncertainty_mixture = uncertainty_mixture\n        self.iterative_refinement = iterative_refinement\n        self.global_corr = GlobalFeatureCorrelationLayer(\n            cyclic_consistency=True)\n        self.local_corr = LocalFeatureCorrelationLayer(\n            patch_size=9)\n\n        # level 4, 16x16, global correlation\n        nd = 16 * 16\n        od = nd\n        self.decoder4 = OpticalFlowEstimatorResidualConnection(\n            in_channels=od, batch_norm=batch_norm, output_x=True)\n\n        # level 3, 32x32, constrained correlation, patchsize 9\n        nd = 9 * 9\n        od = nd + 2\n        if self.estimate_uncertainty:\n            od += 1  # input also the upsampled log_var of previous resolution\n        self.decoder3 = OpticalFlowEstimatorResidualConnection(\n            in_channels=od, batch_norm=batch_norm, output_x=True)\n\n        self.refinement_at_adaptive_res = refinement_at_adaptive_res\n        if self.refinement_at_adaptive_res:\n            self.refinement_module_adaptive = RefinementModule(\n                32, batch_norm=batch_norm)\n\n        nbr_upfeat_channels = 2\n\n        od = nd + 2\n        if self.estimate_uncertainty:\n            od += 1  # input also the upsampled log_var of previous resolution\n        self.decoder2 = OpticalFlowEstimatorResidualConnection(\n            in_channels=od, batch_norm=batch_norm, output_x=True)\n\n        self.reduce = nn.Conv2d(32, nbr_upfeat_channels,\n                                kernel_size=1, bias=True)\n\n        od = nd + 2 + nbr_upfeat_channels\n        if self.estimate_uncertainty:\n            od += 1  # input also the upsampled log_var of previous resolution\n        self.decoder1 = OpticalFlowEstimatorResidualConnection(\n            in_channels=od, batch_norm=batch_norm, output_x=True)\n\n        self.refinement_at_finest_level = refinement_at_finest_level\n        if self.refinement_at_finest_level:\n            self.refinement_module_finest = RefinementModule(\n                32, batch_norm=batch_norm)\n\n        if self.estimate_uncertainty:\n            self.estimate_uncertainty_components4 = UncertaintyModule(in_channels=1,\n                                                                      search_size=16)\n            self.estimate_uncertainty_components3 = UncertaintyModule(in_channels=1,\n                                                                      search_size=9,\n                                                                      feed_in_previous=True)\n            self.estimate_uncertainty_components2 = UncertaintyModule(in_channels=1,\n                                                                      search_size=9,\n                                                                      feed_in_previous=True)\n            self.estimate_uncertainty_components1 = UncertaintyModule(in_channels=1,\n                                                                      search_size=9,\n                                                                      feed_in_previous=True)\n\n        if pretrained is not None:\n            self.load_weights(pretrained)\n\n    def forward(self, trg, src, trg_256, src_256, out_size, **kwargs):\n        c11, c12 = self._transform_inputs(trg)\n        c13, c14 = self._transform_inputs(trg_256)\n        c21, c22 = self._transform_inputs(src)\n        c23, c24 = self._transform_inputs(src_256)\n\n        c11 = nn.functional.normalize(c11, p=2, dim=1)\n        c12 = nn.functional.normalize(c12, p=2, dim=1)\n        c13 = nn.functional.normalize(c13, p=2, dim=1)\n        c14 = nn.functional.normalize(c14, p=2, dim=1)\n        c21 = nn.functional.normalize(c21, p=2, dim=1)\n        c22 = nn.functional.normalize(c22, p=2, dim=1)\n        c23 = nn.functional.normalize(c23, p=2, dim=1)\n        c24 = nn.functional.normalize(c24, p=2, dim=1)\n\n        h_256, w_256 = (256, 256)\n\n        # level 4: 16x16\n        h_4, w_4 = c14.shape[-2:]\n        assert (h_4, w_4) == (16, 16), (h_4, w_4)\n        corr4 = self.global_corr(c24, c14)\n\n        # init_map = corr4.new_zeros(size=(b, 2, h_4, w_4))\n        est_map4, x4 = self.decoder4(corr4)\n        flow4_256 = self.unnormalise_and_convert_mapping_to_flow(est_map4)\n        # scale flow values to 256x256\n        flow4_256[:, 0, :, :] *= w_256 / float(w_4)\n        flow4_256[:, 1, :, :] *= h_256 / float(h_4)\n\n        if self.estimate_uncertainty:\n            uncert_components4_256 = self.estimate_uncertainty_components4(\n                corr4, x4)\n            # scale uncert values to 256\n            assert w_256 / float(w_4) == h_256 / float(h_4)\n            uncert_components4_256[:, 0, :, :] += 2 * \\\n                math.log(w_256 / float(w_4))\n\n        # level 3: 32x32\n        h_3, w_3 = c13.shape[-2:]\n        assert (h_3, w_3) == (32, 32), (h_3, w_3)\n        up_flow4 = nn.functional.interpolate(input=flow4_256, size=(\n            h_3, w_3), mode='bilinear', align_corners=False)\n        if self.estimate_uncertainty:\n            up_uncert_components4 = nn.functional.interpolate(\n                input=uncert_components4_256, size=(h_3, w_3), mode='bilinear', align_corners=False)\n\n        # for warping, we need flow values at 32x32 scale\n        up_flow_4_warping = up_flow4.clone()\n        up_flow_4_warping[:, 0, :, :] *= w_3 / float(w_256)\n        up_flow_4_warping[:, 1, :, :] *= h_3 / float(h_256)\n        warp3 = warp(c23, up_flow_4_warping)\n\n        # constrained correlation\n        corr3 = self.local_corr(warp3, c13)\n        if self.estimate_uncertainty:\n            inp_flow_dec3 = torch.cat(\n                (corr3, up_flow4, up_uncert_components4), 1)\n        else:\n            inp_flow_dec3 = torch.cat((corr3, up_flow4), 1)\n        res_flow3, x3 = self.decoder3(inp_flow_dec3)\n        if self.refinement_at_adaptive_res:\n            res_flow3 = res_flow3 + self.refinement_module_adaptive(x3)\n        flow3 = res_flow3 + up_flow4\n\n        if self.estimate_uncertainty:\n            uncert_components3 = self.estimate_uncertainty_components3(\n                corr3, x3, up_uncert_components4, up_flow4)\n\n        # change from absolute resolutions to relative resolutions\n        # scale flow4 and flow3 magnitude to original resolution\n        h_original, w_original = out_size\n        flow3[:, 0, :, :] *= w_original / float(w_256)\n        flow3[:, 1, :, :] *= h_original / float(h_256)\n        if self.estimate_uncertainty:\n            # APPROXIMATION FOR NON-SQUARE IMAGES --> use the diagonal\n            diag_original = math.sqrt(h_original ** 2 + w_original ** 2)\n            diag_256 = math.sqrt(h_256 ** 2 + w_256 ** 2)\n            uncert_components3[:, 0, :, :] += 2 * \\\n                math.log(diag_original / float(diag_256))\n\n        if self.iterative_refinement and not self.training:\n            # from 32x32 resolution, if upsampling to 1/8*original resolution is too big,\n            # do iterative upsampling so that gap is always smaller than 2.\n            R = float(max(h_original, w_original)) / 8.0 / 32.0\n            minimum_ratio = 3.0  # if the image is >= 1086 in one dimension, do refinement\n            nbr_extra_layers = max(\n                0, int(round(math.log(R / minimum_ratio) / math.log(2))))\n            if nbr_extra_layers > 0:\n                for n in range(nbr_extra_layers):\n                    ratio = 1.0 / (8.0 * 2 ** (nbr_extra_layers - n))\n                    h_this = int(h_original * ratio)\n                    w_this = int(w_original * ratio)\n                    up_flow3 = nn.functional.interpolate(input=flow3, size=(\n                        h_this, w_this), mode='bilinear', align_corners=False)\n                    if self.estimate_uncertainty:\n                        up_uncert_components3 = nn.functional.interpolate(input=uncert_components3, size=(\n                            h_this, w_this), mode='bilinear', align_corners=False)\n                    c23_bis = nn.functional.interpolate(\n                        c22, size=(h_this, w_this), mode='area')\n                    c13_bis = nn.functional.interpolate(\n                        c12, size=(h_this, w_this), mode='area')\n                    warp3 = warp(c23_bis, up_flow3 * ratio)\n                    corr3 = self.local_corr(warp3, c13_bis)\n                    if self.estimate_uncertainty:\n                        inp_flow_dec3 = torch.cat(\n                            (corr3, up_flow3, up_uncert_components3), 1)\n                    else:\n                        inp_flow_dec3 = torch.cat((corr3, up_flow3), 1)\n                    res_flow3, x3 = self.decoder2(inp_flow_dec3)\n                    flow3 = res_flow3 + up_flow3\n                    if self.estimate_uncertainty:\n                        uncert_components3 = self.estimate_uncertainty_components2(\n                            corr3, x3, up_uncert_components3, up_flow3)\n\n        # level 2: 1/8 of original resolution\n        h_2, w_2 = c12.shape[-2:]\n        up_flow3 = nn.functional.interpolate(input=flow3, size=(\n            h_2, w_2), mode='bilinear', align_corners=False)\n        if self.estimate_uncertainty:\n            up_uncert_components3 = nn.functional.interpolate(\n                input=uncert_components3, size=(h_2, w_2), mode='bilinear', align_corners=False)\n\n        up_flow_3_warping = up_flow3.clone()\n        up_flow_3_warping[:, 0, :, :] *= w_2 / float(w_original)\n        up_flow_3_warping[:, 1, :, :] *= h_2 / float(h_original)\n        warp2 = warp(c22, up_flow_3_warping)\n\n        # constrained correlation\n        corr2 = self.local_corr(warp2, c12)\n        if self.estimate_uncertainty:\n            inp_flow_dec2 = torch.cat(\n                (corr2, up_flow3, up_uncert_components3), 1)\n        else:\n            inp_flow_dec2 = torch.cat((corr2, up_flow3), 1)\n        res_flow2, x2 = self.decoder2(inp_flow_dec2)\n        flow2 = res_flow2 + up_flow3\n\n        if self.estimate_uncertainty:\n            uncert_components2 = self.estimate_uncertainty_components2(\n                corr2, x2, up_uncert_components3, up_flow3)\n\n        # level 1: 1/4 of original resolution\n        h_1, w_1 = c11.shape[-2:]\n        up_flow2 = nn.functional.interpolate(input=flow2, size=(\n            h_1, w_1), mode='bilinear', align_corners=False)\n        if self.estimate_uncertainty:\n            up_uncert_components2 = nn.functional.interpolate(\n                input=uncert_components2, size=(h_1, w_1), mode='bilinear', align_corners=False)\n\n        up_feat2 = nn.functional.interpolate(input=x2, size=(\n            h_1, w_1), mode='bilinear', align_corners=False)\n        up_feat2 = self.reduce(up_feat2)\n\n        up_flow_2_warping = up_flow2.clone()\n        up_flow_2_warping[:, 0, :, :] *= w_1 / float(w_original)\n        up_flow_2_warping[:, 1, :, :] *= h_1 / float(h_original)\n        warp1 = warp(c21, up_flow_2_warping)\n\n        corr1 = self.local_corr(warp1, c11)\n        if self.estimate_uncertainty:\n            inp_flow_dec1 = torch.cat(\n                (corr1, up_flow2, up_feat2, up_uncert_components2), 1)\n        else:\n            inp_flow_dec1 = torch.cat((corr1, up_flow2, up_feat2), 1)\n        res_flow1, x = self.decoder1(inp_flow_dec1)\n        if self.refinement_at_finest_level:\n            res_flow1 = res_flow1 + self.refinement_module_finest(x)\n        flow1 = res_flow1 + up_flow2\n\n        # upscale also flow4\n        flow4 = flow4_256.clone()\n        flow4[:, 0, :, :] *= w_original / float(w_256)\n        flow4[:, 1, :, :] *= h_original / float(h_256)\n\n        if self.estimate_uncertainty:\n            uncert_components1 = self.estimate_uncertainty_components1(\n                corr1, x, up_uncert_components2, up_flow2)\n\n            # APPROXIMATION FOR NON-SQUARE IMAGES --> use the diagonal\n            uncert_components4 = uncert_components4_256.clone()\n            uncert_components4[:, 0, :, :] += 2 * \\\n                math.log(diag_original / float(diag_256))\n\n            return (flow4, uncert_components4), (flow3, uncert_components3), (flow2, uncert_components2), (flow1, uncert_components1)\n\n        return flow4, flow3, flow2, flow1\n\n    @staticmethod\n    def unnormalise_and_convert_mapping_to_flow(map, output_channel_first=True):\n        \"\"\"\n        ---------------------------------------------------------------------------\n        Copyright (c) Prune Truong. All rights reserved.\n    \n        This source code is licensed under the license found in the\n        LICENSE file in https://github.com/PruneTruong/DenseMatching.\n        ---------------------------------------------------------------------------\n        \"\"\"\n        if map.shape[1] != 2:\n            # load_size is BxHxWx2\n            map = map.permute(0, 3, 1, 2)\n\n        # channel first, here map is normalised to -1;1\n        # we put it back to 0,W-1, then convert it to flow\n        B, C, H, W = map.size()\n        mapping = torch.zeros_like(map)\n        # mesh grid\n        mapping[:, 0, :, :] = (map[:, 0, :, :] + 1) * \\\n            (W - 1) / 2.0  # unormalise\n        mapping[:, 1, :, :] = (map[:, 1, :, :] + 1) * \\\n            (H - 1) / 2.0  # unormalise\n\n        xx = torch.arange(0, W, dtype=mapping.dtype,\n                        device=mapping.device).view(1, -1).repeat(H, 1)\n        yy = torch.arange(0, H, dtype=mapping.dtype,\n                        device=mapping.device).view(-1, 1).repeat(1, W)\n        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n        grid = torch.cat((xx, yy), 1)\n\n        flow = mapping - grid  # here also channel first\n        if not output_channel_first:\n            flow = flow.permute(0, 2, 3, 1)\n        return flow\n\n    def load_weights(self, pretrain_path):\n        if pretrain_path is None:\n            return\n\n        if pretrain_path == 'megadepth':\n            pretrain_path = model_urls['megadepth']\n\n        if os.path.exists(pretrain_path):\n            checkpoint = torch.load(\n                pretrain_path, map_location=lambda storage, loc: storage)\n        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrain_path)):\n            checkpoint = torch.load(os.path.join(os.environ.get(\n                'TORCH_HOME', ''), 'hub', pretrain_path), map_location=lambda storage, loc: storage)\n        else:\n            checkpoint = torch.hub.load_state_dict_from_url(\n                pretrain_path, progress=True, map_location=lambda storage, loc: storage)\n        if 'state_dict' in checkpoint.keys():\n            state_dict = checkpoint['state_dict']\n        else:\n            state_dict = checkpoint\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if k.startswith('alignment_head.'):\n                new_k = k.replace('alignment_head.', '')\n            else:\n                continue  # ignore the rest\n            new_state_dict[new_k] = v\n        self.load_state_dict(new_state_dict, strict=True)", ""]}
{"filename": "models/heads/deeplabv2.py", "chunked_list": ["import os\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom .base import BaseHead\n\nmodel_urls = {\n    # DeepLabv2 trained on Cityscapes", "model_urls = {\n    # DeepLabv2 trained on Cityscapes\n    \"cityscapes\": \"https://data.vision.ee.ethz.ch/brdavid/coma/deeplabv2_cityscapes.pth\"\n}\n\n\nclass DeepLabV2Head(BaseHead):\n\n    def __init__(self,\n                 in_channels: int,\n                 in_index: Union[List[int], int],\n                 num_classes: int,\n                 input_transform: Optional[str] = None,\n                 dilation_series: List[int] = [6, 12, 18, 24],\n                 padding_series: List[int] = [6, 12, 18, 24],\n                 pretrained: Optional[str] = None):\n        super().__init__(num_classes, in_index, input_transform)\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(\n                nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias=True))\n\n        self.init_weights()\n        self.load_pretrained(pretrained)\n\n    def init_weights(self):\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self._transform_inputs(x)\n        return sum(stage(x) for stage in self.conv2d_list)\n\n    def load_pretrained(self, pretrained):\n        if pretrained is None:\n            return\n\n        if pretrained == 'cityscapes':\n            pretrained = model_urls['cityscapes']\n\n        if os.path.exists(pretrained):\n            checkpoint = torch.load(pretrained, map_location=lambda storage, loc: storage)\n        elif os.path.exists(os.path.join(os.environ.get('TORCH_HOME', ''), 'hub', pretrained)):\n            checkpoint = torch.load(os.path.join(os.environ.get(\n                'TORCH_HOME', ''), 'hub', pretrained), map_location=lambda storage, loc: storage)\n        else:\n            checkpoint = torch.hub.load_state_dict_from_url(\n                pretrained, progress=True, map_location=lambda storage, loc: storage)\n        if 'state_dict' in checkpoint:\n            state_dict = checkpoint['state_dict']\n        else:\n            state_dict = checkpoint\n\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if k.startswith('head.'):\n                new_k = k.replace('head.', '')\n                new_state_dict[new_k] = v\n            else:\n                pass\n        self.load_state_dict(new_state_dict, strict=True)", ""]}
