{"filename": "loss.py", "chunked_list": ["from typing import Tuple, Dict, Any\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\n\ndef tensor_correlation(a, b):\n    return torch.einsum(\"nchw,ncij->nhwij\", a, b)\n\n\ndef norm(t):\n    return F.normalize(t, dim=1, eps=1e-10)", "\n\ndef norm(t):\n    return F.normalize(t, dim=1, eps=1e-10)\n\n\ndef sample(t: torch.Tensor, coords: torch.Tensor):\n    return F.grid_sample(t, coords.permute(0, 2, 1, 3), padding_mode='border', align_corners=True)\n\n", "\n\n@torch.jit.script\ndef super_perm(size: int, device: torch.device):\n    perm = torch.randperm(size, device=device, dtype=torch.long)\n    perm[perm == torch.arange(size, device=device)] += 1\n    return perm % size\n\nclass StegoLoss(nn.Module):\n\n    def __init__(self,\n                 n_classes: int,\n                 cfg: dict,\n                 corr_weight: float = 1.0):\n        super().__init__()\n\n        self.n_classes = n_classes\n        self.corr_weight = corr_weight\n        self.corr_loss = ContrastiveCorrelationLoss(cfg)\n        self.linear_loss = LinearLoss(cfg)\n\n    def forward(self, model_input, model_output, model_pos_output=None, linear_output: torch.Tensor() = None,\n                cluster_output: torch.Tensor() = None) \\\n            -> Tuple[torch.Tensor, Dict[str, float]]:\n        img, label = model_input\n        # feats, code = model_output\n        feats = model_output[0]\n        code = model_output[1]\n\n        if self.corr_weight > 0:\n            # feats_pos, code_pos = model_pos_output\n            feats_pos = model_pos_output[0]\n            code_pos = model_pos_output[1]\n            corr_loss, corr_loss_dict = self.corr_loss(feats, feats_pos, code, code_pos)\n        else:\n            corr_loss_dict = {\"none\": 0}\n            corr_loss = torch.tensor(0, device=feats.device)\n\n        linear_loss = self.linear_loss(linear_output, label, self.n_classes)\n        cluster_loss = cluster_output[0]\n        loss = linear_loss + cluster_loss\n        loss_dict = {\"loss\": loss.item(), \"corr\": corr_loss.item(), \"linear\": linear_loss.item(),\n                     \"cluster\": cluster_loss.item()}\n\n        return loss, loss_dict, corr_loss_dict", "class StegoLoss(nn.Module):\n\n    def __init__(self,\n                 n_classes: int,\n                 cfg: dict,\n                 corr_weight: float = 1.0):\n        super().__init__()\n\n        self.n_classes = n_classes\n        self.corr_weight = corr_weight\n        self.corr_loss = ContrastiveCorrelationLoss(cfg)\n        self.linear_loss = LinearLoss(cfg)\n\n    def forward(self, model_input, model_output, model_pos_output=None, linear_output: torch.Tensor() = None,\n                cluster_output: torch.Tensor() = None) \\\n            -> Tuple[torch.Tensor, Dict[str, float]]:\n        img, label = model_input\n        # feats, code = model_output\n        feats = model_output[0]\n        code = model_output[1]\n\n        if self.corr_weight > 0:\n            # feats_pos, code_pos = model_pos_output\n            feats_pos = model_pos_output[0]\n            code_pos = model_pos_output[1]\n            corr_loss, corr_loss_dict = self.corr_loss(feats, feats_pos, code, code_pos)\n        else:\n            corr_loss_dict = {\"none\": 0}\n            corr_loss = torch.tensor(0, device=feats.device)\n\n        linear_loss = self.linear_loss(linear_output, label, self.n_classes)\n        cluster_loss = cluster_output[0]\n        loss = linear_loss + cluster_loss\n        loss_dict = {\"loss\": loss.item(), \"corr\": corr_loss.item(), \"linear\": linear_loss.item(),\n                     \"cluster\": cluster_loss.item()}\n\n        return loss, loss_dict, corr_loss_dict", "\n\nclass ContrastiveCorrelationLoss(nn.Module):\n\n    def __init__(self, cfg: dict):\n        super().__init__()\n        self.cfg = cfg\n\n    def standard_scale(self, t):\n        t1 = t - t.mean()\n        t2 = t1 / t1.std()\n        return t2\n\n    def helper(self, f1, f2, c1, c2, shift):\n        with torch.no_grad():\n            # Comes straight from backbone which is currently frozen. this saves mem.\n            fd = tensor_correlation(norm(f1), norm(f2))\n\n            if self.cfg[\"pointwise\"]:\n                old_mean = fd.mean()\n                fd -= fd.mean([3, 4], keepdim=True)\n                fd = fd - fd.mean() + old_mean\n\n        cd = tensor_correlation(norm(c1), norm(c2))\n\n        if self.cfg[\"zero_clamp\"]:\n            min_val = 0.0\n        else:\n            min_val = -9999.0\n\n        if self.cfg[\"stabilize\"]:\n            loss = - cd.clamp(min_val, .8) * (fd - shift)\n        else:\n            loss = - cd.clamp(min_val) * (fd - shift)\n\n        return loss, cd\n\n    def forward(self,\n                orig_feats: torch.Tensor,\n                orig_feats_pos: torch.Tensor,\n                orig_code: torch.Tensor,\n                orig_code_pos: torch.Tensor,\n                ):\n\n        coord_shape = [orig_feats.shape[0], self.cfg[\"feature_samples\"], self.cfg[\"feature_samples\"], 2]\n\n        coords1 = torch.rand(coord_shape, device=orig_feats.device) * 2 - 1\n        coords2 = torch.rand(coord_shape, device=orig_feats.device) * 2 - 1\n\n        feats = sample(orig_feats, coords1)\n        code = sample(orig_code, coords1)\n\n        feats_pos = sample(orig_feats_pos, coords2)\n        code_pos = sample(orig_code_pos, coords2)\n\n        pos_intra_loss, pos_intra_cd = self.helper(\n            feats, feats, code, code, self.cfg[\"corr_loss\"][\"pos_intra_shift\"])\n        pos_inter_loss, pos_inter_cd = self.helper(\n            feats, feats_pos, code, code_pos, self.cfg[\"corr_loss\"][\"pos_inter_shift\"])\n\n        neg_losses = []\n        neg_cds = []\n        for i in range(self.cfg[\"neg_samples\"]):\n            perm_neg = super_perm(orig_feats.shape[0], orig_feats.device)\n            feats_neg = sample(orig_feats[perm_neg], coords2)\n            code_neg = sample(orig_code[perm_neg], coords2)\n            neg_inter_loss, neg_inter_cd = self.helper(\n                feats, feats_neg, code, code_neg, self.cfg[\"corr_loss\"][\"neg_inter_shift\"])\n            neg_losses.append(neg_inter_loss)\n            neg_cds.append(neg_inter_cd)\n\n        neg_inter_loss = torch.cat(neg_losses, axis=0)\n        neg_inter_cd = torch.cat(neg_cds, axis=0)\n\n        return (self.cfg[\"corr_loss\"][\"pos_intra_weight\"] * pos_intra_loss.mean() +\n                self.cfg[\"corr_loss\"][\"pos_inter_weight\"] * pos_inter_loss.mean() +\n                self.cfg[\"corr_loss\"][\"neg_inter_weight\"] * neg_inter_loss.mean(),\n                {\"self_loss\": pos_intra_loss.mean().item(),\n                 \"knn_loss\": pos_inter_loss.mean().item(),\n                 \"rand_loss\": neg_inter_loss.mean().item()}\n                )", "\n\nclass LinearLoss(nn.Module):\n\n    def __init__(self, cfg: dict):\n        super(LinearLoss, self).__init__()\n        self.cfg = cfg\n        self.linear_loss = nn.CrossEntropyLoss()\n\n    def forward(self, linear_logits: torch.Tensor, label: torch.Tensor, n_classes: int):\n        flat_label = label.reshape(-1)\n        mask = (flat_label >= 0) & (flat_label < n_classes)\n\n        linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n        linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, n_classes)\n        linear_loss = self.linear_loss(linear_logits[mask], flat_label[mask]).mean()\n\n        return linear_loss", "\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07, contrast_mode='one',\n                 base_temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n\n    def forward(self, modeloutput_z, modeloutput_s_pr=None, modeloutput_f=None,\n                Pool_ag=None, Pool_sp=None, opt=None, lmbd=None, modeloutput_z_mix=None):\n        device = (torch.device('cuda')\n                  if modeloutput_z.is_cuda\n                  else torch.device('cpu'))\n\n        batch_size = modeloutput_z.shape[0]\n\n        spatial_size = opt[\"model\"][\"spatial_size\"]\n\n        split = int(spatial_size*spatial_size)\n        mini_iters = int(batch_size/split)\n\n        negative_mask_one = torch.scatter(torch.ones((split,batch_size), dtype=torch.float16), 1,\n                                        torch.arange(split).view(-1,1),0).to(device)\n        mask_neglect_base = torch.FloatTensor(split,batch_size).uniform_() < opt[\"rho\"]\n        mask_neglect_base = mask_neglect_base.type(torch.float16)\n        mask_neglect_base = mask_neglect_base.cuda()\n\n        loss = torch.tensor(0).to(device)\n        for mi in range(mini_iters):\n            modeloutput_f_one = modeloutput_f[mi*split : (mi+1)*split]\n            with torch.cuda.amp.autocast(enabled=True):\n                output_cossim_one = torch.matmul(modeloutput_f_one, modeloutput_f.transpose(0, 1))\n                Rpoint = torch.matmul(modeloutput_f, Pool_ag.transpose(0, 1))\n\n            Rpoint = torch.max(Rpoint, dim=1).values\n            Rpoint_T = Rpoint.unsqueeze(-1).repeat(1, split)\n            output_cossim_one_T = output_cossim_one.transpose(0, 1)\n            mask_one_T = (Rpoint_T < output_cossim_one_T)\n            mask_one_T = torch.tensor(mask_one_T.transpose(0, 1), dtype=torch.float16)\n            Rpoint_one = Rpoint[mi*split : (mi+1)*split]\n            Rpoint_one = Rpoint_one.unsqueeze(-1).repeat(1, batch_size)\n            mask_one = torch.tensor((Rpoint_one < output_cossim_one), dtype=torch.float16)\n            mask_one = torch.logical_or(mask_one, mask_one_T).type(torch.float16)\n            neglect_mask = torch.logical_or(mask_one, mask_neglect_base).type(torch.float16)\n            neglect_negative_mask_one = negative_mask_one * neglect_mask\n            mask_one = mask_one * negative_mask_one\n\n            modeloutput_s_pr_one = modeloutput_s_pr[mi*split : (mi+1)*split]\n            with torch.cuda.amp.autocast(enabled=True):\n                output_cossim_ema_one = torch.matmul(modeloutput_s_pr_one, modeloutput_s_pr.transpose(0, 1))\n                Rpoint_ema = torch.matmul(modeloutput_s_pr, Pool_sp.transpose(0, 1))\n\n            Rpoint_ema = torch.max(Rpoint_ema, dim=1).values\n            Rpoint_ema_T = Rpoint_ema.unsqueeze(-1).repeat(1, split)\n            output_cossim_ema_one_T = output_cossim_ema_one.transpose(0, 1)\n            mask_ema_one_T = (Rpoint_ema_T < output_cossim_ema_one_T)\n            mask_ema_one_T = torch.tensor(mask_ema_one_T.transpose(0, 1), dtype=torch.float16)\n            Rpoint_ema_one = Rpoint_ema[mi*split : (mi+1)*split]\n            Rpoint_ema_one = Rpoint_ema_one.unsqueeze(-1).repeat(1, batch_size)\n            mask_ema_one = torch.tensor((Rpoint_ema_one < output_cossim_ema_one), dtype=torch.float16)\n            mask_ema_one = torch.logical_or(mask_ema_one, mask_ema_one_T).type(torch.float16)\n            mask_ema_one = mask_ema_one * negative_mask_one\n\n            modeloutput_z_one = modeloutput_z[mi*split : (mi+1)*split]\n            with torch.cuda.amp.autocast(enabled=True):\n                anchor_dot_contrast_one = torch.div(\n                    torch.matmul(modeloutput_z_one, modeloutput_z.T),\n                    self.temperature)\n\n            logits_max_one, _ = torch.max(anchor_dot_contrast_one, dim=1, keepdim=True)\n            logits_one = anchor_dot_contrast_one - logits_max_one.detach()\n            exp_logits_one = torch.exp(logits_one) * neglect_negative_mask_one\n            log_prob_one = logits_one - torch.log(exp_logits_one.sum(1, keepdim=True))\n\n            if opt[\"loss_version\"] == 1:\n                nonzero_idx = torch.where(mask_one.sum(1) != 0.)\n                mask_one = mask_one[nonzero_idx]\n                log_prob_one = log_prob_one[nonzero_idx]\n                mask_ema_one = mask_ema_one[nonzero_idx]\n                weighted_mask = torch.tensor(mask_one.detach()) + torch.tensor(mask_ema_one).detach()*lmbd\n                if opt[\"reweighting\"] == 1:\n                    pnm = torch.tensor(torch.sum(weighted_mask, dim=1), dtype=torch.float32)\n                    pnm = (pnm / torch.sum(pnm))\n                    pnm = pnm / torch.mean(pnm)\n                else:\n                    pnm = 1\n                mean_log_prob_pos_one = (weighted_mask * log_prob_one).sum(1) / (weighted_mask.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n\n            elif opt[\"loss_version\"] == 2:\n                nonzero_idx = torch.where(mask_one.sum(1) != 0.)\n                mask_one = mask_one[nonzero_idx]\n                nonzero_idx_ema = torch.where(mask_ema_one.sum(1) != 0.)\n                mask_ema_one = mask_ema_one[nonzero_idx_ema]\n                if opt[\"reweighting\"] == 1:\n                    pnm = torch.tensor(torch.sum(mask_one, dim=1), dtype=torch.float32)\n                    pnm = (pnm / torch.sum(pnm))\n                    pnm = pnm / torch.mean(pnm)\n                    pnm_ema = torch.tensor(torch.sum(mask_ema_one, dim=1), dtype=torch.float32)\n                    pnm_ema = (pnm_ema / torch.sum(pnm_ema))\n                    pnm_ema = pnm_ema / torch.mean(pnm_ema)\n                else:\n                    pnm = 1\n                    pnm_ema=1\n                mean_log_prob_pos_one = (mask_one * log_prob_one[nonzero_idx]).sum(1) / (mask_one.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n                mean_log_prob_pos_one_ema = (mask_ema_one * log_prob_one).sum(1) / (mask_ema_one.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_ema * pnm_ema) * lmbd\n\n            modeloutput_z_mix_one = modeloutput_z_mix[mi * split: (mi + 1) * split]\n            with torch.cuda.amp.autocast(enabled=True):\n                anchor_dot_contrast_one_lhp = torch.div(\n                    torch.matmul(modeloutput_z_mix_one, modeloutput_z_mix.T),\n                    self.temperature)\n\n            logits_max_one_lhp, _ = torch.max(anchor_dot_contrast_one_lhp, dim=1, keepdim=True)\n            logits_one_lhp = anchor_dot_contrast_one_lhp - logits_max_one_lhp.detach()\n            exp_logits_one_lhp = torch.exp(logits_one_lhp) * neglect_negative_mask_one\n            log_prob_one_lhp = logits_one_lhp - torch.log(exp_logits_one_lhp.sum(1, keepdim=True))\n\n            if opt[\"loss_version\"]==1:\n                log_prob_one_lhp = log_prob_one_lhp[nonzero_idx]\n                mean_log_prob_pos_one_lhp = (weighted_mask * log_prob_one_lhp).sum(1) / (weighted_mask.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_lhp * pnm)\n            elif opt[\"loss_version\"]==2:\n                mean_log_prob_pos_one = (mask_one * log_prob_one[nonzero_idx]).sum(1) / (mask_one.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one * pnm)\n                mean_log_prob_pos_one_ema = (mask_ema_one * log_prob_one[nonzero_idx_ema]).sum(1) / (mask_ema_one.sum(1))\n                loss = loss - torch.mean((self.temperature / self.base_temperature) * mean_log_prob_pos_one_ema * pnm_ema) * lmbd\n\n            negative_mask_one = torch.roll(negative_mask_one, split, dims=1)\n\n        loss = loss / mini_iters / 2\n\n        return loss", ""]}
{"filename": "run.py", "chunked_list": ["from typing import Dict, Tuple\nimport argparse\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.backends import cudnn\nimport torch.distributed as dist\nfrom torch.nn.parallel.distributed import DistributedDataParallel\nfrom torch.utils.data.dataloader import DataLoader", "from torch.nn.parallel.distributed import DistributedDataParallel\nfrom torch.utils.data.dataloader import DataLoader\nimport wandb\nimport os\nfrom tqdm import tqdm\nfrom utils.common_utils import (save_checkpoint, parse, dprint, time_log, compute_param_norm,\n                                freeze_bn, zero_grad_bn, RunningAverage, Timer)\nfrom utils.dist_utils import all_reduce_dict\nfrom utils.wandb_utils import set_wandb\nfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics", "from utils.wandb_utils import set_wandb\nfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics\nfrom build import (build_model, build_criterion, build_dataset, build_dataloader, build_optimizer)\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom torch.optim import Adam, AdamW\nfrom loss import SupConLoss\n\ndef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n    is_train = (not is_test)\n    seed_everything(seed=0)\n    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\n    # -------------------- Folder Setup (Task-Specific) --------------------------#\n    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n    opt[\"full_name\"] = prefix\n\n    cudnn.benchmark = True\n\n    world_size=1\n    local_rank = 0\n    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n\n    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\n    # ------------------------ DataLoader ------------------------------#\n    if is_train:\n        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n    else:\n        train_loader = None\n\n    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n                                  batch_size=16)\n\n    # -------------------------- Define -------------------------------#\n    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n                                                         n_classes=val_dataset.n_classes,\n                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n\n    criterion = build_criterion(n_classes=val_dataset.n_classes,\n                                opt=opt[\"loss\"])\n\n    device = torch.device(\"cuda\", 0)\n    net_model = net_model.to(device)\n    linear_model = linear_model.to(device)\n    cluster_model = cluster_model.to(device)\n\n    project_head = nn.Linear(opt['model']['dim'], opt['model']['dim'])\n    project_head.cuda()\n    head_optimizer = Adam(project_head.parameters(), lr=opt[\"optimizer\"][\"net\"][\"lr\"])\n\n    criterion = criterion.to(device)\n    supcon_criterion = SupConLoss(temperature=opt[\"tau\"]).to(device)\n    pd = nn.PairwiseDistance()\n\n    model = net_model\n    model_m = model\n\n    print(\"Model:\")\n    print(model_m)\n\n    # ------------------- Optimizer  -----------------------#\n    if is_train:\n        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = build_optimizer(\n            main_params=model_m.parameters(),\n            linear_params=linear_model.parameters(),\n            cluster_params=cluster_model.parameters(),\n            opt=opt[\"optimizer\"],\n            model_type=opt[\"model\"][\"name\"])\n    else:\n        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = None, None, None\n\n    start_epoch, current_iter = 0, 0\n    best_metric, best_epoch, best_iter = 0, 0, 0\n\n    num_accum = 1\n\n    timer = Timer()\n\n    if opt[\"model\"][\"pretrained\"][\"model_type\"] == \"vit_small\":\n        feat_dim = 384\n    else:\n        feat_dim = 768\n\n    # ---------------------------- memory ---------------------------- #\n    with torch.no_grad():\n        Pool_ag = torch.zeros((opt[\"model\"][\"pool_size\"], feat_dim), dtype=torch.float16).cuda()\n        Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n        Pool_iter = iter(train_loader_memory)\n\n        for _iter in range(len(train_loader_memory)):\n            data = next(Pool_iter)\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n\n            if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n                break\n            img = img.cuda()\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = net_model(img)\n\n                modeloutput_f = model_output[0].clone().detach()\n                modeloutput_f = modeloutput_f.view(modeloutput_f.size(0), modeloutput_f.size(1), -1)\n\n                modeloutput_s_pr = model_output[2].clone().detach()\n                modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\n            for _iter2 in range(modeloutput_f.size(0)):\n                randidx = np.random.randint(0, model_output[0].size(-1) * model_output[0].size(-2))\n                Pool_ag[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_f[_iter2][:,randidx]\n\n            for _iter2 in range(modeloutput_s_pr.size(0)):\n                randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n                Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:,randidx]\n\n            if _iter % 10 == 0:\n                print (\"Filling Pool Memory [{} / {}]\".format((_iter+1)*opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\n        Pool_ag = F.normalize(Pool_ag, dim=1)\n        Pool_sp = F.normalize(Pool_sp, dim=1)\n\n    # --------------------------- Train --------------------------------#\n    assert is_train\n    max_epoch = opt[\"train\"][\"epoch\"]\n    print_freq = opt[\"train\"][\"print_freq\"]\n    valid_freq = opt[\"train\"][\"valid_freq\"]\n    grad_norm = opt[\"train\"][\"grad_norm\"]\n    freeze_encoder_bn = opt[\"train\"][\"freeze_encoder_bn\"]\n    freeze_all_bn = opt[\"train\"][\"freeze_all_bn\"]\n\n    best_valid_metrics = dict(Cluster_mIoU=0, Cluster_Accuracy=0, Linear_mIoU=0, Linear_Accuracy=0)\n    train_stats = RunningAverage()\n\n    for current_epoch in range(start_epoch, max_epoch):\n        print(f\"-------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\")\n\n        g_norm = torch.zeros(1, dtype=torch.float32, device=device)\n\n        net_model.train()\n        linear_model.train()\n        cluster_model.train()\n        project_head.train()\n\n        train_stats.reset()\n        _ = timer.update()\n\n        maxiter = len(train_loader) * opt[\"train\"][\"epoch\"]\n\n        for i, data in enumerate(train_loader):\n            trainingiter = current_epoch*len(train_loader) + i\n            if trainingiter <= opt[\"model\"][\"warmup\"]:\n                lmbd = 0\n            else:\n                lmbd = (trainingiter - opt[\"model\"][\"warmup\"]) / (maxiter - opt[\"model\"][\"warmup\"])\n\n            # newly initialize\n            if i % opt[\"renew_interval\"] == 0 and i!= 0:\n                with torch.no_grad():\n                    Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n                    for _iter, data in enumerate(train_loader_memory):\n                        if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n                            break\n                        img_net: torch.Tensor = data['img'].to(device, non_blocking=True)\n\n                        with torch.cuda.amp.autocast(enabled=True):\n                            model_output = net_model(img_net)\n\n                            modeloutput_s_pr = model_output[2].clone().detach()\n                            modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\n                        for _iter2 in range(modeloutput_s_pr.size(0)):\n                            randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n                            Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:, randidx]\n\n                        if _iter == 0:\n                            print(\"Filling Pool Memory [{} / {}]\".format(\n                                (_iter + 1) * opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\n                    Pool_sp = F.normalize(Pool_sp, dim=1)\n\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\n            img_aug = data['img_aug'].to(device, non_blocking=True)\n\n            data_time = timer.update()\n\n            if freeze_encoder_bn:\n                freeze_bn(model_m.model)\n            if 0 < freeze_all_bn <= current_epoch:\n                freeze_bn(net_model)\n\n            batch_size = img.shape[0]\n            net_optimizer.zero_grad(set_to_none=True)\n            linear_probe_optimizer.zero_grad(set_to_none=True)\n            cluster_probe_optimizer.zero_grad(set_to_none=True)\n            head_optimizer.zero_grad(set_to_none=True)\n\n            model_input = (img, label)\n\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = net_model(img, train=True)\n                model_output_aug = net_model(img_aug)\n\n            modeloutput_f = model_output[0].clone().detach().permute(0, 2, 3, 1).reshape(-1, feat_dim)\n            modeloutput_f = F.normalize(modeloutput_f, dim=1)\n\n            modeloutput_s = model_output[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\n            modeloutput_s_aug = model_output_aug[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\n            with torch.cuda.amp.autocast(enabled=True):\n                modeloutput_z = project_head(modeloutput_s)\n                modeloutput_z_aug = project_head(modeloutput_s_aug)\n            modeloutput_z = F.normalize(modeloutput_z, dim=1)\n            modeloutput_z_aug = F.normalize(modeloutput_z_aug, dim=1)\n\n            loss_consistency = torch.mean(pd(modeloutput_z, modeloutput_z_aug))\n\n            modeloutput_s_mix = model_output[3].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n            with torch.cuda.amp.autocast(enabled=True):\n                modeloutput_z_mix = project_head(modeloutput_s_mix)\n            modeloutput_z_mix = F.normalize(modeloutput_z_mix, dim=1)\n\n            modeloutput_s_pr = model_output[2].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n            modeloutput_s_pr = F.normalize(modeloutput_s_pr, dim=1)\n\n            loss_supcon = supcon_criterion(modeloutput_z, modeloutput_s_pr=modeloutput_s_pr, modeloutput_f=modeloutput_f,\n                                   Pool_ag=Pool_ag, Pool_sp=Pool_sp,\n                                   opt=opt, lmbd=lmbd, modeloutput_z_mix=modeloutput_z_mix)\n\n\n            detached_code = torch.clone(model_output[1].detach())\n            with torch.cuda.amp.autocast(enabled=True):\n                linear_output = linear_model(detached_code)\n                cluster_output = cluster_model(detached_code, None, is_direct=False)\n\n                loss, loss_dict, corr_dict = criterion(model_input=model_input,\n                                                       model_output=model_output,\n                                                       linear_output=linear_output,\n                                                       cluster_output=cluster_output\n                                                       )\n\n                loss = loss + loss_supcon + loss_consistency*opt[\"alpha\"]\n                # loss = loss / num_accum\n\n\n            forward_time = timer.update()\n\n            scaler.scale(loss).backward()\n\n            if freeze_encoder_bn:\n                zero_grad_bn(model_m)\n            if 0 < freeze_all_bn <= current_epoch:\n                zero_grad_bn(net_model)\n\n            scaler.unscale_(net_optimizer)\n\n            g_norm = nn.utils.clip_grad_norm_(net_model.parameters(), grad_norm)\n            scaler.step(net_optimizer)\n\n            scaler.step(linear_probe_optimizer)\n            scaler.step(cluster_probe_optimizer)\n            scaler.step(head_optimizer)\n\n            scaler.update()\n\n            current_iter += 1\n\n            backward_time = timer.update()\n\n            loss_dict = all_reduce_dict(loss_dict, op=\"mean\")\n            train_stats.append(loss_dict[\"loss\"])\n\n            if i % print_freq == 0:\n                lrs = [int(pg[\"lr\"] * 1e8) / 1e8 for pg in net_optimizer.param_groups]\n                p_norm = compute_param_norm(net_model.parameters())\n                s = time_log()\n                s += f\"epoch: {current_epoch}, iters: {current_iter} \" \\\n                     f\"({i} / {len(train_loader)} -th batch of loader)\\n\"\n                s += f\"loss(now/avg): {loss_dict['loss']:.6f}/{train_stats.avg:.6f}\\n\"\n                if len(loss_dict) > 2:\n                    for loss_k, loss_v in loss_dict.items():\n                        if loss_k != \"loss\":\n                            s += f\"-- {loss_k}(now): {loss_v:.6f}\\n\"\n                            if loss_k == \"corr\":\n                                for k, v in corr_dict.items():\n                                    s += f\"  -- {k}(now): {v:.6f}\\n\"\n                s += f\"time(data/fwd/bwd): {data_time:.3f}/{forward_time:.3f}/{backward_time:.3f}\\n\"\n                s += f\"LR: {lrs}\\n\"\n                s += f\"batch_size x world_size x num_accum: \" \\\n                     f\"{batch_size} x {world_size} x {num_accum} = {batch_size * world_size * num_accum}\\n\"\n                s += f\"norm(param/grad): {p_norm.item():.3f}/{g_norm.item():.3f}\"\n                print(s)\n\n            # --------------------------- Valid --------------------------------#\n            if ((i + 1) % valid_freq == 0) or ((i + 1) == len(train_loader)):\n                _ = timer.update()\n                valid_loss, valid_metrics = evaluate(net_model, linear_model,\n                                                    cluster_model, val_loader,\n                                                     device=device, opt=opt, n_classes=val_dataset.n_classes)\n\n                s = time_log()\n                s += f\"[VAL] -------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\\n\"\n                s += f\"[VAL] epoch: {current_epoch}, iters: {current_iter}\\n\"\n                s += f\"[VAL] loss: {valid_loss:.6f}\\n\"\n\n                metric = \"All\"\n                prev_best_metric = best_metric\n                if best_metric <= (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]):\n                    best_metric = (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"])\n                    best_epoch = current_epoch\n                    best_iter = current_iter\n                    s += f\"[VAL] -------- updated ({metric})! {prev_best_metric:.6f} -> {best_metric:.6f}\\n\"\n\n                    save_checkpoint(\n                        \"ckpt\", net_model, net_optimizer,\n                        linear_model, linear_probe_optimizer,\n                        cluster_model, cluster_probe_optimizer,\n                        current_epoch, current_iter, best_metric, wandb_save_dir, model_only=True)\n                    print (\"SAVED CHECKPOINT\")\n\n                    for metric_k, metric_v in valid_metrics.items():\n                        s += f\"[VAL] {metric_k} : {best_valid_metrics[metric_k]:.6f} -> {metric_v:.6f}\\n\"\n                    best_valid_metrics.update(valid_metrics)\n                else:\n                    now_metric = valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]\n                    s += f\"[VAL] -------- not updated ({metric}).\" \\\n                         f\" (now) {now_metric:.6f} vs (best) {prev_best_metric:.6f}\\n\"\n                    s += f\"[VAL] previous best was at {best_epoch} epoch, {best_iter} iters\\n\"\n                    for metric_k, metric_v in valid_metrics.items():\n                        s += f\"[VAL] {metric_k} : {metric_v:.6f} vs {best_valid_metrics[metric_k]:.6f}\\n\"\n\n                print(s)\n\n                net_model.train()\n                linear_model.train()\n                cluster_model.train()\n                train_stats.reset()\n\n            _ = timer.update()\n\n    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n    loss_out, metrics_out = evaluate(net_model, linear_model,\n        cluster_model, val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes)\n    s = time_log()\n    for metric_k, metric_v in metrics_out.items():\n        s += f\"[before CRF] {metric_k} : {metric_v:.2f}\\n\"\n    print(s)\n\n    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n    loss_out, metrics_out = evaluate(net_model, linear_model, cluster_model,\n        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n    s = time_log()\n    for metric_k, metric_v in metrics_out.items():\n        s += f\"[after CRF] {metric_k} : {metric_v:.2f}\\n\"\n    print(s)\n\n    wandb.finish()\n    print(f\"-------- Train Finished --------\")", "\ndef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n    is_train = (not is_test)\n    seed_everything(seed=0)\n    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\n    # -------------------- Folder Setup (Task-Specific) --------------------------#\n    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n    opt[\"full_name\"] = prefix\n\n    cudnn.benchmark = True\n\n    world_size=1\n    local_rank = 0\n    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n\n    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\n    # ------------------------ DataLoader ------------------------------#\n    if is_train:\n        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n    else:\n        train_loader = None\n\n    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n                                  batch_size=16)\n\n    # -------------------------- Define -------------------------------#\n    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n                                                         n_classes=val_dataset.n_classes,\n                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n\n    criterion = build_criterion(n_classes=val_dataset.n_classes,\n                                opt=opt[\"loss\"])\n\n    device = torch.device(\"cuda\", 0)\n    net_model = net_model.to(device)\n    linear_model = linear_model.to(device)\n    cluster_model = cluster_model.to(device)\n\n    project_head = nn.Linear(opt['model']['dim'], opt['model']['dim'])\n    project_head.cuda()\n    head_optimizer = Adam(project_head.parameters(), lr=opt[\"optimizer\"][\"net\"][\"lr\"])\n\n    criterion = criterion.to(device)\n    supcon_criterion = SupConLoss(temperature=opt[\"tau\"]).to(device)\n    pd = nn.PairwiseDistance()\n\n    model = net_model\n    model_m = model\n\n    print(\"Model:\")\n    print(model_m)\n\n    # ------------------- Optimizer  -----------------------#\n    if is_train:\n        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = build_optimizer(\n            main_params=model_m.parameters(),\n            linear_params=linear_model.parameters(),\n            cluster_params=cluster_model.parameters(),\n            opt=opt[\"optimizer\"],\n            model_type=opt[\"model\"][\"name\"])\n    else:\n        net_optimizer, linear_probe_optimizer, cluster_probe_optimizer = None, None, None\n\n    start_epoch, current_iter = 0, 0\n    best_metric, best_epoch, best_iter = 0, 0, 0\n\n    num_accum = 1\n\n    timer = Timer()\n\n    if opt[\"model\"][\"pretrained\"][\"model_type\"] == \"vit_small\":\n        feat_dim = 384\n    else:\n        feat_dim = 768\n\n    # ---------------------------- memory ---------------------------- #\n    with torch.no_grad():\n        Pool_ag = torch.zeros((opt[\"model\"][\"pool_size\"], feat_dim), dtype=torch.float16).cuda()\n        Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n        Pool_iter = iter(train_loader_memory)\n\n        for _iter in range(len(train_loader_memory)):\n            data = next(Pool_iter)\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n\n            if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n                break\n            img = img.cuda()\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = net_model(img)\n\n                modeloutput_f = model_output[0].clone().detach()\n                modeloutput_f = modeloutput_f.view(modeloutput_f.size(0), modeloutput_f.size(1), -1)\n\n                modeloutput_s_pr = model_output[2].clone().detach()\n                modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\n            for _iter2 in range(modeloutput_f.size(0)):\n                randidx = np.random.randint(0, model_output[0].size(-1) * model_output[0].size(-2))\n                Pool_ag[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_f[_iter2][:,randidx]\n\n            for _iter2 in range(modeloutput_s_pr.size(0)):\n                randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n                Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:,randidx]\n\n            if _iter % 10 == 0:\n                print (\"Filling Pool Memory [{} / {}]\".format((_iter+1)*opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\n        Pool_ag = F.normalize(Pool_ag, dim=1)\n        Pool_sp = F.normalize(Pool_sp, dim=1)\n\n    # --------------------------- Train --------------------------------#\n    assert is_train\n    max_epoch = opt[\"train\"][\"epoch\"]\n    print_freq = opt[\"train\"][\"print_freq\"]\n    valid_freq = opt[\"train\"][\"valid_freq\"]\n    grad_norm = opt[\"train\"][\"grad_norm\"]\n    freeze_encoder_bn = opt[\"train\"][\"freeze_encoder_bn\"]\n    freeze_all_bn = opt[\"train\"][\"freeze_all_bn\"]\n\n    best_valid_metrics = dict(Cluster_mIoU=0, Cluster_Accuracy=0, Linear_mIoU=0, Linear_Accuracy=0)\n    train_stats = RunningAverage()\n\n    for current_epoch in range(start_epoch, max_epoch):\n        print(f\"-------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\")\n\n        g_norm = torch.zeros(1, dtype=torch.float32, device=device)\n\n        net_model.train()\n        linear_model.train()\n        cluster_model.train()\n        project_head.train()\n\n        train_stats.reset()\n        _ = timer.update()\n\n        maxiter = len(train_loader) * opt[\"train\"][\"epoch\"]\n\n        for i, data in enumerate(train_loader):\n            trainingiter = current_epoch*len(train_loader) + i\n            if trainingiter <= opt[\"model\"][\"warmup\"]:\n                lmbd = 0\n            else:\n                lmbd = (trainingiter - opt[\"model\"][\"warmup\"]) / (maxiter - opt[\"model\"][\"warmup\"])\n\n            # newly initialize\n            if i % opt[\"renew_interval\"] == 0 and i!= 0:\n                with torch.no_grad():\n                    Pool_sp = torch.zeros((opt[\"model\"][\"pool_size\"], opt[\"model\"][\"dim\"]), dtype=torch.float16).cuda()\n                    for _iter, data in enumerate(train_loader_memory):\n                        if _iter >= opt[\"model\"][\"pool_size\"] / opt[\"dataloader\"][\"batch_size\"]:\n                            break\n                        img_net: torch.Tensor = data['img'].to(device, non_blocking=True)\n\n                        with torch.cuda.amp.autocast(enabled=True):\n                            model_output = net_model(img_net)\n\n                            modeloutput_s_pr = model_output[2].clone().detach()\n                            modeloutput_s_pr = modeloutput_s_pr.view(modeloutput_s_pr.size(0), modeloutput_s_pr.size(1), -1)\n\n                        for _iter2 in range(modeloutput_s_pr.size(0)):\n                            randidx = np.random.randint(0, model_output[2].size(-1) * model_output[2].size(-2))\n                            Pool_sp[_iter * opt[\"dataloader\"][\"batch_size\"] + _iter2] = modeloutput_s_pr[_iter2][:, randidx]\n\n                        if _iter == 0:\n                            print(\"Filling Pool Memory [{} / {}]\".format(\n                                (_iter + 1) * opt[\"dataloader\"][\"batch_size\"], opt[\"model\"][\"pool_size\"]))\n\n                    Pool_sp = F.normalize(Pool_sp, dim=1)\n\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\n            img_aug = data['img_aug'].to(device, non_blocking=True)\n\n            data_time = timer.update()\n\n            if freeze_encoder_bn:\n                freeze_bn(model_m.model)\n            if 0 < freeze_all_bn <= current_epoch:\n                freeze_bn(net_model)\n\n            batch_size = img.shape[0]\n            net_optimizer.zero_grad(set_to_none=True)\n            linear_probe_optimizer.zero_grad(set_to_none=True)\n            cluster_probe_optimizer.zero_grad(set_to_none=True)\n            head_optimizer.zero_grad(set_to_none=True)\n\n            model_input = (img, label)\n\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = net_model(img, train=True)\n                model_output_aug = net_model(img_aug)\n\n            modeloutput_f = model_output[0].clone().detach().permute(0, 2, 3, 1).reshape(-1, feat_dim)\n            modeloutput_f = F.normalize(modeloutput_f, dim=1)\n\n            modeloutput_s = model_output[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\n            modeloutput_s_aug = model_output_aug[1].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n\n            with torch.cuda.amp.autocast(enabled=True):\n                modeloutput_z = project_head(modeloutput_s)\n                modeloutput_z_aug = project_head(modeloutput_s_aug)\n            modeloutput_z = F.normalize(modeloutput_z, dim=1)\n            modeloutput_z_aug = F.normalize(modeloutput_z_aug, dim=1)\n\n            loss_consistency = torch.mean(pd(modeloutput_z, modeloutput_z_aug))\n\n            modeloutput_s_mix = model_output[3].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n            with torch.cuda.amp.autocast(enabled=True):\n                modeloutput_z_mix = project_head(modeloutput_s_mix)\n            modeloutput_z_mix = F.normalize(modeloutput_z_mix, dim=1)\n\n            modeloutput_s_pr = model_output[2].permute(0, 2, 3, 1).reshape(-1, opt[\"model\"][\"dim\"])\n            modeloutput_s_pr = F.normalize(modeloutput_s_pr, dim=1)\n\n            loss_supcon = supcon_criterion(modeloutput_z, modeloutput_s_pr=modeloutput_s_pr, modeloutput_f=modeloutput_f,\n                                   Pool_ag=Pool_ag, Pool_sp=Pool_sp,\n                                   opt=opt, lmbd=lmbd, modeloutput_z_mix=modeloutput_z_mix)\n\n\n            detached_code = torch.clone(model_output[1].detach())\n            with torch.cuda.amp.autocast(enabled=True):\n                linear_output = linear_model(detached_code)\n                cluster_output = cluster_model(detached_code, None, is_direct=False)\n\n                loss, loss_dict, corr_dict = criterion(model_input=model_input,\n                                                       model_output=model_output,\n                                                       linear_output=linear_output,\n                                                       cluster_output=cluster_output\n                                                       )\n\n                loss = loss + loss_supcon + loss_consistency*opt[\"alpha\"]\n                # loss = loss / num_accum\n\n\n            forward_time = timer.update()\n\n            scaler.scale(loss).backward()\n\n            if freeze_encoder_bn:\n                zero_grad_bn(model_m)\n            if 0 < freeze_all_bn <= current_epoch:\n                zero_grad_bn(net_model)\n\n            scaler.unscale_(net_optimizer)\n\n            g_norm = nn.utils.clip_grad_norm_(net_model.parameters(), grad_norm)\n            scaler.step(net_optimizer)\n\n            scaler.step(linear_probe_optimizer)\n            scaler.step(cluster_probe_optimizer)\n            scaler.step(head_optimizer)\n\n            scaler.update()\n\n            current_iter += 1\n\n            backward_time = timer.update()\n\n            loss_dict = all_reduce_dict(loss_dict, op=\"mean\")\n            train_stats.append(loss_dict[\"loss\"])\n\n            if i % print_freq == 0:\n                lrs = [int(pg[\"lr\"] * 1e8) / 1e8 for pg in net_optimizer.param_groups]\n                p_norm = compute_param_norm(net_model.parameters())\n                s = time_log()\n                s += f\"epoch: {current_epoch}, iters: {current_iter} \" \\\n                     f\"({i} / {len(train_loader)} -th batch of loader)\\n\"\n                s += f\"loss(now/avg): {loss_dict['loss']:.6f}/{train_stats.avg:.6f}\\n\"\n                if len(loss_dict) > 2:\n                    for loss_k, loss_v in loss_dict.items():\n                        if loss_k != \"loss\":\n                            s += f\"-- {loss_k}(now): {loss_v:.6f}\\n\"\n                            if loss_k == \"corr\":\n                                for k, v in corr_dict.items():\n                                    s += f\"  -- {k}(now): {v:.6f}\\n\"\n                s += f\"time(data/fwd/bwd): {data_time:.3f}/{forward_time:.3f}/{backward_time:.3f}\\n\"\n                s += f\"LR: {lrs}\\n\"\n                s += f\"batch_size x world_size x num_accum: \" \\\n                     f\"{batch_size} x {world_size} x {num_accum} = {batch_size * world_size * num_accum}\\n\"\n                s += f\"norm(param/grad): {p_norm.item():.3f}/{g_norm.item():.3f}\"\n                print(s)\n\n            # --------------------------- Valid --------------------------------#\n            if ((i + 1) % valid_freq == 0) or ((i + 1) == len(train_loader)):\n                _ = timer.update()\n                valid_loss, valid_metrics = evaluate(net_model, linear_model,\n                                                    cluster_model, val_loader,\n                                                     device=device, opt=opt, n_classes=val_dataset.n_classes)\n\n                s = time_log()\n                s += f\"[VAL] -------- [{current_epoch}/{max_epoch} (iters: {current_iter})]--------\\n\"\n                s += f\"[VAL] epoch: {current_epoch}, iters: {current_iter}\\n\"\n                s += f\"[VAL] loss: {valid_loss:.6f}\\n\"\n\n                metric = \"All\"\n                prev_best_metric = best_metric\n                if best_metric <= (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]):\n                    best_metric = (valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"])\n                    best_epoch = current_epoch\n                    best_iter = current_iter\n                    s += f\"[VAL] -------- updated ({metric})! {prev_best_metric:.6f} -> {best_metric:.6f}\\n\"\n\n                    save_checkpoint(\n                        \"ckpt\", net_model, net_optimizer,\n                        linear_model, linear_probe_optimizer,\n                        cluster_model, cluster_probe_optimizer,\n                        current_epoch, current_iter, best_metric, wandb_save_dir, model_only=True)\n                    print (\"SAVED CHECKPOINT\")\n\n                    for metric_k, metric_v in valid_metrics.items():\n                        s += f\"[VAL] {metric_k} : {best_valid_metrics[metric_k]:.6f} -> {metric_v:.6f}\\n\"\n                    best_valid_metrics.update(valid_metrics)\n                else:\n                    now_metric = valid_metrics[\"Cluster_mIoU\"] + valid_metrics[\"Cluster_Accuracy\"] + valid_metrics[\"Linear_mIoU\"] + valid_metrics[\"Linear_Accuracy\"]\n                    s += f\"[VAL] -------- not updated ({metric}).\" \\\n                         f\" (now) {now_metric:.6f} vs (best) {prev_best_metric:.6f}\\n\"\n                    s += f\"[VAL] previous best was at {best_epoch} epoch, {best_iter} iters\\n\"\n                    for metric_k, metric_v in valid_metrics.items():\n                        s += f\"[VAL] {metric_k} : {metric_v:.6f} vs {best_valid_metrics[metric_k]:.6f}\\n\"\n\n                print(s)\n\n                net_model.train()\n                linear_model.train()\n                cluster_model.train()\n                train_stats.reset()\n\n            _ = timer.update()\n\n    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n    loss_out, metrics_out = evaluate(net_model, linear_model,\n        cluster_model, val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes)\n    s = time_log()\n    for metric_k, metric_v in metrics_out.items():\n        s += f\"[before CRF] {metric_k} : {metric_v:.2f}\\n\"\n    print(s)\n\n    checkpoint_loaded = torch.load(f\"{wandb_save_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n    loss_out, metrics_out = evaluate(net_model, linear_model, cluster_model,\n        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n    s = time_log()\n    for metric_k, metric_v in metrics_out.items():\n        s += f\"[after CRF] {metric_k} : {metric_v:.2f}\\n\"\n    print(s)\n\n    wandb.finish()\n    print(f\"-------- Train Finished --------\")", "\n\ndef evaluate(net_model: nn.Module,\n             linear_model: nn.Module,\n             cluster_model: nn.Module,\n             eval_loader: DataLoader,\n             device: torch.device,\n             opt: Dict,\n             n_classes: int,\n             is_crf: bool = False,\n             data_type: str = \"\",\n             ) -> Tuple[float, Dict[str, float]]:\n\n    net_model.eval()\n\n    cluster_metrics = UnsupervisedMetrics(\n        \"Cluster_\", n_classes, opt[\"eval\"][\"extra_clusters\"], True)\n    linear_metrics = UnsupervisedMetrics(\n        \"Linear_\", n_classes, 0, False)\n\n    with torch.no_grad():\n        eval_stats = RunningAverage()\n\n        for i, data in enumerate(tqdm(eval_loader)):\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=True):\n                output = net_model(img)\n            feats = output[0]\n            head_code = output[1]\n\n            head_code = F.interpolate(head_code, label.shape[-2:], mode='bilinear', align_corners=False)\n\n            if is_crf:\n                with torch.cuda.amp.autocast(enabled=True):\n                    linear_preds = torch.log_softmax(linear_model(head_code), dim=1)\n\n                with torch.cuda.amp.autocast(enabled=True):\n                    cluster_loss, cluster_preds = cluster_model(head_code, 2, log_probs=True, is_direct=opt[\"eval\"][\"is_direct\"])\n                linear_preds = batched_crf(img, linear_preds).argmax(1).cuda()\n                cluster_preds = batched_crf(img, cluster_preds).argmax(1).cuda()\n\n            else:\n                with torch.cuda.amp.autocast(enabled=True):\n                    linear_preds = linear_model(head_code).argmax(1)\n\n                with torch.cuda.amp.autocast(enabled=True):\n                    cluster_loss, cluster_preds = cluster_model(head_code, None, is_direct=opt[\"eval\"][\"is_direct\"])\n                cluster_preds = cluster_preds.argmax(1)\n\n            linear_metrics.update(linear_preds, label)\n            cluster_metrics.update(cluster_preds, label)\n\n            eval_stats.append(cluster_loss)\n\n        eval_metrics = get_metrics(cluster_metrics, linear_metrics)\n\n        return eval_stats.avg, eval_metrics", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Test mode, no WandB, highest priority.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode, no WandB, second highest priority.\")\n    parser.add_argument(\"--checkpoint\", type=str, default=None, help=\"Checkpoint override\")\n    parser.add_argument(\"--data_path\", type=str, default=None, help=\"Data path override\")\n\n    parser_args = parser.parse_args()\n    parser_opt = parse(parser_args.opt)\n    if parser_args.checkpoint is not None:\n        parser_opt[\"checkpoint\"] = parser_args.checkpoint\n    if parser_args.data_path is not None:\n        parser_opt[\"dataset\"][\"data_path\"] = parser_args.data_path\n\n    run(parser_opt, is_test=parser_args.test, is_debug=parser_args.debug)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval.py", "chunked_list": ["from typing import Dict, Tuple\nimport argparse\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # noqa\nfrom torch.backends import cudnn\nimport torch.distributed as dist\nfrom torch.nn.parallel.distributed import DistributedDataParallel\nfrom torch.utils.data.dataloader import DataLoader", "from torch.nn.parallel.distributed import DistributedDataParallel\nfrom torch.utils.data.dataloader import DataLoader\nimport wandb\nimport os\nfrom tqdm import tqdm\nfrom utils.common_utils import (save_checkpoint, parse, dprint, time_log, compute_param_norm,\n                                freeze_bn, zero_grad_bn, RunningAverage, Timer)\nfrom utils.dist_utils import all_reduce_dict\nfrom utils.wandb_utils import set_wandb\nfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics", "from utils.wandb_utils import set_wandb\nfrom utils.seg_utils import UnsupervisedMetrics, batched_crf, get_metrics\nfrom build import (build_model, build_criterion, build_dataset, build_dataloader, build_optimizer)\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom torch.optim import Adam, AdamW\nfrom loss import SupConLoss\n\ndef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n    is_train = (not is_test)\n    seed_everything(seed=0)\n    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\n    # -------------------- Folder Setup (Task-Specific) --------------------------#\n    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n    opt[\"full_name\"] = prefix\n\n    # -------------------- Distributed Setup --------------------------#\n    if (opt[\"num_gpus\"] == 0) or (not torch.cuda.is_available()):\n        raise ValueError(\"Run requires at least 1 GPU.\")\n\n    if (opt[\"num_gpus\"] > 1) and (not dist.is_initialized()):\n        assert dist.is_available()\n        dist.init_process_group(backend=\"nccl\")  # nccl for NVIDIA GPUs\n        world_size = int(dist.get_world_size())\n        local_rank = int(dist.get_rank())\n        torch.cuda.set_device(local_rank)\n        print_fn = partial(dprint, local_rank=local_rank)  # only prints when local_rank == 0\n        is_distributed = True\n    else:\n        world_size = 1\n        local_rank = 0\n        print_fn = print\n        is_distributed = False\n\n    cudnn.benchmark = True\n\n    is_master = (local_rank == 0)\n    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n\n    if not wandb_save_dir:\n        wandb_save_dir = os.path.join(opt[\"output_dir\"], opt[\"wandb\"][\"name\"])\n    if is_test:\n        wandb_save_dir = \"/\".join(opt[\"checkpoint\"].split(\"/\")[:-1])\n\n    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\n    # ------------------------ DataLoader ------------------------------#\n    if is_train:\n        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n    else:\n        train_loader = None\n\n    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n                                  batch_size=world_size*32)\n\n    # -------------------------- Define -------------------------------#\n    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n                                                         n_classes=val_dataset.n_classes,\n                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n\n    device = torch.device(\"cuda\", local_rank)\n    net_model = net_model.to(device)\n    linear_model = linear_model.to(device)\n    cluster_model = cluster_model.to(device)\n\n    model = net_model\n    model_m = model\n\n    print_fn(\"Model:\")\n    print_fn(model_m)\n\n\n    # --------------------------- Evaluate with Best --------------------------------#\n    loading_dir = os.path.join(opt['output_dir'], opt['checkpoint'])\n    checkpoint_loaded = torch.load(f\"{loading_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n\n    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model, val_loader, device=device,\n                                                                            opt=opt, n_classes=train_dataset.n_classes)\n    s = time_log()\n    s += f\" ------------------- before crf ---------------------\\n\"\n    for metric_k, metric_v in metrics_.items():\n        s += f\"before crf{metric_k} : {metric_v:.2f}\\n\"\n    print_fn(s)\n\n\n    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model,\n        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n\n    s = time_log()\n    s += f\" -------------------after crf ---------------------\\n\"\n    for metric_k, metric_v in metrics_.items():\n        s += f\"[after crf] {metric_k} : {metric_v:.2f}\\n\"\n    print_fn(s)", "\ndef run(opt: dict, is_test: bool = False, is_debug: bool = False):\n    is_train = (not is_test)\n    seed_everything(seed=0)\n    scaler = torch.cuda.amp.GradScaler(init_scale=2048, growth_interval=1000, enabled=True)\n\n    # -------------------- Folder Setup (Task-Specific) --------------------------#\n    prefix = \"{}/{}_{}\".format(opt[\"output_dir\"], opt[\"dataset\"][\"data_type\"], opt[\"wandb\"][\"name\"])\n    opt[\"full_name\"] = prefix\n\n    # -------------------- Distributed Setup --------------------------#\n    if (opt[\"num_gpus\"] == 0) or (not torch.cuda.is_available()):\n        raise ValueError(\"Run requires at least 1 GPU.\")\n\n    if (opt[\"num_gpus\"] > 1) and (not dist.is_initialized()):\n        assert dist.is_available()\n        dist.init_process_group(backend=\"nccl\")  # nccl for NVIDIA GPUs\n        world_size = int(dist.get_world_size())\n        local_rank = int(dist.get_rank())\n        torch.cuda.set_device(local_rank)\n        print_fn = partial(dprint, local_rank=local_rank)  # only prints when local_rank == 0\n        is_distributed = True\n    else:\n        world_size = 1\n        local_rank = 0\n        print_fn = print\n        is_distributed = False\n\n    cudnn.benchmark = True\n\n    is_master = (local_rank == 0)\n    wandb_save_dir = set_wandb(opt, local_rank, force_mode=\"disabled\" if (is_debug or is_test) else None)\n\n    if not wandb_save_dir:\n        wandb_save_dir = os.path.join(opt[\"output_dir\"], opt[\"wandb\"][\"name\"])\n    if is_test:\n        wandb_save_dir = \"/\".join(opt[\"checkpoint\"].split(\"/\")[:-1])\n\n    train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    train_loader_memory = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n\n    # ------------------------ DataLoader ------------------------------#\n    if is_train:\n        train_dataset = build_dataset(opt[\"dataset\"], mode=\"train\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n        train_loader = build_dataloader(train_dataset, opt[\"dataloader\"], shuffle=True)\n    else:\n        train_loader = None\n\n    val_dataset = build_dataset(opt[\"dataset\"], mode=\"val\", model_type=opt[\"model\"][\"pretrained\"][\"model_type\"])\n    val_loader = build_dataloader(val_dataset, opt[\"dataloader\"], shuffle=False,\n                                  batch_size=world_size*32)\n\n    # -------------------------- Define -------------------------------#\n    net_model, linear_model, cluster_model = build_model(opt=opt[\"model\"],\n                                                         n_classes=val_dataset.n_classes,\n                                                         is_direct=opt[\"eval\"][\"is_direct\"])\n\n    device = torch.device(\"cuda\", local_rank)\n    net_model = net_model.to(device)\n    linear_model = linear_model.to(device)\n    cluster_model = cluster_model.to(device)\n\n    model = net_model\n    model_m = model\n\n    print_fn(\"Model:\")\n    print_fn(model_m)\n\n\n    # --------------------------- Evaluate with Best --------------------------------#\n    loading_dir = os.path.join(opt['output_dir'], opt['checkpoint'])\n    checkpoint_loaded = torch.load(f\"{loading_dir}/ckpt.pth\", map_location=device)\n    net_model.load_state_dict(checkpoint_loaded['net_model_state_dict'], strict=True)\n    linear_model.load_state_dict(checkpoint_loaded['linear_model_state_dict'], strict=True)\n    cluster_model.load_state_dict(checkpoint_loaded['cluster_model_state_dict'], strict=True)\n\n    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model, val_loader, device=device,\n                                                                            opt=opt, n_classes=train_dataset.n_classes)\n    s = time_log()\n    s += f\" ------------------- before crf ---------------------\\n\"\n    for metric_k, metric_v in metrics_.items():\n        s += f\"before crf{metric_k} : {metric_v:.2f}\\n\"\n    print_fn(s)\n\n\n    loss_, metrics_ = evaluate(net_model, linear_model, cluster_model,\n        val_loader, device=device, opt=opt, n_classes=train_dataset.n_classes, is_crf=opt[\"eval\"][\"is_crf\"])\n\n    s = time_log()\n    s += f\" -------------------after crf ---------------------\\n\"\n    for metric_k, metric_v in metrics_.items():\n        s += f\"[after crf] {metric_k} : {metric_v:.2f}\\n\"\n    print_fn(s)", "\n\ndef evaluate(net_model: nn.Module,\n             linear_model: nn.Module,\n             cluster_model: nn.Module,\n             eval_loader: DataLoader,\n             device: torch.device,\n             opt: Dict,\n             n_classes: int,\n             is_crf: bool = False,\n             data_type: str = \"\",\n             ) -> Tuple[float, Dict[str, float]]:\n\n    net_model.eval()\n\n    cluster_metrics = UnsupervisedMetrics(\n        \"Cluster_\", n_classes, opt[\"eval\"][\"extra_clusters\"], True)\n    linear_metrics = UnsupervisedMetrics(\n        \"Linear_\", n_classes, 0, False)\n\n    with torch.no_grad():\n        eval_stats = RunningAverage()\n\n        for i, data in enumerate(tqdm(eval_loader)):\n            img: torch.Tensor = data['img'].to(device, non_blocking=True)\n            label: torch.Tensor = data['label'].to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=True):\n                output = net_model(img)\n            feats = output[0]\n            head_code = output[1]\n\n            head_code = F.interpolate(head_code, label.shape[-2:], mode='bilinear', align_corners=False)\n\n            if is_crf:\n                with torch.cuda.amp.autocast(enabled=True):\n                    linear_preds = torch.log_softmax(linear_model(head_code), dim=1)\n\n                with torch.cuda.amp.autocast(enabled=True):\n                    cluster_loss, cluster_preds = cluster_model(head_code, 2, log_probs=True, is_direct=opt[\"eval\"][\"is_direct\"])\n                linear_preds = batched_crf(img, linear_preds).argmax(1).cuda()\n                cluster_preds = batched_crf(img, cluster_preds).argmax(1).cuda()\n\n            else:\n                with torch.cuda.amp.autocast(enabled=True):\n                    linear_preds = linear_model(head_code).argmax(1)\n\n                with torch.cuda.amp.autocast(enabled=True):\n                    cluster_loss, cluster_preds = cluster_model(head_code, None, is_direct=opt[\"eval\"][\"is_direct\"])\n                cluster_preds = cluster_preds.argmax(1)\n\n            linear_metrics.update(linear_preds, label)\n            cluster_metrics.update(cluster_preds, label)\n\n            eval_stats.append(cluster_loss)\n\n        eval_metrics = get_metrics(cluster_metrics, linear_metrics)\n\n        return eval_stats.avg, eval_metrics", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Test mode, no WandB, highest priority.\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode, no WandB, second highest priority.\")\n    parser.add_argument(\"--checkpoint\", type=str, default=None, help=\"Checkpoint override\")\n    parser.add_argument(\"--data_path\", type=str, default=None, help=\"Data path override\")\n\n    parser_args = parser.parse_args()\n    parser_opt = parse(parser_args.opt)\n    # if parser_args.checkpoint is not None:\n    #     parser_opt[\"checkpoint\"] = parser_args.checkpoint\n    if parser_args.data_path is not None:\n        parser_opt[\"dataset\"][\"data_path\"] = parser_args.data_path\n\n    run(parser_opt, is_test=parser_args.test, is_debug=parser_args.debug)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "build.py", "chunked_list": ["from typing import Optional, Dict\nfrom torch.optim import Adam, AdamW\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.dataloader import DataLoader\nfrom model.STEGO import (STEGOmodel)\nfrom model.LambdaLayer import LambdaLayer\nfrom model.dino.DinoFeaturizer import DinoFeaturizer\nfrom dataset.data import ContrastiveSegDataset, get_transform\nfrom torchvision import transforms as T", "from dataset.data import ContrastiveSegDataset, get_transform\nfrom torchvision import transforms as T\nfrom loss import *\n\n\ndef build_model(opt: dict, n_classes: int = 27, is_direct: bool = False):\n    model_type = opt[\"name\"].lower()\n\n    if \"stego\" in model_type:\n        model = STEGOmodel.build(\n            opt=opt,\n            n_classes=n_classes\n        )\n        net_model = model.net\n        linear_model = model.linear_probe\n        cluster_model = model.cluster_probe\n\n    elif model_type == \"dino\":\n        model = nn.Sequential(\n            DinoFeaturizer(20, opt),\n            LambdaLayer(lambda p: p[0])\n        )\n\n    else:\n        raise ValueError(\"No model: {} found\".format(model_type))\n\n    bn_momentum = opt.get(\"bn_momentum\", None)\n    if bn_momentum is not None:\n        for module_name, module in model.named_modules():\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n                module.momentum = bn_momentum\n\n    bn_eps = opt.get(\"bn_eps\", None)\n    if bn_eps is not None:\n        for module_name, module in model.named_modules():\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n                module.eps = bn_eps\n\n    if \"stego\" in model_type:\n        return net_model, linear_model, cluster_model\n    elif model_type == \"dino\":\n        return model", "\n\ndef build_criterion(n_classes: int, opt: dict):\n    # opt = opt[\"loss\"]\n    loss_name = opt[\"name\"].lower()\n    if \"stego\" in loss_name:\n        loss = StegoLoss(n_classes=n_classes, cfg=opt, corr_weight=opt[\"correspondence_weight\"])\n    else:\n        raise ValueError(f\"Unsupported loss type {loss_name}\")\n\n    return loss", "\n\ndef split_params_for_optimizer(model, opt):\n    # opt = opt[\"optimizer\"]\n    params_small_lr = []\n    params_small_lr_no_wd = []\n    params_base_lr = []\n    params_base_lr_no_wd = []\n    for param_name, param_value in model.named_parameters():\n        param_value: torch.Tensor\n        if not param_value.requires_grad:\n            continue\n        if \"encoder\" in param_name:\n            if (param_value.ndim > 1) and (\"position\" not in param_name):\n                params_small_lr.append(param_value)\n            else:\n                params_small_lr_no_wd.append(param_value)\n        else:  # decoder\n            if (param_value.ndim > 1) and (\"position\" not in param_name):\n                params_base_lr.append(param_value)\n            else:\n                params_base_lr_no_wd.append(param_value)\n\n    same_lr = opt.get(\"same_lr\", True)\n    encoder_weight = 1.0 if same_lr else 0.1\n    params_for_optimizer = [\n        {\"params\": params_base_lr},\n        {\"params\": params_base_lr_no_wd, \"weight_decay\": 0.0},\n        # {\"params\": params_small_lr, \"lr\": opt[\"lr\"] * encoder_weight, \"weight_decay\": opt[\"weight_decay\"] * 0.1},\n        {\"params\": params_small_lr, \"lr\": opt[\"lr\"] * encoder_weight},\n        {\"params\": params_small_lr_no_wd, \"lr\": opt[\"lr\"] * encoder_weight, \"weight_decay\": 0.0},\n    ]\n    return params_for_optimizer", "\n\ndef build_optimizer(main_params, linear_params, cluster_params, opt: dict, model_type: str):\n    # opt = opt[\"optimizer\"]\n    model_type = model_type.lower()\n\n    if \"stego\" in model_type:\n        net_optimizer_type = opt[\"net\"][\"name\"].lower()\n        if net_optimizer_type == \"adam\":\n            net_optimizer = Adam(main_params, lr=opt[\"net\"][\"lr\"])\n        elif net_optimizer_type == \"adamw\":\n            net_optimizer = AdamW(main_params, lr=opt[\"net\"][\"lr\"], weight_decay=opt[\"net\"][\"weight_decay\"])\n        else:\n            raise ValueError(f\"Unsupported optimizer type {net_optimizer_type}.\")\n\n        linear_probe_optimizer_type = opt[\"linear\"][\"name\"].lower()\n        if linear_probe_optimizer_type == \"adam\":\n            linear_probe_optimizer = Adam(linear_params, lr=opt[\"linear\"][\"lr\"])\n        else:\n            raise ValueError(f\"Unsupported optimizer type {linear_probe_optimizer_type}.\")\n\n        cluster_probe_optimizer_type = opt[\"cluster\"][\"name\"].lower()\n        if cluster_probe_optimizer_type == \"adam\":\n            cluster_probe_optimizer = Adam(cluster_params, lr=opt[\"cluster\"][\"lr\"])\n        else:\n            raise ValueError(f\"Unsupported optimizer type {cluster_probe_optimizer_type}.\")\n\n\n        return net_optimizer, linear_probe_optimizer, cluster_probe_optimizer\n\n    else:\n        raise ValueError(\"No model: {} found\".format(model_type))", "\n\ndef build_scheduler(opt: dict, optimizer, loader, start_epoch):\n    # opt = opt BE CAREFUL!\n    scheduler_type = opt[\"scheduler\"]['name'].lower()\n\n    if scheduler_type == \"onecycle\":\n        max_lrs = [pg[\"lr\"] for pg in optimizer.param_groups]\n\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(  # noqa\n            optimizer,\n            # max_lr=opt['optimizer']['lr'],\n            max_lr=max_lrs,\n            epochs=opt['train']['epoch'] + 1,\n            steps_per_epoch=len(loader) // opt[\"train\"][\"num_accum\"],\n            cycle_momentum=opt[\"scheduler\"].get(\"cycle_momentum\", True),\n            base_momentum=0.85,\n            max_momentum=0.95,\n            pct_start=opt[\"scheduler\"][\"pct_start\"],\n            last_epoch=start_epoch - 1,\n            div_factor=opt[\"scheduler\"]['div_factor'],\n            final_div_factor=opt[\"scheduler\"]['final_div_factor']\n        )\n    else:\n        raise ValueError(f\"Unsupported scheduler type {scheduler_type}.\")\n\n    return scheduler", "\n\ndef build_dataset(opt: dict, mode: str = \"train\", model_type: str = \"dino\") -> ContrastiveSegDataset:\n    # opt = opt[\"dataset\"]\n    data_type = opt[\"data_type\"].lower()\n\n    if mode == \"train\":\n        geometric_transforms = T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomResizedCrop(size=opt[\"res\"], scale=(0.8, 1.0))\n        ])\n        photometric_transforms = T.Compose([\n            T.ColorJitter(brightness=.3, contrast=.3, saturation=.3, hue=.1),\n            T.RandomGrayscale(.2),\n            T.RandomApply([T.GaussianBlur((5, 5))])\n        ])\n\n        return ContrastiveSegDataset(\n            pytorch_data_dir=opt[\"data_path\"],\n            dataset_name=opt[\"data_type\"],\n            crop_type=opt[\"crop_type\"],\n            model_type=model_type,\n            image_set=mode,\n            transform=get_transform(opt[\"res\"], False, opt[\"loader_crop_type\"]),\n            target_transform=get_transform(opt[\"res\"], True, opt[\"loader_crop_type\"]),\n            cfg=opt,\n            aug_geometric_transform=geometric_transforms,\n            aug_photometric_transform=photometric_transforms,\n            num_neighbors=opt[\"num_neighbors\"],\n            mask=True,\n            pos_images=False,\n            pos_labels=False\n        )\n    elif mode == \"val\" or mode == \"test\":\n        if mode == \"test\":\n            loader_crop = \"center\"\n        elif data_type == \"voc\":\n            loader_crop = None\n        else:\n            loader_crop = \"center\"\n\n        return ContrastiveSegDataset(\n            pytorch_data_dir=opt[\"data_path\"],\n            dataset_name=opt[\"data_type\"],\n            crop_type=None,\n            model_type=model_type,\n            image_set=\"val\",\n            transform=get_transform(320, False, loader_crop),\n            target_transform=get_transform(320, True, loader_crop),\n            mask=True,\n            cfg=opt,\n        )", "\n\ndef build_dataloader(dataset,\n                     opt: dict, shuffle: bool = True, pin_memory: bool = True,\n                     batch_size: Optional[int] = None) -> DataLoader:\n    # opt = opt[\"dataloader\"]\n\n    if batch_size is None:  # override\n        batch_size = opt[\"batch_size\"]\n\n    if not dist.is_initialized():\n        return DataLoader(\n            dataset,\n            batch_size=max(batch_size, 1),\n            shuffle=shuffle,\n            num_workers=opt.get(\"num_workers\", 4),\n            pin_memory=pin_memory,\n            drop_last=shuffle,\n        )\n    else:\n        assert dist.is_available() and dist.is_initialized()\n        ddp_sampler = DistributedSampler(\n            dataset,\n            num_replicas=dist.get_world_size(),\n            rank=dist.get_rank(),\n            shuffle=shuffle,\n            drop_last=shuffle,\n        )\n        world_size = dist.get_world_size()\n        return DataLoader(\n            dataset,\n            batch_size=max(batch_size // world_size, 1),\n            num_workers=(opt.get(\"num_workers\", 4) + world_size - 1) // world_size,\n            pin_memory=pin_memory,\n            sampler=ddp_sampler,\n            prefetch_factor=opt.get(\"prefetch_factor\", 1)\n        )", ""]}
{"filename": "visualize.py", "chunked_list": ["import torch.nn.functional as F\nimport numpy as np\nimport os\nfrom collections import defaultdict\nfrom os.path import join\nfrom dataset.data import create_cityscapes_colormap, create_pascal_label_colormap\nfrom utils.seg_utils import unnorm\nfrom PIL import Image\nfrom utils.seg_utils import UnsupervisedMetrics\n", "from utils.seg_utils import UnsupervisedMetrics\n\n\ndef prep_for_plot(img, rescale=True, resize=None):\n    if resize is not None:\n        img = F.interpolate(img.unsqueeze(0), resize, mode=\"bilinear\")\n    else:\n        img = img.unsqueeze(0)\n    plot_img = unnorm(img).squeeze(0).squeeze(0).cpu().permute(1, 2, 0)\n    if rescale:\n        plot_img = (plot_img - plot_img.min()) / (plot_img.max() - plot_img.min())\n    return plot_img", "\n\ndef visualization(save_dir: str, dataset_type: str, saved_data: defaultdict, cluster_metrics: UnsupervisedMetrics,\n                  is_label: bool = False):\n    if is_label:\n        os.makedirs(join(save_dir, \"label\"), exist_ok=True)\n    os.makedirs(join(save_dir, \"cluster\"), exist_ok=True)\n    os.makedirs(join(save_dir, \"linear\"), exist_ok=True)\n\n    if dataset_type.startswith(\"cityscapes\"):\n        label_cmap = create_cityscapes_colormap()\n    else:\n        label_cmap = create_pascal_label_colormap()\n\n    for index in range(len(saved_data[\"img_path\"])):\n        file_name = str(saved_data[\"img_path\"][index]).split(\"/\")[-1].split(\".\")[0]\n        if is_label:\n            plot_label = (label_cmap[saved_data[\"label\"][index]]).astype(np.uint8)\n            Image.fromarray(plot_label).save(join(join(save_dir, \"label\", file_name + \".png\")))\n\n        plot_cluster = (label_cmap[cluster_metrics.map_clusters(saved_data[\"cluster_preds\"][index])]).astype(np.uint8)\n        Image.fromarray(plot_cluster).save(join(join(save_dir, \"cluster\", file_name + \".png\")))\n\n        plot_linear = (label_cmap[saved_data[\"linear_preds\"][index]]).astype(np.uint8)\n        Image.fromarray(plot_linear).save(join(join(save_dir, \"linear\", file_name + \".png\")))", "\ndef visualization_label(save_dir: str, saved_data: defaultdict):\n    label_cmap = create_pascal_label_colormap()\n\n    for index in range(saved_data[\"label\"][0].size(0)):\n        os.makedirs(join('./visualize/attn', str(index)), exist_ok=True)\n\n        plot_label = (label_cmap[saved_data[\"label\"][0][index]]).astype(np.uint8)\n        imagename = str(index)+\"_classlabel.png\"\n        Image.fromarray(plot_label).save(join(save_dir, str(index), imagename))", ""]}
{"filename": "utils/augment_utils.py", "chunked_list": ["import torchvision.transforms as transforms\nfrom PIL import ImageFilter, ImageOps, Image\nimport random\nimport numpy as np\n\nclass GaussianBlur(object):\n    \"\"\"\n    Apply Gaussian Blur to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img):\n        do_it = random.random() <= self.prob\n        if not do_it:\n            return img\n\n        return img.filter(\n            ImageFilter.GaussianBlur(\n                radius=random.uniform(self.radius_min, self.radius_max)\n            )\n        )", "\nclass Solarization(object):\n    \"\"\"\n    Apply Solarization to the PIL image.\n    \"\"\"\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return ImageOps.solarize(img)\n        else:\n            return img", "\nclass DataAugmentationDINO(object):\n    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n\n        flip_and_color_jitter = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomApply(\n                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n                p=0.8\n            ),\n            transforms.RandomGrayscale(p=0.2),\n        ])\n        normalize = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n\n        # first global crop\n        self.global_transfo1 = transforms.Compose([\n            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n            flip_and_color_jitter,\n            GaussianBlur(1.0),\n            normalize,\n        ])\n        # second global crop\n        self.global_transfo2 = transforms.Compose([\n            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n            flip_and_color_jitter,\n            GaussianBlur(0.1),\n            Solarization(0.2),\n            normalize,\n        ])\n        # transformation for the local small crops\n        self.local_crops_number = local_crops_number\n        self.local_transfo = transforms.Compose([\n            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n            flip_and_color_jitter,\n            GaussianBlur(p=0.5),\n            normalize,\n        ])\n\n    def __call__(self, image):\n        crops = []\n        crops.append(self.global_transfo1(image))\n        crops.append(self.global_transfo2(image))\n        for _ in range(self.local_crops_number):\n            crops.append(self.local_transfo(image))\n        return crops"]}
{"filename": "utils/dist_utils.py", "chunked_list": ["from typing import Any, Dict, List\nfrom numbers import Number\nimport torch\nimport torch.distributed as dist\n\n\n__all__ = [\n    \"all_reduce_scalar\",\n    \"all_reduce_tensor\",\n    \"all_reduce_dict\",", "    \"all_reduce_tensor\",\n    \"all_reduce_dict\",\n    \"all_gather_tensor\",\n]\n\n\ndef all_reduce_scalar(value: Number, op: str = \"sum\") -> Number:\n    \"\"\"All-reduce single scalar value. NOT torch tensor.\"\"\"\n    # https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/utils/distributed.py\n    if dist.is_initialized() and dist.is_available():\n        op = op.lower()\n        if (op == \"sum\") or (op == \"mean\"):\n            dist_op = dist.ReduceOp.SUM\n        elif op == \"min\":\n            dist_op = dist.ReduceOp.MIN\n        elif op == \"max\":\n            dist_op = dist.ReduceOp.MAX\n        elif op == \"product\":\n            dist_op = dist.ReduceOp.PRODUCT\n        else:\n            raise RuntimeError(f\"Invalid all_reduce op: {op}\")\n\n        backend = dist.get_backend()\n        if backend == torch.distributed.Backend.NCCL:\n            device = torch.device(\"cuda\")\n        elif backend == torch.distributed.Backend.GLOO:\n            device = torch.device(\"cpu\")\n        else:\n            raise RuntimeError(f\"Unsupported distributed backend: {backend}\")\n\n        tensor = torch.tensor(value, device=device, requires_grad=False)\n        dist.all_reduce(tensor, op=dist_op)\n        if op == \"mean\":\n            tensor /= dist.get_world_size()\n        ret = tensor.item()\n    else:\n        ret = value\n    return ret", "\n\ndef all_reduce_tensor(tensor: torch.Tensor, op=\"sum\", detach: bool = True) -> torch.Tensor:\n    if dist.is_initialized() and dist.is_available():\n        ret = tensor.clone()\n        if detach:\n            ret = ret.detach()\n        if (op == \"sum\") or (op == \"mean\"):\n            dist_op = dist.ReduceOp.SUM\n        else:\n            raise RuntimeError(f\"Invalid all_reduce op: {op}\")\n\n        dist.all_reduce(ret, op=dist_op)\n        if op == \"mean\":\n            ret /= dist.get_world_size()\n    else:\n        ret = tensor\n    return ret", "\n\ndef all_reduce_dict(result: Dict[str, Any], op=\"sum\") -> Dict[str, Any]:\n    new_result = {}\n    for k, v in result.items():\n        if isinstance(v, torch.Tensor):\n            new_result[k] = all_reduce_tensor(v, op)\n        elif isinstance(v, Number):\n            new_result[k] = all_reduce_scalar(v, op)\n        else:\n            raise RuntimeError(f\"Dictionary all_reduce should only have either tensor or scalar, got: {type(v)}\")\n    return new_result", "\n\ndef all_gather_tensor(tensor: torch.Tensor) -> List[torch.Tensor]:\n    if dist.is_initialized() and dist.is_available():\n        world_size = dist.get_world_size()\n        local_rank = dist.get_rank()\n        output = [\n            tensor if (i == local_rank) else torch.empty_like(tensor) for i in range(world_size)\n        ]\n        dist.all_gather(output, tensor, async_op=False)\n        return output\n    else:\n        return [tensor]", ""]}
{"filename": "utils/layer_utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ClusterLookup(nn.Module):\n\n    def __init__(self, dim: int, n_classes: int):\n        super(ClusterLookup, self).__init__()\n        self.n_classes = n_classes\n        self.dim = dim\n        self.clusters = torch.nn.Parameter(torch.randn(n_classes, dim))\n\n    def reset_parameters(self):\n        with torch.no_grad():\n            self.clusters.copy_(torch.randn(self.n_classes, self.dim))\n\n    def forward(self, x, alpha, log_probs=False, is_direct=False):\n        if is_direct:\n            inner_products = x\n        else:\n            normed_clusters = F.normalize(self.clusters, dim=1)\n            normed_features = F.normalize(x, dim=1)\n            inner_products = torch.einsum(\"bchw,nc->bnhw\", normed_features, normed_clusters)\n\n        if alpha is None:\n            cluster_probs = F.one_hot(torch.argmax(inner_products, dim=1), self.clusters.shape[0]) \\\n                .permute(0, 3, 1, 2).to(torch.float32)\n        else:\n            cluster_probs = nn.functional.softmax(inner_products * alpha, dim=1)\n        cluster_loss = -(cluster_probs * inner_products).sum(1).mean()\n\n        if log_probs:\n            return cluster_loss, nn.functional.log_softmax(inner_products * alpha, dim=1)\n        else:\n            return cluster_loss, cluster_probs", ""]}
{"filename": "utils/__init__.py", "chunked_list": [""]}
{"filename": "utils/seg_utils.py", "chunked_list": ["from typing import Dict, Any\nfrom torchmetrics import Metric\nfrom scipy.optimize import linear_sum_assignment\nfrom utils.dist_utils import all_reduce_dict\n\nimport numpy as np\nimport pydensecrf.densecrf as dcrf\nimport pydensecrf.utils as utils\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as VF\n\nMAX_ITER = 10\nPOS_W = 3\nPOS_XY_STD = 1\nBi_W = 4\nBi_XY_STD = 67\nBi_RGB_STD = 3", "Bi_XY_STD = 67\nBi_RGB_STD = 3\nBGR_MEAN = np.array([104.008, 116.669, 122.675])\n\n\nclass UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image):\n        image2 = torch.clone(image)\n        for t, m, s in zip(image2, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return image2", "\n\nunnorm = UnNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\ndef dense_crf(image_tensor: torch.FloatTensor, output_logits: torch.FloatTensor):\n    image = np.array(VF.to_pil_image(unnorm(image_tensor)))[:, :, ::-1]\n    H, W = image.shape[:2]\n    image = np.ascontiguousarray(image)\n\n    output_logits = F.interpolate(output_logits.unsqueeze(0), size=(H, W), mode=\"bilinear\",\n                                  align_corners=False).squeeze()\n    output_probs = F.softmax(output_logits, dim=0).cpu().numpy()\n\n    c = output_probs.shape[0]\n    h = output_probs.shape[1]\n    w = output_probs.shape[2]\n\n    U = utils.unary_from_softmax(output_probs)\n    U = np.ascontiguousarray(U)\n\n    d = dcrf.DenseCRF2D(w, h, c)\n    d.setUnaryEnergy(U)\n    d.addPairwiseGaussian(sxy=POS_XY_STD, compat=POS_W)\n    d.addPairwiseBilateral(sxy=Bi_XY_STD, srgb=Bi_RGB_STD, rgbim=image, compat=Bi_W)\n\n    Q = d.inference(MAX_ITER)\n    Q = np.array(Q).reshape((c, h, w))\n    return Q", "\n\ndef _apply_crf(tup):\n    return dense_crf(tup[0], tup[1])\n\n\ndef batched_crf(img_tensor, prob_tensor):\n    batch_size = list(img_tensor.size())[0]\n    img_tensor_cpu = img_tensor.detach().cpu()\n    prob_tensor_cpu = prob_tensor.detach().cpu()\n    out = []\n    for i in range(batch_size):\n        out_ = dense_crf(img_tensor_cpu[i], prob_tensor_cpu[i])\n        out.append(out_)\n\n    return torch.cat([torch.from_numpy(arr).unsqueeze(0) for arr in out], dim=0)", "\n\nclass UnsupervisedMetrics(Metric):\n    def __init__(self, prefix: str, n_classes: int, extra_clusters: int, compute_hungarian: bool,\n                 dist_sync_on_step=True):\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n\n        self.n_classes = n_classes\n        self.extra_clusters = extra_clusters\n        self.compute_hungarian = compute_hungarian\n        self.prefix = prefix\n        self.stats = torch.zeros(n_classes + self.extra_clusters, n_classes,\n                                           dtype=torch.int64, device=\"cuda\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        with torch.no_grad():\n            actual = target.reshape(-1) # 32*320*320\n            preds = preds.reshape(-1) # 32*320*320\n\n            mask = (actual >= 0) & (actual < self.n_classes) & (preds >= 0) & (preds < self.n_classes)\n            actual = actual[mask]\n            preds = preds[mask]\n            self.stats += torch.bincount(\n                (self.n_classes + self.extra_clusters) * actual + preds,\n                minlength=self.n_classes * (self.n_classes + self.extra_clusters)) \\\n                .reshape(self.n_classes, self.n_classes + self.extra_clusters).t().to(self.stats.device)\n\n    def map_clusters(self, clusters):\n        if self.extra_clusters == 0:\n            return torch.tensor(self.assignments[1])[clusters]\n        else:\n            missing = sorted(list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0])))\n            cluster_to_class = self.assignments[1]\n            for missing_entry in missing:\n                if missing_entry == cluster_to_class.shape[0]:\n                    cluster_to_class = np.append(cluster_to_class, -1)\n                else:\n                    cluster_to_class = np.insert(cluster_to_class, missing_entry + 1, -1)\n            cluster_to_class = torch.tensor(cluster_to_class)\n            return cluster_to_class[clusters]\n\n    def compute(self):\n        if self.compute_hungarian:  # cluster\n            self.assignments = linear_sum_assignment(self.stats.detach().cpu(), maximize=True)  # row, col\n            if self.extra_clusters == 0:\n                self.histogram = self.stats[np.argsort(self.assignments[1]), :]\n\n            if self.extra_clusters > 0:\n                self.assignments_t = linear_sum_assignment(self.stats.detach().cpu().t(), maximize=True)\n                histogram = self.stats[self.assignments_t[1], :]\n                missing = list(set(range(self.n_classes + self.extra_clusters)) - set(self.assignments[0]))\n                new_row = self.stats[missing, :].sum(0, keepdim=True)\n                histogram = torch.cat([histogram, new_row], axis=0)\n                new_col = torch.zeros(self.n_classes + 1, 1, device=histogram.device)\n                self.histogram = torch.cat([histogram, new_col], axis=1)\n        else:  # linear\n            self.assignments = (torch.arange(self.n_classes).unsqueeze(1),\n                                torch.arange(self.n_classes).unsqueeze(1))\n            self.histogram = self.stats\n\n        tp = torch.diag(self.histogram)\n        fp = torch.sum(self.histogram, dim=0) - tp\n        fn = torch.sum(self.histogram, dim=1) - tp\n\n        iou = tp / (tp + fp + fn)\n        prc = tp / (tp + fn)\n        opc = torch.sum(tp) / torch.sum(self.histogram)\n\n        metric_dict = {self.prefix + \"mIoU\": iou[~torch.isnan(iou)].mean().item(),\n                       self.prefix + \"Accuracy\": opc.item()}\n        return {k: 100 * v for k, v in metric_dict.items()}", "\n\ndef get_metrics(m1: UnsupervisedMetrics, m2: UnsupervisedMetrics) -> Dict[str, Any]:\n    metric_dict_1 = m1.compute()\n    metric_dict_2 = m2.compute()\n    metrics = all_reduce_dict(metric_dict_1, op=\"mean\")\n    tmp = all_reduce_dict(metric_dict_2, op=\"mean\")\n    metrics.update(tmp)\n\n    return metrics", ""]}
{"filename": "utils/wandb_utils.py", "chunked_list": ["from typing import Dict, Optional\nimport os\nimport wandb\n\n__all__ = [\"set_wandb\"]\n\n\ndef set_wandb(opt: Dict, local_rank: int = 0, force_mode: Optional[str] = None) -> str:\n    if local_rank != 0:\n        return \"\"\n\n    # opt = opt\n    save_dir = os.path.join(opt[\"output_dir\"], opt[\"wandb\"][\"name\"]) # root save dir\n\n    wandb_mode = opt[\"wandb\"][\"mode\"].lower()\n    if force_mode is not None:\n        wandb_mode = force_mode.lower()\n    if wandb_mode not in (\"online\", \"offline\", \"disabled\"):\n        raise ValueError(f\"WandB mode {wandb_mode} invalid.\")\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    wandb_project = opt[\"wandb\"][\"project\"]\n    wandb_entity = opt[\"wandb\"][\"entity\"]\n    wandb_name = opt[\"wandb\"][\"name\"]\n    wandb_id = opt[\"wandb\"].get(\"id\", None)\n    wandb_notes = opt[\"wandb\"].get(\"notes\", None)\n    wandb_tags = opt[\"wandb\"].get(\"tags\", None)\n    if wandb_tags is None:\n        wandb_tags = [opt[\"dataset\"][\"data_type\"], ]\n\n    wandb.init(\n        project=wandb_project,\n        entity=wandb_entity,\n        name=wandb_name,\n        dir=save_dir,\n        resume=\"allow\",\n        mode=wandb_mode,\n        id=wandb_id,\n        notes=wandb_notes,\n        tags=wandb_tags,\n        config=opt,\n    )\n    wandb_path = wandb.run.dir if (wandb_mode != \"disabled\") else save_dir\n    return wandb_path", ""]}
{"filename": "utils/common_utils.py", "chunked_list": ["from datetime import datetime\nfrom typing import Dict\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel.distributed import DistributedDataParallel\nimport json\nimport os\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\n\ndef save_checkpoint(prefix: str,\n                    net_model, net_optimizer,\n                    linear_model, linear_optimizer,\n                    cluster_model, cluster_optimizer,\n                    current_epoch, current_iter,\n                    best_value, save_dir: str,\n                    best_epoch=None, best_iter=None,\n                    *, model_only: bool = False) -> None:\n    model_name = f\"{save_dir}/{prefix}.pth\"\n\n    if isinstance(net_model, DistributedDataParallel):\n        net_model = net_model.module\n    if isinstance(linear_model, DistributedDataParallel):\n        linear_model = linear_model.module\n    if isinstance(cluster_model, DistributedDataParallel):\n        cluster_model = cluster_model.module\n\n    torch.save(\n        {\n            'epoch': current_epoch,\n            'iter': current_iter,\n            'best_epoch': best_epoch if (best_epoch is not None) else current_epoch,\n            'best_iter': best_iter if (best_iter is not None) else current_iter,\n            'net_model_state_dict': net_model.state_dict(),\n            'net_optimizer_state_dict': net_optimizer.state_dict() if (not model_only) else None,\n            'linear_model_state_dict': linear_model.state_dict(),\n            'linear_optimizer_state_dict': linear_optimizer.state_dict() if (not model_only) else None,\n            'cluster_model_state_dict': cluster_model.state_dict(),\n            'cluster_optimizer_state_dict': cluster_optimizer.state_dict() if (not model_only) else None,\n            'best': best_value,\n        }, model_name)", "\n\ndef parse(json_path: str) -> dict:\n    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n        opt = json.load(f, object_pairs_hook=OrderedDict)  # noqa\n\n    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n\n    opt['num_gpus'] = len(opt['gpu_ids'])\n\n    print('export CUDA_VISIBLE_DEVICES=' + gpu_list)\n    print('number of GPUs=' + str(opt['num_gpus']))\n\n    os.makedirs(opt[\"output_dir\"], exist_ok=True)\n    with open(opt['output_dir'] + '/option.json', 'w', encoding='utf-8') as f:\n        json.dump(opt, f, indent=\"\\t\")\n\n    return opt", "\n\ndef dprint(*args, local_rank: int = 0, **kwargs) -> None:\n    if local_rank == 0:\n        print(*args, **kwargs)\n\n\ndef time_log() -> str:\n    a = datetime.now()\n    return f\"*\" * 48 + f\"  {a.year:>4}/{a.month:>2}/{a.day:>2} | {a.hour:>2}:{a.minute:>2}:{a.second:>2}\\n\"", "\n\n@torch.no_grad()\ndef compute_param_norm(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.requires_grad]\n    if len(parameters) == 0:\n        return torch.as_tensor(0., dtype=torch.float32)\n\n    device = parameters[0].device\n    total_norm = torch.norm(torch.stack([torch.norm(p, norm_type).to(device) for p in parameters]), norm_type)\n    return total_norm", "\n\ndef freeze_bn(model: nn.Module) -> None:\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n            m.eval()\n\n\ndef zero_grad_bn(model: nn.Module) -> None:\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n            for p in m.parameters():\n                # p.grad.fill_(0.0)\n                p.grad = None", "def zero_grad_bn(model: nn.Module) -> None:\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.SyncBatchNorm)):\n            for p in m.parameters():\n                # p.grad.fill_(0.0)\n                p.grad = None\n\n\nclass RunningAverage:\n    def __init__(self):\n        self._avg = 0.0\n        self._count = 0\n\n    def append(self, value: float) -> None:\n        if isinstance(value, torch.Tensor):\n            value = value.item()\n        self._avg = (value + self._count * self._avg) / (self._count + 1)\n        self._count += 1\n\n    @property\n    def avg(self) -> float:\n        return self._avg\n\n    @property\n    def count(self) -> int:\n        return self._count\n\n    def reset(self) -> None:\n        self._avg = 0.0\n        self._count = 0", "class RunningAverage:\n    def __init__(self):\n        self._avg = 0.0\n        self._count = 0\n\n    def append(self, value: float) -> None:\n        if isinstance(value, torch.Tensor):\n            value = value.item()\n        self._avg = (value + self._count * self._avg) / (self._count + 1)\n        self._count += 1\n\n    @property\n    def avg(self) -> float:\n        return self._avg\n\n    @property\n    def count(self) -> int:\n        return self._count\n\n    def reset(self) -> None:\n        self._avg = 0.0\n        self._count = 0", "\n\nclass RunningAverageDict:\n    def __init__(self):\n        self._dict = None\n\n    def update(self, new_dict):\n        if self._dict is None:\n            self._dict = dict()\n            for key, value in new_dict.items():\n                self._dict[key] = RunningAverage()\n\n        for key, value in new_dict.items():\n            self._dict[key].append(value)\n\n    def get_value(self) -> Dict[str, float]:\n        return {key: value.avg for key, value in self._dict.items()}\n\n    def reset(self) -> None:\n        if self._dict is None:\n            return\n        for k in self._dict.keys():\n            self._dict[k].reset()", "\n\nclass Timer:\n    def __init__(self):\n        self._now = time.process_time()\n        # self._now = time.process_time_ns()\n\n    def update(self) -> float:\n        current = time.process_time()\n        # current = time.process_time_ns()\n        duration = current - self._now\n        self._now = current\n        return duration / 1e6  # ms", ""]}
{"filename": "model/pretrained_download.py", "chunked_list": ["'''\n@article{hamilton2022unsupervised,\n  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},\n  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},\n  journal={arXiv preprint arXiv:2203.08414},\n  year={2022}\n}\n'''\n\nfrom os.path import join, exists", "\nfrom os.path import join, exists\nimport wget\nimport os\n\nmodels_dir = join(\"./\", \"pretrained_models\")\nos.makedirs(models_dir, exist_ok=True)\nmodel_url_root = \"https://marhamilresearch4.blob.core.windows.net/stego-public/models/models/\"\nmodel_names = [\"moco_v2_800ep_pretrain.pth.tar\",\n               \"model_epoch_0720_iter_085000.pth\",", "model_names = [\"moco_v2_800ep_pretrain.pth.tar\",\n               \"model_epoch_0720_iter_085000.pth\",\n               \"picie.pkl\"]\n\nsaved_models_dir = join(\"./\", \"saved_models\")\nos.makedirs(saved_models_dir, exist_ok=True)\nsaved_model_url_root = \"https://marhamilresearch4.blob.core.windows.net/stego-public/saved_models/\"\nsaved_model_names = [\"cityscapes_vit_base_1.ckpt\",\n                     \"cocostuff27_vit_base_5.ckpt\",\n                     \"picie_and_probes.pth\",", "                     \"cocostuff27_vit_base_5.ckpt\",\n                     \"picie_and_probes.pth\",\n                     \"potsdam_test.ckpt\"]\n\ntarget_files = [join(models_dir, mn) for mn in model_names] + \\\n               [join(saved_models_dir, mn) for mn in saved_model_names]\n\ntarget_urls = [model_url_root + mn for mn in model_names] + \\\n              [saved_model_url_root + mn for mn in saved_model_names]\n\nfor target_file, target_url in zip(target_files, target_urls):\n    if not exists(target_file):\n        print(\"\\nDownloading file from {}\".format(target_url))\n        wget.download(target_url, target_file)\n    else:\n        print(\"\\nFound {}, skipping download\".format(target_file))", "              [saved_model_url_root + mn for mn in saved_model_names]\n\nfor target_file, target_url in zip(target_files, target_urls):\n    if not exists(target_file):\n        print(\"\\nDownloading file from {}\".format(target_url))\n        wget.download(target_url, target_file)\n    else:\n        print(\"\\nFound {}, skipping download\".format(target_file))\n\n", "\n"]}
{"filename": "model/STEGO.py", "chunked_list": ["from typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # noqa\nfrom model.dino.DinoFeaturizer import DinoFeaturizer\nfrom utils.layer_utils import ClusterLookup\nimport numpy as np\nimport torch.distributed as dist\nfrom utils.dist_utils import all_reduce_tensor, all_gather_tensor\n", "from utils.dist_utils import all_reduce_tensor, all_gather_tensor\n\n\nclass STEGOmodel(nn.Module):\n    # opt[\"model\"]\n    def __init__(self,\n                 opt: dict,\n                 n_classes:int\n                 ):\n        super().__init__()\n        self.opt = opt\n        self.n_classes= n_classes\n\n\n        if not opt[\"continuous\"]:\n            dim = n_classes\n        else:\n            dim = opt[\"dim\"]\n\n        if opt[\"arch\"] == \"dino\":\n            self.net = DinoFeaturizer(dim, opt)\n        else:\n            raise ValueError(\"Unknown arch {}\".format(opt[\"arch\"]))\n\n        self.cluster_probe = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n        self.linear_probe = nn.Conv2d(dim, n_classes, (1, 1))\n\n        self.cluster_probe2 = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n        self.linear_probe2 = nn.Conv2d(dim, n_classes, (1, 1))\n\n        self.cluster_probe3 = ClusterLookup(dim, n_classes + opt[\"extra_clusters\"])\n        self.linear_probe3 = nn.Conv2d(dim, n_classes, (1, 1))\n\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)[1]\n\n    @classmethod\n    def build(cls, opt, n_classes):\n        # opt = opt[\"model\"]\n        m = cls(\n            opt = opt,\n            n_classes= n_classes\n        )\n        print(f\"Model built! #params {m.count_params()}\")\n        return m\n\n    def count_params(self) -> int:\n        count = 0\n        for p in self.parameters():\n            count += p.numel()\n        return count", "\n\nif __name__ == '__main__':\n    net = STEGOmodel()\n    dummy_input = torch.empty(2, 3, 352, 1216)\n    dummy_output = net(dummy_input)[0]\n    print(dummy_output.shape)  # (2, 1, 88, 304)\n\n", ""]}
{"filename": "model/__init__.py", "chunked_list": [""]}
{"filename": "model/LambdaLayer.py", "chunked_list": ["import torch\n\nimport torch.nn as nn\n\nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)", "\n"]}
{"filename": "model/dino/utils.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMisc functions.\n\nMostly copy-paste from torchvision references or other public repos like DETR:\nhttps://github.com/facebookresearch/detr/blob/master/util/misc.py", "Mostly copy-paste from torchvision references or other public repos like DETR:\nhttps://github.com/facebookresearch/detr/blob/master/util/misc.py\n\"\"\"\nimport os\nimport sys\nimport time\nimport math\nimport random\nimport datetime\nimport subprocess", "import datetime\nimport subprocess\nfrom collections import defaultdict, deque\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\nfrom PIL import ImageFilter, ImageOps\n", "from PIL import ImageFilter, ImageOps\n\n\nclass GaussianBlur(object):\n    \"\"\"\n    Apply Gaussian Blur to the PIL image.\n    \"\"\"\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img):\n        do_it = random.random() <= self.prob\n        if not do_it:\n            return img\n\n        return img.filter(\n            ImageFilter.GaussianBlur(\n                radius=random.uniform(self.radius_min, self.radius_max)\n            )\n        )", "\n\nclass Solarization(object):\n    \"\"\"\n    Apply Solarization to the PIL image.\n    \"\"\"\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return ImageOps.solarize(img)\n        else:\n            return img", "\n\ndef load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size):\n    if os.path.isfile(pretrained_weights):\n        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n        if checkpoint_key is not None and checkpoint_key in state_dict:\n            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n            state_dict = state_dict[checkpoint_key]\n        # remove `module.` prefix\n        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n        # remove `backbone.` prefix induced by multicrop wrapper\n        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n        msg = model.load_state_dict(state_dict, strict=False)\n        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n    else:\n        print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n        url = None\n        if model_name == \"vit_small\" and patch_size == 16:\n            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n        elif model_name == \"vit_small\" and patch_size == 8:\n            url = \"dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\"\n        elif model_name == \"vit_base\" and patch_size == 16:\n            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n        elif model_name == \"vit_base\" and patch_size == 8:\n            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n        if url is not None:\n            print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n            state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n            model.load_state_dict(state_dict, strict=True)\n        else:\n            print(\"There is no reference weights available for this model => We use random weights.\")", "\n\ndef clip_gradients(model, clip):\n    norms = []\n    for name, p in model.named_parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            norms.append(param_norm.item())\n            clip_coef = clip / (param_norm + 1e-6)\n            if clip_coef < 1:\n                p.grad.data.mul_(clip_coef)\n    return norms", "\n\ndef cancel_gradients_last_layer(epoch, model, freeze_last_layer):\n    if epoch >= freeze_last_layer:\n        return\n    for n, p in model.named_parameters():\n        if \"last_layer\" in n:\n            p.grad = None\n\n\ndef restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n    \"\"\"\n    Re-start from checkpoint\n    \"\"\"\n    if not os.path.isfile(ckp_path):\n        return\n    print(\"Found checkpoint at {}\".format(ckp_path))\n\n    # open checkpoint file\n    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n\n    # key is what to look for in the checkpoint file\n    # value is the object to load\n    # example: {'state_dict': model}\n    for key, value in kwargs.items():\n        if key in checkpoint and value is not None:\n            try:\n                msg = value.load_state_dict(checkpoint[key], strict=False)\n                print(\"=> loaded {} from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n            except TypeError:\n                try:\n                    msg = value.load_state_dict(checkpoint[key])\n                    print(\"=> loaded {} from checkpoint '{}'\".format(key, ckp_path))\n                except ValueError:\n                    print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n        else:\n            print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n\n    # re load variable important for the run\n    if run_variables is not None:\n        for var_name in run_variables:\n            if var_name in checkpoint:\n                run_variables[var_name] = checkpoint[var_name]", "\n\ndef restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n    \"\"\"\n    Re-start from checkpoint\n    \"\"\"\n    if not os.path.isfile(ckp_path):\n        return\n    print(\"Found checkpoint at {}\".format(ckp_path))\n\n    # open checkpoint file\n    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n\n    # key is what to look for in the checkpoint file\n    # value is the object to load\n    # example: {'state_dict': model}\n    for key, value in kwargs.items():\n        if key in checkpoint and value is not None:\n            try:\n                msg = value.load_state_dict(checkpoint[key], strict=False)\n                print(\"=> loaded {} from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n            except TypeError:\n                try:\n                    msg = value.load_state_dict(checkpoint[key])\n                    print(\"=> loaded {} from checkpoint '{}'\".format(key, ckp_path))\n                except ValueError:\n                    print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n        else:\n            print(\"=> failed to load {} from checkpoint '{}'\".format(key, ckp_path))\n\n    # re load variable important for the run\n    if run_variables is not None:\n        for var_name in run_variables:\n            if var_name in checkpoint:\n                run_variables[var_name] = checkpoint[var_name]", "\n\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    if warmup_epochs > 0:\n        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n\n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n\n    schedule = np.concatenate((warmup_schedule, schedule))\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule", "\n\ndef bool_flag(s):\n    \"\"\"\n    Parse boolean arguments from the command line.\n    \"\"\"\n    FALSY_STRINGS = {\"off\", \"false\", \"0\"}\n    TRUTHY_STRINGS = {\"on\", \"true\", \"1\"}\n    if s.lower() in FALSY_STRINGS:\n        return False\n    elif s.lower() in TRUTHY_STRINGS:\n        return True\n    else:\n        raise argparse.ArgumentTypeError(\"invalid value for a boolean flag\")", "\n\ndef fix_random_seeds(seed=31):\n    \"\"\"\n    Fix random seeds.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n", "\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.6f} ({global_avg:.6f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)", "\n\ndef reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that all processes\n    have the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.all_reduce(values)\n        if average:\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict", "\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.6f}')\n        data_time = SmoothedValue(fmt='{avg:.6f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        if torch.cuda.is_available():\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}',\n                'max mem: {memory:.0f}'\n            ])\n        else:\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}'\n            ])\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.6f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))", "\n\ndef get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = \"clean\"\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)\n        diff = _run(['git', 'diff-index', 'HEAD'])\n        diff = \"has uncommited changes\" if diff else \"clean\"\n        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n    except Exception:\n        pass\n    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n    return message", "\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()", "\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()", "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)", "\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print", "\n\ndef init_distributed_mode(args):\n    # launched with torch.distributed.launch\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    # launched with submitit on a slurm cluster\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    # launched naively with `python main_dino.py`\n    # we manually add MASTER_ADDR and MASTER_PORT to env variables\n    elif torch.cuda.is_available():\n        print('Will run the code on one GPU.')\n        args.rank, args.gpu, args.world_size = 0, 0, 1\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = '29500'\n    else:\n        print('Does not support training without GPU.')\n        sys.exit(1)\n\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=args.dist_url,\n        world_size=args.world_size,\n        rank=args.rank,\n    )\n\n    torch.cuda.set_device(args.gpu)\n    print('| distributed init (rank {}): {}'.format(\n        args.rank, args.dist_url), flush=True)\n    dist.barrier()\n    setup_for_distributed(args.rank == 0)", "\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n    return [correct[:k].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]", "\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor", "\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n\nclass LARS(torch.optim.Optimizer):\n    \"\"\"\n    Almost copy-paste from https://github.com/facebookresearch/barlowtwins/blob/main/main.py\n    \"\"\"\n    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, eta=0.001,\n                 weight_decay_filter=None, lars_adaptation_filter=None):\n        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n                        eta=eta, weight_decay_filter=weight_decay_filter,\n                        lars_adaptation_filter=lars_adaptation_filter)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self):\n        for g in self.param_groups:\n            for p in g['params']:\n                dp = p.grad\n\n                if dp is None:\n                    continue\n\n                if p.ndim != 1:\n                    dp = dp.add(p, alpha=g['weight_decay'])\n\n                if p.ndim != 1:\n                    param_norm = torch.norm(p)\n                    update_norm = torch.norm(dp)\n                    one = torch.ones_like(param_norm)\n                    q = torch.where(param_norm > 0.,\n                                    torch.where(update_norm > 0,\n                                                (g['eta'] * param_norm / update_norm), one), one)\n                    dp = dp.mul(q)\n\n                param_state = self.state[p]\n                if 'mu' not in param_state:\n                    param_state['mu'] = torch.zeros_like(p)\n                mu = param_state['mu']\n                mu.mul_(g['momentum']).add_(dp)\n\n                p.add_(mu, alpha=-g['lr'])", "\n\nclass MultiCropWrapper(nn.Module):\n    \"\"\"\n    Perform forward pass separately on each resolution input.\n    The inputs corresponding to a single resolution are clubbed and single\n    forward is run on the same resolution inputs. Hence we do several\n    forward passes = number of different resolutions used. We then\n    concatenate all the output features and run the head forward on these\n    concatenated features.\n    \"\"\"\n    def __init__(self, backbone, head):\n        super(MultiCropWrapper, self).__init__()\n        # disable layers dedicated to ImageNet labels classification\n        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n        self.backbone = backbone\n        self.head = head\n\n    def forward(self, x):\n        # convert to list\n        if not isinstance(x, list):\n            x = [x]\n        idx_crops = torch.cumsum(torch.unique_consecutive(\n            torch.tensor([inp.shape[-1] for inp in x]),\n            return_counts=True,\n        )[1], 0)\n        start_idx = 0\n        for end_idx in idx_crops:\n            _out = self.backbone(torch.cat(x[start_idx: end_idx]))\n            if start_idx == 0:\n                output = _out\n            else:\n                output = torch.cat((output, _out))\n            start_idx = end_idx\n        # Run the head forward on the concatenated features.\n        return self.head(output)", "\n\ndef get_params_groups(model):\n    regularized = []\n    not_regularized = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        # we do not regularize biases nor Norm parameters\n        if name.endswith(\".bias\") or len(param.shape) == 1:\n            not_regularized.append(param)\n        else:\n            regularized.append(param)\n    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]", "\n\ndef has_batchnorms(model):\n    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n    for name, module in model.named_modules():\n        if isinstance(module, bn_types):\n            return True\n    return False\n", ""]}
{"filename": "model/dino/DinoFeaturizer.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport model.dino.vision_transformer as vits\n\nclass DinoFeaturizer(nn.Module):\n\n    def __init__(self, dim, cfg):  # cfg[\"pretrained\"]\n        super().__init__()\n        self.cfg = cfg\n        self.dim = dim\n        patch_size = self.cfg[\"pretrained\"][\"dino_patch_size\"]\n        self.patch_size = patch_size\n        self.feat_type = self.cfg[\"pretrained\"][\"dino_feat_type\"]\n        arch = self.cfg[\"pretrained\"][\"model_type\"]\n        self.model = vits.__dict__[arch](\n            patch_size=patch_size,\n            num_classes=0)\n        self.n_feats = 384\n        self.const = 28\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n        self.model.eval().cuda()\n        self.dropout = torch.nn.Dropout2d(p=.1)\n\n        if arch == \"vit_small\" and patch_size == 16:\n            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n        elif arch == \"vit_small\" and patch_size == 8:\n            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n        elif arch == \"vit_base\" and patch_size == 16:\n            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n        elif arch == \"vit_base\" and patch_size == 8:\n            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n        else:\n            raise ValueError(\"Unknown arch and patch size\")\n\n        if arch == \"vit_small\" and patch_size == 16:\n            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n        elif arch == \"vit_small\" and patch_size == 8:\n            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n        elif arch == \"vit_base\" and patch_size == 16:\n            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n        elif arch == \"vit_base\" and patch_size == 8:\n            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n        else:\n            raise ValueError(\"Unknown arch and patch size\")\n\n        if cfg[\"pretrained\"][\"pretrained_weights\"] is not None:\n            state_dict = torch.load(cfg[\"pretrained\"][\"pretrained_weights\"], map_location=\"cpu\")\n            state_dict = state_dict[\"teacher\"]\n            # remove `module.` prefix\n            state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n            # remove `backbone.` prefix induced by multicrop wrapper\n            state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n\n            msg = self.model.load_state_dict(state_dict, strict=False)\n            print('Pretrained weights found at {} and loaded with msg: {}'.format(\n                cfg[\"pretrained\"][\"pretrained_weights\"], msg))\n        else:\n            print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n            state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n            self.model.load_state_dict(state_dict, strict=True)\n\n        if arch == \"vit_small\":\n            self.n_feats = 384\n        else:\n            self.n_feats = 768\n        self.cluster1 = self.make_clusterer(self.n_feats)\n        self.proj_type = cfg[\"pretrained\"][\"projection_type\"]\n        if self.proj_type == \"nonlinear\":\n            self.cluster2 = self.make_nonlinear_clusterer(self.n_feats)\n\n        self.ema_model1 = self.make_clusterer(self.n_feats)\n        self.ema_model2 = self.make_nonlinear_clusterer(self.n_feats)\n\n        for param_q, param_k in zip(self.cluster1.parameters(), self.ema_model1.parameters()):\n            param_k.data.copy_(param_q.detach().data)  # initialize\n            param_k.requires_grad = False  # not update by gradient for eval_net\n        self.ema_model1.cuda()\n        self.ema_model1.eval()\n\n        for param_q, param_k in zip(self.cluster2.parameters(), self.ema_model2.parameters()):\n            param_k.data.copy_(param_q.detach().data)  # initialize\n            param_k.requires_grad = False  # not update by gradient for eval_net\n        self.ema_model2.cuda()\n        self.ema_model2.eval()\n\n        sz = cfg[\"spatial_size\"]\n\n        self.index_mask = torch.zeros((sz*sz, sz*sz), dtype=torch.float16)\n        self.divide_num = torch.zeros((sz*sz), dtype=torch.long)\n        for _im in range(sz*sz):\n            if _im == 0:\n                index_set = torch.tensor([_im, _im+1, _im+sz, _im+(sz+1)])\n            elif _im==(sz-1):\n                index_set = torch.tensor([_im-1, _im, _im+(sz-1), _im+sz])\n            elif _im==(sz*sz-sz):\n                index_set = torch.tensor([_im-sz, _im-(sz-1), _im, _im+1])\n            elif _im==(sz*sz-1):\n                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-1, _im])\n\n            elif ((1 <= _im) and (_im <= (sz-2))):\n                index_set = torch.tensor([_im-1, _im, _im+1, _im+(sz-1), _im+sz, _im+(sz+1)])\n            elif (((sz*sz-sz+1) <= _im) and (_im <= (sz*sz-2))):\n                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-(sz-1), _im-1, _im, _im+1])\n            elif (_im % sz == 0):\n                index_set = torch.tensor([_im-sz, _im-(sz-1), _im, _im+1, _im+sz, _im+(sz+1)])\n            elif ((_im+1) % sz == 0):\n                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-1, _im, _im+(sz-1), _im+sz])\n            else:\n                index_set = torch.tensor([_im-(sz+1), _im-sz, _im-(sz-1), _im-1, _im, _im+1, _im+(sz-1), _im+sz, _im+(sz+1)])\n            self.index_mask[_im][index_set] = 1.\n            self.divide_num[_im] = index_set.size(0)\n\n        self.index_mask = self.index_mask.cuda()\n        self.divide_num = self.divide_num.unsqueeze(1)\n        self.divide_num = self.divide_num.cuda()\n\n\n    def make_clusterer(self, in_channels):\n        return torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n\n    def make_nonlinear_clusterer(self, in_channels):\n        return torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n\n    def make_nonlinear_clusterer_layer3(self, in_channels):\n        return torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels, in_channels, (1, 1)),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels, self.dim, (1, 1)))\n\n    @torch.no_grad()\n    def ema_model_update(self, model, ema_model, ema_m):\n        \"\"\"\n        Momentum update of evaluation model (exponential moving average)\n        \"\"\"\n        for param_train, param_eval in zip(model.parameters(), ema_model.parameters()):\n            param_eval.copy_(param_eval * ema_m + param_train.detach() * (1 - ema_m))\n\n        for buffer_train, buffer_eval in zip(model.buffers(), ema_model.buffers()):\n            buffer_eval.copy_(buffer_train)\n\n    def forward(self, img, n=1, return_class_feat=False, train=False):\n        self.model.eval()\n        batch_size = img.shape[0]\n\n        with torch.no_grad():\n            assert (img.shape[2] % self.patch_size == 0)\n            assert (img.shape[3] % self.patch_size == 0)\n\n            # get selected layer activations\n            feat, attn, qkv = self.model.get_intermediate_feat(img, n=n)\n            feat, attn, qkv = feat[0], attn[0], qkv[0]\n\n            if train==True:\n                attn = attn[:, :, 1:, 1:]\n                attn = torch.mean(attn, dim=1)\n                attn = attn.type(torch.float32)\n                attn_max = torch.quantile(attn, 0.9, dim=2, keepdim=True)\n                attn_min = torch.quantile(attn, 0.1, dim=2, keepdim=True)\n                attn = torch.max(torch.min(attn, attn_max), attn_min)\n\n                attn = attn.softmax(dim=-1)\n                attn = attn*self.const\n                attn[attn < torch.mean(attn, dim=2, keepdim=True)] = 0.\n\n            feat_h = img.shape[2] // self.patch_size\n            feat_w = img.shape[3] // self.patch_size\n\n            if self.feat_type == \"feat\":\n                image_feat = feat[:, 1:, :].reshape(feat.shape[0], feat_h, feat_w, -1).permute(0, 3, 1, 2)\n            elif self.feat_type == \"KK\":\n                image_k = qkv[1, :, :, 1:, :].reshape(feat.shape[0], 6, feat_h, feat_w, -1)\n                B, H, I, J, D = image_k.shape\n                image_feat = image_k.permute(0, 1, 4, 2, 3).reshape(B, H * D, I, J)\n            else:\n                raise ValueError(\"Unknown feat type:{}\".format(self.feat_type))\n\n            if return_class_feat:\n                return feat[:, :1, :].reshape(feat.shape[0], 1, 1, -1).permute(0, 3, 1, 2)\n\n        if self.proj_type is not None:\n            code = self.cluster1(self.dropout(image_feat))\n            code_ema = self.ema_model1(self.dropout(image_feat))\n            if self.proj_type == \"nonlinear\":\n                code += self.cluster2(self.dropout(image_feat))\n                code_ema += self.ema_model2(self.dropout(image_feat))\n        else:\n            code = image_feat\n\n        if train==True:\n            attn = attn * self.index_mask.unsqueeze(0).repeat(batch_size, 1, 1)\n            code_clone = code.clone()\n            code_clone = code_clone.view(code_clone.size(0), code_clone.size(1), -1)\n            code_clone = code_clone.permute(0,2,1)\n\n            code_3x3_all = []\n            for bs in range(batch_size):\n                code_3x3 = attn[bs].unsqueeze(-1) * code_clone[bs].unsqueeze(0)\n                code_3x3 = torch.sum(code_3x3, dim=1)\n                code_3x3 = code_3x3 / self.divide_num\n                code_3x3_all.append(code_3x3)\n            code_3x3_all = torch.stack(code_3x3_all)\n            code_3x3_all = code_3x3_all.permute(0,2,1).view(code.size(0), code.size(1), code.size(2), code.size(3))\n\n        if train==True:\n            with torch.no_grad():\n                self.ema_model_update(self.cluster1, self.ema_model1, self.cfg[\"ema_m\"])\n                self.ema_model_update(self.cluster2, self.ema_model2, self.cfg[\"ema_m\"])\n\n        if train==True:\n            if self.cfg[\"pretrained\"][\"dropout\"]:\n                return self.dropout(image_feat), code, self.dropout(code_ema), self.dropout(code_3x3_all)\n            else:\n                return image_feat, code, code_ema, code_3x3_all\n        else:\n            if self.cfg[\"pretrained\"][\"dropout\"]:\n                return self.dropout(image_feat), code, self.dropout(code_ema)\n            else:\n                return image_feat, code, code_ema", ""]}
{"filename": "model/dino/vision_transformer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMostly copy-paste from timm library.\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\"\"\"\nimport math", "\"\"\"\nimport math\nimport warnings\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom model.dino.utils import trunc_normal_\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output", "\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output", "\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)", "\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x", "\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, return_qkv=False):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn_hook = attn.clone()\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn_hook, qkv", "\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, return_attention=False, return_qkv=False):\n        y, attn, qkv = self.attn(self.norm1(x))\n        if return_attention:\n            return attn\n        x = x + self.drop_path(y)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        if return_qkv:\n            return x, attn, qkv\n        return x", "\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x", "\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer \"\"\"\n\n    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n        super().__init__()\n\n        self.num_features = self.embed_dim = embed_dim\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        # Classifier head\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def interpolate_pos_encoding(self, x, w, h):\n        npatch = x.shape[1] - 1\n        N = self.pos_embed.shape[1] - 1\n        if npatch == N and w == h:\n            return self.pos_embed\n        class_pos_embed = self.pos_embed[:, 0]\n        patch_pos_embed = self.pos_embed[:, 1:]\n        dim = x.shape[-1]\n        w0 = w // self.patch_embed.patch_size\n        h0 = h // self.patch_embed.patch_size\n        # we add a small number to avoid floating point error in the interpolation\n        # see discussion at https://github.com/facebookresearch/dino/issues/8\n        w0, h0 = w0 + 0.1, h0 + 0.1\n        patch_pos_embed = nn.functional.interpolate(\n            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n            mode='bicubic',\n        )\n        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\n    def prepare_tokens(self, x):\n        B, nc, w, h = x.shape\n        x = self.patch_embed(x)  # patch linear embedding\n\n        # add the [CLS] token to the embed patch tokens\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # add positional encoding to each token\n        x = x + self.interpolate_pos_encoding(x, w, h)\n\n        return self.pos_drop(x)\n\n    def forward(self, x):\n        x = self.prepare_tokens(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward_feats(self, x):\n        x = self.prepare_tokens(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n    def get_intermediate_feat(self, x, n=1):\n        x = self.prepare_tokens(x)\n        # we return the output tokens from the `n` last blocks\n        feat = []\n        attns = []\n        qkvs = []\n        for i, blk in enumerate(self.blocks):\n            x, attn, qkv = blk(x, return_qkv=True)\n            if len(self.blocks) - i <= n:\n                feat.append(self.norm(x))\n                qkvs.append(qkv)\n                attns.append(attn)\n        return feat, attns, qkvs\n\n    def get_last_selfattention(self, x):\n        x = self.prepare_tokens(x)\n        for i, blk in enumerate(self.blocks):\n            if i < len(self.blocks) - 1:\n                x = blk(x)\n            else:\n                # return attention of the last block\n                return blk(x, return_attention=True)\n\n    def get_intermediate_layers(self, x, n=1):\n        x = self.prepare_tokens(x)\n        # we return the output tokens from the `n` last blocks\n        output = []\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if len(self.blocks) - i <= n:\n                output.append(self.norm(x))\n        return output", "\n\ndef vit_tiny(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_small(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model", "\ndef vit_small(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\ndef vit_base(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model", "def vit_base(patch_size=16, **kwargs):\n    model = VisionTransformer(\n        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n\n\nclass DINOHead(nn.Module):\n    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048,\n                 bottleneck_dim=256):\n        super().__init__()\n        nlayers = max(nlayers, 1)\n        if nlayers == 1:\n            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n        else:\n            layers = [nn.Linear(in_dim, hidden_dim)]\n            if use_bn:\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.GELU())\n            for _ in range(nlayers - 2):\n                layers.append(nn.Linear(hidden_dim, hidden_dim))\n                if use_bn:\n                    layers.append(nn.BatchNorm1d(hidden_dim))\n                layers.append(nn.GELU())\n            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n            self.mlp = nn.Sequential(*layers)\n        self.apply(self._init_weights)\n        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n        self.last_layer.weight_g.data.fill_(1)\n        if norm_last_layer:\n            self.last_layer.weight_g.requires_grad = False\n\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.mlp(x)\n        x = nn.functional.normalize(x, dim=-1, p=2)\n        x = self.last_layer(x)\n        return x", ""]}
{"filename": "dataset/__init__.py", "chunked_list": [""]}
{"filename": "dataset/data.py", "chunked_list": ["import os\nimport random\nfrom os.path import join\n\nimport numpy as np\nimport torch.multiprocessing\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom scipy.io import loadmat\nfrom torch.utils.data import DataLoader", "from scipy.io import loadmat\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets.cityscapes import Cityscapes\nfrom torchvision.transforms.functional import to_pil_image\nfrom tqdm import tqdm\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n", "from torchvision.utils import save_image\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef get_transform(res, is_label, crop_type):\n    if crop_type == \"center\":\n        cropper = T.CenterCrop(res)\n    elif crop_type == \"random\":\n        cropper = T.RandomCrop(res)\n    elif crop_type is None:\n        cropper = T.Lambda(lambda x: x)\n        res = (res, res)\n    else:\n        raise ValueError(\"Unknown Cropper {}\".format(crop_type))\n\n    if is_label:\n        return T.Compose([T.Resize(res, Image.NEAREST),\n                          cropper,\n                          ToTargetTensor()])\n    else:\n        return T.Compose([T.Resize(res, Image.NEAREST),\n                          cropper,\n                          T.ToTensor(),\n                          normalize])", "\n\nnormalize = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\nclass ToTargetTensor(object):\n    def __call__(self, target):\n        return torch.as_tensor(np.array(target), dtype=torch.int64).unsqueeze(0)\n\n\ndef bit_get(val, idx):\n    \"\"\"Gets the bit value.\n    Args:\n      val: Input value, int or numpy int array.\n      idx: Which bit of the input val.\n    Returns:\n      The \"idx\"-th bit of input val.\n    \"\"\"\n    return (val >> idx) & 1", "\n\ndef bit_get(val, idx):\n    \"\"\"Gets the bit value.\n    Args:\n      val: Input value, int or numpy int array.\n      idx: Which bit of the input val.\n    Returns:\n      The \"idx\"-th bit of input val.\n    \"\"\"\n    return (val >> idx) & 1", "\n\ndef create_pascal_label_colormap():\n    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n    Returns:\n      A colormap for visualizing segmentation results.\n    \"\"\"\n    colormap = np.zeros((512, 3), dtype=int)\n    ind = np.arange(512, dtype=int)\n\n    for shift in reversed(list(range(8))):\n        for channel in range(3):\n            colormap[:, channel] |= bit_get(ind, channel) << shift\n        ind >>= 3\n\n    return colormap", "\n\ndef create_cityscapes_colormap():\n    colors = [(128, 64, 128),\n              (244, 35, 232),\n              (250, 170, 160),\n              (230, 150, 140),\n              (70, 70, 70),\n              (102, 102, 156),\n              (190, 153, 153),\n              (180, 165, 180),\n              (150, 100, 100),\n              (150, 120, 90),\n              (153, 153, 153),\n              (153, 153, 153),\n              (250, 170, 30),\n              (220, 220, 0),\n              (107, 142, 35),\n              (152, 251, 152),\n              (70, 130, 180),\n              (220, 20, 60),\n              (255, 0, 0),\n              (0, 0, 142),\n              (0, 0, 70),\n              (0, 60, 100),\n              (0, 0, 90),\n              (0, 0, 110),\n              (0, 80, 100),\n              (0, 0, 230),\n              (119, 11, 32),\n              (0, 0, 0)]\n    return np.array(colors)", "\n\nclass DirectoryDataset(Dataset):\n    def __init__(self, root, path, image_set, transform, target_transform):\n        super(DirectoryDataset, self).__init__()\n        self.split = image_set\n        self.dir = join(root, path)\n        self.img_dir = join(self.dir, \"imgs\", self.split)\n        self.label_dir = join(self.dir, \"labels\", self.split)\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n        self.img_files = np.array(sorted(os.listdir(self.img_dir)))\n        assert len(self.img_files) > 0\n        if os.path.exists(join(self.dir, \"labels\")):\n            self.label_files = np.array(sorted(os.listdir(self.label_dir)))\n            assert len(self.img_files) == len(self.label_files)\n        else:\n            self.label_files = None\n\n    def __getitem__(self, index):\n        image_fn = self.img_files[index]\n        img = Image.open(join(self.img_dir, image_fn))\n\n        if self.label_files is not None:\n            label_fn = self.label_files[index]\n            label = Image.open(join(self.label_dir, label_fn))\n\n        seed = random.randint(0, 2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        img = self.transform(img)\n\n        if self.label_files is not None:\n            random.seed(seed)\n            torch.manual_seed(seed)\n            label = self.target_transform(label)\n        else:\n            label = torch.zeros(img.shape[1], img.shape[2], dtype=torch.int64) - 1\n\n        mask = (label > 0).to(torch.float32)\n        return img, label, mask\n\n    def __len__(self):\n        return len(self.img_files)", "\n\nclass Potsdam(Dataset):\n    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n        super(Potsdam, self).__init__()\n        self.split = image_set\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n        split_files = {\n            \"train\": [\"labelled_train.txt\"],\n            \"unlabelled_train\": [\"unlabelled_train.txt\"],\n            # \"train\": [\"unlabelled_train.txt\"],\n            \"val\": [\"labelled_test.txt\"],\n            \"train+val\": [\"labelled_train.txt\", \"labelled_test.txt\"],\n            \"all\": [\"all.txt\"]\n        }\n        assert self.split in split_files.keys()\n\n        self.files = []\n        for split_file in split_files[self.split]:\n            with open(join(self.root, split_file), \"r\") as f:\n                self.files.extend(fn.rstrip() for fn in f.readlines())\n\n        self.coarse_labels = coarse_labels\n        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n                               1: 1, 5: 1,  # buildings and clutter\n                               2: 2, 3: 2,  # vegetation and trees\n                               255: -1\n                               }\n\n    def __getitem__(self, index):\n        image_id = self.files[index]\n        img = loadmat(join(self.root, \"imgs\", image_id + \".mat\"))[\"img\"]\n        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])\n        try:\n            label = loadmat(join(self.root, \"gt\", image_id + \".mat\"))[\"gt\"]\n            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n        except FileNotFoundError:\n            label = to_pil_image(torch.ones(1, img.height, img.width))\n\n        seed = random.randint(0, 2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        img = self.transform(img)\n\n        random.seed(seed)\n        torch.manual_seed(seed)\n        label = self.target_transform(label).squeeze(0)\n        if self.coarse_labels:\n            new_label_map = torch.zeros_like(label)\n            for fine, coarse in self.fine_to_coarse.items():\n                new_label_map[label == fine] = coarse\n            label = new_label_map\n\n        mask = (label > 0).to(torch.float32)\n        return img, label, mask\n\n    def __len__(self):\n        return len(self.files)", "\n\nclass PotsdamRaw(Dataset):\n    def __init__(self, root, image_set, transform, target_transform, coarse_labels):\n        super(PotsdamRaw, self).__init__()\n        self.split = image_set\n        self.root = os.path.join(root, \"processed\")\n        self.transform = transform\n        self.target_transform = target_transform\n        self.files = []\n        for im_num in range(38):\n            for i_h in range(15):\n                for i_w in range(15):\n                    self.files.append(\"{}_{}_{}.mat\".format(im_num, i_h, i_w))\n\n        self.coarse_labels = coarse_labels\n        self.fine_to_coarse = {0: 0, 4: 0,  # roads and cars\n                               1: 1, 5: 1,  # buildings and clutter\n                               2: 2, 3: 2,  # vegetation and trees\n                               255: -1\n                               }\n\n    def __getitem__(self, index):\n        image_id = self.files[index]\n        img = loadmat(join(self.root, \"imgs\", image_id))[\"img\"]\n        img = to_pil_image(torch.from_numpy(img).permute(2, 0, 1)[:3])\n        try:\n            label = loadmat(join(self.root, \"gt\", image_id))[\"gt\"]\n            label = to_pil_image(torch.from_numpy(label).unsqueeze(-1).permute(2, 0, 1))\n        except FileNotFoundError:\n            label = to_pil_image(torch.ones(1, img.height, img.width))\n\n        seed = random.randint(0, 2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        img = self.transform(img)\n\n        random.seed(seed)\n        torch.manual_seed(seed)\n        label = self.target_transform(label).squeeze(0)\n        if self.coarse_labels:\n            new_label_map = torch.zeros_like(label)\n            for fine, coarse in self.fine_to_coarse.items():\n                new_label_map[label == fine] = coarse\n            label = new_label_map\n\n        mask = (label > 0).to(torch.float32)\n        return img, label, mask\n\n    def __len__(self):\n        return len(self.files)", "\n\nclass Coco(Dataset):\n    def __init__(self, root, image_set, transform, target_transform,\n                 coarse_labels, exclude_things, subset=None):\n        super(Coco, self).__init__()\n        self.split = image_set\n        self.root = root\n        self.coarse_labels = coarse_labels\n        self.transform = transform\n        self.label_transform = target_transform\n        self.subset = subset\n        self.exclude_things = exclude_things\n\n        if self.subset is None:\n            self.image_list = \"Coco164kFull_Stuff_Coarse.txt\"\n        elif self.subset == 6:  # IIC Coarse\n            self.image_list = \"Coco164kFew_Stuff_6.txt\"\n        elif self.subset == 7:  # IIC Fine\n            self.image_list = \"Coco164kFull_Stuff_Coarse_7.txt\"\n\n        assert self.split in [\"train\", \"val\", \"train+val\"]\n        split_dirs = {\n            \"train\": [\"train2017\"],\n            \"val\": [\"val2017\"],\n            \"train+val\": [\"train2017\", \"val2017\"]\n        }\n\n        self.image_files = []\n        self.label_files = []\n        for split_dir in split_dirs[self.split]:\n            with open(join(self.root, \"curated\", split_dir, self.image_list), \"r\") as f:\n                img_ids = [fn.rstrip() for fn in f.readlines()]\n                for img_id in img_ids:\n                    self.image_files.append(join(self.root, \"images\", split_dir, img_id + \".jpg\"))\n                    self.label_files.append(join(self.root, \"annotations\", split_dir, img_id + \".png\"))\n\n        self.fine_to_coarse = {0: 9, 1: 11, 2: 11, 3: 11, 4: 11, 5: 11, 6: 11, 7: 11, 8: 11, 9: 8, 10: 8, 11: 8, 12: 8,\n                               13: 8, 14: 8, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7,\n                               25: 6, 26: 6, 27: 6, 28: 6, 29: 6, 30: 6, 31: 6, 32: 6, 33: 10, 34: 10, 35: 10, 36: 10,\n                               37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 5, 44: 5, 45: 5, 46: 5, 47: 5, 48: 5,\n                               49: 5, 50: 5, 51: 2, 52: 2, 53: 2, 54: 2, 55: 2, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2,\n                               61: 3, 62: 3, 63: 3, 64: 3, 65: 3, 66: 3, 67: 3, 68: 3, 69: 3, 70: 3, 71: 0, 72: 0,\n                               73: 0, 74: 0, 75: 0, 76: 0, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 4, 84: 4,\n                               85: 4, 86: 4, 87: 4, 88: 4, 89: 4, 90: 4, 91: 17, 92: 17, 93: 22, 94: 20, 95: 20, 96: 22,\n                               97: 15, 98: 25, 99: 16, 100: 13, 101: 12, 102: 12, 103: 17, 104: 17, 105: 23, 106: 15,\n                               107: 15, 108: 17, 109: 15, 110: 21, 111: 15, 112: 25, 113: 13, 114: 13, 115: 13, 116: 13,\n                               117: 13, 118: 22, 119: 26, 120: 14, 121: 14, 122: 15, 123: 22, 124: 21, 125: 21, 126: 24,\n                               127: 20, 128: 22, 129: 15, 130: 17, 131: 16, 132: 15, 133: 22, 134: 24, 135: 21, 136: 17,\n                               137: 25, 138: 16, 139: 21, 140: 17, 141: 22, 142: 16, 143: 21, 144: 21, 145: 25, 146: 21,\n                               147: 26, 148: 21, 149: 24, 150: 20, 151: 17, 152: 14, 153: 21, 154: 26, 155: 15, 156: 23,\n                               157: 20, 158: 21, 159: 24, 160: 15, 161: 24, 162: 22, 163: 25, 164: 15, 165: 20, 166: 17,\n                               167: 17, 168: 22, 169: 14, 170: 18, 171: 18, 172: 18, 173: 18, 174: 18, 175: 18, 176: 18,\n                               177: 26, 178: 26, 179: 19, 180: 19, 181: 24}\n\n        self._label_names = [\n            \"ground-stuff\",\n            \"plant-stuff\",\n            \"sky-stuff\",\n        ]\n        self.cocostuff3_coarse_classes = [23, 22, 21]\n        self.first_stuff_index = 12\n\n    def __getitem__(self, index):\n        image_path = self.image_files[index]\n        label_path = self.label_files[index]\n\n        seed = random.randint(0, 2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        img = self.transform(Image.open(image_path).convert(\"RGB\"))\n\n        random.seed(seed)\n        torch.manual_seed(seed)\n        label = self.label_transform(Image.open(label_path)).squeeze(0)\n        label[label == 255] = -1  # to be consistent with 10k\n        coarse_label = torch.zeros_like(label)\n        for fine, coarse in self.fine_to_coarse.items():\n            coarse_label[label == fine] = coarse\n        coarse_label[label == -1] = -1\n\n        if self.coarse_labels:\n            coarser_labels = -torch.ones_like(label)\n            for i, c in enumerate(self.cocostuff3_coarse_classes):\n                coarser_labels[coarse_label == c] = i\n            return img, coarser_labels, coarser_labels >= 0\n        else:\n            if self.exclude_things:\n                return img, coarse_label - self.first_stuff_index, (coarse_label >= self.first_stuff_index), image_path\n            else:\n                return img, coarse_label, coarse_label >= 0, image_path\n\n    def __len__(self):\n        return len(self.image_files)", "\n\nclass CityscapesSeg(Dataset):\n    def __init__(self, root, image_set, transform, target_transform):\n        super(CityscapesSeg, self).__init__()\n        self.split = image_set\n        self.root = root\n        if image_set == \"train\":\n            # our_image_set = \"train_extra\"\n            # mode = \"coarse\"\n            our_image_set = \"train\"\n            mode = \"fine\"\n        else:\n            our_image_set = image_set\n            mode = \"fine\"\n        self.inner_loader = Cityscapes(self.root, our_image_set,\n                                       mode=mode,\n                                       target_type=\"semantic\",\n                                       transform=None,\n                                       target_transform=None)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.first_nonvoid = 7\n\n    def __getitem__(self, index):\n        if self.transform is not None:\n            image, target = self.inner_loader[index]\n\n            seed = random.randint(0, 2147483647)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            image = self.transform(image)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            target = self.target_transform(target)\n\n            target = target - self.first_nonvoid\n            target[target < 0] = -1\n            mask = target == -1\n            return image, target.squeeze(0), mask\n        else:\n            return self.inner_loader[index]\n\n    def __len__(self):\n        return len(self.inner_loader)", "\n\nclass CroppedDataset(Dataset):\n    def __init__(self, root, dataset_name, crop_type, crop_ratio, image_set, transform, target_transform):\n        super(CroppedDataset, self).__init__()\n        self.dataset_name = dataset_name\n        self.split = image_set\n        self.root = join(root, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n        self.transform = transform\n        self.target_transform = target_transform\n        self.img_dir = join(self.root, \"img\", self.split)\n        self.label_dir = join(self.root, \"label\", self.split)\n        self.num_images = len(os.listdir(self.img_dir))\n        assert self.num_images == len(os.listdir(self.label_dir))\n\n    def __getitem__(self, index):\n        image = Image.open(join(self.img_dir, \"{}.jpg\".format(index))).convert('RGB')\n        target = Image.open(join(self.label_dir, \"{}.png\".format(index)))\n\n        seed = random.randint(0, 2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        image = self.transform(image)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        target = self.target_transform(target)\n\n        target = target - 1\n        mask = target == -1\n        return image, target.squeeze(0), mask, index\n\n    def __len__(self):\n        return self.num_images", "\n\nclass MaterializedDataset(Dataset):\n\n    def __init__(self, ds):\n        self.ds = ds\n        self.materialized = []\n        loader = DataLoader(ds, num_workers=12, collate_fn=lambda l: l[0])\n        for batch in tqdm(loader):\n            self.materialized.append(batch)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, ind):\n        return self.materialized[ind]", "\n\nclass ContrastiveSegDataset(Dataset):\n    def __init__(self,\n                 pytorch_data_dir: str,\n                 dataset_name: str,\n                 crop_type: str,\n                 model_type: str,\n                 image_set: str,\n                 transform,\n                 target_transform,\n                 cfg,\n                 aug_geometric_transform=None,\n                 aug_photometric_transform=None,\n                 num_neighbors=5,\n                 compute_knns=False,\n                 mask=False,\n                 pos_labels=False,\n                 pos_images=False,\n                 extra_transform=None,\n                 model_type_override=None\n                 ):\n        super(ContrastiveSegDataset).__init__()\n        self.num_neighbors = num_neighbors\n        self.image_set = image_set\n        self.dataset_name = dataset_name\n        self.mask = mask\n        self.pos_labels = pos_labels\n        self.pos_images = pos_images\n        self.extra_transform = extra_transform\n        self.model_type = model_type\n        if dataset_name == \"potsdam\":\n            self.n_classes = 3\n            dataset_class = Potsdam\n            extra_args = dict(coarse_labels=True)\n        elif dataset_name == \"potsdamraw\":\n            self.n_classes = 3\n            dataset_class = PotsdamRaw\n            extra_args = dict(coarse_labels=True)\n        elif dataset_name == \"cityscapes\" and crop_type is None:\n            self.n_classes = 27\n            dataset_class = CityscapesSeg\n            extra_args = dict()\n        elif dataset_name == \"cityscapes\" and crop_type is not None:\n            self.n_classes = 27\n            dataset_class = CroppedDataset\n            extra_args = dict(dataset_name=\"cityscapes\", crop_type=crop_type, crop_ratio=cfg[\"crop_ratio\"])\n        elif dataset_name == \"cocostuff3\":\n            self.n_classes = 3\n            dataset_class = Coco\n            extra_args = dict(coarse_labels=True, subset=6, exclude_things=True)\n        elif dataset_name == \"cocostuff15\":\n            self.n_classes = 15\n            dataset_class = Coco\n            extra_args = dict(coarse_labels=False, subset=7, exclude_things=True)\n        elif dataset_name == \"cocostuff27\" and crop_type is not None:\n            self.n_classes = 27\n            dataset_class = CroppedDataset\n            extra_args = dict(dataset_name=\"cocostuff27\", crop_type=cfg[\"crop_type\"], crop_ratio=cfg[\"crop_ratio\"])\n        elif dataset_name == \"cocostuff27\" and crop_type is None:\n            self.n_classes = 27\n            dataset_class = Coco\n            extra_args = dict(coarse_labels=False, subset=None, exclude_things=False)\n            if image_set == \"val\":\n                extra_args[\"subset\"] = 7\n        else:\n            raise ValueError(\"Unknown dataset: {}\".format(dataset_name))\n\n        self.aug_geometric_transform = aug_geometric_transform\n        self.aug_photometric_transform = aug_photometric_transform\n\n        self.dataset = dataset_class(\n            root=pytorch_data_dir,\n            image_set=self.image_set,\n            transform=transform,\n            target_transform=target_transform, **extra_args)\n\n        if model_type_override is not None:\n            model_type = model_type_override\n        else:\n            model_type = self.model_type\n\n        nice_dataset_name = cfg.dir_dataset_name if dataset_name == \"directory\" else dataset_name\n        feature_cache_file = join(\"nns\", \"nns_{}_{}_{}_{}_{}.npz\".format(\n            model_type, nice_dataset_name, image_set, crop_type, cfg[\"res\"]))\n        if pos_labels or pos_images:\n            if not os.path.exists(feature_cache_file) or compute_knns:\n                raise ValueError(\"could not find nn file {} please run precompute_knns\".format(feature_cache_file))\n            else:\n                loaded = np.load(feature_cache_file)\n                self.nns = loaded[\"nns\"]\n            assert len(self.dataset) == self.nns.shape[0]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _set_seed(self, seed):\n        random.seed(seed)  # apply this seed to img tranfsorms\n        torch.manual_seed(seed)  # needed for torchvision 0.7\n        # pass\n\n    def __getitem__(self, ind):\n        pack = self.dataset[ind]\n\n        if self.pos_images or self.pos_labels:\n            ind_pos = self.nns[ind][torch.randint(low=1, high=self.num_neighbors + 1, size=[]).item()]\n            pack_pos = self.dataset[ind_pos]\n\n        seed = random.randint(0, 2147483647)  # make a seed with numpy generator\n\n        self._set_seed(seed)\n        coord_entries = torch.meshgrid([torch.linspace(-1, 1, pack[0].shape[1]),\n                                        torch.linspace(-1, 1, pack[0].shape[2])], indexing=\"ij\")\n        coord = torch.cat([t.unsqueeze(0) for t in coord_entries], 0)\n\n        if self.extra_transform is not None:\n            extra_trans = self.extra_transform\n        else:\n            extra_trans = lambda i, x: x\n\n        ret = {\n            \"ind\": ind,\n            \"img\": extra_trans(ind, pack[0]),\n            \"label\": extra_trans(ind, pack[1]),\n            # \"img_path\": extra_trans(ind, pack[3])\n        }\n\n        if self.pos_images:\n            ret[\"img_pos\"] = extra_trans(ind, pack_pos[0])\n            ret[\"ind_pos\"] = ind_pos\n\n        if self.mask:\n            ret[\"mask\"] = pack[2]\n\n        if self.pos_labels:\n            ret[\"label_pos\"] = extra_trans(ind, pack_pos[1])\n            ret[\"mask_pos\"] = pack_pos[2]\n\n        if self.aug_photometric_transform is not None:\n            # img_aug = self.aug_photometric_transform(self.aug_geometric_transform(pack[0]))\n            img_aug = self.aug_photometric_transform(pack[0])\n            # img_pos_aug = self.aug_photometric_transform(pack_pos[0])\n\n            self._set_seed(seed)\n            coord_aug = self.aug_geometric_transform(coord)\n\n            ret[\"img_aug\"] = img_aug\n            ret[\"coord_aug\"] = coord_aug.permute(1, 2, 0)\n\n            #--- aug for img_pos(KNN) ---#\n            # ret[\"img_pos_aug\"] = img_pos_aug\n\n\n\n        return ret", "\n\ndef get_class_labels(dataset_name):\n    if dataset_name.startswith(\"cityscapes\"):\n        return [\n            'road', 'sidewalk', 'parking', 'rail track', 'building',\n            'wall', 'fence', 'guard rail', 'bridge', 'tunnel',\n            'pole', 'polegroup', 'traffic light', 'traffic sign', 'vegetation',\n            'terrain', 'sky', 'person', 'rider', 'car',\n            'truck', 'bus', 'caravan', 'trailer', 'train',\n            'motorcycle', 'bicycle']\n    elif dataset_name == \"cocostuff27\":\n        return [\n            \"electronic\", \"appliance\", \"food\", \"furniture\", \"indoor\",\n            \"kitchen\", \"accessory\", \"animal\", \"outdoor\", \"person\",\n            \"sports\", \"vehicle\", \"ceiling\", \"floor\", \"food\",\n            \"furniture\", \"rawmaterial\", \"textile\", \"wall\", \"window\",\n            \"building\", \"ground\", \"plant\", \"sky\", \"solid\",\n            \"structural\", \"water\"]\n    elif dataset_name == \"voc\":\n        return [\n            'background',\n            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n            'bus', 'car', 'cat', 'chair', 'cow',\n            'diningtable', 'dog', 'horse', 'motorbike', 'person',\n            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n    elif dataset_name == \"potsdam\":\n        return [\n            'roads and cars',\n            'buildings and clutter',\n            'trees and vegetation']\n    else:\n        raise ValueError(\"Unknown Dataset {}\".format(dataset_name))", "\n\ndef create_cityscapes_colormap():\n    colors = [(128, 64, 128),\n              (244, 35, 232),\n              (250, 170, 160),\n              (230, 150, 140),\n              (70, 70, 70),\n              (102, 102, 156),\n              (190, 153, 153),\n              (180, 165, 180),\n              (150, 100, 100),\n              (150, 120, 90),\n              (153, 153, 153),\n              (153, 153, 153),\n              (250, 170, 30),\n              (220, 220, 0),\n              (107, 142, 35),\n              (152, 251, 152),\n              (70, 130, 180),\n              (220, 20, 60),\n              (255, 0, 0),\n              (0, 0, 142),\n              (0, 0, 70),\n              (0, 60, 100),\n              (0, 0, 90),\n              (0, 0, 110),\n              (0, 80, 100),\n              (0, 0, 230),\n              (119, 11, 32),\n              (0, 0, 0)]\n    return np.array(colors)", "\nif __name__ == \"__main__\":\n    cmap = create_pascal_label_colormap()\n    x = np.arange(27)\n    y = np.ones(27)\n    colors = cmap[:27]/255\n    plt.bar(x, y, color=colors)\n    plt.show()\n", ""]}
{"filename": "dataset/dataset_download.py", "chunked_list": ["'''\n@article{hamilton2022unsupervised,\n  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},\n  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},\n  journal={arXiv preprint arXiv:2203.08414},\n  year={2022}\n}\n'''\n\nimport wget", "\nimport wget\nimport os\n\n\ndef my_app() -> None:\n    pytorch_data_dir = \"/data/Datasets\"\n    dataset_names = [\n        \"potsdam\",\n        \"cityscapes\",\n        \"cocostuff\",\n        \"potsdamraw\"]\n    url_base = \"https://marhamilresearch4.blob.core.windows.net/stego-public/pytorch_data/\"\n\n    os.makedirs(pytorch_data_dir, exist_ok=True)\n    for dataset_name in dataset_names:\n        if (not os.path.exists(os.path.join(pytorch_data_dir, dataset_name))) or \\\n                (not os.path.exists(os.path.join(pytorch_data_dir, dataset_name + \".zip\"))):\n            print(\"\\n Downloading {}\".format(dataset_name))\n            wget.download(url_base + dataset_name + \".zip\", os.path.join(pytorch_data_dir, dataset_name + \".zip\"))\n        else:\n            print(\"\\n Found {}, skipping download\".format(dataset_name))", "\n\nif __name__ == \"__main__\":\n    my_app()\n"]}
{"filename": "dataset/crop_datasets.py", "chunked_list": ["import os\nfrom os.path import join\nfrom dataset.data import ContrastiveSegDataset, ToTargetTensor\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms.functional import five_crop, _get_image_size, crop\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms as T", "from PIL import Image\nfrom torchvision import transforms as T\nimport argparse\nfrom utils.common_utils import parse\n\ndef _random_crops(img, size, seed, n):\n    \"\"\"Crop the given image into four corners and the central crop.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    .. Note::\n        This transform returns a tuple of images and there may be a\n        mismatch in the number of inputs and targets your ``Dataset`` returns.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped.\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n\n    Returns:\n       tuple: tuple (tl, tr, bl, br, center)\n                Corresponding top left, top right, bottom left, bottom right and center crop.\n    \"\"\"\n    if isinstance(size, int):\n        size = (int(size), int(size))\n    elif isinstance(size, (tuple, list)) and len(size) == 1:\n        size = (size[0], size[0])\n\n    if len(size) != 2:\n        raise ValueError(\"Please provide only two dimensions (h, w) for size.\")\n\n    image_width, image_height = _get_image_size(img)\n    crop_height, crop_width = size\n    if crop_width > image_width or crop_height > image_height:\n        msg = \"Requested crop size {} is bigger than input size {}\"\n        raise ValueError(msg.format(size, (image_height, image_width)))\n\n    images = []\n    for i in range(n):\n        seed1 = hash((seed, i, 0))\n        seed2 = hash((seed, i, 1))\n        crop_height, crop_width = int(crop_height), int(crop_width)\n\n        top = seed1 % (image_height - crop_height)\n        left = seed2 % (image_width - crop_width)\n        images.append(crop(img, top, left, crop_height, crop_width))\n\n    return images", "\n\nclass RandomCropComputer(Dataset):\n\n    def _get_size(self, img):\n        if len(img.shape) == 3:\n            return [int(img.shape[1] * self.crop_ratio), int(img.shape[2] * self.crop_ratio)]\n        elif len(img.shape) == 2:\n            return [int(img.shape[0] * self.crop_ratio), int(img.shape[1] * self.crop_ratio)]\n        else:\n            raise ValueError(\"Bad image shape {}\".format(img.shape))\n\n    def random_crops(self, i, img):\n        return _random_crops(img, self._get_size(img), i, 5)\n\n    def five_crops(self, i, img):\n        return five_crop(img, self._get_size(img))\n\n    def __init__(self, cfg, dataset_name, img_set, crop_type, crop_ratio):\n        self.pytorch_data_dir = cfg[\"dataset\"][\"data_path\"]\n        self.crop_ratio = crop_ratio\n        self.save_dir = join(\n            cfg[\"dataset\"][\"data_path\"], \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n        self.img_set = img_set\n        self.dataset_name = dataset_name\n        self.cfg = cfg\n\n        self.img_dir = join(self.save_dir, \"img\", img_set)\n        self.label_dir = join(self.save_dir, \"label\", img_set)\n        os.makedirs(self.img_dir, exist_ok=True)\n        os.makedirs(self.label_dir, exist_ok=True)\n\n        if crop_type == \"random\":\n            cropper = lambda i, x: self.random_crops(i, x)\n        elif crop_type == \"five\":\n            cropper = lambda i, x: self.five_crops(i, x)\n        else:\n            raise ValueError('Unknown crop type {}'.format(crop_type))\n\n        self.dataset = ContrastiveSegDataset(\n            cfg[\"dataset\"][\"data_path\"],\n            dataset_name,\n            None,\n            img_set,\n            T.ToTensor(),\n            ToTargetTensor(),\n            cfg=cfg,\n            num_neighbors=cfg[\"dataset\"][\"num_neighbors\"],\n            pos_labels=False,\n            pos_images=False,\n            mask=False,\n            aug_geometric_transform=None,\n            aug_photometric_transform=None,\n            extra_transform=cropper\n        )\n\n    def __getitem__(self, item):\n        batch = self.dataset[item]\n        imgs = batch['img']\n        labels = batch['label']\n        for crop_num, (img, label) in enumerate(zip(imgs, labels)):\n            img_num = item * 5 + crop_num\n            img_arr = img.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n            label_arr = (label + 1).unsqueeze(0).permute(1, 2, 0).to('cpu', torch.uint8).numpy().squeeze(-1)\n            Image.fromarray(img_arr).save(join(self.img_dir, \"{}.jpg\".format(img_num)), 'JPEG')\n            Image.fromarray(label_arr).save(join(self.label_dir, \"{}.png\".format(img_num)), 'PNG')\n        return True\n\n    def __len__(self):\n        return len(self.dataset)", "\n\ndef my_app(cfg) -> None:\n\n    # dataset_names = [\"cityscapes\", \"cocostuff27\"]\n    dataset_names = [\"cocostuff27\"]\n    img_sets = [\"train\", \"val\"]\n    crop_types = [\"five\"]\n    crop_ratios = [.5, .7]\n\n    # dataset_names = [\"cityscapes\"]\n    # img_sets = [\"train\", \"val\"]\n    # crop_types = [\"five\"]\n    # crop_ratios = [.5]\n\n    for crop_ratio in crop_ratios:\n        for crop_type in crop_types:\n            for dataset_name in dataset_names:\n                for img_set in img_sets:\n                    dataset = RandomCropComputer(cfg, dataset_name, img_set, crop_type, crop_ratio)\n                    loader = DataLoader(dataset, 1, shuffle=False, num_workers=8, collate_fn=lambda l: l)\n                    for _ in tqdm(loader):\n                        pass", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--opt\", type=str, required=True, help=\"Path to option JSON file.\")\n    parser_args = parser.parse_args()\n    parser_opt = parse(parser_args.opt)\n    my_app(parser_opt)\n", ""]}
