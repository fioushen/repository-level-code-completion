{"filename": "inference-from-wav.py", "chunked_list": ["import os\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom configs import CNENHPS\nfrom models import BetaVAEVC\nfrom audio import TestUtils, Audio\n", "from audio import TestUtils, Audio\n\n\ndef read_mels(wav_list_f, audio_processor):\n    mels = []\n    mel_names = []\n    with open(wav_list_f, 'r', encoding='utf-8') as f:\n        for line in f:\n            mel = extract_mel(line.strip(), audio_processor).astype(np.float32)\n            mels.append(mel)\n            name = line.strip().split('/')[-1].split('.')[0]\n            mel_names.append(name)\n    return mels, mel_names", "\n\ndef extract_mel(wav_f, audio_processor):\n    wav_arr = audio_processor.load_wav(wav_f)\n    wav_arr = audio_processor.trim_silence_by_trial(wav_arr, top_db=20., lower_db=25.)\n    wav_arr = wav_arr / max(0.01, np.max(np.abs(wav_arr)))\n    wav_arr = audio_processor.preemphasize(wav_arr)\n    mel = audio_processor.melspectrogram(wav_arr).T\n    return mel\n", "\n\ndef synthesize_from_mel(args):\n    ckpt_path = args.ckpt_path\n    ckpt_step = ckpt_path.split('-')[-1]\n    assert os.path.isfile(args.src_wavs)\n    assert os.path.isfile(args.ref_wavs)\n    test_dir = args.test_dir\n    if not os.path.isdir(test_dir):\n        os.makedirs(test_dir)\n    hparams = CNENHPS()\n    tester = TestUtils(hparams, args.test_dir)\n    audio_processor = Audio(hparams.Audio)\n    # setup model\n    model = BetaVAEVC(hparams)\n    checkpoint = tf.train.Checkpoint(model=model)\n    checkpoint.restore(ckpt_path).expect_partial()\n\n    # set up tf function\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n    def vc(mels, mel_ext, m_lengths):\n        out, _ = model.post_inference(mels, m_lengths, mel_ext)\n        return out\n\n    src_mels, src_names = read_mels(args.src_wavs, audio_processor)\n    ref_mels, ref_names = read_mels(args.ref_wavs, audio_processor)\n    for src_mel, src_name in tqdm(zip(src_mels, src_names)):\n        for ref_mel, ref_name in zip(ref_mels, ref_names):\n            while ref_mel.shape[0] < hparams.Dataset.chunk_size:\n                ref_mel = np.concatenate([ref_mel, ref_mel], axis=0)\n            ref_mel = ref_mel[:hparams.Dataset.chunk_size, :]\n            assert src_mel.shape[1] == hparams.Audio.num_mels\n            src_mel_batch = tf.constant(np.expand_dims(src_mel, axis=0))\n            ref_mel_batch = tf.constant(np.expand_dims(ref_mel, axis=0))\n            mel_len_batch = tf.constant([src_mel.shape[0]])\n            ids = ['{}_to_{}'.format(src_name, ref_name)]\n            prediction = vc(src_mel_batch, ref_mel_batch, mel_len_batch)\n            # tester.synthesize_and_save_wavs(\n            #     ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n            tester.write_mels(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n    return", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('')\n    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n    parser.add_argument('--test_dir', type=str, help='directory to save test results')\n    parser.add_argument('--src_wavs', type=str, help='source wav file list')\n    parser.add_argument('--ref_wavs', type=str, help='reference wav npy file list')\n    main_args = parser.parse_args()\n    synthesize_from_mel(main_args)", ""]}
{"filename": "train.py", "chunked_list": ["import os\nimport sys\nimport random\nimport argparse\nimport datetime\nimport numpy as np\nimport tensorflow as tf\n\nfrom time import time\n", "from time import time\n\nfrom models import BetaVAEVC\nfrom audio import TestUtils\nfrom datasets import TFRecordWriter\nfrom configs import CNENHPS, Logger\n\n\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    return", "def set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    return\n\n\ndef set_global_determinism(seed):\n    set_seeds(seed=seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    return", "def set_global_determinism(seed):\n    set_seeds(seed=seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    return\n\n\ndef main():\n    parser = argparse.ArgumentParser('Training parameters parser')\n    parser.add_argument('--data_dir', type=str, help='dataset tfrecord directory')\n    parser.add_argument('--out_dir', type=str, help='directory to save logs', default='outputs')\n    args = parser.parse_args()\n\n    hparams = CNENHPS()\n    # set random seed\n    set_global_determinism(hparams.Train.random_seed)\n\n    # validate log directories\n    out_dir = args.out_dir\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    model_dir = os.path.join(out_dir, 'models')\n    if not os.path.isdir(model_dir):\n        os.makedirs(model_dir)\n    log_dir = os.path.join(out_dir, 'logs')\n    if not os.path.isdir(log_dir):\n        os.makedirs(log_dir)\n    test_dir = os.path.join(out_dir, 'tests')\n    if not os.path.isdir(test_dir):\n        os.makedirs(test_dir)\n\n    # set up test utils\n    tester = TestUtils(hparams, test_dir)\n\n    # set up logger\n    sys.stdout = Logger(log_dir)\n\n    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    train_dir = os.path.join(log_dir, current_time, 'train')\n    os.makedirs(train_dir)\n    val_dir = os.path.join(log_dir, current_time, 'val')\n    os.makedirs(val_dir)\n\n    # hyperparameters\n    data_records = TFRecordWriter(\n        save_dir=args.data_dir, chunk_size=hparams.Dataset.chunk_size)\n    train_set = data_records.create_dataset(\n        buffer_size=hparams.Dataset.buffer_size,\n        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n        batch_size=hparams.Train.train_batch_size,\n        num_mels=hparams.Audio.num_mels,\n        shuffle_buffer=hparams.Train.shuffle_buffer,\n        shuffle=hparams.Train.shuffle,\n        tfrecord_files=data_records.get_tfrecords_list('train'),\n        seed=hparams.Train.random_seed)\n    val_set = data_records.create_dataset(\n        buffer_size=hparams.Dataset.buffer_size,\n        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n        batch_size=hparams.Train.train_batch_size,\n        num_mels=hparams.Audio.num_mels,\n        shuffle_buffer=hparams.Train.shuffle_buffer,\n        shuffle=hparams.Train.shuffle,\n        tfrecord_files=data_records.get_tfrecords_list('val'),\n        seed=hparams.Train.random_seed)\n    test_set = data_records.create_dataset(\n        buffer_size=hparams.Dataset.buffer_size,\n        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n        batch_size=hparams.Train.test_batch_size,\n        num_mels=hparams.Audio.num_mels,\n        shuffle_buffer=hparams.Train.shuffle_buffer,\n        shuffle=hparams.Train.shuffle,\n        tfrecord_files=data_records.get_tfrecords_list('test'),\n        seed=hparams.Train.random_seed,\n        drop_remainder=True)\n\n    # 2. setup model\n    model = BetaVAEVC(hparams)\n    learning_rate = hparams.Train.learning_rate\n    optimizer = tf.keras.optimizers.Adam(\n        learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n\n    # 3. define training step\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n    def train_step(mels, mel_ext, m_lengths):\n        print('Tracing back at train_step')\n        with tf.GradientTape() as tape:\n            predictions, mel_l2, content_kl, spk_kl = model(\n                inputs=mels, mel_lengths=m_lengths, inp_ext=mel_ext,\n                training=True, reduce_loss=True)\n            loss = (mel_l2 + hparams.Train.content_kl_weight * content_kl\n                    + hparams.Train.spk_kl_weight * spk_kl)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, mel_l2, content_kl, spk_kl\n\n    # 4. define validate step\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n    def val_step(mels, mel_ext, m_lengths):\n        print('Tracing back at val step')\n        predictions, mel_l2, content_kl, spk_kl = model(\n            inputs=mels, mel_lengths=m_lengths, inp_ext=mel_ext,\n            training=False, reduce_loss=True)\n        loss = (mel_l2 + hparams.Train.content_kl_weight * content_kl\n                + hparams.Train.spk_kl_weight * spk_kl)\n        return loss, mel_l2, content_kl, spk_kl\n\n    # @tf.function\n    def train_one_epoch(dataset):\n        # print('tracing back at train_one_epoch')\n        step = 0\n        total = 0.0\n        mel_l2 = 0.0\n        kl = 0.0\n        spk_kl = 0.\n        for _, train_mels, train_m_lengths, train_mel_ext in dataset:\n            step_start = time()\n            _total, _mel_l2, _kl, _spk_kl = train_step(\n                train_mels, train_mel_ext, train_m_lengths)\n            step_end = time()\n            print('Step {}: total {:.4f}, mel-l2 {:.4f}, content-kl {:.4f},'\n                  ' spk-kl: {:.4f}, time {:.4f}'.format(\n                step, _total.numpy(), _mel_l2.numpy(), _kl.numpy(),\n                _spk_kl.numpy(), step_end - step_start))\n            step += 1\n            total += _total.numpy()\n            mel_l2 += _mel_l2.numpy()\n            kl += _kl.numpy()\n            spk_kl += _spk_kl.numpy()\n        return total / step, mel_l2 / step, kl / step, spk_kl / step\n\n    # @tf.function\n    def val_one_epoch(dataset):\n        step = 0\n        total = 0.0\n        mel_l2 = 0.0\n        kl = 0.0\n        spk_kl = 0.\n        for _, val_mels, val_m_lengths, val_mel_ext in dataset:\n            _total, _mel_l2, _kl, _spk_kl = val_step(\n                val_mels, val_mel_ext, val_m_lengths)\n            step += 1\n            total += _total.numpy()\n            mel_l2 += _mel_l2.numpy()\n            kl += _kl.numpy()\n            spk_kl += _spk_kl.numpy()\n        return total / step, mel_l2 / step, kl / step, spk_kl / step\n\n    # 8. setup summary writer\n    train_summary_writer = tf.summary.create_file_writer(train_dir)\n    val_summary_writer = tf.summary.create_file_writer(val_dir)\n\n    # 9. setup checkpoint: all workers will need checkpoint manager to load checkpoint\n    checkpoint = tf.train.Checkpoint(step=tf.Variable(0, dtype=tf.int64, trainable=False),\n                                     optimizer=optimizer, model=model)\n    manager = tf.train.CheckpointManager(checkpoint, model_dir, max_to_keep=20)\n    checkpoint.restore(manager.latest_checkpoint)\n    if manager.latest_checkpoint:\n        print(\"Restored from {}\".format(manager.latest_checkpoint))\n        step = checkpoint.step.numpy()\n    else:\n        print(\"Initializing from scratch.\")\n        step = 0\n\n    # 8. start training\n    for epoch in range(step + 1, hparams.Train.epochs + 1):\n        print('Training Epoch {} ...'.format(epoch))\n        epoch_start = time()\n        train_total, train_mel_l2, train_kl, train_spk_kl = train_one_epoch(train_set)\n        epoch_dur = time() - epoch_start\n        print('\\nTraining Epoch {} finished in {:.3f} Secs'.format(epoch, epoch_dur))\n        # save summary and evaluate\n        with train_summary_writer.as_default():\n            tf.summary.scalar('total-loss', train_total, step=epoch)\n            tf.summary.scalar('recon-loss', train_mel_l2, step=epoch)\n            tf.summary.scalar('content-kl', train_kl, step=epoch)\n            tf.summary.scalar('speaker-kl', train_spk_kl, step=epoch)\n\n        # validation\n        print('Validation ...')\n        val_start = time()\n        val_total, val_mel_l2, val_kl, val_spk_kl = val_one_epoch(val_set)\n        print('Validation finished in {:.3f} Secs'.format(time() - val_start))\n        with val_summary_writer.as_default():\n            tf.summary.scalar('total-loss', val_total, step=epoch)\n            tf.summary.scalar('recon-loss', val_mel_l2, step=epoch)\n            tf.summary.scalar('content-kl', val_kl, step=epoch)\n            tf.summary.scalar('speaker-kl', val_spk_kl, step=epoch)\n\n        print('Epoch {}: l2 {:.4f} / {:.4f}, content-kl {:.4f} / {:.4f}, spk-kl: {:.4f} / {:.4f}'.format(\n            epoch, train_mel_l2, val_mel_l2, train_kl, val_kl, train_spk_kl, val_spk_kl))\n\n        if epoch % hparams.Train.ckpt_interval == 0:\n            # save checkpoint\n            save_path = manager.save(checkpoint_number=epoch)\n            print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n\n        # test\n        if epoch % hparams.Train.test_interval == 0:\n            print('Testing ...')\n            i = 0\n            ref_mels = None\n            ref_spk_ids = None\n            for test_ids, test_mels, test_m_lengths, test_mel_ext in test_set.take(2):\n                if i == 0:\n                    ref_mels = test_mel_ext\n                    ref_spk_ids = test_ids\n                    i += 1\n                    continue\n                post_mel, _ = model.post_inference(test_mels, test_m_lengths, ref_mels)\n                fids = [sid.decode('utf-8') + '-ref-' + rid.decode('utf-8')\n                        for sid, rid in zip(test_ids.numpy(), ref_spk_ids.numpy())]\n                try:\n                    tester.synthesize_and_save_wavs(\n                        epoch, post_mel.numpy(), test_m_lengths.numpy(), fids, 'post')\n                except:\n                    print('Something wrong with the generated waveform!')\n            print('test finished, check {} for the results'.format(test_dir))", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "inference-from-mel.py", "chunked_list": ["import os\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom configs import CNENHPS\nfrom audio import TestUtils\nfrom models import BetaVAEVC\n", "from models import BetaVAEVC\n\n\ndef read_mels(mel_list_f):\n    mels = []\n    mel_names = []\n    with open(mel_list_f, 'r', encoding='utf-8') as f:\n        for line in f:\n            mel = np.load(line.strip()).astype(np.float32)\n            mels.append(mel)\n            name = line.strip().split('/')[-1].split('.')[0]\n            mel_names.append(name)\n    return mels, mel_names", "\n\ndef synthesize_from_mel(args):\n    ckpt_path = args.ckpt_path\n    ckpt_step = ckpt_path.split('-')[-1]\n    assert os.path.isfile(args.src_mels)\n    assert os.path.isfile(args.ref_mels)\n    test_dir = args.test_dir\n    if not os.path.isdir(test_dir):\n        os.makedirs(test_dir)\n    hparams = CNENHPS()\n    tester = TestUtils(hparams, args.test_dir)\n    # setup model\n    model = BetaVAEVC(hparams)\n    checkpoint = tf.train.Checkpoint(model=model)\n    checkpoint.restore(ckpt_path).expect_partial()\n\n    # set up tf function\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n    def vc(mels, mel_ext, m_lengths):\n        out, _ = model.post_inference(mels, m_lengths, mel_ext)\n        return out\n\n    src_mels, src_names = read_mels(args.src_mels)\n    ref_mels, ref_names = read_mels(args.ref_mels)\n    for src_mel, src_name in tqdm(zip(src_mels, src_names)):\n        for ref_mel, ref_name in zip(ref_mels, ref_names):\n            while ref_mel.shape[0] < hparams.Dataset.chunk_size:\n                ref_mel = np.concatenate([ref_mel, ref_mel], axis=0)\n            ref_mel = ref_mel[:hparams.Dataset.chunk_size, :]\n            assert src_mel.shape[1] == hparams.Audio.num_mels\n            src_mel_batch = tf.constant(np.expand_dims(src_mel, axis=0))\n            ref_mel_batch = tf.constant(np.expand_dims(ref_mel, axis=0))\n            mel_len_batch = tf.constant([src_mel.shape[0]])\n            ids = ['{}_to_{}'.format(src_name, ref_name)]\n            prediction = vc(src_mel_batch, ref_mel_batch, mel_len_batch)\n            tester.write_mels(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n            # tester.synthesize_and_save_wavs(ckpt_step, prediction.numpy(), mel_len_batch.numpy(), ids, prefix='test')\n    return", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('')\n    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n    parser.add_argument('--test_dir', type=str, help='directory to save test results')\n    parser.add_argument('--src_mels', type=str, help='source mel npy file list')\n    parser.add_argument('--ref_mels', type=str, help='reference mel npy file list')\n    main_args = parser.parse_args()\n    synthesize_from_mel(main_args)", ""]}
{"filename": "preprocess.py", "chunked_list": ["import os\nimport random\nimport numpy as np\nimport warnings\nfrom configs import CNENHPS\nfrom datasets import VCTK, AiShell3, MelPreprocessor, TFRecordWriter\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef main():\n    hps = CNENHPS()\n    random.seed(hps.Train.random_seed)\n    np.random.seed(hps.Train.random_seed)\n    vctk_writer = VCTK(\n        data_dir=hps.Dataset.VCTK.corpus_dir,\n        out_dir=hps.Dataset.dataset_dir,\n        val_spks=hps.Dataset.VCTK.val_spks,\n        test_spks=hps.Dataset.VCTK.test_spks)\n    vctk_writer.write_summary()\n    aishell_writer = AiShell3(\n        data_dir=hps.Dataset.AiShell3.corpus_dir,\n        out_dir=hps.Dataset.dataset_dir,\n        train_spk_file=None,\n        val_test_spk_file=hps.Dataset.AiShell3.val_test_spk_file)\n    aishell_writer.write_summary()\n    feats_extractor = MelPreprocessor(\n        [vctk_writer.summary_file, aishell_writer.summary_file],\n        save_dir=hps.Dataset.dataset_dir, hps=hps)\n    feats_extractor.feature_extraction()\n    tfrecord_save_dir = os.path.join(hps.Dataset.dataset_dir, 'tfrecords')\n    if not os.path.exists(tfrecord_save_dir):\n        os.makedirs(tfrecord_save_dir)\n    tfrecord_writer = TFRecordWriter(\n        train_split=hps.Dataset.n_record_split,\n        data_dir=hps.Dataset.dataset_dir,\n        save_dir=tfrecord_save_dir,\n        chunk_size=hps.Dataset.chunk_size)\n    tfrecord_writer.write_all()\n\n    # test\n    print('TFRecord test...')\n    tf_dataset = tfrecord_writer.create_dataset(\n        buffer_size=hps.Dataset.buffer_size,\n        num_parallel_reads=hps.Dataset.num_parallel_reads,\n        batch_size=hps.Train.test_batch_size,\n        num_mels=hps.Audio.num_mels,\n        shuffle_buffer=hps.Train.shuffle_buffer,\n        shuffle=hps.Train.shuffle,\n        tfrecord_files=tfrecord_writer.get_tfrecords_list('test'))\n    for epoch in range(2):\n        for i, data in enumerate(tf_dataset):\n            print('epoch {}, step: {}'.format(epoch, i))\n            fid, mel, mel_len, mel_ext = data\n            print(fid.numpy(), mel.shape, mel_len.numpy(), mel_ext.shape)", "\n\ndef main():\n    hps = CNENHPS()\n    random.seed(hps.Train.random_seed)\n    np.random.seed(hps.Train.random_seed)\n    vctk_writer = VCTK(\n        data_dir=hps.Dataset.VCTK.corpus_dir,\n        out_dir=hps.Dataset.dataset_dir,\n        val_spks=hps.Dataset.VCTK.val_spks,\n        test_spks=hps.Dataset.VCTK.test_spks)\n    vctk_writer.write_summary()\n    aishell_writer = AiShell3(\n        data_dir=hps.Dataset.AiShell3.corpus_dir,\n        out_dir=hps.Dataset.dataset_dir,\n        train_spk_file=None,\n        val_test_spk_file=hps.Dataset.AiShell3.val_test_spk_file)\n    aishell_writer.write_summary()\n    feats_extractor = MelPreprocessor(\n        [vctk_writer.summary_file, aishell_writer.summary_file],\n        save_dir=hps.Dataset.dataset_dir, hps=hps)\n    feats_extractor.feature_extraction()\n    tfrecord_save_dir = os.path.join(hps.Dataset.dataset_dir, 'tfrecords')\n    if not os.path.exists(tfrecord_save_dir):\n        os.makedirs(tfrecord_save_dir)\n    tfrecord_writer = TFRecordWriter(\n        train_split=hps.Dataset.n_record_split,\n        data_dir=hps.Dataset.dataset_dir,\n        save_dir=tfrecord_save_dir,\n        chunk_size=hps.Dataset.chunk_size)\n    tfrecord_writer.write_all()\n\n    # test\n    print('TFRecord test...')\n    tf_dataset = tfrecord_writer.create_dataset(\n        buffer_size=hps.Dataset.buffer_size,\n        num_parallel_reads=hps.Dataset.num_parallel_reads,\n        batch_size=hps.Train.test_batch_size,\n        num_mels=hps.Audio.num_mels,\n        shuffle_buffer=hps.Train.shuffle_buffer,\n        shuffle=hps.Train.shuffle,\n        tfrecord_files=tfrecord_writer.get_tfrecords_list('test'))\n    for epoch in range(2):\n        for i, data in enumerate(tf_dataset):\n            print('epoch {}, step: {}'.format(epoch, i))\n            fid, mel, mel_len, mel_ext = data\n            print(fid.numpy(), mel.shape, mel_len.numpy(), mel_ext.shape)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "latent_extraction.py", "chunked_list": ["import os\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom configs import CNENHPS\nfrom models import BetaVAEVC\nfrom datasets import TFRecordWriter\n", "from datasets import TFRecordWriter\n\n\ndef main(args):\n    hparams = CNENHPS()\n    data_records = TFRecordWriter(\n        save_dir=args.data_dir, chunk_size=hparams.Dataset.chunk_size)\n    val_set = data_records.create_dataset(\n        buffer_size=hparams.Dataset.buffer_size,\n        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n        batch_size=hparams.Train.test_batch_size,\n        num_mels=hparams.Audio.num_mels,\n        shuffle_buffer=hparams.Train.shuffle_buffer,\n        shuffle=hparams.Train.shuffle,\n        tfrecord_files=data_records.get_tfrecords_list('val'),\n        seed=hparams.Train.random_seed,\n        drop_remainder=False)\n    test_set = data_records.create_dataset(\n        buffer_size=hparams.Dataset.buffer_size,\n        num_parallel_reads=hparams.Dataset.num_parallel_reads,\n        batch_size=hparams.Train.test_batch_size,\n        num_mels=hparams.Audio.num_mels,\n        shuffle_buffer=hparams.Train.shuffle_buffer,\n        shuffle=hparams.Train.shuffle,\n        tfrecord_files=data_records.get_tfrecords_list('test'),\n        seed=hparams.Train.random_seed,\n        drop_remainder=False)\n\n    ckpt_path = args.ckpt_path\n    save_dir = args.save_dir\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n    cn_dir = os.path.join(save_dir, 'CN')\n    en_dir = os.path.join(save_dir, 'EN')\n    os.makedirs(cn_dir, exist_ok=True)\n    os.makedirs(en_dir, exist_ok=True)\n    # setup model\n    model = BetaVAEVC(hparams)\n    checkpoint = tf.train.Checkpoint(model=model)\n    checkpoint.restore(ckpt_path).expect_partial()\n\n    def save_spk_npy(arrs, fids):\n        for a, n in zip(arrs.numpy(), fids.numpy()):\n            n = n.decode('utf-8') if type(n) is bytes else n\n            if n.startswith('SSB'):\n                save_name = os.path.join(cn_dir, '{}-spk.npy'.format(n))\n            else:\n                save_name = os.path.join(en_dir, '{}-spk.npy'.format(n))\n            np.save(save_name, a)\n        return\n\n    def save_content_npy(arrs, lens, fids):\n        for a, l, n in zip(arrs.numpy(), lens.numpy(), fids.numpy()):\n            n = n.decode('utf-8') if type(n) is bytes else n\n            if n.startswith('SSB'):\n                save_name = os.path.join(cn_dir, '{}-content.npy'.format(n))\n            else:\n                save_name = os.path.join(en_dir, '{}-content.npy'.format(n))\n            np.save(save_name, a[:l, :])\n        return\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, None, hparams.Audio.num_mels], dtype=tf.float32),\n        tf.TensorSpec(shape=[None], dtype=tf.int32)])\n    def inference(mels, mels_ext, mel_lengths):\n        spk_mu, _ = model.spk_posterior(mels_ext)\n        reduced_mels = mels[:, ::model.reduction_factor, :]\n        reduced_lens = (mel_lengths + model.reduction_factor - 1) // model.reduction_factor\n        content_mu, _, _ = model.posterior(reduced_mels, lengths=reduced_lens, training=False)\n        return spk_mu, content_mu, reduced_lens\n\n    for dataset in [val_set, test_set]:\n        for _fids, _mels, _m_lengths, _mels_ext in tqdm(dataset):\n            spk_emb, content_emb, reduced_lengths = inference(_mels, _mels_ext, _m_lengths)\n            save_spk_npy(spk_emb, _fids)\n            save_content_npy(content_emb, reduced_lengths, _fids)\n    return", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('')\n    parser.add_argument('--data_dir', type=str, help='Tf-Records directory')\n    parser.add_argument('--ckpt_path', type=str, help='path to the model ckpt')\n    parser.add_argument('--save_dir', type=str, help='directory to save test results')\n    main_args = parser.parse_args()\n    main(main_args)\n", ""]}
{"filename": "audio/audio.py", "chunked_list": ["import os\nimport re\nimport pysptk\nimport librosa\nimport warnings\nimport soundfile\nimport numpy as np\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom scipy import interpolate", "from scipy.io import wavfile\nfrom scipy import interpolate\nwarnings.filterwarnings(\"ignore\")\n\n\nclass Audio:\n    def __init__(self, audio_hparams):\n        self.hps = audio_hparams\n\n    def load_wav(self, path):\n        return librosa.core.load(path, sr=self.hps.sample_rate)[0]\n\n    def get_duration(self, path):\n        dur = librosa.get_duration(filename=path)\n        return dur\n\n    def save_wav(self, wav, path):\n        wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n        wavfile.write(path, self.hps.sample_rate, wav.astype(np.int16))\n        return\n\n    def trim_silence_by_label(self, wav_arr, inds):\n        \"\"\"remove silence given the starting and ending time in secs\n        wav_arr: waveform samples array\n        inds: [start_time, end_time], float\n        \"\"\"\n        sample_inds = librosa.core.time_to_samples(inds, self.hps.sample_rate)\n        return wav_arr[sample_inds[0]: sample_inds[1]]\n\n    def trim_silence_by_trial(self, wav_arr, top_db=15., lower_db=40., time_threshold=0.3):\n        sample_len_th = int(time_threshold * self.hps.sample_rate)\n        trimed, old_inds = librosa.effects.trim(\n            wav_arr, top_db=top_db, frame_length=self.hps.frame_length_sample,\n            hop_length=self.hps.frame_shift_sample)\n        for top_db in np.arange(top_db, lower_db, 0.1):\n            trimed, inds = librosa.effects.trim(\n                wav_arr, top_db=top_db, frame_length=self.hps.frame_length_sample,\n                hop_length=self.hps.frame_shift_sample)\n            if old_inds[0] - inds[0] + inds[1] - old_inds[1] >= sample_len_th:\n                break\n            else:\n                old_inds = inds\n        trimed = wav_arr[old_inds[0]: old_inds[1]]\n        return trimed\n\n    def spectrogram(self, y, clip_norm=True):\n        D = self._stft(y)\n        S = self._amp_to_db(np.abs(D)) - self.hps.ref_level_db\n        if clip_norm:\n            S = self._normalize(S)\n        return S\n\n    def logf0(self, y):\n        lower_f0 = self.hps.lower_f0\n        upper_f0 = self.hps.upper_f0\n        sr = self.hps.sample_rate\n        hop_len = self.hps.frame_shift_sample\n        x = y / max(abs(y)) * 32768.0\n        f0 = pysptk.sptk.rapt(x, sr, hopsize=hop_len, min=lower_f0, max=upper_f0, otype='f0')\n        logf0 = np.log(np.where(f0 == 0., np.ones_like(f0), f0))\n        return logf0\n\n    def inv_spectrogram(self, spectrogram):\n        S = self._db_to_amp(self._denormalize(spectrogram) + self.hps.ref_level_db)\n        return self._griffin_lim(S ** self.hps.power)\n\n    def test(self, y, clip_norm=True):\n        D = self._stft(y)\n        src = np.abs(D)\n        print('linear: ', np.min(src), np.max(src))\n        mel_ = self._linear_to_mel(np.abs(D))\n        print('mel_linear: ', np.min(mel_), np.max(mel_))\n        mel_db = self._amp_to_db(mel_)\n        print('mel_db: ', np.min(mel_db), np.max(mel_db))\n        mel_db_ref = mel_db - self.hps.ref_level_db\n        print('mel_db_ref: ', np.min(mel_db_ref), np.max(mel_db_ref))\n        if clip_norm:\n            mel_db_ref = self._normalize(mel_db_ref)\n            print('mel_db_ref_norm: ', np.min(mel_db_ref), np.max(mel_db_ref))\n            mel_db_ref_denorm = self._denormalize(mel_db_ref)\n            print('mel_db_ref_denorm: ', np.min(mel_db_ref_denorm), np.max(mel_db_ref_denorm))\n        else:\n            mel_db_ref_denorm = mel_db_ref\n        mel_db_de_ref = mel_db_ref_denorm + self.hps.ref_level_db\n        print('mel_db_de_ref: ', np.min(mel_db_de_ref), np.max(mel_db_de_ref))\n        mel_linear = self._db_to_amp(mel_db_de_ref)\n        print('mel_linear_re: ', np.min(mel_linear), np.max(mel_linear))\n        linear_sp = self._mel_to_linear(mel_linear)\n        print('linear_re: ', np.min(linear_sp), np.max(linear_sp))\n\n        print(np.mean(np.abs(src - linear_sp)))\n\n    def melspectrogram(self, y, clip_norm=True):\n        D = self._stft(y)\n        S = self._amp_to_db(self._linear_to_mel(np.abs(D))) - self.hps.ref_level_db\n        if clip_norm:\n            S = self._normalize(S)\n        return S\n\n    def inv_mel_spectrogram(self, mel_spectrogram):\n        S = self._mel_to_linear(self._db_to_amp(\n            self._denormalize(mel_spectrogram) + self.hps.ref_level_db))\n        return self._griffin_lim(S ** self.hps.power)\n\n    def find_endpoint(self, wav, threshold_db=-40.0, min_silence_sec=0.8):\n        window_length = int(self.hps.sample_rate * min_silence_sec)\n        hop_length = int(window_length / 4)\n        threshold = self._db_to_amp(threshold_db)\n        for x in range(hop_length, len(wav) - window_length, hop_length):\n            if np.max(wav[x: x + window_length]) < threshold:\n                return x + hop_length\n        return len(wav)\n\n    def _griffin_lim(self, S):\n        angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n        S_complex = np.abs(S).astype(np.complex)\n        y = self._istft(S_complex * angles)\n        for i in range(self.hps.griffin_lim_iters):\n            angles = np.exp(1j * np.angle(self._stft(y)))\n            y = self._istft(S_complex * angles)\n        return y\n\n    def _stft(self, y):\n        n_fft, hop_length, win_length = self._stft_parameters()\n        if len(y.shape) == 1:  # [time_steps]\n            return librosa.stft(y=y, n_fft=n_fft,\n                                hop_length=hop_length,\n                                win_length=win_length,\n                                center=self.hps.center)\n        elif len(y.shape) == 2:  # [batch_size, time_steps]\n            if y.shape[0] == 1:  # batch_size=1\n                return np.expand_dims(librosa.stft(y=y[0], n_fft=n_fft,\n                                                   hop_length=hop_length,\n                                                   win_length=win_length,\n                                                   center=self.hps.center),\n                                      axis=0)\n            else:  # batch_size > 1\n                spec_list = list()\n                for wav in y:\n                    spec_list.append(librosa.stft(y=wav, n_fft=n_fft,\n                                                  hop_length=hop_length,\n                                                  win_length=win_length,\n                                                  center=self.hps.center))\n                return np.concatenate(spec_list, axis=0)\n        else:\n            raise Exception('Wav dimension error in stft function!')\n\n    def _istft(self, y):\n        _, hop_length, win_length = self._stft_parameters()\n        if len(y.shape) == 2:  # spectrogram shape: [n_frame, n_fft]\n            return librosa.istft(y, hop_length=hop_length,\n                                 win_length=win_length,\n                                 center=self.hps.center)\n        elif len(y.shape) == 3:  # spectrogram shape: [batch_size, n_frame, n_fft]\n            if y.shape[0] == 1:  # batch_size = 1\n                return np.expand_dims(librosa.istft(y[0],\n                                                    hop_length=hop_length,\n                                                    win_length=win_length,\n                                                    center=self.hps.center),\n                                      axis=0)\n            else:  # batch_size > 1\n                wav_list = list()\n                for spec in y:\n                    wav_list.append(librosa.istft(spec,\n                                                  hop_length=hop_length,\n                                                  win_length=win_length,\n                                                  center=self.hps.center))\n                    return np.concatenate(wav_list, axis=0)\n        else:\n            raise Exception('Spectrogram dimension error in istft function!')\n\n    def _stft_parameters(self):\n        n_fft = (self.hps.num_freq - 1) * 2\n        # hop_length = int(self.hps.frame_shift_ms / 1000 * self.hps.sample_rate)\n        # win_length = int(self.hps.frame_length_ms / 1000 * self.hps.sample_rate)\n        hop_length = self.hps.frame_shift_sample\n        win_length = self.hps.frame_length_sample\n        return n_fft, hop_length, win_length\n\n    def _linear_to_mel(self, spectrogram):\n        _mel_basis = self._build_mel_basis()\n        return np.dot(_mel_basis, spectrogram)\n\n    def _mel_to_linear(self, mel_spectrogram):\n        _inv_mel_basis = np.linalg.pinv(self._build_mel_basis())\n        linear_spectrogram = np.dot(_inv_mel_basis, mel_spectrogram)\n        if len(linear_spectrogram.shape) == 3:\n            # for 3-dimension mel, the shape of\n            # inverse linear spectrogram will be [num_freq, batch_size, n_frame]\n            linear_spectrogram = np.transpose(linear_spectrogram, [1, 0, 2])\n        return np.maximum(1e-10, linear_spectrogram)\n\n    def _build_mel_basis(self):\n        n_fft = (self.hps.num_freq - 1) * 2\n        return librosa.filters.mel(\n            self.hps.sample_rate,\n            n_fft=n_fft,\n            n_mels=self.hps.num_mels,\n            fmin=self.hps.min_mel_freq,\n            fmax=self.hps.max_mel_freq)\n\n    @staticmethod\n    def _amp_to_db(x):\n        return 20 * np.log10(np.maximum(1e-5, x))\n\n    @staticmethod\n    def _db_to_amp(x):\n        return np.power(10.0, x * 0.05)\n\n    def _normalize(self, S):\n        if self.hps.symmetric_specs:\n            return np.clip(\n                (2 * self.hps.max_abs_value) * (\n                        (S - self.hps.min_level_db) / (-self.hps.min_level_db)\n                ) - self.hps.max_abs_value,\n                -self.hps.max_abs_value, self.hps.max_abs_value)\n        else:\n            return np.clip(self.hps.max_abs_value * (\n                    (S - self.hps.min_level_db) / (-self.hps.min_level_db)),\n                           0, self.hps.max_abs_value)\n\n    def _denormalize(self, S):\n        if self.hps.symmetric_specs:\n            return ((np.clip(S, -self.hps.max_abs_value, self.hps.max_abs_value)\n                     + self.hps.max_abs_value) * (-self.hps.min_level_db)\n                    / (2 * self.hps.max_abs_value)\n                    + self.hps.min_level_db)\n        else:\n            return ((np.clip(S, 0, self.hps.max_abs_value) * (-self.hps.min_level_db)\n                     / self.hps.max_abs_value)\n                    + self.hps.min_level_db)\n\n    def preemphasize(self, x):\n        if len(x.shape) == 1:  # [time_steps]\n            return signal.lfilter([1, -self.hps.preemphasize], [1], x).astype(x.dtype)\n        elif len(x.shape) == 2:  # [batch_size, time_steps]\n            if x.shape[0] == 1:\n                return np.expand_dims(\n                    signal.lfilter([1, -self.hps.preemphasize], [1], x[0]), axis=0).astype(x.dtype)\n            wav_list = list()\n            for wav in x:\n                wav_list.append(signal.lfilter([1, -self.hps.preemphasize], [1], wav).astype(x.dtype))\n            return np.concatenate(wav_list, axis=0)\n        else:\n            raise Exception('Wave dimension error in pre-emphasis')\n\n    def inv_preemphasize(self, x):\n        if self.hps.preemphasize is None:\n            return x\n        if len(x.shape) == 1:  # [time_steps]\n            return signal.lfilter([1], [1, -self.hps.preemphasize], x)\n        elif len(x.shape) == 2:  # [batch_size, time_steps]\n            if x.shape[0] == 1:\n                return np.expand_dims(\n                    signal.lfilter([1], [1, -self.hps.preemphasize], x[0]), axis=0)\n            wav_list = list()\n            for wav in x:\n                wav_list.append(signal.lfilter([1], [1, -self.hps.preemphasize], wav))\n            return np.concatenate(wav_list, axis=0)\n        else:\n            raise Exception('Wave dimension error in inverse pre-emphasis')\n\n    def mfcc(self, y):\n        from scipy.fftpack import dct\n        preemphasized = self.preemphasize(y)\n        D = self._stft(preemphasized)\n        S = librosa.power_to_db(self._linear_to_mel(np.abs(D) ** 2))\n        mfcc = dct(x=S, axis=0, type=2, norm='ortho')[:self.hps.n_mfcc]\n        deltas = librosa.feature.delta(mfcc)\n        delta_deltas = librosa.feature.delta(mfcc, order=2)\n        mfcc_feature = np.concatenate((mfcc, deltas, delta_deltas), axis=0)\n        return mfcc_feature.T\n\n    def hyper_parameters_estimation(self, wav_dir):\n        from tqdm import tqdm\n        wavs = []\n        for root, dirs, files in os.walk(wav_dir):\n            for f in files:\n                if re.match(r'.+\\.wav', f):\n                    wavs.append(os.path.join(root, f))\n        mel_db_min = 100.0\n        mel_db_max = -100.0\n        for f in tqdm(wavs):\n            wav_arr = self.load_wav(f)\n            pre_emphasized = self.preemphasize(wav_arr)\n            D = self._stft(pre_emphasized)\n            S = self._amp_to_db(self._linear_to_mel(np.abs(D)))\n            mel_db_max = np.max(S) if np.max(S) > mel_db_max else mel_db_max\n            mel_db_min = np.min(S) if np.min(S) < mel_db_min else mel_db_min\n        return mel_db_min, mel_db_max\n\n    def _magnitude_spectrogram(self, audio, clip_norm):\n        preemp_audio = self.preemphasize(audio)\n        mel_spec = self.melspectrogram(preemp_audio, clip_norm)\n        linear_spec = self.spectrogram(preemp_audio, clip_norm)\n        return mel_spec.T, linear_spec.T\n\n    def _energy_spectrogram(self, audio):\n        preemp_audio = self.preemphasize(audio)\n        linear_spec = np.abs(self._stft(preemp_audio)) ** 2\n        mel_spec = self._linear_to_mel(linear_spec)\n        return mel_spec.T, linear_spec.T\n\n    def _extract_min_max(self, wav_path, mode, post_fn=lambda x: x):\n        num_mels = self.hps.num_mels\n        num_linears = self.hps.num_freq\n\n        wavs = []\n        for root, dirs, files in os.walk(wav_path):\n            for f in files:\n                if re.match(r'.+\\.wav', f):\n                    wavs.append(os.path.join(root, f))\n\n        num_wavs = len(wavs)\n        mel_mins_per_wave = np.zeros((num_wavs, num_mels))\n        mel_maxs_per_wave = np.zeros((num_wavs, num_mels))\n        linear_mins_per_wave = np.zeros((num_wavs, num_linears))\n        linear_maxs_per_wave = np.zeros((num_wavs, num_linears))\n\n        for i, wav in enumerate(post_fn(wavs)):\n            audio, sr = soundfile.read(wav)\n            if mode == 'magnitude':\n                mel, linear = self._magnitude_spectrogram(audio, clip_norm=False)\n            elif mode == 'energy':\n                mel, linear = self._energy_spectrogram(audio)\n            else:\n                raise Exception('Only magnitude or energy is supported!')\n\n            mel_mins_per_wave[i,] = np.amin(mel, axis=0)\n            mel_maxs_per_wave[i,] = np.amax(mel, axis=0)\n            linear_mins_per_wave[i,] = np.amin(linear, axis=0)\n            linear_maxs_per_wave[i,] = np.amax(linear, axis=0)\n\n        mel_mins = np.reshape(np.amin(mel_mins_per_wave, axis=0), (1, num_mels))\n        mel_maxs = np.reshape(np.amax(mel_maxs_per_wave, axis=0), (1, num_mels))\n        linear_mins = np.reshape(np.amin(linear_mins_per_wave, axis=0), (1, num_mels))\n        linear_maxs = np.reshape(np.amax(linear_mins_per_wave, axis=0), (1, num_mels))\n        min_max = {\n            'mel_min': mel_mins,\n            'mel_max': mel_maxs,\n            'linear_mins': linear_mins,\n            'linear_max': linear_maxs\n        }\n        return min_max\n\n    @staticmethod\n    def _normalize_min_max(spec, maxs, mins, max_value=1.0, min_value=0.0):\n        spec_dim = len(spec.T)\n        num_frame = len(spec)\n\n        max_min = maxs - mins\n        max_min = np.reshape(max_min, (1, spec_dim))\n        max_min[max_min <= 0.0] = 1.0\n\n        target_max_min = np.zeros((1, spec_dim))\n        target_max_min.fill(max_value - min_value)\n        target_max_min[max_min <= 0.0] = 1.0\n\n        spec_min = np.tile(mins, (num_frame, 1))\n        target_min = np.tile(min_value, (num_frame, spec_dim))\n        spec_range = np.tile(max_min, (num_frame, 1))\n        norm_spec = np.tile(target_max_min, (num_frame, 1)) / spec_range\n        norm_spec = norm_spec * (spec - spec_min) + target_min\n        return norm_spec\n\n    @staticmethod\n    def _denormalize_min_max(spec, maxs, mins, max_value=1.0, min_value=0.0):\n        spec_dim = len(spec.T)\n        num_frame = len(spec)\n\n        max_min = maxs - mins\n        max_min = np.reshape(max_min, (1, spec_dim))\n        max_min[max_min <= 0.0] = 1.0\n\n        target_max_min = np.zeros((1, spec_dim))\n        target_max_min.fill(max_value - min_value)\n        target_max_min[max_min <= 0.0] = 1.0\n\n        spec_min = np.tile(mins, (num_frame, 1))\n        target_min = np.tile(min_value, (num_frame, spec_dim))\n        spec_range = np.tile(max_min, (num_frame, 1))\n        denorm_spec = spec_range / np.tile(target_max_min, (num_frame, 1))\n        denorm_spec = denorm_spec * (spec - target_min) + spec_min\n        return denorm_spec\n\n    @staticmethod\n    def rescale(mel):\n        x = np.linspace(1, mel.shape[0], mel.shape[0])\n        xn = np.linspace(1, mel.shape[0], int(mel.shape[0] * 1.25))\n        f = interpolate.interp1d(x, mel, kind='cubic', axis=0)\n        rescaled_mel = f(xn)\n        return rescaled_mel", ""]}
{"filename": "audio/__init__.py", "chunked_list": ["from .audio import *\nfrom .utils import *\n"]}
{"filename": "audio/utils.py", "chunked_list": ["import os\nimport threading\nimport multiprocessing\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom audio import Audio\n\n\nclass TestUtils:\n    def __init__(self, hps, save_dir):\n        self.prcocessor = Audio(hps.Audio)\n        self.hps = hps\n        self.save_dir = save_dir\n\n    def write_mels(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        for i in range(mel_batch.shape[0]):\n            mel = mel_batch[i][:mel_lengths[i], :]\n            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n            mel_name = os.path.join(self.save_dir, '{}-{}-{}.npy'.format(prefix, idx, step))\n            np.save(mel_name, mel)\n        return\n\n    def synthesize_and_save_wavs(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        def _synthesize(mel, fid):\n            wav_arr = self.prcocessor.inv_mel_spectrogram(mel.T)\n            wav_arr = self.prcocessor.inv_preemphasize(wav_arr)\n            self.prcocessor.save_wav(wav_arr, os.path.join(self.save_dir, '{}-{}-{}.wav'.format(prefix, fid, step)))\n            return\n        threads = []\n        for i in range(mel_batch.shape[0]):\n            mel = mel_batch[i][:mel_lengths[i], :]\n            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n            t = threading.Thread(target=_synthesize, args=(mel, idx))\n            threads.append(t)\n            t.start()\n        for x in threads:\n            x.join()\n        return\n\n    @staticmethod\n    def draw_mel_process(args):\n        mel, ml, save_name = args\n        plt.imshow(mel[:ml, :].T, aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}'.format(save_name))\n        plt.close()\n\n    def draw_melspectrograms(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        matplotlib.use('agg')\n        save_names = []\n        for idx in ids:\n            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n            save_names.append(save_name)\n        pool = multiprocessing.Pool()\n        data = zip(mel_batch, mel_lengths, save_names)\n        pool.map(TestUtils.draw_mel_process, data)\n        return\n\n    def draw_self_att_process(self, args):\n        ali, mlen, save_name = args\n        fig, ax = plt.subplots()\n        ax.imshow(ali[:mlen, : mlen], aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}.pdf'.format(save_name))\n        plt.close()\n        return\n\n    def draw_multi_head_self_att_process(self, args):\n        ali, mlen, save_name, num_heads = args\n        fig = plt.figure()\n        for j, head_ali in enumerate(ali):\n            ax = fig.add_subplot(2, num_heads // 2, j + 1)\n            ax.imshow(head_ali[:mlen, :mlen], aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}'.format(save_name))\n        plt.close()\n        return\n\n    def multi_draw_self_att_alignments(self, batch_ali, mel_lengths, step, ids, prefix='posterior'):\n        matplotlib.use('agg')\n        save_names = []\n        for idx in ids:\n            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n            save_names.append(save_name)\n        pool = multiprocessing.Pool()\n        if len(batch_ali.shape) == 3:\n            data = zip(batch_ali, mel_lengths, save_names)\n            pool.map(self.draw_self_att_process, data)\n        elif len(batch_ali.shape) == 4:\n            data = zip(batch_ali, mel_lengths, save_names,\n                       [batch_ali.shape[1]] * batch_ali.shape[0])\n            pool.map(self.draw_multi_head_self_att_process, data)\n        print('Attentions for {} are plotted'.format(prefix))\n        return", "\nclass TestUtils:\n    def __init__(self, hps, save_dir):\n        self.prcocessor = Audio(hps.Audio)\n        self.hps = hps\n        self.save_dir = save_dir\n\n    def write_mels(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        for i in range(mel_batch.shape[0]):\n            mel = mel_batch[i][:mel_lengths[i], :]\n            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n            mel_name = os.path.join(self.save_dir, '{}-{}-{}.npy'.format(prefix, idx, step))\n            np.save(mel_name, mel)\n        return\n\n    def synthesize_and_save_wavs(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        def _synthesize(mel, fid):\n            wav_arr = self.prcocessor.inv_mel_spectrogram(mel.T)\n            wav_arr = self.prcocessor.inv_preemphasize(wav_arr)\n            self.prcocessor.save_wav(wav_arr, os.path.join(self.save_dir, '{}-{}-{}.wav'.format(prefix, fid, step)))\n            return\n        threads = []\n        for i in range(mel_batch.shape[0]):\n            mel = mel_batch[i][:mel_lengths[i], :]\n            idx = ids[i].decode('utf-8') if type(ids[i]) is bytes else ids[i]\n            t = threading.Thread(target=_synthesize, args=(mel, idx))\n            threads.append(t)\n            t.start()\n        for x in threads:\n            x.join()\n        return\n\n    @staticmethod\n    def draw_mel_process(args):\n        mel, ml, save_name = args\n        plt.imshow(mel[:ml, :].T, aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}'.format(save_name))\n        plt.close()\n\n    def draw_melspectrograms(self, step, mel_batch, mel_lengths, ids, prefix=''):\n        matplotlib.use('agg')\n        save_names = []\n        for idx in ids:\n            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n            save_names.append(save_name)\n        pool = multiprocessing.Pool()\n        data = zip(mel_batch, mel_lengths, save_names)\n        pool.map(TestUtils.draw_mel_process, data)\n        return\n\n    def draw_self_att_process(self, args):\n        ali, mlen, save_name = args\n        fig, ax = plt.subplots()\n        ax.imshow(ali[:mlen, : mlen], aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}.pdf'.format(save_name))\n        plt.close()\n        return\n\n    def draw_multi_head_self_att_process(self, args):\n        ali, mlen, save_name, num_heads = args\n        fig = plt.figure()\n        for j, head_ali in enumerate(ali):\n            ax = fig.add_subplot(2, num_heads // 2, j + 1)\n            ax.imshow(head_ali[:mlen, :mlen], aspect='auto', origin='lower')\n        plt.tight_layout()\n        plt.savefig('{}'.format(save_name))\n        plt.close()\n        return\n\n    def multi_draw_self_att_alignments(self, batch_ali, mel_lengths, step, ids, prefix='posterior'):\n        matplotlib.use('agg')\n        save_names = []\n        for idx in ids:\n            idx = idx.decode('utf-8') if type(idx) is bytes else idx\n            save_name = self.save_dir + '/{}-{}-{}.pdf'.format(prefix, idx, step)\n            save_names.append(save_name)\n        pool = multiprocessing.Pool()\n        if len(batch_ali.shape) == 3:\n            data = zip(batch_ali, mel_lengths, save_names)\n            pool.map(self.draw_self_att_process, data)\n        elif len(batch_ali.shape) == 4:\n            data = zip(batch_ali, mel_lengths, save_names,\n                       [batch_ali.shape[1]] * batch_ali.shape[0])\n            pool.map(self.draw_multi_head_self_att_process, data)\n        print('Attentions for {} are plotted'.format(prefix))\n        return", ""]}
{"filename": "configs/logger.py", "chunked_list": ["import os\nimport sys\n\n\nclass Logger(object):\n    def __init__(self, log_dir, log_f='train.log'):\n        self.terminal = sys.stdout\n        log_fn = os.path.join(log_dir, log_f)\n        self.log = open(log_fn, \"a\")\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        #this flush method is needed for python 3 compatibility.\n        #this handles the flush command by doing nothing.\n        #you might want to specify some extra behavior here.\n        pass", ""]}
{"filename": "configs/__init__.py", "chunked_list": ["from .hparams import *\nfrom .logger import Logger\n"]}
{"filename": "configs/hparams.py", "chunked_list": ["class CNENHPS:\n    class Train:\n        random_seed = 123\n        epochs = 1000\n        train_batch_size = 32\n        test_batch_size = 8\n        test_interval = 100\n        ckpt_interval = 50\n        shuffle_buffer = 128\n        shuffle = True\n        num_samples = 1\n        length_weight = 1.\n        content_kl_weight = 5e-3\n        spk_kl_weight = 1e-5\n        learning_rate = 1.25e-4\n\n    class Dataset:\n        buffer_size = 65536\n        num_parallel_reads = 64\n        dev_set_rate = 0.05\n        test_set_rate = 0.05\n        chunk_size = 256  # the length of the mel-sepctrogram that is required by speaker encoder\n        segment_size = 16  # the length of the smallest unit to segment and shuffle the chunk\n        n_record_split = 32\n        preprocess_n_jobs = 16\n\n        # define dataset specifics below\n        class VCTK:\n            corpus_dir = '/path/to/extracted/vctk'\n            val_spks = ['p225', 'p243', 'p231', 'p251', 'p258', 'p271', 'p284', 'p326', 'p374', 'p334']\n            test_spks = ['p274', 'p293', 'p360', 'p262', 'p314', 'p239',  'p273', 'p302', 'p270', 'p340']\n\n        class AiShell3:\n            corpus_dir = '/path/to/extracted/data_aishell3'\n            train_spk_file = './train-speakers.txt'\n            val_test_spk_file = './test-speakers.txt'\n\n        dataset_dir = '/path/to/save/features'\n\n    class Audio:\n        num_mels = 80\n        num_freq = 1025\n        min_mel_freq = 0.\n        max_mel_freq = 8000.\n        sample_rate = 16000\n        frame_length_sample = 800\n        frame_shift_sample = 200\n        preemphasize = 0.97\n        sil_trim_db = 20.\n        min_level_db = -100.0\n        ref_level_db = 20.0\n        max_abs_value = 1\n        symmetric_specs = False\n        griffin_lim_iters = 60\n        power = 1.5\n        center = True\n\n    class Common:\n        latent_dim = 128\n        output_dim = 80\n        reduction_factor = 2\n\n    class Decoder:\n        class Transformer:\n            pre_n_conv = 2\n            pre_conv_kernel = 3\n            pre_drop_rate = 0.2\n            nblk = 2\n            attention_dim = 256\n            attention_heads = 4\n            ffn_hidden = 1024\n            attention_temperature = 1.\n            attention_causality = False\n            attention_window = 16\n            post_n_conv = 5\n            post_conv_filters = 256\n            post_conv_kernel = 5\n            post_drop_rate = 0.2\n\n    class ContentPosterior:\n        class Transformer:\n            pre_n_conv = 2\n            pre_conv_kernel = 3\n            pre_hidden = 256\n            pre_drop_rate = 0.2\n            pos_drop_rate = 0.2\n            nblk = 2\n            attention_dim = 256\n            attention_heads = 4\n            temperature = 1.0\n            attention_causality = False\n            attention_window = 8\n            ffn_hidden = 1024\n\n    class SpkPosterior:\n        class ConvSpkEncoder:\n            hidden_channels = 256\n            conv_kernels = [3, 3, 5, 5]\n            activation = 'relu'", ""]}
{"filename": "tests/compute_eer.py", "chunked_list": ["import os\nimport json\nimport random\nimport argparse\nimport scipy.stats\nimport numpy as np\nfrom tqdm import tqdm\nfrom scipy.optimize import brentq\nfrom sklearn.metrics import roc_curve\nfrom scipy.interpolate import interp1d", "from sklearn.metrics import roc_curve\nfrom scipy.interpolate import interp1d\n\n\ndef create_trials(data_dir, mode='spk', anchor_f=None):\n    \"\"\"\n    :param data_dir: under which there are spk_uid.npy\n    :param mode: suffix of the feature numpy filename\n    :return: trials: [(enrolled_vector, test_vector), ...]\n             labels: [0, 1, ...]\n             spk2vecs: {spk: {fid: [arr, ...]}}\n    \"\"\"\n    assert mode in ['spk', 'content']\n\n    def get_spk_fid(basename):\n        if basename.startswith('SSB'):\n            spk = basename[:7]\n            fid = basename.split('-')[0]\n        else:\n            spk = basename.split('_')[0]\n            fid = basename.split('-')[0]\n        return spk, fid\n\n    spk2vecs = {}\n    for f in os.listdir(data_dir):\n        if f.endswith('{}.npy'.format(mode)):\n            spk, fid = get_spk_fid(f)\n            path = os.path.join(data_dir, f)\n            feat = np.load(path)\n            if len(feat.shape) == 2:\n                feat = np.mean(feat, axis=0)\n            if spk in spk2vecs.keys():\n                spk2vecs[spk][fid] = feat\n            else:\n                spk2vecs[spk] = {fid: feat}\n    trials = []\n    labels = []\n    if anchor_f is not None:\n        anchor_dict = get_anchors(anchor_f)\n    else:\n        anchor_dict = {}\n        for spk in spk2vecs.keys():\n            if len(spk2vecs[spk].keys()) <= 100:\n                continue\n            anchors = random.sample(list(spk2vecs[spk].keys()), 4)\n            anchor_dict[spk] = anchors\n        lang = 'CN' if spk.startswith('SSB') else 'EN'\n        with open('SV_anchors-{}.json'.format(lang), 'w', encoding='utf-8') as f:\n            json.dump(anchor_dict, f)\n    for spk in spk2vecs.keys():\n        if len(spk2vecs[spk].keys()) <= 100:\n            continue\n        anchors = anchor_dict[spk]\n        aps = list(spk2vecs[spk].keys())\n        for anchor in anchors:\n            if anchor not in aps:\n                print(anchor)\n            else:\n                aps.remove(anchor)\n        ans = []\n        for s in spk2vecs.keys():\n            if s != spk:\n                ans += list(spk2vecs[s].keys())\n        anchor_key = '{}_anchor'.format(spk)\n        spk2vecs[spk][anchor_key] = np.mean(\n            np.stack([spk2vecs[spk][k] for k in anchors], axis=0), axis=0)\n        for ap in aps:\n            trials.append((anchor_key, ap))\n            labels.append(1)\n        for an in ans:\n            trials.append((anchor_key, an))\n            labels.append(0)\n    return trials, labels, spk2vecs", "\n\ndef get_anchors(anchor_f):\n    with open(anchor_f, 'r') as f:\n        anchor_dict = json.load(f)  # {spk: [fid1, fid2, ...]}\n    return anchor_dict\n\n\ndef mean_confidence_interval(data, confidence=0.95):\n    a = 1.0 * np.array(data)\n    n = len(a)\n    m, se = np.mean(a), scipy.stats.sem(a)\n    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n - 1)\n    return m, h", "def mean_confidence_interval(data, confidence=0.95):\n    a = 1.0 * np.array(data)\n    n = len(a)\n    m, se = np.mean(a), scipy.stats.sem(a)\n    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n - 1)\n    return m, h\n\n\ndef cosine_score(trials, vecs_dict):\n\n    def get_spk(inp_str):\n        if inp_str.startswith('SSB'):\n            spk = inp_str[:7]\n        else:\n            spk = inp_str.split('_')[0]\n        return spk\n\n    scores = []\n    for item in tqdm(trials):\n        spk0 = get_spk(item[0])\n        enroll_vector = vecs_dict[spk0][item[0]]\n        spk1 = get_spk(item[1])\n        test_vector = vecs_dict[spk1][item[1]]\n        score = enroll_vector.dot(test_vector.T)\n        denom = np.linalg.norm(enroll_vector) * np.linalg.norm(test_vector)\n        score = score / denom\n        scores.append(score)\n    return scores", "def cosine_score(trials, vecs_dict):\n\n    def get_spk(inp_str):\n        if inp_str.startswith('SSB'):\n            spk = inp_str[:7]\n        else:\n            spk = inp_str.split('_')[0]\n        return spk\n\n    scores = []\n    for item in tqdm(trials):\n        spk0 = get_spk(item[0])\n        enroll_vector = vecs_dict[spk0][item[0]]\n        spk1 = get_spk(item[1])\n        test_vector = vecs_dict[spk1][item[1]]\n        score = enroll_vector.dot(test_vector.T)\n        denom = np.linalg.norm(enroll_vector) * np.linalg.norm(test_vector)\n        score = score / denom\n        scores.append(score)\n    return scores", "\n\ndef compute_eer(labels, scores):\n    \"\"\"sklearn style compute eer\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n    threshold = interp1d(fpr, thresholds)(eer)\n    return eer, threshold\n", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--data_dir', type=str,\n        help='directory of all test speaker embedding vectors', required=True)\n    parser.add_argument(\n        '--mode', type=str, choices=['spk', 'content'], required=True)\n    parser.add_argument('--n_runs', type=int, default=1)\n    parser.add_argument('--anchor', type=str, help='Anchor file')\n    args = parser.parse_args()\n    assert os.path.isdir(args.data_dir)\n    eers = []\n    labels = []\n    for _ in range(args.n_runs):\n        trials, labels, spk2vecs = create_trials(args.data_dir, args.mode, anchor_f=args.anchor)\n        scores = cosine_score(trials, spk2vecs)\n        eer, _ = compute_eer(labels, scores)\n        eers.append(eer)\n    eer, h = mean_confidence_interval(eers)\n    n_trials = len(labels)\n    n_pos = sum(labels)\n    n_neg = n_trials - n_pos\n    print('There are {} trials for {} representation, {} of them are positive trials '\n          'and {} are negative ones.'.format(n_trials, args.mode, n_pos, n_neg))\n    print('The EER is {:.4f} \u00b1 {:.4f}'.format(eer, h))\n    return", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "models/vc.py", "chunked_list": ["import tensorflow as tf\nfrom modules.decoder import TransformerDecoder\nfrom modules.posterior import TransformerPosterior, ConvSpkEncoder\n\n\nclass BetaVAEVC(tf.keras.Model):\n    def __init__(self, hps, name='BetaVAEVC', **kwargs):\n        super(BetaVAEVC, self).__init__(name=name, **kwargs)\n        self.hps = hps\n        self.n_sample = hps.Train.num_samples\n        self.reduction_factor = hps.Common.reduction_factor\n        self.chunk_size = hps.Dataset.chunk_size\n        self.segment_size = hps.Dataset.segment_size\n        assert self.chunk_size % self.segment_size == 0\n        self.spk_posterior = ConvSpkEncoder(\n            hidden_channels=hps.SpkPosterior.ConvSpkEncoder.hidden_channels,\n            conv_kernels=hps.SpkPosterior.ConvSpkEncoder.conv_kernels,\n            out_channels=hps.Common.latent_dim,\n            activation=hps.SpkPosterior.ConvSpkEncoder.activation)\n        self.decoder = TransformerDecoder(\n            pre_n_conv=hps.Decoder.Transformer.pre_n_conv,\n            pre_conv_kernel=hps.Decoder.Transformer.pre_conv_kernel,\n            pre_drop_rate=hps.Decoder.Transformer.pre_drop_rate,\n            nblk=hps.Decoder.Transformer.nblk,\n            attention_dim=hps.Decoder.Transformer.attention_dim,\n            attention_heads=hps.Decoder.Transformer.attention_heads,\n            temperature=hps.Decoder.Transformer.attention_temperature,\n            attention_causality=hps.Decoder.Transformer.attention_causality,\n            attention_window=hps.Decoder.Transformer.attention_window,\n            ffn_hidden=hps.Decoder.Transformer.ffn_hidden,\n            post_n_conv=hps.Decoder.Transformer.post_n_conv,\n            post_conv_filters=hps.Decoder.Transformer.post_conv_filters,\n            post_conv_kernel=hps.Decoder.Transformer.post_conv_kernel,\n            post_drop_rate=hps.Decoder.Transformer.post_drop_rate,\n            out_dim=hps.Common.output_dim,\n            reduction_factor=hps.Common.reduction_factor,\n            name='transformer_decoder')\n        self.posterior = TransformerPosterior(\n            pre_n_conv=hps.ContentPosterior.Transformer.pre_n_conv,\n            pre_conv_kernel=hps.ContentPosterior.Transformer.pre_conv_kernel,\n            pre_hidden=hps.ContentPosterior.Transformer.pre_hidden,\n            pre_drop_rate=hps.ContentPosterior.Transformer.pre_drop_rate,\n            pos_drop_rate=hps.ContentPosterior.Transformer.pos_drop_rate,\n            nblk=hps.ContentPosterior.Transformer.nblk,\n            attention_dim=hps.ContentPosterior.Transformer.attention_dim,\n            attention_heads=hps.ContentPosterior.Transformer.attention_heads,\n            temperature=hps.ContentPosterior.Transformer.temperature,\n            attention_causality=hps.ContentPosterior.Transformer.attention_causality,\n            attention_window=hps.ContentPosterior.Transformer.attention_window,\n            ffn_hidden=hps.ContentPosterior.Transformer.ffn_hidden,\n            latent_dim=hps.Common.latent_dim)\n\n    def _compute_l2_loss(self, reconstructed, targets, lengths=None, reduce=None):\n        max_time = tf.shape(reconstructed)[1]\n        dim = tf.shape(reconstructed)[2]\n        r = tf.reshape(reconstructed, [-1, self.n_sample, max_time, dim])\n        t = tf.reshape(targets, [-1, self.n_sample, max_time, dim])\n        if lengths is not None:\n            seq_mask = tf.sequence_mask(lengths, max_time, dtype=tf.float32)\n            seq_mask = tf.reshape(seq_mask, [-1, self.n_sample, max_time])\n            reshaped_lens = tf.reshape(lengths, [-1, self.n_sample])\n            l2_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    tf.reduce_mean(tf.square(r - t) + tf.abs(r - t), axis=-1) * seq_mask,\n                    axis=-1) / tf.cast(reshaped_lens, tf.float32),\n                axis=-1)\n        else:\n            l2_loss = tf.math.reduce_mean(tf.square(r - t) + tf.abs(r - t), axis=[1, 2, 3])\n        if reduce:\n            return tf.math.reduce_mean(l2_loss)\n        else:\n            return l2_loss\n\n    @staticmethod\n    def kl_with_normal(logvar, mu, reduce=None):\n        kl = -0.5 * tf.reduce_sum(1. + logvar - mu ** 2 - tf.exp(logvar), axis=1)\n        if reduce:\n            return tf.reduce_mean(kl)\n        else:\n            return kl\n\n    @staticmethod\n    def kl_between_normal2d(mu1, logvar1, mu2, logvar2, reduce=None):\n        var1 = tf.math.exp(logvar1)\n        var2 = tf.math.exp(logvar2)\n        kl = 0.5 * tf.reduce_sum(\n            logvar2 - logvar1 - 1. + var1 / var2 + (mu2 - mu1) ** 2. / var2, axis=1)\n        if reduce:\n            kl = tf.reduce_mean(kl)\n        return kl\n\n    @staticmethod\n    def kl_between_normal3d(mu1, logvar1, mu2, logvar2, lengths=None, reduce=None):\n        var1 = tf.math.exp(logvar1)\n        var2 = tf.math.exp(logvar2)\n        kl = 0.5 * tf.reduce_sum(\n            logvar2 - logvar1 - 1. + var1 / var2 + (mu2 - mu1) ** 2. / var2, axis=2)\n        if lengths is None:\n            kl = tf.reduce_mean(kl, axis=1)\n        else:\n            max_len = tf.shape(mu1)[1]\n            mask = tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\n            kl = tf.reduce_sum(kl * mask, axis=1) / tf.cast(lengths, tf.float32)\n        if reduce:\n            kl = tf.reduce_mean(kl)\n        return kl\n\n    def random_shuffle(self, inputs):\n        \"\"\"\n        :param inputs: [batch, chunk_size, dim]\n        :return:\n        \"\"\"\n        bs = tf.shape(inputs)[0]\n        mel_dim = self.hps.Audio.num_mels\n        reshaped = tf.reshape(\n            inputs, [bs, self.chunk_size // self.segment_size, self.segment_size * mel_dim])\n        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n        shuffled = tf.random.shuffle(transposed)\n        shuffled = tf.transpose(shuffled, perm=[1, 0, 2])\n        shuffled = tf.reshape(shuffled, [bs, self.chunk_size, mel_dim])\n        return shuffled\n\n    def call(self, inputs, mel_lengths, inp_ext, training=None, reduce_loss=None):\n        \"\"\"\n        :param inputs: [batch, mel_max_time, mel_dim]\n        :param mel_lengths: [batch, ]\n        :param inp_ext: [batch, chunk_size, mel_dim]\n        :param training: bool\n        :param reduce_loss: bool\n        :return: predicted mel: [batch, mel_max_time, mel_dim]\n                 loss: float32\n        \"\"\"\n        print('Tracing back at Model.call')\n        # shape info\n        mel_max_len = tf.shape(inputs)[1]\n        # reduce the mels\n        reduced_mels = inputs[:, ::self.reduction_factor, :]\n        reduced_mels.set_shape([None, None, self.hps.Audio.num_mels])\n        reduced_mel_lens = (mel_lengths + self.reduction_factor - 1) // self.reduction_factor\n        reduced_max_len = tf.shape(reduced_mels)[1]\n\n        # text encoding\n        content_mu, content_logvar, _ = self.posterior(\n            reduced_mels, lengths=reduced_mel_lens, training=training)\n        # samples, eps: [batch, n_sample, mel_max_time, dim]\n        samples, eps = self.posterior.reparameterize(\n            content_mu, content_logvar, self.n_sample)\n        posterior_samples = tf.squeeze(samples, axis=1)\n\n        # compute speaker identity prior\n        ref_mel = self.random_shuffle(inp_ext)\n        spk_posterior_mu, spk_posterior_logvar = self.spk_posterior(ref_mel)\n        spk_posterior_embd = self.spk_posterior.sample(spk_posterior_mu, spk_posterior_logvar)\n        spk_kl = self.kl_between_normal2d(\n            spk_posterior_mu, spk_posterior_logvar, tf.zeros_like(spk_posterior_mu),\n            tf.zeros_like(spk_posterior_logvar), reduce=reduce_loss)\n\n        content_kl = self.kl_between_normal3d(\n            content_mu, content_logvar, tf.zeros_like(content_mu),\n            tf.zeros_like(content_logvar), reduced_mel_lens, reduce_loss)\n        posterior_samples_spk = tf.concat(\n            [posterior_samples, tf.tile(tf.expand_dims(spk_posterior_embd, axis=1),\n                                        [1, reduced_max_len, 1])], axis=2)\n        decoded_initial, decoded_post, _ = self.decoder(\n            posterior_samples_spk, reduced_mel_lens, training=training)\n        decoded_initial = decoded_initial[:, :mel_max_len, :]\n        decoded_post = decoded_post[:, :mel_max_len, :]\n        initial_l2_loss = self._compute_l2_loss(\n            decoded_initial, inputs, mel_lengths, reduce_loss)\n        post_l2_loss = self._compute_l2_loss(\n            decoded_post, inputs, mel_lengths, reduce_loss)\n        l2_loss = initial_l2_loss + post_l2_loss\n        return decoded_post, l2_loss, content_kl, spk_kl\n\n    def post_inference(self, inputs, mel_lengths, ref_mels):\n        spk_mu, spk_logs = self.spk_posterior(ref_mels)\n        inputs = inputs[:, ::self.reduction_factor, :]\n        mel_lengths = (mel_lengths + self.reduction_factor - 1) // self.reduction_factor\n        mu, _, _ = self.posterior(\n            inputs, lengths=mel_lengths, training=False)\n        reduced_max_len = tf.shape(mu)[1]\n        mu = tf.concat(\n            [mu, tf.tile(tf.expand_dims(spk_mu, axis=1), [1, reduced_max_len, 1])], axis=2)\n        _, predicted_mel, dec_alignments = self.decoder(\n            inputs=mu, lengths=mel_lengths, training=False)\n        return predicted_mel, dec_alignments", ""]}
{"filename": "models/__init__.py", "chunked_list": ["from .vc import *\n"]}
{"filename": "modules/posterior.py", "chunked_list": ["import numpy as np\nimport tensorflow as tf\nfrom modules.attention import SelfAttentionBLK\nfrom modules.utils import PositionalEncoding, ConvPreNet, EncBlk, get_activation\n\n\nclass TransformerPosterior(tf.keras.layers.Layer):\n    def __init__(self, pre_n_conv, pre_conv_kernel, pre_hidden, pre_drop_rate,\n                 pos_drop_rate, nblk, attention_dim, attention_heads,\n                 temperature, attention_causality, attention_window,\n                 ffn_hidden, latent_dim, name='TransformerPosterior'):\n        super(TransformerPosterior, self).__init__(name=name)\n        self.pos_weight = tf.Variable(1.0, trainable=True)\n        self.prenet = ConvPreNet(\n            nconv=pre_n_conv, hidden=pre_hidden, conv_kernel=pre_conv_kernel, drop_rate=pre_drop_rate)\n        self.pe = PositionalEncoding('EncoderPositionEncoding')\n        self.pe_dropout = tf.keras.layers.Dropout(rate=pos_drop_rate)\n        self.attentions = []\n        for i in range(nblk):\n            attention = SelfAttentionBLK(\n                input_dim=pre_hidden, attention_dim=attention_dim,\n                attention_heads=attention_heads, attention_temperature=temperature,\n                causality=attention_causality, attention_window=attention_window,\n                ffn_hidden=ffn_hidden)\n            self.attentions.append(attention)\n        self.mu_projection = tf.keras.layers.Dense(\n            latent_dim, kernel_initializer='zeros', name='mu_projection')\n        self.logvar_projection = tf.keras.layers.Dense(\n            latent_dim, kernel_initializer='zeros', name='logvar_projection')\n\n    def call(self, inputs, lengths=None, training=None):\n        print('Tracing back at posterior call')\n        prenet_outs = self.prenet(inputs, training=training)\n        max_time = tf.shape(prenet_outs)[1]\n        dim = tf.shape(prenet_outs)[2]\n        pos = self.pe.positional_encoding(max_time, dim)\n        pos_embs = prenet_outs + self.pos_weight * pos\n        pos_embs = self.pe_dropout(pos_embs, training=training)\n        att_outs = pos_embs\n        for att in self.attentions:\n            att_outs, alignments = att(\n                inputs=att_outs, memory=att_outs, query_lengths=lengths,\n                memory_lengths=lengths, training=training)\n        mu = self.mu_projection(att_outs)\n        logvar = self.logvar_projection(att_outs)\n        return mu, logvar, att_outs\n\n    def sample(self, inputs, lengths, nsamples=tf.constant(1),\n               random=tf.constant(True), training=None):\n        mu, logvar, _ = self.call(inputs, lengths, training=training)\n        samples, eps = self.reparameterize(mu, logvar, nsamples, random)\n        log_probs = self.log_probability(mu, logvar, eps, lengths)\n        return samples, log_probs\n\n    @staticmethod\n    def reparameterize(mu, logvar, nsamples=tf.constant(1), random=tf.constant(True)):\n        \"\"\"\n        :param mu: [batch, max_time, dim]\n        :param logvar: [batch, max_time, dim]\n        :param nsamples: int\n        :param random: whether sample from N(0, 1) or just use zeros\n        :return: samples, noises, [batch, nsamples, max_time, dim]\n        \"\"\"\n        print('Tracing back at posterior reparameterize')\n        batch = tf.shape(mu)[0]\n        max_time = tf.shape(mu)[1]\n        dim = tf.shape(mu)[2]\n        std = tf.math.exp(0.5 * logvar)\n        if random:\n            eps = tf.random.normal([batch, nsamples, max_time, dim])\n        else:\n            eps = tf.zeros([batch, nsamples, max_time, dim])\n        samples = eps * tf.expand_dims(std, axis=1) + tf.expand_dims(mu, axis=1)\n        return samples, eps\n\n    @staticmethod\n    def log_probability(mu, logvar, z=None, eps=None, seq_lengths=None, epsilon=tf.constant(1e-8)):\n        \"\"\"\n        :param mu: [batch, max_time, dim]\n        :param logvar: [batch, max_time, dim]\n        :param z: [batch, nsamples, max_time, dim]\n        :param eps: [batch, nsamples, max_time, dim]\n        :param seq_lengths: [batch, ]\n        :param epsilon: small float number to avoid overflow\n        :return: log probabilities, [batch, nsamples]\n        \"\"\"\n        print('Tracing back at posterior log-probability')\n        batch = tf.shape(mu)[0]\n        max_time = tf.shape(mu)[1]\n        dim = tf.shape(mu)[2]\n        std = tf.math.exp(0.5 * logvar)\n        normalized_samples = (eps if eps is not None\n                              else (z - tf.expand_dims(mu, axis=1))\n                                   / (tf.expand_dims(std, axis=1) + epsilon))\n        expanded_logvar = tf.expand_dims(logvar, axis=1)\n        # time_level_log_probs [batch, nsamples, max_time]\n        time_level_log_probs = -0.5 * (\n                tf.cast(dim, tf.float32) * tf.math.log(2 * np.pi)\n                + tf.reduce_sum(expanded_logvar + normalized_samples ** 2.,\n                                axis=3))\n        seq_mask = (tf.sequence_mask(seq_lengths, maxlen=max_time, dtype=tf.float32)\n                    if seq_lengths is not None\n                    else tf.ones([batch, max_time]))\n        seq_mask = tf.expand_dims(seq_mask, axis=1)  # [batch, 1, max_time]\n        sample_level_log_probs = tf.reduce_sum(seq_mask * time_level_log_probs,\n                                               axis=2)  # [batch, nsamples]\n        return sample_level_log_probs", "\n\nclass ConvSpkEncoder(tf.keras.layers.Layer):\n    \"\"\"\n    Encode time-invariant features, e.g., speaker identity\n    \"\"\"\n\n    def __init__(self, hidden_channels, conv_kernels, out_channels, activation, name='ConvSpkEncoder'):\n        super(ConvSpkEncoder, self).__init__(name=name)\n        self.out_channels = out_channels\n        self.activation = get_activation(activation)\n        self.conv_initial = tf.keras.layers.Conv1D(hidden_channels, kernel_size=1, padding='valid')\n        self.conv_layers = [\n            EncBlk(hidden_channels, kernel_size=k, activation=self.activation, pooling_kernel=2)\n            for k in conv_kernels]\n        self.mu_logs_linear = tf.keras.layers.Dense(self.out_channels * 2, kernel_initializer='zeros')\n        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n\n    def call(self, x):\n        \"\"\"\n        :param x: [batch, channels, time-length]\n        :return: mu and logs of shape [batch, out_channels]\n        \"\"\"\n        print('Tracing back at ConvSpkEncoder.call')\n        h = self.conv_initial(x)\n        for conv in self.conv_layers:\n            h = conv(h)\n        h = self.activation(h)\n        h_avg = self.global_avg_pooling(h)\n        mu_logs = self.mu_logs_linear(h_avg)\n        mu, logs = tf.split(mu_logs, 2, axis=1)\n        return mu, logs\n\n    def sample(self, spk_mu, spk_logvar, temperature=1.):\n        batch_size = tf.shape(spk_mu)[0]\n        eps = tf.random.normal(\n            [batch_size, self.out_channels], mean=0., stddev=temperature)\n        return spk_mu + eps * tf.math.exp(0.5 * spk_logvar)\n\n    @staticmethod\n    def log_probability(spk_mu, spk_logvar, samples, epsilon=tf.constant(1e-8)):\n        spk_std = tf.math.exp(0.5 * spk_logvar)\n        normalized_samples = (samples - spk_mu) / (spk_std + epsilon)\n        # [batch,]\n        log_probs = -0.5 * tf.reduce_sum(\n            tf.math.log(2 * np.pi) + spk_logvar + normalized_samples ** 2., axis=1)\n        return log_probs", ""]}
{"filename": "modules/decoder.py", "chunked_list": ["import tensorflow as tf\nfrom modules.utils import Conv1D, PostNet, ConvPreNet\nfrom modules.attention import SelfAttentionBLK\n\n\nclass TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, pre_n_conv, pre_conv_kernel, pre_drop_rate, nblk, attention_dim,\n                 attention_heads, temperature, attention_causality, attention_window,\n                 ffn_hidden, post_n_conv, post_conv_filters, post_conv_kernel, post_drop_rate,\n                 out_dim, reduction_factor, name='TransformerDecoder'):\n        super(TransformerDecoder, self).__init__(name=name)\n        self.reduction_factor = reduction_factor\n        self.out_dim = out_dim\n        self.pre_projection = ConvPreNet(\n            nconv=pre_n_conv, hidden=attention_dim, conv_kernel=pre_conv_kernel, drop_rate=pre_drop_rate)\n        self.attentions = []\n        for i in range(nblk):\n            attention = SelfAttentionBLK(\n                input_dim=attention_dim,\n                attention_dim=attention_dim,\n                attention_heads=attention_heads,\n                attention_temperature=temperature,\n                causality=attention_causality,\n                attention_window=attention_window,\n                ffn_hidden=ffn_hidden, name='decoder-attention-{}'.format(i))\n            self.attentions.append(attention)\n        self.out_projection = tf.keras.layers.Dense(units=out_dim * self.reduction_factor,\n                                                    name='linear_outputs')\n        self.postnet = PostNet(n_conv=post_n_conv, conv_filters=post_conv_filters,\n                               conv_kernel=post_conv_kernel, drop_rate=post_drop_rate,\n                               name='postnet')\n        self.residual_projection = tf.keras.layers.Dense(units=out_dim, name='residual_outputs')\n\n    def call(self, inputs, lengths=None, training=None):\n        print('Tracing back at Self-attention decoder')\n        batch_size = tf.shape(inputs)[0]\n        max_len = tf.shape(inputs)[1]\n        att_outs = self.pre_projection(inputs, training=training)\n        alignments = {}\n        for att in self.attentions:\n            att_outs, ali = att(\n                inputs=att_outs, memory=att_outs, query_lengths=lengths,\n                memory_lengths=lengths, training=training)\n            alignments[att.name] = ali\n        initial_outs = self.out_projection(att_outs)\n        initial_outs = tf.reshape(\n            initial_outs, [batch_size, max_len * self.reduction_factor, self.out_dim])\n        residual = self.postnet(initial_outs, training=training)\n        residual = self.residual_projection(residual)\n        outputs = residual + initial_outs\n        return initial_outs, outputs, alignments", "\n\nclass PitchPredictor(tf.keras.layers.Layer):\n    def __init__(self, n_conv, conv_filters, conv_kernel, drop_rate, name='PitchPredictor'):\n        super(PitchPredictor, self).__init__(name=name)\n        self.conv_stack = [Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n                                  activation=tf.nn.relu, drop_rate=drop_rate)\n                           for _ in range(n_conv)]\n        self.out_proj = tf.keras.layers.Dense(units=4)\n\n    def call(self, inputs, training=None):\n        conv_outs = inputs\n        for conv in self.conv_stack:\n            conv_outs = conv(conv_outs, training=training)\n        outs = self.out_proj(conv_outs)\n        return outs", ""]}
{"filename": "modules/__init__.py", "chunked_list": ["from .decoder import TransformerDecoder\nfrom .posterior import TransformerPosterior\n"]}
{"filename": "modules/utils.py", "chunked_list": ["import tensorflow as tf\n\n\nclass PreNet(tf.keras.layers.Layer):\n    def __init__(self, units, drop_rate, activation, name='PreNet', **kwargs):\n        super(PreNet, self).__init__(name=name, **kwargs)\n        self.dense1 = tf.keras.layers.Dense(\n            units=units, activation=activation, name='dense_1')\n        self.dense2 = tf.keras.layers.Dense(\n            units=units, activation=activation, name='dense_2')\n        self.dropout_layer = tf.keras.layers.Dropout(rate=drop_rate)\n\n    def call(self, inputs, training=None):\n        dense1_out = self.dense1(inputs)\n        dense1_out = self.dropout_layer(dense1_out, training=training)\n        dense2_out = self.dense2(dense1_out)\n        dense2_out = self.dropout_layer(dense2_out, training=training)\n        return dense2_out", "\n\nclass ConvPreNet(tf.keras.layers.Layer):\n    def __init__(self, nconv, hidden, conv_kernel, drop_rate,\n                 activation=tf.nn.relu, bn_before_act=True, name='ConvPrenet', **kwargs):\n        super(ConvPreNet, self).__init__(name=name, **kwargs)\n        self.conv_stack = []\n        for i in range(nconv):\n            conv = Conv1D(filters=hidden, kernel_size=conv_kernel, activation=activation,\n                          drop_rate=drop_rate, bn_before_act=bn_before_act,\n                          name='PreNetConv{}'.format(i))\n            self.conv_stack.append(conv)\n        self.projection = tf.keras.layers.Dense(units=hidden)\n\n    def call(self, inputs, training=None):\n        conv_outs = inputs\n        for conv in self.conv_stack:\n            conv_outs = conv(conv_outs, training=training)\n        projections = self.projection(conv_outs)\n        return projections", "\n\nclass FFN(tf.keras.layers.Layer):\n    def __init__(self, hidden1, hidden2, name='PositionalFeedForward', **kwargs):\n        super(FFN, self).__init__(name=name, **kwargs)\n        self.dense1 = tf.keras.layers.Dense(units=hidden1, activation='relu')\n        self.dense2 = tf.keras.layers.Dense(units=hidden2, activation=None)\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n\n    def call(self, inputs, training=None):\n        dense1_outs = self.dense1(inputs)\n        dense2_outs = self.dense2(dense1_outs)\n        outs = dense2_outs + inputs\n        outs = self.layer_norm(outs, training=training)\n        return outs", "\n\nclass Conv1DLayerNorm(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, activation, drop_rate,\n                 padding='SAME', strides=1, name='Conv1D_with_dropout_IN'):\n        super(Conv1DLayerNorm, self).__init__(name=name)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.drop_rate = drop_rate\n        self.padding = padding\n        self.conv1d = tf.keras.layers.Conv1D(filters=filters,\n                                             kernel_size=kernel_size,\n                                             strides=strides,\n                                             padding=padding,\n                                             activation=None,\n                                             name='conv1d')\n        self.activation = activation if activation is not None else tf.identity\n        self.layer_norm = tf.keras.layers.LayerNormalization(name='layerNorm')\n        self.dropout = tf.keras.layers.Dropout(rate=drop_rate, name='dropout')\n\n    def call(self, inputs, training=None):\n        conv_outs = self.conv1d(inputs)\n        conv_outs = self.layer_norm(conv_outs, training=training)\n        conv_outs = self.activation(conv_outs)\n        dropouts = self.dropout(conv_outs, training=training)\n        return dropouts", "\n\nclass Conv1D(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, activation, drop_rate,\n                 bn_before_act=False, padding='SAME', strides=1,\n                 name='Conv1D_with_dropout_BN', **kwargs):\n        super(Conv1D, self).__init__(name=name, **kwargs)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.drop_rate = drop_rate\n        self.padding = padding\n        self.conv1d = tf.keras.layers.Conv1D(filters=filters,\n                                             kernel_size=kernel_size,\n                                             strides=strides,\n                                             padding=padding,\n                                             activation=None,\n                                             name='conv1d')\n        self.activation = activation if activation is not None else tf.identity\n        self.bn = tf.keras.layers.BatchNormalization(name='batch_norm')\n        self.dropout = tf.keras.layers.Dropout(rate=drop_rate, name='dropout')\n        self.bn_before_act = bn_before_act\n\n    def call(self, inputs, training=None):\n        conv_outs = self.conv1d(inputs)\n        if self.bn_before_act:\n            conv_outs = self.bn(conv_outs, training=training)\n            conv_outs = self.activation(conv_outs)\n        else:\n            conv_outs = self.activation(conv_outs)\n            conv_outs = self.bn(conv_outs, training=training)\n        dropouts = self.dropout(conv_outs, training=training)\n        return dropouts\n\n    def get_config(self):\n        config = super(Conv1D, self).get_config()\n        config.update({'filters': self.filters,\n                       'kernel_size': self.kernel_size,\n                       'padding': self.padding,\n                       'activation': self.activation,\n                       'dropout_rate': self.drop_rate,\n                       'bn_before_act': self.bn_before_act})\n        return config", "\n\nclass PostNet(tf.keras.layers.Layer):\n    def __init__(self, n_conv, conv_filters, conv_kernel,\n                 drop_rate, name='PostNet', **kwargs):\n        super(PostNet, self).__init__(name=name, **kwargs)\n        self.conv_stack = []\n        self.batch_norm_stack = []\n        activations = [tf.math.tanh] * (n_conv - 1) + [tf.identity]\n        for i in range(n_conv):\n            conv = Conv1D(filters=conv_filters, kernel_size=conv_kernel,\n                          padding='same', activation=activations[i],\n                          drop_rate=drop_rate, name='conv_{}'.format(i))\n            self.conv_stack.append(conv)\n\n    def call(self, inputs, training=None):\n        conv_out = inputs\n        for conv in self.conv_stack:\n            conv_out = conv(conv_out, training)\n        return conv_out", "\n\nclass PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, name='PositionalEncoding'):\n        super(PositionalEncoding, self).__init__(name=name)\n\n    @staticmethod\n    def positional_encoding(len, dim, step=1.):\n        \"\"\"\n        :param len: int scalar\n        :param dim: int scalar\n        :param step:\n        :return: position embedding\n        \"\"\"\n        pos_mat = tf.tile(\n            tf.expand_dims(\n                tf.range(0, tf.cast(len, dtype=tf.float32), dtype=tf.float32) * step,\n                axis=-1),\n            [1, dim])\n        dim_mat = tf.tile(\n            tf.expand_dims(\n                tf.range(0, tf.cast(dim, dtype=tf.float32), dtype=tf.float32),\n                axis=0),\n            [len, 1])\n        dim_mat_int = tf.cast(dim_mat, dtype=tf.int32)\n        pos_encoding = tf.where(  # [time, dims]\n            tf.math.equal(tf.math.mod(dim_mat_int, 2), 0),\n            x=tf.math.sin(pos_mat / tf.pow(10000., dim_mat / tf.cast(dim, tf.float32))),\n            y=tf.math.cos(pos_mat / tf.pow(10000., (dim_mat - 1) / tf.cast(dim, tf.float32))))\n        return pos_encoding", "\n\nclass EncBlk(tf.keras.layers.Layer):\n    \"\"\"\n    1D convolutional block for time-invariant feature extraction\n    \"\"\"\n\n    def __init__(self, out_channels, kernel_size, activation, pooling_kernel):\n        super(EncBlk, self).__init__()\n        self.conv1 = tf.keras.layers.Conv1D(out_channels, kernel_size=kernel_size, padding='same')\n        self.conv2 = tf.keras.layers.Conv1D(out_channels, kernel_size=kernel_size, padding='same')\n        self.conv_sc = tf.keras.layers.Conv1D(out_channels, kernel_size=1, padding='valid')\n        self.downsample = tf.keras.layers.AveragePooling1D(pool_size=pooling_kernel)\n        self.activation = activation\n\n    def call(self, x):\n        \"\"\"\n        :param x: [batch, channels, time-length]\n        :return:\n        \"\"\"\n        h = self.activation(x)\n        h = self.activation(self.conv1(h))\n        h = self.conv2(h)\n        h = self.downsample(h)\n        sc = self.downsample(self.conv_sc(x))\n        out = h + sc\n        return out", "\n\ndef get_activation(act_str):\n    return {'relu': tf.nn.relu, 'leaky_relu': tf.nn.leaky_relu, 'tanh': tf.math.tanh}[act_str]\n\n\n@tf.custom_gradient\ndef grad_reverse(x):\n    y = tf.identity(x)\n\n    def custom_grad(dy):\n        return -dy\n\n    return y, custom_grad", "\n\nclass GradReverse(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, x):\n        return grad_reverse(x)\n", ""]}
{"filename": "modules/attention.py", "chunked_list": ["import tensorflow as tf\nfrom typing import Tuple\nfrom .utils import FFN\n\n\nclass BaseAttention(tf.keras.layers.Layer):\n    def __init__(self, attention_dim, name='BaseAttention', **kwargs):\n        super(BaseAttention, self).__init__(name=name, **kwargs)\n        self.attention_dim = attention_dim\n\n    def call(self, inputs, memory, memory_lengths, query_lengths) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"\n        :param inputs: query, [batch, q_time, q_dim]\n        :param memory: [batch, m_time, m_dim]\n        :param memory_lengths: [batch,]\n        :param query_lengths: [batch,]\n        :return: (tensor1, tensor2)\n            tensor1: contexts, [batch, q_time, attention_dim]\n            tensor2: alignments, probabilities, [batch, q_time, m_time]\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def _get_key_mask(batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths):\n        memory_lengths = (memory_lengths if memory_lengths is not None\n                          else tf.ones([batch_size], dtype=tf.int32) * memory_max_time)\n        memeory_mask = tf.sequence_mask(memory_lengths, maxlen=memory_max_time)\n        memeory_mask = tf.tile(tf.expand_dims(memeory_mask, axis=1),  # [batch, 1, m_max_time]\n                               [1, query_max_time, 1])  # [batch, q_max_time, m_max_time]\n        query_lengths = (query_lengths if query_lengths is not None\n                         else tf.ones([batch_size], dtype=tf.int32) * query_max_time)\n        query_mask = tf.sequence_mask(query_lengths, maxlen=query_max_time)  # [batch, q_max_time]\n        query_mask = tf.tile(tf.expand_dims(query_mask, axis=2),  # [batch, q_max_time, 1]\n                             [1, 1, memory_max_time])  # [batch, q_max_time, m_max_time]\n        length_mask = tf.logical_and(memeory_mask, query_mask)\n        return length_mask\n\n    def get_config(self):\n        config = super(BaseAttention, self).get_config()\n        config.update({'attention_dim': self.attention_dim})\n        return config", "\n\nclass BahdanauAttention(BaseAttention):\n    def __init__(self, attention_dim, temperature=1.0, name='attention', **kwargs):\n        \"\"\"\n        :param attention_dim:\n        :param name:\n        :param kwargs:\n        \"\"\"\n        super(BahdanauAttention, self).__init__(attention_dim=attention_dim,\n                                                name=name, **kwargs)\n        self.query_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='query_layer')\n        self.memory_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='memory_layer')\n        self.score_v = tf.Variable(tf.random.normal([attention_dim, ]),\n                                   name='attention_v', trainable=True,\n                                   dtype=tf.float32)\n        self.score_b = tf.Variable(tf.zeros([attention_dim, ]),\n                                   name='attention_b', trainable=True,\n                                   dtype=tf.float32)\n        self.temperature = temperature\n\n    def _bahdanau_score(self, w_queries, w_keys):\n        \"\"\"\n        :param w_queries: [batch, q_max_time, attention_dim]\n        :param w_keys: [batch, k_max_time, attention_dim]\n        :return: [batch, q_max_time, k_max_time], attention score (energy)\n        \"\"\"\n        # [batch, q_max_time, 1, attention_dim]\n        expanded_queries = tf.expand_dims(w_queries, axis=2)\n        # [batch, 1, k_max_time, attention_dim]\n        expanded_keys = tf.expand_dims(w_keys, axis=1)\n        # [batch, q_max_time, k_max_time]\n        return tf.math.reduce_sum(\n            self.score_v * tf.nn.tanh(\n                # [batch, q_max_time, k_max_time, attention_dim]\n                expanded_keys + expanded_queries + self.score_b),\n            axis=3)\n\n    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n        # TODO: add query lengths and query mask\n        \"\"\"\n        :param inputs: query: [batch, q_max_time, query_depth]\n        :param memory: [batch, m_max_time, memory_depth]\n        :param memory_lengths: [batch, ]\n        :param query_lengths: [batch, ]\n        :return: contexts: [batch, q_max_time, attention_dim],\n                 alignments: [batch, q_max_time, m_max_time]\n        \"\"\"\n        processed_query = self.query_layer(inputs)\n        values = self.memory_layer(memory)\n        # energy: [batch, q_max_time, m_max_time]\n        energy = self._bahdanau_score(processed_query, values) / self.temperature\n\n        # apply mask\n        batch_size = tf.shape(memory)[0]\n        memory_max_time = tf.shape(memory)[1]\n        query_max_time = tf.shape(inputs)[1]\n        length_mask = self._get_key_mask(\n            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n        # [batch, q_max_time, m_max_time]\n        paddings = tf.ones_like(energy) * (-2. ** 32 + 1)\n        energy = tf.where(length_mask, energy, paddings)\n        # alignments shape = energy shape = [batch, q_max_time, m_max_time]\n        alignments = tf.math.softmax(energy, axis=2)\n\n        # compute context vector\n        # context: [batch, q_max_time, attention_dim]\n        contexts = tf.linalg.matmul(alignments, values)\n\n        return contexts, alignments", "\n\nclass ScaledDotProductAttention(BaseAttention):\n    def __init__(self, attention_dim, value_dim, temperature=1.0,\n                 name='ScaledDotProductAttention', **kwargs):\n        super(ScaledDotProductAttention, self).__init__(\n            attention_dim=attention_dim, name=name, **kwargs)\n        self.query_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='query_layer')\n        self.key_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='key_layer')\n        self.value_layer = tf.keras.layers.Dense(\n            units=value_dim, use_bias=False, name='value_layer')\n        self.temperature = temperature\n\n    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n        queries = self.query_layer(inputs)  # [batch, Tq, D]\n        keys = self.key_layer(memory)  # [batch, Tk, D]\n        values = self.key_layer(memory)  # [batch, Tk, Dv]\n        logits = tf.linalg.matmul(queries, keys, transpose_b=True)  # [batch, Tq, Tk]\n        logits = logits / tf.math.sqrt(tf.cast(self.attention_dim, tf.float32))  # scale\n        logits = logits / self.temperature  # temperature\n        # apply mask\n        batch_size = tf.shape(memory)[0]\n        memory_max_time = tf.shape(memory)[1]\n        query_max_time = tf.shape(inputs)[1]\n        length_mask = self._get_key_mask(\n            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n        paddings = tf.ones_like(logits) * (-2. ** 32 + 1)\n        logits = tf.where(length_mask, logits, paddings)\n        alignments = tf.math.softmax(logits, axis=2)\n        contexts = tf.linalg.matmul(alignments, values)\n        return contexts, alignments", "\n\nclass MultiHeadScaledProductAttention(BaseAttention):\n    def __init__(self, attention_dim, num_head, temperature=1.0, causality=None,\n                 attention_window=-1, name='attention', **kwargs):\n        assert attention_dim % num_head == 0\n        super(MultiHeadScaledProductAttention, self).__init__(\n            attention_dim=attention_dim, name=name, **kwargs)\n        self.causality = causality\n        self.attention_window = attention_window\n        self.query_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='query_layer')\n        self.key_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='key_layer')\n        self.value_layer = tf.keras.layers.Dense(\n            units=attention_dim, use_bias=False, name='value_layer')\n        self.num_head = num_head\n        self.temperature = temperature\n\n    def _split_head(self, inputs):\n        \"\"\"\n        :param inputs: [batch, time, dim]\n        :return: [batch, num_head, time, dim // head]\n        \"\"\"\n        batch = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        dim = tf.shape(inputs)[2]\n        reshaped = tf.reshape(inputs,\n                              [batch, max_time, self.num_head,\n                               dim // self.num_head])\n        # [batch, time, num_head, dim // head]\n        transposed = tf.transpose(reshaped, [0, 2, 1, 3])\n        # [batch, num_head, time, dim // head]\n        return transposed\n\n    def _merge_head(self, inputs):\n        \"\"\"\n        :param inputs: [batch, num_head, time, dim]\n        :return: [batch, time, attention_dim]\n        \"\"\"\n        batch = tf.shape(inputs)[0]\n        time = tf.shape(inputs)[2]\n        head_dim = tf.shape(inputs)[3]\n        transposed = tf.transpose(inputs, [0, 2, 1, 3])\n        # [batch, time, num_head, dim]\n        reshaped = tf.reshape(transposed, [batch, time, self.num_head * head_dim])\n        return reshaped\n\n    def _get_key_mask(self, batch_size, memory_max_time, query_max_time,\n                      memory_lengths, query_lengths):\n        memory_lengths = (memory_lengths if memory_lengths is not None\n                          else tf.ones([batch_size], dtype=tf.int32) * memory_max_time)\n        memory_mask = tf.sequence_mask(memory_lengths, maxlen=memory_max_time,\n                                       name='length_mask')  # [batch, m_max_time]\n        memory_mask = tf.tile(tf.expand_dims(memory_mask, axis=1),  # [batch, 1, m_max_time]\n                              [1, query_max_time, 1])  # [batch, q_max_time, m_max_time]\n        query_lengths = (query_lengths if query_lengths is not None\n                         else tf.ones([batch_size], dtype=tf.int32) * query_max_time)\n        query_mask = tf.sequence_mask(query_lengths, maxlen=query_max_time)  # [batch, q_max_time]\n        query_mask = tf.tile(tf.expand_dims(query_mask, axis=2),  # [batch, q_max_time, 1]\n                             [1, 1, memory_max_time])  # [batch, q_max_time, m_max_time]\n        length_mask = tf.logical_and(memory_mask, query_mask)\n        length_mask = tf.tile(tf.expand_dims(length_mask, axis=1),\n                              [1, self.num_head, 1, 1])\n        # [batch, num_head, q_max_time, m_max_time]\n        return length_mask\n\n    def get_window_mask(self, logits):\n        win_mask = tf.ones(tf.shape(logits), dtype=tf.bool)\n        if self.causality:\n            win_mask = tf.linalg.band_part(win_mask, self.attention_window, 0, name='causal_mask')\n        else:\n            win_mask = tf.linalg.band_part(\n                win_mask, self.attention_window, self.attention_window, name='causal_mask')\n        return win_mask\n\n    def call(self, inputs, memory, memory_lengths=None, query_lengths=None):\n        queries = self.query_layer(inputs)  # [batch, Tq, D]\n        keys = self.key_layer(memory)  # [batch, Tk, D]\n        values = self.value_layer(memory)  # [batch, Tk, Dv]\n        headed_queries = self._split_head(queries)  # [batch, num_head, Tq, head_dim]\n        headed_keys = self._split_head(keys)  # [batch, num_head, Tk, head_dim]\n        headed_values = self._split_head(values)  # [batch, num_head, Tk, head_dim]\n        logits = tf.linalg.matmul(headed_queries,\n                                  headed_keys,\n                                  transpose_b=True)  # [batch, num_head, Tq, Tk]\n        logits = logits / tf.sqrt(\n            tf.cast(self.attention_dim // self.num_head, dtype=tf.float32))  # scale\n        logits = logits / self.temperature  # temperature\n        # apply mask\n        batch_size = tf.shape(memory)[0]\n        memory_max_time = tf.shape(memory)[1]\n        query_max_time = tf.shape(inputs)[1]\n        length_mask = self._get_key_mask(\n            batch_size, memory_max_time, query_max_time, memory_lengths, query_lengths)\n        window_mask = self.get_window_mask(logits)\n        length_mask = tf.math.logical_and(length_mask, window_mask)\n        # [batch, num_head, q_max_time, m_max_time]\n        paddings = tf.ones_like(logits, dtype=tf.float32) * (-2. ** 32 + 1)\n        logits = tf.where(length_mask, logits, paddings)\n        alignments = tf.math.softmax(logits, axis=3)  # [batch, num_head, Tq, Tk]\n        contexts = tf.linalg.matmul(alignments, headed_values)\n        # [batch, num_head, Tq, head_dim]\n        contexts = self._merge_head(contexts)  # [batch, Tq, attention_dim]\n        return contexts, alignments", "\n\nclass SelfAttentionBLK(tf.keras.layers.Layer):\n    def __init__(self, input_dim, attention_dim, attention_heads, attention_temperature, ffn_hidden,\n                 causality=None, attention_window=-1, name='self_attention_blk', **kwargs):\n        super(SelfAttentionBLK, self).__init__(name=name, **kwargs)\n        self.input_dim = input_dim\n        self.attention_dim = attention_dim\n        self.attention = MultiHeadScaledProductAttention(attention_dim=attention_dim,\n                                                         num_head=attention_heads,\n                                                         temperature=attention_temperature,\n                                                         causality=causality,\n                                                         attention_window=attention_window)\n        self.att_proj = tf.keras.layers.Dense(units=input_dim)\n        self.layer_norm = tf.keras.layers.LayerNormalization()\n        self.ffn = FFN(hidden1=ffn_hidden, hidden2=input_dim)\n\n    def call(self, inputs, memory, query_lengths, memory_lengths, training=None):\n        att_outs, alignments = self.attention(inputs=inputs, memory=memory,\n                                              query_lengths=query_lengths,\n                                              memory_lengths=memory_lengths)\n        contexts = tf.concat([inputs, att_outs], axis=-1)\n        contexts.set_shape([None, None, self.attention_dim + self.input_dim])\n        att_outs = self.att_proj(contexts)\n        att_outs = self.layer_norm(inputs + att_outs, training=training)\n        ffn_outs = self.ffn(att_outs, training=training)\n        return ffn_outs, alignments", "\n\nclass CrossAttentionBLK(tf.keras.layers.Layer):\n    def __init__(self, input_dim, attention_dim, attention_heads, attention_temperature,\n                 ffn_hidden, causality=None, attention_window=-1, name='cross_attention_blk', **kwargs):\n        super(CrossAttentionBLK, self).__init__(name=name, **kwargs)\n        self.input_dim = input_dim\n        self.attention_dim = attention_dim\n        self.self_attention = MultiHeadScaledProductAttention(\n            attention_dim=attention_dim, num_head=attention_heads,\n            temperature=attention_temperature, name='self_attention',\n            causality=causality, attention_window=attention_window)\n        self.att_proj1 = tf.keras.layers.Dense(units=input_dim)\n        self.layer_norm1 = tf.keras.layers.LayerNormalization(name='layerNorm1')\n        self.cross_attention = MultiHeadScaledProductAttention(\n            attention_dim=attention_dim, num_head=attention_heads,\n            temperature=attention_temperature, causality=False,\n            attention_window=-1, name='cross_attention')\n        self.att_proj2 = tf.keras.layers.Dense(units=attention_dim)\n        self.layer_norm2 = tf.keras.layers.LayerNormalization(name='layerNorm2')\n        self.ffn = FFN(hidden1=ffn_hidden, hidden2=attention_dim)\n\n    def call(self, inputs, memory, query_lengths, memory_lengths, training=None):\n        self_att_outs, self_ali = self.self_attention(\n            inputs=inputs, memory=inputs, query_lengths=query_lengths,\n            memory_lengths=query_lengths)\n        contexts = tf.concat([inputs, self_att_outs], axis=-1)\n        contexts.set_shape([None, None, self.attention_dim + self.input_dim])\n        self_att_outs = self.att_proj1(contexts)\n        self_att_outs = self.layer_norm1(self_att_outs + inputs, training=training)\n        att_outs, cross_ali = self.cross_attention(\n            inputs=self_att_outs, memory=memory, query_lengths=query_lengths,\n            memory_lengths=memory_lengths)\n        contexts = tf.concat([self_att_outs, att_outs], axis=-1)\n        contexts.set_shape([None, None, self.attention_dim * 2])\n        att_outs = self.att_proj2(contexts)\n        att_outs = self.layer_norm2(att_outs + self_att_outs, training=training)\n        ffn_outs = self.ffn(att_outs, training=training)\n        return ffn_outs, cross_ali", ""]}
{"filename": "datasets/VCTK.py", "chunked_list": ["import os\nimport json\nimport random\n\n\nclass VCTK:\n    \"\"\"\n    Generate the json file obtained by dumping the dictionary\n    {'train': {fid: wav_path, ...}, 'validation': {fid: wav_path, ...}, 'test': {fid: wav_path, ...}}\n    \"\"\"\n    def __init__(self, data_dir, out_dir, val_spks=None, test_spks=None):\n        self.data_dir = data_dir\n        self.out_dir = out_dir\n        self.summary_file = os.path.join(out_dir, 'vctk-summary.json')\n        self.wav_ext = 'mic2.flac'\n        self.dataset_summary = {}\n        assert ((val_spks is None and test_spks is None) or\n                (val_spks is not None and test_spks is not None),\n                \"Please specify both val and test speakers!\")\n        self.val_spks = val_spks\n        self.test_spks = test_spks\n        self.validate_dir()\n\n    def validate_dir(self):\n        if not os.path.isdir(self.data_dir):\n            raise NotADirectoryError('{} is not a valid directory!'.format(self.data_dir))\n        if not os.path.isdir(self.out_dir):\n            os.makedirs(self.out_dir)\n        return\n\n    @staticmethod\n    def extract_spk_fid(filename):\n        \"\"\"\n        :param filename:\n        :return: spk_name, fid\n        \"\"\"\n        basename = filename.split('/')[-1].split('.')[0]\n        spk = basename.split('_')[0]\n        uid = basename.split('_')[1]\n        fid = '{}_{}'.format(spk, uid)\n        return spk, fid\n\n    def write_dataset_info(self):\n        with open(self.summary_file, 'w') as f:\n            json.dump(self.dataset_summary, f, sort_keys=True, indent=4)\n        return\n\n    def write_summary(self):\n        dataset_summary = {}\n        for root, dirs, files in os.walk(self.data_dir, followlinks=True):\n            for basename in files:\n                if basename.endswith(self.wav_ext):\n                    filename = os.path.join(root, basename)\n                    spk, fid = self.extract_spk_fid(filename)\n                    wav_path = os.path.join(root, basename)\n                    if spk not in dataset_summary.keys():\n                        dataset_summary[spk] = {}\n                        dataset_summary[spk][fid] = wav_path\n                    else:\n                        dataset_summary[spk][fid] = wav_path\n        if self.val_spks is None and self.test_spks is None:\n            all_spks = list(dataset_summary.keys())\n            random.shuffle(all_spks)\n            self.test_spks = all_spks[-10:]\n            self.val_spks = all_spks[-20: -10]\n        train_summary = {}\n        val_summary = {}\n        test_summary = {}\n        for spk in dataset_summary.keys():\n            if spk in self.val_spks:\n                val_summary.update(dataset_summary[spk])\n            elif spk in self.test_spks:\n                test_summary.update(dataset_summary[spk])\n            else:\n                train_summary.update(dataset_summary[spk])\n\n        self.dataset_summary = {'train': train_summary,\n                                'validation': val_summary,\n                                'test': test_summary}\n        self.write_dataset_info()\n        return", ""]}
{"filename": "datasets/AiShell3.py", "chunked_list": ["import os\nimport json\nimport random\n\n\nclass AiShell3:\n    \"\"\"\n    Generate the json file obtained by dumping the dictionary\n    {'train': {fid: wav_path, ...}, 'validation': {fid: wav_path, ...}, 'test': {fid: wav_path, ...}}\n    \"\"\"\n    def __init__(self, data_dir, out_dir, train_spk_file=None, val_test_spk_file=None):\n        self.data_dir = data_dir\n        self.out_dir = out_dir\n        self.train_spks = self.read_train_spks(train_spk_file)\n        val_test_spks = self.read_val_test_spks(val_test_spk_file)\n        random.shuffle(val_test_spks)\n        self.val_spks = val_test_spks[:len(val_test_spks) // 2]\n        self.test_spks = val_test_spks[len(val_test_spks) // 2:]\n        self.summary_file = os.path.join(out_dir, 'aishell3-summary.json')\n        self.wav_ext = '.wav'\n        self.dataset_summary = {}\n        self.validate_dir()\n\n    def read_train_spks(self, f):\n        train_spks = []\n        if f is not None:\n            with open(f, 'r', encoding='utf-8') as f:\n                for line in f:\n                    train_spks.append(line.strip())\n        else:\n            train_spks = os.listdir(os.path.join(self.data_dir, 'train/wav'))\n        assert len(train_spks) > 0\n        return train_spks\n\n    def read_val_test_spks(self, f):\n        vt_spks = []\n        if f is not None:\n            with open(f, 'r', encoding='utf-8') as f:\n                for line in f:\n                    vt_spks.append(line.strip())\n        else:\n            vt_spks = [spk for spk in os.listdir(os.path.join(self.data_dir, 'test/wav'))\n                       if spk not in self.train_spks]\n        assert len(vt_spks) > 0\n        return vt_spks\n\n    def validate_dir(self):\n        if not os.path.isdir(self.data_dir):\n            raise NotADirectoryError('{} is not a valid directory!'.format(self.data_dir))\n        if not os.path.isdir(self.out_dir):\n            os.makedirs(self.out_dir)\n        return\n\n    @staticmethod\n    def extract_spk_fid(filename):\n        \"\"\"\n        :param filename:\n        :return: spk_name, fid\n        \"\"\"\n        fid = filename.split('/')[-1].split('.')[0]\n        spk = filename.split('/')[-2]\n        return spk, fid\n\n    def write_dataset_info(self):\n        with open(self.summary_file, 'w') as f:\n            json.dump(self.dataset_summary, f, sort_keys=True, indent=4)\n        return\n\n    def write_summary(self):\n        train_summary = {}\n        val_summary = {}\n        test_summary = {}\n        for root, dirs, files in os.walk(self.data_dir, followlinks=True):\n            for basename in files:\n                if basename.endswith(self.wav_ext):\n                    wav_path = os.path.join(root, basename)\n                    spk, fid = self.extract_spk_fid(wav_path)\n                    if spk in self.train_spks:\n                        train_summary[fid] = wav_path\n                    elif spk in self.val_spks:\n                        val_summary[fid] = wav_path\n                    elif spk in self.test_spks:\n                        test_summary[fid] = wav_path\n                    else:\n                        continue\n        self.dataset_summary = {'train': train_summary,\n                                'validation': val_summary,\n                                'test': test_summary}\n        self.write_dataset_info()\n        return", ""]}
{"filename": "datasets/tfrecord_maker.py", "chunked_list": ["import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\n\n\nclass TFRecordWriter:\n    def __init__(self, train_split=None, data_dir=None, save_dir=None, chunk_size=None):\n        self.train_split = train_split\n        self.data_dir = data_dir\n        self.save_dir = save_dir\n        self.chunk_size = chunk_size\n        self.train_ids_file = os.path.join(self.data_dir, 'train.txt') if data_dir is not None else None\n        self.val_ids_file = os.path.join(self.data_dir, 'val.txt') if data_dir is not None else None\n        self.test_ids_file = os.path.join(self.data_dir, 'test.txt') if data_dir is not None else None\n\n    @staticmethod\n    def _bytes_feature(value):\n        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n        if isinstance(value, type(tf.constant(0))):\n            value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n    @staticmethod\n    def _float_feature(value):\n        \"\"\"Returns a float_list from a float / double.\"\"\"\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n    @staticmethod\n    def _int64_feature(value):\n        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n    @staticmethod\n    def serialize_example(fid, mel, mel_len):\n        \"\"\"\n        :param fid: string\n        :param mel: np array, [mel_len, num_mels]\n        :param mel_len: int32\n        :return: byte string\n        \"\"\"\n        feature = {\n            'fid': TFRecordWriter._bytes_feature(fid.encode('utf-8')),\n            'mel': TFRecordWriter._bytes_feature(tf.io.serialize_tensor(mel)),\n            'mel_len': TFRecordWriter._int64_feature(mel_len)}\n        # Create a Features message using tf.train.Example.\n        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n        return example_proto.SerializeToString()\n\n    def _parse_fids(self, mode='train'):\n        fids_f = {'train': self.train_ids_file,\n                  'val': self.val_ids_file,\n                  'test': self.test_ids_file}[mode]\n        fids = []\n        with open(fids_f, 'r', encoding='utf-8') as f:\n            for line in f:\n                fids.append(line.strip())\n        return fids\n\n    def _get_features(self, fid):\n        mel = np.load(os.path.join(self.data_dir, 'mels', '{}.npy'.format(fid))).astype(np.float64)\n        mel_len = mel.shape[0]\n        return mel, mel_len\n\n    def mel_exist(self, fid):\n        mel_npy = os.path.join(self.data_dir, 'mels', '{}.npy'.format(fid))\n        return os.path.isfile(mel_npy)\n\n    def write(self, mode='train'):\n        fids = self._parse_fids(mode)\n        if mode == 'train':\n            splited_fids = [fids[i::self.train_split] for i in range(self.train_split)]\n        else:\n            splited_fids = [fids]\n        for i, ids in enumerate(splited_fids):\n            tfrecord_path = os.path.join(self.save_dir, '{}-{}.tfrecords'.format(mode, i))\n            with tf.io.TFRecordWriter(tfrecord_path) as writer:\n                for fid in tqdm(ids):\n                    if not self.mel_exist(fid):\n                        continue\n                    mel, mel_len = self._get_features(fid)\n                    serialized_example = self.serialize_example(fid, mel, mel_len)\n                    writer.write(serialized_example)\n        return\n\n    def write_all(self):\n        self.write('train')\n        self.write('val')\n        self.write('test')\n        return\n\n    def pad2chunk(self, inputs):\n        inp_exp = tf.tile(inputs, tf.constant([14, 1]))\n        inp_exp = inp_exp[:self.chunk_size, :]\n        return inp_exp\n\n    def parse_example(self, serialized_example):\n        feature_description = {\n            'fid': tf.io.FixedLenFeature((), tf.string),\n            'mel': tf.io.FixedLenFeature((), tf.string),\n            'mel_len': tf.io.FixedLenFeature((), tf.int64)}\n        example = tf.io.parse_single_example(serialized_example, feature_description)\n\n        fid = example['fid']\n        mel = tf.io.parse_tensor(example['mel'], out_type=tf.float64)\n        mel_ext = self.pad2chunk(mel)\n        mel_len = example['mel_len']\n        return (fid,\n                tf.cast(mel, tf.float32),\n                tf.cast(mel_len, tf.int32),\n                tf.cast(mel_ext, tf.float32))\n\n    def create_dataset(self, buffer_size, num_parallel_reads,\n                       batch_size, num_mels, shuffle_buffer, shuffle,\n                       tfrecord_files, seed=1, drop_remainder=False):\n        tfrecord_dataset = tf.data.TFRecordDataset(\n            tfrecord_files, buffer_size=buffer_size,\n            num_parallel_reads=num_parallel_reads)\n        tfdataset = tfrecord_dataset.map(\n            self.parse_example,\n            num_parallel_calls=num_parallel_reads)\n        tfdataset = tfdataset.padded_batch(\n            batch_size=batch_size,\n            drop_remainder=drop_remainder,\n            padded_shapes=([], [None, num_mels], [], [None, num_mels]))\n        tfdataset = (tfdataset.shuffle(buffer_size=shuffle_buffer, seed=seed)\n                     if shuffle else tfdataset)\n        tfdataset = tfdataset.prefetch(tf.data.experimental.AUTOTUNE)\n        return tfdataset\n\n    def get_tfrecords_list(self, mode='train'):\n        assert self.save_dir is not None\n        assert mode in ['train', 'val', 'test']\n        return [os.path.join(self.save_dir, f)\n                for f in os.listdir(self.save_dir) if f.startswith(mode)]", ""]}
{"filename": "datasets/preprocessor.py", "chunked_list": ["import os\nimport json\nimport joblib\nimport random\nimport numpy as np\nfrom audio import Audio\n\n\nclass MelPreprocessor:\n    \"\"\"\n    Extract mel-spectrograms given the multiple data summary json file\n    \"\"\"\n    def __init__(self, data_summary_paths, save_dir, hps):\n        \"\"\"\n        :param data_summary_paths: list of data summary json files containing paths of the waveform\n        :param save_dir: directory to save the feature\n        :param hps: hyper-parameters\n        \"\"\"\n        self.save_dir = save_dir\n        self.data_summary_paths = data_summary_paths\n        self.data_summary = self.load_dataset_info()\n        self.hps = hps\n        self.mel_dir = os.path.join(self.save_dir, 'mels')\n        self.train_list_f = os.path.join(self.save_dir, 'train.txt')\n        self.val_list_f = os.path.join(self.save_dir, 'val.txt')\n        self.test_list_f = os.path.join(self.save_dir, 'test.txt')\n        self.num_mels = hps.Audio.num_mels\n        self.audio_processor = Audio(hps.Audio)\n        self.n_jobs = hps.Dataset.preprocess_n_jobs\n        self.train_set_size = None\n        self.dev_set_size = None\n        self.test_set_size = None\n\n    def feature_extraction(self):\n        self._validate_dir()\n        print('Process text file...')\n        print('Split the data set into train, dev and test set...')\n        self.train_set_size, self.dev_set_size, self.test_set_size = self.write_splits()\n        print('Extracting Mel-Spectrograms...')\n        self.extract_mels()\n        return\n\n    def load_dataset_info(self):\n        train_summary = {}\n        val_summary = {}\n        test_summary = {}\n        for summary_f in self.data_summary_paths:\n            if not os.path.isfile(summary_f):\n                raise FileNotFoundError(\n                    '{} not exists! Please generate it first!'.format(summary_f))\n            with open(summary_f, 'r') as f:\n                summary = json.load(f)\n                train_summary.update(summary['train'])\n                val_summary.update(summary['validation'])\n                test_summary.update(summary['test'])\n        dataset_summary = {'train': train_summary, 'validation': val_summary, 'test': test_summary}\n        return dataset_summary\n\n    def _validate_dir(self):\n        if not os.path.isdir(self.save_dir):\n            os.makedirs(self.save_dir)\n        if not os.path.isdir(self.mel_dir):\n            os.makedirs(self.mel_dir)\n        return\n\n    def write_splits(self):\n        train_set = [fid for fid in self.data_summary['train'].keys()]\n        val_set = [fid for fid in self.data_summary['validation'].keys()]\n        test_set = [fid for fid in self.data_summary['test'].keys()]\n        random.shuffle(train_set)\n        random.shuffle(val_set)\n        random.shuffle(test_set)\n        with open(self.train_list_f, 'w') as f:\n            for idx in train_set:\n                f.write(\"{}\\n\".format(idx))\n        with open(self.val_list_f, 'w') as f:\n            for idx in val_set:\n                f.write(\"{}\\n\".format(idx))\n        with open(self.test_list_f, 'w') as f:\n            for idx in test_set:\n                f.write(\"{}\\n\".format(idx))\n        return len(train_set), len(val_set), len(test_set)\n\n    def single_mel_lf0_extraction(self, wav_f, fid):\n        mel_name = os.path.join(self.mel_dir, '{}.npy'.format(fid))\n        if os.path.isfile(mel_name):\n            return\n        else:\n            wav_arr = self.audio_processor.load_wav(wav_f)\n            wav_arr = self.audio_processor.trim_silence_by_trial(wav_arr, top_db=15., lower_db=25.)\n            wav_arr = wav_arr / max(0.01, np.max(np.abs(wav_arr)))\n            wav_arr = self.audio_processor.preemphasize(wav_arr)\n            mel = self.audio_processor.melspectrogram(wav_arr)\n            np.save(mel_name, mel.T)\n            return\n\n    def extract_mels(self):\n        wav_list = []\n        for split in self.data_summary.keys():\n            for fid in self.data_summary[split].keys():\n                wav_list.append((self.data_summary[split][fid], fid))\n        jobs = [joblib.delayed(self.single_mel_lf0_extraction)(wav_f, fid)\n                for wav_f, fid in wav_list]\n        _ = joblib.Parallel(n_jobs=self.n_jobs, verbose=True)(jobs)\n        return", "class MelPreprocessor:\n    \"\"\"\n    Extract mel-spectrograms given the multiple data summary json file\n    \"\"\"\n    def __init__(self, data_summary_paths, save_dir, hps):\n        \"\"\"\n        :param data_summary_paths: list of data summary json files containing paths of the waveform\n        :param save_dir: directory to save the feature\n        :param hps: hyper-parameters\n        \"\"\"\n        self.save_dir = save_dir\n        self.data_summary_paths = data_summary_paths\n        self.data_summary = self.load_dataset_info()\n        self.hps = hps\n        self.mel_dir = os.path.join(self.save_dir, 'mels')\n        self.train_list_f = os.path.join(self.save_dir, 'train.txt')\n        self.val_list_f = os.path.join(self.save_dir, 'val.txt')\n        self.test_list_f = os.path.join(self.save_dir, 'test.txt')\n        self.num_mels = hps.Audio.num_mels\n        self.audio_processor = Audio(hps.Audio)\n        self.n_jobs = hps.Dataset.preprocess_n_jobs\n        self.train_set_size = None\n        self.dev_set_size = None\n        self.test_set_size = None\n\n    def feature_extraction(self):\n        self._validate_dir()\n        print('Process text file...')\n        print('Split the data set into train, dev and test set...')\n        self.train_set_size, self.dev_set_size, self.test_set_size = self.write_splits()\n        print('Extracting Mel-Spectrograms...')\n        self.extract_mels()\n        return\n\n    def load_dataset_info(self):\n        train_summary = {}\n        val_summary = {}\n        test_summary = {}\n        for summary_f in self.data_summary_paths:\n            if not os.path.isfile(summary_f):\n                raise FileNotFoundError(\n                    '{} not exists! Please generate it first!'.format(summary_f))\n            with open(summary_f, 'r') as f:\n                summary = json.load(f)\n                train_summary.update(summary['train'])\n                val_summary.update(summary['validation'])\n                test_summary.update(summary['test'])\n        dataset_summary = {'train': train_summary, 'validation': val_summary, 'test': test_summary}\n        return dataset_summary\n\n    def _validate_dir(self):\n        if not os.path.isdir(self.save_dir):\n            os.makedirs(self.save_dir)\n        if not os.path.isdir(self.mel_dir):\n            os.makedirs(self.mel_dir)\n        return\n\n    def write_splits(self):\n        train_set = [fid for fid in self.data_summary['train'].keys()]\n        val_set = [fid for fid in self.data_summary['validation'].keys()]\n        test_set = [fid for fid in self.data_summary['test'].keys()]\n        random.shuffle(train_set)\n        random.shuffle(val_set)\n        random.shuffle(test_set)\n        with open(self.train_list_f, 'w') as f:\n            for idx in train_set:\n                f.write(\"{}\\n\".format(idx))\n        with open(self.val_list_f, 'w') as f:\n            for idx in val_set:\n                f.write(\"{}\\n\".format(idx))\n        with open(self.test_list_f, 'w') as f:\n            for idx in test_set:\n                f.write(\"{}\\n\".format(idx))\n        return len(train_set), len(val_set), len(test_set)\n\n    def single_mel_lf0_extraction(self, wav_f, fid):\n        mel_name = os.path.join(self.mel_dir, '{}.npy'.format(fid))\n        if os.path.isfile(mel_name):\n            return\n        else:\n            wav_arr = self.audio_processor.load_wav(wav_f)\n            wav_arr = self.audio_processor.trim_silence_by_trial(wav_arr, top_db=15., lower_db=25.)\n            wav_arr = wav_arr / max(0.01, np.max(np.abs(wav_arr)))\n            wav_arr = self.audio_processor.preemphasize(wav_arr)\n            mel = self.audio_processor.melspectrogram(wav_arr)\n            np.save(mel_name, mel.T)\n            return\n\n    def extract_mels(self):\n        wav_list = []\n        for split in self.data_summary.keys():\n            for fid in self.data_summary[split].keys():\n                wav_list.append((self.data_summary[split][fid], fid))\n        jobs = [joblib.delayed(self.single_mel_lf0_extraction)(wav_f, fid)\n                for wav_f, fid in wav_list]\n        _ = joblib.Parallel(n_jobs=self.n_jobs, verbose=True)(jobs)\n        return", ""]}
{"filename": "datasets/__init__.py", "chunked_list": ["from .VCTK import VCTK\nfrom .AiShell3 import AiShell3\nfrom .preprocessor import MelPreprocessor\nfrom .tfrecord_maker import TFRecordWriter\n"]}
