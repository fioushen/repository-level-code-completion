{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python3\n\nfrom setuptools import setup\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nsetup(\n    name=\"DecAF\",\n    version=\"1.0.0\",", "    name=\"DecAF\",\n    version=\"1.0.0\",\n    description=\"Joint Decoding Answer and Logical Form for Knowledge Base Question Answering through Retrieval\",\n    classifiers=[\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    long_description=readme,", "    ],\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    setup_requires=[\n        \"setuptools>=18.0\",\n    ],\n)"]}
{"filename": "DecAF/Datasets/QA/evaluate.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#", "# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.", "# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport json\nimport argparse\nfrom DecAF.Knowledge.linearize import (\n    load_nameid_dict,\n    get_raw_name,\n)", "    get_raw_name,\n)\nfrom DecAF.Datasets.QA.utils import (\n    Prefix_to_id_all, \n    answer_ensemble, \n    id_to_name,\n    to_webqsp_format,\n    to_grailqa_format,\n    to_cwq_format,\n    compute_hits1_dict,", "    to_cwq_format,\n    compute_hits1_dict,\n)\nimport logging\nlogging.basicConfig(level=logging.INFO, format='%(message)s')\n\nDATA_DIR = os.environ['DATA_DIR']\nSAVE_DIR = os.environ['SAVE_DIR']\n\nparser = argparse.ArgumentParser(description='dataset evaluation')", "\nparser = argparse.ArgumentParser(description='dataset evaluation')\nparser.add_argument('--dataset', type=str, help=\"WebQSP, GrailQA, CWQ, FreebaseQA\")\nparser.add_argument('--result_path', type=str, help=\"result directory\")\nargs = parser.parse_args()\n\nresult_path = args.result_path\nif not os.path.exists(result_path):\n    exit(f\"{result_path} not found\")\n", "\n# the evaluation for FreebaseQA is different from the other three datasets\n# since it does not provide the labeled logical forms\n# we only evaluate the predicted answers under the QA setting\nif args.dataset == \"FreebaseQA\":\n    with open(result_path, \"r\") as rf:\n        results = json.load(rf)\n    gold_answers, pred_answers = {}, {}\n    for key in results:\n        gold_answers[key] = results[key][\"gold answers\"]\n        gold_answers[key] = [get_raw_name(each) for each in gold_answers[key]]\n        pred_answers[key] = results[key][\"predicted answers\"]\n        pred_answers[key] = [get_raw_name(each) for each in pred_answers[key]]\n    logging.info(\"Hits@1: {}\".format(compute_hits1_dict(pred_answers, gold_answers)))\n    exit()", "\nname_dir = DATA_DIR + \"/knowledge_source/Freebase/id2name_parts_disamb\"\nname2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\nname_dir = DATA_DIR + \"/knowledge_source/Freebase/id2name_parts\"\nname2id_dict_orig, id2name_dict_orig = load_nameid_dict(name_dir, lower=False)\n\nlogging.info(\"parse generation results to answers\")\nsave_file_path_all = Prefix_to_id_all(result_path, name2id_dict)\n", "save_file_path_all = Prefix_to_id_all(result_path, name2id_dict)\n\nlogging.info(\"Combine answers from SP and QA\")\nsave_file_path_id_final = answer_ensemble(save_file_path_all)\n\nif args.dataset == \"WebQSP\":\n    logging.info(\"Evaluation Results on WebQSP:\")\n    save_file_path_id_offical = to_webqsp_format(save_file_path_id_final)\n    eval_code = os.path.join(DATA_DIR, \"tasks/QA/WebQSP/raw/eval/eval.py\")\n    gold_file = os.path.join(DATA_DIR, \"tasks/QA/WebQSP/raw/data/WebQSP.test.json\")\n    os.system(f\"python2 {eval_code} {gold_file} {save_file_path_id_offical}\")\nelif args.dataset == \"GrailQA\":\n    logging.info(\"Evaluation Results on GrailQA:\")\n    save_file_path_id_offical = to_grailqa_format(save_file_path_id_final)\n    gold_file = os.path.join(DATA_DIR, f\"tasks/QA/GrailQA/raw/grailqa_v1.0_dev.json\")\n    os.system(f\"python GrailQA/evaluate.py {gold_file} {save_file_path_id_offical} --fb_roles SP_tools/ontology/fb_roles --fb_types SP_tools/ontology/fb_types --reverse_properties SP_tools/ontology/reverse_properties\")\nelif args.dataset == \"CWQ\":\n    logging.info(\"Evaluation Results on CWQ:\")\n    save_file_path_name_final = id_to_name(save_file_path_id_final, id2name_dict_orig)\n    save_file_path_name_official = to_cwq_format(save_file_path_name_final)\n    gold_file = os.path.join(DATA_DIR, f\"tasks/QA/CWQ/raw/ComplexWebQuestions_test.json\")\n    os.system(f\"python CWQ/eval_script.py {gold_file} {save_file_path_name_official}\")", ""]}
{"filename": "DecAF/Datasets/QA/utils.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#", "# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.", "# specific language governing permissions and limitations\n# under the License.\n\n\nimport random\nimport json\nimport itertools\nfrom collections import defaultdict\nfrom DecAF.Knowledge.linearize import get_raw_name\nfrom collections import OrderedDict", "from DecAF.Knowledge.linearize import get_raw_name\nfrom collections import OrderedDict\nfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import get_label, execute_query\nfrom DecAF.Datasets.QA.SP_tools.executor.logic_form_util import lisp_to_sparql\nimport re\nfrom tqdm import tqdm\nimport time as time\nimport signal\n\n#Close session\ndef handler(signum, frame):\n    print(\"SPARQL Executation Time out!\")\n    raise Exception('Action took too much time')", "\n#Close session\ndef handler(signum, frame):\n    print(\"SPARQL Executation Time out!\")\n    raise Exception('Action took too much time')\n\n\ndef denorm_final(expr, entity_label_map):\n\n    expr = expr.replace('^^http', 'http')\n    expr = expr.replace('http', '^^http')\n    expr = expr.replace('( ', '(').replace(' )', ')')\n            \n    entities = re.findall(r'\\[ (.*?) \\]', expr)\n    expr_list = [expr]\n    for e in entities:\n        # delete the beginning and end space of e\n        orig_e = f\"[ {e} ]\"\n\n        new_expr_list = []\n        name_e = e\n        if name_e in entity_label_map:\n            for expr_i in expr_list:\n                for id_map in entity_label_map[name_e]:\n                    new_expr_list.append(expr_i.replace(orig_e, id_map))\n        \n        expr_list = new_expr_list\n    expr_list = expr_list[:10]\n    return expr_list", "\n\ndef revise_only_name(expr, entity_label_map):\n    expr = expr.replace('(', ' ( ')\n    expr = expr.replace(')', ' ) ')\n    toks = expr.split(' ')\n    toks = [x for x in toks if len(x)]\n    norm_toks = []\n    for t in toks:\n        # normalize entity\n        if t.startswith('m.') or t.startswith('g.'):\n            if t in entity_label_map:\n                t = entity_label_map[t]\n            else:\n                name = get_label(t)\n                if name is not None:\n                    entity_label_map[t] = name\n                    t = name\n            t = \"[ \" + t + \" ]\"\n        # normalize type\n        norm_toks.append(t)\n    return ' '.join(norm_toks)", "\n\ndef is_valid(answer):\n    if answer is not None and answer != '':\n        return True\n    else:\n        return False\n\ndef parse_answer(raw_ans_list, original_name=False):\n    answers = [ans['text'] if ans['text'] is not None else ans['freebaseId'] for ans in raw_ans_list]\n    if not answers:\n        answers = [\"None\"]\n    if original_name:\n        answers = [get_raw_name(ans) for ans in answers]\n    return answers", "def parse_answer(raw_ans_list, original_name=False):\n    answers = [ans['text'] if ans['text'] is not None else ans['freebaseId'] for ans in raw_ans_list]\n    if not answers:\n        answers = [\"None\"]\n    if original_name:\n        answers = [get_raw_name(ans) for ans in answers]\n    return answers\n\ndef parse_answer_id(raw_ans_list):\n    answers = [ans['freebaseId'] for ans in raw_ans_list]\n    return answers", "def parse_answer_id(raw_ans_list):\n    answers = [ans['freebaseId'] for ans in raw_ans_list]\n    return answers\n\ndef compute_f1(pred_set, gold_set):\n    pred_set = set(pred_set)\n    gold_set = set(gold_set)\n    precision = len(pred_set & gold_set) / (len(pred_set) + 1e-8)\n    recall = len(pred_set & gold_set) / (len(gold_set) + 1e-8)\n    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n    return f1", "\ndef compute_hits1_dict(pred_dict, gold_dict):\n    score = 0\n    for key in gold_dict:\n        if key in pred_dict:\n            if pred_dict[key][0] in gold_dict[key]:\n                score += 1\n    return score / len(gold_dict)\n\n# permute the list to perform augmentation", "\n# permute the list to perform augmentation\n# [A, B, C] -> [\"A | B | C\", \"B | A | C\", \"C | A | B\", ...]\ndef aug_list_old(org_list, augmentation_num=10):\n    all_orders = []\n    for i, answer_order in enumerate(itertools.permutations(org_list)):\n        all_orders.append(answer_order)\n        if i == augmentation_num - 1:\n            break\n    return [\" | \".join(answers_i) for answers_i in all_orders]", "\n\ndef aug_list(org_list):\n    augmentation_num = min( max(1, len(org_list) * (len(org_list) - 1)) , 10)\n    all_orders = []\n    for _ in range(augmentation_num):\n        all_orders.append(random.sample(org_list, len(org_list)))\n    return [\" | \".join(answers_i) for answers_i in all_orders]\n\n", "\n\n# transform a freebase id to a name\ndef id2name(m_id, id2name_dict):\n    if m_id in id2name_dict:\n        return id2name_dict[m_id]\n    else:\n        try:\n            name = get_label(m_id)\n            if name is not None:\n                return name\n            else:\n                return m_id\n        except:\n            return m_id", "\n\ndef oracle_rerank(raw_answer_dict, answer_true):\n    '''\n    choose the answer with the highest F1 score with the true answer\n    '''\n    new_answer_dict = {\n        \"SP\": [each[\"answer\"] for each in raw_answer_dict[\"SP\"]],\n        \"QA\": [each[\"answer\"] for each in raw_answer_dict[\"QA\"]]\n    }\n    if len(new_answer_dict[\"SP\"]) == 0:\n        if len(new_answer_dict[\"QA\"]) > 0:\n            return new_answer_dict[\"QA\"][0], None\n        else:\n            return [\"None\"], None\n    if len(new_answer_dict[\"QA\"]) == 0:\n        return new_answer_dict[\"SP\"][0], raw_answer_dict[\"SP\"][0][\"logical_form\"]\n    # compute the F1 score for top answer\n    top_answer_sp = new_answer_dict[\"SP\"][0]\n    top_f1_sp = compute_f1(set(top_answer_sp), set(answer_true))\n    top_answer_qa = new_answer_dict[\"QA\"][0]\n    top_f1_qa = compute_f1(set(top_answer_qa), set(answer_true))\n    if top_f1_sp >= top_f1_qa:\n        return top_answer_sp, raw_answer_dict[\"SP\"][0][\"logical_form\"]\n    else:\n        return top_answer_qa, None", "\n\n\ndef score_rerank(raw_answer_dict, sp_weight=1.0):\n    '''\n    SP: [[A, B, C], [B, C]], QA: [[A, B], [B, C]]\n    '''\n    new_answer_dict = {\n        \"SP\": [each[\"answer\"] for each in raw_answer_dict[\"SP\"]],\n        \"QA\": [each[\"answer\"] for each in raw_answer_dict[\"QA\"]]\n    }\n    answer2logic = {}\n    for each in raw_answer_dict[\"SP\"]:\n        if tuple(each[\"answer\"]) not in answer2logic:\n            answer2logic[tuple(each[\"answer\"])] = each[\"logical_form\"]\n    \n    if len(new_answer_dict[\"QA\"]) == 0:\n        new_answer_dict[\"QA\"] = [[\"None\"]]\n\n    if len(new_answer_dict[\"SP\"]) == 0:\n        if len(new_answer_dict[\"QA\"]) > 0:\n            return new_answer_dict[\"QA\"][0], None\n        else:\n            return [\"None\"], None\n    \n    score_dict = {}\n    for i in range(len(new_answer_dict[\"SP\"])):\n        score_dict[\"SP{}\".format(i)] = 1/(1+i) * sp_weight\n        # score_dict[\"SP{}\".format(i)] = (len(new_answer_dict[\"QA\"]) - i) * sp_weight\n    for i in range(len(new_answer_dict[\"QA\"])):\n        score_dict[\"QA{}\".format(i)] = 1/(1+i) * (1-sp_weight)\n        # score_dict[\"QA{}\".format(i)] = (len(new_answer_dict[\"QA\"]) - i) * (1-sp_weight)\n    \n    answer2score_dict = {}\n    \n    for i, sp_i in enumerate(new_answer_dict[\"SP\"]):\n        if tuple(sp_i) in answer2score_dict:\n            continue\n        hits = False\n        for j, qa_j in enumerate(new_answer_dict[\"QA\"]):\n            if set(sp_i) == set(qa_j):\n                answer2score_dict[tuple(sp_i)] = score_dict[\"SP{}\".format(i)]\n                answer2score_dict[tuple(sp_i)] += score_dict[\"QA{}\".format(j)]\n                hits = True\n                break\n        if not hits:\n            answer2score_dict[tuple(sp_i)] = score_dict[\"SP{}\".format(i)]\n\n    for j, qa_j in enumerate(new_answer_dict[\"QA\"]):\n        if tuple(qa_j) not in answer2score_dict:\n            answer2score_dict[tuple(qa_j)] = score_dict[\"QA{}\".format(j)]    \n\n    # sort the dict based on value\n    sorted_score_dict = sorted(answer2score_dict.items(), key=lambda x: x[1], reverse=True)\n    top_answer = sorted_score_dict[0][0]\n\n    if top_answer in answer2logic:\n        return list(top_answer), answer2logic[top_answer]\n    else:\n        return list(top_answer), None", "\n\ndef Prefix_to_id_all(in_file_path, name2id_dict, SP_early_break=True):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        task = key.split(\":\")[-1]\n        q_key = \":\".join(key.split(\":\")[:-1])\n        if q_key not in new_data:\n            new_data[q_key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": {\n                    task: data[key][\"predicted answers\"]\n                }\n            }\n        else:\n            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        SP_answers = answers['SP']\n        find_answer = False\n        for sp_i, answer in enumerate(SP_answers):\n            for result in denorm_final(answer, name2id_dict):\n                new_answer = execute_vanilla_s_expr(result)\n                if len(new_answer) > 0:\n                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n                    find_answer = True\n                    break\n            if find_answer and SP_early_break:\n                break\n\n        QA_answers = answers['QA']\n        for qa_i, answer in enumerate(QA_answers):\n            if \" | \" in answer:\n                # multiple answers\n                total_answers = []\n                answer_split = answer.split(\" | \")\n                for each in answer_split:\n                    if each in name2id_dict:\n                        total_answers += name2id_dict[each]\n                if len(total_answers) > 0:\n                    # deduplication but keep original order\n                    total_answers = list(OrderedDict.fromkeys(total_answers))\n                    new_answer = total_answers\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n            else:\n                if answer in name2id_dict:\n                    new_answer = name2id_dict[answer]\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef SP_to_id_all(in_file_path, name2id_dict):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        task = \"SP\"\n        q_key = key\n        if q_key not in new_data:\n            new_data[q_key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": {\n                    task: data[key][\"predicted answers\"]\n                }\n            }\n        else:\n            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        SP_answers = answers['SP']\n        find_answer = False\n        for sp_i, answer in enumerate(SP_answers):\n            for result in denorm_final(answer, name2id_dict):\n                new_answer = execute_vanilla_s_expr(result)\n                if len(new_answer) > 0:\n                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n                    find_answer = True\n                    break\n            if find_answer:\n                break\n\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef QA_to_id_all(in_file_path, name2id_dict):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        task = \"QA\"\n        q_key = key\n        if q_key not in new_data:\n            new_data[q_key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": {\n                    task: data[key][\"predicted answers\"]\n                }\n            }\n        else:\n            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        QA_answers = answers['QA']\n        for qa_i, answer in enumerate(QA_answers):\n            if \" | \" in answer:\n                # multiple answers\n                total_answers = []\n                answer_split = answer.split(\" | \")\n                for each in answer_split:\n                    if each in name2id_dict:\n                        total_answers += name2id_dict[each]\n                if len(total_answers) > 0:\n                    # deduplication but keep original order\n                    total_answers = list(OrderedDict.fromkeys(total_answers))\n                    new_answer = total_answers\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n            else:\n                if answer in name2id_dict:\n                    new_answer = name2id_dict[answer]\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef QA_to_name_all(in_file_path):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        task = \"QA\"\n        q_key = key\n        if q_key not in new_data:\n            new_data[q_key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": {\n                    task: data[key][\"predicted answers\"]\n                }\n            }\n        else:\n            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        QA_answers = answers['QA']\n        for qa_i, answer in enumerate(QA_answers):\n            if \" | \" in answer:\n                # multiple answers\n                total_answers = []\n                answer_split = answer.split(\" | \")\n                for each in answer_split:\n                    total_answers += [get_raw_name(each)]\n                if len(total_answers) > 0:\n                    # deduplication but keep original order\n                    total_answers = list(OrderedDict.fromkeys(total_answers))\n                    new_answer = total_answers\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n            else:\n                new_answer = [get_raw_name(answer)]\n                new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_name_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef Combine_QA_SP(in_file_path_qa, in_file_path_sp):\n    with open(in_file_path_qa, \"r\") as f:\n        data_qa = json.load(f)\n    print(len(data_qa))\n    print(list(data_qa.keys())[0], data_qa[list(data_qa.keys())[0]])\n    with open(in_file_path_sp, \"r\") as f:\n        data_sp = json.load(f)\n    print(len(data_sp))\n    print(list(data_sp.keys())[0], data_sp[list(data_sp.keys())[0]])\n    assert len(data_sp) == len(data_qa)\n    new_data = {}\n    for key in data_qa:\n        new_data[key + \":QA\"] = data_qa[key]\n    for key in data_sp:\n        new_data[key + \":SP\"] = data_sp[key]\n    print(len(new_data))\n    save_file = in_file_path_qa.replace(\".json\", \"_combine.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)\n    return save_file", "\n\n\ndef Prefix_to_name_all(in_file_path, name2id_dict, id2name_dict):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        task = key.split(\":\")[-1]\n        q_key = \":\".join(key.split(\":\")[:-1])\n        if q_key not in new_data:\n            new_data[q_key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": {\n                    task: data[key][\"predicted answers\"]\n                }\n            }\n        else:\n            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        SP_answers = answers['SP']\n        find_answer = False\n        for sp_i, answer in enumerate(SP_answers):\n            for result in denorm_final(answer, name2id_dict):\n                new_answer = execute_vanilla_s_expr(result)\n                if len(new_answer) > 0:\n                    new_answer = [id2name(answer_i, id2name_dict) for answer_i in new_answer]\n                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n                    find_answer = True\n                    break\n            if find_answer:\n                break\n\n        QA_answers = answers['QA']\n        for qa_i, answer in enumerate(QA_answers):\n            if \" | \" in answer:\n                # multiple answers\n                total_answers = []\n                answer_split = answer.split(\" | \")\n                for each in answer_split:\n                    # if each in name2id_dict:\n                    total_answers += [get_raw_name(each)]\n                if len(total_answers) > 0:\n                    # deduplication but keep original order\n                    total_answers = list(OrderedDict.fromkeys(total_answers))\n                    new_answer = total_answers\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n            else:\n                # if answer in name2id_dict:\n                new_answer = [get_raw_name(answer)]\n                new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_name_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef Concat_to_id_all(in_file_path, name2id_dict):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    new_data = {}\n    for key in data:\n        sp_answers, qa_answers = [], []\n        for each in data[key][\"predicted answers\"]:\n            sp_answers.append(each.split(\"expression: \")[-1].split(\" answer: \")[0])\n            qa_answers.append(each.split(\"answer: \")[-1])\n        new_data[key] = {\n            \"question\": data[key][\"question\"],\n            \"gold answers\": data[key][\"gold answers\"],\n            \"predicted answers\": {\n                \"SP\": sp_answers,\n                \"QA\": qa_answers\n            }\n        }\n    print(len(new_data))\n    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\n    result_dict = {}\n    for i, key in tqdm(enumerate(new_data)):\n        data_i = new_data[key]\n        answers = data_i['predicted answers']\n        new_answer_total = {\"SP\": [], \"QA\": []}\n\n        SP_answers = answers['SP']\n        for sp_i, answer in enumerate(SP_answers):\n            for result in denorm_final(answer, name2id_dict):\n                new_answer = execute_vanilla_s_expr(result)\n                if len(new_answer) > 0:\n                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n                    break\n\n        QA_answers = answers['QA']\n        for qa_i, answer in enumerate(QA_answers):\n            if \" | \" in answer:\n                # multiple answers\n                total_answers = []\n                answer_split = answer.split(\" | \")\n                for each in answer_split:\n                    if each in name2id_dict:\n                        total_answers += name2id_dict[each]\n                if len(total_answers) > 0:\n                    # deduplication but keep original order\n                    total_answers = list(OrderedDict.fromkeys(total_answers))\n                    new_answer = total_answers\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n            else:\n                if answer in name2id_dict:\n                    new_answer = name2id_dict[answer]\n                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n        result_dict[key] = {\n                \"question\": new_data[key][\"question\"],\n                \"gold answers\": new_data[key][\"gold answers\"],\n                \"predicted answers\": new_answer_total,\n            }\n    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n    print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file", "\n\ndef sort_SP_with_QA(SP_answers, QA_answers):\n    # assign scores to QA answers based on rank\n    QA_answers_score_dict = defaultdict(float)\n    for qa_i, answer in enumerate(QA_answers):\n        QA_answers_score_dict[answer] = 1/(1+qa_i)\n    # sort SP answers based on QA answers\n    SP_answers_sorted = []\n    for sp_i, answer in enumerate(SP_answers):\n        SP_answers_sorted.append({\"answer\": answer, \"score\": QA_answers_score_dict[answer]})\n    SP_answers_sorted = sorted(SP_answers_sorted, key=lambda x: x[\"score\"], reverse=True)\n    SP_answers_sorted_answers = [each[\"answer\"] for each in SP_answers_sorted]\n    return SP_answers_sorted_answers", "\n\ndef answer_ensemble(in_file_path, mode=\"SP_first\", sp_weight=1.0, gold_answers=None):\n    '''\n    combine the SP and QA answer to the final answers\n    '''\n    def get_top_k(answer_list, k=1):\n        top_k = min(k, len(answer_list))\n        output_answer = []\n        for i in range(top_k):\n            output_answer += answer_list[i][\"answer\"]\n        output_answer = list(OrderedDict.fromkeys(output_answer))\n        return output_answer\n    \n    assert \"_all.json\" in in_file_path\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    # print(len(data))\n    # print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    num_sp = 0\n    for key in data:\n        pred_answers = data[key][\"predicted answers\"]\n        if gold_answers is not None:\n            answer_true = gold_answers[key]\n        if mode == \"SP_first\":\n            data[key][\"predicted answers\"] = []\n            QA_answers = []\n            if len(pred_answers[\"QA\"]) > 0:\n                # data[key][\"predicted answers\"] = pred_answers[\"QA\"][0][\"answer\"]\n                data[key][\"predicted answers\"] = get_top_k(pred_answers[\"QA\"], k=1)\n                QA_answers = pred_answers[\"QA\"][0][\"answer\"]\n                # QA_answers = [pred_answers[\"QA\"][i][\"answer\"][0] for i in range(len(pred_answers[\"QA\"]))]\n            if len(pred_answers[\"SP\"]) > 0:\n                # data[key][\"predicted answers\"] = pred_answers[\"SP\"][0][\"answer\"]\n                data[key][\"predicted answers\"] = sort_SP_with_QA(pred_answers[\"SP\"][0][\"answer\"], QA_answers)\n                data[key][\"logical_form\"] = pred_answers[\"SP\"][0][\"logical_form\"]\n                num_sp += 1\n        elif mode == \"SP_only\":\n            data[key][\"predicted answers\"] = []\n            if len(pred_answers[\"SP\"]) > 0:\n                data[key][\"predicted answers\"] = pred_answers[\"SP\"][0][\"answer\"]\n                data[key][\"logical_form\"] = pred_answers[\"SP\"][0][\"logical_form\"]\n                num_sp += 1\n        elif mode == \"QA_only\":\n            data[key][\"predicted answers\"] = []\n            if len(pred_answers[\"QA\"]) > 0:\n                # data[key][\"predicted answers\"] = pred_answers[\"QA\"][0][\"answer\"]\n                # combine top-k QA answers\n                data[key][\"predicted answers\"] = get_top_k(pred_answers[\"QA\"], k=1)\n        elif mode == \"Score\":\n            data[key][\"predicted answers\"], logical_form = score_rerank(pred_answers, sp_weight)\n            if len(pred_answers[\"QA\"]) > 0:\n                data[key][\"predicted answers\"] = sort_SP_with_QA(data[key][\"predicted answers\"], pred_answers[\"QA\"][0][\"answer\"])\n            if logical_form is not None:\n                num_sp += 1\n                data[key][\"logical_form\"] = logical_form\n        elif mode == \"oracle\":\n            data[key][\"predicted answers\"], logical_form = oracle_rerank(pred_answers, answer_true)\n            if len(pred_answers[\"QA\"]) > 0:\n                data[key][\"predicted answers\"] = sort_SP_with_QA(data[key][\"predicted answers\"], pred_answers[\"QA\"][0][\"answer\"])\n            if logical_form is not None:\n                num_sp += 1\n                data[key][\"logical_form\"] = logical_form\n    \n    # print(\"num_sp:\", num_sp)\n    save_file = in_file_path.replace(\"_all.json\", \"_final.json\")\n    # print(save_file)\n    with open(save_file, \"w\") as wf:\n        json.dump(data, wf, indent=2)\n    return save_file", "    \n\ndef Joint_to_id(in_file_path, name2id_dict):\n\n    with open(in_file_path, \"r\") as f:\n        data = json.load(f)\n    print(len(data))\n    print(list(data.keys())[0], data[list(data.keys())[0]])\n\n    result_dict = {}\n    num_qa = 0\n    for i, key in tqdm(enumerate(data)):\n        data_i = data[key]\n        answers = data_i['predicted answers']\n        new_answer = []\n        find_answer = False\n        logical_form = \"None\"\n        for answer in answers:\n            if \"expression:\" in answer:\n                answer = answer.split(\"expression: \")[-1]\n                for result in denorm_final(answer, name2id_dict):\n                    new_answer = execute_vanilla_s_expr(result)\n                    if len(new_answer) > 0:\n                        find_answer = True\n                        logical_form = result\n                        break\n                if find_answer:\n                    break\n            elif \"answer:\" in answer:\n                answer = answer.split(\"answer: \")[-1]\n                num_qa += 1\n                if \" | \" in answer:\n                    # multiple answers\n                    total_answers = []\n                    answer_split = answer.split(\" | \")\n                    for each in answer_split:\n                        if each in name2id_dict:\n                            total_answers += name2id_dict[each]\n                    if len(total_answers) > 0:\n                        # deduplication but keep original order\n                        total_answers = list(OrderedDict.fromkeys(total_answers))\n                        new_answer = total_answers\n                        break\n                else:\n                    if answer in name2id_dict:\n                        new_answer = name2id_dict[answer]\n                        break\n        result_dict[key] = {\n                \"question\": data[key][\"question\"],\n                \"gold answers\": data[key][\"gold answers\"],\n                \"predicted answers\": [new_answer],\n                \"logical_form\": logical_form,\n            }\n    print(num_qa)\n    save_file_id = in_file_path.replace(\".json\", \"_id.json\")\n    print(save_file_id)\n    with open(save_file_id, \"w\") as wf:\n        json.dump(result_dict, wf, indent=2)\n    return save_file_id", "\n\ndef execute_vanilla_s_expr(s_expr):\n    signal.signal(signal.SIGALRM, handler)\n    signal.alarm(10) #Set the parameter to the amount of seconds you want to wait\n    try:\n        # print('parse', query_expr)\n        sparql_query = lisp_to_sparql(s_expr)\n        # print('sparql', sparql_query)\n        # set maximum time as 100 seconds\n        denotation = execute_query(sparql_query)\n    except:\n        denotation = []\n    signal.alarm(10) #Resets the alarm to 10 new seconds\n    signal.alarm(0) #Disables the alarm\n    return denotation", "\n\ndef denormalize(expr, entity_label_map):\n\n    expr = expr.replace('^^http', 'http')\n    expr = expr.replace('http', '^^http')\n    expr = expr.replace('( ', '(').replace(' )', ')')\n            \n    entities = re.findall(r'\\[ (.*?) \\]', expr)\n    expr_list = [expr]\n    for e in entities:\n        # delete the beginning and end space of e\n        orig_e = f\"[ {e} ]\"\n\n        new_expr_list = []\n        name_e = e\n        if name_e in entity_label_map:\n            for expr_i in expr_list:\n                for id_map in entity_label_map[name_e]:\n                    new_expr_list.append(expr_i.replace(orig_e, id_map))\n        \n        expr_list = new_expr_list\n    expr_list = expr_list[:10]\n    return expr_list", "\n\ndef SP_to_id(in_file, name2id_dict):\n    with open(in_file, 'r') as rf:\n        data = json.load(rf)\n    print(len(data))\n    print(\"Data Example:\")\n    print(data[list(data.keys())[0]])\n    new_data = {}\n    wrong_data = []\n    num_no_answers = 0\n    for key in tqdm(data):\n        logical_form = \"None\"\n        answers = []\n        for predict_exp in data[key][\"predicted answers\"]:\n            predict_exp = denormalize(predict_exp, name2id_dict)\n            if isinstance(predict_exp, list):\n                for predict_exp_i in predict_exp:\n                    predict_answers = execute_vanilla_s_expr(predict_exp_i)\n                    if len(predict_answers) != 0:\n                        logical_form = predict_exp_i\n                        answers = predict_answers\n                        break\n            else:\n                predict_answers = execute_vanilla_s_expr(predict_exp)\n                if len(predict_answers) != 0:\n                    logical_form = predict_exp\n                    answers = predict_answers\n            if len(answers) != 0:\n                break\n        \n        if len(answers) == 0:\n            num_no_answers += 1\n            data[key].update({\"id\": key})\n            wrong_data.append(data[key])\n        \n        new_data[key] = {\n            \"question\": data[key][\"question\"],\n            \"gold answers\": data[key][\"gold answers\"],\n            \"logical_form\": logical_form,\n            \"predicted answers\": [answers]\n        }\n    print(\"num_no_answers: \", num_no_answers)\n    print(\"Output Example:\")\n    print(new_data[list(new_data.keys())[0]])\n    out_file = in_file.replace(\".json\", \"_id.json\")\n    with open(out_file, 'w') as wf:\n        json.dump(new_data, wf, indent=2)\n    wrong_file = in_file.replace(\".json\", \"_wrong.json\")\n    with open(wrong_file, \"w\") as wf:\n        json.dump(wrong_data, wf, indent=2)\n    return out_file", "\n\ndef id_to_name(in_file_path, id2name_dict):\n    with open(in_file_path, \"r\") as rf:\n        data = json.load(rf)\n    # print(\"Data example: \")\n    # print(data[list(data.keys())[0]])\n    new_data = {}\n    for key in tqdm(data):\n        new_answer_list = []\n        for answer in data[key][\"predicted answers\"]:\n            new_answer_list.append(id2name(answer, id2name_dict))\n        new_data[key] = {\n            \"question\": data[key][\"question\"],\n            \"gold answers\": data[key][\"gold answers\"],\n            \"predicted answers\": new_answer_list\n        }\n    # print(\"Output example: \")\n    # print(new_data[list(new_data.keys())[0]])\n    out_save_path = in_file_path.replace(\".json\", \"_name.json\")\n    # print(\"Output File:\", os.path.basename(out_save_path))\n    with open(out_save_path, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)\n    return out_save_path", "\n######## Transform results to official evaluation format ########\n\ndef to_webqsp_format(in_file_path):\n    with open(in_file_path, \"r\") as rf:\n        data = json.load(rf)\n    new_data = []\n    for key in data:\n        new_data.append({\"QuestionId\": key, \"Answers\": data[key][\"predicted answers\"]})\n    # print(\"Output example: \")\n    # print(new_data[0])\n    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n    # print(\"Output File:\", os.path.basename(out_save_path))\n    with open(out_save_path, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)\n    return out_save_path", "\n\ndef to_cwq_format(in_file_path):\n    with open(in_file_path, \"r\") as rf:\n        data = json.load(rf)\n    new_data = []\n    for key in data:\n        answer = data[key][\"predicted answers\"]\n        if len(answer) > 0:\n            # randomly pick one as answer\n            answer = answer[0]\n            # answer = random.sample(answer, 1)[0]\n        else:\n            answer = \"None\"\n        new_data.append({\"ID\": key, \"answer\": answer.lower().strip()})\n    # print(\"Output example: \")\n    # print(new_data[0])\n    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n    # print(\"Output File:\", os.path.basename(out_save_path))\n    with open(out_save_path, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)\n    return out_save_path", "\n\ndef to_grailqa_format(in_file_path):\n    with open(in_file_path, \"r\") as rf:\n        data = json.load(rf)\n    new_data = []\n    for key in data:\n        new_data_i = {\n            \"qid\": key,\n            \"logical_form\": data[key][\"logical_form\"] if \"logical_form\" in data[key] else \"None\",\n            \"answer\": data[key][\"predicted answers\"]\n        }\n        new_data.append(new_data_i)\n    # print(\"Output example: \")\n    # print(new_data[0])\n    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n    # print(\"Output File:\", os.path.basename(out_save_path))\n    lines = [json.dumps(x) for x in new_data]\n    with open(out_save_path, 'w') as wf:\n        wf.writelines([x+'\\n' for x in lines])\n    return out_save_path"]}
{"filename": "DecAF/Datasets/QA/disambiguate.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#", "# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.", "# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport json\nfrom tqdm import tqdm\nimport argparse\nfrom DecAF.Preprocess.linearize import load_nameid_dict\nimport logging\nlogging.basicConfig(level=logging.INFO)", "import logging\nlogging.basicConfig(level=logging.INFO)\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='disambiguate dataset answer entities')\nparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\nargs = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    # load id2name mapping\n    logging.info(\"Loading id2name mapping\")\n    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\n    # load original QA dataset\n    for split in [\"dev\", \"test\", \"train\"]:\n        if f\"{split}.json\" not in os.listdir(args.data_dir):\n            continue\n        org_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n        with open(org_file_path, \"r\") as f:\n            org_data = json.load(f)\n\n        new_data = []\n        for item in tqdm(org_data):\n            new_answers = []\n            for answer in item[\"Answers\"]:\n                if answer[\"freebaseId\"] in id2name_dict:\n                    new_answers.append({\n                        \"freebaseId\": answer[\"freebaseId\"],\n                        \"text\": id2name_dict[answer[\"freebaseId\"]],\n                    })\n                else:\n                    new_answers.append(answer)\n            new_data.append({\n                \"QuestionId\": item[\"QuestionId\"],\n                \"Question\": item[\"Question\"],\n                \"Answers\": new_answers,\n            })\n\n        save_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n        os.makedirs(os.path.dirname(save_file_path), exist_ok=True)\n        with open(save_file_path, \"w\") as f:\n            json.dump(new_data, f, indent=2)", "\n\nif __name__ == \"__main__\":\n    # load id2name mapping\n    logging.info(\"Loading id2name mapping\")\n    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\n    # load original QA dataset\n    for split in [\"dev\", \"test\", \"train\"]:\n        if f\"{split}.json\" not in os.listdir(args.data_dir):\n            continue\n        org_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n        with open(org_file_path, \"r\") as f:\n            org_data = json.load(f)\n\n        new_data = []\n        for item in tqdm(org_data):\n            new_answers = []\n            for answer in item[\"Answers\"]:\n                if answer[\"freebaseId\"] in id2name_dict:\n                    new_answers.append({\n                        \"freebaseId\": answer[\"freebaseId\"],\n                        \"text\": id2name_dict[answer[\"freebaseId\"]],\n                    })\n                else:\n                    new_answers.append(answer)\n            new_data.append({\n                \"QuestionId\": item[\"QuestionId\"],\n                \"Question\": item[\"Question\"],\n                \"Answers\": new_answers,\n            })\n\n        save_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n        os.makedirs(os.path.dirname(save_file_path), exist_ok=True)\n        with open(save_file_path, \"w\") as f:\n            json.dump(new_data, f, indent=2)"]}
{"filename": "DecAF/Datasets/QA/CWQ/parse_sparql.py", "chunked_list": ["# Copyright (c) 2021, salesforce.com, inc. and its affiliates.\n# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport pickle\nimport json\nimport os", "import json\nimport os\nimport random\nfrom re import L\nimport shutil\nfrom collections import Counter\nfrom DecAF.Datasets.QA.SP_tools.components.utils import *\nfrom DecAF.Datasets.QA.SP_tools.components.expr_parser import parse_s_expr, extract_entities, tokenize_s_expr\nfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query\nfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query", "from DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query\nfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query\nfrom DecAF.Datasets.QA.SP_tools.executor.logic_form_util import lisp_to_sparql\nimport argparse\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='generate s-expression')\nparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ/raw')\nargs = parser.parse_args()", "parser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ/raw')\nargs = parser.parse_args()\n\nclass ParseError(Exception):\n    pass\n\nclass Parser:\n    def __init__(self):\n        pass\n\n    def parse_query(self, query, topic_mid):\n        # print('input', query)\n        # print(topic_mid)\n        lines = query.split('\\n')\n        lines = [x for x in lines if x]\n\n        assert lines[0] != '#MANUAL SPARQL'\n\n        prefix_stmts = []        \n        line_num = 0\n        while True:\n            l = lines[line_num]\n            if l.startswith('PREFIX'):\n                prefix_stmts.append(l)\n            else:\n                break\n            line_num = line_num + 1\n\n        next_line = lines[line_num]\n        assert next_line.startswith('SELECT DISTINCT ?x')\n        line_num = line_num + 1\n        next_line = lines[line_num]\n        assert next_line == 'WHERE {'\n        assert lines[-1] in ['}', 'LIMIT 1']\n\n        # print(lines)\n\n        lines = lines[line_num :]\n        assert all(['FILTER (str' not in x for x in lines])\n        # normalize body lines\n\n        body_lines, spec_condition = self.normalize_body_lines(lines)\n        # assert all([x.startswith('?') or x.startswith('ns') or x.startswith('FILTER') for x in body_lines])\n        # we only parse query following this format\n        # print(body_lines)\n        if body_lines[0].startswith('FILTER'):\n            predefined_filter0 = body_lines[0]\n            predefined_filter1 = body_lines[1]\n            # assert predefined_filter0 == f'FILTER (?x != ?c)'\n            # assert predefined_filter1 == \"FILTER (!isLiteral(?x) OR lang(?x) = '' OR langMatches(lang(?x), 'en'))\"\n            # if predefined_filter0 != f'FILTER (?x != ns:{topic_mid})':\n            #     print('QUERY', query)\n            #     print('First Filter')\n            # if predefined_filter1 != \"FILTER (!isLiteral(?x) OR lang(?x) = '' OR langMatches(lang(?x), 'en'))\":\n            #     print('QUERY', query)\n            #     print('Second Filter')\n            # if any([not (x.startswith('?') or x.startswith('ns:')) for x in body_lines]):\n            #     print('Unprincipled Filter')\n            #     print('QUERY', query)\n            body_lines = body_lines[2:]\n\n        # print(body_lines)\n        assert all([(x.startswith('?') or x.startswith('ns:')) for x in body_lines])\n        var_dep_list = self.parse_naive_body(body_lines, '?x')\n        s_expr = self.dep_graph_to_s_expr(var_dep_list, '?x', spec_condition)\n        return s_expr\n\n    def normalize_body_lines(self, lines):\n        if lines[-1] == 'LIMIT 1':\n            # spec_condition = argmax\n            # who did jackie robinson first play for?\n            # WHERE {\n            # ns:m.0443c ns:sports.pro_athlete.teams ?y .\n            # ?y ns:sports.sports_team_roster.team ?x .\n            # ?y ns:sports.sports_team_roster.from ?sk0 .\n            # }\n            # ORDER BY DESC(xsd:datetime(?sk0))\n            # LIMIT 1\n            order_line = lines[-2]\n            direction = 'argmax' if 'DESC(' in order_line else 'argmin'\n            # assert '?sk0' in\n            # print(line)\n            assert ('?sk0' in order_line)\n            _tmp_body_lines = lines[1:-3]\n            body_lines = []\n            hit = False\n            for l in _tmp_body_lines:\n                if '?sk0' in l:\n                    self.parse_assert(l.endswith('?sk0 .') and not hit)\n                    hit = True\n                    arg_var, arg_r = l.split(' ')[0], l.split(' ')[1]\n                    arg_r = arg_r[3:] #rm ns:\n                else:\n                    body_lines.append(l)\n                    \n            return body_lines, (direction, arg_var, arg_r)\n        # check if xxx\n        elif lines[-4].startswith('FILTER(NOT EXISTS {?'):\n            # WHERE {\n            # ns:m.04f_xd8 ns:government.government_office_or_title.office_holders ?y .\n            # ?y ns:government.government_position_held.office_holder ?x .\n            # FILTER(NOT EXISTS {?y ns:government.government_position_held.from ?sk0} || \n            # EXISTS {?y ns:government.government_position_held.from ?sk1 . \n            # FILTER(xsd:datetime(?sk1) <= \"2009-12-31\"^^xsd:dateTime) })\n            # FILTER(NOT EXISTS {?y ns:government.government_position_held.to ?sk2} || \n            # EXISTS {?y ns:government.government_position_held.to ?sk3 . \n            # FILTER(xsd:datetime(?sk3) >= \"2009-01-01\"^^xsd:dateTime) })\n            # }\n            body_lines = lines[1:-7]\n            range_lines = lines[-7:-1]\n            range_prompt = range_lines[0]\n            range_prompt = range_prompt[range_prompt.index('{') + 1:range_prompt.index('}')]\n            range_var = range_prompt.split(' ')[0]\n            range_relation = range_prompt.split(' ')[1]\n            range_relation = '.'.join(range_relation.split('.')[:2]) + '.time_macro'\n            range_relation = range_relation[3:] # rm ns:\n\n            range_start = range_lines[2].split(' ')[2]\n            range_start = range_start[1:]\n            range_start = range_start[:range_start.index('\"')]\n            range_end = range_lines[5].split(' ')[2]\n            range_end = range_end[1:]\n            range_end = range_end[:range_end.index('\"')]\n            assert range_start[:4] == range_end[:4]\n            range_year = range_start[:4] + '^^http://www.w3.org/2001/XMLSchema#date' #to fit parsable\n            return body_lines, ('range', range_var, range_relation, range_year)\n        else:\n            body_lines = lines[1:-1]\n            return body_lines, None        \n        \n\n    def dep_graph_to_s_expr(self, var_dep_list, ret_var, spec_condition=None):\n        self.parse_assert(var_dep_list[0][0] == ret_var)\n        var_dep_list.reverse()\n        parsed_dict = {}\n\n        spec_var = spec_condition[1] if spec_condition is not None else None\n\n        for var_name, dep_relations in var_dep_list:\n            # expr = ''\n            dep_relations[0]\n            clause = self.triplet_to_clause(var_name,  dep_relations[0], parsed_dict)\n            for tri in dep_relations[1:]:\n                n_clause = self.triplet_to_clause(var_name, tri, parsed_dict)\n                clause = 'AND ({}) ({})'.format(n_clause, clause)\n            if var_name == spec_var:\n                if  spec_condition[0] == 'argmax' or spec_condition[0] == 'argmin':\n                    relation = spec_condition[2]\n                    clause = '{} ({}) {}'.format(spec_condition[0].upper(), clause, relation)\n                elif spec_condition[0] == 'range':\n                    relation, time_point = spec_condition[2], spec_condition[3]\n                    n_clause = 'JOIN {} {}'.format(relation, time_point)\n                    clause = 'AND ({}) ({})'.format(n_clause, clause)\n            parsed_dict[var_name] = clause\n        return '(' + parsed_dict[ret_var] + ')'\n\n    def triplet_to_clause(self, tgt_var, triplet, parsed_dict):\n        if triplet[0] == tgt_var:\n            this = triplet[0]\n            other = triplet[-1]\n            if other in parsed_dict:\n                other = '(' + parsed_dict[other] + ')'\n            return 'JOIN {} {}'.format(triplet[1], other)\n        elif triplet[-1] == tgt_var:\n            this = triplet[-1]\n            other = triplet[0]\n            if other in parsed_dict:\n                other = '(' + parsed_dict[other] + ')'\n            return 'JOIN (R {}) {}'.format(triplet[1], other)\n        else:\n            raise ParseError()\n\n\n    def parse_assert(self, eval):\n        if not eval:\n            raise ParseError()\n\n    def parse_naive_body(self, body_lines, ret_var):\n        # ret_variable\n        # body_lines\n        body_lines = [x.strip() for x in body_lines]\n        assert all([x[-1] == '.' for x in body_lines])\n        triplets = [x.split(' ') for x in body_lines]\n        triplets = [x[:-1] for x in triplets]\n        # print(\"triplets: \", triplets)\n        # remove ns \n        triplets = [[x[3:] if x.startswith('ns:') else x for x in tri] for tri in triplets]\n        # dependancy graph\n        triplets_pool = triplets\n        # while True:\n        var_dep_list = []\n        successors = []\n        dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, ret_var, successors)\n        var_dep_list.append((ret_var, dep_triplets))        \n        # vars_pool = []\n        # go over un resolved vars\n        # for tri in triplets_pool:\n        #     if tri[0].startswith('?') and tri[0] not in vars_pool and tri[0] != ret_var:\n        #         vars_pool.append(tri[0])\n        #     if tri[-1].startswith('?') and tri[-1] not in vars_pool and tri[-1] != ret_var:\n        #         vars_pool.append(tri[-1])\n        \n        # for tgt_var in vars_pool:\n        #     dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, tgt_var)\n        #     self.parse_assert(len(dep_triplets) > 0)\n        #     var_dep_list.append((tgt_var, dep_triplets))\n        while len(successors):\n            tgt_var = successors[0]\n            successors = successors[1:]\n            dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, tgt_var, successors)\n            self.parse_assert(len(dep_triplets) > 0)\n            var_dep_list.append((tgt_var, dep_triplets))\n\n        self.parse_assert(len(triplets_pool) == 0)\n        return var_dep_list\n        \n\n    def resolve_dependancy(self, triplets, target_var, successors):\n        dep = []\n        left = []\n        for tri in triplets:\n            if tri[0] == target_var:\n                dep.append(tri)\n                if tri[-1].startswith('?') and tri[-1] not in successors:\n                    successors.append(tri[-1])\n            elif tri[-1] == target_var:\n                dep.append(tri)\n                if tri[0].startswith('?') and tri[0] not in successors:\n                    successors.append(tri[0])\n            else:\n                left.append(tri)\n        return dep, left", "\nclass SparqlParse:\n    def __init__(self):\n        select_stmt = None\n        prefix_stmts = None\n        where_stmts = None\n        query_stmts = None\n\n\ndef get_mid_from_sparql(sparql):\n    for each in sparql.split(' '):\n        if each.startswith('ns:m.'):\n            return each[3:]", "\ndef get_mid_from_sparql(sparql):\n    for each in sparql.split(' '):\n        if each.startswith('ns:m.'):\n            return each[3:]\n\ndef convert_parse_instance(parse):\n    sparql = parse['sparql']\n    # print(parse.keys())\n    # print(parse['PotentialTopicEntityMention'])\n    # print(parse['TopicEntityMid'], parse['TopicEntityName'])\n    try:\n        s_expr = parser.parse_query(sparql, get_mid_from_sparql(sparql))\n        # print('---GOOD------')\n        # print(sparql)\n        # print(s_expr)\n    except AssertionError:\n        s_expr = 'null'\n    # print(parse[''])\n    parse['SExpr'] = s_expr\n    return parse, s_expr != 'null'", "\n# def s_expr_to_sparql(sparql_query):\n#     print(sparql_query)\n\ndef webq_s_expr_to_sparql_query(s_expr):\n    ast = parse_s_expr(s_expr)\n\ndef execute_webq_s_expr(s_expr):\n    try:\n        sparql_query = lisp_to_sparql(s_expr)\n        denotation = execute_query(sparql_query)\n    except:\n        denotation = []\n    return denotation", "\ndef augment_with_s_expr(split):\n    dataset = load_json(args.input_dir + f'/ComplexWebQuestions_{split}.json')\n    total_num = 0\n    hit_num = 0\n    new_dataset = []\n    for data in dataset:\n        total_num += 1\n        instance, flag_success = convert_parse_instance(data)\n        if flag_success:\n            hit_num += 1\n        new_dataset.append(instance)\n    print(hit_num, total_num, hit_num/total_num, len(new_dataset))\n    dump_json(new_dataset, args.input_dir + f'/ComplexWebQuestions_{split}.expr.json', indent=2)", "\n\ndef find_macro_template_from_query(query, topic_mid):\n    # print('QUERY', query)\n    lines = query.split('\\n')\n    lines = [x for x in lines if x]\n\n    assert lines[0] != '#MANUAL SPARQL'\n\n    prefix_stmts = []        \n    line_num = 0\n    while True:\n        l = lines[line_num]\n        if l.startswith('PREFIX'):\n            prefix_stmts.append(l)\n        else:\n            break\n        line_num = line_num + 1\n\n    next_line = lines[line_num]\n    assert next_line.startswith('SELECT DISTINCT ?x')\n    line_num = line_num + 1\n    next_line = lines[line_num]\n    assert next_line == 'WHERE {'\n    assert lines[-1] in ['}', 'LIMIT 1']\n\n    lines = lines[line_num :]\n    assert all(['FILTER (str' not in x for x in lines])\n    # normalize body lines\n    # return_val = check_time_macro_from_body_lines(lines)\n    # if return_val:\n        \n    # relation_prefix, suffix_pair = c\n    return check_time_macro_from_body_lines(lines)", "\ndef check_time_macro_from_body_lines(lines):\n    # check if xxx\n    if lines[-4].startswith('FILTER(NOT EXISTS {?'):\n        # WHERE {\n        # ns:m.04f_xd8 ns:government.government_office_or_title.office_holders ?y .\n        # ?y ns:government.government_position_held.office_holder ?x .\n        # FILTER(NOT EXISTS {?y ns:government.government_position_held.from ?sk0} || \n        # EXISTS {?y ns:government.government_position_held.from ?sk1 . \n        # FILTER(xsd:datetime(?sk1) <= \"2009-12-31\"^^xsd:dateTime) })\n        # FILTER(NOT EXISTS {?y ns:government.government_position_held.to ?sk2} || \n        # EXISTS {?y ns:government.government_position_held.to ?sk3 . \n        # FILTER(xsd:datetime(?sk3) >= \"2009-01-01\"^^xsd:dateTime) })\n        # }\n        body_lines = lines[1:-7]\n        range_lines = lines[-7:-1]\n        range_prompt_start = range_lines[0]\n        range_prompt_start = range_prompt_start[range_prompt_start.index('{') + 1:range_prompt_start.index('}')]\n        range_relation_start = range_prompt_start.split(' ')[1]\n        \n        # range_relation = '.'.join(range_relation.split('.')[:2]) + '.time_macro'\n        # range_relation = range_relation[3:] # rm ns:\n\n        range_prompt_end = range_lines[3]\n        range_prompt_end = range_prompt_end[range_prompt_end.index('{') + 1:range_prompt_end.index('}')]\n        range_relation_end = range_prompt_end.split(' ')[1]\n        \n        assert range_relation_start.split('.')[:2] == range_relation_end.split('.')[:2]\n        start_suffix = range_relation_start.split('.')[-1]\n        end_suffix = range_relation_end.split('.')[-1]\n        prefix = '.'.join(range_relation_start.split('.')[:2])[3:]\n        return prefix, start_suffix, end_suffix\n    else:\n        return None", "    \n\ndef extract_macro_template_from_instance(parse):\n    sparql = parse['Sparql']\n    # print(parse.keys())\n    # print(parse['PotentialTopicEntityMention'])\n    # print(parse['TopicEntityMid'], parse['TopicEntityName'])\n    try:\n        return find_macro_template_from_query(sparql, parse['TopicEntityMid'])\n    except AssertionError:\n        return None", "\n\ntrain_templates = [('american_football.football_historical_coach_position', 'from', 'to'), ('architecture.ownership', 'start_date', 'end_date'),\n ('award.award_honor', 'year', 'year'), ('business.employment_tenure', 'from', 'to'), ('business.sponsorship', 'from', 'to'),\n ('celebrities.romantic_relationship', 'start_date', 'end_date'), ('chemistry.chemical_element', 'discovery_date', 'discovery_date'),\n ('film.film', 'initial_release_date', 'initial_release_date'),('government.government_position_held', 'from', 'to'),\n ('law.invention', 'date_of_invention', 'date_of_invention'), ('law.judicial_tenure', 'from_date', 'to_date'),\n ('organization.organization_relationship', 'to', 'from'), ('people.marriage', 'from', 'to'),\n ('people.place_lived', 'end_date', 'start_date'), ('sports.sports_team_coach_tenure', 'from', 'to'),\n ('sports.sports_team_roster', 'from', 'to'), ('sports.team_venue_relationship', 'from', 'to'),", " ('people.place_lived', 'end_date', 'start_date'), ('sports.sports_team_coach_tenure', 'from', 'to'),\n ('sports.sports_team_roster', 'from', 'to'), ('sports.team_venue_relationship', 'from', 'to'),\n ('time.event', 'start_date', 'end_date'), ('tv.regular_tv_appearance', 'from', 'to'), ('tv.tv_network_duration', 'from', 'to')]\n\ntrain_constraints = [('m.0kpys4', 'US State'), ('m.044801x', 'Professional Sports Team'), ('m.01xljyt', 'American Football team'),\n('m.01m9', 'City/Town/Village'), ('m.01xpjyz', 'Airport'), ('m.025dnr9', 'American Football Conference'), ('m.01xs05k', 'River'),\n('m.01xryvm', 'Book'), ('m.01mh', 'Continent'), ('m.01y2hnl', 'College/University'),('m.01xljv1', 'Super bowl'), ('m.01xxv5b', 'Island Group'),\n('m.02_3pws', 'Mexican state'), ('m.025dnqw', 'American Football Division'), ('m.01y2hn6', 'School'), ('m.01n7', 'Location'),\n('m.03jz7ls', 'Written Work'), ('m.08scbsj', 'Subatomic particle'), ('m.03w5clp', 'Production company'), ('m.0kpym_', 'US County'),\n('m.01xljtp', 'Hospital'), ('m.04fnrhx', 'Monarch'), ('m.01xs039', 'Mountain range'), ('m.01mp', 'Country'), ('m.02knxyp', 'Religious Text'),", "('m.03jz7ls', 'Written Work'), ('m.08scbsj', 'Subatomic particle'), ('m.03w5clp', 'Production company'), ('m.0kpym_', 'US County'),\n('m.01xljtp', 'Hospital'), ('m.04fnrhx', 'Monarch'), ('m.01xs039', 'Mountain range'), ('m.01mp', 'Country'), ('m.02knxyp', 'Religious Text'),\n('m.0256985', 'Baseball Team'), ('m.05czz29', 'Brand'), ('m.01nt', 'Region'), ('m.02ht342', 'Automobile Make'), ('m.02_3phk', 'Dutch province')]\n\n\n\nif __name__ == '__main__':\n    parser = Parser()\n    augment_with_s_expr('train')\n    augment_with_s_expr('dev')\n    augment_with_s_expr('test')"]}
{"filename": "DecAF/Datasets/QA/CWQ/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nfrom tqdm import tqdm\nimport argparse\nfrom collections import defaultdict\nfrom DecAF.Datasets.QA.utils import is_valid\n", "from DecAF.Datasets.QA.utils import is_valid\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process CWQ dataset')\nparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ/raw')\nparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n    # process the test data so that each answer contains the \"aliases\" key\n    # this is just for the requirement of the evaluation script\n    infname = os.path.join(args.input_dir, \"ComplexWebQuestions_test.json\")\n    data = json.load(open(infname, \"r\"))\n    for question in data:\n        for answer in question[\"answers\"]:\n            if \"aliases\" not in answer:\n                answer[\"aliases\"] = []\n    json.dump(data, open(infname, \"w\"), indent=2)\n\n    # process the train/dev/test data\n    questions_list = defaultdict(list)\n    for split in ['train', 'dev', 'test']:\n        infname = os.path.join(args.input_dir, f\"ComplexWebQuestions_{split}.json\")\n        data = json.load(open(infname))\n        for question in data:\n            q_obj = {\n                \"QuestionId\": question[\"ID\"],\n                \"Question\": question[\"question\"],\n                \"Answers\": [\n                    {\"freebaseId\": answer[\"answer_id\"],\n                    \"text\": answer[\"answer\"]}\n                    for answer in question[\"answers\"]\n                ]\n            }\n            questions_list[split].append(q_obj)\n    \n    # write down the processed dataset\n    os.makedirs(args.output_dir, exist_ok=True)\n    for key in ['train', 'dev', 'test']:\n        print(f\"{key} has {len(questions_list[key])} questions\")\n        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(questions_list[key], wf, indent=2)", "\nif __name__ == \"__main__\":\n    # process the test data so that each answer contains the \"aliases\" key\n    # this is just for the requirement of the evaluation script\n    infname = os.path.join(args.input_dir, \"ComplexWebQuestions_test.json\")\n    data = json.load(open(infname, \"r\"))\n    for question in data:\n        for answer in question[\"answers\"]:\n            if \"aliases\" not in answer:\n                answer[\"aliases\"] = []\n    json.dump(data, open(infname, \"w\"), indent=2)\n\n    # process the train/dev/test data\n    questions_list = defaultdict(list)\n    for split in ['train', 'dev', 'test']:\n        infname = os.path.join(args.input_dir, f\"ComplexWebQuestions_{split}.json\")\n        data = json.load(open(infname))\n        for question in data:\n            q_obj = {\n                \"QuestionId\": question[\"ID\"],\n                \"Question\": question[\"question\"],\n                \"Answers\": [\n                    {\"freebaseId\": answer[\"answer_id\"],\n                    \"text\": answer[\"answer\"]}\n                    for answer in question[\"answers\"]\n                ]\n            }\n            questions_list[split].append(q_obj)\n    \n    # write down the processed dataset\n    os.makedirs(args.output_dir, exist_ok=True)\n    for key in ['train', 'dev', 'test']:\n        print(f\"{key} has {len(questions_list[key])} questions\")\n        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(questions_list[key], wf, indent=2)"]}
{"filename": "DecAF/Datasets/QA/CWQ/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nfrom collections import Counter\nimport argparse\nfrom DecAF.Preprocess.linearize import load_nameid_dict\nfrom DecAF.Datasets.QA.utils import execute_vanilla_s_expr, revise_only_name\nfrom collections import defaultdict", "from DecAF.Datasets.QA.utils import execute_vanilla_s_expr, revise_only_name\nfrom collections import defaultdict\n\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process dataset')\nparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\n    # Only keep the training data with correct excution results\n    if not os.path.exists(os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train_filtered.expr.json\")):\n        print(\"filtering the training data\")\n        data_file = os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train.expr.json\")\n        with open(data_file, \"r\") as rf:\n            data = json.load(rf)\n        new_data = []\n        for data_i in data:\n            answers = [answer_i[\"answer_id\"] for answer_i in data_i[\"answers\"]]\n            parsing_results = execute_vanilla_s_expr(data_i[\"SExpr\"])\n            if set(answers) == set(parsing_results):\n                new_data.append(data_i)\n        print(\"remaining rate: \", len(new_data) / len(data))\n        save_file = os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train_filtered.expr.json\")\n        with open(save_file, \"w\") as wf:\n            json.dump(new_data, wf, indent=2)\n\n    # preprocess the s-expression\n    num_s_expr = []\n    data_dict = defaultdict(list)\n    none_answer_dict = defaultdict(int)\n\n    ID2LFs = defaultdict(list)\n\n    for split in ['train', 'dev', 'test']:\n        if split == \"train\":\n            infname = os.path.join(args.data_dir, f\"raw/ComplexWebQuestions_{split}_filtered.expr.json\")\n        else:\n            infname = os.path.join(args.data_dir, f\"raw/ComplexWebQuestions_{split}.expr.json\")\n        data = json.load(open(infname))\n\n        for question in data:\n            s_express_list = [question[\"SExpr\"]] if question[\"SExpr\"] != \"null\" else []\n            num_s_expr.append(len(s_express_list))\n            if len(s_express_list) == 0:\n                none_answer_dict[split] += 1\n                if split == \"train\":\n                    continue\n                LFs = {}\n            else:\n                LFs = {\n                    \"LF_original\": s_express_list,\n                    \"LF_processed\": [revise_only_name(s_expr, id2name_dict) for s_expr in s_express_list],\n                } \n            ID2LFs[question[\"ID\"]] = LFs\n\n    print(Counter(num_s_expr))\n\n    for split in ['train', 'dev', 'test']:\n        infname = os.path.join(args.data_dir, f\"{split}.json\")\n        data = json.load(open(infname))\n\n        for question in data:\n            question_id = question[\"QuestionId\"]\n            question.update(ID2LFs[question_id])\n            data_dict[split].append(question)\n\n    # write down the processed dataset\n    os.makedirs(args.data_dir, exist_ok=True)\n    for split in ['train', 'dev', 'test']:\n        print(len(data_dict[split]))\n        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n        with open(outfname, 'w') as f:\n            json.dump(data_dict[split], f, indent=2)", "\n\n"]}
{"filename": "DecAF/Datasets/QA/GrailQA/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nimport argparse\nfrom collections import defaultdict\n\nDATA_DIR = os.environ['DATA_DIR']\n", "DATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process dataset')\nparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA_v1.0/raw')\nparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA_v1.0')\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n    data_dict = defaultdict(list)\n    for split in ['train', 'dev']:\n        infname = os.path.join(args.input_dir, f\"grailqa_v1.0_{split}.json\")\n        data = json.load(open(infname))\n        for question in data:\n            q_obj = {\n                \"QuestionId\": question[\"qid\"],\n                \"Question\": question[\"question\"],\n                \"Answers\": [\n                    {\"freebaseId\": answer[\"answer_argument\"],\n                    \"text\": answer[\"entity_name\"] if \"entity_name\" in answer else answer[\"answer_argument\"]} for answer in question[\"answer\"]\n                ]\n            }\n            data_dict[split].append(q_obj)\n\n    infname = os.path.join(args.input_dir, f\"grailqa_v1.0_test_public.json\")\n    data = json.load(open(infname))\n    for question in data:\n        q_obj = {\n            \"QuestionId\": question[\"qid\"],\n            \"Question\": question[\"question\"],\n            \"Answers\": [\n                {\"freebaseId\": \"None\",\n                    \"text\": \"None\" } \n            ]\n        }\n        data_dict[\"test\"].append(q_obj)\n\n    os.makedirs(args.output_dir, exist_ok=True)\n    for split in ['train', 'dev', 'test']:\n        print(len(data_dict[split]))\n        outfname = os.path.join(args.output_dir, f\"{split}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(data_dict[split], wf, indent=2)"]}
{"filename": "DecAF/Datasets/QA/GrailQA/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nimport argparse\nfrom DecAF.Preprocess.linearize import load_nameid_dict\nfrom DecAF.Datasets.QA.utils import revise_only_name\nfrom collections import defaultdict\nimport logging", "from collections import defaultdict\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process dataset')\nparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA')\nargs = parser.parse_args()", "parser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA')\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\n    data_dict = defaultdict(list)\n    ID2LFs = defaultdict(list)\n    for split in ['train', 'dev']:\n        infname = os.path.join(args.data_dir, f\"raw/grailqa_v1.0_{split}.json\")\n        data = json.load(open(infname))\n        for question in data:\n            assert isinstance(question[\"s_expression\"], str)\n            LFs = {\n                    \"LF_original\": [question[\"s_expression\"]],\n                    \"LF_processed\": [revise_only_name(question[\"s_expression\"], id2name_dict)],\n                } \n            ID2LFs[question[\"qid\"]] = LFs\n    \n    for split in ['train', 'dev', 'test']:\n        infname = os.path.join(args.data_dir, f\"{split}.json\")\n        data = json.load(open(infname))\n\n        for question in data:\n            question_id = question[\"QuestionId\"]\n            question.update(ID2LFs[question_id])\n            data_dict[split].append(question)\n\n    # write down the processed dataset\n    os.makedirs(args.data_dir, exist_ok=True)\n    for split in ['train', 'dev', 'test']:\n        print(len(data_dict[split]))\n        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(data_dict[split], wf, indent=2)", "\n\n"]}
{"filename": "DecAF/Datasets/QA/FreebaseQA/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nfrom collections import defaultdict, Counter\nimport argparse\n\nDATA_DIR = os.environ['DATA_DIR']\n", "DATA_DIR = os.environ['DATA_DIR']\n\n# read data\ndef get_answers(question):\n    \"\"\"extract unique answers from question parses.\"\"\"\n    answers = set()\n    for parse in question[\"Parses\"]:\n        for answer in parse[\"Answers\"]:\n            assert len(answer[\"AnswersName\"]) == 1\n            answers.add((answer[\"AnswersMid\"],\n                answer[\"AnswersName\"][0]))\n    return answers", "\ndef get_entities(question):\n    \"\"\"extract oracle entities from question parses.\"\"\"\n    entities = set()\n    for parse in question[\"Parses\"]:\n        if parse[\"TopicEntityMid\"] is not None:\n            entities.add((parse[\"TopicEntityMid\"], parse[\"TopicEntityName\"]))\n    return entities\n\nparser = argparse.ArgumentParser(description='process dataset')", "\nparser = argparse.ArgumentParser(description='process dataset')\nparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/FreebaseQA/raw')\nparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/FreebaseQA')\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n    questions_list = defaultdict(list)\n    for split in ['train', 'dev', 'test']:\n        num_answer_list = []\n        num_without_answers = 0\n        if split == 'test':\n            infname = os.path.join(args.input_dir, f\"FreebaseQA-eval.json\")\n        else:\n            infname = os.path.join(args.input_dir, f\"FreebaseQA-{split}.json\")\n        data = json.load(open(infname))\n        for question in data[\"Questions\"]:\n            q_obj = {\n                \"QuestionId\": question[\"Question-ID\"],\n                \"Question\": question[\"RawQuestion\"],\n                \"Answers\": [\n                    {\"freebaseId\": answer[0],\n                    \"text\": answer[1]}\n                    for answer in get_answers(question)\n                ]\n            }\n            num_answer_list.append(len(q_obj[\"Answers\"]))\n            # for answer in get_answers(question):\n            #     if not answer[0].startswith(\"m.\"):\n            #         print(answer)\n            if len(get_answers(question)) == 0:\n                num_without_answers += 1\n    \n            questions_list[f\"{split}\"].append(q_obj)\n        print(\"num_without_answers: \", num_without_answers)\n        print(\"answer num distribution: \", Counter(num_answer_list))\n\n    for key in questions_list:\n        print(key, len(questions_list[key]))\n    \n    # write down the processed dataset\n    os.makedirs(args.output_dir, exist_ok=True)\n    for key in [\"train\", \"dev\", \"test\"]:\n        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(questions_list[key], wf, indent=2)", "\n"]}
{"filename": "DecAF/Datasets/QA/WebQSP/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nfrom collections import defaultdict\nimport argparse\n\nDATA_DIR = os.environ['DATA_DIR']\n\ndef get_answers(question):\n    \"\"\"extract unique answers from question parses.\"\"\"\n    answers = set()\n    for parse in question[\"Parses\"]:\n        for answer in parse[\"Answers\"]:\n            answers.add((answer[\"AnswerArgument\"],\n                answer[\"EntityName\"]))\n    return answers", "DATA_DIR = os.environ['DATA_DIR']\n\ndef get_answers(question):\n    \"\"\"extract unique answers from question parses.\"\"\"\n    answers = set()\n    for parse in question[\"Parses\"]:\n        for answer in parse[\"Answers\"]:\n            answers.add((answer[\"AnswerArgument\"],\n                answer[\"EntityName\"]))\n    return answers", "\ndef get_entities(question):\n    \"\"\"extract oracle entities from question parses.\"\"\"\n    entities = set()\n    for parse in question[\"Parses\"]:\n        if parse[\"TopicEntityMid\"] is not None:\n            entities.add((parse[\"TopicEntityMid\"], parse[\"TopicEntityName\"]))\n    return entities\n\nparser = argparse.ArgumentParser(description='process WebQSP dataset')", "\nparser = argparse.ArgumentParser(description='process WebQSP dataset')\nparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP/raw/data')\nparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP')\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n    questions_list = defaultdict(list)\n    for split in ['train', 'test']:\n        num_without_answers = 0\n        infname = os.path.join(args.input_dir, f\"WebQSP.{split}.json\")\n        data = json.load(open(infname))\n        for question in data[\"Questions\"]:\n            q_obj = {\n                \"QuestionId\": question[\"QuestionId\"],\n                \"Question\": question[\"ProcessedQuestion\"],\n                \"Answers\": [\n                    {\"freebaseId\": answer[0],\n                    \"text\": answer[1]}\n                    for answer in get_answers(question)\n                ]\n            }\n            if len(get_answers(question)) == 0:\n                num_without_answers += 1\n\n            questions_list[split].append(q_obj)\n        print(num_without_answers)\n    \n    # write down the processed dataset\n    os.makedirs(args.output_dir, exist_ok=True)\n    for key in [\"train\", \"test\"]:\n        print(f\"{key} has {len(questions_list[key])} questions\")\n        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n        with open(outfname, 'w') as wf:\n            json.dump(questions_list[key], wf, indent=2)", "\n"]}
{"filename": "DecAF/Datasets/QA/WebQSP/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nfrom collections import Counter\nimport argparse\nfrom DecAF.Preprocess.linearize import load_nameid_dict\nfrom DecAF.Datasets.QA.utils import revise_only_name\nfrom collections import defaultdict", "from DecAF.Datasets.QA.utils import revise_only_name\nfrom collections import defaultdict\n\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process dataset')\nparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP')\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\n    # preprocess the s-expression\n    num_s_expr = []\n    data_dict = defaultdict(list)\n    none_answer_dict = defaultdict(int)\n\n    ID2LFs = defaultdict(list)\n\n    for split in ['train', 'test']:\n        infname = os.path.join(args.data_dir, f\"raw/data/WebQSP.{split}.expr.json\")\n        data = json.load(open(infname))\n\n        for question in data:\n            s_express_list = [parse[\"SExpr\"] for parse in question[\"Parses\"] if parse[\"SExpr\"] != \"null\"]\n            num_s_expr.append(len(s_express_list))\n            if len(s_express_list) == 0:\n                none_answer_dict[split] += 1\n                if split == \"train\":\n                    continue\n                LFs = {}\n            else:\n                LFs = {\n                    \"LF_original\": s_express_list,\n                    \"LF_processed\": [revise_only_name(s_expr, id2name_dict) for s_expr in s_express_list],\n                } \n            ID2LFs[question[\"QuestionId\"]] = LFs\n\n    print(Counter(num_s_expr))\n\n    for split in ['train', 'test']:\n        infname = os.path.join(args.data_dir, f\"{split}.json\")\n        data = json.load(open(infname))\n\n        for question in data:\n            question_id = question[\"QuestionId\"]\n            question.update(ID2LFs[question_id])\n            data_dict[split].append(question)\n\n    # write down the processed dataset\n    os.makedirs(args.data_dir, exist_ok=True)\n    for split in ['train', 'test']:\n        print(len(data_dict[split]))\n        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n        with open(outfname, 'w') as f:\n            json.dump(data_dict[split], f, indent=2)", "\n\n"]}
{"filename": "DecAF/Retrieval/utils.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n'''\nevaluation functions for retrieval\n'''", "evaluation functions for retrieval\n'''\n\nimport unicodedata\nfrom collections import defaultdict\nimport numpy as np\nimport regex as re\nfrom tqdm import tqdm\nfrom DecAF.Datasets.QA.utils import parse_answer\n", "from DecAF.Datasets.QA.utils import parse_answer\n\n\ndef has_answer(answers, text, tokenizer, match_type) -> bool:\n    \"\"\"Check if a document contains an answer string.\n    If `match_type` is string, token matching is done between the text and answer.\n    If `match_type` is regex, we search the whole text with the regex.\n    \"\"\"\n    text = _normalize(text)\n\n    if match_type == \"string\":\n        # Answer is a list of possible strings\n        if tokenizer is None:\n            text = text.lower()\n            for single_answer in answers:\n                norm_answer = _normalize(single_answer).lower()\n                if norm_answer in text:\n                    return True\n        else:\n            text = tokenizer.tokenize(text).words(uncased=True)\n\n            for single_answer in answers:\n                single_answer = _normalize(single_answer)\n                single_answer = tokenizer.tokenize(single_answer)\n                single_answer = single_answer.words(uncased=True)\n\n                for i in range(0, len(text) - len(single_answer) + 1):\n                    if single_answer == text[i : i + len(single_answer)]:\n                        return True\n\n    elif match_type == \"regex\":\n        # Answer is a regex\n        for single_answer in answers:\n            single_answer = _normalize(single_answer)\n            if regex_match(text, single_answer):\n                return True\n    return False", "\ndef _normalize(text):\n    return unicodedata.normalize(\"NFD\", text)\n\ndef regex_match(text, pattern):\n    \"\"\"Test if a regex pattern is contained within a text.\"\"\"\n    try:\n        pattern = re.compile(pattern, flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n    except BaseException:\n        return False\n    return pattern.search(text) is not None", "\ndef eval_top_k_one(data_i, top_k=100, tokenizer=None):\n    recall = 0\n    answers = parse_answer(data_i['Answers'], original_name=True)\n    for answer in answers:\n        for ctx in data_i['ctxs'][:top_k]:\n            context = ctx['title'] + \" \" + ctx['text']\n            if has_answer([answer], context, tokenizer, \"string\"):\n                recall += 1\n                break\n    return recall / (len(answers) + 1e-8)", "\ndef recall_ctx(ctx, answers, tokenizer=None):\n    context = ctx['title'] + \" \" + ctx['text']\n    recall = 0\n    for answer in answers:\n        if has_answer([answer], context, tokenizer, \"string\"):\n            recall += 1\n    return recall / (len(answers) + 1e-8)\n\ndef eval_top_k(output_data, top_k_list=[1, 20, 50, 100, 200, 500], tokenizer=None):\n    print(\"Evaluation\")\n    hits_dict = defaultdict(int)\n    recall_dict = defaultdict(float)\n    num_tokens_dict = defaultdict(list)\n    top_k_list = [k for k in top_k_list if k <= len(output_data[0]['ctxs'])]\n    for data_i in tqdm(output_data):\n        for k in top_k_list:\n            recall = eval_top_k_one(data_i, top_k=k, tokenizer=tokenizer)\n            if recall > 0:\n                hits_dict[k] += 1\n            recall_dict[k] += recall\n            num_tokens_dict[k].append(sum([len(ctx[\"text\"].split(\" \"))+len(ctx[\"title\"].split(\" \")) for ctx in data_i['ctxs'][:k]]))\n    for k in top_k_list:\n        print(\"Top {}\".format(k), \n              \"Hits: \", round(hits_dict[k] * 100 / len(output_data), 1), \n              \"Recall: \", round(recall_dict[k] * 100 / len(output_data), 1))", "\ndef eval_top_k(output_data, top_k_list=[1, 20, 50, 100, 200, 500], tokenizer=None):\n    print(\"Evaluation\")\n    hits_dict = defaultdict(int)\n    recall_dict = defaultdict(float)\n    num_tokens_dict = defaultdict(list)\n    top_k_list = [k for k in top_k_list if k <= len(output_data[0]['ctxs'])]\n    for data_i in tqdm(output_data):\n        for k in top_k_list:\n            recall = eval_top_k_one(data_i, top_k=k, tokenizer=tokenizer)\n            if recall > 0:\n                hits_dict[k] += 1\n            recall_dict[k] += recall\n            num_tokens_dict[k].append(sum([len(ctx[\"text\"].split(\" \"))+len(ctx[\"title\"].split(\" \")) for ctx in data_i['ctxs'][:k]]))\n    for k in top_k_list:\n        print(\"Top {}\".format(k), \n              \"Hits: \", round(hits_dict[k] * 100 / len(output_data), 1), \n              \"Recall: \", round(recall_dict[k] * 100 / len(output_data), 1))", ""]}
{"filename": "DecAF/Retrieval/Pyserini/utils.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\n\nINDEX_DIR = os.environ[\"DATA_DIR\"] + \"/knowledge_source\"\nINDEX_MAP_DICT = {\n    \"Freebase\": f\"{INDEX_DIR}/Freebase/processed/index/pyserini_bm25\",\n}", "}"]}
{"filename": "DecAF/Retrieval/Pyserini/search.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nfrom pyserini.search.lucene import LuceneSearcher\nimport json\nfrom tqdm import tqdm\nimport os\nimport argparse\nfrom DecAF.Retrieval.utils import eval_top_k\nfrom DecAF.Retrieval.Pyserini.utils import INDEX_MAP_DICT", "from DecAF.Retrieval.utils import eval_top_k\nfrom DecAF.Retrieval.Pyserini.utils import INDEX_MAP_DICT\nimport multiprocessing.pool\nfrom functools import partial\n\n\ndef print_results(hits):\n    for i in range(len(hits)):\n        print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {hits[i].raw}')\n\nclass Bm25Searcher:\n    def __init__(self, index_dir, args):\n        self.index_dir = index_dir\n        self.args = args\n        try:\n            self.searcher = LuceneSearcher(index_dir)\n        except:\n            print(\"index dir not found\")\n            self.searcher = LuceneSearcher.from_prebuilt_index(index_dir)\n        self.searcher.set_bm25(args.k1, args.b)\n        if len(args.ignore_string) > 0:\n            self.ignore_list = args.ignore_string.split(',')\n            print(f'ignore list: {self.ignore_list}')\n        else:\n            self.ignore_list = []\n    \n    def perform_search(self, data_i, top_k):\n        \n        query = data_i[\"Question\"]\n        for string in self.ignore_list:\n            query = query.replace(string, ' ')\n        query = query.strip()\n        results = self.searcher.search(query, k=top_k)\n\n        ctxs = []\n        for result in results:\n            doc_dict = json.loads(result.raw)\n            ctx_text = doc_dict[\"contents\"]\n            ctx = {\"title\": doc_dict[\"title\"], \"text\": ctx_text, \"score\": result.score}\n            ctxs.append(ctx)\n\n        output_i = data_i.copy()\n        output_i[\"ctxs\"] = ctxs\n        return output_i", "\nclass Bm25Searcher:\n    def __init__(self, index_dir, args):\n        self.index_dir = index_dir\n        self.args = args\n        try:\n            self.searcher = LuceneSearcher(index_dir)\n        except:\n            print(\"index dir not found\")\n            self.searcher = LuceneSearcher.from_prebuilt_index(index_dir)\n        self.searcher.set_bm25(args.k1, args.b)\n        if len(args.ignore_string) > 0:\n            self.ignore_list = args.ignore_string.split(',')\n            print(f'ignore list: {self.ignore_list}')\n        else:\n            self.ignore_list = []\n    \n    def perform_search(self, data_i, top_k):\n        \n        query = data_i[\"Question\"]\n        for string in self.ignore_list:\n            query = query.replace(string, ' ')\n        query = query.strip()\n        results = self.searcher.search(query, k=top_k)\n\n        ctxs = []\n        for result in results:\n            doc_dict = json.loads(result.raw)\n            ctx_text = doc_dict[\"contents\"]\n            ctx = {\"title\": doc_dict[\"title\"], \"text\": ctx_text, \"score\": result.score}\n            ctxs.append(ctx)\n\n        output_i = data_i.copy()\n        output_i[\"ctxs\"] = ctxs\n        return output_i", "\ndef search_all(process_idx, num_process, searcher, args):\n\n    with open(args.query_data_path, 'r') as rf:\n        data = json.load(rf)\n\n    output_data = []\n    for i, data_i in tqdm(enumerate(data)):\n        if i % num_process != process_idx:\n            continue\n        if i > args.num_queries and args.num_queries != -1:\n            break\n\n        output_i = searcher.perform_search(data_i, args.top_k)\n        output_data.append(output_i)\n    return output_data", "\n\n# argparse for root_dir, index_dir, query_data_path, output_dir\nparser = argparse.ArgumentParser(description='Search using pySerini')\nparser.add_argument(\"--index_name\", type=str, default='Wikidata',\n                    help=\"directory to store the search index\")\nparser.add_argument(\"--query_data_path\", type=str, default='/home/ubuntu/data/KBQA/WebQSP/data/WebQSP_processed.test.json',\n                    help=\"directory to store the queries\")\nparser.add_argument(\"--output_dir\", type=str, default='/home/ubuntu/data/KBQA/GeneralKB/Retrieval/pyserini/search_results',\n                    help=\"directory to store the retrieved output\")", "parser.add_argument(\"--output_dir\", type=str, default='/home/ubuntu/data/KBQA/GeneralKB/Retrieval/pyserini/search_results',\n                    help=\"directory to store the retrieved output\")\nparser.add_argument(\"--num_process\", type=int, default=10,\n                    help=\"number of processes to use for multi-threading\")\nparser.add_argument(\"--top_k\", type=int, default=150,\n                    help=\"number of passages to be retrieved for each query\")\nparser.add_argument(\"--ignore_string\", type=str, default=\"\",\n                    help=\"string to ignore in the query, split by comma\")\nparser.add_argument(\"--b\", type=float, default=0.4,\n                    help=\"parameter of BM25\")", "parser.add_argument(\"--b\", type=float, default=0.4,\n                    help=\"parameter of BM25\")\nparser.add_argument(\"--k1\", type=float, default=0.9,\n                    help=\"parameter of BM25\")\nparser.add_argument(\"--num_queries\", type=int, default=1000,\n                    help=\"number of queries to test\")\nparser.add_argument(\"--save\", action=\"store_true\",\n                    help=\"whether to save the output\")\nparser.add_argument(\"--eval\", action=\"store_true\",\n                    help=\"whether to evaluate the output\")", "parser.add_argument(\"--eval\", action=\"store_true\",\n                    help=\"whether to evaluate the output\")\nargs = parser.parse_args()\n\n\nif __name__ == '__main__':\n    if args.index_name in INDEX_MAP_DICT:\n        index_dir = INDEX_MAP_DICT[args.index_name]\n    else:\n        exit(\"no such index\")\n    print(\"index dir: \", index_dir)\n    searcher = Bm25Searcher(index_dir, args)\n\n    num_process = args.num_process\n    pool = multiprocessing.pool.ThreadPool(processes=num_process)\n    sampleData = [x for x in range(num_process)]\n    search_all_part = partial(search_all, \n                                searcher = searcher,\n                                num_process = num_process,\n                                args = args)\n    results = pool.map(search_all_part, sampleData)\n    pool.close()\n\n    output_data = []\n    for result in results:\n        output_data += result\n\n    # sort the output data by question id\n    output_data = sorted(output_data, key=lambda item: item['QuestionId'])\n    tokenizer = None\n    if args.eval:\n        eval_top_k(output_data, top_k_list=[5, 10, 20, 100], tokenizer=tokenizer)\n\n    # save output data\n    # create output dir recursively if not exist\n    if args.save:\n        os.makedirs(args.output_dir, exist_ok=True)\n        output_name = args.query_data_path.split('/')[-1]\n        output_name_split = output_name.split('.')\n        output_name = '.'.join(output_name_split[-2:])\n        output_path = os.path.join(args.output_dir, output_name)\n        print(\"saving output data to {}\".format(output_path))\n        with open(output_path, \"w\") as wf:\n            json.dump(output_data, wf, indent=2)", ""]}
{"filename": "DecAF/Reading/process_fid.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport json\nimport argparse\nfrom tqdm import tqdm\nfrom DecAF.Datasets.QA.utils import parse_answer\n\n", "\n\nparser = argparse.ArgumentParser(description='Process the retrieved data to fit the format of FiD model')\nparser.add_argument(\"--retrieval_data_path\", type=str, \n                    default='/home/ubuntu/data/KBQA/WebQSP/test.json')\nparser.add_argument(\"--mode\", type=str, \n                    default='SPQA', help=\"SPQA or QA or SP\")\nargs = parser.parse_args()\n\n\nfor split in [\"dev\", \"train\", \"test\"]:\n\n    file_path = os.path.join(args.retrieval_data_path, f\"{split}.json\")\n    if not os.path.exists(file_path):\n        continue\n    with open(file_path, \"r\") as rf:\n        data = json.load(rf)\n\n    new_data_qa = []\n    new_data_sp = []\n    for data_i in tqdm(data):\n        if args.mode != \"QA\":\n            if \"LF_processed\" in data_i:\n                new_data_i = {\n                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n                    \"answers\": data_i[\"LF_processed\"],\n                    \"ctxs\": data_i[\"ctxs\"],\n                }\n                new_data_sp.append(new_data_i)\n            elif split != \"train\":\n                new_data_i = {\n                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n                    \"answers\": [\"none\"],\n                    \"ctxs\": data_i[\"ctxs\"],\n                }\n                new_data_sp.append(new_data_i)\n        if args.mode != \"SP\":\n            new_data_i = {\n                \"id\": str(data_i[\"QuestionId\"]) + \":QA\",\n                \"question\": \"Question Answering: \" + data_i[\"Question\"],\n                \"answers\": parse_answer(data_i[\"Answers\"]),\n                \"ctxs\": data_i[\"ctxs\"],\n            }\n            new_data_qa.append(new_data_i)\n    new_data = new_data_qa + new_data_sp\n    print(len(new_data))\n\n    output_file = file_path.replace(\".json\", f\"_fid_{args.mode}.json\")\n    print(output_file)\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)", "\n\nfor split in [\"dev\", \"train\", \"test\"]:\n\n    file_path = os.path.join(args.retrieval_data_path, f\"{split}.json\")\n    if not os.path.exists(file_path):\n        continue\n    with open(file_path, \"r\") as rf:\n        data = json.load(rf)\n\n    new_data_qa = []\n    new_data_sp = []\n    for data_i in tqdm(data):\n        if args.mode != \"QA\":\n            if \"LF_processed\" in data_i:\n                new_data_i = {\n                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n                    \"answers\": data_i[\"LF_processed\"],\n                    \"ctxs\": data_i[\"ctxs\"],\n                }\n                new_data_sp.append(new_data_i)\n            elif split != \"train\":\n                new_data_i = {\n                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n                    \"answers\": [\"none\"],\n                    \"ctxs\": data_i[\"ctxs\"],\n                }\n                new_data_sp.append(new_data_i)\n        if args.mode != \"SP\":\n            new_data_i = {\n                \"id\": str(data_i[\"QuestionId\"]) + \":QA\",\n                \"question\": \"Question Answering: \" + data_i[\"Question\"],\n                \"answers\": parse_answer(data_i[\"Answers\"]),\n                \"ctxs\": data_i[\"ctxs\"],\n            }\n            new_data_qa.append(new_data_i)\n    new_data = new_data_qa + new_data_sp\n    print(len(new_data))\n\n    output_file = file_path.replace(\".json\", f\"_fid_{args.mode}.json\")\n    print(output_file)\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as wf:\n        json.dump(new_data, wf, indent=2)"]}
{"filename": "DecAF/Reading/train_reader.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport time\nimport sys\nimport torch", "import sys\nimport torch\nimport transformers\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\nfrom src.options import Options\n\nimport src.slurm\nimport src.util", "import src.slurm\nimport src.util\nimport src.evaluation\nimport src.data\nimport src.model\n\n\ndef train(model, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path):\n\n    if opt.is_main:\n        try:\n            tb_logger = torch.utils.tensorboard.SummaryWriter(Path(opt.checkpoint_dir)/opt.name)\n        except:\n            tb_logger = None\n            logger.warning('Tensorboard is not available.')\n\n    torch.manual_seed(opt.global_rank + opt.seed) #different seed for different sampling depending on global_rank\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=opt.per_gpu_batch_size,\n        drop_last=True,\n        num_workers=10,\n        collate_fn=collator\n    )\n\n    loss, curr_loss = 0.0, 0.0\n    epoch = 1\n    model.train()\n    while step < opt.total_steps:\n        epoch += 1\n        for i, batch in enumerate(train_dataloader):\n            step += 1\n            (idx, labels, _, context_ids, context_mask) = batch\n\n            train_loss = model(\n                input_ids=context_ids.cuda(),\n                attention_mask=context_mask.cuda(),\n                labels=labels.cuda()\n            )[0]\n\n            train_loss = train_loss / opt.accumulation_steps # add this to average the loss\n            train_loss.backward()\n\n            if step % opt.accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n\n            train_loss = src.util.average_main(train_loss, opt)\n            curr_loss += train_loss.item()\n\n            if step % opt.eval_freq == 0:\n                dev_em = evaluate(model, eval_dataset, tokenizer, collator, opt)\n                model.train()\n                if opt.is_main:\n                    if dev_em > best_dev_em:\n                        best_dev_em = dev_em\n                        src.util.save(model, optimizer, scheduler, step, best_dev_em,\n                                  opt, checkpoint_path, 'best_dev')\n                    log = f\"{step} / {opt.total_steps} |\"\n                    log += f\"train: {curr_loss/opt.eval_freq:.3f} |\"\n                    log += f\"evaluation: {100*dev_em:.2f}EM |\"\n                    log += f\"lr: {scheduler.get_last_lr()[0]:.5f}\"\n                    logger.info(log)    \n                    if tb_logger is not None:\n                        tb_logger.add_scalar(\"Evaluation\", dev_em, step)\n                        tb_logger.add_scalar(\"Training\", curr_loss / (opt.eval_freq), step)\n                    curr_loss = 0.\n\n            if opt.is_main and step % opt.save_freq == 0:\n                src.util.save(model, optimizer, scheduler, step, best_dev_em,\n                          opt, checkpoint_path, f\"step-{step}\")\n            if step > opt.total_steps:\n                break", "\ndef evaluate(model, dataset, tokenizer, collator, opt):\n    sampler = SequentialSampler(dataset)\n    dataloader = DataLoader(dataset,\n        sampler=sampler,\n        batch_size=opt.per_gpu_batch_size,\n        drop_last=False,\n        num_workers=10,\n        collate_fn=collator\n    )\n    model.eval()\n    total = 0\n    exactmatch = []\n    model = model.module if hasattr(model, \"module\") else model\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            (idx, _, _, context_ids, context_mask) = batch\n\n            outputs = model.generate(\n                input_ids=context_ids.cuda(),\n                attention_mask=context_mask.cuda(),\n                max_length=opt.answer_maxlength,\n            )\n\n            for k, o in enumerate(outputs):\n                ans = tokenizer.decode(o, skip_special_tokens=True)\n                gold = dataset.get_example(idx[k])['answers']\n                score = src.evaluation.ems(ans, gold)\n                total += 1\n                exactmatch.append(score)\n\n    exactmatch, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n    return exactmatch", "\nif __name__ == \"__main__\":\n    options = Options()\n    options.add_reader_options()\n    options.add_optim_options()\n    options.parser.add_argument('--total_batch_size', type=int, default=16)\n    opt = options.parse()\n    #opt = options.get_options(use_reader=True, use_optim=True)\n\n    torch.manual_seed(opt.seed)\n    src.slurm.init_distributed_mode(opt)\n    src.slurm.init_signal_handler()\n\n    # set training steps according to total GPU numbers\n    step_scale_size = (opt.total_batch_size // opt.world_size // opt.per_gpu_batch_size)\n    opt.total_steps *= step_scale_size\n    opt.accumulation_steps *= step_scale_size\n    opt.save_freq *= step_scale_size\n    opt.eval_freq *= step_scale_size\n\n    checkpoint_path = Path(opt.checkpoint_dir)/opt.name\n    # checkpoint_exists = checkpoint_path.exists()\n    checkpoint_exists = False\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    checkpoint_path.mkdir(parents=True, exist_ok=True)\n    #if not checkpoint_exists and opt.is_main:\n    #    options.print_options(opt)\n    #checkpoint_path, checkpoint_exists = util.get_checkpoint_path(opt)\n\n    logger = src.util.init_logger(\n        opt.is_main,\n        opt.is_distributed,\n        checkpoint_path / 'run.log'\n    )\n\n    model_name = 't5-' + opt.model_size\n    model_class = src.model.FiDT5\n\n    #load data\n    tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)\n    collator = src.data.Collator(opt.text_maxlength, tokenizer, answer_maxlength=opt.answer_maxlength)\n\n    # use golbal rank and world size to split the eval set on multiple gpus\n    train_examples = src.data.load_data(\n        opt.train_data, \n        global_rank=opt.global_rank, \n        world_size=opt.world_size,\n    )\n    train_dataset = src.data.Dataset(train_examples, opt.n_context)\n    # use golbal rank and world size to split the eval set on multiple gpus\n    eval_examples = src.data.load_data(\n        opt.eval_data,\n        global_rank=opt.global_rank,\n        world_size=opt.world_size,\n    )\n    eval_dataset = src.data.Dataset(eval_examples, opt.n_context)\n\n    if not checkpoint_exists and opt.model_path == \"none\":\n        t5 = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n        model = src.model.FiDT5(t5.config)\n        model.load_t5(t5.state_dict())\n        model = model.to(opt.local_rank)\n        optimizer, scheduler = src.util.set_optim(opt, model)\n        step, best_dev_em = 0, 0.0\n    elif opt.model_path == \"none\":\n        load_path = checkpoint_path / 'checkpoint' / 'latest'\n        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n            src.util.load(model_class, load_path, opt, reset_params=False)\n        logger.info(f\"Model loaded from {load_path}\")\n    else:\n        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n            src.util.load(model_class, opt.model_path, opt, reset_params=True)\n        logger.info(f\"Model loaded from {opt.model_path}\")\n\n    model.set_checkpoint(opt.use_checkpoint)\n\n    if opt.is_distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[opt.local_rank],\n            output_device=opt.local_rank,\n            find_unused_parameters=False,\n        )\n\n    logger.info(\"Start training\")\n    train(\n        model,\n        optimizer,\n        scheduler,\n        step,\n        train_dataset,\n        eval_dataset,\n        opt,\n        collator,\n        best_dev_em,\n        checkpoint_path\n    )", ""]}
{"filename": "DecAF/Reading/test_reader.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport transformers\nimport numpy as np", "import transformers\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, SequentialSampler\n\n\nimport src.slurm\nimport src.util\nfrom src.options import Options\nimport src.data", "from src.options import Options\nimport src.data\nimport src.evaluation\nimport src.model\nfrom tqdm import tqdm\nimport json\n\nclass FiDT5(src.model.FiDT5):\n    def generate(self, input_ids, attention_mask, max_length, \n                    num_beams=1, num_return_sequences=1, use_cache=False):\n        self.encoder.n_passages = input_ids.size(1)\n        \n        return transformers.T5ForConditionalGeneration.generate(\n            self,\n            input_ids=input_ids.view(input_ids.size(0), -1),\n            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n            max_length=max_length,\n            num_beams=num_beams,\n            num_return_sequences=num_return_sequences,\n            use_cache=use_cache,\n        )", "\n\ndef write_output(glob_path, output_path):\n    files = list(glob_path.glob('*.json'))\n    files.sort()\n    with open(output_path, 'w') as outfile:\n        output_data = {}\n        for path in files:\n            with open(path, 'r') as f:\n                data = json.load(f)\n                output_data.update(data)\n            path.unlink()\n        json.dump(output_data, outfile, indent=4)\n    glob_path.rmdir()", "\ndef evaluate(model, dataset, dataloader, tokenizer, opt):\n    loss, curr_loss = 0.0, 0.0\n    model.eval()\n    if hasattr(model, \"module\"):\n        model = model.module\n    if opt.write_crossattention_scores:\n        model.overwrite_forward_crossattention()\n        model.reset_score_storage() \n    total = 0\n    exactmatch = []\n    if opt.write_results:\n        write_path = Path(opt.checkpoint_dir) / opt.name / 'test_results'\n        fw = open(write_path / ('%d.json'%opt.global_rank), 'w')\n        output_results = {}\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(dataloader)):\n            (idx, _, _, context_ids, context_mask) = batch\n\n            if opt.write_crossattention_scores:\n                model.reset_score_storage()\n\n            outputs = model.generate(\n                input_ids=context_ids.cuda(),\n                attention_mask=context_mask.cuda(),\n                max_length=opt.answer_maxlength,\n                num_beams=opt.num_beams,\n                num_return_sequences=opt.num_beams,\n                use_cache=opt.use_cache_eval,\n            )\n            \n            outputs = outputs.reshape(len(idx), -1, outputs.shape[-1])\n            \n            if opt.write_crossattention_scores:\n                crossattention_scores = model.get_crossattention_scores(context_mask.cuda())\n\n            for k, o in enumerate(outputs):\n                ans_list = []\n                for o_i in o:\n                    ans = tokenizer.decode(o_i, skip_special_tokens=True)\n                    ans_list.append(ans)\n                example = dataset.data[idx[k]]\n                if 'answers' in example:\n                    score = src.evaluation.ems(ans_list[0], example['answers'])\n                    exactmatch.append(score)\n\n                if opt.write_results:\n                    # fw.write(str(example['id']) + \"\\t\" + ans + '\\n')\n                    output_results[example[\"id\"]] = {\n                        \"question\": example[\"question\"],\n                        \"gold answers\": example['answers'],\n                        \"predicted answers\": ans_list\n                    }\n                if opt.write_crossattention_scores:\n                    for j in range(context_ids.size(1)):\n                        example['ctxs'][j]['score'] = crossattention_scores[k, j].item()\n\n                total += 1\n            if (i + 1) % opt.eval_print_freq == 0:\n                log = f'Process rank:{opt.global_rank}, {i+1} / {len(dataloader)}'\n                if len(exactmatch) == 0:\n                    log += '| no answer to compute scores'\n                else:\n                    log += f' | average = {np.mean(exactmatch):.3f}'\n                logger.warning(log)\n\n    if opt.write_results:\n        json.dump(output_results, fw, indent=4)\n        fw.close()\n    logger.warning(f'Process rank:{opt.global_rank}, total {total} | average = {np.mean(exactmatch):.3f}')\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    score, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n    \n    return score, total", "\n\nif __name__ == \"__main__\":\n    options = Options()\n    options.add_reader_options()\n    options.add_eval_options()\n    options.parser.add_argument('--num_beams', type=int, default=1)\n    options.parser.add_argument('--use_cache_eval', action='store_true')\n    opt = options.parse()\n    src.slurm.init_distributed_mode(opt)\n    src.slurm.init_signal_handler()\n    opt.train_batch_size = opt.per_gpu_batch_size * max(1, opt.world_size)\n\n    dir_path = Path(opt.checkpoint_dir)/opt.name\n    directory_exists = dir_path.exists()\n    if opt.is_distributed:\n        torch.distributed.barrier()\n    dir_path.mkdir(parents=True, exist_ok=True)\n    if opt.write_results:\n        (dir_path / 'test_results').mkdir(parents=True, exist_ok=True)\n    logger = src.util.init_logger(opt.is_main, opt.is_distributed, Path(opt.checkpoint_dir) / opt.name / 'run.log')\n    if not directory_exists and opt.is_main:\n        options.print_options(opt)\n\n    tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base', return_dict=False)\n\n    collator_function = src.data.Collator(opt.text_maxlength, tokenizer)\n    eval_examples = src.data.load_data(\n        opt.eval_data, \n        global_rank=opt.global_rank, #use the global rank and world size attibutes to split the eval set on multiple gpus\n        world_size=opt.world_size\n    )\n    eval_dataset = src.data.Dataset(\n        eval_examples, \n        opt.n_context, \n    )\n\n    eval_sampler = SequentialSampler(eval_dataset) \n    eval_dataloader = DataLoader(\n        eval_dataset, \n        sampler=eval_sampler, \n        batch_size=opt.per_gpu_batch_size,\n        num_workers=20, \n        collate_fn=collator_function\n    )\n    \n    model_class = FiDT5\n    model = model_class.from_pretrained(opt.model_path)\n    model = model.to(opt.device)\n\n    logger.info(\"Start eval\")\n    exactmatch, total = evaluate(model, eval_dataset, eval_dataloader, tokenizer, opt)\n\n    logger.info(f'EM {100*exactmatch:.2f}, Total number of example {total}')\n\n    if opt.write_results and opt.is_main:\n        glob_path = Path(opt.checkpoint_dir) / opt.name / 'test_results'\n        eval_data_name = opt.eval_data.split('/')[-1].replace('.json', '')\n        write_path = Path(opt.checkpoint_dir) / opt.name / f'final_output_{eval_data_name}.json'\n        write_output(glob_path, write_path) \n    if opt.write_crossattention_scores:\n        src.util.save_distributed_dataset(eval_dataset.data, opt)", "\n"]}
{"filename": "DecAF/Knowledge/process_freebase.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n# SPDX-License-Identifier: CC-BY-NC-4.0\n\nimport os\nimport jsonlines\nimport csv\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom multiprocessing import Pool\nfrom functools import partial", "from multiprocessing import Pool\nfrom functools import partial\nimport argparse\nfrom DecAF.Knowledge.linearize import Relation, convert_relation_to_text\n\nDATA_DIR = os.environ['DATA_DIR']\n\nparser = argparse.ArgumentParser(description='process Freebase')\nparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/knowledge_source/Freebase')\nargs = parser.parse_args()", "parser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/knowledge_source/Freebase')\nargs = parser.parse_args()\n\n\n# disambuity for entity name\n# if the entity name has appeared in previous ids, we add indicator like \"v1\" \"v2\" to the name\nname_dir = os.path.join(args.data_dir, \"id2name_parts\")\noutput_dir = os.path.join(args.data_dir, \"id2name_parts_disamb\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    name_num_dict = defaultdict(int)\n    file_list = os.listdir(name_dir)\n    file_list.sort()\n    for file_name in tqdm(file_list):\n        print(file_name)\n        id_name_list = []\n        with open(os.path.join(name_dir, file_name), 'r') as rf:\n            data_input = csv.reader(rf, delimiter=\"\\t\")\n            for row in data_input:\n                if row[2] not in name_num_dict:\n                    id_name_list.append(row)\n                    name_num_dict[row[2]] += 1\n                else:\n                    new_row = row[:2] + [row[2] + \" v\" + str(name_num_dict[row[2]])]\n                    id_name_list.append(new_row)\n                    name_num_dict[row[2]] += 1\n        # save the list of rows to a new tsv file\n        with open(os.path.join(output_dir, file_name), 'w') as wf:    \n            data_output = csv.writer(wf, delimiter=\"\\t\")\n            for row in id_name_list:\n                data_output.writerow(row)", "if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    name_num_dict = defaultdict(int)\n    file_list = os.listdir(name_dir)\n    file_list.sort()\n    for file_name in tqdm(file_list):\n        print(file_name)\n        id_name_list = []\n        with open(os.path.join(name_dir, file_name), 'r') as rf:\n            data_input = csv.reader(rf, delimiter=\"\\t\")\n            for row in data_input:\n                if row[2] not in name_num_dict:\n                    id_name_list.append(row)\n                    name_num_dict[row[2]] += 1\n                else:\n                    new_row = row[:2] + [row[2] + \" v\" + str(name_num_dict[row[2]])]\n                    id_name_list.append(new_row)\n                    name_num_dict[row[2]] += 1\n        # save the list of rows to a new tsv file\n        with open(os.path.join(output_dir, file_name), 'w') as wf:    \n            data_output = csv.writer(wf, delimiter=\"\\t\")\n            for row in id_name_list:\n                data_output.writerow(row)", "\n\n# load Freebase entity name\nname_dir = os.path.join(args.data_dir, \"id2name_parts_disamb\")\nid2name_dict = {}\nfor file_name in tqdm(os.listdir(name_dir)):\n    with open(os.path.join(name_dir, file_name), 'r') as rf:\n        data_input = csv.reader(rf, delimiter=\"\\t\")\n        for row in data_input:\n            id2name_dict[row[0]] = row[2]", "print(\"number of entities with names: \", len(id2name_dict))\n\n\n# load topic and type information from Freebase\ntopic_dir = os.path.join(args.data_dir, \"topic_entities_parts\")\nid2topic_dict = defaultdict(set)\nfile_list = os.listdir(topic_dir)\n# sort file list\nfile_list.sort()\nfor file_name in tqdm(file_list):\n    with open(os.path.join(topic_dir, file_name), 'r') as rf:\n        for row in rf:\n            row = row.strip().split(\"\\t\")\n            if len(row) != 3:\n                continue\n            if \"web\" in row[1] or \"type.object.name\" in row[1]:\n                continue\n            topic = row[1].split(\".\")[-1].replace('.', ' ').replace('_', ' ')\n            topic += \" : \" + row[2].replace('.', ' ').replace('_', ' ')\n            id2topic_dict[row[0]].add(topic)", "file_list.sort()\nfor file_name in tqdm(file_list):\n    with open(os.path.join(topic_dir, file_name), 'r') as rf:\n        for row in rf:\n            row = row.strip().split(\"\\t\")\n            if len(row) != 3:\n                continue\n            if \"web\" in row[1] or \"type.object.name\" in row[1]:\n                continue\n            topic = row[1].split(\".\")[-1].replace('.', ' ').replace('_', ' ')\n            topic += \" : \" + row[2].replace('.', ' ').replace('_', ' ')\n            id2topic_dict[row[0]].add(topic)", "print(\"number of entities with topics: \", len(id2topic_dict))\n\n\n# transform each entity centric 1-hop subgraph (only out relations) as one passage\ndef transform_triples_group_woid(process_idx, num_process, triple_dir, save_dir):\n    # do not keep CVT node id\n    for file_name in tqdm(os.listdir(triple_dir)):\n        file_id = int(file_name.split(\"-\")[-1])\n        if file_id % num_process != process_idx:\n            continue\n        file_path = os.path.join(triple_dir, file_name)\n        grouped_entity_triples = defaultdict(list)\n        with open(file_path, \"r\") as rf:\n            for row in rf:\n                if len(row.strip().split(\"\\t\")) != 3:\n                    continue\n                triple = Relation(row)\n                if triple.should_ignore(id2name_dict):\n                    continue\n                if triple.obj.startswith(\"m\") and triple.obj not in id2name_dict:\n                    continue\n                subj = triple.subj\n                if triple.subj not in id2name_dict:\n                    triple.subj = \"\"\n                grouped_entity_triples[subj].append(convert_relation_to_text(triple, id2name_dict))\n                \n        with jsonlines.open(os.path.join(save_dir, file_name) + \".jsonl\", \"w\") as wf:\n            data_id = 0\n            for key in grouped_entity_triples:\n                if key in id2name_dict:\n                    name_key = id2name_dict[key]\n                    title = name_key + \" \" + key\n                else:\n                    name_key = \"\"\n                    title = key\n                contents = \" \".join(grouped_entity_triples[key])\n                # add topic information\n                if key in id2topic_dict:\n                    topics = id2topic_dict[key]\n                    topics = [name_key + \" \" + topic + \" .\" for topic in topics]\n                    topics_contents = \" \".join(topics)\n                    contents += \" \" + topics_contents\n                \n                contents = contents.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n                title = title.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n                # split contents by 100 words each chunk\n                contents_split = contents.split(\" \")\n                contents_chunks = [\" \".join(contents_split[i:i+100]) for i in range(0, len(contents_split), 100)]\n                for contents_chunk in contents_chunks:\n                    data_output_i = {\"id\": \"freebase-file{}doc{}\".format(file_id, data_id),\n                                    \"contents\": contents_chunk,\n                                    \"title\": title}\n                    wf.write(data_output_i)\n                    data_id += 1", "\n\n# Multi-process data processing\ndef transform_triples_multip(triple_dir, save_dir):\n    num_process = 10\n    pool = Pool(num_process)\n    sampleData = [x for x in range(num_process)]\n    transform_triples_part = partial(transform_triples_group_woid, \n                                     triple_dir=triple_dir, \n                                     num_process=num_process, \n                                     save_dir=save_dir)\n    pool.map(transform_triples_part, sampleData)\n    pool.close()\n    pool.join()", "\ntriple_dir = os.path.join(args.data_dir, \"triple_edges_parts\")\nsave_dir = os.path.join(args.data_dir, \"processed/document\")\nos.makedirs(save_dir, exist_ok=True)\ntransform_triples_multip(triple_dir, save_dir)"]}
{"filename": "DecAF/Knowledge/linearize.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport csv\nfrom tqdm import tqdm", "import csv\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nclass Relation:\n    def __init__(self, line):\n        if line is None:\n            self.subj = self.rel = self.obj = None\n            return\n        e1, rel, e2 = line.strip().split(\"\\t\")\n        self.subj = e1\n        self.rel = rel\n        self.obj = e2\n    \n    def __hash__(self):\n        return hash((self.subj, self.rel, self.obj))\n        \n    def _filter_relation(self):\n        relation = self.rel\n        if relation == \"type.object.name\":\n            return True\n        return False\n\n    def should_ignore(self, id2name_dict):\n        if self._filter_relation():\n            return True\n        return False\n    \n    def __repr__(self):\n        return f\"Subj: {self.subj}; Rel: {self.rel}; Obj: {self.obj}\"", "\n\ndef convert_relation_to_text(relation, entity_names):\n    if isinstance(relation, Relation):\n        subj, rel, obj = relation.subj, relation.rel, relation.obj\n    else:\n        subj, rel, obj = relation\n\n    # subject\n    if subj in entity_names:\n        subj_surface = entity_names[subj]\n    else:\n        subj_surface = subj\n        \n    # object\n    if obj in entity_names:\n        obj_surface = entity_names[obj]\n    else:\n        obj_surface = obj\n            \n    # relation\n    # e.g. film.film.other_crew\n    # replace '.' and '_' with ' '\n    rel_surface = rel.replace('.', ' ')\n    rel_surface = rel_surface.replace('_', ' ')\n    \n    return ' '.join([subj_surface, rel_surface, obj_surface, '.'])", "\n\n# replace \"{name} v2\" to \"{name}\"\ndef get_raw_name(name_wversion):\n    dict_name = name_wversion.split(\" \")\n    if dict_name[-1].startswith(\"v\") and dict_name[-1][1:].isnumeric():\n        dict_name = \" \".join(dict_name[:-1])\n    else:\n        dict_name = \" \".join(dict_name)\n    return dict_name", "\n\ndef load_nameid_dict(file_dir, lower):\n    print(\"Loading name2id and id2name dict...\")\n    name2id_dict = defaultdict(list)\n    id2name_dict = {}\n    for file in tqdm(os.listdir(file_dir)):\n        with open(os.path.join(file_dir, file), 'r') as rf:\n            data_input = csv.reader(rf, delimiter=\"\\t\")\n            for row in data_input:\n                if lower:\n                    procesed_name = row[2].lower()\n                else:\n                    procesed_name = row[2]\n                name2id_dict[procesed_name].append(row[0])\n                id2name_dict[row[0]] = procesed_name\n    return name2id_dict, id2name_dict"]}
