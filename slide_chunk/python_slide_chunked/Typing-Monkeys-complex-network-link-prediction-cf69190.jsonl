{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\nfrom pathlib import Path\n\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / \"README.md\").read_text()\n\nsetup(\n    name='complex-network-link-prediction',\n    version='1.3',\n    license='MIT',", "    version='1.3',\n    license='MIT',\n    description='A python library for link prediction in social networks',\n    author=\"Cristian Cosci, Fabrizio Fagiolo, Nicol\u00f2 Vescera, Nicol\u00f2 Posta, Tommaso Romani\",\n    packages=find_packages(where='.', include=['cnlp*']),\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/Typing-Monkeys/social-network-link-prediction',\n    keywords='Link Prediction, Social Network, Complex Network Analisys',\n    install_requires=[", "    keywords='Link Prediction, Social Network, Complex Network Analisys',\n    install_requires=[\n        'networkx',\n        'scipy',\n        'numpy',\n    ],\n)\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["from .Configs import Configs\n"]}
{"filename": "tests/Configs.py", "chunked_list": ["import networkx as nx\nfrom sknetwork.data import miserables\n\n\nclass Configs:\n    __dataset = miserables(metadata=True)\n    __graph_normal = None\n    __graph_labels = None\n\n    @staticmethod\n    def load_normal_dataset():\n        if Configs.__graph_normal is None:\n            Configs.__graph_normal = nx.to_networkx_graph(\n                Configs.__dataset['adjacency'])\n\n        return Configs.__graph_normal\n\n    @staticmethod\n    def load_labels_dataset():\n        if Configs.__graph_labels is None:\n            if Configs.__graph_normal is None:\n                Configs.load_normal_dataset()\n\n            names = Configs.__dataset['names']\n            map = {idx: name for idx, name in enumerate(names)}\n\n            Configs.__graph_labels = nx.relabel_nodes(Configs.__graph_normal,\n                                                      map)\n\n        return Configs.__graph_labels", ""]}
{"filename": "tests/unittests/test_othermethods.py", "chunked_list": ["import unittest\nfrom cnlp import other_methods\nfrom tests import Configs\nimport numpy as np\nfrom scipy import sparse\n\n\nclass TestDimensionalityReductionMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        with self.subTest('Int Labels'):\n            res = fun(g, **params)\n\n            if debug:\n                print(res)\n                print(type(res))\n\n            self.assertIsNotNone(res, \"None result is returned\")\n            self.assertTrue(\n                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n                \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        with self.subTest('CMP Results'):\n            try:\n                self.assertTrue(\n                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n                    \"Results are different !\")\n            except AssertionError as e:\n                print(e)\n\n        return res\n\n    def test_MI(self):\n        self.__perform_test(other_methods.information_theory.MI)\n\n    def test_pathentropy(self):\n        self.__perform_test(other_methods.information_theory.path_entropy)", "\n\nif __name__ == '__main__':\n    unittest.main()\n"]}
{"filename": "tests/unittests/__init__.py", "chunked_list": [""]}
{"filename": "tests/unittests/test_dimreduction.py", "chunked_list": ["import unittest\nfrom cnlp import dimensionality_reduction_methods\nfrom tests import Configs\nimport numpy as np\nfrom scipy import sparse\n\n\nclass TestDimensionalityReductionMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        with self.subTest('Int Labels'):\n            res = fun(g, **params)\n\n            if debug:\n                print(res)\n                print(type(res))\n\n            self.assertIsNotNone(res, \"None result is returned\")\n            self.assertTrue(\n                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n                \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        with self.subTest('CMP Results'):\n            try:\n                self.assertTrue(\n                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n                    \"Results are different !\")\n            except AssertionError as e:\n                print(e)\n\n        return res\n\n    def test_SVD(self):\n        self.__perform_test(\n            dimensionality_reduction_methods.link_prediction_svd, {'k': 40})\n\n    def test_NMF(self):\n        self.__perform_test(\n            dimensionality_reduction_methods.link_prediction_nmf, {\n                'num_features': 10,\n                'num_iterations': 100\n            })", "\n    # def setUp(self):\n    #     self.start_time = time()\n\n    # def tearDown(self):\n    #     t = time() - self.start_time\n\n    #     print(f\"{round(t, 2)} s\")\n\n    # # TODO: @ncvesvera @Cosci @posta ricontrollare", "\n    # # TODO: @ncvesvera @Cosci @posta ricontrollare\n    # # @unittest.skip(\"Metodo non ancora implementato\")\n    # def test_svd_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n    #     k = 100\n\n    #     svd = SVD(n_components=k)\n\n    #     embedding = svd.fit_transform(adjacency)", "\n    #     embedding = svd.fit_transform(adjacency)\n    #     our_embedding = dimensionality_reduction_methods.link_prediction_svd(\n    #         g, k=k)\n\n    #     print()\n    #     print(svd.predict(adjacency))\n    #     print()\n    #     print()\n    #     print(embedding)", "    #     print()\n    #     print(embedding)\n    #     print(our_embedding)\n\n    #     self.assertEqual(embedding, our_embedding)\n\n    # @timeout(Configs.timeout)\n    # def test_svd_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n    #     k = 100", "    #     g, adjacency = Configs.load_hard_dataset()\n    #     k = 100\n    #     our_embedding = dimensionality_reduction_methods.link_prediction_svd(\n    #         g, k=k)\n\n    #     self.assertIsNotNone(our_embedding)\n\n\nif __name__ == '__main__':\n    unittest.main()", "if __name__ == '__main__':\n    unittest.main()\n"]}
{"filename": "tests/unittests/test_probabilisticmethods.py", "chunked_list": ["import unittest\nfrom cnlp import probabilistic_methods\nfrom tests import Configs\nimport numpy as np\nfrom scipy import sparse\n\n\nclass TestDimensionalityReductionMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        # g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        # with self.subTest('Int Labels'):\n        #     res = fun(g, **params)\n        #\n        #     if debug:\n        #         print(res)\n        #         print(type(res))\n        #\n        #     self.assertIsNotNone(res, \"None result is returned\")\n        #     self.assertTrue(\n        #         type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n        #         \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        # with self.subTest('CMP Results'):\n        #     try:\n        #         self.assertTrue((res.__round__(4)\n        #                          != res_labels.__round__(4)).nnz == 0,\n        #                         \"Results are different !\")\n        #     except AssertionError as e:\n        #         print(e)\n\n        return res\n\n    def test_sbm(self):\n        self.__perform_test(probabilistic_methods.stochastic_block_model, {\n            'n': 1,\n        })", ""]}
{"filename": "tests/unittests/similarity_methods/test_global.py", "chunked_list": ["import unittest\nfrom cnlp.similarity_methods import global_similarity\nfrom tests import Configs\nfrom scipy import sparse\nimport numpy as np\n\n\nclass TestGlobalSimilarityMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        with self.subTest('Int Labels'):\n            res = fun(g, **params)\n\n            if debug:\n                print(res)\n                print(type(res))\n\n            self.assertIsNotNone(res, \"None result is returned\")\n            self.assertTrue(\n                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n                \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        with self.subTest('CMP Results'):\n            try:\n                self.assertTrue(\n                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n                    \"Results are different !\")\n            except AssertionError as e:\n                print(e)\n\n        return res\n\n    def test_katz(self):\n        self.__perform_test(global_similarity.katz_index, {'beta': .001})\n\n    def test_randwalk(self):\n        self.__perform_test(global_similarity.link_prediction_rwr, {\n            'c': .05,\n            'max_iters': 10\n        })\n\n    def test_rootedpage(self):\n        self.__perform_test(global_similarity.rooted_page_rank, {'alpha': .5})\n\n    def test_shortestpath(self):\n        self.__perform_test(global_similarity.shortest_path, {'cutoff': None})\n\n    def test_simrank(self):\n        self.__perform_test(global_similarity.sim_rank)", "\n    # @timeout(Configs.timeout)\n    # def test_katz_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n    #     beta = .001\n\n    #     res = global_similarity.katz_index(g, beta=beta)\n    #     # print(res)\n\n    #     self.assertIsNotNone(res)", "\n    #     self.assertIsNotNone(res)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"]}
{"filename": "tests/unittests/similarity_methods/test_local.py", "chunked_list": ["import unittest\nfrom cnlp.similarity_methods import local_similarity\nfrom tests import Configs\nfrom scipy import sparse\nimport numpy as np\n\n\nclass TestLocalSimilarityMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        with self.subTest('Int Labels'):\n            res = fun(g, **params)\n\n            if debug:\n                print(res)\n                print(type(res))\n\n            self.assertIsNotNone(res, \"None result is returned\")\n            self.assertTrue(\n                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n                \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        with self.subTest('CMP Results'):\n            try:\n                self.assertTrue(\n                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n                    \"Results are different !\")\n            except AssertionError as e:\n                print(e)\n\n        return res\n\n    def test_common_neighbors(self):\n        self.__perform_test(local_similarity.common_neighbors)\n\n    def test_adamicadar(self):\n        self.__perform_test(local_similarity.adamic_adar)\n\n    def test_jaccard(self):\n        self.__perform_test(local_similarity.jaccard)\n\n    def test_sorensen(self):\n        self.__perform_test(local_similarity.sorensen)\n\n    def test_hubpromoted(self):\n        self.__perform_test(local_similarity.hub_promoted)\n\n    def test_hubdepressed(self):\n        self.__perform_test(local_similarity.hub_depressed)\n\n    def test_resourceallocation(self):\n        self.__perform_test(local_similarity.resource_allocation)\n\n    def test_prefattachment(self):\n        self.__perform_test(local_similarity.preferential_attachment)\n\n    def test_cosine(self):\n        self.__perform_test(local_similarity.cosine_similarity)\n\n    def test_nodeclustering(self):\n        self.__perform_test(local_similarity.node_clustering)", "\n    # def setUp(self):\n    #     self.start_time = time()\n\n    # def tearDown(self):\n    #     t = time() - self.start_time\n    #     print(f\"{round(t, 2)} s\")\n\n    # def __perform_sktest(self,\n    #                      our_values,", "    # def __perform_sktest(self,\n    #                      our_values,\n    #                      skfunction,\n    #                      samples_range,\n    #                      samples_number=20,\n    #                      decimal_precision=3):\n    #     indxes = [(random.randrange(samples_range),\n    #                random.randrange(samples_range))\n    #               for a in range(samples_number)]\n", "    #               for a in range(samples_number)]\n\n    #     for i, j in indxes:\n    #         expected = skfunction.predict((i, j))\n    #         our_res = our_values[i, j]\n\n    #         self.assertAlmostEqual(expected, our_res, decimal_precision)\n\n    # def test_common_neighbors_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()", "    # def test_common_neighbors_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n    #     cn = CommonNeighbors()\n\n    #     cn.fit(adjacency)\n    #     our_sim = local_similarity.common_neighbors(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0], 20)\n\n    # def test_common_neighbors_2(self):", "\n    # def test_common_neighbors_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = CommonNeighbors()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.common_neighbors(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])", "\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_common_neighbors_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n    #     cn = CommonNeighbors()\n\n    #     cn.fit(adjacency)\n    #     our_sim = local_similarity.common_neighbors(g)", "    #     cn.fit(adjacency)\n    #     our_sim = local_similarity.common_neighbors(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_jaccard_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = JaccardIndex()\n    #     cn.fit(adjacency)", "    #     cn = JaccardIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.jaccard(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_jaccard_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n", "    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = JaccardIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.jaccard(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)", "\n    # @timeout(Configs.timeout)\n    # def test_jaccard_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = JaccardIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.jaccard(g)\n", "    #     our_sim = local_similarity.jaccard(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_sorensen_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = SorensenIndex()\n    #     cn.fit(adjacency)\n", "    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.sorensen(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_sorensen_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = SorensenIndex()", "\n    #     cn = SorensenIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.sorensen(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_sorensen_3(self):", "    # @timeout(Configs.timeout)\n    # def test_sorensen_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = SorensenIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.sorensen(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])", "\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_hubpromoted_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = HubPromotedIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_promoted(g)", "\n    #     our_sim = local_similarity.hub_promoted(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_hubpromoted_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = HubPromotedIndex()\n    #     cn.fit(adjacency)", "    #     cn = HubPromotedIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_promoted(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_hubpromoted_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()", "    # def test_hubpromoted_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = HubPromotedIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_promoted(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n", "    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_hubdepressed_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = HubDepressedIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_depressed(g)\n", "    #     our_sim = local_similarity.hub_depressed(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_hubdepressed_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = HubDepressedIndex()\n    #     cn.fit(adjacency)\n", "    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_depressed(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_hubdepressede_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n", "    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = HubDepressedIndex()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.hub_depressed(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_adamicadar_1(self):", "\n    # def test_adamicadar_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = AdamicAdar()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.adamic_adar(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])", "\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_adamicadar_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = AdamicAdar()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.adamic_adar(g)", "\n    #     our_sim = local_similarity.adamic_adar(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_adamicadar_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = AdamicAdar()", "\n    #     cn = AdamicAdar()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.adamic_adar(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_resourceallocation_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()", "    # def test_resourceallocation_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = ResourceAllocation()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.resource_allocation(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n", "    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_resourceallocation_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = ResourceAllocation()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.resource_allocation(g)\n", "    #     our_sim = local_similarity.resource_allocation(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_resourceallocation_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = ResourceAllocation()\n    #     cn.fit(adjacency)", "    #     cn = ResourceAllocation()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.resource_allocation(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_preferentialattachment_1(self):\n    #     g, adjacency = Configs.load_easy_dataset()\n", "    #     g, adjacency = Configs.load_easy_dataset()\n\n    #     cn = PreferentialAttachment()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.preferential_attachment(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # def test_preferentialattachment_2(self):", "\n    # def test_preferentialattachment_2(self):\n    #     g, adjacency = Configs.load_medium_dataset()\n\n    #     cn = PreferentialAttachment()\n    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.preferential_attachment(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])", "\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @timeout(Configs.timeout)\n    # def test_preferentialattachment_3(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     cn = PreferentialAttachment()\n    #     cn.fit(adjacency)\n", "    #     cn.fit(adjacency)\n\n    #     our_sim = local_similarity.preferential_attachment(g)\n\n    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_cosine_1(self):\n    #     pass\n", "    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_cosine_2(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_cosine_3(self):\n    #     pass\n", "    #     pass\n\n    # @timeout(Configs.timeout)\n    # def test_cosine_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     our_sim = local_similarity.cosine_similarity(g)\n\n    #     self.assertIsNotNone(our_sim)\n", "    #     self.assertIsNotNone(our_sim)\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_nodeclustering_1(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_nodeclustering_2(self):\n    #     pass\n", "    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_nodeclustering_3(self):\n    #     pass\n\n    # @timeout(Configs.timeout)\n    # def test_nodeclustering_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n", "    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     our_sim = local_similarity.node_clustering(g)\n\n    #     self.assertIsNotNone(our_sim)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", ""]}
{"filename": "tests/unittests/similarity_methods/__init__.py", "chunked_list": [""]}
{"filename": "tests/unittests/similarity_methods/test_quasi_local.py", "chunked_list": ["import unittest\nfrom cnlp.similarity_methods import quasi_local_similarity\nfrom tests import Configs\nfrom scipy import sparse\nimport numpy as np\n\n\nclass TestQuasiGlobalSimilarityMethods(unittest.TestCase):\n\n    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n        g = Configs.load_normal_dataset()\n        g_labels = Configs.load_labels_dataset()\n\n        res = None\n        res_labels = None\n\n        with self.subTest('Int Labels'):\n            res = fun(g, **params)\n\n            if debug:\n                print(res)\n                print(type(res))\n\n            self.assertIsNotNone(res, \"None result is returned\")\n            self.assertTrue(\n                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n                \"Wrong return type\")\n\n        with self.subTest('String Labels'):\n            res_labels = fun(g_labels, **params)\n\n            if debug:\n                print(res_labels)\n                print(type(res_labels))\n\n            self.assertIsNotNone(res_labels, \"None result is returned\")\n            self.assertTrue(\n                type(res_labels) is sparse.csr_matrix\n                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\n        with self.subTest('CMP Results'):\n            try:\n                self.assertTrue(\n                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n                    \"Results are different !\")\n            except AssertionError as e:\n                print(e)\n\n        return res\n\n    def test_LPI(self):\n        self.__perform_test(quasi_local_similarity.local_path_index, {\n            'epsilon': .1,\n            'n': 10\n        })\n\n    def test_PL3(self):\n        self.__perform_test(quasi_local_similarity.path_of_length_three)", "\n    # def setUp(self):\n    #     self.start_time = time()\n\n    # def tearDown(self):\n    #     t = time() - self.start_time\n\n    #     print(f\"{round(t, 2)} s\")\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")", "\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_lpi_1(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_lpi_2(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")", "\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_lpi_3(self):\n    #     pass\n\n    # # TODO: @ncvesvera @fagiolo ricontrollare\n    # @timeout(Configs.timeout)\n    # def test_lpi_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n    #     epsilon = .1", "    #     g, adjacency = Configs.load_hard_dataset()\n    #     epsilon = .1\n    #     n = 10\n\n    #     res = quasi_local_similarity.local_path_index(g, epsilon, n)\n    #     # print(res)\n\n    #     self.assertIsNotNone(res)\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")", "\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_pol3_1(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_pol3_2(self):\n    #     pass\n\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")", "\n    # @unittest.skip(\"Non \u00e8 presente in scikit-network\")\n    # def test_pol3_3(self):\n    #     pass\n\n    # @timeout(Configs.timeout)\n    # def test_pol3_time(self):\n    #     g, adjacency = Configs.load_hard_dataset()\n\n    #     res = quasi_local_similarity.path_of_length_three(g)", "\n    #     res = quasi_local_similarity.path_of_length_three(g)\n    #     # print(res)\n\n    #     self.assertIsNotNone(res)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", ""]}
{"filename": "cnlp/dimensionality_reduction_methods.py", "chunked_list": ["\"\"\"Dimentionality Reduction based Methods for Link Prediction.\n\nThe curse of dimensionality is a well-known\nproblem in machine learning.\nSome researchers employ dimension reduction techniques\nto tackle the above problem and apply it in the link prediction scenario.\n\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom scipy.sparse import csr_matrix, spdiags", "import numpy as np\nfrom scipy.sparse import csr_matrix, spdiags\nfrom cnlp.utils import to_adjacency_matrix, only_unconnected\nfrom scipy.sparse.linalg import svds\n\n\ndef link_prediction_svd(G: nx.Graph,\n                        k: int = 5,\n                        normalize: bool = False) -> csr_matrix:\n    \"\"\"Compute the SVD Decomposition for the Graph Adjacency Matrix.\n    The similarity decinoisutuin is defined as:\n\n    .. math::\n        X_\\\\pm \\\\approx F G^T\n\n    where \\\\(F \\\\in \\\\mathbb{R}^{p \\\\times k}\\\\) contains\n    the bases of the latent space and is called the basis matrix;\n    \\\\(G \\\\in \\\\mathbb{R}^{n \\\\times k}\\\\) contains combination of coefficients\n    of the bases for reconstructing the matrix \\\\(X\\\\), and is called\n    the coefficient matrix; \\\\(k\\\\) is the dimention of the latent space\n    (\\\\(k<n\\\\)) and \\\\(n\\\\) is the nunber of data vector\n    (as columns) in \\\\(X\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    k: int :\n        dimention of the latent space (must be \\\\(< n\\\\))\n         (Default value = 5)\n    normalize: bool :\n        if True, normalize the output values\n         (Default value = False)\n\n    Returns\n    -------\n    predicted_adj_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    Typically, the latent features are extracted and using these features,\n    each vertex is represented in latent space, and such representations are\n    used in a supervised or unsupervised framework for link prediction.\n    To further improve the prediction results, some additional node/link or\n    other attribute information can be used.\n\n    In most of the works, non-negative matrix factorization has been used.\n    Some authors also applied the singular value decomposition technique.\n    \"\"\"\n\n    # Create the adjacency matrix of the graph\n    adj_matrix = to_adjacency_matrix(G)\n\n    # Perform the singular value decomposition\n    U, S, Vt = svds(csr_matrix(adj_matrix).astype(float), k=k)\n\n    # Make the diagonal matrix sparse\n    S = spdiags(S, [0], k, k)\n\n    # Compute the predicted adj_matrix\n    predicted_adj_matrix = ((U @ S) @ Vt)\n\n    if normalize:\n        # Normalize the predicted edge weights to the range [0, 1]\n        rows, cols = np.nonzero(predicted_adj_matrix)\n        max_weight = np.max(predicted_adj_matrix[rows, cols])\n        for i in range(len(rows)):\n            predicted_adj_matrix[rows[i], cols[i]] /= max_weight\n\n    return only_unconnected(G, csr_matrix(predicted_adj_matrix))", "\n\ndef link_prediction_nmf(graph: nx.Graph,\n                        num_features: int = 2,\n                        num_iterations: int = 100,\n                        seed: int = 69) -> csr_matrix:\n    \"\"\"Compute the _Non-negative Matrix Factorization_ Decomposition for the Graph Adjacency Matrix.\n    The similarity decinoisutuin is defined as:\n\n    .. math::\n        X_\\\\pm \\\\approx F_+ G^T_+\n\n    where \\\\(F \\\\in \\\\mathbb{R}^{p \\\\times k}\\\\) contains\n    the bases of the latent space and is called the basis matrix;\n    \\\\(G \\\\in \\\\mathbb{R}^{n \\\\times k}\\\\) contains combination of coefficients\n    of the bases for reconstructing the matrix \\\\(X\\\\), and is called\n    the coefficient matrix; \\\\(k\\\\) is the dimention of the latent space\n    ( \\\\(k<n\\\\) ) and \\\\(n\\\\) is the nunber of data vector\n    (as columns) in \\\\(X\\\\).\n\n    Parameters\n    ----------\n    graph: nx.Graph :\n        input Graph (a networkx Graph)\n    num_features: int :\n        dimention of the latent space (must be \\\\(< n\\\\))\n         (Default value = 2)\n    num_iterations: int :\n        max number of iteration for the algorithm convergence\n         (Default value = 100)\n    sedd: int :\n        the seed for the random initialization\n         (Default value = 69)\n\n    Returns\n    -------\n    predicted_adj_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    Typically, the latent features are extracted and using these features,\n    each vertex is represented in latent space, and such representations are\n    used in a supervised or unsupervised framework for link prediction.\n    To further improve the prediction results, some additional node/link or\n    other attribute information can be used.\n\n    In most of the works, non-negative matrix factorization has been used.\n    Some authors also applied the singular value decomposition technique.\n    \"\"\"\n\n    adj_matrix = to_adjacency_matrix(graph, sparse=False)\n\n    # Initialize the feature and coefficient matrices randomly\n    num_nodes = adj_matrix.shape[0]\n\n    np.random.seed(seed)\n\n    feature_matrix = np.random.rand(num_nodes, num_features)\n\n    coefficient_matrix = np.random.rand(num_features, num_nodes)\n\n    # Perform NMF using multiplicative updates\n    for i in range(num_iterations):\n        # Update the coefficient matrix\n        numerator = feature_matrix.T @ adj_matrix\n        denominator = (\n            (feature_matrix.T @ feature_matrix) @ coefficient_matrix)\n\n        coefficient_matrix *= (numerator / denominator)\n\n        numerator = adj_matrix @ coefficient_matrix.T\n        denominator = (\n            feature_matrix @ (coefficient_matrix @ coefficient_matrix.T))\n\n        feature_matrix *= (numerator / denominator)\n\n    # Compute the predicted adjacency matrix\n    predicted_adj_matrix = feature_matrix @ coefficient_matrix\n\n    return only_unconnected(graph, csr_matrix(predicted_adj_matrix))", "\n\nif __name__ == \"__main__\":\n    # Load a test graph\n    graph = nx.karate_club_graph()\n\n    # Compute the SVD of the similarity matrix and do link prediction on it\n    predicted_adj_matrix = link_prediction_nmf(graph)\n\n    adj_matrix = nx.to_numpy_array(graph)\n\n    # Initialize the feature and coefficient matrices randomly\n    num_nodes = adj_matrix.shape[0]\n    # Get the edges with the highest predicted probabilities\n    edges = []\n    for i in range(num_nodes):\n        for j in range(i + 1, num_nodes):\n            prob = predicted_adj_matrix[i, j]\n            edges.append((i, j, prob))\n\n    edges = sorted(edges, key=lambda x: x[2], reverse=True)[:10]\n\n    # Normalize the probabilities to be between 0 and 100\n    max_prob = max([e[2] for e in edges])\n    for edge in edges:\n        edge_prob = (edge[2] / max_prob) * 100\n        print(f\"({edge[0]}, {edge[1]}) - Probability: {edge_prob:.2f}%\")", ""]}
{"filename": "cnlp/probabilistic_methods.py", "chunked_list": ["\"\"\"Collection of Probabilistic Methods for Link Prediction.\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom networkx.algorithms.community import louvain_communities\nfrom cnlp.utils import to_adjacency_matrix\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom scipy.special import comb\nfrom typing import List, Set\nfrom itertools import product, combinations_with_replacement\nfrom cnlp.utils import nodes_to_indexes", "from itertools import product, combinations_with_replacement\nfrom cnlp.utils import nodes_to_indexes\n\n\ndef stochastic_block_model(G: nx.Graph,\n                           n: int,\n                           p: float = .05,\n                           seed: int = 42) -> csr_matrix:\n    \"\"\"Compute the Sotchastic Block Model Similarity for\n    all the nodes in the network.\n\n    This similarity is defined as:\n\n    .. math::\n        R_{xy} = \\\\frac{1}{Z} \\\\sum_{P \\\\in P^*}\n            ( \\\\frac{l^1_{\\\\sigma_x \\\\sigma_y}+1}{r^0_{\\\\sigma_x \\\\sigma_y} +2}) exp[-H(P)]\n\n    where the sum is over all possible partitions \\\\(P^*\\\\) of the network\n    into groups, \\\\( \\\\sigma_x \\\\) and \\\\( \\\\sigma_y \\\\) are vertices \\\\(x\\\\)\n    and \\\\(y\\\\) groups in partition \\\\(P\\\\) respectively.\n    Moreover, \\\\(l^0_{ \\\\sigma_{\\\\alpha} \\\\sigma_{\\\\beta}} \\\\)\n    and \\\\(r^0_{ \\\\sigma_{\\\\alpha} \\\\sigma_{\\\\beta}} \\\\)\n    are the number of links and maximum possible links in the observed network\n    between groups \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    n: int :\n        number of samples\n    p: float :\n        chaos probability, higher means more chaos in\n        the matrix\n         (Default value = .05)\n    seed: int :\n        seed for the random generated values\n         (Default value = 42)\n\n    Returns\n    -------\n    R: csr_matrix : the Similarity Matrix (in sparse format)\n    \"\"\"\n\n    def __random_changes(A_0: csr_matrix, p: float = .05) -> lil_matrix:\n        \"\"\"Perform random changes in the Adjacency Matrix\n\n        This function is used to generate chaos in the network\n        for the sample (blocks) collection.\n\n        Parameters\n        ----------\n        A_0: csr_matrix :\n            the original Adjacency Matrix\n        p: float :\n            chaos probability, higher means more\n            chaos\n             (Default value = 0.05)\n\n        Returns\n        -------\n        A_chaos: lil_matrix : the new chaos matrix\n        \"\"\"\n        A_chaos = A_0.copy().tolil()\n        num_changes = int(A_chaos.shape[0]**2 * p)\n        indexes = []\n\n        while len(indexes) < num_changes:\n            x = np.random.randint(0, high=A_chaos.shape[0])\n            y = np.random.randint(0, high=A_chaos.shape[0])\n\n            if x == y:\n                continue\n\n            if (x, y) in indexes or (y, x) in indexes:\n                continue\n\n            indexes.append((x, y))\n\n        for x, y in indexes:\n            A_chaos[x, y] = abs(A_0[x, y] - 1)\n            A_chaos[y, x] = A_chaos[x, y]\n\n        return A_chaos\n\n    def __generate_samples(A_0: csr_matrix,\n                           n: int,\n                           p: np.float32 = .05,\n                           seed: int = 42) -> List[List[Set]]:\n        \"\"\"Generate the samples (block) given the orginal\n        adjacecy matrix.\n\n        Parameters\n        ----------\n        A_0: csr_matrix :\n            the original Adjacency Matrix\n        n: int :\n            number of samples\n        p: float :\n            chaos probability, higher means more\n            chaos\n             (Default value = 0.05)\n        seed: int :\n            seed for random generated values\n             (Default values = 42)\n\n        Returns\n        -------\n        samples_list: List[List[Set]] : all the samples\n        \"\"\"\n        samples_list = []\n\n        for _ in range(n):\n            A_chaos = __random_changes(A_0, p)\n            G_chaos = nx.Graph(A_chaos)\n            res = louvain_communities(G_chaos, seed)\n            samples_list.append(res)\n\n        return samples_list\n\n    def __get_node_block(sample: List[Set], x: int) -> Set:\n        \"\"\"Return the block that node X belongs to\n\n        Parameters\n        ----------\n        samole: List[Set] :\n            current sample to analize\n        x: int :\n            node\n\n        Returns\n        -------\n        Set : the block\n        \"\"\"\n        for i in sample:\n            if x in i:\n                return i\n\n    def __l(A_0: lil_matrix, alpha: Set, beta: Set) -> int:\n        \"\"\"Number of links between groups Alpha and Beta\"\"\"\n        return np.sum([A_0[x, y] for x, y in product(alpha, beta)])\n\n    def __r(alpha: Set, beta: Set) -> int:\n        \"\"\"Maxmimum possible links between groups Alpha and Beta\"\"\"\n        len_a = len(alpha)\n        len_b = len(beta)\n\n        if alpha == beta:\n            return len_a * (len_a - 1)\n\n        return (len_a * len_b)\n\n    def __H(A_0: csr_matrix, P: List[Set]) -> float:\n        res = 0\n        for a, b in combinations_with_replacement(P, 2):\n            r = __r(a, b)\n            l0 = __l(A_0, a, b)\n\n            res += np.log(r + 1) + np.log(comb(r, l0))\n\n        return res\n\n    def __link_reliability(A_0: csr_matrix, samples_list: List[List[Set]],\n                           x: int, y: int) -> float:\n        summing_result = 0\n        HP_mem = []\n\n        for sample in samples_list:\n            x_block = __get_node_block(sample, x)\n            y_block = __get_node_block(sample, y)\n\n            l_xy = __l(A_0, x_block, y_block) + 1\n            r_xy = __r(x_block, y_block) + 2\n\n            second_term = np.exp(-__H(A_0, sample))\n            HP_mem.append(second_term)\n\n            tmp_ratio = l_xy / r_xy\n\n            summing_result += tmp_ratio * second_term\n\n        return summing_result / np.sum(HP_mem)\n\n    np.random.seed(seed)\n\n    A_0 = to_adjacency_matrix(G)\n    samples_list = __generate_samples(A_0, n, p, seed=seed)\n    nodes_to_indexes_map = nodes_to_indexes(G)\n\n    R = lil_matrix(A_0.shape)\n\n    for x, y in nx.complement(G).edges():\n        _x = nodes_to_indexes_map[x]\n        _y = nodes_to_indexes_map[y]\n\n        R[_x, _y] = __link_reliability(A_0, samples_list, _x, _y)\n\n    return R.tocsr()", "\n\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n\n    G = nx.karate_club_graph()\n    A = to_adjacency_matrix(G)\n\n    res = stochastic_block_model(G, 10)\n\n    predicted_edges = []\n    for u in range(res.shape[0]):\n        for v in range(u + 1, res.shape[1]):\n            if G.has_edge(u, v):\n                continue\n            w = res[u, v]\n            predicted_edges.append((u, v, w))\n\n    # Sort the predicted edges by weight in descending order\n    predicted_edges.sort(key=lambda x: x[2], reverse=True)\n\n    # Print the predicted edges with their weight score\n    print(\"Top Predicted edges:\")\n    for edge in predicted_edges[:50]:\n        print(f\"({edge[0]}, {edge[1]}): {edge[2]}\")\n\n    nx.draw(G)\n    plt.show()", ""]}
{"filename": "cnlp/__init__.py", "chunked_list": [""]}
{"filename": "cnlp/utils.py", "chunked_list": ["\"\"\"Some utilities functions.\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom typing import Union\nfrom scipy.sparse import csr_matrix, csc_matrix, lil_matrix\nfrom typing import Dict, List, Tuple\n\n\ndef nodes_to_indexes(G: nx.Graph) -> dict[any, int]:\n    \"\"\"Node Label - Index encoder\n\n    Associate, for each node label, and index starting from 0.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        the graph from which you want the node-to-index mapping\n\n    Returns\n    -------\n    Dict[Any, int]: the encoding Node Label - Index dictionary\n\n    Notes\n    -----\n    The method `Graph.nodes` return the nodes in the exactly same order, and\n    the first node (at index 0) represent the index 0 in the Adjacency Matrix\n    obtained with the method `Graph.to_adjacency_matrix` or\n    `Graph.to_numpy_array`.\n    \"\"\"\n    return {node_name: index for index, node_name in enumerate(G.nodes)}", "def nodes_to_indexes(G: nx.Graph) -> dict[any, int]:\n    \"\"\"Node Label - Index encoder\n\n    Associate, for each node label, and index starting from 0.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        the graph from which you want the node-to-index mapping\n\n    Returns\n    -------\n    Dict[Any, int]: the encoding Node Label - Index dictionary\n\n    Notes\n    -----\n    The method `Graph.nodes` return the nodes in the exactly same order, and\n    the first node (at index 0) represent the index 0 in the Adjacency Matrix\n    obtained with the method `Graph.to_adjacency_matrix` or\n    `Graph.to_numpy_array`.\n    \"\"\"\n    return {node_name: index for index, node_name in enumerate(G.nodes)}", "\n\ndef to_adjacency_matrix(G: nx.Graph,\n                        sparse: bool = True) -> Union[csc_matrix, np.ndarray]:\n    \"\"\"Convert a ginven Graph in to its Adjacency Matrix\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    sparse: bool:\n        if True, return the Adjacency Matrix in sparse format,\n        otherwise in full format.\n         (Default value = True)\n\n    Returns\n    -------\n    csc_matrix | np.ndarray: the Adjacency Matrix\n    \"\"\"\n    # TODO: ricontrollare se i pesi servono\n    return nx.adjacency_matrix(\n        G, weight=None) if sparse else nx.to_numpy_array(G, weight=None)", "\n\ndef only_unconnected(graph: nx.Graph, sim_matrix: lil_matrix) -> csr_matrix:\n    \"\"\"Filter the given matrix and return only previously unconnected\n    nodes \"similarity\" values\n\n    Parameters\n    ----------\n    graph: nx.Graph :\n        input graph\n    sim_matrix: csr_matrix :\n        similarity matrix\n\n    Returns\n    -------\n    sim_matrix: csr_matrix : the similarity matrix without the previously\n    connected nodes similarity\n    \"\"\"\n    node_idexies_map = nodes_to_indexes(graph)\n\n    for x, y in graph.edges():\n        sim_matrix[node_idexies_map[x], node_idexies_map[y]] = 0\n\n    sim_matrix = sim_matrix.tocsr()\n    sim_matrix.eliminate_zeros()\n\n    return sim_matrix.tocsr()", "\n\ndef get_top_predicted_link(predicted_adj_matrix: csr_matrix,\n                           number_of_nodes: int,\n                           pct_new_link: float,\n                           name_index_map: Dict[any, int],\n                           verbose: bool = False) -> List[Tuple[any, any]]:\n    \"\"\"Get the edges (new link) with the highest predicted probabilities\n                           verbose: bool = False):\n\n    Parameters\n    ----------\n    predicted_adj_matrix: csr_matrix,\n        input Similarity Matrix\n    number_of_nodes: int :\n        number of node in the graph\n    pct_new_link: float :\n        top x% best new links\n    name_index_map: Dict[any, int] :\n        node to index (starting from 0) mapping\n    verbose: bool :\n        if True print some usefull outputs\n         (Default value = False)\n\n    Returns\n    -------\n    new_link: List[Tuple[any, any]] : top % new link predicted\n    \"\"\"\n    max_possible_edges = predicted_adj_matrix.nnz\n    number_of_new_link = int(np.ceil(max_possible_edges / 100 * pct_new_link))\n    edges = []\n    new_link = []\n    for i in range(number_of_nodes):\n        for j in range(i + 1, number_of_nodes):\n            prob = predicted_adj_matrix[i, j]\n            edges.append((i, j, prob))\n\n    edges = sorted(edges, key=lambda x: x[2],\n                   reverse=True)[:number_of_new_link]\n\n    new_link = []\n    for edge in edges:\n        if verbose:\n            print(\n                f\"({name_index_map[edge[0]][0]}, {name_index_map[edge[1]][0]}) - Similarity: {edge[2]:.2f}\"\n            )\n\n        new_link.append(\n            (name_index_map[edge[0]][0], name_index_map[edge[1]][0]))\n\n    return new_link", ""]}
{"filename": "cnlp/similarity_methods/quasi_local_similarity.py", "chunked_list": ["\"\"\"Collection of Quasi-local Similarity Methods for Link Prediction.\n\nQuasi-local indices have been introduced as a trade-off between\nlocal and global approaches or performance and complexity.\nThese metrics are as efficient to compute as local indices.\nSome of these indices extract the entire topological information\nof the network.\n\nThe time complexities of these indices are still below compared\nto the global approaches.", "The time complexities of these indices are still below compared\nto the global approaches.\n\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom cnlp.utils import to_adjacency_matrix, nodes_to_indexes, only_unconnected\nfrom scipy.sparse import csr_matrix, lil_matrix\n\n\ndef local_path_index(G: nx.Graph, epsilon: float, n: int) -> csr_matrix:\n    \"\"\"Compute the Local Path  Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S^{LP} = A^{2} + \\\\epsilon A^{3} +\n        \\\\epsilon^{2} A^{4} + \\\\ldots + \\\\epsilon^{n - 2} A^{n}\n\n    where \\\\(\\\\epsilon\\\\) is a free parameter,\n    \\\\(A\\\\) is the Adjacency Matrix and \\\\(n\\\\) is the maximal order.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    epsilon: float :\n        free parameter\n    n: int :\n        maximal order\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This metric has the intent to furnish a good trade-off\n    between accuracy and computational complexity.\n\n    Clearly, the measurement converges to common neighbor when\n    \\\\(\\\\epsilon = 0\\\\). If there is no direct connection between\n    \\\\(x\\\\) and \\\\(y\\\\), \\\\((A^{3})_{x,y}\\\\) is equated to the total\n    different paths of length 3 between \\\\(x\\\\) and \\\\(y\\\\).\n\n    Computing this index becomes more complicated with the increasing\n    value of \\\\(n\\\\). The LP index outperforms the proximity-based indices,\n    such as RA, AA, and CN.\n    \"\"\"\n    A = to_adjacency_matrix(G)\n    A = A @ A\n    S = np.power(epsilon, 0) * (A)\n\n    # Calculate the remaining terms of the sum\n    for i in range(1, n - 2):\n        A = A @ A\n        S += np.power(epsilon, i) * (A)\n\n    return only_unconnected(G, lil_matrix(S))", "\ndef local_path_index(G: nx.Graph, epsilon: float, n: int) -> csr_matrix:\n    \"\"\"Compute the Local Path  Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S^{LP} = A^{2} + \\\\epsilon A^{3} +\n        \\\\epsilon^{2} A^{4} + \\\\ldots + \\\\epsilon^{n - 2} A^{n}\n\n    where \\\\(\\\\epsilon\\\\) is a free parameter,\n    \\\\(A\\\\) is the Adjacency Matrix and \\\\(n\\\\) is the maximal order.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    epsilon: float :\n        free parameter\n    n: int :\n        maximal order\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This metric has the intent to furnish a good trade-off\n    between accuracy and computational complexity.\n\n    Clearly, the measurement converges to common neighbor when\n    \\\\(\\\\epsilon = 0\\\\). If there is no direct connection between\n    \\\\(x\\\\) and \\\\(y\\\\), \\\\((A^{3})_{x,y}\\\\) is equated to the total\n    different paths of length 3 between \\\\(x\\\\) and \\\\(y\\\\).\n\n    Computing this index becomes more complicated with the increasing\n    value of \\\\(n\\\\). The LP index outperforms the proximity-based indices,\n    such as RA, AA, and CN.\n    \"\"\"\n    A = to_adjacency_matrix(G)\n    A = A @ A\n    S = np.power(epsilon, 0) * (A)\n\n    # Calculate the remaining terms of the sum\n    for i in range(1, n - 2):\n        A = A @ A\n        S += np.power(epsilon, i) * (A)\n\n    return only_unconnected(G, lil_matrix(S))", "\n\ndef path_of_length_three(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Path of Length Three Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\sum \\\\frac{a_{x,u}a_{u,v}a_{v,y}}{k_u k_v}\n\n    where \\\\(k_x\\\\) is the degree of node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        grafo da analizzare\n\n    Returns\n    -------\n    S: csr_matrix : matrice di Similarit\u00e0\n\n    Notes\n    -----\n    Georg Simmel, a German sociologist, first coined the concept\n    \u201ctriadic closure\u201d and made popular by Mark Granovetter in his work\n    \u201cThe Strength of Weak Ties\u201d. The authors proposed a similarity index\n    in protein-protein interaction (PPI) network,\n    called path of length 3 (or L3) published in the\n    Nature Communication. They experimentally show that the triadic closure\n    principle (TCP) does not work well with PPI networks. They showed the\n    paradoxical behavior of the TCP (i.e., the path of length 2),\n    which does not follow the structural and evolutionary mechanism that\n    governs protein interaction. The TCP predicts well to the interaction of\n    self-interaction proteins (SIPs), which are very small (4%) in PPI networks\n    and fails in prediction between SIP and non SIP that amounts to 96%.\n    \"\"\"\n\n    def __path_of_length_three_iter(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Path of Length Three Index for 2 given nodes.\"\"\"\n        k_x = G.degree(x)\n        k_y = G.degree(y)\n\n        score = 0\n\n        # Enroll all neighbors\n        for u in G[x]:\n            for v in G[y]:\n                # Base case\n                if u == v:\n                    continue\n\n                # Calcolate the score with the multiply of\n                # value of node and divide for degree\n                if G.has_edge(u, v):\n                    a_xu = G[x][u].get(\n                        'weight',\n                        1)  # prende il 'peso' dell'arco (1 se non ha peso)\n                    a_uv = G[u][v].get('weight', 1)\n                    a_vy = G[v][y].get('weight', 1)\n\n                    score += (a_xu * a_uv * a_vy) / (k_x * k_y)\n\n        return score\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    name_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = name_index_map[x]\n        _y = name_index_map[y]\n\n        S[_x, _y] = __path_of_length_three_iter(G, x, y)\n        S[_y, _x] = S[_x, _y]\n\n    return S.tocsr()", ""]}
{"filename": "cnlp/similarity_methods/local_similarity.py", "chunked_list": ["\"\"\"Collection of Local Similarity Methods for Link Prediction.\n\nLocal indices are generally calculated using information about\ncommon neighbors and node degree.\nThese indices **consider immediate neighbors of a node**.\n\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom cnlp.utils import nodes_to_indexes\nfrom scipy.sparse import lil_matrix, csr_matrix", "from cnlp.utils import nodes_to_indexes\nfrom scipy.sparse import lil_matrix, csr_matrix\n\n\ndef adamic_adar(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Adamic and Adar Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)}\n        \\\\frac{1}{\\\\log k_z}\n\n    where \\\\(k_z\\\\) is the degree of node \\\\(z\\\\)\n    and \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    It is clear from the equation that more weights are assigned to\n    the common neighbors having smaller degrees.\n    This is also intuitive in the real-world scenario, for example,\n    a person with more number of friends spend less time/resource\n    with an individual friend as compared to the less number of friends.\n    \"\"\"\n\n    def __adamic_adar(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Adamic and Adar Index for 2 given nodes.\"\"\"\n        return sum([1 / np.log(G.degree[z]) for z in set(G[x]) & set(G[y])])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __adamic_adar(G, x, y)\n\n    return S.tocsr()", "\n\ndef __common_neighbors(G: nx.Graph, x, y) -> int:\n    \"\"\"Compute the Common Neighbors Index for 2 given nodes.\"\"\"\n    return len(set(G[x]).intersection(set(G[y])))\n\n\ndef common_neighbors(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Common Neighbors Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    The likelihood of the existence of a link between \\\\(x\\\\)\n    and \\\\(y\\\\) increases with the number of common neighbors between them.\n    \"\"\"\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __common_neighbors(G, x, y)\n        # S[_y, _x] = S[_x, _y]\n\n    return S.tocsr()", "\n\ndef cosine_similarity(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Cosine Similarity Index\n    (a.k.a. Salton Index) for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\frac{|\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\sqrt{k_x k_y}}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This similarity index between two nodes is measured by\n    calculating the Cosine of the angle between them.\n    The metric is all about the orientation and not magnitude.\n    \"\"\"\n\n    def __cosine_similarity(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Cosine Similarity Index for 2 given nodes.\"\"\"\n        return __common_neighbors(G, x, y) / np.sqrt(G.degree[x] * G.degree[y])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __cosine_similarity(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef hub_depressed(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Hub Depressed Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\max(k_x, k_y)}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This index is the same as the previous one but with the\n    opposite goal as it avoids the formation of\n    links between hubs and low degree nodes in the networks.\n    The Hub depressed index promotes the links evolution\n    between the hubs as well as the low degree nodes.\n    \"\"\"\n\n    def __hub_depressed(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Hub Depressed Index for 2 given nodes.\"\"\"\n        return __common_neighbors(G, x, y) / max(G.degree[x], G.degree[y])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __hub_depressed(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef hub_promoted(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Hub Promoted Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\min(k_x, k_y)}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This similarity index promotes the formation of links between\n    the sparsely connected nodes and hubs.\n    It also tries to prevent links formation between the hub nodes.\n    \"\"\"\n\n    def __hub_promoted(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Hub Promoted Index for 2 given nodes.\"\"\"\n        return __common_neighbors(G, x, y) / min(G.degree[x], G.degree[y])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __hub_promoted(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef jaccard(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Jaccard Coefficient for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\frac{| \\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{| \\\\Gamma(x) \\\\cup \\\\Gamma(y)|}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    The Jaccard coefficient is defined as the probability of selection\n    of common neighbors of pairwise vertices from all the neighbors of\n    either vertex. The pairwise Jaccard score increases with the number of\n    common neighbors between the two vertices considered. Some researcher\n    (**Liben-Nowell et al.**) demonstrated that this similarity metric\n    performs worse as compared to Common Neighbors.\n    \"\"\"\n\n    def __jaccard(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Jaccard Coefficient for 2 given nodes.\"\"\"\n        total_neighbor_number = len(set(G[x]).union(set(G[y])))\n        if total_neighbor_number == 0:\n            return 0\n        \n        return __common_neighbors(G, x, y) / total_neighbor_number\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __jaccard(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef node_clustering(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Hub Depressed Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)} C(z)\n\n    where\n\n    .. math::\n        C(z) = \\\\frac{t(z)}{k_z(k_z - 1)}\n\n    is the clustering coefficient of node \\\\(z\\\\), \\\\(t(z)\\\\)\n    is the total triangles passing through the node \\\\(z\\\\),\n    \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    This index is also based on the clustering coefficient property\n    of the network in which the clustering coefficients of all\n    the common neighbors of a seed node pair are computed\n    and summed to find the final similarity score of the pair.\n    \"\"\"\n\n    def __t(G: nx.Graph, z) -> int:\n        \"\"\"Number of triangles passing through the node z\"\"\"\n        return nx.triangles(G, z)\n\n    def __C(G: nx.Graph, z) -> float:\n        \"\"\"Clustering Coefficient\"\"\"\n        z_degree = G.degree[z]\n\n        # avoiding 0 divition error\n        if z_degree == 1:\n            return 0\n\n        return __t(G, z) / (z_degree * (z_degree - 1))\n\n    def __node_clustering(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Node Clustering Coefficient for 2 given nodes.\"\"\"\n        return sum([__C(G, z) for z in (set(G[x]) & set(G[y]))])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __node_clustering(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef preferential_attachment(G: nx.Graph, sum: bool = False) -> csr_matrix:\n    \"\"\"Compute the Preferential Attachment Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = k_x k_y\n\n    where \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    sum: bool :\n        Replace multiplication with summation when computing the index.\n         (Default value = False)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    The idea of preferential attachment is applied to generate a growing\n    scale-free network. The term growing represents the incremental nature\n    of nodes over time in the network. The likelihood incrementing new\n    connection associated with a node \\\\(x\\\\) is proportional to\n    \\\\(k_x\\\\), the degree of the node.\n\n    This index shows the worst performance on most networks.\n    The **simplicity** (as it requires the least information\n    for the score calculation) and the computational time of this metric\n    are the main advantages. PA shows better results if larger\n    degree nodes are densely connected,\n    and lower degree nodes are rarely connected.\n\n    In the above equation, summation can also be used instead of\n    multiplication as an aggregate function (`sum = True`).\n    \"\"\"\n\n    def __preferential_attachment(G: nx.Graph,\n                                  x,\n                                  y,\n                                  sum: bool = False) -> float:\n        \"\"\"Compute the Preferential Attachment Index for 2 given nodes.\"\"\"\n        return G.degree[x] * G.degree[y] if not sum else G.degree[\n            x] + G.degree[y]\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __preferential_attachment(G, x, y, sum=sum)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef resource_allocation(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Resource Allocation Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)} \\\\frac{1}{k_z}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\) and\n    \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    Consider two non-adjacent vertices \\\\(x\\\\) and \\\\(y\\\\).\n    Suppose node \\\\(x\\\\) sends some resources to \\\\(y\\\\)\n    through the common nodes of both \\\\(x\\\\) and \\\\(y\\\\)\n    then the similarity between the two vertices is computed in terms\n    of resources sent from \\\\(x\\\\) to \\\\(y\\\\).\n\n    The difference between Resource Allocation (**RA**) and\n    Adamic and Adar (**AA**) is that the RA index\n    heavily penalizes to higher degree nodes compared to the AA index.\n    Prediction results of these indices become almost the same\n    for smaller average degree networks.\n\n    This index shows good performance on heterogeneous\n    networks with a high clustering coefficient, especially\n    on transportation networks.\n    \"\"\"\n\n    def __resource_allocation(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Resource Allocation Index for 2 given nodes.\"\"\"\n        return sum([1 / G.degree[z] for z in set(G[x]) & set(G[y])])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __resource_allocation(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", "\n\ndef sorensen(G: nx.Graph) -> csr_matrix:\n    \"\"\"Compute the Sorensen Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{k_x + k_y}\n\n    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    It is very similar to the Jaccard index. **McCune et al.** show\n    that it is more robust than Jaccard against the outliers.\n    \"\"\"\n\n    def __sorensen(G: nx.Graph, x, y) -> float:\n        \"\"\"Compute the Sorensen Index for 2 given nodes.\"\"\"\n        return (2 * __common_neighbors(G, x, y)) / (G.degree[x] + G.degree[y])\n\n    size = G.number_of_nodes()\n    S = lil_matrix((size, size))\n    node_index_map = nodes_to_indexes(G)\n\n    for x, y in nx.complement(G).edges():\n        _x = node_index_map[x]\n        _y = node_index_map[y]\n\n        S[_x, _y] = __sorensen(G, x, y)\n        # S[y, x] = S[x, y]\n\n    return S.tocsr()", ""]}
{"filename": "cnlp/similarity_methods/__init__.py", "chunked_list": ["\"\"\"Similarity based Methods for Link Prediction.\n\nSimilarity-based metrics are the simplest one in link prediction,\nin which for each pair \\\\(x\\\\) and \\\\(y\\\\) , a similarity score\n\\\\(S(x, y)\\\\) is calculated.\nThe score \\\\(S(x, y)\\\\) is based on the structural or node\u2019s properties of\nthe considered pair. The non-observed links (i.e., \\\\(U - E^T\\\\)) are assigned\nscores according to their similarities. **The pair of nodes having a higher\nscore represents the predicted link between them**.\nThe similarity measures between every pair can be _calculated using several", "score represents the predicted link between them**.\nThe similarity measures between every pair can be _calculated using several\nproperties of the network_, one of which is structural property.\nScores based on this property can be grouped in several categories\nlike **local** and **global**, and so on.\n\"\"\"\n"]}
{"filename": "cnlp/similarity_methods/global_similarity.py", "chunked_list": ["\"\"\"Collection of Global Similarity Methods for Link Prediction.\n\nGlobal indices are computed using entire\ntopological information of a network.\nThe computational complexities of such methods\nare higher and seem to be infeasible for large networks.\n\"\"\"\nimport networkx as nx\nimport numpy as np\nfrom scipy.sparse import csr_matrix, linalg, identity, lil_matrix, hstack, lil_array", "import numpy as np\nfrom scipy.sparse import csr_matrix, linalg, identity, lil_matrix, hstack, lil_array\nfrom cnlp.utils import nodes_to_indexes, only_unconnected, to_adjacency_matrix\n\n\ndef katz_index(G: nx.Graph, beta: int = 1) -> csr_matrix:\n    \"\"\"Compute the Katz Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = (I - \\\\beta A)^{-1} - I\n\n    where \\\\(I\\\\) is the Identity Matrix,\n    \\\\(\\\\beta\\\\) is a dumping factor that controls the path weights\n    and \\\\(A\\\\) is the Adjacency Matrix\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    beta: int :\n        Dumping Factor\n         (Default value = 1)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    For the convergence of above equation,\n\n    .. math::\n        \\\\beta < \\\\frac{1}{\\\\lambda_1}\n\n    where \\\\(\\\\lambda_1\\\\) is the maximum eighenvalue of the matrix \\\\(A\\\\).\n\n    The computational complexity of the given metric is high,\n    and it can be roughly estimated to be cubic complexity\n    which is not feasible for a large network.\n    \"\"\"\n\n    def __power_method(A: csr_matrix,\n                       max_iterations: int = 100,\n                       tol: float = 1e-12,\n                       verbose: bool = False):\n        \"\"\"Perfome the Power Method\"\"\"\n        n = A.shape[0]\n        x = np.ones(n) / np.sqrt(n)  # initialize a vector x\n        # r = A @ x - np.dot(A @ x, x) * x # residual initialization\n        r = A @ x - ((A @ x) @ x) * x\n        eigenvalue = x @ (A @ x)  # residual eigenvalue\n        # eigenvalue = np.dot(x, A @ x) # residual eigenvalue\n\n        for i in range(max_iterations):\n            # Compute the new vector x\n            x = A @ x\n            # vector normalization\n            x = x / np.linalg.norm(x)\n\n            # Residual and eigenvalue computation\n            r = A @ x - ((A @ x) @ x) * x\n            eigenvalue = x @ (A @ x)\n            # r = A @ x - np.dot(A @ x, x) * x\n            # eigenvalue = np.dot(x, A @ x)\n\n            # If the norm of r is less than the tolerance, break out of the loop.\n            if np.linalg.norm(r) < tol:\n                if verbose:\n                    print(f'Computation done after {i} steps')\n                break\n\n        return eigenvalue, x\n\n    A = to_adjacency_matrix(G)\n    largest_eigenvalue = __power_method(A)  # lambda_1\n    if beta >= (1 / largest_eigenvalue[0]):\n        print(f'Warning, Beta should be less than {largest_eigenvalue}')\n\n    eye = identity(A.shape[0], format='csc')\n    S = linalg.inv((eye - beta * A.tocsc())) - eye\n\n    return only_unconnected(G, csr_matrix(S))", "\n\ndef link_prediction_rwr(G: nx.Graph,\n                        c: int = 0.05,\n                        max_iters: int = 10) -> csr_matrix:\n    \"\"\"Compute the Random Walk with Restart Algorithm.\n\n    The similarity between two nodes is defined as:\n\n    .. math::\n        S(x, y) = q_{xy} + q_{yx}\n\n    where \\\\(q_x\\\\) is defined as \\\\( (1-\\\\alpha) (I - \\\\alpha P^T)^{-1} e_x\\\\)\n    and \\\\(e_x\\\\) is the seed vector of length \\\\(|V|\\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    c: int :\n        TODO\n         (Default value = 0.05)\n    max_iters: int :\n        max number of iteration for the algorithm convergence\n         (Default value = 10)\n\n    Returns\n    -------\n    similarity_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    Let \\\\(\\\\alpha\\\\) be a probability that a random walker\n    iteratively moves to an arbitrary neighbor and returns to the same\n    starting vertex with probability \\\\( (1 - \\\\alpha )\\\\).\n    Consider \\\\(q_{xy}\\\\) to be the probability that a random walker\n    who starts walking from vertex x and located at the vertex y in steady-state.\n\n    The seed vector \\\\(e_x\\\\) consists of zeros for all components except the\n    elements \\\\(x\\\\) itself.\n\n    The transition matrix \\\\(P\\\\) can be expressed as\n\n    .. math::\n        P_{xy} = \\\\begin{cases}\n                \\\\frac{1}{k_x} & \\\\text{if } x \\\\text{ and } y \\\\text{ are connected,} \\\\\\\\\n                0 & \\\\text{otherwise.}\n            \\\\end{cases}\n    \"\"\"\n\n    def random_walk_with_restart(e: lil_array,\n                                 W_normalized: csr_matrix,\n                                 c: int = 0.05,\n                                 max_iters: int = 100) -> lil_array:\n        \"\"\"Generates the probability vector\n\n        Parameters\n        ----------\n        e: lil_array :\n            input probability vector\n        W_normalized: csr_matrix :\n            TODO\n        c: int :\n            TODO\n             (Default value = 0.05)\n        max_iters: int :\n            max number of iteration for the algorithm convergence\n             (Default value = 100)\n\n        Returns\n        -------\n        e: lil_array : the updated probability vector\n        \"\"\"\n        # Initialize the current probability vector to the initial one and the error to 1\n        old_e = e\n        err = 1.\n\n        # Perform the random walk with restart until the maximum number\n        # of iterations is reached or the error becomes less than 1e-6\n        for _ in range(max_iters):\n            e = (c * (W_normalized @ old_e)) + ((1 - c) * e)\n            err = linalg.norm(e - old_e, 1)\n            if err <= 1e-6:\n                break\n            old_e = e\n\n        # Return the current probability vector\n        return e\n\n    # Convert the graph G into an adjacency matrix A\n    A = to_adjacency_matrix(G)\n\n    # Extract the number of nodes of matrix A\n    m = A.shape[0]\n\n    # Initialize the diagonal matrix D as a sparse lil_matrix\n    D = lil_matrix(A.shape)\n\n    # Create a map that associates each node with a row index in matrix A\n    nodes_to_indexes_map = nodes_to_indexes(G)\n\n    # Build the diagonal matrix D so that the elements on the diagonal\n    # are equal to the degree of the corresponding node\n    for node in G.nodes():\n        D[nodes_to_indexes_map[node],\n          nodes_to_indexes_map[node]] = G.degree[node]\n\n    # Convert the diagonal matrix D into csc_matrix format\n    D = D.tocsc()\n\n    try:\n        # Build the normalized transition matrix W_normalized\n        W_normalized = linalg.inv(D) @ A.tocsc()\n    except RuntimeError as e:\n        print('Possible presence of singleton nodes in the graph G')\n        print(e)\n        exit(1)\n\n    # Initialize an matrix to hold the similarities between node pairs\n    # We put an initial column made of Zeros so we can use the hstack\n    # method later on and keep the code more clean\n    similarity_matrix = csr_matrix((m, 1))\n\n    # For each node i, create a probability vector and perform the\n    # random walk with restart starting from that node\n    for i in range(m):\n        e = lil_array((m, 1))\n        e[i, 0] = 1\n        # Concatenate the similarity vectors into a similarity matrix\n        # The use of hstack allows the lil_array returned from the\n        # random walk function to be transposed and added to the\n        # similarity matrix as a new column in just one line of code\n        similarity_matrix = hstack([\n            similarity_matrix,\n            random_walk_with_restart(e=e,\n                                     W_normalized=W_normalized,\n                                     c=c,\n                                     max_iters=max_iters)\n        ])\n\n    # Return the similarity matrix and remove the fisrt column\n    # In order to keep the results consistent without the added column of zeros at the beginning\n    return only_unconnected(G, csr_matrix(similarity_matrix)[:, 1:])", "\n\ndef rooted_page_rank(G: nx.Graph, alpha: float = .5) -> csr_matrix:\n    \"\"\"Compute the Rooted Page Rank for all nodes in the Graph.\n    This score is defined as:\n\n    .. math::\n        S = (1 - \\\\alpha) (I - \\\\alpha \\\\hat{N})^{-1}\n\n    where \\\\(\\\\hat{N} = D^{-1}A\\\\) is the normalized\n    Adjacency Matrix with the diagonal degree matrix\n    \\\\(D[i,i] = \\\\sum_j A[i,j]\\\\)\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    alpha: float :\n        random walk probability\n         (Default value = 0.5)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    The idea of PageRank was originally proposed to rank the web pages based on\n    the importance of those pages. The algorithm is based on the assumption that\n    a random walker randomly goes to a web page with probability \\\\(\\\\alpha\\\\)\n    and follows hyper-link embedded in the page with probability \\\\( (1 - \\\\alpha ) \\\\).\n    Chung et al. used this concept incorporated with a random walk in\n    link prediction framework. The importance of web pages, in a random walk,\n    can be replaced by stationary distribution. The similarity between two vertices\n    \\\\(x\\\\) and \\\\(y\\\\) can be measured by the stationary probability of\n    \\\\(x\\\\) from \\\\(y\\\\) in a random walk where the walker moves to an\n    arbitrary neighboring vertex with probability \\\\(\\\\alpha\\\\)\n    and returns to \\\\(x\\\\) with probability \\\\( ( 1 - \\\\alpha )\\\\).\n    \"\"\"\n    A = to_adjacency_matrix(G)\n    D = lil_matrix(A.shape)\n\n    nodes_to_indexes_map = nodes_to_indexes(G)\n    for node in G.nodes():\n        D[nodes_to_indexes_map[node],\n          nodes_to_indexes_map[node]] = G.degree[node]\n\n    D = D.tocsc()\n    N_hat = linalg.inv(D) @ A.tocsc()\n    eye = identity(A.shape[0], format='csc')\n    S = (1 - alpha) * linalg.inv(eye - alpha * N_hat)\n\n    return only_unconnected(G, S.tocsr())", "\n\ndef shortest_path(G: nx.Graph, cutoff: int = None) -> csr_matrix:\n    \"\"\"Compute the Shortest Path Index for all nodes in the Graph.\n    Each similarity value is defined as:\n\n    .. math::\n        S(x, y) = - |d(x,y)|\n\n    where Dijkstra algorithm  is applied to efficiently\n    compute the shortest path \\\\(d(x, y)\\\\) between the\n    node pair \\\\( (x, y) \\\\).\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    cutoff: int :\n        max path length\n         (Default value = None)\n\n    Returns\n    -------\n    S: csr_matrix : the Similarity Matrix (in sparse format)\n\n    Notes\n    -----\n    Liben-Nowell et al. provided the shortest path with its negation as a\n    metric to link prediction.\n\n    The prediction accuracy\n    of this index is low compared to most local indices.\n    \"\"\"\n    dim = G.number_of_nodes()\n    if cutoff is None:\n        cutoff = dim\n\n    lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff))\n    nodes_to_indexes_map = nodes_to_indexes(G)\n    prexisting_links = list(G.edges())\n\n    S = lil_matrix((dim, dim))\n    for source_node in lengths.keys():\n        for dest_node in lengths[source_node].keys():\n            # If the link already exists in the starting graph the computation is skipped\n            if (nodes_to_indexes_map[source_node],\n                    nodes_to_indexes_map[dest_node]) not in prexisting_links:\n                S[nodes_to_indexes_map[source_node],\n                  nodes_to_indexes_map[dest_node]] = -lengths[source_node][\n                      dest_node]\n\n    return S.tocsr()", "\n\ndef sim_rank(G: nx.Graph,\n             k: int = 5,\n             cutoff: int = 4,\n             c: int = 0.8) -> csr_matrix:\n    \"\"\"Compute the SimRank index for all the nodes in the Graph.\n\n    This method is defined as:\n\n    .. math::\n        S(x, y) = \\\\begin{cases}\n                \\\\frac{\\\\alpha}{k_x k_y} \\\\sum_{i=1}^{k_x} \\\\sum_{j=1}^{k_y}\n                    S( \\\\Gamma_i(x), \\\\Gamma_j(y)) & x \\\\neq y \\\\\\\\\n                1 & x = y\n            \\\\end{cases}\n\n    where \\\\( \\\\alpha \\\\in (0,1) \\\\) is a constant. \\\\(\\\\Gamma_i(x)\\\\) and \\\\( \\\\Gamma_j(y) \\\\)\n    are the ith and jth elements in the neighborhood sets.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    k: int :\n         (Default value = 5)\n    cutoff: int :\n         (Default value = 4)\n    c: int :\n         (Default value = 0.8)\n\n    Returns\n    -------\n    sim_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n    \"\"\"\n\n    def init_similarity_matrix(G: nx.Graph, n: int) -> lil_matrix:\n        \"\"\"Generate an Identity matrix: the starting Similarity\n        Matrix.\n\n        Parameters\n        ----------\n        G: nx.Graph :\n            input Graph (a networkx Graph)\n        n: int :\n           the new matrix size\n\n        Returns\n        -------\n        sim_matrix: lil_matrix : the starting Similarity Matrix\n        \"\"\"\n        # inizializzo la matrice similarity\n        # gli elementi con loro stessi (lungo la diagonale) hanno similarit\u00e0 massima\n        sim_matrix = identity(n).tolil()\n        return sim_matrix\n\n    def compute_sim_rank(G: nx.Graph,\n                         a,\n                         b,\n                         sim_matrix: lil_matrix,\n                         C: int = 0.8) -> float:\n        \"\"\"Compute the Sim Rank method between the given\n        nodes a and b.\n\n        Parameters\n        ----------\n        G: nx.Graph :\n            input Graph (a networkx Graph)\n        a :\n           first node\n        b :\n           second node\n        sim_matrix: lil_matrix :\n            the similarity matrix\n        C: int :\n            free parameter\n             (Default value = 0.8)\n\n        Returns\n        -------\n        new_SimRank: float : the SimRank value between a and b\n        \"\"\"\n\n        # se i nodi sono uguali allora similarit\u00e0 massima\n        if (a == b):\n            return 1\n\n        a_neigh = list(G.neighbors(a))\n        b_neigh = list(G.neighbors(b))\n        len_a = len(a_neigh)\n        len_b = len(b_neigh)\n\n        # nodi isolati hanno similarit\u00e0 0\n        if (len_a == 0 or len_b == 0):\n            return 0\n\n        # mi recupero e sommo i valori di similarit\u00e0 calcolati in precedenza\n        simRank_sum = 0\n        for i in a_neigh:\n            for j in b_neigh:\n                simRank_sum += sim_matrix[i, j]\n        # moltiplico secondo la definizione del paper\n        scale = C / (len_a * len_b)\n        new_SimRank = scale * simRank_sum\n        return new_SimRank\n\n    G = nx.convert_node_labels_to_integers(G, 0)\n\n    nodes_num = G.number_of_nodes()\n    sim_matrix = init_similarity_matrix(G, nodes_num)\n\n    for a in range(nodes_num):\n        for b in range(nodes_num):\n            # fa pruning evitando di calcolare la similarit\u00e0 di archi a distanza maggiore di 5\n            if (nx.has_path(G, a, b)\n                    and (nx.shortest_path_length(G, a, b) > cutoff)):\n                sim_matrix[a, b] = 0\n            else:\n                # se non deve fare pruning si calcola il valore di similarit\u00e0 per i nodi a e b\n                for i in range(k):\n                    sim_matrix[a, b] = compute_sim_rank(G,\n                                                        a,\n                                                        b,\n                                                        sim_matrix=sim_matrix,\n                                                        C=c)\n\n    # imposta a 0 gli elementi della diagonale che prima avevano similarit\u00e0 uguale ad 1\n    for a in range(nodes_num):\n        sim_matrix[a, a] = 0\n    return only_unconnected(G, sim_matrix)", ""]}
{"filename": "cnlp/other_methods/__init__.py", "chunked_list": ["import cnlp.other_methods.information_theory\n"]}
{"filename": "cnlp/other_methods/information_theory.py", "chunked_list": ["import networkx as nx\nimport numpy as np\nimport scipy.sparse as scipy\nimport math\nimport itertools\nfrom cnlp.utils import nodes_to_indexes\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom typing import Generator\n\n\ndef MI(G: nx.Graph) -> scipy.csr_matrix:\n    \"\"\"Neighbor Set Information\n\n    Il modello di link prediction basato su information theory che sfrutta la neighbor\n    set information \u00e8 un approccio utilizzato per prevedere la probabilit\u00e0 di esistenza\n    di un link tra due nodi in una rete. In questo modello, l'informazione contenuta nei\n    neighbor set dei due nodi in questione viene utilizzata per stimare la probabilit\u00e0\n    di connessione.\n\n    L'idea alla base di questo modello \u00e8 che i nodi che hanno molti neighbor in comune\n    sono pi\u00f9 propensi a essere connessi tra loro rispetto a nodi con neighbor set diversi.\n    Questo perch\u00e9 i nodi con neighbor set simili tendono a essere coinvolti in attivit\u00e0\n    simili all'interno della rete, come ad esempio\n    partecipare agli stessi gruppi o condividere gli stessi interessi.\n\n    Per utilizzare questa informazione per prevedere la probabilit\u00e0 di connessione tra due nodi,\n    il modello utilizza l'entropia di Shannon, una misura dell'incertezza di una\n    distribuzione di probabilit\u00e0.\n    In particolare, l'entropia viene calcolata sui neighbor set dei due nodi, e la differenza tra le\n    entropie dei due set viene utilizzata per stimare la probabilit\u00e0 di connessione.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    res_sparse: csr_matrix : the Similarity Matrix (in sparse format)\n    \"\"\"\n\n    def overlap_info(G: nx.Graph, x, y, edge_num: int) -> float:\n        \"\"\"Two Information Definition.\n        Overlapping nodes of different sets and\n        the existence of link across different sets\n\n        Parameters\n        ----------\n        G: nx.Graph :\n            input graph\n        x :\n            first node\n        y :\n            second node\n        edge_num: int :\n            TODO !\n\n        Returns\n        -------\n        s_Overlap: float : TODO\n        \"\"\"\n        # ottenimento dei dati da cui ottenere le informazioni\n        o_nodes = nx.common_neighbors(G, x, y)\n        p_prob_overlap = -np.log2(prior(x, y, G, edge_num))\n\n        # utilizzo delle informazioni per stimarsi la likelihood\n        # con gli overlapping nodes\n        coeff = 0\n        overlap_info_value = 0\n        overlap = 0\n\n        for z in o_nodes:\n            # degree of z\n            kz = G.degree(z)\n\n            coeff = 1 / (kz * (kz - 1))\n\n            # sum over edges = neighbors of z\n            overlap = 0\n            for m, n in itertools.combinations(G.neighbors(z), 2):\n                priorInfo = -np.log2(prior(m, n, G, edge_num))\n                likelihoodInfo = -np.log2(likelihood(z, G))\n                # print(f\"a = {x}, b = {y}, priorInfo = { priorInfo},\n                #   lilelihoodInfo = {likelihoodInfo}\")\n                # combine mutual information\n                overlap += 2 * (priorInfo - likelihoodInfo)\n                # print(f\"a = {x}, b = {y}, zOverlap = { 2*(priorInfo -likelihoodInfo)}\")\n\n        # add average mutual information per neighbor\n        overlap_info_value += coeff * overlap\n        s_Overlap = overlap_info_value - p_prob_overlap\n        return s_Overlap\n\n    def prior(m, n, G: nx.Graph, edge_num: int) -> float:\n        \"\"\"Calcola la probabilit\u00e0 a priori dati due nodi e\n        un grafo riferita alla probabilit\u00e0 con cui non si forma un cammino\n        tra i due nodi\n\n        Parameters\n        ----------\n        m :\n            first node\n        n :\n            second node\n        G: nx.Graph :\n            input graph\n        edge_num: int :\n            TODO\n\n        Returns\n        -------\n        float: the prior probability\n        \"\"\"\n        kn = G.degree(n)\n        km = G.degree(m)\n\n        return 1 - math.comb(edge_num - kn, km) / math.comb(edge_num, km)\n\n    def likelihood(z, G: nx.Graph) -> float:\n        \"\"\"probabilit\u00e0 condizionata che in questo caso \u00e8 definita come il clustering\n        coefficient dei common neighbor dei nodi x e y\n\n        Parameters\n        ----------\n        z :\n            input node\n        G: nx.Graph :\n            input graph\n\n        Returns\n        -------\n        float: TODO\n        \"\"\"\n        kz = G.degree(z)\n        N_triangles = nx.triangles(G, z)\n        N_triads = math.comb(kz, 2)\n\n        return N_triangles / N_triads\n\n    I_Oxy = 0\n    edge_num = G.number_of_edges()\n    node_num = G.number_of_nodes()\n    edge_num = G.number_of_edges()\n    res_sparse = scipy.lil_matrix((node_num, node_num))\n\n    nodes_to_indexes_map = nodes_to_indexes(G)\n    for i, j in nx.complement(G).edges():\n        I_Oxy = overlap_info(G, i, j, edge_num)\n        res_sparse[nodes_to_indexes_map[i], nodes_to_indexes_map[j]] = I_Oxy\n\n    return res_sparse.tocsr()", "\n\ndef MI(G: nx.Graph) -> scipy.csr_matrix:\n    \"\"\"Neighbor Set Information\n\n    Il modello di link prediction basato su information theory che sfrutta la neighbor\n    set information \u00e8 un approccio utilizzato per prevedere la probabilit\u00e0 di esistenza\n    di un link tra due nodi in una rete. In questo modello, l'informazione contenuta nei\n    neighbor set dei due nodi in questione viene utilizzata per stimare la probabilit\u00e0\n    di connessione.\n\n    L'idea alla base di questo modello \u00e8 che i nodi che hanno molti neighbor in comune\n    sono pi\u00f9 propensi a essere connessi tra loro rispetto a nodi con neighbor set diversi.\n    Questo perch\u00e9 i nodi con neighbor set simili tendono a essere coinvolti in attivit\u00e0\n    simili all'interno della rete, come ad esempio\n    partecipare agli stessi gruppi o condividere gli stessi interessi.\n\n    Per utilizzare questa informazione per prevedere la probabilit\u00e0 di connessione tra due nodi,\n    il modello utilizza l'entropia di Shannon, una misura dell'incertezza di una\n    distribuzione di probabilit\u00e0.\n    In particolare, l'entropia viene calcolata sui neighbor set dei due nodi, e la differenza tra le\n    entropie dei due set viene utilizzata per stimare la probabilit\u00e0 di connessione.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n\n    Returns\n    -------\n    res_sparse: csr_matrix : the Similarity Matrix (in sparse format)\n    \"\"\"\n\n    def overlap_info(G: nx.Graph, x, y, edge_num: int) -> float:\n        \"\"\"Two Information Definition.\n        Overlapping nodes of different sets and\n        the existence of link across different sets\n\n        Parameters\n        ----------\n        G: nx.Graph :\n            input graph\n        x :\n            first node\n        y :\n            second node\n        edge_num: int :\n            TODO !\n\n        Returns\n        -------\n        s_Overlap: float : TODO\n        \"\"\"\n        # ottenimento dei dati da cui ottenere le informazioni\n        o_nodes = nx.common_neighbors(G, x, y)\n        p_prob_overlap = -np.log2(prior(x, y, G, edge_num))\n\n        # utilizzo delle informazioni per stimarsi la likelihood\n        # con gli overlapping nodes\n        coeff = 0\n        overlap_info_value = 0\n        overlap = 0\n\n        for z in o_nodes:\n            # degree of z\n            kz = G.degree(z)\n\n            coeff = 1 / (kz * (kz - 1))\n\n            # sum over edges = neighbors of z\n            overlap = 0\n            for m, n in itertools.combinations(G.neighbors(z), 2):\n                priorInfo = -np.log2(prior(m, n, G, edge_num))\n                likelihoodInfo = -np.log2(likelihood(z, G))\n                # print(f\"a = {x}, b = {y}, priorInfo = { priorInfo},\n                #   lilelihoodInfo = {likelihoodInfo}\")\n                # combine mutual information\n                overlap += 2 * (priorInfo - likelihoodInfo)\n                # print(f\"a = {x}, b = {y}, zOverlap = { 2*(priorInfo -likelihoodInfo)}\")\n\n        # add average mutual information per neighbor\n        overlap_info_value += coeff * overlap\n        s_Overlap = overlap_info_value - p_prob_overlap\n        return s_Overlap\n\n    def prior(m, n, G: nx.Graph, edge_num: int) -> float:\n        \"\"\"Calcola la probabilit\u00e0 a priori dati due nodi e\n        un grafo riferita alla probabilit\u00e0 con cui non si forma un cammino\n        tra i due nodi\n\n        Parameters\n        ----------\n        m :\n            first node\n        n :\n            second node\n        G: nx.Graph :\n            input graph\n        edge_num: int :\n            TODO\n\n        Returns\n        -------\n        float: the prior probability\n        \"\"\"\n        kn = G.degree(n)\n        km = G.degree(m)\n\n        return 1 - math.comb(edge_num - kn, km) / math.comb(edge_num, km)\n\n    def likelihood(z, G: nx.Graph) -> float:\n        \"\"\"probabilit\u00e0 condizionata che in questo caso \u00e8 definita come il clustering\n        coefficient dei common neighbor dei nodi x e y\n\n        Parameters\n        ----------\n        z :\n            input node\n        G: nx.Graph :\n            input graph\n\n        Returns\n        -------\n        float: TODO\n        \"\"\"\n        kz = G.degree(z)\n        N_triangles = nx.triangles(G, z)\n        N_triads = math.comb(kz, 2)\n\n        return N_triangles / N_triads\n\n    I_Oxy = 0\n    edge_num = G.number_of_edges()\n    node_num = G.number_of_nodes()\n    edge_num = G.number_of_edges()\n    res_sparse = scipy.lil_matrix((node_num, node_num))\n\n    nodes_to_indexes_map = nodes_to_indexes(G)\n    for i, j in nx.complement(G).edges():\n        I_Oxy = overlap_info(G, i, j, edge_num)\n        res_sparse[nodes_to_indexes_map[i], nodes_to_indexes_map[j]] = I_Oxy\n\n    return res_sparse.tocsr()", "\n\ndef path_entropy(G: nx.Graph, max_path: int = 3) -> csr_matrix:\n    \"\"\"Compute the Path Entropy Measure for all nodes in the Graph.\n\n    This Similarity measure between two nodes \\\\(X\\\\) and \\\\(Y\\\\)\n    is calculated with:\n\n    .. math::\n        S_{x,y} = -I(L^1_{xy}|U_{i=2}^{maxlen} D_{xy}^i)\n\n    where \\\\(D^i_{xy}\\\\) represents the set consisting of all simple\n    paths of length i between the two vertices and maxlen is the maximum\n    length of simple path of the network.\n\n    Parameters\n    ----------\n    G: nx.Graph :\n        input Graph (a networkx Graph)\n    max_path: int :\n        maximal path length\n         (Default value = 3)\n\n    Returns\n    -------\n    similarity_matrix: csr_matix: the Similarity Matrix (in sparse format)\n    \"\"\"\n\n    def simple_path_entropy(paths: Generator[list, None, None],\n                            G: nx.Graph) -> float:\n        \"\"\"Calcola l'entropia data dalla probabilit\u00e0 che si vengano a creare\n        i vari simple paths tra i nodi tra cui si\n        vuole fare link prediction\n\n        Parameters\n        ----------\n        paths: Generator[List] :\n            generator ritornato dalla funzione nx.all_simple_paths()\n\n        G: nx.Graph :\n            input graph\n\n        Returns\n        -------\n        float: simple path entropy\n        \"\"\"\n        tmp = .0\n        for path in paths:\n            for a, b in list(nx.utils.pairwise(path)):\n                tmp += new_link_entropy(G, a, b)\n        return tmp\n\n    def new_link_entropy(G: nx.Graph, a, b) -> float:\n        \"\"\"Calcola l'entropia basata sulla probabilit\u00e0\n        a priori della creazione del link diretto\n        tra le coppie di noti senza link diretti\n\n        Parameters\n        ----------\n        G: nx.Graph :\n            input graph\n        a :\n            first node\n        b :\n            second node\n\n        Returns\n        -------\n        float: entropy between node A and B\n        \"\"\"\n        deg_a = G.degree(a)\n        deg_b = G.degree(b)\n        M = G.number_of_edges()\n\n        return -1 * np.log2(1 - (math.comb(M - deg_a, deg_b) /\n                                   math.comb(M, deg_b)))\n\n    similarity_matrix = lil_matrix((G.number_of_nodes(), G.number_of_nodes()))\n    nodes_to_indexes_map = nodes_to_indexes(G)\n    missing_edges = list(nx.complement(G).edges())\n\n    for elem in missing_edges:\n        paths = nx.all_simple_paths(G, elem[0], elem[1], max_path)\n        tmp = 0\n        for i in range(2, (max_path + 1)):\n            tmp += (1 / (i - 1)) * simple_path_entropy(paths=paths, G=G)\n        tmp = tmp - new_link_entropy(G, elem[0], elem[1])\n        similarity_matrix[nodes_to_indexes_map[elem[0]],\n                          nodes_to_indexes_map[elem[1]]] = tmp\n    return similarity_matrix.tocsr()", "\n\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n\n    G = nx.karate_club_graph()\n\n    # converte gli id dei nodi in interi affinche possano essere usati come indici\n    # G_to_int = nx.convert_node_labels_to_integers(G, 0)\n    nx.draw(G, with_labels=True)\n\n    ranking = MI(G)\n    # da aggiungere informazioni dei nodi che hanno fatto ottentere il\n    # ranking migliore\n\n    # va preso il risultato pi\u00f9 piccolo perch\u00e8 si tratta di entropia\n    print(ranking)\n    for i, j in nx.complement(G).edges():\n        if (ranking[i, j] == ranking.toarray().min()):\n            print(\n                f\"Il link pi\u00f9 probabile tra quelli possibili \u00e8 tra {i} e {j}, con un valore di {ranking[i,j]}\"\n            )\n    plt.show()", ""]}
