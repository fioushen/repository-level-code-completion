{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(name='trainer', version='1.0', packages=find_packages())"]}
{"filename": "trainer/slurm_scripts/slurm_train.py", "chunked_list": ["import os\nimport random\nimport sys\nimport hydra\nimport submitit\nfrom omegaconf import DictConfig\n\nfrom trainer.accelerators.utils import nvidia_smi_gpu_memory_stats, print_config\n\nDEEPSPEED_MULTINODE = \"<is_deepspeed_multinode>\"", "\nDEEPSPEED_MULTINODE = \"<is_deepspeed_multinode>\"\n\ndef print_env():\n    for key in sorted(os.environ.keys()):\n        if not (\n                key.startswith((\"SLURM_\", \"SUBMITIT_\"))\n                or key in (\"MASTER_ADDR\", \"MASTER_PORT\", \"RANK\", \"WORLD_SIZE\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\")\n        ):\n            continue\n        value = os.environ[key]\n        print(f\"{key}={value}\")", "\n\nclass Task:\n\n    def __init__(self, cfg: DictConfig):\n        self.cfg = cfg\n\n    def __call__(self):\n        print(\"Running task on slurm\")\n        print(\"exporting PyTorch distributed environment variables\")\n        dist_env = submitit.helpers.TorchDistributedEnvironment()\n        rng = random.Random(dist_env._job_env.job_id)\n        dist_env.master_port = rng.randint(10000, 20000)\n        dist_env = dist_env.export()\n        os.environ.update(**{\n            \"CUDA_LAUNCH_BLOCKING\": \"0\",\n            \"NCCL_DEBUG\": \"info\",\n            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",\n            \"XDG_CACHE_HOME\": \"/fsx/yuval/.cache/\",\n            \"TOKENIZERS_PARALLELISM\": \"false\",\n            \"OMP_NUM_THREADS\": \"1\",\n        })\n        print(nvidia_smi_gpu_memory_stats())\n        print(f\"master: {dist_env.master_addr}:{dist_env.master_port}\")\n        print(f\"rank: {dist_env.rank}\")\n        print(f\"world size: {dist_env.world_size}\")\n        print(f\"local rank: {dist_env.local_rank}\")\n        print(f\"local world size: {dist_env.local_world_size}\")\n        print(\"Running training script\")\n        print(f\"Local rank {dist_env.local_rank}: {os.environ['CUDA_VISIBLE_DEVICES']=}\")\n        num_processes = self.cfg.slurm.n_processes * self.cfg.slurm.n_nodes\n        machine_rank = dist_env.rank // self.cfg.slurm.n_processes\n\n        cmd = f\"accelerate launch --dynamo_backend no --gpu_ids all --num_processes {num_processes} {DEEPSPEED_MULTINODE} --num_machines {self.cfg.slurm.n_nodes} --use_deepspeed --machine_rank {machine_rank} --main_process_ip {dist_env.master_addr} --main_process_port {dist_env.master_port} trainer/scripts/train.py {self.cfg.slurm.cmd}\"\n\n        if self.cfg.slurm.n_nodes > 1:\n            hostfile_dir = \"hostfiles\"\n            os.makedirs(hostfile_dir, exist_ok=True)\n            hostfile = os.path.realpath(f\"{hostfile_dir}/{dist_env._job_env.job_id}.txt\")\n            if dist_env.rank == 0:\n                with open(hostfile, \"w\") as f:\n                    for host in dist_env._job_env.hostnames:\n                        f.write(f\"{host} slots={self.cfg.slurm.n_processes}\\n\")\n                print(f\"Created hostfile: {hostfile}\")\n            cmd = cmd.replace(DEEPSPEED_MULTINODE, f\"--deepspeed_hostfile {hostfile} --deepspeed_multinode_launcher standard\")\n        else:\n            cmd = cmd.replace(DEEPSPEED_MULTINODE, \"\")\n\n        if dist_env.local_rank == 0:\n            print(f\"Running command: {cmd}\")\n            exit_code = os.system(cmd)\n        else:\n            exit_code = 0\n            print(\"Waiting for master to finish\")\n        if exit_code != 0:\n            raise RuntimeError(f\"Command {cmd} failed with exit code {exit_code}\")\n\n    def checkpoint(self):\n        print(\"checkpointing\")\n        return submitit.helpers.DelayedSubmission(self)", "\n\n@hydra.main(version_base=None, config_path=\"../conf\", config_name=\"slurm_config\")\ndef main(cfg: DictConfig) -> None:\n    # import pydevd_pycharm\n    # pydevd_pycharm.settrace('localhost', port=5900, stdoutToServer=True, stderrToServer=True)\n    executor = submitit.AutoExecutor(folder=\"logs\")\n    print_config(cfg)\n\n    slurm_additional_parameters = {\n        \"gpus\": cfg.slurm.n_processes,\n        \"ntasks_per_node\": cfg.slurm.n_processes,\n    }\n\n    if cfg.slurm.account is not None:\n        slurm_additional_parameters[\"account\"] = cfg.slurm.account\n\n    print(f\"SLURM additional parameters: {slurm_additional_parameters}\")\n\n    slurm_kwargs = {\n        \"slurm_job_name\": cfg.slurm.job_name,\n        \"slurm_partition\": cfg.slurm.partition,\n        \"slurm_nodes\": cfg.slurm.n_nodes,\n        \"slurm_additional_parameters\": slurm_additional_parameters,\n        \"slurm_cpus_per_task\": 12,\n        \"slurm_time\": cfg.slurm.time_limit,\n        \"slurm_exclude\": cfg.slurm.exclude if cfg.slurm.exclude else \"\",\n        \"stderr_to_stdout\": True,\n        \"slurm_mem\": \"50GB\",\n    }\n    executor.update_parameters(**slurm_kwargs)\n\n    task = Task(cfg)\n    job = executor.submit(task)\n    submitit.helpers.monitor_jobs([job])", "\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"]}
{"filename": "trainer/datasetss/clip_hf_dataset.py", "chunked_list": ["from dataclasses import dataclass\nfrom io import BytesIO\nfrom typing import Optional\n\nimport torch\nfrom PIL import Image\nfrom accelerate.logging import get_logger\nfrom datasets import load_from_disk, load_dataset, Dataset\nfrom hydra.utils import instantiate\nfrom omegaconf import II", "from hydra.utils import instantiate\nfrom omegaconf import II\n\nfrom trainer.datasetss.base_dataset import BaseDataset, BaseDatasetConfig\n\nlogger = get_logger(__name__)\n\n\ndef simple_collate(batch, column_name):\n    return torch.cat([item[column_name] for item in batch], dim=0)", "def simple_collate(batch, column_name):\n    return torch.cat([item[column_name] for item in batch], dim=0)\n\n\n@dataclass\nclass ProcessorConfig:\n    _target_: str = \"transformers.AutoProcessor.from_pretrained\"\n    pretrained_model_name_or_path: str = II(\"model.pretrained_model_name_or_path\")\n\n", "\n\n@dataclass\nclass CLIPHFDatasetConfig(BaseDatasetConfig):\n    _target_: str = \"trainer.datasetss.clip_hf_dataset.CLIPHFDataset\"\n    dataset_name: str = \"yuvalkirstain/pickapic_v1\"\n    dataset_config_name: str = \"null\"\n\n    from_disk: bool = False\n    train_split_name: str = \"train\"\n    valid_split_name: str = \"validation_unique\"\n    test_split_name: str = \"test_unique\"\n    cache_dir: Optional[str] = None\n\n    caption_column_name: str = \"caption\"\n    input_ids_column_name: str = \"input_ids\"\n    image_0_column_name: str = \"jpg_0\"\n    image_1_column_name: str = \"jpg_1\"\n    label_0_column_name: str = \"label_0\"\n    label_1_column_name: str = \"label_1\"\n    are_different_column_name: str = \"are_different\"\n    has_label_column_name: str = \"has_label\"\n\n    pixels_0_column_name: str = \"pixel_values_0\"\n    pixels_1_column_name: str = \"pixel_values_1\"\n\n    num_examples_per_prompt_column_name: str = \"num_example_per_prompt\"\n\n    keep_only_different: bool = False\n    keep_only_with_label: bool = False\n    keep_only_with_label_in_non_train: bool = True\n\n    processor: ProcessorConfig = ProcessorConfig()\n\n    limit_examples_per_prompt: int = -1\n\n    only_on_best: bool = False", "\n\nclass CLIPHFDataset(BaseDataset):\n\n    def __init__(self, cfg: CLIPHFDatasetConfig, split: str = \"train\"):\n        self.cfg = cfg\n        self.split = split\n        logger.info(f\"Loading {self.split} dataset\")\n\n        self.dataset = self.load_hf_dataset(self.split)\n        logger.info(f\"Loaded {len(self.dataset)} examples from {self.split} dataset\")\n\n        if self.cfg.keep_only_different:\n            self.dataset = self.dataset.filter(lambda x: x[self.cfg.are_different_column_name])\n\n        if self.cfg.keep_only_with_label:\n            logger.info(f\"Keeping only examples with label\")\n            self.dataset = self.dataset.filter(lambda x: x[self.cfg.has_label_column_name])\n            logger.info(f\"Kept {len(self.dataset)} examples from {self.split} dataset\")\n        elif self.cfg.keep_only_with_label_in_non_train and self.split != self.cfg.train_split_name:\n            logger.info(f\"Keeping only examples with label in {self.split} split\")\n            self.dataset = self.dataset.filter(lambda x: x[self.cfg.has_label_column_name])\n            logger.info(f\"Kept {len(self.dataset)} examples from {self.split} dataset\")\n\n        if self.cfg.limit_examples_per_prompt > 0:\n            logger.info(f\"Limiting examples per prompt to {self.cfg.limit_examples_per_prompt}\")\n            df = self.dataset.to_pandas()\n            df = df.drop('__index_level_0__', axis=1)\n            logger.info(f\"Loaded {len(df)} examples from {self.split} dataset\")\n            df = df.groupby(self.cfg.caption_column_name).head(self.cfg.limit_examples_per_prompt)\n            logger.info(f\"Kept {len(df)} examples from {self.split} dataset\")\n            self.dataset = Dataset.from_pandas(df)\n\n        if self.cfg.only_on_best and self.split == self.cfg.train_split_name:\n            logger.info(f\"Keeping only best examples for training\")\n            train_dataset = self.dataset.remove_columns([self.cfg.image_0_column_name, self.cfg.image_1_column_name])\n            df = train_dataset.to_pandas()\n            df = df[df[self.cfg.has_label_column_name] == 1]\n            image_0_wins_df = df[df[self.cfg.label_0_column_name] == 1]\n            image_1_wins_df = df[df[self.cfg.label_0_column_name] == 0]\n            bad_image_0_to_good_image_1 = dict(zip(image_1_wins_df.image_0_uid, image_1_wins_df.image_1_uid))\n            bad_image_1_to_good_image_0 = dict(zip(image_0_wins_df.image_1_uid, image_0_wins_df.image_0_uid))\n            bad_images_uids2good_images_uids = bad_image_0_to_good_image_1 | bad_image_1_to_good_image_0\n            image_0_uid2image_col_name = dict(zip(df.image_0_uid, [self.cfg.image_0_column_name] * len(df.image_0_uid)))\n            image_1_uid2image_col_name = dict(zip(df.image_1_uid, [self.cfg.image_1_column_name] * len(df.image_1_uid)))\n            uid2image_col_name = image_0_uid2image_col_name | image_1_uid2image_col_name\n\n            bad_uids = set()\n            for bad_image, good_image in bad_images_uids2good_images_uids.items():\n                cur_good = {bad_image}\n                while good_image in bad_images_uids2good_images_uids:\n                    if good_image in cur_good:\n                        bad_uids.add(bad_image)\n                        break\n                    cur_good.add(good_image)\n                    good_image = bad_images_uids2good_images_uids[good_image]\n                bad_images_uids2good_images_uids[bad_image] = good_image\n\n            df = df[~(df.image_0_uid.isin(bad_uids) | df.image_1_uid.isin(bad_uids))]\n            keep_ids = df.index.tolist()\n            self.dataset = self.dataset.select(keep_ids)\n            new_ids = list(range(len(df)))\n            uid2index = dict(zip(df.image_0_uid, new_ids)) | dict(zip(df.image_1_uid, new_ids))\n            logger.info(f\"Kept only {len(self.dataset)} best examples for training\")\n            self.bad_images_uids2good_images_uids = bad_images_uids2good_images_uids\n            self.uid2index = uid2index\n            self.uid2image_col_name = uid2image_col_name\n\n        logger.info(f\"Loaded {len(self.dataset)} examples from {self.split} dataset\")\n\n        processor = instantiate(cfg.processor)\n        self.tokenizer = processor.tokenizer\n        self.image_processor = processor.image_processor\n\n    def load_hf_dataset(self, split: str) -> Dataset:\n        if self.cfg.from_disk:\n            dataset = load_from_disk(self.cfg.dataset_name)[split]\n        else:\n            dataset = load_dataset(\n                self.cfg.dataset_name,\n                self.cfg.dataset_config_name,\n                cache_dir=self.cfg.cache_dir,\n                split=split\n            )\n        return dataset\n\n    def tokenize(self, example):\n        caption = example[self.cfg.caption_column_name]\n        input_ids = self.tokenizer(\n            caption,\n            max_length=self.tokenizer.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).input_ids\n        return input_ids\n\n    def process_image(self, image):\n        if isinstance(image, dict):\n            image = image[\"bytes\"]\n        if isinstance(image, bytes):\n            image = Image.open(BytesIO(image))\n        image = image.convert(\"RGB\")\n        pixel_values = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n        return pixel_values\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n\n        if self.cfg.only_on_best and self.split == self.cfg.train_split_name:\n            if example[self.cfg.label_0_column_name]:\n                bad_image_uid = example[\"image_1_uid\"]\n                good_image_column_name = self.cfg.image_0_column_name\n            else:\n                bad_image_uid = example[\"image_0_uid\"]\n                good_image_column_name = self.cfg.image_1_column_name\n            good_image_uid = self.bad_images_uids2good_images_uids[bad_image_uid]\n            good_image_index = self.uid2index[good_image_uid]\n            example[good_image_column_name] = self.dataset[good_image_index][self.uid2image_col_name[good_image_uid]]\n\n        input_ids = self.tokenize(example)\n\n        pixel_0_values = self.process_image(example[self.cfg.image_0_column_name])\n        pixel_1_values = self.process_image(example[self.cfg.image_1_column_name])\n\n        item = {\n            self.cfg.input_ids_column_name: input_ids,\n            self.cfg.pixels_0_column_name: pixel_0_values,\n            self.cfg.pixels_1_column_name: pixel_1_values,\n            self.cfg.label_0_column_name: torch.tensor(example[self.cfg.label_0_column_name])[None],\n            self.cfg.label_1_column_name: torch.tensor(example[self.cfg.label_1_column_name])[None],\n            self.cfg.num_examples_per_prompt_column_name: torch.tensor(example[self.cfg.num_examples_per_prompt_column_name])[None],\n        }\n        return item\n\n    def collate_fn(self, batch):\n        input_ids = simple_collate(batch, self.cfg.input_ids_column_name)\n        pixel_0_values = simple_collate(batch, self.cfg.pixels_0_column_name)\n        pixel_1_values = simple_collate(batch, self.cfg.pixels_1_column_name)\n        label_0 = simple_collate(batch, self.cfg.label_0_column_name)\n        label_1 = simple_collate(batch, self.cfg.label_1_column_name)\n        num_examples_per_prompt = simple_collate(batch, self.cfg.num_examples_per_prompt_column_name)\n\n        pixel_0_values = pixel_0_values.to(memory_format=torch.contiguous_format).float()\n        pixel_1_values = pixel_1_values.to(memory_format=torch.contiguous_format).float()\n\n        collated = {\n            self.cfg.input_ids_column_name: input_ids,\n            self.cfg.pixels_0_column_name: pixel_0_values,\n            self.cfg.pixels_1_column_name: pixel_1_values,\n            self.cfg.label_0_column_name: label_0,\n            self.cfg.label_1_column_name: label_1,\n            self.cfg.num_examples_per_prompt_column_name: num_examples_per_prompt,\n        }\n        return collated\n\n    def __len__(self):\n        return len(self.dataset)", ""]}
{"filename": "trainer/datasetss/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.datasetss.clip_hf_dataset import CLIPHFDatasetConfig\n\ncs = ConfigStore.instance()\ncs.store(group=\"dataset\", name=\"clip\", node=CLIPHFDatasetConfig)\n"]}
{"filename": "trainer/datasetss/base_dataset.py", "chunked_list": ["from dataclasses import dataclass\n\nimport torch\n\n\n@dataclass\nclass BaseDatasetConfig:\n    train_split_name: str = \"train\"\n    valid_split_name: str = \"validation\"\n    test_split_name: str = \"test\"\n\n    batch_size: int = 4\n    num_workers: int = 2\n    drop_last: bool = True", "\n\nclass BaseDataset(torch.utils.data.Dataset):\n    pass\n"]}
{"filename": "trainer/configs/configs.py", "chunked_list": ["from dataclasses import dataclass, field\nfrom typing import List, Any, Dict\n\nfrom omegaconf import DictConfig, MISSING\n\nimport trainer.accelerators\nimport trainer.tasks\nimport trainer.models\nimport trainer.criterions\nimport trainer.datasetss", "import trainer.criterions\nimport trainer.datasetss\nimport trainer.optimizers\nimport trainer.lr_schedulers\nfrom trainer.accelerators.base_accelerator import BaseAcceleratorConfig\nfrom trainer.models.base_model import BaseModelConfig\nfrom trainer.tasks.base_task import BaseTaskConfig\n\n\ndef _locate(path: str) -> Any:\n    \"\"\"\n    Locate an object by name or dotted path, importing as necessary.\n    This is similar to the pydoc function `locate`, except that it checks for\n    the module from the given path from back to front.\n    \"\"\"\n    if path == \"\":\n        raise ImportError(\"Empty path\")\n    from importlib import import_module\n    from types import ModuleType\n\n    parts = [part for part in path.split(\".\")]\n    for part in parts:\n        if not len(part):\n            raise ValueError(\n                f\"Error loading '{path}': invalid dotstring.\"\n                + \"\\nRelative imports are not supported.\"\n            )\n    assert len(parts) > 0\n    part0 = parts[0]\n    try:\n        obj = import_module(part0)\n    except Exception as exc_import:\n        raise ImportError(\n            f\"Error loading '{path}':\\n{repr(exc_import)}\"\n            + f\"\\nAre you sure that module '{part0}' is installed?\"\n        ) from exc_import\n    for m in range(1, len(parts)):\n        part = parts[m]\n        try:\n            obj = getattr(obj, part)\n        except AttributeError as exc_attr:\n            parent_dotpath = \".\".join(parts[:m])\n            if isinstance(obj, ModuleType):\n                mod = \".\".join(parts[: m + 1])\n                try:\n                    obj = import_module(mod)\n                    continue\n                except ModuleNotFoundError as exc_import:\n                    raise ImportError(\n                        f\"Error loading '{path}':\\n{repr(exc_import)}\"\n                        + f\"\\nAre you sure that '{part}' is importable from module '{parent_dotpath}'?\"\n                    ) from exc_import\n                except Exception as exc_import:\n                    raise ImportError(\n                        f\"Error loading '{path}':\\n{repr(exc_import)}\"\n                    ) from exc_import\n            raise ImportError(\n                f\"Error loading '{path}':\\n{repr(exc_attr)}\"\n                + f\"\\nAre you sure that '{part}' is an attribute of '{parent_dotpath}'?\"\n            ) from exc_attr\n    return obj", "\ndef _locate(path: str) -> Any:\n    \"\"\"\n    Locate an object by name or dotted path, importing as necessary.\n    This is similar to the pydoc function `locate`, except that it checks for\n    the module from the given path from back to front.\n    \"\"\"\n    if path == \"\":\n        raise ImportError(\"Empty path\")\n    from importlib import import_module\n    from types import ModuleType\n\n    parts = [part for part in path.split(\".\")]\n    for part in parts:\n        if not len(part):\n            raise ValueError(\n                f\"Error loading '{path}': invalid dotstring.\"\n                + \"\\nRelative imports are not supported.\"\n            )\n    assert len(parts) > 0\n    part0 = parts[0]\n    try:\n        obj = import_module(part0)\n    except Exception as exc_import:\n        raise ImportError(\n            f\"Error loading '{path}':\\n{repr(exc_import)}\"\n            + f\"\\nAre you sure that module '{part0}' is installed?\"\n        ) from exc_import\n    for m in range(1, len(parts)):\n        part = parts[m]\n        try:\n            obj = getattr(obj, part)\n        except AttributeError as exc_attr:\n            parent_dotpath = \".\".join(parts[:m])\n            if isinstance(obj, ModuleType):\n                mod = \".\".join(parts[: m + 1])\n                try:\n                    obj = import_module(mod)\n                    continue\n                except ModuleNotFoundError as exc_import:\n                    raise ImportError(\n                        f\"Error loading '{path}':\\n{repr(exc_import)}\"\n                        + f\"\\nAre you sure that '{part}' is importable from module '{parent_dotpath}'?\"\n                    ) from exc_import\n                except Exception as exc_import:\n                    raise ImportError(\n                        f\"Error loading '{path}':\\n{repr(exc_import)}\"\n                    ) from exc_import\n            raise ImportError(\n                f\"Error loading '{path}':\\n{repr(exc_attr)}\"\n                + f\"\\nAre you sure that '{part}' is an attribute of '{parent_dotpath}'?\"\n            ) from exc_attr\n    return obj", "\n\ndef instantiate_with_cfg(cfg: DictConfig, **kwargs):\n    target = _locate(cfg._target_)\n    return target(cfg, **kwargs)\n\n\ndefaults = [\n    {\"accelerator\": \"deepspeed\"},\n    {\"task\": \"clip\"},", "    {\"accelerator\": \"deepspeed\"},\n    {\"task\": \"clip\"},\n    {\"model\": \"clip\"},\n    {\"criterion\": \"clip\"},\n    {\"dataset\": \"clip\"},\n    {\"optimizer\": \"dummy\"},\n    {\"lr_scheduler\": \"dummy\"},\n]\n\n", "\n\n@dataclass\nclass DebugConfig:\n    activate: bool = False\n    port: int = 5900\n\n\n@dataclass\nclass TrainerConfig:\n    defaults: List[Any] = field(default_factory=lambda: defaults)\n    accelerator: BaseAcceleratorConfig = MISSING\n    task: BaseTaskConfig = MISSING\n    model: BaseModelConfig = MISSING\n    criterion: Any = MISSING\n    dataset: Any = MISSING\n    optimizer: Any = MISSING\n    lr_scheduler: Any = MISSING\n    debug: DebugConfig = DebugConfig()\n    output_dir: str = \"outputs\"", "@dataclass\nclass TrainerConfig:\n    defaults: List[Any] = field(default_factory=lambda: defaults)\n    accelerator: BaseAcceleratorConfig = MISSING\n    task: BaseTaskConfig = MISSING\n    model: BaseModelConfig = MISSING\n    criterion: Any = MISSING\n    dataset: Any = MISSING\n    optimizer: Any = MISSING\n    lr_scheduler: Any = MISSING\n    debug: DebugConfig = DebugConfig()\n    output_dir: str = \"outputs\"", ""]}
{"filename": "trainer/configs/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.configs.configs import TrainerConfig\n\ncs = ConfigStore.instance()\ncs.store(name=\"base_config\", node=TrainerConfig)\n"]}
{"filename": "trainer/utils/data_utils.py", "chunked_list": ["import logging\nfrom glob import glob\nfrom io import BytesIO\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset, concatenate_datasets, Dataset, load_from_disk\n\nlogger = logging.getLogger(__name__)\n\n\ndef parquet2dataset(parquet_path: str):\n    datasets = []\n    for path in sorted(glob(f\"{parquet_path}/*.parquet\")):\n        datasets.append(load_dataset(\"parquet\", data_files=path)[\"train\"])\n    dataset = concatenate_datasets(datasets)\n    return dataset", "\n\ndef parquet2dataset(parquet_path: str):\n    datasets = []\n    for path in sorted(glob(f\"{parquet_path}/*.parquet\")):\n        datasets.append(load_dataset(\"parquet\", data_files=path)[\"train\"])\n    dataset = concatenate_datasets(datasets)\n    return dataset\n\n\ndef bytes2image(bytes: bytes):\n    image = Image.open(BytesIO(bytes))\n    image = image.convert(\"RGB\")\n    return image", "\n\ndef bytes2image(bytes: bytes):\n    image = Image.open(BytesIO(bytes))\n    image = image.convert(\"RGB\")\n    return image\n\n\ndef dataset2images(dataset, pool, col):\n    image_bytes = dataset[col]\n    images = list(tqdm(pool.imap(bytes2image, image_bytes), total=len(image_bytes)))\n    return images", "def dataset2images(dataset, pool, col):\n    image_bytes = dataset[col]\n    images = list(tqdm(pool.imap(bytes2image, image_bytes), total=len(image_bytes)))\n    return images\n"]}
{"filename": "trainer/utils/__init__.py", "chunked_list": [""]}
{"filename": "trainer/utils/slurm_utils.py", "chunked_list": ["from collections import Counter\nfrom time import sleep\nfrom tqdm import tqdm\n\n\ndef track_jobs_with_pbar(jobs):\n    num_completed = 0\n    with tqdm(total=len(jobs)) as pbar:\n        while any(job.state not in [\"COMPLETED\", \"FAILED\", \"DONE\"] for job in jobs):\n            sleep(2)\n            job_infos = [j.get_info() for j in jobs]\n            state2count = Counter([info['State'] if 'State' in info else \"None\" for info in job_infos])\n            newly_completed = state2count[\"COMPLETED\"] - num_completed\n            pbar.update(newly_completed)\n            num_completed = state2count[\"COMPLETED\"]\n            s = [f\"{k}: {v}\" for k, v in state2count.items()]\n            pbar.set_description(\" | \".join(s))\n    return num_completed", ""]}
{"filename": "trainer/utils/FID/img_data.py", "chunked_list": ["import os\nimport torch\nfrom torch.utils import data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom datasets import Dataset as HFDataset\n\n\nclass Dataset(data.Dataset):\n    'Characterizes a dataset for PyTorch'\n\n    def __init__(self, path, transform=None):\n        'Initialization'\n        self.file_names = self.get_filenames(path)\n        self.transform = transform\n\n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.file_names)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        img = Image.open(self.file_names[index]).convert('RGB')\n        # Convert image and label to torch tensors\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def get_filenames(self, data_path):\n        images = []\n        for path, subdirs, files in os.walk(data_path):\n            for name in files:\n                if name.rfind('jpg') != -1 or name.rfind('png') != -1:\n                    filename = os.path.join(path, name)\n                    if os.path.isfile(filename):\n                        images.append(filename)\n        return images", "class Dataset(data.Dataset):\n    'Characterizes a dataset for PyTorch'\n\n    def __init__(self, path, transform=None):\n        'Initialization'\n        self.file_names = self.get_filenames(path)\n        self.transform = transform\n\n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.file_names)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        img = Image.open(self.file_names[index]).convert('RGB')\n        # Convert image and label to torch tensors\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def get_filenames(self, data_path):\n        images = []\n        for path, subdirs, files in os.walk(data_path):\n            for name in files:\n                if name.rfind('jpg') != -1 or name.rfind('png') != -1:\n                    filename = os.path.join(path, name)\n                    if os.path.isfile(filename):\n                        images.append(filename)\n        return images", "\n\nclass HFImgDataset:\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        example = self.dataset[item]\n        if self.transform is not None:\n            example[\"image\"] = self.transform(example[\"image\"])\n        return example[\"image\"]", "\n\nif __name__ == '__main__':\n    path = \"/media/twilightsnow/workspace/gan/AttnGAN/output/birds_attn2_2018_06_24_14_52_20/Model/netG_avg_epoch_300\"\n    batch_size = 16\n    dataset = Dataset(path, transforms.Compose([\n        transforms.Resize(299),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]))\n    print(dataset.__len__())\n    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n    for i, batch in enumerate(dataloader):\n        print(batch)\n        break", ""]}
{"filename": "trainer/utils/FID/inception.py", "chunked_list": ["import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        \"\"\"Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        \"\"\"\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in\n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output\n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode='bilinear', align_corners=True)\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp"]}
{"filename": "trainer/utils/FID/__init__.py", "chunked_list": [""]}
{"filename": "trainer/utils/FID/fid_score.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nPorted from https://github.com/MinfengZhu/DM-GAN/blob/master/eval/FID/fid_score.py\n\nCalculates the Frechet Inception Distance (FID) to evalulate GANs\n\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\nWhen run as a stand-alone program, it compares the distribution of", "of these distributions, while the 2nd distribution is given by a GAN.\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectivly.\nSee --help to see further details.\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow", "Code apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow\nCopyright 2018 Institute of Bioinformatics, JKU Linz\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n   http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "distributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport os\nimport pathlib\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nfrom glob import glob\n", "from glob import glob\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom datasets import load_from_disk, concatenate_datasets\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC", "\nfrom imageio import imread\nfrom scipy import linalg\nfrom torch.autograd import Variable\nfrom torch.nn.functional import adaptive_avg_pool2d\nimport torchvision.transforms as transforms\nimport torch.utils.data\nfrom PIL import Image\nfrom torch.utils import data\nfrom trainer.utils.FID.inception import InceptionV3", "from torch.utils import data\nfrom trainer.utils.FID.inception import InceptionV3\nimport trainer.utils.FID.img_data as img_data\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n#parser.add_argument('path', type=str, nargs=2,\n#                    help=('Path to the generated images or '\n#                          'to .npz statistic files'))\nparser.add_argument('--batch-size', type=int, default=64,\n                    help='Batch size to use')", "parser.add_argument('--batch-size', type=int, default=64,\n                    help='Batch size to use')\nparser.add_argument('--dims', type=int, default=2048,\n                    choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n                    help=('Dimensionality of Inception features to use. '\n                          'By default, uses pool3 features'))\nparser.add_argument('-c', '--gpu', default='', type=str,\n                    help='GPU to use (leave blank for CPU only)')\nparser.add_argument('--path1', type=str, default=64)\nparser.add_argument('--path2', type=str, default=64)", "parser.add_argument('--path1', type=str, default=64)\nparser.add_argument('--path2', type=str, default=64)\n\ndef get_activations(images, model, batch_size=64, dims=2048, cuda=False, verbose=True):\n    \"\"\"Calculates the activations of the pool_3 layer for all images.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : the images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size depends\n                     on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    \"\"\"\n    model.eval()\n\n    #d0 = images.shape[0]\n\n    d0 = images.__len__() * batch_size\n    if batch_size > d0:\n        print(('Warning: batch size is bigger than the data size. '\n               'Setting batch size to data size'))\n        batch_size = d0\n\n    n_batches = d0 // batch_size\n    n_used_imgs = n_batches * batch_size\n\n    pred_arr = np.empty((n_used_imgs, dims))\n    #for i in range(n_batches):\n    for i, batch in enumerate(images):\n        #batch = batch[0]\n        #if verbose:\n            #print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n        #import ipdb\n        #ipdb.set_trace()\n        start = i * batch_size\n        end = start + batch_size\n\n        #batch = torch.from_numpy(images[start:end]).type(torch.FloatTensor)\n        #batch = Variable(batch, volatile=True)\n\n        if cuda:\n            batch = batch.cuda()\n\n        pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n\n    if verbose:\n        print(' done')\n\n    return pred_arr", "\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    Stable version by Dougal J. Sutherland.\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function 'get_predictions')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an\n               representive data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an\n               representive data set.\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)", "\n\ndef calculate_activation_statistics(images, model, batch_size=64,\n                                    dims=2048, cuda=False, verbose=True):\n    \"\"\"Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : The images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size\n                     depends on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the\n                     number of calculated batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the inception model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the inception model.\n    \"\"\"\n    act = get_activations(images, model, batch_size, dims, cuda, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma", "\ndef _compute_statistics_of_path(path, model, batch_size, dims, cuda):\n    if path.endswith('.npz'):\n        f = np.load(path)\n        m, s = f['mu'][:], f['sigma'][:]\n        f.close()\n\n    else:\n        dataset_transforms = transforms.Compose([\n            transforms.Resize(256, interpolation=BICUBIC),\n            transforms.CenterCrop(256),\n            transforms.Resize((299, 299)),\n            transforms.ToTensor(),\n        ])\n        if path.endswith('*'):\n            dataset = concatenate_datasets([load_from_disk(ds_path) for ds_path in glob(path)])\n            dataset = img_data.HFImgDataset(dataset, dataset_transforms)\n        else:\n            dataset = img_data.Dataset(path, dataset_transforms)\n        print(dataset.__len__())\n        dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=8)\n        m, s = calculate_activation_statistics(dataloader, model, batch_size, dims, cuda)\n    return m, s", "\ndef calculate_fid_given_paths(paths, batch_size, cuda, dims):\n    \"\"\"Calculates the FID of two paths\"\"\"\n    for p in paths:\n        if not os.path.exists(p) and \"*\" not in p:\n            raise RuntimeError('Invalid path: %s' % p)\n\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n    model = InceptionV3([block_idx])\n    if cuda:\n        model.cuda()\n\n    m1, s1 = _compute_statistics_of_path(paths[0], model, batch_size, dims, cuda)\n    m2, s2 = _compute_statistics_of_path(paths[1], model, batch_size, dims, cuda)\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n    return fid_value", "\n@torch.no_grad()\ndef image2pred(model, batch):\n    model.eval()\n    pred = model(batch)[0]\n\n    # If model output is not scalar, apply global spatial average pooling.\n    # This happens if you choose a dimensionality not equal 2048.\n    if pred.shape[2] != 1 or pred.shape[3] != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    pred = pred.data.view(batch.size(0), -1)\n\n    return pred", "\nif __name__ == '__main__':\n    args = parser.parse_args()\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n    paths = [\"\",\"\"]\n    paths[0] = args.path1\n    paths[1] = args.path2\n    print(paths)\n    fid_value = calculate_fid_given_paths(paths, args.batch_size,args.gpu,args.dims)\n    print('FID: ', fid_value)"]}
{"filename": "trainer/scripts/train.py", "chunked_list": ["import json\nimport os\nfrom typing import Any\n\nimport hydra\nimport torch\nfrom hydra.utils import instantiate\nfrom accelerate.logging import get_logger\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch import nn", "from omegaconf import DictConfig, OmegaConf\nfrom torch import nn\n\nfrom trainer.accelerators.base_accelerator import BaseAccelerator\nfrom trainer.configs.configs import TrainerConfig, instantiate_with_cfg\n\nlogger = get_logger(__name__)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef load_dataloaders(cfg: DictConfig) -> Any:\n    dataloaders = {}\n    for split in [cfg.train_split_name, cfg.valid_split_name, cfg.test_split_name]:\n        dataset = instantiate_with_cfg(cfg, split=split)\n        should_shuffle = split == cfg.train_split_name\n        dataloaders[split] = torch.utils.data.DataLoader(\n            dataset,\n            shuffle=should_shuffle,\n            batch_size=cfg.batch_size,\n            collate_fn=dataset.collate_fn,\n            num_workers=cfg.num_workers\n        )\n    return dataloaders", "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef load_dataloaders(cfg: DictConfig) -> Any:\n    dataloaders = {}\n    for split in [cfg.train_split_name, cfg.valid_split_name, cfg.test_split_name]:\n        dataset = instantiate_with_cfg(cfg, split=split)\n        should_shuffle = split == cfg.train_split_name\n        dataloaders[split] = torch.utils.data.DataLoader(\n            dataset,\n            shuffle=should_shuffle,\n            batch_size=cfg.batch_size,\n            collate_fn=dataset.collate_fn,\n            num_workers=cfg.num_workers\n        )\n    return dataloaders", "\n\ndef load_optimizer(cfg: DictConfig, model: nn.Module):\n    optimizer = instantiate(cfg, model=model)\n    return optimizer\n\n\ndef load_scheduler(cfg: DictConfig, optimizer):\n    scheduler = instantiate_with_cfg(cfg, optimizer=optimizer)\n    return scheduler", "\n\ndef load_task(cfg: DictConfig, accelerator: BaseAccelerator):\n    task = instantiate_with_cfg(cfg, accelerator=accelerator)\n    return task\n\n\ndef verify_or_write_config(cfg: TrainerConfig):\n    os.makedirs(cfg.output_dir, exist_ok=True)\n    yaml_path = os.path.join(cfg.output_dir, \"config.yaml\")\n    if not os.path.exists(yaml_path):\n        OmegaConf.save(cfg, yaml_path, resolve=True)\n    with open(yaml_path) as f:\n        existing_config = f.read()\n    if existing_config != OmegaConf.to_yaml(cfg, resolve=True):\n        raise ValueError(f\"Config was not saved correctly - {yaml_path}\")\n    logger.info(f\"Config can be found in {yaml_path}\")", "\n\n@hydra.main(version_base=None, config_path=\"../conf\", config_name=\"config\")\ndef main(cfg: TrainerConfig) -> None:\n    accelerator = instantiate_with_cfg(cfg.accelerator)\n\n    if cfg.debug.activate and accelerator.is_main_process:\n        import pydevd_pycharm\n        pydevd_pycharm.settrace('localhost', port=cfg.debug.port, stdoutToServer=True, stderrToServer=True)\n\n    if accelerator.is_main_process:\n        verify_or_write_config(cfg)\n\n    logger.info(f\"Loading task\")\n    task = load_task(cfg.task, accelerator)\n    logger.info(f\"Loading model\")\n    model = instantiate_with_cfg(cfg.model)\n    logger.info(f\"Loading criterion\")\n    criterion = instantiate_with_cfg(cfg.criterion)\n    logger.info(f\"Loading optimizer\")\n    optimizer = load_optimizer(cfg.optimizer, model)\n    logger.info(f\"Loading lr scheduler\")\n    lr_scheduler = load_scheduler(cfg.lr_scheduler, optimizer)\n    logger.info(f\"Loading dataloaders\")\n    split2dataloader = load_dataloaders(cfg.dataset)\n\n    dataloaders = list(split2dataloader.values())\n    model, optimizer, lr_scheduler, *dataloaders = accelerator.prepare(model, optimizer, lr_scheduler, *dataloaders)\n    split2dataloader = dict(zip(split2dataloader.keys(), dataloaders))\n\n    accelerator.load_state_if_needed()\n\n    accelerator.recalc_train_length_after_prepare(len(split2dataloader[cfg.dataset.train_split_name]))\n\n    accelerator.init_training(cfg)\n\n    def evaluate():\n        model.eval()\n        end_of_train_dataloader = accelerator.gradient_state.end_of_dataloader\n        logger.info(f\"*** Evaluating {cfg.dataset.valid_split_name} ***\")\n        metrics = task.evaluate(model, criterion, split2dataloader[cfg.dataset.valid_split_name])\n        accelerator.update_metrics(metrics)\n        accelerator.gradient_state.end_of_dataloader = end_of_train_dataloader\n\n    logger.info(f\"task: {task.__class__.__name__}\")\n    logger.info(f\"model: {model.__class__.__name__}\")\n    logger.info(f\"num. model params: {int(sum(p.numel() for p in model.parameters()) // 1e6)}M\")\n    logger.info(\n        f\"num. model trainable params: {int(sum(p.numel() for p in model.parameters() if p.requires_grad) // 1e6)}M\")\n    logger.info(f\"criterion: {criterion.__class__.__name__}\")\n    logger.info(f\"num. train examples: {len(split2dataloader[cfg.dataset.train_split_name].dataset)}\")\n    logger.info(f\"num. valid examples: {len(split2dataloader[cfg.dataset.valid_split_name].dataset)}\")\n    logger.info(f\"num. test examples: {len(split2dataloader[cfg.dataset.test_split_name].dataset)}\")\n\n    for epoch in range(accelerator.cfg.num_epochs):\n        train_loss, lr = 0.0, 0.0\n        for step, batch in enumerate(split2dataloader[cfg.dataset.train_split_name]):\n            if accelerator.should_skip(epoch, step):\n                accelerator.update_progbar_step()\n                continue\n\n            if accelerator.should_eval():\n                evaluate()\n\n            if accelerator.should_save():\n                accelerator.save_checkpoint()\n\n            model.train()\n\n            with accelerator.accumulate(model):\n                loss = task.train_step(model, criterion, batch)\n                avg_loss = accelerator.gather(loss).mean().item()\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters())\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            train_loss += avg_loss / accelerator.cfg.gradient_accumulation_steps\n\n            if accelerator.sync_gradients:\n                accelerator.update_global_step(train_loss)\n                train_loss = 0.0\n\n            if accelerator.global_step > 0:\n                lr = lr_scheduler.get_last_lr()[0]\n\n            accelerator.update_step(avg_loss, lr)\n\n            if accelerator.should_end():\n                evaluate()\n                accelerator.save_checkpoint()\n                break\n\n        if accelerator.should_end():\n            break\n\n        accelerator.update_epoch()\n\n    accelerator.wait_for_everyone()\n    accelerator.load_best_checkpoint()\n    logger.info(f\"*** Evaluating {cfg.dataset.valid_split_name} ***\")\n    metrics = task.evaluate(model, criterion, split2dataloader[cfg.dataset.valid_split_name])\n    accelerator.update_metrics(metrics)\n    logger.info(f\"*** Evaluating {cfg.dataset.test_split_name} ***\")\n    metrics = task.evaluate(model, criterion, split2dataloader[cfg.dataset.test_split_name])\n    metrics = {f\"{cfg.dataset.test_split_name}_{k}\": v for k, v in metrics.items()}\n    accelerator.update_metrics(metrics)\n    accelerator.unwrap_and_save(model)\n    accelerator.end_training()", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "trainer/scripts/eval_preference_predictor.py", "chunked_list": ["import numpy as np\nfrom transformers import AutoProcessor, AutoModel\nfrom datasets import load_from_disk, load_dataset\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\nfrom tqdm.auto import tqdm\nfrom fire import Fire\n\n\ndef open_image(image):\n    if isinstance(image, bytes):\n        image = Image.open(BytesIO(image))\n    image = image.convert(\"RGB\")\n    return image", "\n\ndef open_image(image):\n    if isinstance(image, bytes):\n        image = Image.open(BytesIO(image))\n    image = image.convert(\"RGB\")\n    return image\n\n\n@torch.no_grad()\ndef infer_example(images, prompt, clip_model, clip_processor, device):\n    images = [open_image(image) for image in images]\n\n    image_inputs = clip_processor(\n        images=images,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    text_inputs = clip_processor(\n        text=prompt,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    with torch.no_grad():\n        image_embs = clip_model.get_image_features(**image_inputs)\n        image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)\n\n        text_embs = clip_model.get_text_features(**text_inputs)\n        text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)\n\n        scores = clip_model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n\n        probs = torch.softmax(scores, dim=-1)\n\n    return probs.cpu().tolist()", "\n@torch.no_grad()\ndef infer_example(images, prompt, clip_model, clip_processor, device):\n    images = [open_image(image) for image in images]\n\n    image_inputs = clip_processor(\n        images=images,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    text_inputs = clip_processor(\n        text=prompt,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    with torch.no_grad():\n        image_embs = clip_model.get_image_features(**image_inputs)\n        image_embs = image_embs / torch.norm(image_embs, dim=-1, keepdim=True)\n\n        text_embs = clip_model.get_text_features(**text_inputs)\n        text_embs = text_embs / torch.norm(text_embs, dim=-1, keepdim=True)\n\n        scores = clip_model.logit_scale.exp() * (text_embs @ image_embs.T)[0]\n\n        probs = torch.softmax(scores, dim=-1)\n\n    return probs.cpu().tolist()", "\n\ndef calc_probs_for_dataset(ds, clip_model, clip_processor, device):\n    probs = []\n    for example in tqdm(ds):\n        prob_0, prob_1 = infer_example(\n            [example[\"jpg_0\"], example[\"jpg_1\"]],\n            example[\"caption\"],\n            clip_model,\n            clip_processor,\n            device\n        )\n        probs.append((prob_0, prob_1))\n    return probs", "\n\ndef get_label(example):\n    if example[\"label_0\"] == 0.5:\n        label = \"tie\"\n    elif example[\"label_0\"] == 1:\n        label = \"0\"\n    else:\n        label = \"1\"\n    return label", "\n\ndef get_pred(prob_0, prob_1, threshold):\n    if abs(prob_1 - prob_0) <= threshold:\n        pred = \"tie\"\n    elif prob_0 > prob_1:\n        pred = \"0\"\n    else:\n        pred = \"1\"\n    return pred", "\n\ndef calc_score(label, pred):\n    if label == pred:\n        score = 1\n    elif \"tie\" in [label, pred]:\n        score = 0.5\n    else:\n        score = 0\n    return score", "\n\ndef calc_acc(probs, ds, threshold):\n    res = []\n    for example, (prob_0, prob_1) in zip(ds, probs):\n        label = get_label(example)\n        pred = get_pred(prob_0, prob_1, threshold)\n        score = calc_score(label, pred)\n        res.append(score)\n    return sum(res) / len(res)", "\n\ndef calc_scores(ds, thresholds, clip_model, clip_processor, device):\n    if isinstance(thresholds, (float, int)):\n        thresholds = [thresholds]\n\n    probs = calc_probs_for_dataset(ds, clip_model, clip_processor, device)\n\n    res = []\n    for threshold in thresholds:\n        acc = calc_acc(probs, ds, threshold)\n        res.append(acc)\n\n    return res", "\n\ndef calc_random_scores(ds, thresholds):\n    if isinstance(thresholds, (float, int)):\n        thresholds = [thresholds]\n\n    total = len(ds)\n    num_ties = len(ds.filter(lambda x: x[\"label_0\"] == 0.5))\n    num_no_ties = total - num_ties\n    random_scores = [((threshold * 1 + ((1 - threshold) / 2)) * num_ties / total) + ((num_no_ties / total) * 0.5) for\n                     threshold\n                     in thresholds]\n\n    return random_scores", "\n\ndef main(processor_pretrained_name_or_path: str = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n         model_pretrained_name_or_path: str = \"yuvalkirstain/PickScore_v1\",\n         dataset_name_or_path: str = \"yuvalkirstain/pickapic_v1\",\n         should_load_from_disk: bool = False):\n    device = \"cuda:0\"\n\n    print(f\"Loading dataset {dataset_name_or_path}\")\n    if should_load_from_disk:\n        dataset = load_from_disk(dataset_name_or_path)\n    else:\n        dataset = load_dataset(dataset_name_or_path)\n\n    print(f\"Loading model {model_pretrained_name_or_path}\")\n    clip_processor = AutoProcessor.from_pretrained(processor_pretrained_name_or_path)\n    clip_model = AutoModel.from_pretrained(model_pretrained_name_or_path).eval().to(device)\n\n    thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n    validation_split = \"validation_unique\"\n    test_split = \"test_unique\"\n\n    print(\"Calculating validation accuracy\")\n    ours_validation_results = calc_scores(dataset[validation_split], thresholds, clip_model, clip_processor, device)\n    best_threshold = np.argmax(ours_validation_results)\n    print(ours_validation_results)\n    print(f\"Best Threshold: {thresholds[best_threshold]} | Acc: {ours_validation_results[best_threshold]}\")\n    print(\"Calculating test accuracy\")\n    test_result = calc_scores(dataset[test_split], thresholds[best_threshold], clip_model, clip_processor, device)\n    print(f\"Test Acc: {test_result[0]}\")", "\n\nif __name__ == '__main__':\n    Fire(main)\n"]}
{"filename": "trainer/lr_schedulers/dummy_lr_scheduler.py", "chunked_list": ["from dataclasses import dataclass\n\nimport torch\nfrom accelerate.utils import DummyScheduler\nfrom hydra.utils import instantiate\nfrom omegaconf import II\n\ntry:\n    import torch.distributed.nn\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False", "\n\n@dataclass\nclass DummyLRSchedulerConfig:\n    _target_: str = \"trainer.lr_schedulers.dummy_lr_scheduler.instantiate_dummy_lr_scheduler\"\n    lr: float = II(\"optimizer.lr\")\n    lr_warmup_steps: int = 500\n    total_num_steps: int = II(\"accelerator.max_steps\")\n\n\ndef instantiate_dummy_lr_scheduler(cfg: DummyLRSchedulerConfig, optimizer):\n    try:\n        num_processes = torch.distributed.get_world_size()\n    except RuntimeError:\n        num_processes = 1\n    return DummyScheduler(\n        optimizer,\n        total_num_steps=cfg.total_num_steps * num_processes,\n        warmup_num_steps=cfg.lr_warmup_steps,\n        warmup_max_lr=cfg.lr,\n    )", "\n\ndef instantiate_dummy_lr_scheduler(cfg: DummyLRSchedulerConfig, optimizer):\n    try:\n        num_processes = torch.distributed.get_world_size()\n    except RuntimeError:\n        num_processes = 1\n    return DummyScheduler(\n        optimizer,\n        total_num_steps=cfg.total_num_steps * num_processes,\n        warmup_num_steps=cfg.lr_warmup_steps,\n        warmup_max_lr=cfg.lr,\n    )", ""]}
{"filename": "trainer/lr_schedulers/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.lr_schedulers.constant_with_warmup import ConstantWithWarmupLRSchedulerConfig\nfrom trainer.lr_schedulers.dummy_lr_scheduler import DummyLRSchedulerConfig\n\ncs = ConfigStore.instance()\ncs.store(group=\"lr_scheduler\", name=\"dummy\", node=DummyLRSchedulerConfig)\ncs.store(group=\"lr_scheduler\", name=\"constant_with_warmup\", node=ConstantWithWarmupLRSchedulerConfig)\n", ""]}
{"filename": "trainer/lr_schedulers/constant_with_warmup.py", "chunked_list": ["from dataclasses import dataclass\n\nfrom omegaconf import II\nfrom transformers import get_constant_schedule_with_warmup\n\n\n@dataclass\nclass ConstantWithWarmupLRSchedulerConfig:\n    _target_: str = \"trainer.lr_schedulers.constant_with_warmup.instantiate_dummy_lr_scheduler\"\n    lr: float = II(\"optimizer.lr\")\n    lr_warmup_steps: int = 500\n    total_num_steps: int = II(\"accelerator.max_steps\")", "\n\ndef instantiate_dummy_lr_scheduler(cfg: ConstantWithWarmupLRSchedulerConfig, optimizer):\n    return get_constant_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.lr_warmup_steps,\n    )\n"]}
{"filename": "trainer/models/clip_model.py", "chunked_list": ["from dataclasses import dataclass\nfrom transformers import CLIPModel as HFCLIPModel\n\nfrom torch import nn\n\nfrom trainer.models.base_model import BaseModelConfig\n\n\n@dataclass\nclass ClipModelConfig(BaseModelConfig):\n    _target_: str = \"trainer.models.clip_model.CLIPModel\"\n    pretrained_model_name_or_path: str = \"openai/clip-vit-base-patch32\"", "@dataclass\nclass ClipModelConfig(BaseModelConfig):\n    _target_: str = \"trainer.models.clip_model.CLIPModel\"\n    pretrained_model_name_or_path: str = \"openai/clip-vit-base-patch32\"\n\n\nclass CLIPModel(nn.Module):\n    def __init__(self, cfg: ClipModelConfig):\n        super().__init__()\n        self.model = HFCLIPModel.from_pretrained(cfg.pretrained_model_name_or_path)\n\n    def get_text_features(self, *args, **kwargs):\n        return self.model.get_text_features(*args, **kwargs)\n\n    def get_image_features(self, *args, **kwargs):\n        return self.model.get_image_features(*args, **kwargs)\n\n    def forward(self, text_inputs=None, image_inputs=None):\n        outputs = ()\n        if text_inputs is not None:\n            outputs += self.model.get_text_features(text_inputs),\n        if image_inputs is not None:\n            outputs += self.model.get_image_features(image_inputs),\n        return outputs\n\n\n    @property\n    def logit_scale(self):\n        return self.model.logit_scale\n\n    def save(self, path):\n        self.model.save_pretrained(path)", "\n"]}
{"filename": "trainer/models/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.models.clip_model import ClipModelConfig\n\ncs = ConfigStore.instance()\ncs.store(group=\"model\", name=\"clip\", node=ClipModelConfig)\n\n"]}
{"filename": "trainer/models/base_model.py", "chunked_list": ["from dataclasses import dataclass\n\n\n\n@dataclass\nclass BaseModelConfig:\n    pass\n"]}
{"filename": "trainer/optimizers/dummy_optimizer.py", "chunked_list": ["from dataclasses import dataclass\nfrom accelerate.utils import DummyOptim\n\n\n@dataclass\nclass DummyOptimizerConfig:\n    _target_: str = \"trainer.optimizers.dummy_optimizer.BaseDummyOptim\"\n    lr: float = 3e-6\n    weight_decay: float = 0.3\n", "\n\nclass BaseDummyOptim(DummyOptim):\n    def __init__(self, model, lr=0.001, weight_decay=0, **kwargs):\n        self.params = [p for p in model.parameters() if p.requires_grad]\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.kwargs = kwargs\n", ""]}
{"filename": "trainer/optimizers/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.optimizers.adamw import AdamWOptimizerConfig\nfrom trainer.optimizers.dummy_optimizer import DummyOptimizerConfig\n\ncs = ConfigStore.instance()\ncs.store(group=\"optimizer\", name=\"dummy\", node=DummyOptimizerConfig)\ncs.store(group=\"optimizer\", name=\"adamw\", node=AdamWOptimizerConfig)\n", ""]}
{"filename": "trainer/optimizers/adamw.py", "chunked_list": ["from dataclasses import dataclass\n\n\n@dataclass\nclass AdamWOptimizerConfig:\n    _target_: str = \"torch.optim.adamw.AdamW\"\n    lr: float = 1e-6\n\n", ""]}
{"filename": "trainer/tasks/base_task.py", "chunked_list": ["from dataclasses import dataclass\n\nimport torch\nfrom PIL import Image\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import LoggerType\n\nlogger = get_logger(__name__)\n\n\ndef flatten(list_of_lists):\n    return [item for sublist in list_of_lists for item in sublist]", "\n\ndef flatten(list_of_lists):\n    return [item for sublist in list_of_lists for item in sublist]\n\n\n@dataclass\nclass BaseTaskConfig:\n    limit_examples_to_wandb: int = 50\n    pass", "\n\nclass BaseTask:\n\n    def __init__(self, cfg: BaseTaskConfig, accelerator):\n        self.accelerator = accelerator\n        self.cfg = cfg\n\n    def train_step(self, model, criterion, batch):\n        pass\n\n    def valid_step(self, model, criterion, batch):\n        pass\n\n    def evaluate(self, model, criterion, dataloader):\n        pass\n\n    def log_to_wandb(self, eval_dict, table_name=\"test_predictions\"):\n        if not self.accelerator.is_main_process or not LoggerType.WANDB == self.accelerator.cfg.log_with:\n            logger.info(\"Not logging to wandb\")\n            return\n        import wandb\n        logger.info(\"Uploading to wandb\")\n        for key, value in eval_dict.items():\n            eval_dict[key] = [wandb.Image(maybe_img) if isinstance(maybe_img, Image.Image) else maybe_img for maybe_img\n                              in value]\n            if self.cfg.limit_examples_to_wandb > 0:\n                eval_dict[key] = eval_dict[key][:self.cfg.limit_examples_to_wandb]\n        columns, predictions = list(zip(*sorted(eval_dict.items())))\n        predictions += ([self.accelerator.global_step] * len(predictions[0]),)\n        columns += (\"global_step\",)\n        data = list(zip(*predictions))\n        table = wandb.Table(columns=list(columns), data=data)\n        wandb.log({table_name: table}, commit=False, step=self.accelerator.global_step)\n\n    @staticmethod\n    def gather_iterable(it, num_processes):\n        output_objects = [None for _ in range(num_processes)]\n        torch.distributed.all_gather_object(output_objects, it)\n        return flatten(output_objects)\n\n    @torch.no_grad()\n    def valid_step(self, model, criterion, batch):\n        loss = criterion(model, batch)\n        return loss\n\n    def gather_dict(self, eval_dict):\n        logger.info(\"Gathering dict from all processes...\")\n        for k, v in eval_dict.items():\n            eval_dict[k] = self.gather_iterable(v, self.accelerator.num_processes)\n        return eval_dict", ""]}
{"filename": "trainer/tasks/__init__.py", "chunked_list": ["\nfrom hydra.core.config_store import ConfigStore\n\nfrom trainer.tasks.clip_task import CLIPTaskConfig\n\ncs = ConfigStore.instance()\ncs.store(group=\"task\", name=\"clip\", node=CLIPTaskConfig)\n\n", ""]}
{"filename": "trainer/tasks/clip_task.py", "chunked_list": ["import collections\nfrom dataclasses import dataclass\n\nimport torch\nfrom PIL import Image\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import LoggerType\nfrom omegaconf import II\nfrom transformers import AutoTokenizer\n", "from transformers import AutoTokenizer\n\nfrom trainer.accelerators.base_accelerator import BaseAccelerator\nfrom trainer.tasks.base_task import BaseTaskConfig, BaseTask\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass CLIPTaskConfig(BaseTaskConfig):\n    _target_: str = \"trainer.tasks.clip_task.CLIPTask\"\n    pretrained_model_name_or_path: str = II(\"model.pretrained_model_name_or_path\")\n    label_0_column_name: str = II(\"dataset.label_0_column_name\")\n    label_1_column_name: str = II(\"dataset.label_1_column_name\")\n\n    input_ids_column_name: str = II(\"dataset.input_ids_column_name\")\n    pixels_0_column_name: str = II(\"dataset.pixels_0_column_name\")\n    pixels_1_column_name: str = II(\"dataset.pixels_1_column_name\")", "@dataclass\nclass CLIPTaskConfig(BaseTaskConfig):\n    _target_: str = \"trainer.tasks.clip_task.CLIPTask\"\n    pretrained_model_name_or_path: str = II(\"model.pretrained_model_name_or_path\")\n    label_0_column_name: str = II(\"dataset.label_0_column_name\")\n    label_1_column_name: str = II(\"dataset.label_1_column_name\")\n\n    input_ids_column_name: str = II(\"dataset.input_ids_column_name\")\n    pixels_0_column_name: str = II(\"dataset.pixels_0_column_name\")\n    pixels_1_column_name: str = II(\"dataset.pixels_1_column_name\")", "\n\ndef numpy_to_pil(images):\n    images = (images * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nclass CLIPTask(BaseTask):\n    def __init__(self, cfg: CLIPTaskConfig, accelerator: BaseAccelerator):\n        super().__init__(cfg, accelerator)\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_model_name_or_path)\n        self.cfg = cfg\n\n    def train_step(self, model, criterion, batch):\n        loss = criterion(model, batch)\n        return loss\n\n    @staticmethod\n    def features2probs(model, text_features, image_0_features, image_1_features):\n        image_0_scores = model.logit_scale.exp() * torch.diag(\n            torch.einsum('bd,cd->bc', text_features, image_0_features))\n        image_1_scores = model.logit_scale.exp() * torch.diag(\n            torch.einsum('bd,cd->bc', text_features, image_1_features))\n        scores = torch.stack([image_0_scores, image_1_scores], dim=-1)\n        probs = torch.softmax(scores, dim=-1)\n        image_0_probs, image_1_probs = probs[:, 0], probs[:, 1]\n        return image_0_probs, image_1_probs\n\n    @torch.no_grad()\n    def valid_step(self, model, criterion, batch):\n        image_0_features, image_1_features, text_features = criterion.get_features(\n            model,\n            batch[self.cfg.input_ids_column_name],\n            batch[self.cfg.pixels_0_column_name],\n            batch[self.cfg.pixels_1_column_name]\n        )\n        return self.features2probs(model, text_features, image_0_features, image_1_features)\n\n    @staticmethod\n    def pixel_values_to_pil_images(pixel_values):\n        images = (pixel_values / 2 + 0.5).clamp(0, 1)\n        images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n        images = numpy_to_pil(images)\n        return images\n\n    def run_inference(self, model, criterion, dataloader):\n        eval_dict = collections.defaultdict(list)\n        logger.info(\"Running clip score...\")\n        for batch in dataloader:\n            image_0_probs, image_1_probs = self.valid_step(model, criterion, batch)\n            agree_on_0 = (image_0_probs > image_1_probs) * batch[self.cfg.label_0_column_name]\n            agree_on_1 = (image_0_probs < image_1_probs) * batch[self.cfg.label_1_column_name]\n            is_correct = agree_on_0 + agree_on_1\n            eval_dict[\"is_correct\"] += is_correct.tolist()\n            eval_dict[\"captions\"] += self.tokenizer.batch_decode(\n                batch[self.cfg.input_ids_column_name],\n                skip_special_tokens=True\n            )\n            eval_dict[\"image_0\"] += self.pixel_values_to_pil_images(batch[self.cfg.pixels_0_column_name])\n            eval_dict[\"image_1\"] += self.pixel_values_to_pil_images(batch[self.cfg.pixels_1_column_name])\n            eval_dict[\"prob_0\"] += image_0_probs.tolist()\n            eval_dict[\"prob_1\"] += image_1_probs.tolist()\n\n            eval_dict[\"label_0\"] += batch[self.cfg.label_0_column_name].tolist()\n            eval_dict[\"label_1\"] += batch[self.cfg.label_1_column_name].tolist()\n\n        return eval_dict\n\n    @torch.no_grad()\n    def evaluate(self, model, criterion, dataloader):\n        eval_dict = self.run_inference(model, criterion, dataloader)\n        eval_dict = self.gather_dict(eval_dict)\n        metrics = {\n            \"accuracy\": sum(eval_dict[\"is_correct\"]) / len(eval_dict[\"is_correct\"]),\n            \"num_samples\": len(eval_dict[\"is_correct\"])\n        }\n        if LoggerType.WANDB == self.accelerator.cfg.log_with:\n            self.log_to_wandb(eval_dict)\n        return metrics", "class CLIPTask(BaseTask):\n    def __init__(self, cfg: CLIPTaskConfig, accelerator: BaseAccelerator):\n        super().__init__(cfg, accelerator)\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_model_name_or_path)\n        self.cfg = cfg\n\n    def train_step(self, model, criterion, batch):\n        loss = criterion(model, batch)\n        return loss\n\n    @staticmethod\n    def features2probs(model, text_features, image_0_features, image_1_features):\n        image_0_scores = model.logit_scale.exp() * torch.diag(\n            torch.einsum('bd,cd->bc', text_features, image_0_features))\n        image_1_scores = model.logit_scale.exp() * torch.diag(\n            torch.einsum('bd,cd->bc', text_features, image_1_features))\n        scores = torch.stack([image_0_scores, image_1_scores], dim=-1)\n        probs = torch.softmax(scores, dim=-1)\n        image_0_probs, image_1_probs = probs[:, 0], probs[:, 1]\n        return image_0_probs, image_1_probs\n\n    @torch.no_grad()\n    def valid_step(self, model, criterion, batch):\n        image_0_features, image_1_features, text_features = criterion.get_features(\n            model,\n            batch[self.cfg.input_ids_column_name],\n            batch[self.cfg.pixels_0_column_name],\n            batch[self.cfg.pixels_1_column_name]\n        )\n        return self.features2probs(model, text_features, image_0_features, image_1_features)\n\n    @staticmethod\n    def pixel_values_to_pil_images(pixel_values):\n        images = (pixel_values / 2 + 0.5).clamp(0, 1)\n        images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n        images = numpy_to_pil(images)\n        return images\n\n    def run_inference(self, model, criterion, dataloader):\n        eval_dict = collections.defaultdict(list)\n        logger.info(\"Running clip score...\")\n        for batch in dataloader:\n            image_0_probs, image_1_probs = self.valid_step(model, criterion, batch)\n            agree_on_0 = (image_0_probs > image_1_probs) * batch[self.cfg.label_0_column_name]\n            agree_on_1 = (image_0_probs < image_1_probs) * batch[self.cfg.label_1_column_name]\n            is_correct = agree_on_0 + agree_on_1\n            eval_dict[\"is_correct\"] += is_correct.tolist()\n            eval_dict[\"captions\"] += self.tokenizer.batch_decode(\n                batch[self.cfg.input_ids_column_name],\n                skip_special_tokens=True\n            )\n            eval_dict[\"image_0\"] += self.pixel_values_to_pil_images(batch[self.cfg.pixels_0_column_name])\n            eval_dict[\"image_1\"] += self.pixel_values_to_pil_images(batch[self.cfg.pixels_1_column_name])\n            eval_dict[\"prob_0\"] += image_0_probs.tolist()\n            eval_dict[\"prob_1\"] += image_1_probs.tolist()\n\n            eval_dict[\"label_0\"] += batch[self.cfg.label_0_column_name].tolist()\n            eval_dict[\"label_1\"] += batch[self.cfg.label_1_column_name].tolist()\n\n        return eval_dict\n\n    @torch.no_grad()\n    def evaluate(self, model, criterion, dataloader):\n        eval_dict = self.run_inference(model, criterion, dataloader)\n        eval_dict = self.gather_dict(eval_dict)\n        metrics = {\n            \"accuracy\": sum(eval_dict[\"is_correct\"]) / len(eval_dict[\"is_correct\"]),\n            \"num_samples\": len(eval_dict[\"is_correct\"])\n        }\n        if LoggerType.WANDB == self.accelerator.cfg.log_with:\n            self.log_to_wandb(eval_dict)\n        return metrics", ""]}
{"filename": "trainer/criterions/clip_criterion.py", "chunked_list": ["from dataclasses import dataclass\nimport torch\nfrom omegaconf import II\nfrom torch.nn.modules.loss import _Loss\n\n\n@dataclass\nclass CLIPCriterionConfig:\n    _target_: str = \"trainer.criterions.clip_criterion.CLIPCriterion\"\n    is_distributed: bool = True\n    label_0_column_name: str = II(\"dataset.label_0_column_name\")\n    label_1_column_name: str = II(\"dataset.label_1_column_name\")\n\n    input_ids_column_name: str = II(\"dataset.input_ids_column_name\")\n    pixels_0_column_name: str = II(\"dataset.pixels_0_column_name\")\n    pixels_1_column_name: str = II(\"dataset.pixels_1_column_name\")\n    num_examples_per_prompt_column_name: str = II(\"dataset.num_examples_per_prompt_column_name\")\n    in_batch_negatives: bool = False\n    pass", "\n\nclass CLIPCriterion(_Loss):\n    def __init__(self, cfg: CLIPCriterionConfig):\n        super().__init__()\n        self.cfg = cfg\n\n    @staticmethod\n    def get_features(model, input_ids, pixels_0_values, pixels_1_values):\n        all_pixel_values = torch.cat([pixels_0_values, pixels_1_values], dim=0)\n        text_features, all_image_features = model(text_inputs=input_ids, image_inputs=all_pixel_values)\n        all_image_features = all_image_features / all_image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        image_0_features, image_1_features = all_image_features.chunk(2, dim=0)\n        return image_0_features, image_1_features, text_features\n\n    @staticmethod\n    def gather_features(features):\n        all_features = torch.cat(torch.distributed.nn.all_gather(features), dim=0)\n        return all_features\n\n    def calc_loss(\n            self,\n            text_features,\n            image_0_features,\n            image_1_features,\n            logit_scale,\n            label_0,\n            label_1,\n            num_examples_per_prompt,\n            *args,\n            **kwargs\n    ):\n        device = image_0_features.device\n\n        # gather features\n        if self.cfg.is_distributed:\n            image_0_features = self.gather_features(image_0_features)\n            image_1_features = self.gather_features(image_1_features)\n            text_features = self.gather_features(text_features)\n            label_0 = self.gather_features(label_0)\n            label_1 = self.gather_features(label_1)\n            num_examples_per_prompt = self.gather_features(num_examples_per_prompt)\n\n        # calc logits # TODO use local loss as open-clip does\n        all_image_features = torch.cat([image_0_features, image_1_features], dim=0)  # (2 * batch_size, dim)\n        logits_per_image = logit_scale * all_image_features @ text_features.T\n        image_0_logits, image_1_logits = logits_per_image.chunk(2, dim=0)\n        text_logits = logit_scale * text_features @ all_image_features.T\n\n        if self.cfg.in_batch_negatives:\n            # get labels\n            num_images = all_image_features.shape[0]\n            image_labels = torch.arange(num_images, device=device, dtype=torch.long)\n            image_0_labels, image_1_labels = image_labels.chunk(2, dim=0)\n            num_texts = text_features.shape[0]\n            text_labels = torch.arange(num_texts, device=device, dtype=torch.long)\n\n            # image loss - we want to increase the logits of the preferred image to the text\n            image_0_loss = torch.nn.functional.cross_entropy(image_0_logits, text_labels, reduction=\"none\")\n            image_1_loss = torch.nn.functional.cross_entropy(image_1_logits, text_labels, reduction=\"none\")\n            # if we have a tie, we will increase both images equally, and average so the image loss of each example is\n            # proportional\n            image_loss = label_0 * image_0_loss + label_1 * image_1_loss\n\n            # text loss - we want to increase the logits of the text to the preferred image\n            text_0_loss = torch.nn.functional.cross_entropy(text_logits, image_0_labels, reduction=\"none\")\n            text_1_loss = torch.nn.functional.cross_entropy(text_logits, image_1_labels, reduction=\"none\")\n\n        else:\n            text_0_logits, text_1_logits = text_logits.chunk(2, dim=-1)\n            index = torch.arange(text_0_logits.shape[0], device=device, dtype=torch.long)\n            text_0_logits = text_0_logits[index, index]\n            text_1_logits = text_1_logits[index, index]\n            text_logits = torch.stack([text_0_logits, text_1_logits], dim=-1)\n            text_0_labels = torch.zeros(text_logits.shape[0], device=device, dtype=torch.long)\n            text_1_labels = text_0_labels + 1\n            text_0_loss = torch.nn.functional.cross_entropy(text_logits, text_0_labels, reduction=\"none\")\n            text_1_loss = torch.nn.functional.cross_entropy(text_logits, text_1_labels, reduction=\"none\")\n\n        # if we have a tie we want the logits of for each image to be equal\n        text_loss = label_0 * text_0_loss + label_1 * text_1_loss\n        # we want the ideal loss to be 0, currently, if there is a tie, it is 0.5 * log(0.5) + 0.5 * log(0.5)\n        # so we add log(0.5) to the loss\n        is_tie = (label_0 == label_1).float()\n        is_tie *= torch.log(torch.tensor(0.5, device=device))\n        text_loss += is_tie\n\n        # we average the image and text loss\n        if self.cfg.in_batch_negatives:\n            loss = (image_loss + text_loss) / 2\n        else:\n            loss = text_loss\n\n        # some prompts have lots of interactions, we want weight them accordingly\n        absolute_example_weight = 1 / num_examples_per_prompt\n        denominator = absolute_example_weight.sum()\n        weight_per_example = absolute_example_weight / denominator\n        loss *= weight_per_example\n\n        loss = loss.sum()\n        return loss\n\n    def forward(self, model, batch):\n        image_0_features, image_1_features, text_features = self.get_features(\n            model,\n            batch[self.cfg.input_ids_column_name],\n            batch[self.cfg.pixels_0_column_name],\n            batch[self.cfg.pixels_1_column_name]\n        )\n        loss = self.calc_loss(\n            text_features,\n            image_0_features,\n            image_1_features,\n            model.logit_scale.exp(),\n            batch[self.cfg.label_0_column_name],\n            batch[self.cfg.label_1_column_name],\n            batch[self.cfg.num_examples_per_prompt_column_name],\n        )\n        return loss", ""]}
{"filename": "trainer/criterions/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.criterions.clip_criterion import CLIPCriterionConfig\n\n\ncs = ConfigStore.instance()\ncs.store(group=\"criterion\", name=\"clip\", node=CLIPCriterionConfig)\n"]}
{"filename": "trainer/accelerators/debug_accelerator.py", "chunked_list": ["from dataclasses import dataclass\nfrom accelerate import Accelerator\nfrom trainer.accelerators.base_accelerator import BaseAcceleratorConfig, BaseAccelerator\n\n\n@dataclass\nclass DebugAcceleratorConfig(BaseAcceleratorConfig):\n    _target_: str = \"trainer.accelerators.debug_accelerator.DebugAccelerator\"\n\n\nclass DebugAccelerator(BaseAccelerator):\n    def __init__(self, cfg: DebugAcceleratorConfig):\n        super().__init__(cfg)\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n            mixed_precision=cfg.mixed_precision,\n            log_with=cfg.log_with,\n            project_dir=cfg.output_dir,\n            dynamo_backend=cfg.dynamo_backend,\n        )\n        self.post_init()", "\n\nclass DebugAccelerator(BaseAccelerator):\n    def __init__(self, cfg: DebugAcceleratorConfig):\n        super().__init__(cfg)\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n            mixed_precision=cfg.mixed_precision,\n            log_with=cfg.log_with,\n            project_dir=cfg.output_dir,\n            dynamo_backend=cfg.dynamo_backend,\n        )\n        self.post_init()", ""]}
{"filename": "trainer/accelerators/deepspeed_accelerator.py", "chunked_list": ["import os\nfrom dataclasses import dataclass, field\nfrom typing import Any\n\nimport torch\nfrom accelerate.utils import PrecisionType\nfrom accelerate import Accelerator, DeepSpeedPlugin\nfrom omegaconf import OmegaConf, MISSING, II\n\nfrom trainer.accelerators.base_accelerator import BaseAcceleratorConfig, BaseAccelerator", "\nfrom trainer.accelerators.base_accelerator import BaseAcceleratorConfig, BaseAccelerator\n\n\n@dataclass\nclass MixedPrecisionConfig:\n    enabled: bool = MISSING\n\n\n@dataclass\nclass DeepSpeedConfig:\n    fp16: MixedPrecisionConfig = MixedPrecisionConfig(enabled=False)\n    bf16: MixedPrecisionConfig = MixedPrecisionConfig(enabled=False)\n    optimizer: dict = field(default_factory=lambda: {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": True,\n            \"adam_w_mode\": True\n        }\n    })\n    scheduler: dict = field(default_factory=lambda: {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    })\n    zero_optimization: dict = field(default_factory=lambda: {\n        \"stage\": 2,\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 500000000,\n        \"contiguous_gradients\": True\n    })\n    gradient_accumulation_steps: int = 16\n    gradient_clipping: float = 1.0\n    steps_per_print: int = 1\n    train_batch_size: str = \"auto\"\n    train_micro_batch_size_per_gpu: str = \"auto\"\n    #     train_micro_batch_size_per_gpu: int = II(\"dataset.batch_size\")\n    wall_clock_breakdown: bool = False", "\n@dataclass\nclass DeepSpeedConfig:\n    fp16: MixedPrecisionConfig = MixedPrecisionConfig(enabled=False)\n    bf16: MixedPrecisionConfig = MixedPrecisionConfig(enabled=False)\n    optimizer: dict = field(default_factory=lambda: {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": True,\n            \"adam_w_mode\": True\n        }\n    })\n    scheduler: dict = field(default_factory=lambda: {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    })\n    zero_optimization: dict = field(default_factory=lambda: {\n        \"stage\": 2,\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 500000000,\n        \"contiguous_gradients\": True\n    })\n    gradient_accumulation_steps: int = 16\n    gradient_clipping: float = 1.0\n    steps_per_print: int = 1\n    train_batch_size: str = \"auto\"\n    train_micro_batch_size_per_gpu: str = \"auto\"\n    #     train_micro_batch_size_per_gpu: int = II(\"dataset.batch_size\")\n    wall_clock_breakdown: bool = False", "\n\n@dataclass\nclass DeepSpeedAcceleratorConfig(BaseAcceleratorConfig):\n    _target_: str = \"trainer.accelerators.deepspeed_accelerator.DeepSpeedAccelerator\"\n    deepspeed: DeepSpeedConfig = DeepSpeedConfig()\n    deepspeed_final: Any = None\n\n\nclass DeepSpeedAccelerator(BaseAccelerator):\n    def __init__(self, cfg: DeepSpeedAcceleratorConfig):\n        super().__init__(cfg)\n        self.set_mixed_precision()\n        deepspeed_plugin = DeepSpeedPlugin(\n            hf_ds_config=OmegaConf.to_container(self.cfg.deepspeed, resolve=True),\n            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n        )\n        self.cfg.deepspeed_final = OmegaConf.create(deepspeed_plugin.deepspeed_config)\n        self.accelerator = Accelerator(\n            deepspeed_plugin=deepspeed_plugin,\n            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n            mixed_precision=self.cfg.mixed_precision,\n            log_with=self.cfg.log_with,\n            project_dir=self.cfg.output_dir,\n            dynamo_backend=self.cfg.dynamo_backend,\n        )\n        self.post_init()\n\n    def set_mixed_precision(self):\n        if self.cfg.mixed_precision == PrecisionType.BF16:\n            self.cfg.deepspeed.bf16.enabled = True\n            self.cfg.deepspeed.fp16.enabled = False\n        elif self.cfg.mixed_precision == PrecisionType.FP16:\n            self.cfg.deepspeed.fp16.enabled = True\n            self.cfg.deepspeed.bf16.enabled = False\n        else:\n            self.cfg.deepspeed.fp16.enabled = False\n            self.cfg.deepspeed.bf16.enabled = False\n\n    def prepare(self, *args, device_placement=None):\n        prepared = self.accelerator.prepare(*args, device_placement=device_placement)\n        for obj in prepared:\n            if isinstance(obj, torch.nn.Module):\n                if self.cfg.mixed_precision == PrecisionType.BF16:\n                    obj.forward = torch.autocast(device_type=self.device.type, dtype=torch.bfloat16)(obj.forward)\n                elif self.cfg.mixed_precision == PrecisionType.FP16:\n                    obj.forward = torch.autocast(device_type=self.device.type, dtype=torch.float16)(obj.forward)\n        return prepared", "\nclass DeepSpeedAccelerator(BaseAccelerator):\n    def __init__(self, cfg: DeepSpeedAcceleratorConfig):\n        super().__init__(cfg)\n        self.set_mixed_precision()\n        deepspeed_plugin = DeepSpeedPlugin(\n            hf_ds_config=OmegaConf.to_container(self.cfg.deepspeed, resolve=True),\n            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n        )\n        self.cfg.deepspeed_final = OmegaConf.create(deepspeed_plugin.deepspeed_config)\n        self.accelerator = Accelerator(\n            deepspeed_plugin=deepspeed_plugin,\n            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n            mixed_precision=self.cfg.mixed_precision,\n            log_with=self.cfg.log_with,\n            project_dir=self.cfg.output_dir,\n            dynamo_backend=self.cfg.dynamo_backend,\n        )\n        self.post_init()\n\n    def set_mixed_precision(self):\n        if self.cfg.mixed_precision == PrecisionType.BF16:\n            self.cfg.deepspeed.bf16.enabled = True\n            self.cfg.deepspeed.fp16.enabled = False\n        elif self.cfg.mixed_precision == PrecisionType.FP16:\n            self.cfg.deepspeed.fp16.enabled = True\n            self.cfg.deepspeed.bf16.enabled = False\n        else:\n            self.cfg.deepspeed.fp16.enabled = False\n            self.cfg.deepspeed.bf16.enabled = False\n\n    def prepare(self, *args, device_placement=None):\n        prepared = self.accelerator.prepare(*args, device_placement=device_placement)\n        for obj in prepared:\n            if isinstance(obj, torch.nn.Module):\n                if self.cfg.mixed_precision == PrecisionType.BF16:\n                    obj.forward = torch.autocast(device_type=self.device.type, dtype=torch.bfloat16)(obj.forward)\n                elif self.cfg.mixed_precision == PrecisionType.FP16:\n                    obj.forward = torch.autocast(device_type=self.device.type, dtype=torch.float16)(obj.forward)\n        return prepared", ""]}
{"filename": "trainer/accelerators/base_accelerator.py", "chunked_list": ["import abc\nimport hashlib\nimport json\nimport math\nimport os\nimport shutil\nfrom dataclasses import field, dataclass\nfrom glob import glob\nfrom typing import List, Optional\n", "from typing import List, Optional\n\nimport datasets\nimport torch\nimport transformers\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed as accelerate_set_seed, PrecisionType\nfrom accelerate.utils.dataclasses import BaseEnum, LoggerType, DynamoBackend\nfrom omegaconf import DictConfig, OmegaConf, II\nfrom tqdm import tqdm", "from omegaconf import DictConfig, OmegaConf, II\nfrom tqdm import tqdm\n\nfrom trainer.accelerators.utils import get_nvidia_smi_gpu_memory_stats_str, print_config, _flatten_dict\n\nlogger = get_logger(__name__)\n\nTRAINING_STAGE_PATH = \"training_stage.json\"\n\n\ndef debug(port):\n    logger.info(\"Connecting to debugger...\")\n    import pydevd_pycharm\n    pydevd_pycharm.settrace('localhost', port=port, stdoutToServer=True, stderrToServer=True)", "\n\ndef debug(port):\n    logger.info(\"Connecting to debugger...\")\n    import pydevd_pycharm\n    pydevd_pycharm.settrace('localhost', port=port, stdoutToServer=True, stderrToServer=True)\n\n\n@dataclass\nclass DebugConfig:\n    activate: bool = False\n    port: int = 5900", "@dataclass\nclass DebugConfig:\n    activate: bool = False\n    port: int = 5900\n\n\nclass TrainingMode(BaseEnum):\n    SKIPPING = \"skipping\"\n    TRAINING = \"training\"\n", "\n\nclass MetricMode(BaseEnum):\n    MAX = \"max\"\n    MIN = \"min\"\n\n\n@dataclass\nclass BaseAcceleratorConfig:\n    _target_: str = \"trainer.accelerators.base_accelerator.Accelerator\"\n    output_dir: str = II(\"output_dir\")\n    mixed_precision: PrecisionType = PrecisionType.NO\n    gradient_accumulation_steps: int = 1\n    log_with: Optional[LoggerType] = LoggerType.WANDB\n    debug: DebugConfig = DebugConfig()\n    seed: int = 42\n    resume_from_checkpoint: bool = True\n    max_steps: int = 4000\n    num_epochs: int = 10\n    validate_steps: int = 100\n    eval_on_start: bool = True\n    project_name: str = \"reward\"\n    max_grad_norm: float = 1.0\n    save_steps: int = 100\n    metric_name: str = \"accuracy\"\n    metric_mode: MetricMode = MetricMode.MAX\n    limit_num_checkpoints: int = 1\n    save_only_if_best: bool = True\n    dynamo_backend: DynamoBackend = DynamoBackend.NO\n    keep_best_ckpts: bool = True", "class BaseAcceleratorConfig:\n    _target_: str = \"trainer.accelerators.base_accelerator.Accelerator\"\n    output_dir: str = II(\"output_dir\")\n    mixed_precision: PrecisionType = PrecisionType.NO\n    gradient_accumulation_steps: int = 1\n    log_with: Optional[LoggerType] = LoggerType.WANDB\n    debug: DebugConfig = DebugConfig()\n    seed: int = 42\n    resume_from_checkpoint: bool = True\n    max_steps: int = 4000\n    num_epochs: int = 10\n    validate_steps: int = 100\n    eval_on_start: bool = True\n    project_name: str = \"reward\"\n    max_grad_norm: float = 1.0\n    save_steps: int = 100\n    metric_name: str = \"accuracy\"\n    metric_mode: MetricMode = MetricMode.MAX\n    limit_num_checkpoints: int = 1\n    save_only_if_best: bool = True\n    dynamo_backend: DynamoBackend = DynamoBackend.NO\n    keep_best_ckpts: bool = True", "\n\nclass BaseAccelerator(abc.ABC):\n\n    def __init__(self, cfg: BaseAcceleratorConfig):\n        self.cfg = cfg\n        self.accelerator = None\n        self.epoch = 0\n        self.step = 0\n        self.global_step = 0\n        self.step_loss = 0.0\n        self.lr = None\n        self.metrics = {}\n        self.progress_bar = None\n        self.mode = TrainingMode.TRAINING\n        self.num_update_steps_per_epoch = None\n        self.num_steps_per_epoch = None\n\n    def post_init(self):\n        self.set_seed()\n        self.debug()\n        logger.info(f\"Initialized accelerator: rank={self.accelerator.process_index}\", main_process_only=False)\n        self.set_logging_level()\n\n    def set_logging_level(self):\n        if self.accelerator.is_local_main_process:\n            datasets.utils.logging.set_verbosity_warning()\n            transformers.utils.logging.set_verbosity_warning()\n        else:\n            datasets.utils.logging.set_verbosity_error()\n            transformers.utils.logging.set_verbosity_error()\n\n    def debug(self):\n        if self.accelerator.is_main_process and self.cfg.debug.activate:\n            debug(self.cfg.debug.port)\n\n    def set_seed(self):\n        logger.info(f\"Setting seed {self.cfg.seed}\")\n        accelerate_set_seed(self.cfg.seed, device_specific=True)\n\n    def prepare(self, *args, device_placement=None):\n        return self.accelerator.prepare(*args, device_placement=device_placement)\n\n    def get_latest_checkpoint(self):\n        all_ckpts = list(glob(os.path.join(self.cfg.output_dir, \"checkpoint-*\")))\n        if len(all_ckpts) == 0:\n            return\n        all_ckpts.sort(key=os.path.getctime)\n        if \"final\" in all_ckpts[-1]:\n            all_ckpts.pop()\n        return all_ckpts[-1] if len(all_ckpts) > 0 else None\n\n    def load_state_if_needed(self):\n        if not self.cfg.resume_from_checkpoint:\n            return\n        ckpt_path = self.get_latest_checkpoint()\n\n        if ckpt_path is None:\n            logger.info(\"No checkpoint found, training from scratch\")\n            return\n\n        stage = json.load(open(os.path.join(ckpt_path, TRAINING_STAGE_PATH)))\n        self.epoch, self.step, self.global_step, self.metrics = stage[\"epoch\"], stage[\"step\"], stage[\"global_step\"], \\\n            stage[\"metrics\"]\n        logger.info(\n            f\"Resuming from checkpoint: {ckpt_path} | epoch={self.epoch} step={self.step} gstep={self.global_step}\")\n        self.accelerator.load_state(ckpt_path)\n        logger.info(\"Checkpoint loaded\")\n\n    @property\n    def is_main_process(self):\n        return self.accelerator.is_main_process\n\n    @property\n    def num_processes(self):\n        return self.accelerator.num_processes\n\n    def pre_training_log(self, cfg: DictConfig):\n        total_batch_size = cfg.dataset.batch_size * self.num_processes * self.cfg.gradient_accumulation_steps\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Instantaneous batch size per device = {cfg.dataset.batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {self.cfg.gradient_accumulation_steps}\")\n        logger.info(f\"  Total warmup steps = {cfg.lr_scheduler.lr_warmup_steps}\")\n        logger.info(f\"  Total training steps = {self.cfg.max_steps * self.cfg.gradient_accumulation_steps}\")\n        logger.info(f\"  Total epochs = {self.cfg.num_epochs}\")\n        logger.info(f\"  Steps per epoch = {self.num_steps_per_epoch}\")\n        logger.info(f\"  Update steps per epoch = {self.num_update_steps_per_epoch}\")\n        logger.info(f\"  Total optimization steps = {self.cfg.max_steps}\")\n        logger.info(f\"  Mixed precision = {self.cfg.mixed_precision}\")\n        logger.info(f\"  World size = {self.accelerator.num_processes}\")\n\n    def init_training(self, cfg: DictConfig):\n        if self.is_main_process:\n            yaml = OmegaConf.to_yaml(cfg, resolve=True, sort_keys=True)\n            log_cfg = _flatten_dict(OmegaConf.create(yaml))\n            logger.info(\"Initializing trackers\")\n            self.accelerator.init_trackers(self.cfg.project_name, log_cfg)\n            logger.info(\"Training config:\")\n            print_config(cfg)\n        logger.info(get_nvidia_smi_gpu_memory_stats_str())\n        self.pre_training_log(cfg)\n        self.progress_bar = tqdm(range(self.cfg.max_steps * self.cfg.gradient_accumulation_steps), disable=not self.accelerator.is_main_process)\n        self.progress_bar.set_description(\"Steps\")\n\n    def should_skip(self, epoch, step):\n        should = epoch < self.epoch or (epoch == self.epoch and step < self.step)\n        if should:\n            self.mode = TrainingMode.SKIPPING\n            self.progress_bar.set_postfix(**{\"status\": TrainingMode.SKIPPING})\n        else:\n            self.mode = TrainingMode.TRAINING\n        return should\n\n    def update_progbar_step(self):\n        self.progress_bar.update(1)\n\n    def log(self, data):\n        if self.is_main_process:\n            self.accelerator.log(data, step=self.global_step)\n\n    def recalc_train_length_after_prepare(self, num_batches):\n        num_update_steps_per_epoch = math.ceil(num_batches / self.cfg.gradient_accumulation_steps)\n        if self.cfg.max_steps is None:\n            self.cfg.max_steps = self.cfg.num_epochs * num_update_steps_per_epoch\n        self.num_update_steps_per_epoch = num_update_steps_per_epoch\n        self.num_steps_per_epoch = num_batches\n        self.cfg.num_epochs = math.ceil(self.cfg.max_steps / num_update_steps_per_epoch)\n\n    def accumulate(self, model):\n        return self.accelerator.accumulate(model)\n\n    def gather(self, data):\n        return self.accelerator.gather(data)\n\n    @property\n    def sync_gradients(self):\n        return self.accelerator.sync_gradients\n\n    def update_step_loss(self, loss):\n        self.step_loss = loss\n\n    def update_global_step(self, loss):\n        self.global_step += 1\n        self.log({\n            \"lr\": self.lr,\n            \"step\": self.step,\n            \"epoch\": self.epoch,\n            \"global_step\": self.global_step,\n            \"loss\": loss,\n        })\n\n    def get_allocated_cuda_memory(self):\n        return round(torch.cuda.max_memory_allocated(self.accelerator.device) / 1024 / 1024 / 1024, 2)\n\n    def update_step(self, loss, lr):\n        self.step += 1\n        self.lr = lr\n        logs = {\n            \"stl\": loss,\n            \"gstl\": loss,\n            \"mem\": self.get_allocated_cuda_memory(),\n            \"st\": self.step,\n            \"ep\": self.epoch,\n            \"gst\": self.global_step,\n            \"lr\": self.lr,\n        }\n        self.progress_bar.set_postfix(**logs)\n        self.update_progbar_step()\n\n    def wait_for_everyone(self):\n        self.accelerator.wait_for_everyone()\n\n    def update_epoch(self):\n        if self.mode == TrainingMode.SKIPPING:\n            return\n        logger.info(f\"Epoch {self.epoch} finished\")\n        self.epoch += 1\n        self.step = 0\n\n    def update_metrics(self, metrics):\n        self.metrics.update(metrics)\n        logger.info(f\"Metrics: {self.metrics}\")\n        self.log(metrics)\n\n    def end_training(self):\n        self.accelerator.wait_for_everyone()\n        self.accelerator.end_training()\n\n    def unwrap_and_save(self, model):\n        if not self.is_main_process:\n            return\n        model = self.accelerator.unwrap_model(model)\n        save_dir = os.path.join(self.cfg.output_dir, f\"checkpoint-final\")\n        logger.info(f\"Saving final checkpoint to {save_dir}\")\n        model.save(save_dir)\n        self.save_training_stage(save_dir)\n        logger.info(f\"Saved checkpoint to {save_dir}\")\n\n    def should_end(self):\n        return self.global_step >= self.cfg.max_steps\n\n    def backward(self, loss):\n        self.accelerator.backward(loss)\n\n    def clip_grad_norm_(self, params):\n        self.accelerator.clip_grad_norm_(params, self.cfg.max_grad_norm)\n\n    def should_eval(self):\n        if not self.mode == TrainingMode.TRAINING:\n            return False\n        if self.step == 0 and self.global_step == 0 and self.cfg.eval_on_start:\n            return True\n        if self.global_step > 0 and self.sync_gradients and self.global_step % self.cfg.validate_steps == 0:\n            return True\n        return False\n\n    def should_save(self):\n        return self.sync_gradients and self.global_step > 0 and self.cfg.save_steps > 0 and self.global_step % self.cfg.save_steps == 0\n\n    @property\n    def training_stage(self):\n        return {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"global_step\": self.global_step,\n            \"step_loss\": self.step_loss,\n            \"lr\": self.lr,\n            \"metrics\": self.metrics,\n        }\n\n    def save_training_stage(self, save_dir):\n        json.dump(self.training_stage, open(os.path.join(save_dir, TRAINING_STAGE_PATH), \"w\"), indent=4)\n\n    def save_checkpoint(self):\n        if self.cfg.save_only_if_best:\n            all_ckpts = self.get_all_ckpts()\n            for ckpt in all_ckpts:\n                training_stage = json.load(open(os.path.join(ckpt, TRAINING_STAGE_PATH)))\n                metric_val = training_stage[\"metrics\"][self.cfg.metric_name]\n                cur_metric_val = self.training_stage[\"metrics\"][self.cfg.metric_name]\n                if (self.cfg.metric_mode == MetricMode.MIN and metric_val < cur_metric_val) or \\\n                        (self.cfg.metric_mode == MetricMode.MAX and metric_val > cur_metric_val):\n                    logger.info(\n                        f\"Metric {self.cfg.metric_name}={cur_metric_val} is not better than {metric_val} of {ckpt}, skipping checkpoint\")\n                    return\n        self.cleanup_checkpoints()\n        self.accelerator.wait_for_everyone()\n        save_dir = os.path.join(self.cfg.output_dir, f\"checkpoint-gstep{self.global_step}\")\n        logger.info(f\"Saving checkpoint to {save_dir}\")\n        self.accelerator.save_state(save_dir)\n        self.save_training_stage(save_dir)\n        logger.info(f\"Saved checkpoint to {save_dir}\")\n\n    @property\n    def gradient_state(self):\n        return self.accelerator.gradient_state\n\n    def get_all_ckpts(self):\n        return list(glob(os.path.join(self.cfg.output_dir, f\"checkpoint-*\")))\n\n    def load_best_checkpoint(self):\n        all_ckpts = self.get_all_ckpts()\n        if not self.cfg.keep_best_ckpts:\n            all_ckpts.sort(key=os.path.getctime, reverse=True)\n            logger.info(f\"Returning the most recent checkpoint: {all_ckpts[0]}\")\n            return all_ckpts[0]\n        logger.info(f\"Found {len(all_ckpts)} checkpoints in {self.cfg.output_dir}\")\n        logger.info(all_ckpts)\n        if len(all_ckpts) == 0:\n            logger.info(f\"No checkpoint found in {self.cfg.output_dir} to load. Keeping current model.\")\n            return\n        best_ckpt, best_metric_val = None, math.inf if self.cfg.metric_mode == MetricMode.MIN else -math.inf\n        for ckpt in all_ckpts:\n            training_stage = json.load(open(os.path.join(ckpt, TRAINING_STAGE_PATH)))\n            metric_val = training_stage[\"metrics\"][self.cfg.metric_name]\n            if (self.cfg.metric_mode == MetricMode.MIN and metric_val < best_metric_val) or \\\n                    (self.cfg.metric_mode == MetricMode.MAX and metric_val > best_metric_val):\n                best_ckpt, best_metric_val = ckpt, metric_val\n        logger.info(f\"Loading best checkpoint from {best_ckpt} with metric {self.cfg.metric_name}={best_metric_val}\")\n        self.accelerator.load_state(best_ckpt)\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    def cleanup_checkpoints(self):\n        if self.cfg.limit_num_checkpoints <= 0 or not self.accelerator.is_main_process:\n            logger.info(f\"Not cleaning up checkpoints as limit_num_checkpoints={self.cfg.limit_num_checkpoints}\")\n            return\n\n        all_ckpts = self.get_all_ckpts()\n        if len(all_ckpts) <= self.cfg.limit_num_checkpoints:\n            logger.info(f\"Not cleaning up checkpoints as only {len(all_ckpts)} checkpoints found\")\n            return\n\n        logger.info(f\"Found {len(all_ckpts)} checkpoints in {self.cfg.output_dir}\")\n        ckpts_to_delete = self.get_ckpts_to_delete()\n        ckpts_to_delete.sort(key=os.path.getctime)\n\n        ckpts_to_delete = ckpts_to_delete[:-1]\n        for ckpt in ckpts_to_delete:\n            logger.info(f\"Deleting checkpoint {ckpt}\")\n            shutil.rmtree(ckpt)\n\n    def get_ckpts_to_delete(self):\n        all_ckpts = self.get_all_ckpts()\n        if self.cfg.keep_best_ckpts:\n            metric_vals = []\n            for ckpt in all_ckpts:\n                training_stage = json.load(open(os.path.join(ckpt, TRAINING_STAGE_PATH)))\n                metric_val = training_stage[\"metrics\"][self.cfg.metric_name]\n                metric_vals.append(metric_val)\n            metric_ckpt = list(zip(metric_vals, all_ckpts))\n            metric_ckpt.sort(key=lambda x: x[0], reverse=self.cfg.metric_mode == MetricMode.MAX)\n            ckpts_to_delete = [ckpt for _, ckpt in metric_ckpt[self.cfg.limit_num_checkpoints:]]\n        else:\n            all_ckpts.sort(key=os.path.getctime, reverse=True)\n            ckpts_to_delete = all_ckpts[self.cfg.limit_num_checkpoints:]\n        return ckpts_to_delete"]}
{"filename": "trainer/accelerators/__init__.py", "chunked_list": ["from hydra.core.config_store import ConfigStore\n\nfrom trainer.accelerators.debug_accelerator import DebugAcceleratorConfig\nfrom trainer.accelerators.deepspeed_accelerator import DeepSpeedAcceleratorConfig\n\nACCELERATOR_GROUP_NAME = \"accelerator\"\n\ncs = ConfigStore.instance()\ncs.store(group=ACCELERATOR_GROUP_NAME, name=\"deepspeed\", node=DeepSpeedAcceleratorConfig)\ncs.store(group=ACCELERATOR_GROUP_NAME, name=\"debug\", node=DebugAcceleratorConfig)", "cs.store(group=ACCELERATOR_GROUP_NAME, name=\"deepspeed\", node=DeepSpeedAcceleratorConfig)\ncs.store(group=ACCELERATOR_GROUP_NAME, name=\"debug\", node=DebugAcceleratorConfig)\n"]}
{"filename": "trainer/accelerators/utils.py", "chunked_list": ["import subprocess\nfrom typing import MutableMapping, Any, Dict\n\nimport rich.tree\nimport rich.syntax\nfrom accelerate.logging import get_logger\nfrom omegaconf import DictConfig, OmegaConf\n\nlogger = get_logger(__name__)\n", "logger = get_logger(__name__)\n\n\ndef nvidia_smi_gpu_memory_stats():\n    \"\"\"\n    Parse the nvidia-smi output and extract the memory used stats.\n    \"\"\"\n    out_dict = {}\n    try:\n        sp = subprocess.Popen(\n            [\"nvidia-smi\", \"--query-gpu=index,memory.used\", \"--format=csv,noheader\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            close_fds=True,\n        )\n        out_str = sp.communicate()\n        out_list = out_str[0].decode(\"utf-8\").split(\"\\n\")\n        out_dict = {}\n        for item in out_list:\n            if \" MiB\" in item:\n                gpu_idx, mem_used = item.split(',')\n                gpu_key = f\"gpu_{gpu_idx}_mem_used_gb\"\n                out_dict[gpu_key] = int(mem_used.strip().split(\" \")[0]) / 1024\n    except FileNotFoundError:\n        logger.error(\n            \"Failed to find the 'nvidia-smi' executable for printing GPU stats\"\n        )\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"nvidia-smi returned non zero error code: {e.returncode}\")\n\n    return out_dict", "\n\ndef get_nvidia_smi_gpu_memory_stats_str():\n    return f\"nvidia-smi stats: {nvidia_smi_gpu_memory_stats()}\"\n\n\ndef print_config(cfg: DictConfig):\n    style = \"bright\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n    fields = cfg.keys()\n    for field in fields:\n        branch = tree.add(field, style=style, guide_style=style)\n        config_section = cfg.get(field)\n        branch_content = str(config_section)\n        if isinstance(config_section, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_section, resolve=True)\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n    rich.print(tree)", "\n\ndef _flatten_dict(params: MutableMapping, delimiter: str = \"/\", parent_key: str = \"\") -> Dict[str, Any]:\n    result: Dict[str, Any] = {}\n    for k, v in params.items():\n        new_key = parent_key + delimiter + str(k) if parent_key else str(k)\n        if isinstance(v, MutableMapping):\n            result = {**result, **_flatten_dict(v, parent_key=new_key, delimiter=delimiter)}\n        else:\n            result[new_key] = v\n    return result", ""]}
