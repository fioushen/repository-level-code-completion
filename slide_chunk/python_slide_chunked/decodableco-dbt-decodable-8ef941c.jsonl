{"filename": "decodable/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "decodable/config/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "decodable/config/client_config.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom dataclasses import dataclass\n\n", "\n\n@dataclass\nclass DecodableClientConfig:\n    account_name: str\n    access_token: str\n    api_url: str\n\n    def decodable_api_url(self) -> str:\n        return f\"https://{self.account_name}.{self.api_url}\"", ""]}
{"filename": "decodable/config/profile_reader.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport os\nfrom pathlib import Path\nfrom yaml import SafeLoader, load", "from pathlib import Path\nfrom yaml import SafeLoader, load\nfrom typing import Optional\n\nfrom decodable.config.profile import DecodableAccessTokens\n\nDEFAULT_PROFILE_PATH = f\"{str(Path.home())}/.decodable/auth\"\nPROFILE_ENV_VARIABLE_NAME = \"DECODABLE_PROFILE\"\n\n\nclass DecodableProfileReader:\n    @staticmethod\n    def load_profiles(default_profile_path: str = DEFAULT_PROFILE_PATH) -> DecodableAccessTokens:\n        profiles_path = Path(default_profile_path)\n        if profiles_path.is_file() is False:\n            raise Exception(\n                f\"No decodable profile under path: {profiles_path}. Execute 'decodable login' command first\"\n            )\n\n        with open(profiles_path, \"r\") as file:\n            content = file.read()\n            return DecodableProfileReader._load_profile_access_tokens(content)\n\n    @staticmethod\n    def get_profile_name(profile_name: Optional[str]) -> Optional[str]:\n        if profile_name is not None:\n            return profile_name\n        else:\n            return os.getenv(PROFILE_ENV_VARIABLE_NAME)\n\n    @staticmethod\n    def _load_profile_access_tokens(yaml: str) -> DecodableAccessTokens:\n        config_data = load(yaml, Loader=SafeLoader)\n        access_tokens = {}\n        for profile_name in config_data[\"tokens\"]:\n            access_tokens[profile_name] = config_data[\"tokens\"][profile_name][\"access_token\"]\n        return DecodableAccessTokens(profile_tokens=access_tokens)", "\n\nclass DecodableProfileReader:\n    @staticmethod\n    def load_profiles(default_profile_path: str = DEFAULT_PROFILE_PATH) -> DecodableAccessTokens:\n        profiles_path = Path(default_profile_path)\n        if profiles_path.is_file() is False:\n            raise Exception(\n                f\"No decodable profile under path: {profiles_path}. Execute 'decodable login' command first\"\n            )\n\n        with open(profiles_path, \"r\") as file:\n            content = file.read()\n            return DecodableProfileReader._load_profile_access_tokens(content)\n\n    @staticmethod\n    def get_profile_name(profile_name: Optional[str]) -> Optional[str]:\n        if profile_name is not None:\n            return profile_name\n        else:\n            return os.getenv(PROFILE_ENV_VARIABLE_NAME)\n\n    @staticmethod\n    def _load_profile_access_tokens(yaml: str) -> DecodableAccessTokens:\n        config_data = load(yaml, Loader=SafeLoader)\n        access_tokens = {}\n        for profile_name in config_data[\"tokens\"]:\n            access_tokens[profile_name] = config_data[\"tokens\"][profile_name][\"access_token\"]\n        return DecodableAccessTokens(profile_tokens=access_tokens)", ""]}
{"filename": "decodable/config/profile.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Dict\nfrom dataclasses import dataclass\n", "from dataclasses import dataclass\n\n\n@dataclass(init=True)\nclass DecodableAccessTokens:\n    profile_tokens: Dict[str, str]\n"]}
{"filename": "decodable/client/types.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom __future__ import annotations\n\nimport re", "\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import List, Optional, Type\n\n\n@dataclass(frozen=True)\nclass FieldType:\n    synonyms: List[FieldType] = field(init=False, repr=False, default_factory=lambda: [])\n\n    def __eq__(self, __o: object) -> bool:\n        if not issubclass(__o.__class__, FieldType):\n            return False\n\n        if __o in self.synonyms:\n            return True\n\n        return self.__repr__() == __o.__repr__()\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [\n            NotNull,\n            StringType,\n            BinaryType,\n            NumericType,\n            DateTimeType,\n            CompoundType,\n            Boolean,\n            Interval,\n            Multiset,\n            PrimaryKey,\n        ]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "class FieldType:\n    synonyms: List[FieldType] = field(init=False, repr=False, default_factory=lambda: [])\n\n    def __eq__(self, __o: object) -> bool:\n        if not issubclass(__o.__class__, FieldType):\n            return False\n\n        if __o in self.synonyms:\n            return True\n\n        return self.__repr__() == __o.__repr__()\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [\n            NotNull,\n            StringType,\n            BinaryType,\n            NumericType,\n            DateTimeType,\n            CompoundType,\n            Boolean,\n            Interval,\n            Multiset,\n            PrimaryKey,\n        ]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass NotNull(FieldType):\n    inner_type: FieldType\n\n    def __repr__(self) -> str:\n        return f\"{repr(self.inner_type)} NOT NULL\"\n\n    def __eq__(self, __o: object) -> bool:\n        if not isinstance(__o, NotNull):\n            return False\n        return self.inner_type == __o.inner_type\n\n    def __hash__(self) -> int:\n        return super().__hash__()\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"(?P<inner>.*) NOT NULL\", type)\n\n        if not found:\n            return None\n\n        inner_type = FieldType.from_str(found[\"inner\"])\n\n        if not inner_type:\n            return None\n\n        return cls(inner_type)", "\n\n@dataclass(frozen=True, eq=False)\nclass StringType(FieldType):\n    length: int\n    is_synonym: bool = False\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Char, Varchar, String]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Char(StringType):\n    def __repr__(self) -> str:\n        return f\"CHAR({self.length})\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"CHAR\\((?P<length>\\d+)\\)\", type)\n\n        if not found:\n            return None\n\n        return cls(int(found[\"length\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass Varchar(StringType):\n    def __repr__(self) -> str:\n        return f\"VARCHAR({self.length})\"\n\n    def __post_init__(self):\n        if not self.is_synonym and self.length == String.length:\n            self.synonyms.append(String(is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"VARCHAR\\((?P<length>\\d+)\\)\", type)\n\n        if not found:\n            return None\n\n        return cls(int(found[\"length\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass String(StringType):\n    length: int = 2147483647\n\n    def __repr__(self) -> str:\n        return \"STRING\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Varchar(self.length, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"STRING\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass BinaryType(FieldType):\n    length: int\n    is_synonym: bool = False\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Binary, Varbinary, Bytes]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Binary(BinaryType):\n    def __repr__(self) -> str:\n        return f\"BINARY({self.length})\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"BINARY\\((?P<length>\\d+)\\)\", type)\n\n        if not found:\n            return None\n\n        return cls(int(found[\"length\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass Varbinary(BinaryType):\n    def __repr__(self) -> str:\n        return f\"VARBINARY({self.length})\"\n\n    def __post_init__(self):\n        if not self.is_synonym and self.length == Bytes.length:\n            self.synonyms.append(Bytes(is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"VARBINARY\\((?P<length>\\d+)\\)\", type)\n\n        if not found:\n            return None\n\n        return cls(int(found[\"length\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass Bytes(BinaryType):\n    length: int = 2147483647\n\n    def __repr__(self) -> str:\n        return \"BYTES\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Varbinary(self.length, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"BYTES\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass NumericType(FieldType):\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [\n            ExactNumericType,\n            TinyInt,\n            SmallInt,\n            Int,\n            BigInt,\n            Float,\n            Double,\n        ]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass ExactNumericType(NumericType):\n    precision: int\n    scale: int\n    is_synonym: bool = False\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Decimal, Dec, Numeric]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Decimal(ExactNumericType):\n    precision: int = 10\n    scale: int = 0\n\n    def __repr__(self) -> str:\n        return f\"DECIMAL({self.precision}, {self.scale})\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Dec(self.precision, self.scale, is_synonym=True))\n            self.synonyms.append(Numeric(self.precision, self.scale, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"DECIMAL(\\((?P<precision>\\d+)(, (?P<scale>\\d+))?\\))?\", type)\n\n        if not found:\n            return None\n\n        if not found[\"precision\"]:\n            return cls()\n        elif not found[\"scale\"]:\n            return cls(precision=int(found[\"precision\"]))\n        else:\n            return cls(precision=int(found[\"precision\"]), scale=int(found[\"scale\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass Dec(ExactNumericType):\n    precision: int = 10\n    scale: int = 0\n\n    def __repr__(self) -> str:\n        return f\"DEC({self.precision}, {self.scale})\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Decimal(self.precision, self.scale, is_synonym=True))\n            self.synonyms.append(Numeric(self.precision, self.scale, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"DEC(\\((?P<precision>\\d+)(, (?P<scale>\\d+))?\\))?\", type)\n\n        if not found:\n            return None\n\n        if not found[\"precision\"]:\n            return cls()\n        elif not found[\"scale\"]:\n            return cls(precision=int(found[\"precision\"]))\n        else:\n            return cls(precision=int(found[\"precision\"]), scale=int(found[\"scale\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass Numeric(ExactNumericType):\n    precision: int = 10\n    scale: int = 0\n\n    def __repr__(self) -> str:\n        return f\"NUMERIC({self.precision}, {self.scale})\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Dec(self.precision, self.scale, is_synonym=True))\n            self.synonyms.append(Decimal(self.precision, self.scale, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"NUMERIC(\\((?P<precision>\\d+)(, (?P<scale>\\d+))?\\))?\", type)\n\n        if not found:\n            return None\n\n        if not found[\"precision\"]:\n            return cls()\n        elif not found[\"scale\"]:\n            return cls(precision=int(found[\"precision\"]))\n        else:\n            return cls(precision=int(found[\"precision\"]), scale=int(found[\"scale\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass TinyInt(NumericType):\n    def __repr__(self) -> str:\n        return \"TINYINT\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"TINYINT\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass SmallInt(NumericType):\n    def __repr__(self) -> str:\n        return \"SMALLINT\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"SMALLINT\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Int(NumericType):\n    def __repr__(self) -> str:\n        return \"INT\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"INT\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass BigInt(NumericType):\n    def __repr__(self) -> str:\n        return \"BIGINT\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"BIGINT\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Float(NumericType):\n    is_synonym: bool = False\n\n    def __repr__(self) -> str:\n        return \"FLOAT\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Double(is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"FLOAT\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Double(NumericType):\n    is_synonym: bool = False\n\n    def __repr__(self) -> str:\n        return \"DOUBLE\"\n\n    def __post_init__(self):\n        if not self.is_synonym:\n            self.synonyms.append(Float(is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"DOUBLE\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass DateTimeType(FieldType):\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Date, Time, TimestampType]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Date(DateTimeType):\n    def __repr__(self) -> str:\n        return \"DATE\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"DATE\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Time(DateTimeType):\n    precision: int\n\n    def __repr__(self) -> str:\n        return f\"TIME({self.precision})\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"TIME\\((?P<precision>\\d+)\\)\", type)\n\n        if not found:\n            return None\n\n        return cls(int(found[\"precision\"]))", "\n\n@dataclass(frozen=True, eq=False)\nclass TimestampType(DateTimeType):\n    precision: int\n    timezone: bool\n    is_synonym: bool = False\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Timestamp, TimestampLocal]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Timestamp(TimestampType):\n    timezone: bool = False\n\n    def __repr__(self) -> str:\n        if self.timezone:\n            w = \"WITH\"\n        else:\n            w = \"WITHOUT\"\n\n        return f\"TIMESTAMP({self.precision}) {w} TIME ZONE\"\n\n    def __post_init__(self):\n        if not self.is_synonym and self.timezone:\n            self.synonyms.append(TimestampLocal(self.precision, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(\n            r\"TIMESTAMP\\((?P<precision>\\d+)\\)(?P<timezone_clause> (?P<with_clause>WITH|WITHOUT) TIME ZONE)?\",\n            type,\n        )\n\n        if not found:\n            return None\n\n        timezone: bool\n\n        if not found[\"timezone_clause\"]:\n            timezone = False\n        else:\n            if found[\"with_clause\"] == \"WITH\":\n                timezone = True\n            else:\n                timezone = False\n\n        return cls(precision=int(found[\"precision\"]), timezone=timezone)", "\n\n@dataclass(frozen=True, eq=False)\nclass TimestampLocal(TimestampType):\n    timezone: bool = True\n\n    def __repr__(self) -> str:\n        return f\"TIMESTAMP_LTZ({self.precision})\"\n\n    def __post_init__(self):\n        if not self.is_synonym and self.timezone:\n            self.synonyms.append(Timestamp(self.precision, timezone=True, is_synonym=True))\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found_shorthand = re.fullmatch(r\"TIMESTAMP_LTZ\\((?P<precision>\\d+)\\)\", type)\n        found_full_version = re.fullmatch(\n            r\"TIMESTAMP\\((?P<precision>\\d+)\\) WITH LOCAL TIME ZONE\", type\n        )\n\n        if found_shorthand:\n            return cls(int(found_shorthand[\"precision\"]))\n\n        if found_full_version:\n            return cls(int(found_full_version[\"precision\"]))\n\n        return None", "\n\n@dataclass(frozen=True)\nclass CompoundType(FieldType):\n    class ContainerType(Enum):\n        ARRAY = 0\n        MAP = 1\n        ROW = 2\n\n    container_type: ContainerType = field(init=False, repr=False)\n    internal_types: List[FieldType] = field(init=False, repr=False, default_factory=lambda: [])\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [ArrayType, Map, Row]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found\n\n    def __eq__(self, __o: object) -> bool:\n        if not issubclass(__o.__class__, CompoundType):\n            return False\n\n        return (\n            self.container_type\n            == __o.container_type  # pyright: ignore [reportGeneralTypeIssues], we know __o is a CompoundType\n            and self.internal_types\n            == __o.internal_types  # pyright: ignore [reportGeneralTypeIssues], we know __o is a CompoundType\n        )\n\n    def __hash__(self) -> int:\n        return super().__hash__()", "\n\n@dataclass(frozen=True, eq=False)\nclass ArrayType(CompoundType):\n    container_type: CompoundType.ContainerType = field(\n        init=False, repr=False, default=CompoundType.ContainerType.ARRAY\n    )\n    type: FieldType\n\n    def __post_init__(self):\n        self.internal_types.append(self.type)\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        candidates: List[Type[FieldType]] = [Array, TArray]\n\n        found: Optional[FieldType] = None\n        for candidate in candidates:\n            found = candidate.from_str(type)\n            if found:\n                break\n\n        return found", "\n\n@dataclass(frozen=True, eq=False)\nclass Array(ArrayType):\n    def __repr__(self) -> str:\n        return f\"ARRAY<{self.type}>\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"ARRAY<(?P<inner>.*)>\", type)\n\n        if not found:\n            return None\n\n        inner_type = FieldType.from_str(found[\"inner\"])\n\n        if not inner_type:\n            return None\n\n        return cls(inner_type)", "\n\n@dataclass(frozen=True, eq=False)\nclass TArray(ArrayType):\n    def __repr__(self) -> str:\n        return f\"{self.type} ARRAY\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"(?P<inner>.*) ARRAY\", type)\n\n        if not found:\n            return None\n\n        inner_type = FieldType.from_str(found[\"inner\"])\n\n        if not inner_type:\n            return None\n\n        return cls(inner_type)", "\n\n@dataclass(frozen=True, eq=False)\nclass Map(CompoundType):\n    container_type: CompoundType.ContainerType = field(\n        init=False, repr=False, default=CompoundType.ContainerType.MAP\n    )\n    key: FieldType\n    value: FieldType\n\n    def __post_init__(self):\n        self.internal_types.append(self.key)\n        self.internal_types.append(self.value)\n\n    def __repr__(self) -> str:\n        return f\"MAP<{self.key}, {self.value}>\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"MAP<(?P<key>.*), (?P<value>.*)>\", type)\n\n        if not found:\n            return None\n\n        key_type = FieldType.from_str(found[\"key\"])\n        value_type = FieldType.from_str(found[\"value\"])\n\n        if not key_type or not value_type:\n            return None\n\n        return cls(key_type, value_type)", "\n\n@dataclass(frozen=True, eq=False)\nclass Row(CompoundType):\n    # TODO: Handle ROW types\n    container_type: CompoundType.ContainerType = field(\n        init=False, repr=False, default=CompoundType.ContainerType.ROW\n    )\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        return None", "\n\n@dataclass(frozen=True, eq=False)\nclass PrimaryKey(FieldType):\n    inner_type: FieldType\n\n    def __repr__(self) -> str:\n        return f\"{self.inner_type} PRIMARY KEY\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"(?P<type>.*) PRIMARY KEY\", type)\n\n        if not found:\n            return None\n\n        inner_type = FieldType.from_str(found[\"type\"])\n\n        if not inner_type:\n            return None\n\n        return cls(inner_type)", "\n\n@dataclass(frozen=True, eq=False)\nclass Boolean(FieldType):\n    def __repr__(self) -> str:\n        return \"BOOLEAN\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"BOOLEAN\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Interval(FieldType):\n    def __repr__(self) -> str:\n        return \"INTERVAL\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"INTERVAL\", type)\n\n        if not found:\n            return None\n\n        return cls()", "\n\n@dataclass(frozen=True, eq=False)\nclass Multiset(FieldType):\n    def __repr__(self) -> str:\n        return \"MULTISET\"\n\n    @classmethod\n    def from_str(cls, type: str) -> Optional[FieldType]:\n        found = re.fullmatch(r\"MULTISET\", type)\n\n        if not found:\n            return None\n\n        return cls()", ""]}
{"filename": "decodable/client/client.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom __future__ import annotations\nfrom typing import Any, Dict, Optional, List\nfrom typing_extensions import override", "from typing import Any, Dict, Optional, List\nfrom typing_extensions import override\nimport requests\nfrom dataclasses import dataclass\n\nfrom decodable.client.api import Connector, ConnectionType, StartPosition\nfrom decodable.client.types import FieldType\nfrom decodable.config.client_config import DecodableClientConfig\n\n", "\n\n@dataclass\nclass ApiResponse:\n    items: List[Any]\n    next_page_token: Optional[str]\n\n\n@dataclass\nclass PreviewResponse:\n    id: str\n    output_stream_type: str\n    results: List[Dict[str, Any]]\n    next_token: str\n\n    @classmethod\n    def from_dict(cls, response: Dict[str, Any]) -> PreviewResponse:\n        return cls(\n            id=response[\"id\"],\n            output_stream_type=response[\"output_stream_type\"],\n            results=response[\"results\"],\n            next_token=response[\"next_token\"],\n        )", "@dataclass\nclass PreviewResponse:\n    id: str\n    output_stream_type: str\n    results: List[Dict[str, Any]]\n    next_token: str\n\n    @classmethod\n    def from_dict(cls, response: Dict[str, Any]) -> PreviewResponse:\n        return cls(\n            id=response[\"id\"],\n            output_stream_type=response[\"output_stream_type\"],\n            results=response[\"results\"],\n            next_token=response[\"next_token\"],\n        )", "\n\nclass DecodableAPIException(Exception):\n    @classmethod\n    def category(cls) -> str:\n        return \"DecodableAPIException\"\n\n    @override\n    def __str__(self) -> str:\n        return f\"Decodable: {self.category()}: {self.args[0]}\"", "\n\nclass InvalidRequest(DecodableAPIException):\n    @classmethod\n    def category(cls) -> str:\n        return \"InvalidRequest\"\n\n\nclass ResourceAlreadyExists(DecodableAPIException):\n    @classmethod\n    def category(cls) -> str:\n        return \"ResourceAlreadyExists\"", "class ResourceAlreadyExists(DecodableAPIException):\n    @classmethod\n    def category(cls) -> str:\n        return \"ResourceAlreadyExists\"\n\n\nclass ResourceNotFound(DecodableAPIException):\n    @classmethod\n    def category(cls) -> str:\n        return \"ResourceNotFound\"", "\n\ndef raise_api_exception(code: int, reason: str):\n    if code == 400:\n        raise InvalidRequest(reason)\n    elif code == 404:\n        raise ResourceNotFound(reason)\n    elif code == 409:\n        raise ResourceAlreadyExists(reason)\n    else:\n        raise DecodableAPIException(reason)", "\n\n@dataclass\nclass SchemaField:\n    name: str\n    type: FieldType\n\n    def __hash__(self) -> int:\n        res = hash(self.name)\n        res ^= hash(self.type)\n        return res", "\n\nclass DecodableApiClient:\n    config: DecodableClientConfig\n\n    def __init__(self, config: DecodableClientConfig):\n        self.config = config\n\n    def test_connection(self) -> requests.Response:\n        response = requests.get(\n            url=f\"{self.config.decodable_api_url()}/streams\",\n            headers={\n                \"accept\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n        return response\n\n    def list_streams(self) -> ApiResponse:\n        response = self._get_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/streams\",\n        )\n        return self._parse_response(response.json())\n\n    def get_stream_id(self, name: str) -> Optional[str]:\n        streams = self.list_streams().items\n        stream_id = None\n\n        for stream in streams:\n            if stream[\"name\"] == name:\n                stream_id = stream[\"id\"]\n\n        return stream_id\n\n    def get_stream_information(self, stream_id: str) -> Dict[str, Any]:\n        endpoint_url = f\"{self.config.decodable_api_url()}/streams/{stream_id}\"\n        response = requests.get(\n            url=endpoint_url,\n            headers={\n                \"accept\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if response.ok:\n            return response.json()\n        else:\n            raise_api_exception(response.status_code, response.json())\n\n    def get_stream_from_sql(self, sql: str) -> Dict[str, Any]:\n        payload = {\"sql\": sql}\n\n        return self._post_api_request(\n            payload=payload,\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/outputStream\",\n        ).json()\n\n    def create_stream(\n        self, name: str, schema_fields: List[SchemaField], watermark: Optional[str] = None\n    ) -> ApiResponse:\n        payload = {\n            \"schema\": [{\"name\": field.name, \"type\": repr(field.type)} for field in schema_fields],\n            \"name\": name,\n        }\n\n        if watermark:\n            payload[\"watermark\"] = watermark\n\n        return self._post_api_request(\n            payload=payload, endpoint_url=f\"{self.config.decodable_api_url()}/streams\"\n        ).json()\n\n    def update_stream(self, stream_id: str, props: Dict[str, Any]) -> ApiResponse:\n        endpoint_url = f\"{self.config.decodable_api_url()}/streams/{stream_id}\"\n        return self._patch_api_request(payload=props, endpoint_url=endpoint_url).json()\n\n    def delete_stream(self, stream_id: str) -> None:\n        return self._delete_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/streams/{stream_id}\"\n        )\n\n    def clear_stream(self, stream_id: str) -> None:\n        self._post_api_request(\n            payload={}, endpoint_url=f\"{self.config.decodable_api_url()}/streams/{stream_id}/clear\"\n        )\n\n    def list_pipelines(self) -> ApiResponse:\n        response = self._get_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines\",\n        )\n        return self._parse_response(response.json())\n\n    def get_pipeline_id(self, name: str) -> Optional[str]:\n        pipelines = self.list_pipelines().items\n        pipeline_id = None\n\n        for pipeline in pipelines:\n            if pipeline[\"name\"] == name:\n                pipeline_id = pipeline[\"id\"]\n\n        return pipeline_id\n\n    def get_pipeline_information(self, pipeline_id: str) -> Dict[str, Any]:\n        endpoint_url = f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}\"\n        response = requests.get(\n            url=endpoint_url,\n            headers={\n                \"accept\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if response.ok:\n            return response.json()\n        else:\n            raise_api_exception(response.status_code, response.json())\n\n    def get_associated_streams(self, pipeline_id: str) -> ApiResponse:\n        response = self._get_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}/streams\"\n        )\n        return self._parse_response(response.json())\n\n    def create_pipeline(self, sql: str, name: str, description: str) -> Dict[str, Any]:\n        payload = {\n            \"sql\": sql,\n            \"name\": name,\n            \"description\": description,\n        }\n\n        return self._post_api_request(\n            payload=payload, endpoint_url=f\"{self.config.decodable_api_url()}/pipelines\"\n        ).json()\n\n    def update_pipeline(self, pipeline_id: str, props: Dict[str, Any]) -> Any:\n        return self._patch_api_request(\n            payload=props,\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}\",\n        ).json()\n\n    def activate_pipeline(self, pipeline_id: str) -> Dict[str, Any]:\n        return self._post_api_request(\n            payload={},\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}/activate\",\n        ).json()\n\n    def deactivate_pipeline(self, pipeline_id: str) -> Dict[str, Any]:\n        return self._post_api_request(\n            payload={},\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}/deactivate\",\n        ).json()\n\n    def delete_pipeline(self, pipeline_id: str) -> None:\n        return self._delete_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/pipelines/{pipeline_id}\"\n        )\n\n    def create_preview(\n        self,\n        sql: str,\n        preview_start: StartPosition = StartPosition.LATEST,\n        input_streams: List[str] = [],\n    ) -> PreviewResponse:\n        payload = {\n            \"sql\": sql,\n            \"support_change_stream\": True,\n            \"start_positions\": {\n                stream: {\"type\": \"TAG\", \"value\": preview_start.value} for stream in input_streams\n            },\n        }\n\n        response = self._post_api_request(\n            payload=payload, endpoint_url=f\"{self.config.decodable_api_url()}/preview\"\n        )\n        return PreviewResponse.from_dict(response.json())\n\n    def run_preview(self, id: str, token: str) -> PreviewResponse:\n        response = self._get_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/preview/{id}?token={token}\"\n        )\n        return PreviewResponse.from_dict(response.json())\n\n    def get_preview_dependencies(self, sql: str) -> Dict[str, Any]:\n        payload = {\"sql\": sql}\n\n        return self._post_api_request(\n            payload=payload, endpoint_url=f\"{self.config.decodable_api_url()}/preview/dependencies\"\n        ).json()\n\n    def list_connections(self) -> ApiResponse:\n        response = self._get_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections\",\n        )\n        return self._parse_response(response.json())\n\n    def get_connection_id(self, name: str) -> Optional[str]:\n        connections = self.list_connections().items\n        conn_id = None\n\n        for conn in connections:\n            if conn[\"name\"] == name:\n                conn_id = conn[\"id\"]\n                break\n\n        return conn_id\n\n    def create_connection(\n        self,\n        name: str,\n        schema: List[SchemaField],\n        stream: Optional[str] = None,\n        connector: Connector = Connector.REST,\n        connection_type: ConnectionType = ConnectionType.SOURCE,\n    ) -> Dict[str, Any]:\n        if not stream:\n            stream = name\n\n        payload = {\n            \"name\": name,\n            \"connector\": connector.value,\n            \"type\": connection_type.value,\n            \"schema\": [{\"name\": field.name, \"type\": repr(field.type)} for field in schema],\n        }\n\n        return self._post_api_request(\n            payload=payload,\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections?stream_name={stream}\",\n        ).json()\n\n    def activate_connection(self, conn_id: str) -> Dict[str, Any]:\n        return self._post_api_request(\n            payload={},\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections/{conn_id}/activate\",\n        ).json()\n\n    def deactivate_connection(self, conn_id: str) -> Dict[str, Any]:\n        return self._post_api_request(\n            payload={},\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections/{conn_id}/deactivate\",\n        ).json()\n\n    def delete_connection(self, conn_id: str):\n        self._delete_api_request(\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections/{conn_id}\"\n        )\n\n    def send_events(self, id: str, events: List[Dict[str, Any]]) -> int:\n        payload = {\"events\": events}\n\n        response = self._post_api_request(\n            payload=payload,\n            endpoint_url=f\"{self.config.decodable_api_url()}/connections/{id}/events\",\n        ).json()\n\n        return response[\"count\"]\n\n    def _parse_response(self, result: Any) -> ApiResponse:\n        return ApiResponse(items=result[\"items\"], next_page_token=result[\"next_page_token\"])\n\n    def _post_api_request(self, payload: Any, endpoint_url: str) -> requests.Response:\n        response = requests.post(\n            url=endpoint_url,\n            json=payload,\n            headers={\n                \"accept\": \"application/json\",\n                \"content-type\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if response.ok:\n            return response\n        else:\n            raise_api_exception(response.status_code, response.json())\n\n    def _patch_api_request(self, payload: Any, endpoint_url: str) -> requests.Response:\n        response = requests.patch(\n            url=endpoint_url,\n            json=payload,\n            headers={\n                \"accept\": \"application/json\",\n                \"content-type\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if response.ok:\n            return response\n        else:\n            raise_api_exception(response.status_code, response.json())\n\n    def _get_api_request(self, endpoint_url: str) -> requests.Response:\n        response = requests.get(\n            url=endpoint_url,\n            headers={\n                \"accept\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if response.ok:\n            return response\n        else:\n            raise_api_exception(response.status_code, response.json())\n\n    def _delete_api_request(self, endpoint_url: str) -> None:\n        response = requests.delete(\n            url=endpoint_url,\n            headers={\n                \"accept\": \"application/json\",\n                \"authorization\": f\"Bearer {self.config.access_token}\",\n            },\n        )\n\n        if not response.ok:\n            raise_api_exception(response.status_code, response.json())", ""]}
{"filename": "decodable/client/client_factory.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional\n\nfrom decodable.client.client import DecodableApiClient", "\nfrom decodable.client.client import DecodableApiClient\nfrom decodable.config.client_config import DecodableClientConfig\nfrom decodable.config.profile_reader import DecodableProfileReader\n\n\nclass DecodableClientFactory:\n    @staticmethod\n    def create_client(\n        api_url: str,\n        profile_name: Optional[str] = \"default\",\n        decodable_account_name: Optional[str] = None,\n    ) -> DecodableApiClient:\n        if decodable_account_name is None:\n            raise Exception(\"Undefined Decodable account name. Update DBT profile\")\n        profile_access_tokens = DecodableProfileReader.load_profiles()\n        if profile_name not in profile_access_tokens.profile_tokens:\n            raise Exception(\n                f\"Undefined '{profile_name} in decodable profile file ~/.decodable/auth\"\n            )\n        access_token = profile_access_tokens.profile_tokens[profile_name]\n        return DecodableApiClient(\n            config=DecodableClientConfig(\n                access_token=access_token, account_name=decodable_account_name, api_url=api_url\n            )\n        )", ""]}
{"filename": "decodable/client/api.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nfrom enum import Enum\n", "from enum import Enum\n\n\nclass StartPosition(Enum):\n    EARLIEST = \"earliest\"\n    LATEST = \"latest\"\n\n\nclass Connector(Enum):\n    DATAGEN = \"datagen\"\n    KAFKA = \"kafka\"\n    KINESIS = \"kinesis\"\n    REST = \"rest\"\n    S3 = \"s3\"", "class Connector(Enum):\n    DATAGEN = \"datagen\"\n    KAFKA = \"kafka\"\n    KINESIS = \"kinesis\"\n    REST = \"rest\"\n    S3 = \"s3\"\n\n\nclass ConnectionType(Enum):\n    SOURCE = \"source\"\n    SINK = \"sink\"", "class ConnectionType(Enum):\n    SOURCE = \"source\"\n    SINK = \"sink\"\n"]}
{"filename": "decodable/client/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport pytest\nimport os", "import pytest\nimport os\n\npytest_plugins = [\"dbt.tests.fixtures.project\"]\n\n\n# The profile dictionary, used to write out profiles.yml\n@pytest.fixture(scope=\"class\")  # pyright: ignore [reportUntypedFunctionDecorator]\ndef dbt_profile_target():\n    return {\n        \"type\": \"decodable\",\n        \"database\": \"\",\n        \"schema\": \"\",\n        \"account_name\": os.getenv(\"DECODABLE_ACCOUNT_NAME\", \"test_account\"),\n        \"profile_name\": os.getenv(\"DECODABLE_PROFILE_NAME\", \"test_profile\"),\n        \"local_namespace\": \"functional_tests\",\n    }", "def dbt_profile_target():\n    return {\n        \"type\": \"decodable\",\n        \"database\": \"\",\n        \"schema\": \"\",\n        \"account_name\": os.getenv(\"DECODABLE_ACCOUNT_NAME\", \"test_account\"),\n        \"profile_name\": os.getenv(\"DECODABLE_PROFILE_NAME\", \"test_profile\"),\n        \"local_namespace\": \"functional_tests\",\n    }\n", ""]}
{"filename": "tests/functional/adapter/simple/test_simple_project.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nfrom typing import Any, List\nimport pytest", "from typing import Any, List\nimport pytest\nfrom dbt.tests.util import run_dbt  # pyright: ignore [reportMissingImports]\n\nfrom fixtures import (\n    my_seed_csv,\n    my_model_sql,\n    my_model_yml,\n)\n", ")\n\n\nclass TestSimpleProject:\n    @pytest.fixture(scope=\"class\")\n    def project_config_update(self):\n        return {\"name\": \"simple\", \"models\": {\"+materialized\": \"table\"}}\n\n    @pytest.fixture(scope=\"class\")\n    def seeds(self):\n        return {\n            \"my_seed.csv\": my_seed_csv,\n        }\n\n    @pytest.fixture(scope=\"class\")\n    def models(self):\n        return {\n            \"my_model.sql\": my_model_sql,\n            \"my_model.yml\": my_model_yml,\n        }\n\n    def test_run_seed_test(self, project: Any):\n        \"\"\"\n        Seed, then run, then test. Perform cleanup at the end,\n        so that nothing persists on Decodbale.\n        \"\"\"\n\n        # seed seeds\n        results: List[Any] = run_dbt([\"seed\"])\n        assert len(results) == 1\n\n        # run models\n        results = run_dbt([\"run\"])\n        assert len(results) == 1\n\n        # test tests\n        results = run_dbt([\"test\"])\n        assert len(results) == 1\n\n        # validate that the results include one pass and one failure\n        assert results[0].status == \"pass\"\n\n        # Decodable cleanup\n        run_dbt([\"run-operation\", \"cleanup\"])", ""]}
{"filename": "tests/functional/adapter/simple/fixtures.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\n# seeds/my_seed.csv\nmy_seed_csv = \"\"\"", "# seeds/my_seed.csv\nmy_seed_csv = \"\"\"\nname,age\nAdam,31\nGeorge,27\nLily,59\n\"\"\".lstrip()\n\n# models/my_model.sql\nmy_model_sql = \"\"\"", "# models/my_model.sql\nmy_model_sql = \"\"\"\nselect CHAR_LENGTH(name) as name_length from {{ ref('my_seed') }}\n\"\"\"\n\n# models/my_model.yml\nmy_model_yml = \"\"\"\nversion: 2\nmodels:\n  - name: my_model", "models:\n  - name: my_model\n    columns:\n      - name: name_length\n        tests:\n          - not_null\n\"\"\"\n"]}
{"filename": "tests/unit/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "tests/unit/decodable/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "tests/unit/decodable/config/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "tests/unit/decodable/config/test_profile_reader.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport os\nfrom unittest import mock\nfrom decodable.config.profile import DecodableAccessTokens", "from unittest import mock\nfrom decodable.config.profile import DecodableAccessTokens\nfrom decodable.config.profile_reader import DecodableProfileReader, PROFILE_ENV_VARIABLE_NAME\n\nTEST_PROFILE_NAME = \"default\"\nTEST_PROFILE_ACCESS_TOKEN = \"yyy\"\n\n\nclass TestProfileAdapter:\n    \"\"\"Test getting profile name from env variable\"\"\"\n\n    @mock.patch.dict(os.environ, {PROFILE_ENV_VARIABLE_NAME: \"test\"})\n    def test_get_profile_name(self):\n        assert DecodableProfileReader.get_profile_name(profile_name=None) == \"test\"\n        assert DecodableProfileReader.get_profile_name(profile_name=\"default\") == \"default\"\n\n    \"\"\"Test loading default profile\"\"\"\n\n    def test_load_default_profile(self):\n        test_profile: DecodableAccessTokens = DecodableProfileReader.load_profiles(\n            f\"{os.path.dirname(__file__)}/test_profile.yml\"\n        )\n        assert test_profile.profile_tokens[TEST_PROFILE_NAME] == TEST_PROFILE_ACCESS_TOKEN", "class TestProfileAdapter:\n    \"\"\"Test getting profile name from env variable\"\"\"\n\n    @mock.patch.dict(os.environ, {PROFILE_ENV_VARIABLE_NAME: \"test\"})\n    def test_get_profile_name(self):\n        assert DecodableProfileReader.get_profile_name(profile_name=None) == \"test\"\n        assert DecodableProfileReader.get_profile_name(profile_name=\"default\") == \"default\"\n\n    \"\"\"Test loading default profile\"\"\"\n\n    def test_load_default_profile(self):\n        test_profile: DecodableAccessTokens = DecodableProfileReader.load_profiles(\n            f\"{os.path.dirname(__file__)}/test_profile.yml\"\n        )\n        assert test_profile.profile_tokens[TEST_PROFILE_NAME] == TEST_PROFILE_ACCESS_TOKEN", ""]}
{"filename": "tests/unit/decodable/client/test_types.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nfrom decodable.client import types\n", "from decodable.client import types\n\n\nclass TestTypes:\n    def test_concrete_type_from_str(self):\n        t = types.Char.from_str(\"CHAR(15)\")\n        assert t == types.Char(15)\n\n        t = types.Char.from_str(\"CHAR()\")\n        assert t is None\n\n        t = types.Char.from_str(\"Char(10)\")\n        assert t is None\n\n        t = types.Timestamp.from_str(\"TIMESTAMP(15) WITH TIME ZONE\")\n        assert t == types.Timestamp(precision=15, timezone=True)\n\n        t = types.TimestampLocal.from_str(\"TIMESTAMP(3) WITH LOCAL TIME ZONE\")\n        assert t == types.TimestampLocal(precision=3)\n\n    def test_from_str_dispatch(self):\n        str_types = [\"DECIMAL\", \"STRING\", \"ARRAY<CHAR(1)>\", \"VARBINAR\", \"TIMESTAMP(3)\"]\n\n        expected = [\n            types.Decimal(),\n            types.String(),\n            types.Array(types.Char(1)),\n            None,\n            types.Timestamp(precision=3, timezone=False),\n        ]\n\n        for a, b in zip(str_types, expected):\n            assert types.FieldType.from_str(a) == b\n\n    def test_from_str_defaults(self):\n        a = types.FieldType.from_str(\"DECIMAL\")\n        b = types.FieldType.from_str(\"DECIMAL(10)\")\n        c = types.FieldType.from_str(\"DECIMAL(10, 0)\")\n\n        assert a == b\n        assert b == c\n        assert c == a\n\n    def test_synonyms_equality(self):\n        assert types.Decimal() == types.Dec()\n        assert types.Numeric(15, 3) == types.Decimal(15, 3)\n        assert types.Decimal(5, 1) != types.Numeric(3, 1)\n\n        assert types.Varbinary(types.Bytes.length) == types.Bytes()\n        assert types.Varbinary(100) != types.Bytes()\n\n        assert types.Array(types.Decimal()) == types.TArray(types.Decimal())\n        assert types.Array(types.Decimal()) != types.TArray(types.String())\n        assert types.Array(types.Decimal()) == types.Array(types.Numeric())\n        assert types.Array(types.Decimal()) == types.TArray(types.Numeric())\n\n        assert types.NotNull(types.Decimal()) == types.NotNull(types.Numeric())\n        assert types.NotNull(types.Array(types.Dec())) == types.NotNull(\n            types.TArray(types.Decimal())\n        )\n        assert types.NotNull(types.Array(types.String())) != types.NotNull(\n            types.Array(types.Boolean())\n        )\n        assert types.NotNull(types.Array(types.Bytes())) != types.Bytes()", ""]}
{"filename": "tests/unit/decodable/client/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
{"filename": "dbt/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom pkgutil import extend_path\n\n__path__ = extend_path(__path__, __name__)", "\n__path__ = extend_path(__path__, __name__)\n"]}
{"filename": "dbt/adapters/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom pkgutil import extend_path\n\n__path__ = extend_path(__path__, __name__)", "\n__path__ = extend_path(__path__, __name__)\n"]}
{"filename": "dbt/adapters/decodable/impl.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nfrom dataclasses import dataclass\nfrom typing import Any, ContextManager, Dict, Hashable, List, Optional, Set, Type", "from dataclasses import dataclass\nfrom typing import Any, ContextManager, Dict, Hashable, List, Optional, Set, Type\n\nfrom agate.table import Table as AgateTable\nfrom dbt.adapters.base import BaseAdapter, BaseRelation, Column\nfrom dbt.contracts.connection import Connection\nfrom dbt.contracts.graph.manifest import Manifest\nfrom dbt.contracts.graph.parsed import ParsedNode\nfrom dbt.adapters.base.meta import available\nfrom dbt.contracts.relation import RelationType", "from dbt.adapters.base.meta import available\nfrom dbt.contracts.relation import RelationType\nfrom dbt.events import AdapterLogger\nfrom dbt.exceptions import (\n    NotImplementedException,\n    raise_compiler_error,\n    raise_database_error,\n    raise_parsing_error,\n)\n", ")\n\nfrom dbt.adapters.decodable.connections import (\n    DecodableAdapterConnectionManager,\n    DecodableAdapterCredentials,\n)\nfrom dbt.adapters.decodable.handler import DecodableHandler\nfrom dbt.adapters.decodable.relation import DecodableRelation\nfrom dbt.adapters.protocol import AdapterConfig\nfrom decodable.client.client import DecodableApiClient, SchemaField", "from dbt.adapters.protocol import AdapterConfig\nfrom decodable.client.client import DecodableApiClient, SchemaField\nfrom decodable.client.types import (\n    FieldType,\n    PrimaryKey,\n    String,\n    Boolean,\n    TimestampLocal,\n    Date,\n    Time,", "    Date,\n    Time,\n    Decimal,\n)\n\n\n@dataclass\nclass DecodableConfig(AdapterConfig):\n    watermark: Optional[str] = None\n", "\n\nclass DecodableAdapter(BaseAdapter):\n    \"\"\"\n    Controls actual implmentation of adapter, and ability to override certain methods.\n    \"\"\"\n\n    Relation: Type[DecodableRelation] = DecodableRelation\n    AdapterSpecificConfigs: Type[AdapterConfig] = DecodableConfig\n    ConnectionManager = DecodableAdapterConnectionManager\n\n    connections: DecodableAdapterConnectionManager\n    logger = AdapterLogger(\"Decodable\")\n\n    # AdapterProtocol impl\n\n    def set_query_header(self, manifest: Manifest) -> None:\n        raise NotImplementedError()\n\n    @staticmethod\n    def get_thread_identifier() -> Hashable:\n        raise NotImplementedError()\n\n    def get_thread_connection(self) -> Connection:\n        return self.connections.get_thread_connection()\n\n    def set_thread_connection(self, conn: Connection) -> None:\n        self.set_thread_connection(conn)\n\n    def get_if_exists(self) -> Optional[Connection]:\n        raise NotImplementedError()\n\n    def clear_thread_connection(self) -> None:\n        self.connections.clear_thread_connection()\n\n    def exception_handler(self, sql: str) -> ContextManager[Any]:\n        raise NotImplementedError()\n\n    def set_connection_name(self, name: Optional[str] = None) -> Connection:\n        return self.connections.set_connection_name(name)\n\n    def cancel_open(self) -> Optional[List[str]]:\n        raise NotImplementedError()\n\n    @classmethod\n    def open(cls, connection: Connection) -> Connection:\n        raise NotImplementedError()\n\n    def release(self) -> None:\n        raise NotImplementedError()\n\n    def cleanup_all(self) -> None:\n        raise NotImplementedError()\n\n    def begin(self) -> None:\n        raise NotImplementedError()\n\n    def commit(self) -> None:\n        raise NotImplementedError()\n\n    @classmethod\n    def close(cls, connection: Connection) -> Connection:\n        raise NotImplementedError()\n\n    # AdapterProtocol impl end\n\n    @classmethod\n    def date_function(cls):\n        \"\"\"\n        Returns canonical date func\n        \"\"\"\n        return \"datenow()\"\n\n    @classmethod\n    def convert_text_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(String())\n\n    @classmethod\n    def convert_number_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(Decimal())\n\n    @classmethod\n    def convert_boolean_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(Boolean())\n\n    @classmethod\n    def convert_datetime_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(TimestampLocal(3))\n\n    @classmethod\n    def convert_date_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(Date())\n\n    @classmethod\n    def convert_time_type(cls, agate_table: AgateTable, col_idx: int) -> str:\n        return repr(Time(3))\n\n    @classmethod\n    def is_cancelable(cls) -> bool:\n        return False\n\n    def list_schemas(self, database: str) -> List[str]:\n        return []\n\n    @available.parse_none\n    def create_schema(self, relation: BaseRelation):\n        pass\n\n    @available.parse_none\n    def drop_schema(self, relation: BaseRelation):\n        \"\"\"Drop the given schema (and everything in it) if it exists.\"\"\"\n        pass\n        # raise NotImplementedException(\"`drop_schema` is not implemented for this adapter!\")\n\n    @available\n    @classmethod\n    def quote(cls, identifier: str) -> str:\n        \"\"\"Quote the given identifier, as appropriate for the database.\"\"\"\n        return f\"`{identifier}`\"\n\n    @available.parse_none\n    def drop_relation(self, relation: BaseRelation) -> None:\n        \"\"\"Drop the given relation.\n\n        *Implementors must call self.cache.drop() to preserve cache state!*\n        \"\"\"\n        self.cache_dropped(relation)\n\n        if relation.type is None:\n            raise_compiler_error(f\"Tried to drop relation {relation}, but its type is null.\")\n\n        if relation.identifier is None:\n            return\n\n        client = self._client()\n\n        self.logger.debug(f\"Dropping pipeline '{relation}'...\")\n\n        pipeline_id = client.get_pipeline_id(relation.render())\n        if pipeline_id:\n            pipe_info = client.get_pipeline_information(pipeline_id)\n            if pipe_info[\"actual_state\"] == \"RUNNING\" or pipe_info[\"target_state\"] == \"RUNNING\":\n                client.deactivate_pipeline(pipeline_id)\n            client.delete_pipeline(pipeline_id)\n            self.logger.debug(f\"Pipeline '{relation}' deleted successfully\")\n\n        self.logger.debug(f\"Dropping stream '{relation}'...\")\n\n        stream_id = client.get_stream_id(relation.render())\n\n        if not stream_id:\n            return\n\n        # We need to first delete any pipelines that rely on this stream as their source\n        pipelines = client.list_pipelines().items\n        for pipeline in pipelines:\n            pipe_id = pipeline[\"id\"]\n\n            streams = client.get_associated_streams(pipe_id).items\n            should_delete = False\n            for stream in streams:\n                if stream[\"is_source\"] and stream[\"stream_id\"] == stream_id:\n                    should_delete = True\n                    break\n\n            if not should_delete:\n                continue\n\n            pipe_info = client.get_pipeline_information(pipe_id)\n            # TODO: Reference cache\n            self.drop_relation(\n                self.Relation.create(\n                    database=relation.database,\n                    schema=relation.schema,\n                    identifier=pipe_info[\"name\"],\n                    type=RelationType.Table,\n                )\n            )\n\n        client.delete_stream(stream_id)\n        self.logger.debug(f\"Stream '{relation}' deleted successfully\")\n\n    @available.parse_none\n    def truncate_relation(self, relation: BaseRelation) -> None:\n        \"\"\"Truncate the given relation.\"\"\"\n        if not relation.identifier:\n            raise_compiler_error(\"Cannot truncate an unnamed relation\")\n\n        client = self._client()\n        stream_id = client.get_stream_id(relation.render())\n\n        if not stream_id:\n            raise_database_error(\n                f\"Error clearing stream `{relation.render()}`: stream doesn't exist\"\n            )\n\n        client.clear_stream(stream_id)\n\n    @available.parse_none\n    def rename_relation(self, from_relation: BaseRelation, to_relation: BaseRelation) -> None:\n        \"\"\"Rename the relation from from_relation to to_relation.\n\n        Implementors must call self.cache.rename() to preserve cache state.\n        \"\"\"\n        client = self._client()\n        self.cache_renamed(from_relation, to_relation)\n\n        if not from_relation.identifier:\n            raise_compiler_error(\"Cannot rename an unnamed relation\")\n\n        stream_id = client.get_stream_id(from_relation.render())\n\n        if not stream_id:\n            raise_database_error(f\"Cannot rename '{from_relation}': stream does not exist\")\n\n        if not to_relation.identifier:\n            raise_compiler_error(f\"Cannot rename relation {from_relation} to nothing\")\n\n        client.update_stream(stream_id=stream_id, props={\"name\": to_relation.render()})\n        self.logger.debug(f\"Renamed stream '{from_relation}' to '{to_relation}'\")\n\n        pipeline_id = client.get_pipeline_id(from_relation.render())\n\n        if not pipeline_id:\n            raise_database_error(\n                f\"Cannot rename '{from_relation.render()}': pipeline does not exist\"\n            )\n\n        pipe_info = client.get_pipeline_information(pipeline_id)\n        if not pipe_info[\"sql\"]:\n            raise_database_error(\n                f\"Cannot rename relation '{from_relation}': pipeline returned no sql\"\n            )\n        sql: str = pipe_info[\"sql\"]\n\n        client.update_pipeline(\n            pipeline_id=pipeline_id,\n            props={\n                \"name\": to_relation.render(),\n                \"sql\": self._replace_sink(from_relation, to_relation, sql),\n                \"description\": self._pipeline_description(to_relation),\n            },\n        )\n        self.logger.debug(f\"Renamed pipeline '{from_relation}' to '{to_relation}'\")\n\n        # Update the sql for any pipelines that had `from_relation` as an inbound stream\n        renamed_sources: int = 0\n        pipelines = client.list_pipelines().items\n        for pipeline in pipelines:\n            pipe_id = pipeline[\"id\"]\n\n            streams = client.get_associated_streams(pipe_id).items\n            should_update = False\n            for stream in streams:\n                if stream[\"is_source\"] and stream[\"stream_id\"] == stream_id:\n                    should_update = True\n                    break\n\n            if not should_update:\n                continue\n\n            pipe_info = client.get_pipeline_information(pipe_id)\n            if pipe_info[\"sql\"]:\n                client.update_pipeline(\n                    pipeline_id=pipe_id,\n                    props={\n                        \"sql\": self._replace_source(from_relation, to_relation, pipe_info[\"sql\"])\n                    },\n                )\n                renamed_sources += 1\n\n        self.logger.debug(\n            f\"Renamed sources from '{from_relation}' to '{to_relation}' in {renamed_sources} pipelines\"\n        )\n\n    def expand_column_types(self, goal: BaseRelation, current: BaseRelation) -> None:\n        \"\"\"Expand the current table's types to match the goal table. (passable)\n\n        :param self.Relation goal: A relation that currently exists in the\n            database with columns of the desired types.\n        :param self.Relation current: A relation that currently exists in the\n            database with columns of unspecified types.\n        \"\"\"\n        raise NotImplementedException(\n            \"`expand_target_column_types` is not implemented for this adapter!\"\n        )\n\n    def list_relations_without_caching(self, schema_relation: BaseRelation) -> List[BaseRelation]:\n        relations: List[BaseRelation] = []\n\n        stream_list: List[Dict[str, Any]] = self._client().list_streams().items\n        for stream in stream_list:\n            relations.append(\n                self.Relation.create(\n                    database=schema_relation.database,\n                    schema=schema_relation.schema,\n                    identifier=stream[\"name\"],\n                    type=RelationType.Table,\n                )\n            )\n\n        return relations\n\n    @available.parse_list\n    def get_columns_in_relation(\n        self,\n        relation: BaseRelation,\n    ) -> List[Column]:\n        columns: List[Column] = []\n        if not relation.identifier:\n            return []\n\n        stream_info = self._client().get_stream_information(stream_id=relation.render())\n\n        for schema_column in stream_info[\"schema\"]:\n            columns.append(\n                Column.create(name=schema_column[\"name\"], label_or_dtype=schema_column[\"type\"])\n            )\n\n        return columns\n\n    @available\n    def has_changed(\n        self,\n        sql: str,\n        relation: BaseRelation,\n        watermark: Optional[str] = None,\n        primary_key: Optional[str] = None,\n    ) -> bool:\n        client = self._client()\n\n        new_pipe_sql = self._wrap_as_pipeline(relation.render(), sql)\n        fields: List[Dict[str, str]] = client.get_stream_from_sql(new_pipe_sql)[\"schema\"]\n\n        schema: List[SchemaField]\n        try:\n            schema = self._schema_from_json(fields)\n            # The API returns the current primary key field. For the to-be value in this comparison, we set it to\n            # the field specified via config.\n            self._remove_primary_key(schema)\n            if primary_key:\n                self._set_primary_key(primary_key, schema)\n        except Exception as err:\n            raise_compiler_error(f\"Error checking changes to the '{relation}' stream: {err}\")\n\n        pipe_id = client.get_pipeline_id(relation.render())\n        if not pipe_id:\n            return True\n        pipe_info = client.get_pipeline_information(pipe_id)\n\n        stream_id = client.get_stream_id(relation.render())\n        if not stream_id:\n            return True\n        stream_info = client.get_stream_information(stream_id)\n\n        if pipe_info[\"sql\"] != new_pipe_sql:\n            return True\n\n        if stream_info[\"watermark\"] != watermark:\n            return True\n\n        existing_schema: List[SchemaField]\n        existing_schema_fields = stream_info[\"schema\"]\n        try:\n            existing_schema = self._schema_from_json(existing_schema_fields)\n        except Exception as err:\n            raise_compiler_error(f\"Error checking changes to the '{relation}' stream: {err}\")\n\n        if existing_schema != schema:\n            return True\n\n        return False\n\n    @available\n    def create_table(\n        self,\n        sql: str,\n        temporary: bool,\n        relation: BaseRelation,\n        nodes: Dict[str, Any],\n        watermark: Optional[str] = None,\n        primary_key: Optional[str] = None,\n    ) -> None:\n        if not relation.identifier:\n            raise_compiler_error(\"Cannot create an unnamed relation\")\n\n        self.logger.debug(f\"Creating table {relation}\")\n\n        name: str = relation.identifier.split(\"__\")[0]  # strip any suffixes added by dbt\n        model: Optional[ParsedNode] = None\n        for node, info in nodes.items():\n            if info[\"alias\"] == name:\n                model = ParsedNode.from_dict(nodes[node])\n\n        if not model:\n            self.logger.debug(f\"Model {relation.render()} not found in dbt graph\")\n\n        client = self._client()\n\n        fields: List[Dict[str, str]] = client.get_stream_from_sql(\n            self._wrap_as_pipeline(relation.render(), sql)\n        )[\"schema\"]\n\n        if not fields:\n            raise_database_error(\n                f\"Error creating the {relation} stream: empty schema returned for sql:\\n{sql}\"\n            )\n\n        schema: List[SchemaField]\n        try:\n            schema = self._schema_from_json(fields)\n        except Exception as err:\n            raise_compiler_error(f\"Error creating the {relation} stream: {err}\")\n\n        schema_hints: Set[SchemaField]\n        try:\n            if model:\n                schema_hints = self._get_model_schema_hints(model)\n            else:\n                schema_hints = set()\n        except Exception as err:\n            raise_parsing_error(f\"Error creating the {relation} stream: {err}\")\n\n        if not schema_hints.issubset(schema):\n            self.logger.warning(\n                f\"Column hints for '{name}' don't match the resulting schema:\\n{self._pretty_schema(list(schema_hints), 1, 'hints')}\\n{self._pretty_schema(schema, 1, 'schema')}\"\n            )\n\n        if primary_key:\n            for field in schema:\n                if field.name == primary_key:\n                    field.type = PrimaryKey(field.type)\n\n        stream_id = client.get_stream_id(relation.render())\n        if not stream_id:\n            client.create_stream(relation.render(), schema, watermark)\n            self.logger.debug(f\"Stream '{relation}' successfully created!\")\n        else:\n            raise_database_error(f\"Error creating the {relation} stream: stream already exists!\")\n\n        # Both stream and source should exist now, so we can create the pipeline\n        pipeline = client.create_pipeline(\n            sql=self._wrap_as_pipeline(relation.render(), sql),\n            name=relation.render(),\n            description=self._pipeline_description(relation),\n        )\n        client.activate_pipeline(pipeline_id=pipeline[\"id\"])\n        self.logger.debug(f\"Pipeline '{relation}' successfully created!\")\n\n    @available\n    def create_seed_table(\n        self, table_name: str, agate_table: AgateTable, column_override: Dict[str, str]\n    ):\n        schema: List[SchemaField] = []\n\n        column_names: tuple[str] = agate_table.column_names\n        for ix, col_name in enumerate(column_names):\n            type: Optional[str] = self.convert_type(agate_table, ix)\n            if not type:\n                raise_compiler_error(\n                    f\"Couldn't infer type for column `{col_name}` in seed `{table_name}`\"\n                )\n\n            field_type = FieldType.from_str(type)\n            override = column_override.get(col_name, \"\")\n\n            if override:\n                override_field_type = FieldType.from_str(override)\n                if not override_field_type:\n                    self.logger.warning(\n                        f\"Type override `{override}` for column `{col_name}` in seed `{table_name}` doesn't match any of Decodable's known types. Falling back to inferred type.\"\n                    )\n                else:\n                    field_type = override_field_type\n\n            if not field_type:\n                raise_compiler_error(\n                    f\"Inferred type `{type}` for column `{col_name}` doesn't match any of Decodable's known types\"\n                )\n\n            schema.append(SchemaField(name=col_name, type=field_type))\n\n        client = self._client()\n\n        self.logger.debug(f\"Creating connection and stream for seed `{table_name}`...\")\n        response = client.create_connection(name=table_name, schema=schema)\n        self.logger.debug(f\"Connection and stream `{table_name}` successfully created!\")\n\n        self.logger.debug(f\"Activating connection `{table_name}`...\")\n        client.activate_connection(conn_id=response[\"id\"])\n        self.logger.debug(f\"Connection `{table_name}` activated!\")\n\n    @available\n    def send_seed_as_events(self, seed_name: str, data: AgateTable):\n        self.logger.debug(f\"Sending data to connection `{seed_name}`\")\n        client = self._client()\n\n        conn_id = client.get_connection_id(seed_name)\n        if not conn_id:\n            raise_database_error(\n                f\"Trying to send seed events to a non-existing connection `{seed_name}`\"\n            )\n\n        events: List[Dict[str, Any]] = []\n        for row in data.rows:\n            event: Dict[str, Any] = {\n                col_name: str(row[col_name])  # pyright: ignore [reportUnknownArgumentType]\n                for col_name in data.column_names\n            }\n            events.append(event)\n\n        events_received = client.send_events(conn_id, events)\n        if len(events) != events_received:\n            self.logger.warning(\n                f\"While seeding data for `{seed_name}`: sent {len(events)} but connection reported only {events_received} events received.\"\n            )\n\n        client.deactivate_connection(conn_id)\n\n    @available\n    def reactivate_connection(self, connection: Relation):\n        client = self._client()\n\n        conn_id = client.get_connection_id(connection.render())\n        if not conn_id:\n            raise_database_error(f\"Unable to reactivate connection: '{connection}' does not exist\")\n\n        client.activate_connection(conn_id)\n\n    @available\n    def stop_pipeline(self, pipe: Relation):\n        client = self._client()\n\n        pipe_id = client.get_pipeline_id(pipe.render())\n        if not pipe_id:\n            raise_database_error(f\"Unable to deactivate pipeline: '{pipe}' does not exist\")\n\n        client.deactivate_pipeline(pipe_id)\n\n    @available\n    def delete_pipeline(self, pipe: Relation):\n        client = self._client()\n\n        pipeline_id = client.get_pipeline_id(pipe.render())\n        if pipeline_id:\n            pipe_info = client.get_pipeline_information(pipeline_id)\n            if pipe_info[\"actual_state\"] == \"RUNNING\" or pipe_info[\"target_state\"] == \"RUNNING\":\n                client.deactivate_pipeline(pipeline_id)\n            client.delete_pipeline(pipeline_id)\n\n    @available\n    def delete_stream(self, stream: Relation, skip_errors: bool = False):\n        client = self._client()\n\n        stream_id = client.get_stream_id(stream.render())\n        if not stream_id:\n            raise_database_error(f\"Unable to delete stream: `{stream}` does not exist\")\n\n        try:\n            client.delete_stream(stream_id)\n        except Exception as e:\n            if skip_errors:\n                self.logger.warning(f\"Deleting stream `{stream}` failed: {e}\")\n            else:\n                raise_database_error(f\"Deleting stream `{stream}` failed: {e}\")\n\n    @available\n    def delete_connection(self, conn: Relation):\n        client = self._client()\n\n        conn_id = client.get_connection_id(conn.render())\n        if not conn_id:\n            raise_database_error(f\"Unable to delete connection: `{conn}` does not exist\")\n\n        client.deactivate_connection(conn_id)\n        client.delete_connection(conn_id)\n\n    @available\n    def replace_disallowed_operations(self, sql: str) -> str:\n        return sql.replace(\"!=\", \"<>\")\n\n    @available\n    def should_materialize_tests(self) -> bool:\n        credentials: Optional[\n            DecodableAdapterCredentials\n        ] = self.get_thread_connection().credentials\n        if not credentials:\n            return False\n        return credentials.materialize_tests\n\n    def _client(self) -> DecodableApiClient:\n        handle: DecodableHandler = (\n            self.get_thread_connection().handle\n        )  # pyright: ignore [reportGeneralTypeIssues]\n        return handle.client\n\n    @classmethod\n    def _get_model_schema_hints(cls, model: ParsedNode) -> Set[SchemaField]:\n        schema: Set[SchemaField] = set()\n        for column in model.columns.values():\n            name: str = column.name\n            data_type: Optional[str] = column.data_type\n\n            if not data_type:\n                continue\n\n            t = FieldType.from_str(data_type)\n            if not t:\n                raise_compiler_error(f\"Type '{data_type}' not recognized\")\n\n            schema.add(SchemaField(name=name, type=t))\n\n        return schema\n\n    @staticmethod\n    def _set_primary_key(primary_key_field: str, schema: List[SchemaField]) -> None:\n        \"\"\"\n        Sets the primary key to the specified field in the provided schema. Does nothing if the schema does not contain\n        the specified field.\n        \"\"\"\n        for field in schema:\n            if isinstance(field.type, PrimaryKey):\n                raise ValueError(\n                    f\"Trying to set primary key to {primary_key_field}, but schema already has a primary \"\n                    f\"key assigned to {field.name}\"\n                )\n            if field.name == primary_key_field:\n                field.type = PrimaryKey(field.type)\n\n    @staticmethod\n    def _remove_primary_key(schema: List[SchemaField]) -> None:\n        \"\"\"\n        Removes the primary key from the provided schema (if present).\n        \"\"\"\n        for field in schema:\n            if isinstance(field.type, PrimaryKey):\n                field.type = field.type.inner_type\n\n    @staticmethod\n    def _pretty_schema(\n        schema: List[SchemaField], indent: int = 0, name: Optional[str] = None\n    ) -> str:\n        fields = \"\"\n        for field in sorted(schema, key=lambda sf: sf.name):\n            i = \"\\t\" * (indent + 1)\n            fields += f\"{i}{field.name}: {field.type},\\n\"\n\n        i = \"\\t\" * indent\n        prefix = f\"{i}{{\"\n        if name:\n            prefix = f\"{i}{name} = {{\"\n\n        suffix = f\"{i}}}\"\n        if not fields:\n            suffix = \"}\"\n\n        return f\"{prefix}\\n{fields}{suffix}\"\n\n    @classmethod\n    def _schema_from_json(cls, json: List[Dict[str, str]]) -> List[SchemaField]:\n        schema: List[SchemaField] = []\n        for field in json:\n            t = FieldType.from_str(field[\"type\"])\n            if not t:\n                raise_compiler_error(f\"Type '{field['type']}' not recognized\")\n            schema.append(SchemaField(name=field[\"name\"], type=t))\n        return schema\n\n    @classmethod\n    def _wrap_as_pipeline(cls, sink: str, sql: str) -> str:\n        return f\"INSERT INTO {sink} {sql}\"\n\n    @classmethod\n    def _replace_sink(cls, old_sink: BaseRelation, new_sink: BaseRelation, sql: str) -> str:\n        return sql.replace(f\"INSERT INTO {old_sink}\", f\"INSERT INTO {new_sink}\", 1)\n\n    @classmethod\n    def _replace_source(cls, old_source: BaseRelation, new_source: BaseRelation, sql: str) -> str:\n        sql = sql.replace(f\"from {old_source}\", f\"from {new_source}\")\n        return sql.replace(f\"FROM {old_source}\", f\"FROM {new_source}\")\n\n    @classmethod\n    def _pipeline_description(cls, relation: BaseRelation) -> str:\n        return f\"Pipeline for the '{relation}' dbt model\"", ""]}
{"filename": "dbt/adapters/decodable/relation.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom dataclasses import dataclass\nfrom dbt.adapters.base import BaseRelation\nfrom dbt.contracts.relation import Policy", "from dbt.adapters.base import BaseRelation\nfrom dbt.contracts.relation import Policy\n\n\n@dataclass\nclass DecodableQuotePolicy(Policy):\n    database: bool = False\n    schema: bool = False\n    identifier: bool = False\n", "\n\n@dataclass\nclass DecodableIncludePolicy(Policy):\n    database: bool = False\n    schema: bool = False\n    identifier: bool = True\n\n\n@dataclass(frozen=True, eq=False, repr=False)\nclass DecodableRelation(BaseRelation):\n    quote_policy: Policy = DecodableQuotePolicy()\n    include_policy: Policy = DecodableIncludePolicy()\n    dbt_created: bool = True", "\n@dataclass(frozen=True, eq=False, repr=False)\nclass DecodableRelation(BaseRelation):\n    quote_policy: Policy = DecodableQuotePolicy()\n    include_policy: Policy = DecodableIncludePolicy()\n    dbt_created: bool = True\n"]}
{"filename": "dbt/adapters/decodable/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom dbt.adapters.decodable.connections import DecodableAdapterCredentials\nfrom dbt.adapters.decodable.impl import DecodableAdapter\n", "from dbt.adapters.decodable.impl import DecodableAdapter\n\nfrom dbt.adapters.base import AdapterPlugin\nimport dbt.include.decodable as decodable\n\n\nPlugin = AdapterPlugin(\n    adapter=DecodableAdapter,\n    credentials=DecodableAdapterCredentials,\n    include_path=decodable.PACKAGE_PATH,", "    credentials=DecodableAdapterCredentials,\n    include_path=decodable.PACKAGE_PATH,\n)\n"]}
{"filename": "dbt/adapters/decodable/connections.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Any, Optional, Tuple\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass", "from contextlib import contextmanager\nfrom dataclasses import dataclass\n\nfrom agate.table import Table\nfrom dbt.adapters.sql.connections import SQLConnectionManager\nfrom dbt.contracts.connection import (\n    AdapterResponse,\n    Connection,\n    Credentials,\n)", "    Credentials,\n)\nfrom dbt.events import AdapterLogger\nfrom dbt.exceptions import RuntimeException\n\nfrom dbt.adapters.decodable.handler import DecodableCursor, DecodableHandler\nfrom decodable.client.client_factory import DecodableClientFactory\nfrom decodable.client.api import StartPosition\n\n", "\n\n@dataclass\nclass DecodableAdapterCredentials(Credentials):\n    \"\"\"\n    Defines database specific credentials that get added to\n    profiles.yml to connect to new adapter\n    \"\"\"\n\n    profile_name: str\n    account_name: str\n    materialize_tests: bool = False\n    preview_start: StartPosition = StartPosition.EARLIEST\n    request_timeout_ms: int = 60000\n    local_namespace: Optional[str] = None\n\n    api_url: str = \"api.decodable.co/v1alpha2\"\n\n    _ALIASES = {\n        \"profile\": \"profile_name\",\n        \"account\": \"account_name\",\n        \"request_timeout\": \"request_timeout_ms\",\n        \"timeout_ms\": \"request_timeout_ms\",\n        \"timeout\": \"request_timeout_ms\",\n    }\n\n    @property\n    def unique_field(self) -> str:\n        return f\"{self.account_name}.{self.profile_name}\"\n\n    @property\n    def type(self):\n        \"\"\"Return name of adapter.\"\"\"\n        return \"decodable\"\n\n    def _connection_keys(self):\n        \"\"\"\n        List of keys to display in the `dbt debug` output.\n        \"\"\"\n        return (\n            \"profile_name\",\n            \"account_name\",\n            \"materialize_tests\",\n            \"preview_start\",\n            \"request_timeout_ms\",\n            \"local_namespace\",\n        )", "\n\nclass DecodableAdapterConnectionManager(SQLConnectionManager):\n    TYPE: str = \"decodable\"\n\n    logger = AdapterLogger(\"Decodable\")\n\n    @classmethod\n    def get_response(cls, cursor: Any) -> AdapterResponse:\n        \"\"\"Get the status of the cursor.\"\"\"\n        return AdapterResponse(\"OK\")\n\n    @contextmanager\n    def exception_handler(self, sql: str):\n        \"\"\"\n        Returns a context manager, that will handle exceptions raised\n        from queries, catch, log, and raise dbt exceptions it knows how to handle.\n        \"\"\"\n        try:\n            yield\n        except Exception as e:\n            self.logger.error(\"Exception thrown during execution: {}\".format(str(e)))\n            raise RuntimeException(str(e))\n\n    @classmethod\n    def open(cls, connection: Connection) -> Connection:\n        \"\"\"\n        Receives a connection object and a Credentials object\n        and moves it to the \"open\" state.\n        \"\"\"\n        if not connection.credentials:\n            raise RuntimeException(\"Cannot open a Decodable connection without credentials\")\n\n        credentials: DecodableAdapterCredentials = connection.credentials\n        client = DecodableClientFactory.create_client(\n            api_url=credentials.api_url,\n            profile_name=credentials.profile_name,\n            decodable_account_name=credentials.account_name,\n        )\n\n        decodable_connection_test = client.test_connection()\n        if not decodable_connection_test.ok:\n            error_message = \"\"\n            if (\n                decodable_connection_test.reason is not None\n                and len(decodable_connection_test.reason) > 0\n            ):\n                error_message = f\"\\nReason: {decodable_connection_test.reason}\"\n            raise RuntimeException(\n                f\"Status code: {decodable_connection_test.status_code}. Decodable connection failed. Try running 'decodable login' first{error_message}\"\n            )\n\n        connection.handle = DecodableHandler(\n            client, credentials.preview_start, credentials.request_timeout_ms / 1000\n        )\n        return connection\n\n    def cancel(self, connection: Connection):\n        \"\"\"\n        Gets a connection object and attempts to cancel any ongoing queries.\n        \"\"\"\n        pass\n\n    def begin(self) -> Connection:\n        return self.get_thread_connection()\n\n    def commit(self) -> Connection:\n        return self.get_thread_connection()\n\n    def execute(\n        self, sql: str, auto_begin: bool = False, fetch: bool = False\n    ) -> Tuple[AdapterResponse, Table]:\n        sql = self._add_query_comment(sql)\n        if fetch:\n            _, cursor = self.add_query(sql, auto_begin)\n            response = self.get_response(cursor)\n            table = self.get_result_from_cursor(cursor)\n        else:\n            response = AdapterResponse(\"OK\")\n            cursor = self._dummy_cursor()\n            cursor.seed_fake_results()\n            table = self.get_result_from_cursor(cursor)\n        return response, table\n\n    def _dummy_cursor(self) -> DecodableCursor:\n        conn = self.get_thread_connection()\n        return (\n            conn.handle.cursor()  # pyright: ignore [reportOptionalMemberAccess, reportGeneralTypeIssues]\n        )", ""]}
{"filename": "dbt/adapters/decodable/handler.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom random import randint\nfrom time import sleep\nfrom typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple", "from time import sleep\nfrom typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple\n\nfrom dbt.events import AdapterLogger\n\nfrom decodable.client.client import DecodableApiClient\nfrom decodable.client.api import StartPosition\n\n\ndef exponential_backoff(timeout: float) -> Iterator[float]:\n    epsilon = 0.001\n    backoff: float = 1.0\n    total_time: float = 0\n    while True:\n        yield total_time\n        stagger = randint(0, 1000) / 1000\n        time = min(backoff + stagger, timeout - total_time)\n        sleep(time)\n        total_time += time\n        backoff *= 2\n\n        if timeout - total_time < epsilon:\n            break", "\ndef exponential_backoff(timeout: float) -> Iterator[float]:\n    epsilon = 0.001\n    backoff: float = 1.0\n    total_time: float = 0\n    while True:\n        yield total_time\n        stagger = randint(0, 1000) / 1000\n        time = min(backoff + stagger, timeout - total_time)\n        sleep(time)\n        total_time += time\n        backoff *= 2\n\n        if timeout - total_time < epsilon:\n            break", "\n\nclass DecodableCursor:\n    logger = AdapterLogger(\"Decodable\")\n\n    def __init__(self, client: DecodableApiClient, preview_start: StartPosition, timeout: float):\n        self.logger.debug(\n            f\"Creating new cursor(preview_start: {preview_start}, timeout: {timeout})\"\n        )\n        self.client = client\n        self.preview_start = preview_start\n        self.timeout = timeout\n        self.last_sql: Optional[str] = None\n        self.last_result: Optional[Sequence[Dict[str, Any]]]\n\n    def execute(self, sql: str, bindings: Optional[Sequence[Any]] = None) -> None:\n        self.logger.debug(f\"Execute(sql): {sql}\")\n        inputs: List[Dict[str, Any]] = self.client.get_preview_dependencies(sql)[\"inputs\"]\n        input_streams: List[str] = [i[\"resourceName\"] for i in inputs]\n        response = self.client.create_preview(sql, self.preview_start, input_streams)\n        self.logger.debug(f\"Create preview response: {response}\")\n\n        append_stream = response.output_stream_type == \"APPEND\"\n        self.last_result = []\n\n        for _ in exponential_backoff(self.timeout):\n            token = response.next_token\n            response = self.client.run_preview(id=response.id, token=token)\n            self.logger.debug(f\"Run preview response: {response}\")\n\n            if append_stream:\n                self.last_result.extend(response.results)\n            elif response.results:\n                last_change = response.results[-1]\n                if not last_change[\"after\"]:\n                    self.last_result = []\n                else:\n                    self.last_result = [last_change[\"after\"]]\n\n            if response.next_token is None:\n                break\n\n        if not self.last_result:\n            self.seed_fake_results()\n\n    def fetchall(self) -> Sequence[Tuple[Any]]:\n        results = self.last_result\n        self.last_result = None\n\n        tuples: Sequence[Tuple[Any]] = []\n        if results:\n            l = []\n            for result in results:\n                for val in result.values():\n                    l.append(val)\n                tuples.append(tuple(l))\n\n        return tuples\n\n    @property\n    def description(self) -> List[Tuple[str]]:\n        result: List[Tuple[str]] = []\n\n        if not self.last_result:\n            return [(\"failures\",), (\"should_warn\",), (\"should_error\",)]\n\n        for name in self.last_result[0].keys():\n            result.append((name,))\n        return result\n\n    def seed_fake_results(self):\n        self.last_result = [{\"failures\": 0, \"should_warn\": False, \"should_error\": False}]", "\n\nclass DecodableHandler:\n    def __init__(self, client: DecodableApiClient, preview_start: StartPosition, timeout: float):\n        self.client = client\n        self.preview_start = preview_start\n        self.timeout = timeout\n\n    def cursor(self) -> DecodableCursor:\n        return DecodableCursor(self.client, self.preview_start, self.timeout)", ""]}
{"filename": "dbt/adapters/decodable/__version__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nversion = \"1.3.2\"\n", "version = \"1.3.2\"\n"]}
{"filename": "dbt/include/decodable/__init__.py", "chunked_list": ["#\n#  Copyright 2023 decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport os\n\nPACKAGE_PATH = os.path.dirname(__file__)", "\nPACKAGE_PATH = os.path.dirname(__file__)\n"]}
{"filename": ".github/plugin-discovery/plugin_discovery.py", "chunked_list": ["import re\nimport subprocess\nimport sys\n\n\ndef main():\n    out = subprocess.run([\"dbt\", \"--version\"], stderr=subprocess.PIPE, text=True)\n    plugin_detected = (\n        re.search(r\"Plugins:.*\\s- decodable: \\d+\\.\\d+\\.\\d+\", out.stderr, flags=re.DOTALL)\n        is not None\n    )\n    if not plugin_detected:\n        sys.exit(\n            f\"Decodable plugin not recognized by dbt! Received output of `dbt --version`:\\n{out.stderr}\"\n        )", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": ".github/license-check/license-header.py", "chunked_list": ["#\n#  Copyright %year% decodable Inc.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software", "#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n"]}
