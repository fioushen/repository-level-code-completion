{"filename": "concept_erasure/oracle.py", "chunked_list": ["from dataclasses import dataclass\n\nimport torch\nfrom torch import Tensor\n\nfrom .caching import cached_property, invalidates_cache\nfrom .shrinkage import optimal_linear_shrinkage\n\n\n@dataclass(frozen=True)\nclass OracleEraser:\n    \"\"\"Surgically erases a concept from a representation, given concept labels.\"\"\"\n\n    coef: Tensor\n    mean_z: Tensor\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"OracleEraser\":\n        \"\"\"Convenience method to fit an OracleEraser on data and return it.\"\"\"\n        return OracleFitter.fit(x, z, **kwargs).eraser\n\n    def __call__(self, x: Tensor, z: Tensor) -> Tensor:\n        \"\"\"Replace `x` with the OLS residual given `z`.\"\"\"\n        # Ensure Z is at least 2D\n        z = z.reshape(len(z), -1).type_as(x)\n        expected_x = (z - self.mean_z) @ self.coef.T\n\n        return x.sub(expected_x).type_as(x)", "\n@dataclass(frozen=True)\nclass OracleEraser:\n    \"\"\"Surgically erases a concept from a representation, given concept labels.\"\"\"\n\n    coef: Tensor\n    mean_z: Tensor\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"OracleEraser\":\n        \"\"\"Convenience method to fit an OracleEraser on data and return it.\"\"\"\n        return OracleFitter.fit(x, z, **kwargs).eraser\n\n    def __call__(self, x: Tensor, z: Tensor) -> Tensor:\n        \"\"\"Replace `x` with the OLS residual given `z`.\"\"\"\n        # Ensure Z is at least 2D\n        z = z.reshape(len(z), -1).type_as(x)\n        expected_x = (z - self.mean_z) @ self.coef.T\n\n        return x.sub(expected_x).type_as(x)", "\n\nclass OracleFitter:\n    \"\"\"Compute stats needed for surgically erasing a concept Z from a random vector X.\n\n    Unlike `LeaceFitter`, the resulting erasure function requires oracle concept labels\n    at inference time. In exchange, it achieves more surgical edits.\n    \"\"\"\n\n    mean_x: Tensor\n    \"\"\"Running mean of X.\"\"\"\n\n    mean_z: Tensor\n    \"\"\"Running mean of Z.\"\"\"\n\n    sigma_xz_: Tensor\n    \"\"\"Unnormalized cross-covariance matrix X^T Z.\"\"\"\n\n    sigma_zz_: Tensor\n    \"\"\"Unnormalized covariance matrix Z^T Z.\"\"\"\n\n    n: Tensor\n    \"\"\"Number of X samples seen so far.\"\"\"\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"OracleFitter\":\n        \"\"\"Convenience method to fit a OracleFitter on data and return it.\"\"\"\n        n, d = x.shape\n        _, k = z.reshape(n, -1).shape\n\n        fitter = OracleFitter(d, k, device=x.device, dtype=x.dtype, **kwargs)\n        return fitter.update(x, z)\n\n    def __init__(\n        self,\n        x_dim: int,\n        z_dim: int,\n        *,\n        device: str | torch.device | None = None,\n        dtype: torch.dtype | None = None,\n        shrinkage: bool = False,\n        svd_tol: float = 0.01,\n    ):\n        \"\"\"Initialize a `OracleFitter`.\n\n        Args:\n            x_dim: Dimensionality of the representation.\n            z_dim: Dimensionality of the concept.\n            method: Type of projection matrix to use.\n            device: Device to put the statistics on.\n            dtype: Data type to use for the statistics.\n            shrinkage: Whether to use shrinkage to estimate the covariance matrix of X.\n            svd_tol: Threshold for singular values of the covariance matrix of Z.\n        \"\"\"\n        super().__init__()\n\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n\n        self.shrinkage = shrinkage\n\n        assert svd_tol > 0.0, \"`svd_tol` must be positive for numerical stability.\"\n        self.svd_tol = svd_tol\n\n        self.mean_x = torch.zeros(x_dim, device=device, dtype=dtype)\n        self.mean_z = torch.zeros(z_dim, device=device, dtype=dtype)\n\n        self.n = torch.tensor(0, device=device, dtype=dtype)\n        self.sigma_xz_ = torch.zeros(x_dim, z_dim, device=device, dtype=dtype)\n        self.sigma_zz_ = torch.zeros(z_dim, z_dim, device=device, dtype=dtype)\n\n    @torch.no_grad()\n    @invalidates_cache(\"eraser\")\n    def update(self, x: Tensor, z: Tensor) -> \"OracleFitter\":\n        \"\"\"Update the running statistics with a new batch of data.\"\"\"\n        d, c = self.sigma_xz_.shape\n        x = x.reshape(-1, d).type_as(self.mean_x)\n        n, d2 = x.shape\n\n        assert d == d2, f\"Unexpected number of features {d2}\"\n        self.n += n\n\n        # Welford's online algorithm\n        delta_x = x - self.mean_x\n        self.mean_x += delta_x.sum(dim=0) / self.n\n\n        z = z.reshape(n, -1).type_as(x)\n        assert z.shape[-1] == c, f\"Unexpected number of classes {z.shape[-1]}\"\n\n        delta_z = z - self.mean_z\n        self.mean_z += delta_z.sum(dim=0) / self.n\n        delta_z2 = z - self.mean_z\n\n        self.sigma_xz_.addmm_(delta_x.mT, delta_z2)\n        self.sigma_zz_.addmm_(delta_z.mT, delta_z2)\n\n        return self\n\n    @cached_property\n    def eraser(self) -> OracleEraser:\n        \"\"\"Erasure function lazily computed given the current statistics.\"\"\"\n        return OracleEraser(\n            self.sigma_xz @ torch.linalg.pinv(self.sigma_zz, atol=self.svd_tol),\n            self.mean_z,\n        )\n\n    @property\n    def sigma_zz(self) -> Tensor:\n        \"\"\"The covariance matrix of Z.\"\"\"\n        assert self.n > 1, \"Call update() before accessing sigma_xx\"\n        assert (\n            self.sigma_zz_ is not None\n        ), \"Covariance statistics are not being tracked for X\"\n\n        # Accumulated numerical error may cause this to be slightly non-symmetric\n        S_hat = (self.sigma_zz_ + self.sigma_zz_.mT) / 2\n\n        # Apply Random Matrix Theory-based shrinkage\n        if self.shrinkage:\n            return optimal_linear_shrinkage(S_hat / self.n, self.n)\n\n        # Just apply Bessel's correction\n        else:\n            return S_hat / (self.n - 1)\n\n    @property\n    def sigma_xz(self) -> Tensor:\n        \"\"\"The cross-covariance matrix.\"\"\"\n        assert self.n > 1, \"Call update() with labels before accessing sigma_xz\"\n        return self.sigma_xz_ / (self.n - 1)", ""]}
{"filename": "concept_erasure/optimal_transport.py", "chunked_list": ["import torch\nfrom torch import Tensor\n\nfrom .shrinkage import trace\n\n\ndef is_positive_definite(A: Tensor) -> Tensor:\n    \"\"\"Efficiently check if `A` is p.d. by attempting Cholesky decomposition.\"\"\"\n    return torch.linalg.cholesky_ex(A).info.eq(0)\n", "\n\n@torch.jit.script\ndef psd_sqrt(A: Tensor) -> Tensor:\n    \"\"\"Compute the unique p.s.d. square root of a positive semidefinite matrix.\"\"\"\n    L, U = torch.linalg.eigh(A)\n    L = L[..., None, :].clamp_min(0.0)\n    return U * L.sqrt() @ U.mT\n\n\ndef psd_sqrt_rsqrt(A: Tensor) -> tuple[Tensor, Tensor]:\n    \"\"\"Efficiently compute both the p.s.d. sqrt & pinv sqrt of p.s.d. matrix `A`.\"\"\"\n    L, U = torch.linalg.eigh(A)\n    L = L[..., None, :].clamp_min(0.0)\n\n    # Square root is easy\n    sqrt = U * L.sqrt() @ U.mT\n\n    # We actually compute the pseudo-inverse here for numerical stability.\n    # Use the same heuristic as `torch.linalg.pinv` to determine the tolerance.\n    thresh = L[..., None, -1] * A.shape[-1] * torch.finfo(A.dtype).eps\n    rsqrt = U * L.rsqrt().where(L > thresh, 0.0) @ U.mT\n\n    return sqrt, rsqrt", "\n\ndef psd_sqrt_rsqrt(A: Tensor) -> tuple[Tensor, Tensor]:\n    \"\"\"Efficiently compute both the p.s.d. sqrt & pinv sqrt of p.s.d. matrix `A`.\"\"\"\n    L, U = torch.linalg.eigh(A)\n    L = L[..., None, :].clamp_min(0.0)\n\n    # Square root is easy\n    sqrt = U * L.sqrt() @ U.mT\n\n    # We actually compute the pseudo-inverse here for numerical stability.\n    # Use the same heuristic as `torch.linalg.pinv` to determine the tolerance.\n    thresh = L[..., None, -1] * A.shape[-1] * torch.finfo(A.dtype).eps\n    rsqrt = U * L.rsqrt().where(L > thresh, 0.0) @ U.mT\n\n    return sqrt, rsqrt", "\n\ndef ot_barycenter(\n    Ks: Tensor, weights: Tensor | None = None, *, max_iter: int = 100\n) -> Tensor:\n    \"\"\"Fixed-point iteration for the 2-Wasserstein barycenter of a set of Gaussians.\n\n    Algorithm derived in \"A fixed-point approach to barycenters in Wasserstein space\"\n    by \u00c1lvarez-Esteban et al. (2016) <https://arxiv.org/abs/1511.05355>.\n\n    Args:\n        Ks: `[n, d, d]` batch of covariance matrices, one for each centered Gaussian\n            in the set whose barycenter we want to compute.\n        weights: `[n]` batch of weights for each Gaussian.\n\n    Returns:\n        Covariance matrix of the barycenter.\n    \"\"\"\n    n = len(Ks)\n    assert n > 1, \"Need at least two Gaussians to compute a barycenter\"\n\n    # Uniform weights by default\n    if weights is None:\n        weights = Ks.new_ones(n) / n\n    else:\n        assert len(weights) == n, \"Need one weight per Gaussian\"\n        weights = weights / weights.sum()\n\n    # Bookkeeping variables\n    loss = torch.inf\n    tol = torch.finfo(Ks.dtype).eps\n    weights = weights.view(-1, 1, 1)  # Broadcastable to Ks\n\n    # Initialize with arithmetic mean of covariance matrices\n    mu = Ks.mul(weights).sum(dim=0)\n    trace_avg = mu.trace()\n\n    # Begin \u00c1lvarez-Esteban et al. fixed-point iteration\n    for _ in range(max_iter):\n        sqrt_mu, rsqrt_mu = psd_sqrt_rsqrt(mu)\n        inner = psd_sqrt(sqrt_mu @ Ks @ sqrt_mu)\n\n        # Equation 15 from \u00c1lvarez-Esteban et al. (2016)\n        new_loss = mu.trace() + trace_avg - 2 * inner.mul(weights).sum(dim=0).trace()\n\n        # Break if the loss is not decreasing\n        if loss - new_loss < tol:\n            break\n        else:\n            loss = new_loss\n\n        # Equation 7 from \u00c1lvarez-Esteban et al. (2016)\n        T = torch.sum(weights * rsqrt_mu @ inner @ rsqrt_mu, dim=0)\n        mu = T @ mu @ T.mT\n\n    return mu", "\n\ndef ot_distance(K1: Tensor, K2: Tensor) -> Tensor:\n    \"\"\"2-Wasserstein distance between N(0, K1) and N(0, K2).\"\"\"\n    sqrt_K1 = psd_sqrt(K1)\n    inner = psd_sqrt(sqrt_K1 @ K2 @ sqrt_K1)\n\n    # Compute the 2-Wasserstein distance\n    dist = torch.sqrt(trace(K1) + trace(K2) - 2 * trace(inner))\n    return dist.squeeze(-1).squeeze(-1)", "\n\ndef ot_map(K1: Tensor, K2: Tensor) -> Tensor:\n    \"\"\"Optimal transport map from N(0, K1) to N(0, K2) in matrix form.\n\n    Args:\n        K1: Covariance matrix of the first Gaussian.\n        K2: Covariance matrix of the second Gaussian.\n\n    Returns:\n        Unique p.s.d. matrix A such that N(0, A @ K1 @ A.T) = N(0, K2).\n    \"\"\"\n    sqrt_K1, rsqrt_K1 = psd_sqrt_rsqrt(K1)\n    return rsqrt_K1 @ psd_sqrt(sqrt_K1 @ K2 @ sqrt_K1) @ rsqrt_K1", "\n\ndef ot_midpoint(K1: Tensor, K2: Tensor, w1: float = 0.5, w2: float = 0.5) -> Tensor:\n    \"\"\"Covariance matrix of the 2-Wasserstein barycenter of N(0, K1) and N(0, K2).\n\n    The barycenter of a set of distributions S is the unique distribution mu which\n    minimizes the mean squared Wasserstein distance from each distribution in S to mu.\n\n    Derived in \"On Gaussian Wasserstein Barycenters\" (Wessel Bruinsma & Gabriel Arpino)\n    <https://gabrielarpino.github.io/files/wasserstein.pdf>.\n\n    Args:\n        K1: Covariance matrix of the first Gaussian.\n        K2: Covariance matrix of the second Gaussian.\n        w1: Weight of the first Gaussian.\n        w2: Weight of the second Gaussian.\n\n    Returns:\n        Covariance matrix of the barycenter.\n    \"\"\"\n    sqrt_K1, rsqrt_K1 = psd_sqrt_rsqrt(K1)\n    product = sqrt_K1 @ psd_sqrt(sqrt_K1 @ K2 @ sqrt_K1) @ rsqrt_K1\n\n    return w1 * w1 * K1 + w2 * w2 * K2 + w1 * w2 * (product + product.T)", ""]}
{"filename": "concept_erasure/groupby.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Callable, Iterator\n\nimport torch\nfrom torch import LongTensor, Tensor\n\n\n@dataclass(frozen=True)\nclass GroupedTensor:\n    \"\"\"A tensor split into groups along a given dimension.\n\n    This class contains all the information needed to reconstruct the original tensor,\n    or to take a list of tensors derived from the groups and coalesce them in such a\n    way that the original order is restored.\n    \"\"\"\n\n    dim: int\n    \"\"\"Dimension along which the tensor was split.\"\"\"\n\n    groups: list[Tensor]\n    \"\"\"List of tensors such that `groups[i]` contains all elements of `x` whose group\n    label is `labels[i]`.\"\"\"\n\n    indices: LongTensor\n    \"\"\"Indices used to sort the original tensor.\"\"\"\n\n    labels: list[int]\n    \"\"\"Unique label for each element of `groups`.\"\"\"\n\n    def coalesce(self, groups: list[Tensor] | None = None) -> Tensor:\n        \"\"\"Fuse `groups or self.groups` back together, restoring the original order.\n\n        This method is most useful when you want to group a tensor, perform an operation\n        on each group, then combine the results back together.\n        \"\"\"\n        if groups is None:\n            groups = self.groups\n\n        # First concatenate the groups back together\n        fused = torch.cat(groups, dim=self.dim)\n\n        # Invert the permutation to restore the original order\n        return fused.index_select(self.dim, invert_indices(self.indices))\n\n    def map(self, fn: Callable[[int, Tensor], Tensor]) -> \"GroupedTensor\":\n        \"\"\"Apply `fn` to each group & return a new `GroupedTensor` with the results.\"\"\"\n        results = [fn(label, group) for label, group in zip(self.labels, self.groups)]\n        return GroupedTensor(self.dim, results, self.indices, self.labels)\n\n    def __iter__(self) -> Iterator[tuple[int, Tensor]]:\n        \"\"\"Iterate over the groups and their labels.\"\"\"\n        for label, group in zip(self.labels, self.groups):\n            yield label, group", "class GroupedTensor:\n    \"\"\"A tensor split into groups along a given dimension.\n\n    This class contains all the information needed to reconstruct the original tensor,\n    or to take a list of tensors derived from the groups and coalesce them in such a\n    way that the original order is restored.\n    \"\"\"\n\n    dim: int\n    \"\"\"Dimension along which the tensor was split.\"\"\"\n\n    groups: list[Tensor]\n    \"\"\"List of tensors such that `groups[i]` contains all elements of `x` whose group\n    label is `labels[i]`.\"\"\"\n\n    indices: LongTensor\n    \"\"\"Indices used to sort the original tensor.\"\"\"\n\n    labels: list[int]\n    \"\"\"Unique label for each element of `groups`.\"\"\"\n\n    def coalesce(self, groups: list[Tensor] | None = None) -> Tensor:\n        \"\"\"Fuse `groups or self.groups` back together, restoring the original order.\n\n        This method is most useful when you want to group a tensor, perform an operation\n        on each group, then combine the results back together.\n        \"\"\"\n        if groups is None:\n            groups = self.groups\n\n        # First concatenate the groups back together\n        fused = torch.cat(groups, dim=self.dim)\n\n        # Invert the permutation to restore the original order\n        return fused.index_select(self.dim, invert_indices(self.indices))\n\n    def map(self, fn: Callable[[int, Tensor], Tensor]) -> \"GroupedTensor\":\n        \"\"\"Apply `fn` to each group & return a new `GroupedTensor` with the results.\"\"\"\n        results = [fn(label, group) for label, group in zip(self.labels, self.groups)]\n        return GroupedTensor(self.dim, results, self.indices, self.labels)\n\n    def __iter__(self) -> Iterator[tuple[int, Tensor]]:\n        \"\"\"Iterate over the groups and their labels.\"\"\"\n        for label, group in zip(self.labels, self.groups):\n            yield label, group", "\n\ndef groupby(\n    x: Tensor, key: Tensor, dim: int = 0, *, stable: bool = False\n) -> GroupedTensor:\n    \"\"\"Efficiently split `x` into groups along `dim` according to `key`.\n\n    This function is intended to mimic the behavior of `itertools.groupby`, but for\n    PyTorch tensors. Under the hood, we sort `x` by `key` once, then return views\n    onto the sorted tensor in order to minimize the number of memcpy and equality\n    checking operations performed.\n\n    By necessity this operation performs a host-device sync since we need to know\n    the number of groups and their sizes in order to create a view for each.\n\n    Args:\n        x: Tensor to split into groups.\n        key: Tensor of group labels.\n        dim: Dimension along which to split `x`.\n        stable: If `True`, use a stable sorting algorithm. This is slower but ensures\n            that the order of elements within each group is preserved.\n\n    Returns:\n        A `GroupedTensor` containing the groups, sorting indices, and labels.\n    \"\"\"\n    assert key.dtype == torch.int64, \"`key` must be int64\"\n    assert key.ndim == 1, \"`key` must be 1D\"\n\n    key, indices = key.sort(stable=stable)\n    labels, counts = key.unique_consecutive(return_counts=True)\n\n    # Sort `x` by `key` along `dim`\n    x = x.index_select(dim, indices)\n    groups = x.split(counts.tolist(), dim=dim)\n\n    return GroupedTensor(dim, groups, indices, labels.tolist())", "\n\n@torch.jit.script\ndef invert_indices(indices: Tensor) -> Tensor:\n    \"\"\"Efficiently invert the permutation represented by `indices`.\n\n    Example:\n        >>> indices = torch.tensor([2, 0, 1])\n        >>> invert_indices(indices)\n        tensor([1, 2, 0])\n    \"\"\"\n    # Create an empty tensor to hold the reverse permutation\n    reverse_indices = torch.empty_like(indices)\n\n    # Scatter the indices to reverse the permutation\n    reverse_indices.scatter_(0, indices, torch.arange(len(indices)))\n\n    return reverse_indices", ""]}
{"filename": "concept_erasure/concept_scrubber.py", "chunked_list": ["from contextlib import contextmanager\nfrom functools import partial\nfrom typing import Callable\n\nfrom torch import Tensor, nn\n\nfrom .leace import LeaceEraser\nfrom .utils import assert_type, is_norm_layer, mangle_module_path\n\n\nclass ConceptScrubber:\n    \"\"\"Wrapper for a dictionary mapping module paths to `LeaceEraser` objects.\"\"\"\n\n    def __init__(self, pre_hook: bool = False):\n        super().__init__()\n\n        self.erasers: dict[str, LeaceEraser] = {}\n        self.pre_hook = pre_hook\n\n    @contextmanager\n    def scrub(self, model):\n        \"\"\"Add hooks to the model which apply the erasers during a forward pass.\"\"\"\n\n        def scrub_hook(key: str, x: Tensor):\n            eraser = assert_type(LeaceEraser, self.erasers[key])\n            return eraser(x).type_as(x)\n\n        with self.apply_hook(model, scrub_hook):\n            yield self\n\n    @contextmanager\n    def apply_hook(\n        self,\n        model: nn.Module,\n        hook_fn: Callable[[str, Tensor], Tensor | None],\n    ):\n        \"\"\"Apply a `hook_fn` to each submodule in `model` that we're scrubbing.\"\"\"\n\n        def post_wrapper(_, __, output, name: str) -> Tensor | None:\n            key = mangle_module_path(name)\n            return hook_fn(key, output)\n\n        def pre_wrapper(_, inputs, name: str) -> tuple[Tensor | None, ...]:\n            x, *extras = inputs\n            key = mangle_module_path(name)\n            return hook_fn(key, x), *extras\n\n        # Unwrap the base model if necessary. This is needed to ensure we don't try to\n        # scrub right before the unembedding layer\n        from transformers import PreTrainedModel\n\n        if isinstance(model, PreTrainedModel):\n            model = assert_type(PreTrainedModel, model.base_model)\n\n        handles = [\n            (\n                mod.register_forward_pre_hook(partial(pre_wrapper, name=name))\n                if self.pre_hook\n                else mod.register_forward_hook(partial(post_wrapper, name=name))\n            )\n            for name, mod in model.named_modules()\n            if is_norm_layer(mod) and mangle_module_path(name) in self.erasers\n        ]\n        assert len(handles) == len(self.erasers), \"Not all erasers can be applied\"\n\n        try:\n            yield self\n        finally:\n            # Make sure to remove the hooks even if an exception is raised\n            for handle in handles:\n                handle.remove()", "\n\nclass ConceptScrubber:\n    \"\"\"Wrapper for a dictionary mapping module paths to `LeaceEraser` objects.\"\"\"\n\n    def __init__(self, pre_hook: bool = False):\n        super().__init__()\n\n        self.erasers: dict[str, LeaceEraser] = {}\n        self.pre_hook = pre_hook\n\n    @contextmanager\n    def scrub(self, model):\n        \"\"\"Add hooks to the model which apply the erasers during a forward pass.\"\"\"\n\n        def scrub_hook(key: str, x: Tensor):\n            eraser = assert_type(LeaceEraser, self.erasers[key])\n            return eraser(x).type_as(x)\n\n        with self.apply_hook(model, scrub_hook):\n            yield self\n\n    @contextmanager\n    def apply_hook(\n        self,\n        model: nn.Module,\n        hook_fn: Callable[[str, Tensor], Tensor | None],\n    ):\n        \"\"\"Apply a `hook_fn` to each submodule in `model` that we're scrubbing.\"\"\"\n\n        def post_wrapper(_, __, output, name: str) -> Tensor | None:\n            key = mangle_module_path(name)\n            return hook_fn(key, output)\n\n        def pre_wrapper(_, inputs, name: str) -> tuple[Tensor | None, ...]:\n            x, *extras = inputs\n            key = mangle_module_path(name)\n            return hook_fn(key, x), *extras\n\n        # Unwrap the base model if necessary. This is needed to ensure we don't try to\n        # scrub right before the unembedding layer\n        from transformers import PreTrainedModel\n\n        if isinstance(model, PreTrainedModel):\n            model = assert_type(PreTrainedModel, model.base_model)\n\n        handles = [\n            (\n                mod.register_forward_pre_hook(partial(pre_wrapper, name=name))\n                if self.pre_hook\n                else mod.register_forward_hook(partial(post_wrapper, name=name))\n            )\n            for name, mod in model.named_modules()\n            if is_norm_layer(mod) and mangle_module_path(name) in self.erasers\n        ]\n        assert len(handles) == len(self.erasers), \"Not all erasers can be applied\"\n\n        try:\n            yield self\n        finally:\n            # Make sure to remove the hooks even if an exception is raised\n            for handle in handles:\n                handle.remove()", ""]}
{"filename": "concept_erasure/__init__.py", "chunked_list": ["from .concept_scrubber import ConceptScrubber\nfrom .groupby import GroupedTensor, groupby\nfrom .leace import ErasureMethod, LeaceEraser, LeaceFitter\nfrom .oracle import OracleEraser, OracleFitter\nfrom .quadratic import QuadraticEraser, QuadraticFitter\nfrom .shrinkage import optimal_linear_shrinkage\nfrom .utils import assert_type\n\n__all__ = [\n    \"assert_type\",", "__all__ = [\n    \"assert_type\",\n    \"groupby\",\n    \"optimal_linear_shrinkage\",\n    \"ConceptScrubber\",\n    \"ErasureMethod\",\n    \"GroupedTensor\",\n    \"LeaceEraser\",\n    \"LeaceFitter\",\n    \"OracleEraser\",", "    \"LeaceFitter\",\n    \"OracleEraser\",\n    \"OracleFitter\",\n    \"QuadraticEraser\",\n    \"QuadraticFitter\",\n]\n"]}
{"filename": "concept_erasure/utils.py", "chunked_list": ["from typing import Any, Type, TypeVar, cast\n\nfrom torch import nn\n\nT = TypeVar(\"T\")\n\n\ndef assert_type(typ: Type[T], obj: Any) -> T:\n    \"\"\"Assert that an object is of a given type at runtime and return it.\"\"\"\n    if not isinstance(obj, typ):\n        raise TypeError(f\"Expected {typ.__name__}, got {type(obj).__name__}\")\n\n    return cast(typ, obj)", "\n\ndef is_norm_layer(module: nn.Module) -> bool:\n    \"\"\"Return `True` if the module is a normalization layer.\"\"\"\n    cls_name = type(module).__name__\n    return cls_name.endswith(\"LayerNorm\") or cls_name.endswith(\"RMSNorm\")\n\n\ndef mangle_module_path(name: str) -> str:\n    \"\"\"Mangle a module path to make it a valid key in a `nn.ModuleDict`.\"\"\"\n    # This is a weird edge case we probably don't need to support\n    assert \"-\" not in name, \"Module path cannot contain `-`\"\n    return name.replace(\".\", \"-\")", "def mangle_module_path(name: str) -> str:\n    \"\"\"Mangle a module path to make it a valid key in a `nn.ModuleDict`.\"\"\"\n    # This is a weird edge case we probably don't need to support\n    assert \"-\" not in name, \"Module path cannot contain `-`\"\n    return name.replace(\".\", \"-\")\n"]}
{"filename": "concept_erasure/shrinkage.py", "chunked_list": ["import torch\nfrom torch import Tensor\n\n\ndef optimal_linear_shrinkage(S_n: Tensor, n: int | Tensor) -> Tensor:\n    \"\"\"Optimal linear shrinkage for a sample covariance matrix or batch thereof.\n\n    Given a sample covariance matrix `S_n` of shape (*, p, p) and a sample size `n`,\n    this function computes the optimal shrinkage coefficients `alpha` and `beta`, then\n    returns the covariance estimate `alpha * S_n + beta * Sigma0`, where ``Sigma0` is\n    an isotropic covariance matrix with the same trace as `S_n`.\n\n    The formula is distribution-free and asymptotically optimal in the Frobenius norm\n    among all linear shrinkage estimators as the dimensionality `p` and sample size `n`\n    jointly tend to infinity, with the ratio `p / n` converging to a finite positive\n    constant `c`. The derivation is based on Random Matrix Theory and assumes that the\n    underlying distribution has finite moments up to 4 + eps, for some eps > 0.\n\n    See \"On the Strong Convergence of the Optimal Linear Shrinkage Estimator for Large\n    Dimensional Covariance Matrix\" <https://arxiv.org/abs/1308.2608> for details.\n\n    Args:\n        S_n: Sample covariance matrices of shape (*, p, p).\n        n: Sample size.\n    \"\"\"\n    p = S_n.shape[-1]\n    assert S_n.shape[-2:] == (p, p)\n\n    # TODO: Make this configurable, try using diag(S_n) or something\n    eye = torch.eye(p, dtype=S_n.dtype, device=S_n.device).expand_as(S_n)\n    trace_S = trace(S_n)\n    sigma0 = eye * trace_S / p\n\n    sigma0_norm_sq = sigma0.pow(2).sum(dim=(-2, -1), keepdim=True)\n    S_norm_sq = S_n.pow(2).sum(dim=(-2, -1), keepdim=True)\n\n    prod_trace = trace(S_n @ sigma0)\n    top = trace_S.pow(2) * sigma0_norm_sq / n\n    bottom = S_norm_sq * sigma0_norm_sq - prod_trace**2\n\n    # Epsilon prevents dividing by zero for the zero matrix. In that case we end up\n    # setting alpha = 0, beta = 1, but it doesn't matter since we're shrinking toward\n    # tr(0)*I = 0, so it's a no-op.\n    eps = torch.finfo(S_n.dtype).eps\n    alpha = 1 - (top + eps) / (bottom + eps)\n    beta = (1 - alpha) * (prod_trace + eps) / (sigma0_norm_sq + eps)\n\n    return alpha * S_n + beta * sigma0", "\n\ndef trace(matrices: Tensor) -> Tensor:\n    \"\"\"Version of `torch.trace` that works for batches of matrices.\"\"\"\n    diag = torch.linalg.diagonal(matrices)\n    return diag.sum(dim=-1, keepdim=True).unsqueeze(-1)\n"]}
{"filename": "concept_erasure/leace.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Literal\n\nimport torch\nfrom torch import Tensor\n\nfrom .caching import cached_property, invalidates_cache\nfrom .shrinkage import optimal_linear_shrinkage\n\nErasureMethod = Literal[\"leace\", \"orth\"]", "\nErasureMethod = Literal[\"leace\", \"orth\"]\n\n\n@dataclass(frozen=True)\nclass LeaceEraser:\n    \"\"\"LEACE eraser that surgically erases a concept from a representation.\n\n    Since the LEACE projection matrix is guaranteed to be a rank k - 1 perturbation of\n    the identity, we store it implicitly in the d x k matrices `proj_left` and\n    `proj_right`. The full matrix is given by `torch.eye(d) - proj_left @ proj_right`.\n    \"\"\"\n\n    proj_left: Tensor\n    proj_right: Tensor\n    bias: Tensor | None\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"LeaceEraser\":\n        \"\"\"Convenience method to fit a LeaceEraser on data and return it.\"\"\"\n        return LeaceFitter.fit(x, z, **kwargs).eraser\n\n    @property\n    def P(self) -> Tensor:\n        \"\"\"The projection matrix.\"\"\"\n        eye = torch.eye(\n            self.proj_left.shape[0],\n            device=self.proj_left.device,\n            dtype=self.proj_left.dtype,\n        )\n        return eye - self.proj_left @ self.proj_right\n\n    def __call__(self, x: Tensor) -> Tensor:\n        \"\"\"Apply the projection to the input tensor.\"\"\"\n        delta = x - self.bias if self.bias is not None else x\n\n        # Ensure we do the matmul in the most efficient order.\n        x_ = x - (delta @ self.proj_right.T) @ self.proj_left.T\n        return x_.type_as(x)", "\n\nclass LeaceFitter:\n    \"\"\"Fits an affine transform that surgically erases a concept from a representation.\n\n    This class implements Least-squares Concept Erasure (LEACE) from\n    https://arxiv.org/abs/2306.03819. You can also use a slightly simpler orthogonal\n    projection-based method by setting `method=\"orth\"`.\n\n    This class stores all the covariance statistics needed to compute the LEACE eraser.\n    This allows the statistics to be updated incrementally with `update()`.\n    \"\"\"\n\n    mean_x: Tensor\n    \"\"\"Running mean of X.\"\"\"\n\n    mean_z: Tensor\n    \"\"\"Running mean of Z.\"\"\"\n\n    sigma_xz_: Tensor\n    \"\"\"Unnormalized cross-covariance matrix X^T Z.\"\"\"\n\n    sigma_xx_: Tensor | None\n    \"\"\"Unnormalized covariance matrix X^T X.\"\"\"\n\n    n: Tensor\n    \"\"\"Number of X samples seen so far.\"\"\"\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"LeaceFitter\":\n        \"\"\"Convenience method to fit a LeaceFitter on data and return it.\"\"\"\n        n, d = x.shape\n        _, k = z.reshape(n, -1).shape\n\n        fitter = LeaceFitter(d, k, device=x.device, dtype=x.dtype, **kwargs)\n        return fitter.update(x, z)\n\n    def __init__(\n        self,\n        x_dim: int,\n        z_dim: int,\n        method: ErasureMethod = \"leace\",\n        *,\n        affine: bool = True,\n        constrain_cov_trace: bool = True,\n        device: str | torch.device | None = None,\n        dtype: torch.dtype | None = None,\n        shrinkage: bool = True,\n        svd_tol: float = 0.01,\n    ):\n        \"\"\"Initialize a `LeaceFitter`.\n\n        Args:\n            x_dim: Dimensionality of the representation.\n            z_dim: Dimensionality of the concept.\n            method: Type of projection matrix to use.\n            affine: Whether to use a bias term to ensure the unconditional mean of the\n                features remains the same after erasure.\n            constrain_cov_trace: Whether to constrain the trace of the covariance of X\n                after erasure to be no greater than before erasure. This is especially\n                useful when injecting the scrubbed features back into a model. Without\n                this constraint, the norm of the model's hidden states may diverge in\n                some cases.\n            device: Device to put the statistics on.\n            dtype: Data type to use for the statistics.\n            shrinkage: Whether to use shrinkage to estimate the covariance matrix of X.\n            svd_tol: Singular values under this threshold are truncated, both during\n                the phase where we do SVD on the cross-covariance matrix, and at the\n                phase where we compute the pseudoinverse of the projected covariance\n                matrix. Higher values are more numerically stable and result in less\n                damage to the representation, but may leave trace correlations intact.\n        \"\"\"\n        super().__init__()\n\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n\n        self.affine = affine\n        self.constrain_cov_trace = constrain_cov_trace\n        self.method = method\n        self.shrinkage = shrinkage\n\n        assert svd_tol > 0.0, \"`svd_tol` must be positive for numerical stability.\"\n        self.svd_tol = svd_tol\n\n        self.mean_x = torch.zeros(x_dim, device=device, dtype=dtype)\n        self.mean_z = torch.zeros(z_dim, device=device, dtype=dtype)\n\n        self.n = torch.tensor(0, device=device, dtype=dtype)\n        self.sigma_xz_ = torch.zeros(x_dim, z_dim, device=device, dtype=dtype)\n\n        if self.method == \"leace\":\n            self.sigma_xx_ = torch.zeros(x_dim, x_dim, device=device, dtype=dtype)\n        elif self.method == \"orth\":\n            self.sigma_xx_ = None\n        else:\n            raise ValueError(f\"Unknown projection type {self.method}\")\n\n    @torch.no_grad()\n    @invalidates_cache(\"eraser\")\n    def update(self, x: Tensor, z: Tensor) -> \"LeaceFitter\":\n        \"\"\"Update the running statistics with a new batch of data.\"\"\"\n        d, c = self.sigma_xz_.shape\n        x = x.reshape(-1, d).type_as(self.mean_x)\n        n, d2 = x.shape\n\n        assert d == d2, f\"Unexpected number of features {d2}\"\n        self.n += n\n\n        # Welford's online algorithm\n        delta_x = x - self.mean_x\n        self.mean_x += delta_x.sum(dim=0) / self.n\n        delta_x2 = x - self.mean_x\n\n        # Update the covariance matrix of X if needed (for LEACE)\n        if self.method == \"leace\":\n            assert self.sigma_xx_ is not None\n            self.sigma_xx_.addmm_(delta_x.mT, delta_x2)\n\n        z = z.reshape(n, -1).type_as(x)\n        assert z.shape[-1] == c, f\"Unexpected number of classes {z.shape[-1]}\"\n\n        delta_z = z - self.mean_z\n        self.mean_z += delta_z.sum(dim=0) / self.n\n        delta_z2 = z - self.mean_z\n\n        # Update the cross-covariance matrix\n        self.sigma_xz_.addmm_(delta_x.mT, delta_z2)\n\n        return self\n\n    @cached_property\n    def eraser(self) -> LeaceEraser:\n        \"\"\"Erasure function lazily computed given the current statistics.\"\"\"\n        eye = torch.eye(self.x_dim, device=self.mean_x.device, dtype=self.mean_x.dtype)\n\n        # Compute the whitening and unwhitening matrices\n        if self.method == \"leace\":\n            sigma = self.sigma_xx\n            L, V = torch.linalg.eigh(sigma)\n\n            # Threshold used by torch.linalg.pinv\n            mask = L > (L[-1] * sigma.shape[-1] * torch.finfo(L.dtype).eps)\n\n            # Assuming PSD; account for numerical error\n            L.clamp_min_(0.0)\n\n            W = V * L.rsqrt().where(mask, 0.0) @ V.mT\n            W_inv = V * L.sqrt().where(mask, 0.0) @ V.mT\n        else:\n            W, W_inv = eye, eye\n\n        u, s, _ = torch.linalg.svd(W @ self.sigma_xz, full_matrices=False)\n\n        # Throw away singular values that are too small\n        u *= s > self.svd_tol\n\n        proj_left = W_inv @ u\n        proj_right = u.T @ W\n\n        if self.constrain_cov_trace and self.method == \"leace\":\n            P = eye - proj_left @ proj_right\n\n            # Prevent the covariance trace from increasing\n            sigma = self.sigma_xx\n            old_trace = torch.trace(sigma)\n            new_trace = torch.trace(P @ sigma @ P.mT)\n\n            # If applying the projection matrix increases the variance, this might\n            # cause instability, especially when erasure is applied multiple times.\n            # We regularize toward the orthogonal projection matrix to avoid this.\n            if new_trace > old_trace:\n                Q = eye - u @ u.T\n\n                # Set up the variables for the quadratic equation\n                x = new_trace\n                y = 2 * torch.trace(P @ sigma @ Q.mT)\n                z = torch.trace(Q @ sigma @ Q.mT)\n                w = old_trace\n\n                # Solve for the mixture of P and Q that makes the trace equal to the\n                # trace of the original covariance matrix\n                discr = torch.sqrt(\n                    4 * w * x - 4 * w * y + 4 * w * z - 4 * x * z + y**2\n                )\n                alpha1 = (-y / 2 + z - discr / 2) / (x - y + z)\n                alpha2 = (-y / 2 + z + discr / 2) / (x - y + z)\n\n                # Choose the positive root\n                alpha = torch.where(alpha1 > 0, alpha1, alpha2).clamp(0, 1)\n                P = alpha * P + (1 - alpha) * Q\n\n                # TODO: Avoid using SVD here\n                u, s, vh = torch.linalg.svd(eye - P)\n                proj_left = u * s.sqrt()\n                proj_right = vh * s.sqrt()\n\n        return LeaceEraser(\n            proj_left, proj_right, bias=self.mean_x if self.affine else None\n        )\n\n    @property\n    def sigma_xx(self) -> Tensor:\n        \"\"\"The covariance matrix of X.\"\"\"\n        assert self.n > 1, \"Call update() before accessing sigma_xx\"\n        assert (\n            self.sigma_xx_ is not None\n        ), \"Covariance statistics are not being tracked for X\"\n\n        # Accumulated numerical error may cause this to be slightly non-symmetric\n        S_hat = (self.sigma_xx_ + self.sigma_xx_.mT) / 2\n\n        # Apply Random Matrix Theory-based shrinkage\n        if self.shrinkage:\n            return optimal_linear_shrinkage(S_hat / self.n, self.n)\n\n        # Just apply Bessel's correction\n        else:\n            return S_hat / (self.n - 1)\n\n    @property\n    def sigma_xz(self) -> Tensor:\n        \"\"\"The cross-covariance matrix.\"\"\"\n        assert self.n > 1, \"Call update() with labels before accessing sigma_xz\"\n        return self.sigma_xz_ / (self.n - 1)", ""]}
{"filename": "concept_erasure/quadratic.py", "chunked_list": ["from dataclasses import dataclass\n\nimport torch\nfrom torch import Tensor\nfrom torch.distributions import MultivariateNormal\n\nfrom .caching import cached_property, invalidates_cache\nfrom .groupby import groupby\nfrom .optimal_transport import ot_barycenter, ot_map, ot_midpoint\nfrom .shrinkage import optimal_linear_shrinkage", "from .optimal_transport import ot_barycenter, ot_map, ot_midpoint\nfrom .shrinkage import optimal_linear_shrinkage\n\n\n@dataclass(frozen=True)\nclass QuadraticEraser:\n    \"\"\"Performs surgical quadratic concept erasure given oracle concept labels.\"\"\"\n\n    class_means: Tensor\n    \"\"\"`[k, d]` batch of class centroids.\"\"\"\n\n    class_prior: Tensor\n    \"\"\"`[k]` prior probability of each class.\"\"\"\n\n    global_mean: Tensor\n    \"\"\"`[d]` global centroid of the dataset.\"\"\"\n\n    ot_maps: Tensor\n    \"\"\"`[k, d, d]` batch of optimal transport matrices to the concept barycenter.\"\"\"\n\n    scale_trils: Tensor | None = None\n    \"\"\"`[k, d, d]` batch of covariance Cholesky factors for each class.\"\"\"\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"QuadraticEraser\":\n        \"\"\"Convenience method to fit a QuadraticEraser on data and return it.\"\"\"\n        return QuadraticFitter.fit(x, z, **kwargs).eraser\n\n    def optimal_transport(self, z: int, x: Tensor) -> Tensor:\n        \"\"\"Transport `x` to the barycenter, assuming it was sampled from class `z`\"\"\"\n        return (x - self.class_means[z]) @ self.ot_maps[z].mT + self.global_mean\n\n    def predict(self, x: Tensor) -> Tensor:\n        \"\"\"Compute the log posterior p(z|x) for each class z.\"\"\"\n        assert self.scale_trils is not None, \"Set store_covariance=True for prediction\"\n\n        # Because we provide the Cholesky factor directly, the initialization is cheap\n        gaussian = MultivariateNormal(\n            loc=self.class_means, scale_tril=self.scale_trils, validate_args=False\n        )\n        # Bayes rule\n        log_prior = self.class_prior.log()\n        log_likelihood = gaussian.log_prob(x[:, None])\n        return torch.log_softmax(log_prior + log_likelihood, dim=-1)\n\n    def __call__(self, x: Tensor, z: Tensor | None = None) -> Tensor:\n        \"\"\"Apply erasure to `x` with oracle labels `z`.\n\n        If `z` is not provided, we will estimate it from `x` using `self.predict`. This\n        is only possible if `store_covariance=True` was passed to `QuadraticFitter`.\n        \"\"\"\n        if z is None:\n            assert self.scale_trils is not None, \"Set store_covariance=True\"\n            z = self.predict(x).argmax(-1)\n\n        # Efficiently group `x` by `z`, optimally transport each group, then coalesce\n        return groupby(x, z).map(self.optimal_transport).coalesce()", "\n\nclass QuadraticFitter:\n    \"\"\"Compute barycenter & optimal transport maps for a quadratic concept eraser.\"\"\"\n\n    mean_x: Tensor\n    \"\"\"Running mean of X.\"\"\"\n\n    mean_z: Tensor\n    \"\"\"Running mean of Z.\"\"\"\n\n    sigma_xx_: Tensor\n    \"\"\"Unnormalized class-conditional covariance matrices X^T X.\"\"\"\n\n    n: Tensor\n    \"\"\"Number of samples seen so far in each class.\"\"\"\n\n    @classmethod\n    def fit(cls, x: Tensor, z: Tensor, **kwargs) -> \"QuadraticFitter\":\n        \"\"\"Convenience method to fit a OracleFitter on data and return it.\"\"\"\n        d = x.shape[-1]\n        k = int(z.max()) + 1  # Number of classes\n\n        fitter = QuadraticFitter(d, k, device=x.device, dtype=x.dtype, **kwargs)\n        return fitter.update(x, z)\n\n    def __init__(\n        self,\n        x_dim: int,\n        num_classes: int,\n        *,\n        device: str | torch.device | None = None,\n        dtype: torch.dtype | None = None,\n        store_covariance: bool = False,\n        shrinkage: bool = True,\n    ):\n        \"\"\"Initialize a `QuadraticFitter`.\n\n        Args:\n            x_dim: Dimensionality of the representation.\n            num_classes: Number of distinct classes in the dataset.\n            device: Device to put the statistics on.\n            dtype: Data type to use for the statistics.\n            store_covariance: Whether to store the precision matrices for each class in\n                addition to the optimal transport maps in the eraser. This is necessary\n                if you want to use the eraser to predict concept labels, or attempt to\n                perform approximate erasure without oracle labels.\n            shrinkage: Whether to use shrinkage to estimate the covariance matrix of X.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.shrinkage = shrinkage\n        self.store_covariance = store_covariance\n\n        self.mean_x = torch.zeros(num_classes, x_dim, device=device, dtype=dtype)\n        self.n = torch.zeros(num_classes, device=device, dtype=torch.long)\n        self.sigma_xx_ = torch.zeros(\n            num_classes, x_dim, x_dim, device=device, dtype=dtype\n        )\n\n    def update(self, x: Tensor, z: Tensor) -> \"QuadraticFitter\":\n        \"\"\"Update the running statistics with a new batch of data.\"\"\"\n        x = x.flatten(0, -2).type_as(self.mean_x)\n\n        for label, group in groupby(x, z, dim=0):\n            self.update_single(group, label)\n\n        return self\n\n    @torch.no_grad()\n    @invalidates_cache(\"eraser\")\n    def update_single(self, x: Tensor, z: int) -> \"QuadraticFitter\":\n        \"\"\"Update the running statistics with `x`, all sampled from class `z`.\"\"\"\n        x = x.flatten(0, -2).type_as(self.mean_x)\n\n        self.n[z] += len(x)\n\n        # Welford's online algorithm\n        delta_x = x - self.mean_x[z]\n        self.mean_x[z] += delta_x.sum(dim=0) / self.n[z]\n        delta_x2 = x - self.mean_x[z]\n\n        self.sigma_xx_[z].addmm_(delta_x.mT, delta_x2)\n\n        return self\n\n    @cached_property\n    def eraser(self) -> QuadraticEraser:\n        \"\"\"Erasure function lazily computed given the current statistics.\"\"\"\n\n        class_prior = self.n / self.n.sum()\n        sigmas = self.sigma_xx\n\n        # Compute Wasserstein barycenter of the classes\n        if self.num_classes == 2:\n            # Use closed form solution for the binary case\n            center = ot_midpoint(sigmas[0], sigmas[1], *class_prior.tolist())\n        else:\n            # Use fixed point iteration for the general case\n            center = ot_barycenter(self.sigma_xx, self.n)\n\n        # Then compute the optimal ransport maps from each class to the barycenter\n        ot_maps = ot_map(sigmas, center)\n\n        if self.store_covariance:\n            # Add jitter to ensure positive-definiteness\n            torch.linalg.diagonal(sigmas).add_(1e-3)\n            scale_trils = torch.linalg.cholesky(sigmas)\n        else:\n            scale_trils = None\n\n        return QuadraticEraser(\n            self.mean_x, class_prior, self.mean_x.mean(dim=0), ot_maps, scale_trils\n        )\n\n    @property\n    def sigma_xx(self) -> Tensor:\n        \"\"\"Class-conditional covariance matrices of X.\"\"\"\n        assert torch.all(self.n > 1), \"Some classes have < 2 samples\"\n        assert (\n            self.sigma_xx_ is not None\n        ), \"Covariance statistics are not being tracked for X\"\n\n        # Accumulated numerical error may cause this to be slightly non-symmetric\n        S_hat = (self.sigma_xx_ + self.sigma_xx_.mT) / 2\n\n        # Apply Random Matrix Theory-based shrinkage\n        n = self.n.view(-1, 1, 1)\n        if self.shrinkage:\n            return optimal_linear_shrinkage(S_hat / n, n)\n\n        # Just apply Bessel's correction\n        else:\n            return S_hat / (n - 1)", ""]}
{"filename": "concept_erasure/caching.py", "chunked_list": ["from functools import wraps\nfrom typing import Callable\n\n\ndef cached_property(func: Callable) -> property:\n    \"\"\"Decorator that converts a method into a lazily-evaluated cached property\"\"\"\n    # Create a secret attribute name for the cached property\n    attr_name = \"_cached_\" + func.__name__\n\n    @property\n    @wraps(func)\n    def _cached_property(self):\n        # If the secret attribute doesn't exist, compute the property and set it\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, func(self))\n\n        # Otherwise, return the cached property\n        return getattr(self, attr_name)\n\n    return _cached_property", "\n\ndef invalidates_cache(dependent_prop_name: str) -> Callable:\n    \"\"\"Invalidates a cached property when the decorated function is called\"\"\"\n    attr_name = \"_cached_\" + dependent_prop_name\n\n    # The actual decorator\n    def _invalidates_cache(func: Callable) -> Callable:\n        # The wrapper function\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            # Check if the secret attribute exists; if so delete it so that\n            # the cached property is recomputed\n            if hasattr(self, attr_name):\n                delattr(self, attr_name)\n\n            return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return _invalidates_cache", ""]}
{"filename": "concept_erasure/scrubbing/neox.py", "chunked_list": ["from types import MethodType\n\nimport torch\nimport torch.nn.functional as F\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    GPTNeoXForCausalLM,\n    GPTNeoXLayer,\n    GPTNeoXModel,", "    GPTNeoXLayer,\n    GPTNeoXModel,\n)\nfrom transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXAttention\n\nfrom concept_erasure import ConceptScrubber, ErasureMethod, LeaceFitter\nfrom concept_erasure.utils import assert_type\n\n\ndef patch_attention_neox_(model: torch.nn.Module):\n    \"\"\"Patch the attention layers of a GPTNeoX model to use fast attention kernels.\"\"\"\n\n    def fast_attn(self, q, k, v, attention_mask=None, head_mask=None):\n        assert attention_mask is None, \"attention_mask not implemented for fast_attn\"\n        assert head_mask is None, \"head_mask not implemented for fast_attn\"\n\n        # Queries and keys are always float32 because of the rotary embeddings\n        q, k = q.type_as(v), k.type_as(v)\n        return F.scaled_dot_product_attention(q, k, v, is_causal=True), None\n\n    for mod in model.modules():\n        if isinstance(mod, GPTNeoXAttention):\n            mod._attn = MethodType(fast_attn, mod)", "\ndef patch_attention_neox_(model: torch.nn.Module):\n    \"\"\"Patch the attention layers of a GPTNeoX model to use fast attention kernels.\"\"\"\n\n    def fast_attn(self, q, k, v, attention_mask=None, head_mask=None):\n        assert attention_mask is None, \"attention_mask not implemented for fast_attn\"\n        assert head_mask is None, \"head_mask not implemented for fast_attn\"\n\n        # Queries and keys are always float32 because of the rotary embeddings\n        q, k = q.type_as(v), k.type_as(v)\n        return F.scaled_dot_product_attention(q, k, v, is_causal=True), None\n\n    for mod in model.modules():\n        if isinstance(mod, GPTNeoXAttention):\n            mod._attn = MethodType(fast_attn, mod)", "\n\n@torch.no_grad()\ndef scrub_neox(\n    model: GPTNeoXForCausalLM,\n    train: Dataset,\n    z_column: str | None,\n    batch_size: int = 32,\n    method: ErasureMethod = \"leace\",\n    affine: bool = True,\n) -> tuple[ConceptScrubber | None, float]:\n    base = assert_type(GPTNeoXModel, model.base_model)\n    d = assert_type(int, base.config.hidden_size)\n\n    # Put model in eval mode\n    model.eval()\n    model.requires_grad_(False)\n\n    if z_column is None:\n        k = -1\n        scrubber = None\n    else:\n        k = assert_type(int, train.features[z_column].feature.num_classes)\n        scrubber = ConceptScrubber()\n\n    losses = []\n    xs = []\n    zs = []\n    N = len(train) // batch_size\n    train = train.with_format(\"torch\", device=model.device)\n\n    # Embed all the inputs\n    embed_fn = base.get_input_embeddings()\n    for i, batch in enumerate(tqdm(train.iter(batch_size), desc=\"Embedding\", total=N)):\n        assert isinstance(batch, dict)\n\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n        x = embed_fn(tokens)\n        xs.append(x.to(\"cpu\", non_blocking=True))\n\n        # We don't actually need to move these to the CPU since they're small\n        if z_column is not None:\n            zs.append(F.one_hot(batch[z_column], num_classes=k))\n\n    # Enumerate the layers\n    for j, layer in enumerate(tqdm(base.layers, unit=\"layer\")):\n        assert isinstance(layer, GPTNeoXLayer)\n        assert layer.use_parallel_residual, \"Only parallel residual is supported\"\n\n        attn_eraser, mlp_eraser = None, None\n        if scrubber is not None:\n            attn_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n            scrubber.erasers[f\"layers-{j}-input_layernorm\"] = attn_fitter\n\n            mlp_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n            scrubber.erasers[f\"layers-{j}-post_attention_layernorm\"] = mlp_fitter\n\n            # Fit the next eraser on the previous hidden states\n            for i, (x, z) in tqdm(enumerate(zip(xs, zs)), desc=\"Fitting\", total=N):\n                x = x.to(model.device)\n\n                # Discard post-LN output and recompute during application to save RAM\n                attn_norm_out = layer.input_layernorm(x)\n                attn_fitter.update(attn_norm_out, z)\n\n                mlp_norm_out = layer.post_attention_layernorm(attn_norm_out)\n                mlp_fitter.update(mlp_norm_out, z)\n\n            attn_eraser, mlp_eraser = attn_fitter.eraser, mlp_fitter.eraser\n            del attn_fitter, mlp_fitter  # Save VRAM\n\n        # Run attention & MLP with the erasers we just fit\n        for i, x in tqdm(enumerate(xs), desc=\"Applying\", total=N):\n            x = x.to(model.device)\n            h = layer.input_layernorm(x)  # Recomputing from above\n\n            if attn_eraser is not None:\n                h = attn_eraser(h)\n\n            pos_ids = torch.arange(0, x.shape[-2], device=x.device, dtype=torch.long)\n            pos_ids = pos_ids.unsqueeze(0).view(-1, x.shape[-2])\n            attn_res, *_ = layer.attention(h, None, pos_ids)\n\n            # Parallel residual\n            h = layer.post_attention_layernorm(x)  # Recomputing from above\n            if mlp_eraser is not None:\n                h = mlp_eraser(h)\n\n            mlp_res = layer.mlp(h)\n            x = x + attn_res + mlp_res\n            xs[i] = x.to(\"cpu\", non_blocking=True)\n\n    for batch, x in tqdm(zip(train.iter(batch_size), xs), total=N):\n        assert isinstance(batch, dict)\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n\n        x = x.to(model.device)\n        x = base.final_layer_norm(x)\n        logits = model.embed_out(x)\n\n        labels = tokens.to(logits.device)\n        shift_logits = logits[:, :-1, :].contiguous()\n        labels = labels[:, 1:].contiguous()\n        lm_loss = F.cross_entropy(\n            shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)\n        )\n        losses.append(lm_loss)\n\n    return scrubber, torch.stack(losses).mean().item()", ""]}
{"filename": "concept_erasure/scrubbing/auto.py", "chunked_list": ["from warnings import warn\n\nimport torch.nn.functional as F\nfrom datasets import Dataset\nfrom transformers import GPTNeoXForCausalLM, LlamaForCausalLM, PreTrainedModel\n\nfrom ..concept_scrubber import ConceptScrubber\nfrom ..leace import ErasureMethod\nfrom .llama import patch_attention_llama_, scrub_llama\nfrom .neox import patch_attention_neox_, scrub_neox", "from .llama import patch_attention_llama_, scrub_llama\nfrom .neox import patch_attention_neox_, scrub_neox\n\n\ndef patch_attention_(model: PreTrainedModel):\n    \"\"\"Patch the attention layers of a model to use fast attention kernels.\"\"\"\n    ty = model.config.model_type\n    if not hasattr(F, \"scaled_dot_product_attention\"):\n        warn(\n            \"Fast, memory-efficient attention requires PyTorch >= 2.0.0. \"\n            \"For best performance, please upgrade your version of PyTorch.\"\n        )\n        return\n\n    if ty == \"gpt_neox\":\n        patch_attention_neox_(model)\n    elif ty == \"llama\":\n        patch_attention_llama_(model)\n    else:\n        raise NotImplementedError(f\"Unsupported model type: {type(model)}\")", "\n\ndef scrub(\n    model: PreTrainedModel,\n    train: Dataset,\n    z_column: str | None,\n    affine: bool = True,\n    batch_size: int = 1,\n    method: ErasureMethod = \"leace\",\n    sublayers: bool = True,\n) -> tuple[ConceptScrubber | None, float]:\n    \"\"\"Apply concept scrubbing to `model` on dataset `train`, returning the scrubber.\"\"\"\n    if isinstance(model, GPTNeoXForCausalLM):\n        return scrub_neox(model, train, z_column, batch_size, method, affine)\n    elif isinstance(model, LlamaForCausalLM):\n        return scrub_llama(\n            model, train, z_column, batch_size, method, sublayers, affine\n        )\n    else:\n        raise NotImplementedError(f\"Unsupported model type: {type(model).__name__}\")", ""]}
{"filename": "concept_erasure/scrubbing/llama.py", "chunked_list": ["from types import MethodType\n\nimport torch\nimport torch.nn.functional as F\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    LlamaForCausalLM,\n    LlamaModel,\n)", "    LlamaModel,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaAttention,\n    LlamaDecoderLayer,\n    apply_rotary_pos_emb,\n)\n\nfrom concept_erasure import ConceptScrubber, ErasureMethod, LeaceFitter\nfrom concept_erasure.utils import assert_type", "from concept_erasure import ConceptScrubber, ErasureMethod, LeaceFitter\nfrom concept_erasure.utils import assert_type\n\n\ndef _fast_attn(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: torch.Tensor | None = None,\n    position_ids: torch.LongTensor | None = None,\n    past_key_value: tuple[torch.Tensor, ...] | None = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor, ...] | None]:\n    \"\"\"Fast version of LLaMA attention.\"\"\"\n    assert not output_attentions, \"output_attentions not implemented for fast_attn\"\n    del attention_mask  # unused, but it's non-None in the base class\n\n    bsz, q_len, _ = hidden_states.size()\n    q = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    k = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    v = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = k.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(v, seq_len=kv_seq_len)\n    q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        k = torch.cat([past_key_value[0], k], dim=2)\n        v = torch.cat([past_key_value[1], v], dim=2)\n\n    past_key_value = (k, v) if use_cache else None\n    attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n\n    return attn_output, None, past_key_value", "\n\ndef patch_attention_llama_(model: torch.nn.Module):\n    \"\"\"Patch the attention layers of a LLaMA model to use fast attention kernels.\"\"\"\n    for mod in model.modules():\n        if isinstance(mod, LlamaAttention):\n            mod.forward = MethodType(_fast_attn, mod)\n\n\n@torch.no_grad()\ndef scrub_llama(\n    model: LlamaForCausalLM,\n    train: Dataset,\n    z_column: str | None,\n    batch_size: int = 1,\n    method: ErasureMethod = \"leace\",\n    sublayers: bool = True,\n    affine: bool = True,\n) -> tuple[ConceptScrubber | None, float]:\n    base = assert_type(LlamaModel, model.base_model)\n    d = assert_type(int, base.config.hidden_size)\n\n    if z_column is None:\n        k = -1\n        scrubber = None\n    else:\n        k = assert_type(int, train.features[z_column].feature.num_classes)\n        scrubber = ConceptScrubber()\n\n    losses = []\n\n    xs = []\n    zs = []\n    N = len(train) // batch_size\n    train = train.with_format(\"torch\", device=model.device)\n\n    # Embed all the inputs\n    embed_fn = base.get_input_embeddings()\n    for i, batch in enumerate(tqdm(train.iter(batch_size), desc=\"Embedding\", total=N)):\n        assert isinstance(batch, dict)\n\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n        x = embed_fn(tokens)\n        xs.append(x.to(\"cpu\", non_blocking=True))\n\n        # We don't actually need to move these to the CPU since they're small\n        if z_column is not None:\n            zs.append(F.one_hot(batch[z_column], num_classes=k))\n\n    # Enumerate the layers\n    for j, layer in enumerate(tqdm(base.layers, unit=\"layer\")):\n        assert isinstance(layer, LlamaDecoderLayer)\n\n        attn_eraser = None\n        if scrubber is not None:\n            attn_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n\n            # Fit the next eraser on the previous hidden states\n            for x, z in tqdm(zip(xs, zs), desc=\"Fitting (attn)\", total=N):\n                assert scrubber is not None\n\n                # Discard post-LN output and recompute during application to save RAM\n                x = layer.input_layernorm(x.to(model.device))\n                attn_fitter.update(x, z)\n\n            attn_eraser = attn_fitter.eraser\n            scrubber.erasers[f\"layers-{j}-input_layernorm\"] = attn_eraser\n            del attn_fitter  # Save VRAM\n\n        # Run attention & MLP with the erasers we just fit\n        for i, x in tqdm(enumerate(xs), desc=\"Applying (attn)\", total=N):\n            # Bring back to the accelerator\n            x = x.to(model.device)\n            h = layer.input_layernorm(x)  # Recomputing from above\n\n            # Apply the eraser\n            if attn_eraser is not None and scrubber is not None:\n                h = attn_eraser(h).type_as(h)\n\n            pos_ids = torch.arange(0, h.shape[-2], device=h.device, dtype=torch.long)\n            pos_ids = pos_ids.unsqueeze(0).view(-1, h.shape[-2])\n\n            h, _, __ = layer.self_attn(h, position_ids=pos_ids)\n            h = x = x + h  # Post-attention residual connection\n\n            # We're not scrubbing the sublayers, so run the rest of the layer\n            if not sublayers:\n                h = layer.post_attention_layernorm(h)\n                h = layer.mlp(h)\n                h = x + h  # Post-MLP residual connection\n\n            xs[i] = h.to(\"cpu\", non_blocking=True)\n\n        # Skip the part where we scrub the MLP if we're not doing that\n        if not sublayers:\n            continue\n\n        mlp_eraser = None\n        if scrubber is not None:\n            mlp_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n\n            # Fit the next eraser on the previous hidden states\n            for x, z in tqdm(zip(xs, zs), desc=\"Fitting (MLP)\", total=N):\n                assert scrubber is not None\n\n                # Discard post-LN output and recompute during application to save RAM\n                x = layer.post_attention_layernorm(x.to(model.device))\n                mlp_fitter.update(x, z)\n\n            mlp_eraser = mlp_fitter.eraser\n            scrubber.erasers[f\"layers-{j}-post_attention_layernorm\"] = mlp_eraser\n            del mlp_fitter  # Save VRAM\n\n        for i, x in tqdm(enumerate(xs), desc=\"Applying (MLP)\", total=N):\n            # Bring back to the accelerator\n            x = x.to(model.device)\n            h = layer.post_attention_layernorm(x)  # Recomputing from above\n\n            # Apply the eraser\n            if mlp_eraser is not None and scrubber is not None:\n                h = mlp_eraser.eraser(h).type_as(h)\n\n            h = layer.mlp(h)\n            h = x + h  # Post-MLP residual connection\n            xs[i] = h.to(\"cpu\", non_blocking=True)\n\n    for batch, x in tqdm(zip(train.iter(batch_size), xs), total=N):\n        assert isinstance(batch, dict)\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n\n        x = x.to(model.device)\n        x = base.norm(x)\n        logits = model.lm_head(x)\n\n        labels = tokens.to(logits.device)\n        shift_logits = logits[:, :-1, :].contiguous()\n        labels = labels[:, 1:].contiguous()\n        lm_loss = F.cross_entropy(\n            shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)\n        )\n        losses.append(lm_loss)\n\n    return scrubber, torch.stack(losses).mean().item()", "\n@torch.no_grad()\ndef scrub_llama(\n    model: LlamaForCausalLM,\n    train: Dataset,\n    z_column: str | None,\n    batch_size: int = 1,\n    method: ErasureMethod = \"leace\",\n    sublayers: bool = True,\n    affine: bool = True,\n) -> tuple[ConceptScrubber | None, float]:\n    base = assert_type(LlamaModel, model.base_model)\n    d = assert_type(int, base.config.hidden_size)\n\n    if z_column is None:\n        k = -1\n        scrubber = None\n    else:\n        k = assert_type(int, train.features[z_column].feature.num_classes)\n        scrubber = ConceptScrubber()\n\n    losses = []\n\n    xs = []\n    zs = []\n    N = len(train) // batch_size\n    train = train.with_format(\"torch\", device=model.device)\n\n    # Embed all the inputs\n    embed_fn = base.get_input_embeddings()\n    for i, batch in enumerate(tqdm(train.iter(batch_size), desc=\"Embedding\", total=N)):\n        assert isinstance(batch, dict)\n\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n        x = embed_fn(tokens)\n        xs.append(x.to(\"cpu\", non_blocking=True))\n\n        # We don't actually need to move these to the CPU since they're small\n        if z_column is not None:\n            zs.append(F.one_hot(batch[z_column], num_classes=k))\n\n    # Enumerate the layers\n    for j, layer in enumerate(tqdm(base.layers, unit=\"layer\")):\n        assert isinstance(layer, LlamaDecoderLayer)\n\n        attn_eraser = None\n        if scrubber is not None:\n            attn_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n\n            # Fit the next eraser on the previous hidden states\n            for x, z in tqdm(zip(xs, zs), desc=\"Fitting (attn)\", total=N):\n                assert scrubber is not None\n\n                # Discard post-LN output and recompute during application to save RAM\n                x = layer.input_layernorm(x.to(model.device))\n                attn_fitter.update(x, z)\n\n            attn_eraser = attn_fitter.eraser\n            scrubber.erasers[f\"layers-{j}-input_layernorm\"] = attn_eraser\n            del attn_fitter  # Save VRAM\n\n        # Run attention & MLP with the erasers we just fit\n        for i, x in tqdm(enumerate(xs), desc=\"Applying (attn)\", total=N):\n            # Bring back to the accelerator\n            x = x.to(model.device)\n            h = layer.input_layernorm(x)  # Recomputing from above\n\n            # Apply the eraser\n            if attn_eraser is not None and scrubber is not None:\n                h = attn_eraser(h).type_as(h)\n\n            pos_ids = torch.arange(0, h.shape[-2], device=h.device, dtype=torch.long)\n            pos_ids = pos_ids.unsqueeze(0).view(-1, h.shape[-2])\n\n            h, _, __ = layer.self_attn(h, position_ids=pos_ids)\n            h = x = x + h  # Post-attention residual connection\n\n            # We're not scrubbing the sublayers, so run the rest of the layer\n            if not sublayers:\n                h = layer.post_attention_layernorm(h)\n                h = layer.mlp(h)\n                h = x + h  # Post-MLP residual connection\n\n            xs[i] = h.to(\"cpu\", non_blocking=True)\n\n        # Skip the part where we scrub the MLP if we're not doing that\n        if not sublayers:\n            continue\n\n        mlp_eraser = None\n        if scrubber is not None:\n            mlp_fitter = LeaceFitter(\n                d, k, affine=affine, device=model.device, method=method\n            )\n\n            # Fit the next eraser on the previous hidden states\n            for x, z in tqdm(zip(xs, zs), desc=\"Fitting (MLP)\", total=N):\n                assert scrubber is not None\n\n                # Discard post-LN output and recompute during application to save RAM\n                x = layer.post_attention_layernorm(x.to(model.device))\n                mlp_fitter.update(x, z)\n\n            mlp_eraser = mlp_fitter.eraser\n            scrubber.erasers[f\"layers-{j}-post_attention_layernorm\"] = mlp_eraser\n            del mlp_fitter  # Save VRAM\n\n        for i, x in tqdm(enumerate(xs), desc=\"Applying (MLP)\", total=N):\n            # Bring back to the accelerator\n            x = x.to(model.device)\n            h = layer.post_attention_layernorm(x)  # Recomputing from above\n\n            # Apply the eraser\n            if mlp_eraser is not None and scrubber is not None:\n                h = mlp_eraser.eraser(h).type_as(h)\n\n            h = layer.mlp(h)\n            h = x + h  # Post-MLP residual connection\n            xs[i] = h.to(\"cpu\", non_blocking=True)\n\n    for batch, x in tqdm(zip(train.iter(batch_size), xs), total=N):\n        assert isinstance(batch, dict)\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n\n        x = x.to(model.device)\n        x = base.norm(x)\n        logits = model.lm_head(x)\n\n        labels = tokens.to(logits.device)\n        shift_logits = logits[:, :-1, :].contiguous()\n        labels = labels[:, 1:].contiguous()\n        lm_loss = F.cross_entropy(\n            shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)\n        )\n        losses.append(lm_loss)\n\n    return scrubber, torch.stack(losses).mean().item()", ""]}
{"filename": "concept_erasure/scrubbing/__init__.py", "chunked_list": ["from .auto import patch_attention_, scrub\nfrom .random_scrub import random_scrub\n\n__all__ = [\"patch_attention_\", \"random_scrub\", \"scrub\"]\n"]}
{"filename": "concept_erasure/scrubbing/random_scrub.py", "chunked_list": ["from contextlib import contextmanager\nfrom functools import partial\nfrom typing import TYPE_CHECKING\n\nimport torch\nfrom torch import Tensor, nn\n\nif (\n    TYPE_CHECKING\n):  # Don't import this unless we're type checking, since it's slow to import\n    from transformers import PreTrainedModel", "\nfrom ..utils import is_norm_layer\n\n\n@contextmanager\ndef random_scrub(model: \"PreTrainedModel\", subspace_dim: int):\n    \"\"\"Add hooks to the model which erase a random subspace during `forward`.\"\"\"\n    d = model.config.hidden_size\n\n    u = torch.empty(d, subspace_dim, device=model.device, dtype=model.dtype)\n    nn.init.orthogonal_(u)\n\n    def random_erase(_, __, x: Tensor, u: Tensor) -> Tensor:\n        return x - u @ (u.T @ x)\n\n    handles = [\n        mod.register_forward_hook(partial(random_erase, u=u))\n        for mod in model.modules()\n        if is_norm_layer(mod)\n    ]\n\n    try:\n        yield\n    finally:\n        # Make sure to remove the hooks even if an exception is raised\n        for handle in handles:\n            handle.remove()", ""]}
{"filename": "tests/test_shrinkage.py", "chunked_list": ["import pytest\nimport torch\nfrom torch.distributions import MultivariateNormal\n\nfrom concept_erasure import optimal_linear_shrinkage\n\n\n@pytest.mark.parametrize(\n    \"p,n\",\n    [", "    \"p,n\",\n    [\n        # Test the n < p case\n        (32, 16),\n        (64, 32),\n        (128, 64),\n        # And the n > p case\n        (4, 64),\n        (8, 128),\n        (16, 256),", "        (8, 128),\n        (16, 256),\n    ],\n)\ndef test_olse_shrinkage(p: int, n: int):\n    torch.manual_seed(42)\n\n    # Number of matrices\n    N = 1000\n\n    # Generate a random covariance matrix\n    A = torch.randn(N, p, p)\n    S_true = A @ A.mT / p\n    torch.linalg.diagonal(S_true).add_(1e-3)\n\n    # Generate data with this covariance\n    mean = torch.zeros(N, p)\n    dist = MultivariateNormal(mean, S_true)\n    X = dist.sample([n]).movedim(1, 0)\n    assert X.shape == (N, n, p)\n\n    # Compute the sample covariance matrix\n    X_centered = X - X.mean(dim=0, keepdim=True)\n    S_hat = (X_centered.mT @ X_centered) / n\n\n    # Apply shrinkage\n    S_olse = optimal_linear_shrinkage(S_hat, n)\n\n    # Check that the Frobenius norm of the difference has decreased\n    norm_naive = torch.norm(S_hat - S_true, dim=(-1, -2)).mean()\n    norm_olse = torch.norm(S_olse - S_true, dim=(-1, -2)).mean()\n\n    assert norm_olse <= norm_naive", ""]}
{"filename": "tests/test_leace.py", "chunked_list": ["from itertools import pairwise, product\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.datasets import make_classification\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss", "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.svm import LinearSVC\n\nfrom concept_erasure import (\n    ErasureMethod,\n    LeaceFitter,\n    OracleEraser,\n    OracleFitter,\n    QuadraticEraser,", "    OracleFitter,\n    QuadraticEraser,\n    optimal_linear_shrinkage,\n)\nfrom concept_erasure.optimal_transport import is_positive_definite\n\n\n@pytest.mark.parametrize(\"shrinkage\", [False, True])\ndef test_stats(shrinkage: bool):\n    batch_size = 10\n    num_batches = 5\n    num_classes = 2\n    num_features = 3\n    N = batch_size * num_batches\n\n    fitter = LeaceFitter(num_features, num_classes, shrinkage=shrinkage)\n    oracle = OracleFitter(num_features, num_classes, shrinkage=shrinkage)\n\n    # Generate random data\n    torch.manual_seed(42)\n    x_data = [torch.randn(batch_size, num_features) for _ in range(num_batches)]\n    z_data = [\n        torch.randint(0, num_classes, (batch_size, num_classes))\n        for _ in range(num_batches)\n    ]\n    projections = []\n\n    # Compute cross-covariance matrix using batched updates\n    for x, z in zip(x_data, z_data):\n        fitter.update(x, z)\n        oracle.update(x, z)\n        projections.append(fitter.eraser.P)\n\n    # Make sure the cached eraser is getting invalidated on update() correctly\n    for p1, p2 in pairwise(projections):\n        assert not torch.allclose(p1, p2)\n\n    # Compute the expected cross-covariance matrix using the whole dataset\n    x_all = torch.cat(x_data)\n    z_all = torch.cat(z_data)\n    mean_x = x_all.mean(dim=0)\n    mean_z = z_all.type_as(x_all).mean(dim=0)\n    x_centered = x_all - mean_x\n    z_centered = z_all - mean_z\n\n    expected_sigma_xx = torch.einsum(\"b...m,b...n->...mn\", x_centered, x_centered)\n    expected_sigma_zz = torch.einsum(\"b...m,b...n->...mn\", z_centered, z_centered)\n    if shrinkage:\n        expected_sigma_xx = optimal_linear_shrinkage(\n            expected_sigma_xx / N, batch_size * num_batches\n        )\n        expected_sigma_zz = optimal_linear_shrinkage(\n            expected_sigma_zz / N, batch_size * num_batches\n        )\n    else:\n        expected_sigma_xx /= N - 1\n        expected_sigma_zz /= N - 1\n\n    expected_sigma_xz = torch.einsum(\"b...m,b...n->...mn\", x_centered, z_centered)\n    expected_sigma_xz /= N - 1\n\n    torch.testing.assert_close(fitter.sigma_xx, expected_sigma_xx)\n    torch.testing.assert_close(fitter.sigma_xz, expected_sigma_xz)\n\n    torch.testing.assert_close(oracle.sigma_xz, expected_sigma_xz)\n    torch.testing.assert_close(oracle.sigma_zz, expected_sigma_zz)", "def test_stats(shrinkage: bool):\n    batch_size = 10\n    num_batches = 5\n    num_classes = 2\n    num_features = 3\n    N = batch_size * num_batches\n\n    fitter = LeaceFitter(num_features, num_classes, shrinkage=shrinkage)\n    oracle = OracleFitter(num_features, num_classes, shrinkage=shrinkage)\n\n    # Generate random data\n    torch.manual_seed(42)\n    x_data = [torch.randn(batch_size, num_features) for _ in range(num_batches)]\n    z_data = [\n        torch.randint(0, num_classes, (batch_size, num_classes))\n        for _ in range(num_batches)\n    ]\n    projections = []\n\n    # Compute cross-covariance matrix using batched updates\n    for x, z in zip(x_data, z_data):\n        fitter.update(x, z)\n        oracle.update(x, z)\n        projections.append(fitter.eraser.P)\n\n    # Make sure the cached eraser is getting invalidated on update() correctly\n    for p1, p2 in pairwise(projections):\n        assert not torch.allclose(p1, p2)\n\n    # Compute the expected cross-covariance matrix using the whole dataset\n    x_all = torch.cat(x_data)\n    z_all = torch.cat(z_data)\n    mean_x = x_all.mean(dim=0)\n    mean_z = z_all.type_as(x_all).mean(dim=0)\n    x_centered = x_all - mean_x\n    z_centered = z_all - mean_z\n\n    expected_sigma_xx = torch.einsum(\"b...m,b...n->...mn\", x_centered, x_centered)\n    expected_sigma_zz = torch.einsum(\"b...m,b...n->...mn\", z_centered, z_centered)\n    if shrinkage:\n        expected_sigma_xx = optimal_linear_shrinkage(\n            expected_sigma_xx / N, batch_size * num_batches\n        )\n        expected_sigma_zz = optimal_linear_shrinkage(\n            expected_sigma_zz / N, batch_size * num_batches\n        )\n    else:\n        expected_sigma_xx /= N - 1\n        expected_sigma_zz /= N - 1\n\n    expected_sigma_xz = torch.einsum(\"b...m,b...n->...mn\", x_centered, z_centered)\n    expected_sigma_xz /= N - 1\n\n    torch.testing.assert_close(fitter.sigma_xx, expected_sigma_xx)\n    torch.testing.assert_close(fitter.sigma_xz, expected_sigma_xz)\n\n    torch.testing.assert_close(oracle.sigma_xz, expected_sigma_xz)\n    torch.testing.assert_close(oracle.sigma_zz, expected_sigma_zz)", "\n\ndef check_linear_guardedness(\n    dirty_x: np.ndarray | None, scrubbed_x: np.ndarray, y: np.ndarray, eps: float\n):\n    # Logistic regression should not be able to learn anything.\n    null_lr = LogisticRegression(penalty=None, tol=0.0).fit(\n        # Weirdly, in order for this to work consistently with solver='lbfgs', we\n        # need to center the design matrix first; otherwise the coefficients aren't\n        # quite small enough. Other solvers don't have this problem but they're\n        # slow. In theory, the centroid of X shouldn't matter to the solution.\n        scrubbed_x - scrubbed_x.mean(axis=0),\n        y,\n    )\n    assert abs(null_lr.coef_).max() < eps\n\n    # Linear SVM shouldn't be able to learn anything either\n    null_svm = LinearSVC(\n        # The dual formulation injects random noise into the solution\n        dual=False,\n        # Unfortunately the intercept is subject to L2 regularization; setting this\n        # to a large value largely cancels out the effect. Regularizing the\n        # intercept can cause the coefficients to get larger than they should be.\n        intercept_scaling=1e6,\n        tol=eps,\n    ).fit(scrubbed_x, y)\n    assert abs(null_svm.coef_).max() < eps\n\n    if dirty_x is not None:\n        # Sanity check that it DOES learn something before erasure\n        real_lr = LogisticRegression(penalty=None, tol=0.0).fit(\n            # Do the same centering operation here to be consistent\n            dirty_x - dirty_x.mean(axis=0),\n            y,\n        )\n        assert abs(real_lr.coef_).max() > 0.05\n\n        real_svm = LinearSVC(dual=False, intercept_scaling=1e6, tol=eps).fit(dirty_x, y)\n        assert abs(real_svm.coef_).max() > 0.05", "\n\n# Both `1` and `2` are binary classification problems, but `1` means the labels are\n# encoded in a 1D one-hot vector, while `2` means the labels are encoded in an\n# n x 2 one-hot matrix.\n@pytest.mark.parametrize(\"num_classes\", [1, 2, 3, 5, 10, 20])\ndef test_linear_erasure(num_classes: int):\n    eps = 2e-9\n    n, d = 2048, 128\n    num_distinct = max(num_classes, 2)\n\n    X, Y = make_classification(\n        n_samples=n,\n        n_features=d,\n        n_classes=num_distinct,\n        n_informative=num_distinct,\n        random_state=42,\n    )\n    X_t = torch.from_numpy(X)\n    Y_t = torch.from_numpy(Y)\n    if num_classes > 1:\n        Y_1h = torch.nn.functional.one_hot(Y_t, num_classes)\n    else:\n        Y_1h = Y_t\n\n    # Benchmark against the theoretical optimal eraser\n    oracle = OracleEraser.fit(X_t, Y_1h)\n    X_oracle = oracle(X_t, Y_1h)\n\n    check_linear_guardedness(None, X_oracle.numpy(), Y.reshape(n, -1), eps)\n    oracle_loss = F.mse_loss(X_oracle, X_t)  # Optimal surgicality\n\n    bools = [False, True]\n    methods: list[ErasureMethod] = [\"leace\", \"orth\"]\n    for affine, method, shrink in product(bools, methods, bools):\n        # Shrinkage not applicable to orthogonal projection\n        if method == \"orth\" and shrink:\n            continue\n\n        fitter = LeaceFitter(\n            d,\n            num_classes,\n            affine=affine,\n            dtype=X_t.dtype,\n            method=method,\n            shrinkage=shrink,\n        ).update(X_t, Y_1h)\n\n        eraser = fitter.eraser\n        X_ = eraser(X_t)\n\n        # Check first-order optimality condition. To account for the nullspace\n        # constraint, we use the canonical form for oblique projection matrices,\n        # fixing the nullspace and only optimizing over the range space.\n        A, s, B_t = torch.linalg.svd(eraser.P)\n        A, B = A[:, s > 0.5].requires_grad_(True), B_t[s > 0.5].T\n\n        # See \"A matrix representation formula for a nonzero projection operator\" in\n        # https://en.wikipedia.org/wiki/Projection_(linear_algebra)\n        P = A @ torch.inverse(B.T @ A) @ B.T\n        x_ = (X_t - fitter.mean_x) @ P.T + fitter.mean_x if affine else X_t @ P.T\n\n        # Define a random positive definite inner product\n        L = torch.randn(d, d, dtype=X_t.dtype) / d**0.5\n        loss = F.mse_loss(x_ @ L, X_t @ L)\n        loss.backward()\n\n        # Should be optimal iff we're using LEACE with the bias term and no shrinkage\n        assert (A.grad.norm() < eps) == (affine and method == \"leace\" and not shrink)\n\n        # Sanity check that we're not beating the oracle\n        assert oracle_loss <= loss\n\n        # Check idempotence\n        torch.testing.assert_close(eraser(X_), X_)\n\n        # Check that the rank of the update <= num_classes\n        rank = torch.linalg.svdvals(X_t - X_).gt(eps).sum()\n        assert rank <= num_classes\n\n        # Check that the unconditional mean is unchanged\n        if affine:\n            torch.testing.assert_close(X_t.mean(dim=0), X_.mean(dim=0))\n\n        # Compute class means and check that they are equal after the projection\n        class_means_ = torch.stack(\n            [X_[Y_t == c].mean(dim=0) for c in range(num_distinct)]\n        )\n        torch.testing.assert_close(class_means_[1:], class_means_[:-1])\n\n        # Sanity check that class means are NOT equal before the projection\n        class_means = torch.stack(\n            [X_t[Y_t == c].mean(dim=0) for c in range(num_distinct)]\n        )\n        assert not torch.allclose(class_means[1:], class_means[:-1])\n\n        check_linear_guardedness(X, X_.numpy(), Y, eps)", "\n\n@pytest.mark.parametrize(\"num_classes\", [2, 3, 5, 10, 20])\ndef test_quadratic_erasure(num_classes: int):\n    tol = 2e-9\n    n, d = 1024, 32\n\n    X, Y = make_classification(\n        n_samples=n,\n        n_features=d,\n        n_classes=num_classes,\n        n_informative=num_classes,\n        # QDA goes berserk if the covariance matrices are singular\n        n_redundant=0,\n        random_state=42,\n    )\n    X_t = torch.from_numpy(X)\n    Y_t = torch.from_numpy(Y)\n\n    eraser = QuadraticEraser.fit(X_t, Y_t, shrinkage=False)\n    X_scrubbed = eraser(X_t, Y_t).numpy()\n\n    # Quadratic LEACE should ensure both linear & quadratic guardedness\n    check_linear_guardedness(X, X_scrubbed, Y.reshape(n, -1), tol)\n\n    # Now check quadratic guardedness using QDA\n    qda = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X_scrubbed, Y)\n    loss = log_loss(Y, qda.predict_proba(X_scrubbed))\n    trivial_loss = log_loss(Y, np.tile(qda.priors_, [n, 1]))\n    np.testing.assert_allclose(loss, trivial_loss)\n\n    # Sanity check that it DOES learn something before erasure\n    real_qda = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X, Y)\n    real_loss = log_loss(Y, real_qda.predict_proba(X))\n    assert real_loss < trivial_loss\n\n    # Check that the covariance matrices and means are all the same\n    cov, mu = qda.covariance_, np.asarray(qda.means_)\n    np.testing.assert_allclose(cov[1:], cov[:-1])\n    np.testing.assert_allclose(mu[1:], mu[:-1])\n\n    # Optimal transport maps should all be positive definite\n    assert is_positive_definite(eraser.ot_maps).all()", ""]}
{"filename": "tests/test_scrubbing.py", "chunked_list": ["import math\n\nimport pytest\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom concept_erasure.scrubbing import patch_attention_, scrub\n\n", "\n\n@pytest.mark.slow\n@pytest.mark.parametrize(\"model_str\", [\"EleutherAI/pythia-160m\", \"huggyllama/llama-7b\"])\n@torch.inference_mode()\ndef test_scrubbing(model_str: str):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    # It turns out to be important to use the actual pretrained weights and not random\n    # weights, since random models tend to get near-constant loss even if you implement\n    # them incorrectly.\n    model = AutoModelForCausalLM.from_pretrained(model_str).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_str)\n\n    ## Test attention patching by itself ##\n    # Get the output pre-patching\n    input_ids = tokenizer.encode(\"Hello world\", return_tensors=\"pt\").to(device)\n    clean_logits = model(input_ids).logits\n\n    # Patch the model\n    patch_attention_(model)\n\n    # Check that the outputs are the same\n    patched_logits = model(input_ids).logits\n    torch.testing.assert_close(clean_logits, patched_logits, atol=5e-5, rtol=5e-5)\n\n    ## Test attention patching + concept erasure ##\n    tokenized = tokenizer(\n        [\"The quick brown fox jumps over the lazy dog\", \"Hello world!\"]\n    )\n    ds = Dataset.from_dict(dict(tokenized)).with_format(\"torch\")\n    gt_losses = []\n\n    for record in ds:\n        input_ids = record[\"input_ids\"].unsqueeze(0).to(device)\n        gt_losses.append(model(input_ids, labels=input_ids).loss)\n\n    gt_loss = torch.stack(gt_losses).mean().item()\n    _, loss = scrub(model, ds, z_column=None)\n    assert math.isclose(gt_loss, loss, rel_tol=1e-4)", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_groupby.py", "chunked_list": ["import pytest\nimport torch\n\nfrom concept_erasure import groupby\n\n\n@pytest.mark.parametrize(\"stable\", [True, False])\ndef test_groupby(stable: bool):\n    n, k, d = 1000, 3, 10\n\n    x = torch.randn(n, d)\n    z = torch.randint(0, k, (n,))\n    grouped = groupby(x, z, stable=stable)\n\n    # Check that we can coalesce the groups back together\n    torch.testing.assert_allclose(x, grouped.coalesce())\n    torch.testing.assert_allclose(\n        x + 1,\n        grouped.map(lambda _, g: g + 1).coalesce(),\n    )\n\n    # We only expect these to be the same when stable=True\n    if stable:\n        for label in torch.unique(z):\n            torch.testing.assert_allclose(\n                x[z == label],\n                grouped.groups[label],\n            )", ""]}
{"filename": "tests/test_ot.py", "chunked_list": ["import pytest\nimport torch\nfrom torch.distributions import Dirichlet\n\nfrom concept_erasure.optimal_transport import (\n    is_positive_definite,\n    ot_barycenter,\n    ot_distance,\n    ot_map,\n    ot_midpoint,", "    ot_map,\n    ot_midpoint,\n)\n\n\n@pytest.mark.parametrize(\"d\", [1, 2, 4, 8, 16, 32])\ndef test_gaussian_ot_map(d: int):\n    torch.manual_seed(42)\n\n    # Generate random positive definite matrices A and B\n    A = torch.randn(d, d, dtype=torch.float64)\n    A = A @ A.T / d\n\n    B = torch.randn_like(A)\n    B = B @ B.T / d\n\n    C = torch.randn_like(A)\n    C = C @ C.T / d\n\n    # Compute the optimal transport map\n    M = ot_map(A, B)\n\n    # Analytically compute the covariance after transport\n    torch.testing.assert_close(M @ A @ M, B)\n    assert is_positive_definite(M)\n\n    # Midpoint of two Gaussians, with different weights\n    for w1 in [0.25, 0.5, 0.75]:\n        w2 = 1 - w1\n\n        mu = ot_midpoint(A, B, w1, w2)\n        assert is_positive_definite(mu)\n\n        # Check first order optimality condition\n        mu.requires_grad_(True)\n        loss = w1 * ot_distance(mu, A).square() + w2 * ot_distance(mu, B).square()\n        loss.backward()\n\n        torch.testing.assert_close(mu.grad, torch.zeros_like(mu), rtol=1e-6, atol=2e-6)\n\n    # Barycenter of three Gaussians, with a few random weights\n    weight_prior = Dirichlet(A.new_ones(3))\n    for weights in weight_prior.sample(torch.Size([3])):\n        Ks = torch.stack([A, B, C])\n        mu = ot_barycenter(Ks, weights)\n        assert is_positive_definite(mu)\n\n        # Check first order optimality condition\n        mu.requires_grad_(True)\n        loss = ot_distance(mu, Ks).square().mul(weights).sum()\n        loss.backward()\n        torch.testing.assert_close(mu.grad, torch.zeros_like(mu), rtol=1e-6, atol=2e-6)", ""]}
{"filename": "experiments/scrub.py", "chunked_list": ["import math\nimport os\nfrom argparse import ArgumentParser\n\nimport torch\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,", "    AutoModelForCausalLM,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n)\n\nfrom concept_erasure.scrubbing import patch_attention_, random_scrub, scrub\nfrom concept_erasure.utils import assert_type\n\n", "\n\n@torch.inference_mode()\ndef evaluate(ds, model, args, nats_to_bpb):\n    losses = []\n    pbar = tqdm(\n        ds.iter(args.batch_size),\n        desc=\"Evaluating\",\n        total=len(ds) // args.batch_size,\n    )\n\n    for batch in pbar:\n        assert isinstance(batch, dict)\n\n        tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n        loss = model(tokens, labels=tokens).loss\n        losses.append(loss)\n\n        pbar.set_postfix(\n            loss=torch.stack(losses).mean().item() * nats_to_bpb,\n        )", "\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"model\", type=str)\n    parser.add_argument(\"dataset\", type=str)\n    parser.add_argument(\"--batch-size\", type=int, default=32)\n    parser.add_argument(\"--proj-type\", type=str, default=\"leace\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda:0\")\n    parser.add_argument(\"--int8\", action=\"store_true\")\n    parser.add_argument(\"--no-bias\", action=\"store_true\")\n    parser.add_argument(\"--random\", action=\"store_true\")\n    parser.add_argument(\"--skip-sublayers\", action=\"store_true\")\n    parser.add_argument(\"--z-column\", type=str, default=\"pos\")\n    args = parser.parse_args()\n\n    # Step 1: Load the model\n    print(f\"Using device {args.device}\")\n    model = assert_type(\n        PreTrainedModel,\n        AutoModelForCausalLM.from_pretrained(\n            args.model,\n            device_map={\"\": args.device},\n            load_in_8bit=args.int8,\n            torch_dtype=torch.float16,\n        ),\n    )\n    tokenizer = assert_type(\n        PreTrainedTokenizerBase,\n        AutoTokenizer.from_pretrained(args.model, use_fast=\"llama\" not in args.model),\n    )\n    patch_attention_(model)\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    torch.manual_seed(42)\n\n    # Step 2: Load the dataset\n    ds = Dataset.load_from_disk(args.dataset).with_format(\n        \"torch\",\n        device=model.device,\n    )\n    splits = ds.train_test_split(train_size=2**15, seed=42)\n    train = splits[\"train\"].select(range(2**14))\n    test = splits[\"test\"].select(range(2**14))\n\n    byte_lengths = assert_type(torch.Tensor, ds[\"num_bytes\"])\n    length_ratio = float(2048 / byte_lengths.float().mean())\n    nats_to_bpb = length_ratio / math.log(2)\n\n    if args.random:\n        k = assert_type(int, train.features[args.z_column].feature.num_classes)\n\n        with torch.inference_mode():\n            losses = []\n            pbar = tqdm(\n                ds.iter(args.batch_size),\n                desc=\"Evaluating\",\n                total=len(ds) // args.batch_size,\n            )\n            base = assert_type(PreTrainedModel, model.base_model)\n\n            for batch in pbar:\n                assert isinstance(batch, dict)\n\n                tokens = assert_type(torch.Tensor, batch[\"input_ids\"])\n                with random_scrub(base, subspace_dim=k):\n                    loss = model(tokens, labels=tokens).loss\n                    losses.append(loss)\n\n                pbar.set_postfix(\n                    loss=torch.stack(losses).mean().item() * nats_to_bpb,\n                )\n\n    scrubber, dirty_loss = scrub(\n        model,\n        train,\n        affine=not args.no_bias,\n        batch_size=args.batch_size,\n        method=args.method,\n        sublayers=not args.skip_sublayers,\n        z_column=args.z_column,\n    )\n    print(f\"Train loss (bits per byte): {dirty_loss * nats_to_bpb:.2f}\")\n\n    if scrubber:\n        with scrubber.scrub(model):\n            evaluate(test, model, args, nats_to_bpb)", ""]}
