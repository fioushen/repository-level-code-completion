{"filename": "dev.py", "chunked_list": ["import modules.ui as ui\n\ndemo = ui.create_ui()\n"]}
{"filename": "launch.py", "chunked_list": ["import importlib.util\nimport os\nimport shlex\nimport subprocess\nimport sys\n\ncommandline_args = os.environ.get(\"COMMANDLINE_ARGS\", \"\")\nsys.argv += shlex.split(commandline_args)\n\npython = sys.executable", "\npython = sys.executable\ngit = os.environ.get(\"GIT\", \"git\")\nindex_url = os.environ.get(\"INDEX_URL\", \"\")\nstored_commit_hash = None\nskip_install = False\n\n\ndef run(command, desc=None, errdesc=None, custom_env=None):\n    if desc is not None:\n        print(desc)\n\n    result = subprocess.run(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        shell=True,\n        env=os.environ if custom_env is None else custom_env,\n    )\n\n    if result.returncode != 0:\n        message = f\"\"\"{errdesc or 'Error running command'}.\nCommand: {command}\nError code: {result.returncode}\nstdout: {result.stdout.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stdout)>0 else '<empty>'}\nstderr: {result.stderr.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stderr)>0 else '<empty>'}\n\"\"\"\n        raise RuntimeError(message)\n\n    return result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")", "def run(command, desc=None, errdesc=None, custom_env=None):\n    if desc is not None:\n        print(desc)\n\n    result = subprocess.run(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        shell=True,\n        env=os.environ if custom_env is None else custom_env,\n    )\n\n    if result.returncode != 0:\n        message = f\"\"\"{errdesc or 'Error running command'}.\nCommand: {command}\nError code: {result.returncode}\nstdout: {result.stdout.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stdout)>0 else '<empty>'}\nstderr: {result.stderr.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stderr)>0 else '<empty>'}\n\"\"\"\n        raise RuntimeError(message)\n\n    return result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")", "\n\ndef check_run(command):\n    result = subprocess.run(\n        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n    )\n    return result.returncode == 0\n\n\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None", "\ndef is_installed(package):\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        return False\n\n    return spec is not None\n\n\ndef commit_hash():\n    global stored_commit_hash\n\n    if stored_commit_hash is not None:\n        return stored_commit_hash\n\n    try:\n        stored_commit_hash = run(f\"{git} rev-parse HEAD\").strip()\n    except Exception:\n        stored_commit_hash = \"<none>\"\n\n    return stored_commit_hash", "\n\ndef commit_hash():\n    global stored_commit_hash\n\n    if stored_commit_hash is not None:\n        return stored_commit_hash\n\n    try:\n        stored_commit_hash = run(f\"{git} rev-parse HEAD\").strip()\n    except Exception:\n        stored_commit_hash = \"<none>\"\n\n    return stored_commit_hash", "\n\ndef run_pip(args, desc=None):\n    if skip_install:\n        return\n\n    index_url_line = f\" --index-url {index_url}\" if index_url != \"\" else \"\"\n    return run(\n        f'\"{python}\" -m pip {args} --prefer-binary{index_url_line}',\n        desc=f\"Installing {desc}\",\n        errdesc=f\"Couldn't install {desc}\",\n    )", "\n\ndef run_python(code, desc=None, errdesc=None):\n    return run(f'\"{python}\" -c \"{code}\"', desc, errdesc)\n\n\ndef extract_arg(args, name):\n    return [x for x in args if x != name], name in args\n\n\ndef prepare_environment():\n    commit = commit_hash()\n\n    print(f\"Python {sys.version}\")\n    print(f\"Commit hash: {commit}\")\n\n    torch_command = os.environ.get(\n        \"TORCH_COMMAND\",\n        \"pip install torch torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\",\n    )\n\n    sys.argv, skip_install = extract_arg(sys.argv, \"--skip-install\")\n    if skip_install:\n        return\n\n    sys.argv, reinstall_torch = extract_arg(sys.argv, \"--reinstall-torch\")\n    ngrok = \"--ngrok\" in sys.argv\n\n    if reinstall_torch or not is_installed(\"torch\") or not is_installed(\"torchaudio\"):\n        run(\n            f'\"{python}\" -m {torch_command}',\n            \"Installing torch and torchaudio\",\n            \"Couldn't install torch\",\n        )\n\n    if not is_installed(\"pyngrok\") and ngrok:\n        run_pip(\"install pyngrok\", \"ngrok\")\n\n    run(\n        f'\"{python}\" -m pip install -r requirements.txt',\n        desc=f\"Installing requirements\",\n        errdesc=f\"Couldn't install requirements\",\n    )", "\n\ndef prepare_environment():\n    commit = commit_hash()\n\n    print(f\"Python {sys.version}\")\n    print(f\"Commit hash: {commit}\")\n\n    torch_command = os.environ.get(\n        \"TORCH_COMMAND\",\n        \"pip install torch torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\",\n    )\n\n    sys.argv, skip_install = extract_arg(sys.argv, \"--skip-install\")\n    if skip_install:\n        return\n\n    sys.argv, reinstall_torch = extract_arg(sys.argv, \"--reinstall-torch\")\n    ngrok = \"--ngrok\" in sys.argv\n\n    if reinstall_torch or not is_installed(\"torch\") or not is_installed(\"torchaudio\"):\n        run(\n            f'\"{python}\" -m {torch_command}',\n            \"Installing torch and torchaudio\",\n            \"Couldn't install torch\",\n        )\n\n    if not is_installed(\"pyngrok\") and ngrok:\n        run_pip(\"install pyngrok\", \"ngrok\")\n\n    run(\n        f'\"{python}\" -m pip install -r requirements.txt',\n        desc=f\"Installing requirements\",\n        errdesc=f\"Couldn't install requirements\",\n    )", "\n\ndef start():\n    os.environ[\"PATH\"] = (\n        os.path.join(os.path.dirname(__file__), \"bin\")\n        + os.pathsep\n        + os.environ.get(\"PATH\", \"\")\n    )\n    subprocess.run(\n        [python, \"webui.py\", *sys.argv[1:]],\n    )", "\n\nif __name__ == \"__main__\":\n    prepare_environment()\n    start()\n"]}
{"filename": "webui.py", "chunked_list": ["import os\n\nfrom modules import cmd_opts, ui\n\n# \u306a\u3093\u304b\u77e5\u3089\u3093\u304c\u6e67\u3044\u3066\u51fa\u3066\u304f\u308b \".DS_Store\"\u3000\u3092\u7121\u8996\u3059\u308b\u3002\n# \u3053\u3053\u306b\u3053\u3093\u306a\u30b3\u30fc\u30c9\u3092\u7f6e\u304f\u3079\u304d\u304b\u306f\u308f\u304b\u3089\u306a\u3044\u3051\u3069\u2026\n_list_dir = os.listdir\n\ndef listdir4mac(path):\n    return [file for file in _list_dir(path) if not file.startswith(\".\")]", "def listdir4mac(path):\n    return [file for file in _list_dir(path) if not file.startswith(\".\")]\n\nos.listdir = listdir4mac\n\n\ndef webui():\n    app = ui.create_ui()\n    app.queue(64)\n    app, local_url, share_url = app.launch(\n        server_name=cmd_opts.opts.host,\n        server_port=cmd_opts.opts.port,\n        share=cmd_opts.opts.share,\n    )", "\n\nif __name__ == \"__main__\":\n    webui()\n"]}
{"filename": "server.py", "chunked_list": ["import io\nimport json\nimport os\nimport traceback\nfrom typing import *\n\nimport soundfile as sf\nfrom flask import Flask, make_response, request, send_file\nfrom scipy.io.wavfile import write\n", "from scipy.io.wavfile import write\n\nfrom modules.server.model import VoiceServerModel\n\nmodel: Optional[VoiceServerModel] = None\napp = Flask(__name__)\n\n@app.route('/ping')\ndef ping():\n    return make_response(\"server is alive\", 200)", "def ping():\n    return make_response(\"server is alive\", 200)\n\n@app.route('/upload_model', methods=['POST'])\ndef upload_model():\n    \"\"\"\n    input:\n        json:\n            rvc_model_file: str\n                specify rvc model's absolute path (.pt, .pth)\n            faiss_index_file: Optional[str]\n                specify faiss index'S absolute path (.index)\n    \"\"\"\n    global model\n    if request.method == \"POST\":\n        rvc_model_file = request.json[\"rvc_model_file\"]\n        faiss_index_file =request.json[\"faiss_index_file\"] if \"faiss_index_file\" in request.json else \"\"\n        try:\n            model = VoiceServerModel(rvc_model_file, faiss_index_file)\n            return make_response(\"model is load\", 200)\n        except:\n            traceback.print_exc()\n            return make_response(\"model load error\", 400)\n    else:\n        return make_response(\"use post method\", 400)", "\n@app.route('/convert_sound', methods=['POST'])\ndef convert_sound():\n    \"\"\"\n    input:\n        params: json\n            speaker_id: int\n                default: 0\n            transpose: int\n                default: 0\n            pitch_extraction_algo: str\n                default: dio\n                value: [\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"]\n            retrieval_feature_ratio: float\n                default: 0\n                value: 0. ~ 1.\n        input_wav: wav file\n\n    output:\n        wavfile\n    \"\"\"\n    global model\n    if model is None:\n        return make_response(\"please upload model\", 400)\n    print(\"start\")\n    if request.method == \"POST\":\n        input_buffer = io.BytesIO(request.files[\"input_wav\"].stream.read())\n        audio, sr = sf.read(input_buffer)\n\n        req_json = json.load(io.BytesIO(request.files[\"params\"].stream.read()))\n        sid = int(req_json.get(\"speaker_id\", 0))\n        transpose = int(req_json.get(\"transpose\", 0))\n        pitch_extraction_algo = req_json.get(\"pitch_extraction_algo\", \"dio\")\n        if not pitch_extraction_algo in [\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"]:\n            return make_response(\"bad pitch extraction algo\", 400)\n        retrieval_feature_ratio = float(req_json.get(\"retrieval_feature_ratio\", 0.))\n\n        out_audio = model(audio, sr, sid, transpose, pitch_extraction_algo, retrieval_feature_ratio)\n        output_buffer = io.BytesIO()\n        write(output_buffer, rate=model.tgt_sr, data=out_audio)\n        output_buffer.seek(0)\n        response = make_response(send_file(output_buffer, mimetype=\"audio/wav\"), 200)\n        return response\n    else:\n        return make_response(\"use post method\", 400)", "\nif __name__ == \"__main__\":\n    app.run()"]}
{"filename": "lib/rvc/losses.py", "chunked_list": ["import torch\n\n\ndef feature_loss(fmap_r, fmap_g):\n    loss = 0\n    for dr, dg in zip(fmap_r, fmap_g):\n        for rl, gl in zip(dr, dg):\n            rl = rl.float().detach()\n            gl = gl.float()\n            loss += torch.mean(torch.abs(rl - gl))\n\n    return loss * 2", "\n\ndef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n    loss = 0\n    r_losses = []\n    g_losses = []\n    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n        dr = dr.float()\n        dg = dg.float()\n        r_loss = torch.mean((1 - dr) ** 2)\n        g_loss = torch.mean(dg**2)\n        loss += r_loss + g_loss\n        r_losses.append(r_loss.item())\n        g_losses.append(g_loss.item())\n\n    return loss, r_losses, g_losses", "\n\ndef generator_loss(disc_outputs):\n    loss = 0\n    gen_losses = []\n    for dg in disc_outputs:\n        dg = dg.float()\n        l = torch.mean((1 - dg) ** 2)\n        gen_losses.append(l)\n        loss += l\n\n    return loss, gen_losses", "\n\ndef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n    \"\"\"\n    z_p, logs_q: [b, h, t_t]\n    m_p, logs_p: [b, h, t_t]\n    \"\"\"\n    z_p = z_p.float()\n    logs_q = logs_q.float()\n    m_p = m_p.float()\n    logs_p = logs_p.float()\n    z_mask = z_mask.float()\n\n    kl = logs_p - logs_q - 0.5\n    kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n    kl = torch.sum(kl * z_mask)\n    l = kl / torch.sum(z_mask)\n    return l", ""]}
{"filename": "lib/rvc/attentions.py", "chunked_list": ["import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom . import commons\nfrom .modules import LayerNorm\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        window_size=10,\n        **kwargs\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.window_size = window_size\n\n        self.drop = nn.Dropout(p_dropout)\n        self.attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n        for i in range(self.n_layers):\n            self.attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels,\n                    hidden_channels,\n                    n_heads,\n                    p_dropout=p_dropout,\n                    window_size=window_size,\n                )\n            )\n            self.norm_layers_1.append(LayerNorm(hidden_channels))\n            self.ffn_layers.append(\n                FFN(\n                    hidden_channels,\n                    hidden_channels,\n                    filter_channels,\n                    kernel_size,\n                    p_dropout=p_dropout,\n                )\n            )\n            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n    def forward(self, x, x_mask):\n        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        x = x * x_mask\n        for i in range(self.n_layers):\n            y = self.attn_layers[i](x, x, attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.drop(y)\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x", "\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        window_size=10,\n        **kwargs\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.window_size = window_size\n\n        self.drop = nn.Dropout(p_dropout)\n        self.attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n        for i in range(self.n_layers):\n            self.attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels,\n                    hidden_channels,\n                    n_heads,\n                    p_dropout=p_dropout,\n                    window_size=window_size,\n                )\n            )\n            self.norm_layers_1.append(LayerNorm(hidden_channels))\n            self.ffn_layers.append(\n                FFN(\n                    hidden_channels,\n                    hidden_channels,\n                    filter_channels,\n                    kernel_size,\n                    p_dropout=p_dropout,\n                )\n            )\n            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n    def forward(self, x, x_mask):\n        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        x = x * x_mask\n        for i in range(self.n_layers):\n            y = self.attn_layers[i](x, x, attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.drop(y)\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x", "\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        proximal_bias=False,\n        proximal_init=True,\n        **kwargs\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.proximal_bias = proximal_bias\n        self.proximal_init = proximal_init\n\n        self.drop = nn.Dropout(p_dropout)\n        self.self_attn_layers = nn.ModuleList()\n        self.norm_layers_0 = nn.ModuleList()\n        self.encdec_attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n        for i in range(self.n_layers):\n            self.self_attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels,\n                    hidden_channels,\n                    n_heads,\n                    p_dropout=p_dropout,\n                    proximal_bias=proximal_bias,\n                    proximal_init=proximal_init,\n                )\n            )\n            self.norm_layers_0.append(LayerNorm(hidden_channels))\n            self.encdec_attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout\n                )\n            )\n            self.norm_layers_1.append(LayerNorm(hidden_channels))\n            self.ffn_layers.append(\n                FFN(\n                    hidden_channels,\n                    hidden_channels,\n                    filter_channels,\n                    kernel_size,\n                    p_dropout=p_dropout,\n                    causal=True,\n                )\n            )\n            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n    def forward(self, x, x_mask, h, h_mask):\n        \"\"\"\n        x: decoder input\n        h: encoder output\n        \"\"\"\n        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n            device=x.device, dtype=x.dtype\n        )\n        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        x = x * x_mask\n        for i in range(self.n_layers):\n            y = self.self_attn_layers[i](x, x, self_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_0[i](x + y)\n\n            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.drop(y)\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x", "\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        channels,\n        out_channels,\n        n_heads,\n        p_dropout=0.0,\n        window_size=None,\n        heads_share=True,\n        block_length=None,\n        proximal_bias=False,\n        proximal_init=False,\n    ):\n        super().__init__()\n        assert channels % n_heads == 0\n\n        self.channels = channels\n        self.out_channels = out_channels\n        self.n_heads = n_heads\n        self.p_dropout = p_dropout\n        self.window_size = window_size\n        self.heads_share = heads_share\n        self.block_length = block_length\n        self.proximal_bias = proximal_bias\n        self.proximal_init = proximal_init\n        self.attn = None\n\n        self.k_channels = channels // n_heads\n        self.conv_q = nn.Conv1d(channels, channels, 1)\n        self.conv_k = nn.Conv1d(channels, channels, 1)\n        self.conv_v = nn.Conv1d(channels, channels, 1)\n        self.conv_o = nn.Conv1d(channels, out_channels, 1)\n        self.drop = nn.Dropout(p_dropout)\n\n        if window_size is not None:\n            n_heads_rel = 1 if heads_share else n_heads\n            rel_stddev = self.k_channels**-0.5\n            self.emb_rel_k = nn.Parameter(\n                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n                * rel_stddev\n            )\n            self.emb_rel_v = nn.Parameter(\n                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n                * rel_stddev\n            )\n\n        nn.init.xavier_uniform_(self.conv_q.weight)\n        nn.init.xavier_uniform_(self.conv_k.weight)\n        nn.init.xavier_uniform_(self.conv_v.weight)\n        if proximal_init:\n            with torch.no_grad():\n                self.conv_k.weight.copy_(self.conv_q.weight)\n                self.conv_k.bias.copy_(self.conv_q.bias)\n\n    def forward(self, x, c, attn_mask=None):\n        q = self.conv_q(x)\n        k = self.conv_k(c)\n        v = self.conv_v(c)\n\n        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n\n        x = self.conv_o(x)\n        return x\n\n    def attention(self, query, key, value, mask=None):\n        # reshape [b, d, t] -> [b, n_h, t, d_k]\n        b, d, t_s, t_t = (*key.size(), query.size(2))\n        query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n        key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n        value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\n        scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n        if self.window_size is not None:\n            assert (\n                t_s == t_t\n            ), \"Relative attention is only available for self-attention.\"\n            key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n            rel_logits = self._matmul_with_relative_keys(\n                query / math.sqrt(self.k_channels), key_relative_embeddings\n            )\n            scores_local = self._relative_position_to_absolute_position(rel_logits)\n            scores = scores + scores_local\n        if self.proximal_bias:\n            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n            scores = scores + self._attention_bias_proximal(t_s).to(\n                device=scores.device, dtype=scores.dtype\n            )\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e4)\n            if self.block_length is not None:\n                assert (\n                    t_s == t_t\n                ), \"Local attention is only available for self-attention.\"\n                block_mask = (\n                    torch.ones_like(scores)\n                    .triu(-self.block_length)\n                    .tril(self.block_length)\n                )\n                scores = scores.masked_fill(block_mask == 0, -1e4)\n        p_attn = F.softmax(scores, dim=-1)  # [b, n_h, t_t, t_s]\n        p_attn = self.drop(p_attn)\n        output = torch.matmul(p_attn, value)\n        if self.window_size is not None:\n            relative_weights = self._absolute_position_to_relative_position(p_attn)\n            value_relative_embeddings = self._get_relative_embeddings(\n                self.emb_rel_v, t_s\n            )\n            output = output + self._matmul_with_relative_values(\n                relative_weights, value_relative_embeddings\n            )\n        output = (\n            output.transpose(2, 3).contiguous().view(b, d, t_t)\n        )  # [b, n_h, t_t, d_k] -> [b, d, t_t]\n        return output, p_attn\n\n    def _matmul_with_relative_values(self, x, y):\n        \"\"\"\n        x: [b, h, l, m]\n        y: [h or 1, m, d]\n        ret: [b, h, l, d]\n        \"\"\"\n        ret = torch.matmul(x, y.unsqueeze(0))\n        return ret\n\n    def _matmul_with_relative_keys(self, x, y):\n        \"\"\"\n        x: [b, h, l, d]\n        y: [h or 1, m, d]\n        ret: [b, h, l, m]\n        \"\"\"\n        ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n        return ret\n\n    def _get_relative_embeddings(self, relative_embeddings, length):\n        max_relative_position = 2 * self.window_size + 1\n        # Pad first before slice to avoid using cond ops.\n        pad_length = max(length - (self.window_size + 1), 0)\n        slice_start_position = max((self.window_size + 1) - length, 0)\n        slice_end_position = slice_start_position + 2 * length - 1\n        if pad_length > 0:\n            padded_relative_embeddings = F.pad(\n                relative_embeddings,\n                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),\n            )\n        else:\n            padded_relative_embeddings = relative_embeddings\n        used_relative_embeddings = padded_relative_embeddings[\n            :, slice_start_position:slice_end_position\n        ]\n        return used_relative_embeddings\n\n    def _relative_position_to_absolute_position(self, x):\n        \"\"\"\n        x: [b, h, l, 2*l-1]\n        ret: [b, h, l, l]\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # Concat columns of pad to shift from relative to absolute indexing.\n        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]]))\n\n        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n        x_flat = x.view([batch, heads, length * 2 * length])\n        x_flat = F.pad(\n            x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [0, length - 1]])\n        )\n\n        # Reshape and slice out the padded elements.\n        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[\n            :, :, :length, length - 1 :\n        ]\n        return x_final\n\n    def _absolute_position_to_relative_position(self, x):\n        \"\"\"\n        x: [b, h, l, l]\n        ret: [b, h, l, 2*l-1]\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # padd along column\n        x = F.pad(\n            x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length - 1]])\n        )\n        x_flat = x.view([batch, heads, length**2 + length * (length - 1)])\n        # add 0's in the beginning that will skew the elements after reshape\n        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n        return x_final\n\n    def _attention_bias_proximal(self, length):\n        \"\"\"Bias for self-attention to encourage attention to close positions.\n        Args:\n          length: an integer scalar.\n        Returns:\n          a Tensor with shape [1, 1, length, length]\n        \"\"\"\n        r = torch.arange(length, dtype=torch.float32)\n        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)", "\n\nclass FFN(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        filter_channels,\n        kernel_size,\n        p_dropout=0.0,\n        activation=None,\n        causal=False,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.activation = activation\n        self.causal = causal\n\n        if causal:\n            self.padding = self._causal_padding\n        else:\n            self.padding = self._same_padding\n\n        self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n        self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n        self.drop = nn.Dropout(p_dropout)\n\n    def forward(self, x, x_mask):\n        x = self.conv_1(self.padding(x * x_mask))\n        if self.activation == \"gelu\":\n            x = x * torch.sigmoid(1.702 * x)\n        else:\n            x = torch.relu(x)\n        x = self.drop(x)\n        x = self.conv_2(self.padding(x * x_mask))\n        return x * x_mask\n\n    def _causal_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = self.kernel_size - 1\n        pad_r = 0\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, commons.convert_pad_shape(padding))\n        return x\n\n    def _same_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = (self.kernel_size - 1) // 2\n        pad_r = self.kernel_size // 2\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, commons.convert_pad_shape(padding))\n        return x", ""]}
{"filename": "lib/rvc/transforms.py", "chunked_list": ["import numpy as np\nimport torch\nfrom torch.nn import functional as F\n\nDEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\n\n\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\"tails\": tails, \"tail_bound\": tail_bound}\n\n    outputs, logabsdet = spline_fn(\n        inputs=inputs,\n        unnormalized_widths=unnormalized_widths,\n        unnormalized_heights=unnormalized_heights,\n        unnormalized_derivatives=unnormalized_derivatives,\n        inverse=inverse,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative,\n        **spline_kwargs\n    )\n    return outputs, logabsdet", "\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    if tails is None:\n        spline_fn = rational_quadratic_spline\n        spline_kwargs = {}\n    else:\n        spline_fn = unconstrained_rational_quadratic_spline\n        spline_kwargs = {\"tails\": tails, \"tail_bound\": tail_bound}\n\n    outputs, logabsdet = spline_fn(\n        inputs=inputs,\n        unnormalized_widths=unnormalized_widths,\n        unnormalized_heights=unnormalized_heights,\n        unnormalized_derivatives=unnormalized_derivatives,\n        inverse=inverse,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative,\n        **spline_kwargs\n    )\n    return outputs, logabsdet", "\n\ndef searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n\n\ndef unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n    outside_interval_mask = ~inside_interval_mask\n\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    if tails == \"linear\":\n        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n        constant = np.log(np.exp(1 - min_derivative) - 1)\n        unnormalized_derivatives[..., 0] = constant\n        unnormalized_derivatives[..., -1] = constant\n\n        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n        logabsdet[outside_interval_mask] = 0\n    else:\n        raise RuntimeError(\"{} tails are not implemented.\".format(tails))\n\n    (\n        outputs[inside_interval_mask],\n        logabsdet[inside_interval_mask],\n    ) = rational_quadratic_spline(\n        inputs=inputs[inside_interval_mask],\n        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n        inverse=inverse,\n        left=-tail_bound,\n        right=tail_bound,\n        bottom=-tail_bound,\n        top=tail_bound,\n        min_bin_width=min_bin_width,\n        min_bin_height=min_bin_height,\n        min_derivative=min_derivative,\n    )\n\n    return outputs, logabsdet", "\n\ndef rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    left=0.0,\n    right=1.0,\n    bottom=0.0,\n    top=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n    min_derivative=DEFAULT_MIN_DERIVATIVE,\n):\n    if torch.min(inputs) < left or torch.max(inputs) > right:\n        raise ValueError(\"Input to a transform is not within its domain\")\n\n    num_bins = unnormalized_widths.shape[-1]\n\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError(\"Minimal bin width too large for the number of bins\")\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError(\"Minimal bin height too large for the number of bins\")\n\n    widths = F.softmax(unnormalized_widths, dim=-1)\n    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n    cumwidths = torch.cumsum(widths, dim=-1)\n    cumwidths = F.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n    cumwidths = (right - left) * cumwidths + left\n    cumwidths[..., 0] = left\n    cumwidths[..., -1] = right\n    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\n    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\n    heights = F.softmax(unnormalized_heights, dim=-1)\n    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n    cumheights = torch.cumsum(heights, dim=-1)\n    cumheights = F.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n    cumheights = (top - bottom) * cumheights + bottom\n    cumheights[..., 0] = bottom\n    cumheights[..., -1] = top\n    heights = cumheights[..., 1:] - cumheights[..., :-1]\n\n    if inverse:\n        bin_idx = searchsorted(cumheights, inputs)[..., None]\n    else:\n        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\n    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\n    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n    delta = heights / widths\n    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\n    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\n    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\n    if inverse:\n        a = (inputs - input_cumheights) * (\n            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n        ) + input_heights * (input_delta - input_derivatives)\n        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n        )\n        c = -input_delta * (inputs - input_cumheights)\n\n        discriminant = b.pow(2) - 4 * a * c\n        assert (discriminant >= 0).all()\n\n        root = (2 * c) / (-b - torch.sqrt(discriminant))\n        outputs = root * input_bin_widths + input_cumwidths\n\n        theta_one_minus_theta = root * (1 - root)\n        denominator = input_delta + (\n            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n            * theta_one_minus_theta\n        )\n        derivative_numerator = input_delta.pow(2) * (\n            input_derivatives_plus_one * root.pow(2)\n            + 2 * input_delta * theta_one_minus_theta\n            + input_derivatives * (1 - root).pow(2)\n        )\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, -logabsdet\n    else:\n        theta = (inputs - input_cumwidths) / input_bin_widths\n        theta_one_minus_theta = theta * (1 - theta)\n\n        numerator = input_heights * (\n            input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta\n        )\n        denominator = input_delta + (\n            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n            * theta_one_minus_theta\n        )\n        outputs = input_cumheights + numerator / denominator\n\n        derivative_numerator = input_delta.pow(2) * (\n            input_derivatives_plus_one * theta.pow(2)\n            + 2 * input_delta * theta_one_minus_theta\n            + input_derivatives * (1 - theta).pow(2)\n        )\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\n        return outputs, logabsdet", ""]}
{"filename": "lib/rvc/train.py", "chunked_list": ["import glob\nimport json\nimport operator\nimport os\nimport shutil\nimport time\nfrom random import shuffle\nfrom typing import *\n\nimport faiss", "\nimport faiss\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torchaudio\nimport tqdm\nfrom sklearn.cluster import MiniBatchKMeans\nfrom torch.cuda.amp import GradScaler, autocast", "from sklearn.cluster import MiniBatchKMeans\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.nn import functional as F\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom . import commons, utils\nfrom .checkpoints import save\nfrom .config import DatasetMetadata, TrainConfig", "from .checkpoints import save\nfrom .config import DatasetMetadata, TrainConfig\nfrom .data_utils import (DistributedBucketSampler, TextAudioCollate,\n                         TextAudioCollateMultiNSFsid, TextAudioLoader,\n                         TextAudioLoaderMultiNSFsid)\nfrom .losses import discriminator_loss, feature_loss, generator_loss, kl_loss\nfrom .mel_processing import mel_spectrogram_torch, spec_to_mel_torch\nfrom .models import (MultiPeriodDiscriminator, SynthesizerTrnMs256NSFSid,\n                     SynthesizerTrnMs256NSFSidNono)\nfrom .preprocessing.extract_feature import (MODELS_DIR, get_embedder,", "                     SynthesizerTrnMs256NSFSidNono)\nfrom .preprocessing.extract_feature import (MODELS_DIR, get_embedder,\n                                            load_embedder)\n\n\ndef is_audio_file(file: str):\n    if \".\" not in file:\n        return False\n    ext = os.path.splitext(file)[1]\n    return ext.lower() in [\n        \".wav\",\n        \".flac\",\n        \".ogg\",\n        \".mp3\",\n        \".m4a\",\n        \".wma\",\n        \".aiff\",\n    ]", "\n\ndef glob_dataset(\n    glob_str: str,\n    speaker_id: int,\n    multiple_speakers: bool = False,\n    recursive: bool = True,\n    training_dir: str = \".\",\n):\n    globs = glob_str.split(\",\")\n    speaker_count = 0\n    datasets_speakers = []\n    speaker_to_id_mapping = {}\n    for glob_str in globs:\n        if os.path.isdir(glob_str):\n            if multiple_speakers:\n                # Multispeaker format:\n                # dataset_path/\n                # - speakername/\n                #     - {wav name here}.wav\n                #     - ...\n                # - next_speakername/\n                #     - {wav name here}.wav\n                #     - ...\n                # - ...\n                print(\"Multispeaker dataset enabled; Processing speakers.\")\n                for dir in tqdm.tqdm(os.listdir(glob_str)):\n                    print(\"Speaker ID \" + str(speaker_count) + \": \" + dir)\n                    speaker_to_id_mapping[dir] = speaker_count\n                    speaker_path = glob_str + \"/\" + dir\n                    for audio in tqdm.tqdm(os.listdir(speaker_path)):\n                        if is_audio_file(glob_str + \"/\" + dir + \"/\" + audio):\n                            datasets_speakers.append((glob_str + \"/\" + dir + \"/\" + audio, speaker_count))\n                    speaker_count += 1\n                with open(os.path.join(training_dir, \"speaker_info.json\"), \"w\") as outfile:\n                    print(\"Dumped speaker info to {}\".format(os.path.join(training_dir, \"speaker_info.json\")))\n                    json.dump(speaker_to_id_mapping, outfile)\n                continue # Skip the normal speaker extend\n\n            glob_str = os.path.join(glob_str, \"**\", \"*\")\n        print(\"Single speaker dataset enabled; Processing speaker as ID \" + str(speaker_id) + \".\")\n        datasets_speakers.extend(\n            [\n                (file, speaker_id)\n                for file in glob.iglob(glob_str, recursive=recursive)\n                if is_audio_file(file)\n            ]\n        )\n\n    return sorted(datasets_speakers)", "\n\ndef create_dataset_meta(training_dir: str, f0: bool):\n    gt_wavs_dir = os.path.join(training_dir, \"0_gt_wavs\")\n    co256_dir = os.path.join(training_dir, \"3_feature256\")\n\n    def list_data(dir: str):\n        files = []\n        for subdir in os.listdir(dir):\n            speaker_dir = os.path.join(dir, subdir)\n            for name in os.listdir(speaker_dir):\n                files.append(os.path.join(subdir, name.split(\".\")[0]))\n        return files\n\n    names = set(list_data(gt_wavs_dir)) & set(list_data(co256_dir))\n\n    if f0:\n        f0_dir = os.path.join(training_dir, \"2a_f0\")\n        f0nsf_dir = os.path.join(training_dir, \"2b_f0nsf\")\n        names = names & set(list_data(f0_dir)) & set(list_data(f0nsf_dir))\n\n    meta = {\n        \"files\": {},\n    }\n\n    for name in names:\n        speaker_id = os.path.dirname(name).split(\"_\")[0]\n        speaker_id = int(speaker_id) if speaker_id.isdecimal() else 0\n        if f0:\n            gt_wav_path = os.path.join(gt_wavs_dir, f\"{name}.wav\")\n            co256_path = os.path.join(co256_dir, f\"{name}.npy\")\n            f0_path = os.path.join(f0_dir, f\"{name}.wav.npy\")\n            f0nsf_path = os.path.join(f0nsf_dir, f\"{name}.wav.npy\")\n            meta[\"files\"][name] = {\n                \"gt_wav\": gt_wav_path,\n                \"co256\": co256_path,\n                \"f0\": f0_path,\n                \"f0nsf\": f0nsf_path,\n                \"speaker_id\": speaker_id,\n            }\n        else:\n            gt_wav_path = os.path.join(gt_wavs_dir, f\"{name}.wav\")\n            co256_path = os.path.join(co256_dir, f\"{name}.npy\")\n            meta[\"files\"][name] = {\n                \"gt_wav\": gt_wav_path,\n                \"co256\": co256_path,\n                \"speaker_id\": speaker_id,\n            }\n\n    with open(os.path.join(training_dir, \"meta.json\"), \"w\") as f:\n        json.dump(meta, f, indent=2)", "\n\ndef change_speaker(net_g, speaker_info, embedder, embedding_output_layer, phone, phone_lengths, pitch, pitchf, spec_lengths):\n    \"\"\"\n    random change formant\n    inspired by https://github.com/auspicious3000/contentvec/blob/d746688a32940f4bee410ed7c87ec9cf8ff04f74/contentvec/data/audio/audio_utils_1.py#L179\n    \"\"\"\n    N = phone.shape[0]\n    device = phone.device\n    dtype = phone.dtype\n\n    f0_bin = 256\n    f0_max = 1100.0\n    f0_min = 50.0\n    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\n    pitch_median = torch.median(pitchf, 1).values\n    lo = 75. + 25. * (pitch_median >= 200).to(dtype=dtype)\n    hi = 250. + 150. * (pitch_median >= 200).to(dtype=dtype)\n    pitch_median = torch.clip(pitch_median, lo, hi).unsqueeze(1)\n\n    shift_pitch = torch.exp2((1. - 2. * torch.rand(N)) / 4).unsqueeze(1).to(device, dtype)   # \u30d4\u30c3\u30c1\u3092\u534a\u30aa\u30af\u30bf\u30fc\u30d6\u306e\u7bc4\u56f2\u3067\u305a\u3089\u3059\n\n    new_sid = np.random.choice(np.arange(len(speaker_info))[speaker_info > 0], size=N)\n    rel_pitch = pitchf / pitch_median\n    new_pitch_median = torch.from_numpy(speaker_info[new_sid]).to(device, dtype).unsqueeze(1) * shift_pitch\n    new_pitchf = new_pitch_median * rel_pitch\n    new_sid = torch.from_numpy(new_sid).to(device)\n\n    new_pitch = 1127. * torch.log(1. + new_pitchf / 700.)\n    new_pitch = (pitch - f0_mel_min) * (f0_bin - 2.) / (f0_mel_max - f0_mel_min) + 1.\n    new_pitch = torch.clip(new_pitch, 1, f0_bin - 1).to(dtype=torch.int)\n\n    aug_wave = net_g.infer(phone, phone_lengths, new_pitch, new_pitchf, new_sid)[0]\n    aug_wave_16k = torchaudio.functional.resample(aug_wave, net_g.sr, 16000, rolloff=0.99).squeeze(1)\n    padding_mask = torch.arange(aug_wave_16k.shape[1]).unsqueeze(0).to(device) > (spec_lengths.unsqueeze(1) * 160).to(device)\n\n    inputs = {\n        \"source\": aug_wave_16k.to(device, dtype),\n        \"padding_mask\": padding_mask.to(device),\n        \"output_layer\": embedding_output_layer\n    }\n    logits = embedder.extract_features(**inputs)\n    if phone.shape[-1] == 768:\n        feats = logits[0]\n    else:\n        feats = embedder.final_proj(logits[0])\n    feats = torch.repeat_interleave(feats, 2, 1)\n    new_phone = torch.zeros(phone.shape).to(device, dtype)\n    new_phone[:, :feats.shape[1]] = feats[:, :phone.shape[1]]\n    return new_phone.to(device), aug_wave", "\n\ndef change_speaker_nono(net_g, embedder, embedding_output_layer, phone, phone_lengths, spec_lengths):\n    \"\"\"\n    random change formant\n    inspired by https://github.com/auspicious3000/contentvec/blob/d746688a32940f4bee410ed7c87ec9cf8ff04f74/contentvec/data/audio/audio_utils_1.py#L179\n    \"\"\"\n    N = phone.shape[0]\n    device = phone.device\n    dtype = phone.dtype\n\n    new_sid = np.random.randint(net_g.spk_embed_dim, size=N)\n    new_sid = torch.from_numpy(new_sid).to(device)\n\n    aug_wave = net_g.infer(phone, phone_lengths, new_sid)[0]\n    aug_wave_16k = torchaudio.functional.resample(aug_wave, net_g.sr, 16000, rolloff=0.99).squeeze(1)\n    padding_mask = torch.arange(aug_wave_16k.shape[1]).unsqueeze(0).to(device) > (spec_lengths.unsqueeze(1) * 160).to(device)\n\n    inputs = {\n        \"source\": aug_wave_16k.to(device, dtype),\n        \"padding_mask\": padding_mask.to(device),\n        \"output_layer\": embedding_output_layer\n    }\n\n    logits = embedder.extract_features(**inputs)\n    if phone.shape[-1] == 768:\n        feats = logits[0]\n    else:\n        feats = embedder.final_proj(logits[0])\n    feats = torch.repeat_interleave(feats, 2, 1)\n    new_phone = torch.zeros(phone.shape).to(device, dtype)\n    new_phone[:, :feats.shape[1]] = feats[:, :phone.shape[1]]\n    return new_phone.to(device), aug_wave", "\n\ndef train_index(\n    training_dir: str,\n    model_name: str,\n    out_dir: str,\n    emb_ch: int,\n    num_cpu_process: int,\n    maximum_index_size: Optional[int],\n):\n    checkpoint_path = os.path.join(out_dir, model_name)\n    feature_256_dir = os.path.join(training_dir, \"3_feature256\")\n    index_dir = os.path.join(os.path.dirname(checkpoint_path), f\"{model_name}_index\")\n    os.makedirs(index_dir, exist_ok=True)\n    for speaker_id in tqdm.tqdm(\n        sorted([dir for dir in os.listdir(feature_256_dir) if dir.isdecimal()])\n    ):\n        feature_256_spk_dir = os.path.join(feature_256_dir, speaker_id)\n        speaker_id = int(speaker_id)\n        npys = []\n        for name in [\n            os.path.join(feature_256_spk_dir, file)\n            for file in os.listdir(feature_256_spk_dir)\n            if file.endswith(\".npy\")\n        ]:\n            phone = np.load(os.path.join(feature_256_spk_dir, name))\n            npys.append(phone)\n\n        # shuffle big_npy to prevent reproducing the sound source\n        big_npy = np.concatenate(npys, 0)\n        big_npy_idx = np.arange(big_npy.shape[0])\n        np.random.shuffle(big_npy_idx)\n        big_npy = big_npy[big_npy_idx]\n\n        if not maximum_index_size is None and big_npy.shape[0] > maximum_index_size:\n            kmeans = MiniBatchKMeans(\n                n_clusters=maximum_index_size,\n                batch_size=256 * num_cpu_process,\n                init=\"random\",\n                compute_labels=False,\n            )\n            kmeans.fit(big_npy)\n            big_npy = kmeans.cluster_centers_\n\n        # recommend parameter in https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n        emb_ch = big_npy.shape[1]\n        emb_ch_half = emb_ch // 2\n        n_ivf = int(8 * np.sqrt(big_npy.shape[0]))\n        if big_npy.shape[0] >= 1_000_000:\n            index = faiss.index_factory(\n                emb_ch, f\"IVF{n_ivf},PQ{emb_ch_half}x4fsr,RFlat\"\n            )\n        else:\n            index = faiss.index_factory(emb_ch, f\"IVF{n_ivf},Flat\")\n\n        index.train(big_npy)\n        batch_size_add = 8192\n        for i in range(0, big_npy.shape[0], batch_size_add):\n            index.add(big_npy[i : i + batch_size_add])\n        np.save(\n            os.path.join(index_dir, f\"{model_name}.{speaker_id}.big.npy\"),\n            big_npy,\n        )\n        faiss.write_index(\n            index,\n            os.path.join(index_dir, f\"{model_name}.{speaker_id}.index\"),\n        )", "\n\ndef train_model(\n    gpus: List[int],\n    config: TrainConfig,\n    training_dir: str,\n    model_name: str,\n    out_dir: str,\n    sample_rate: int,\n    f0: bool,\n    batch_size: int,\n    augment: bool,\n    augment_path: Optional[str],\n    speaker_info_path: Optional[str],\n    cache_batch: bool,\n    total_epoch: int,\n    save_every_epoch: int,\n    save_wav_with_checkpoint: bool,\n    pretrain_g: str,\n    pretrain_d: str,\n    embedder_name: str,\n    embedding_output_layer: int,\n    save_only_last: bool = False,\n    device: Optional[Union[str, torch.device]] = None,\n):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = str(utils.find_empty_port())\n\n    deterministic = torch.backends.cudnn.deterministic\n    benchmark = torch.backends.cudnn.benchmark\n    PREV_CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = False\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(gpu) for gpu in gpus])\n\n    start = time.perf_counter()\n\n    # Mac(MPS)\u3067\u3084\u308b\u3068\u3001mp.spawn\u3067\u306a\u3093\u304b\u30c8\u30e9\u30d6\u30eb\u304c\u51fa\u308b\u306e\u3067\u666e\u901a\u306btraining_runner\u3092\u547c\u3073\u51fa\u3059\u3002\n    if device is not None:\n        training_runner(\n            0,  # rank\n            1,  # world size\n            config,\n            training_dir,\n            model_name,\n            out_dir,\n            sample_rate,\n            f0,\n            batch_size,\n            augment,\n            augment_path,\n            speaker_info_path,\n            cache_batch,\n            total_epoch,\n            save_every_epoch,\n            save_wav_with_checkpoint,\n            pretrain_g,\n            pretrain_d,\n            embedder_name,\n            embedding_output_layer,\n            save_only_last,\n            device,\n        )\n    else:\n        mp.spawn(\n            training_runner,\n            nprocs=len(gpus),\n            args=(\n                len(gpus),\n                config,\n                training_dir,\n                model_name,\n                out_dir,\n                sample_rate,\n                f0,\n                batch_size,\n                augment,\n                augment_path,\n                speaker_info_path,\n                cache_batch,\n                total_epoch,\n                save_every_epoch,\n                save_wav_with_checkpoint,\n                pretrain_g,\n                pretrain_d,\n                embedder_name,\n                embedding_output_layer,\n                save_only_last,\n                device,\n            ),\n        )\n\n    end = time.perf_counter()\n\n    print(f\"Time: {end - start}\")\n\n    if PREV_CUDA_VISIBLE_DEVICES is None:\n        del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n    else:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = PREV_CUDA_VISIBLE_DEVICES\n\n    torch.backends.cudnn.deterministic = deterministic\n    torch.backends.cudnn.benchmark = benchmark", "\n\ndef training_runner(\n    rank: int,\n    world_size: List[int],\n    config: TrainConfig,\n    training_dir: str,\n    model_name: str,\n    out_dir: str,\n    sample_rate: int,\n    f0: bool,\n    batch_size: int,\n    augment: bool,\n    augment_path: Optional[str],\n    speaker_info_path: Optional[str],\n    cache_in_gpu: bool,\n    total_epoch: int,\n    save_every_epoch: int,\n    save_wav_with_checkpoint: bool,\n    pretrain_g: str,\n    pretrain_d: str,\n    embedder_name: str,\n    embedding_output_layer: int,\n    save_only_last: bool = False,\n    device: Optional[Union[str, torch.device]] = None,\n):\n    config.train.batch_size = batch_size\n    log_dir = os.path.join(training_dir, \"logs\")\n    state_dir = os.path.join(training_dir, \"state\")\n    training_files_path = os.path.join(training_dir, \"meta.json\")\n    training_meta = DatasetMetadata.parse_file(training_files_path)\n    embedder_out_channels = config.model.emb_channels\n\n    is_multi_process = world_size > 1\n\n    if device is not None:\n        if type(device) == str:\n            device = torch.device(device)\n\n    global_step = 0\n    is_main_process = rank == 0\n\n    if is_main_process:\n        os.makedirs(log_dir, exist_ok=True)\n        os.makedirs(state_dir, exist_ok=True)\n        writer = SummaryWriter(log_dir=log_dir)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    if not dist.is_initialized():\n        dist.init_process_group(\n            backend=\"gloo\", init_method=\"env://\", rank=rank, world_size=world_size\n        )\n\n    if is_multi_process:\n        torch.cuda.set_device(rank)\n\n    torch.manual_seed(config.train.seed)\n\n    if f0:\n        train_dataset = TextAudioLoaderMultiNSFsid(training_meta, config.data)\n    else:\n        train_dataset = TextAudioLoader(training_meta, config.data)\n\n    train_sampler = DistributedBucketSampler(\n        train_dataset,\n        config.train.batch_size * world_size,\n        [100, 200, 300, 400, 500, 600, 700, 800, 900],\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True,\n    )\n\n    if f0:\n        collate_fn = TextAudioCollateMultiNSFsid()\n    else:\n        collate_fn = TextAudioCollate()\n\n    train_loader = DataLoader(\n        train_dataset,\n        num_workers=4,\n        shuffle=False,\n        pin_memory=True,\n        collate_fn=collate_fn,\n        batch_sampler=train_sampler,\n        persistent_workers=True,\n        prefetch_factor=8,\n    )\n    speaker_info = None\n    if os.path.exists(os.path.join(training_dir, \"speaker_info.json\")):\n        with open(os.path.join(training_dir, \"speaker_info.json\"), \"r\") as f:\n            speaker_info = json.load(f)\n            config.model.spk_embed_dim = len(speaker_info)\n    if f0:\n        net_g = SynthesizerTrnMs256NSFSid(\n            config.data.filter_length // 2 + 1,\n            config.train.segment_size // config.data.hop_length,\n            **config.model.dict(),\n            is_half=False, # config.train.fp16_run,\n            sr=int(sample_rate[:-1] + \"000\"),\n        )\n    else:\n        net_g = SynthesizerTrnMs256NSFSidNono(\n            config.data.filter_length // 2 + 1,\n            config.train.segment_size // config.data.hop_length,\n            **config.model.dict(),\n            is_half=False, # config.train.fp16_run,\n            sr=int(sample_rate[:-1] + \"000\"),\n        )\n\n    if is_multi_process:\n        net_g = net_g.cuda(rank)\n    else:\n        net_g = net_g.to(device=device)\n\n    if config.version == \"v1\":\n        periods = [2, 3, 5, 7, 11, 17]\n    elif config.version == \"v2\":\n        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n    net_d = MultiPeriodDiscriminator(config.model.use_spectral_norm, periods=periods)\n    if is_multi_process:\n        net_d = net_d.cuda(rank)\n    else:\n        net_d = net_d.to(device=device)\n\n    optim_g = torch.optim.AdamW(\n        net_g.parameters(),\n        config.train.learning_rate,\n        betas=config.train.betas,\n        eps=config.train.eps,\n    )\n    optim_d = torch.optim.AdamW(\n        net_d.parameters(),\n        config.train.learning_rate,\n        betas=config.train.betas,\n        eps=config.train.eps,\n    )\n\n    last_d_state = utils.latest_checkpoint_path(state_dir, \"D_*.pth\")\n    last_g_state = utils.latest_checkpoint_path(state_dir, \"G_*.pth\")\n\n    if last_d_state is None or last_g_state is None:\n        epoch = 1\n        global_step = 0\n        if os.path.exists(pretrain_g) and os.path.exists(pretrain_d):\n            net_g_state = torch.load(pretrain_g, map_location=\"cpu\")[\"model\"]\n            emb_spk_size = (config.model.spk_embed_dim, config.model.gin_channels)\n            emb_phone_size = (config.model.hidden_channels, config.model.emb_channels)\n            if emb_spk_size != net_g_state[\"emb_g.weight\"].size():\n                original_weight = net_g_state[\"emb_g.weight\"]\n                net_g_state[\"emb_g.weight\"] = original_weight.mean(dim=0, keepdims=True) * torch.ones(emb_spk_size, device=original_weight.device, dtype=original_weight.dtype)\n            if emb_phone_size != net_g_state[\"enc_p.emb_phone.weight\"].size():\n                # interpolate\n                orig_shape = net_g_state[\"enc_p.emb_phone.weight\"].size()\n                if net_g_state[\"enc_p.emb_phone.weight\"].dtype == torch.half:\n                    net_g_state[\"enc_p.emb_phone.weight\"] = (\n                        F.interpolate(\n                            net_g_state[\"enc_p.emb_phone.weight\"]\n                            .float()\n                            .unsqueeze(0)\n                            .unsqueeze(0),\n                            size=emb_phone_size,\n                            mode=\"bilinear\",\n                        )\n                        .half()\n                        .squeeze(0)\n                        .squeeze(0)\n                    )\n                else:\n                    net_g_state[\"enc_p.emb_phone.weight\"] = (\n                        F.interpolate(\n                            net_g_state[\"enc_p.emb_phone.weight\"]\n                            .unsqueeze(0)\n                            .unsqueeze(0),\n                            size=emb_phone_size,\n                            mode=\"bilinear\",\n                        )\n                        .squeeze(0)\n                        .squeeze(0)\n                    )\n                print(\n                    \"interpolated pretrained state enc_p.emb_phone from\",\n                    orig_shape,\n                    \"to\",\n                    emb_phone_size,\n                )\n            if is_multi_process:\n                net_g.module.load_state_dict(net_g_state)\n            else:\n                net_g.load_state_dict(net_g_state)\n\n            del net_g_state\n\n            if is_multi_process:\n                net_d.module.load_state_dict(\n                    torch.load(pretrain_d, map_location=\"cpu\")[\"model\"]\n                )\n            else:\n                net_d.load_state_dict(\n                    torch.load(pretrain_d, map_location=\"cpu\")[\"model\"]\n                )\n            if is_main_process:\n                print(f\"loaded pretrained {pretrain_g} {pretrain_d}\")\n\n    else:\n        _, _, _, epoch = utils.load_checkpoint(last_d_state, net_d, optim_d)\n        _, _, _, epoch = utils.load_checkpoint(last_g_state, net_g, optim_g)\n        if is_main_process:\n            print(f\"loaded last state {last_d_state} {last_g_state}\")\n\n        epoch += 1\n        global_step = (epoch - 1) * len(train_loader)\n\n    if augment:\n        # load embedder\n        embedder_filepath, _, embedder_load_from = get_embedder(embedder_name)\n\n        if embedder_load_from == \"local\":\n            embedder_filepath = os.path.join(\n                MODELS_DIR, \"embeddings\", embedder_filepath\n            )\n        embedder, _ = load_embedder(embedder_filepath, device)\n        if not config.train.fp16_run:\n            embedder = embedder.float()\n\n        if (augment_path is not None):\n            state_dict = torch.load(augment_path, map_location=\"cpu\")\n            if state_dict[\"f0\"] == 1:\n                augment_net_g = SynthesizerTrnMs256NSFSid(\n                    **state_dict[\"params\"], is_half=config.train.fp16_run\n                )\n                augment_speaker_info = np.load(speaker_info_path)\n            else:\n                augment_net_g = SynthesizerTrnMs256NSFSidNono(\n                    **state_dict[\"params\"], is_half=config.train.fp16_run\n                )\n\n            augment_net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n            augment_net_g.eval().to(device)\n\n        else:\n            augment_net_g = net_g\n            if f0:\n                medians = [[] for _ in range(augment_net_g.spk_embed_dim)]\n                for file in training_meta.files.values():\n                    f0f = np.load(file.f0nsf)\n                    if np.any(f0f > 0):\n                        medians[file.speaker_id].append(np.median(f0f[f0f > 0]))\n                augment_speaker_info = np.array([np.median(x) if len(x) else 0. for x in medians])\n                np.save(os.path.join(training_dir, \"speaker_info.npy\"), augment_speaker_info)\n\n    if is_multi_process:\n        net_g = DDP(net_g, device_ids=[rank])\n        net_d = DDP(net_d, device_ids=[rank])\n\n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(\n        optim_g, gamma=config.train.lr_decay, last_epoch=epoch - 2\n    )\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(\n        optim_d, gamma=config.train.lr_decay, last_epoch=epoch - 2\n    )\n\n    scaler = GradScaler(enabled=config.train.fp16_run)\n\n    cache = []\n    progress_bar = tqdm.tqdm(range((total_epoch - epoch + 1) * len(train_loader)))\n    progress_bar.set_postfix(epoch=epoch)\n    step = -1 + len(train_loader) * (epoch - 1)\n    for epoch in range(epoch, total_epoch + 1):\n        train_loader.batch_sampler.set_epoch(epoch)\n\n        net_g.train()\n        net_d.train()\n\n        use_cache = len(cache) == len(train_loader)\n        data = cache if use_cache else enumerate(train_loader)\n\n        if is_main_process:\n            lr = optim_g.param_groups[0][\"lr\"]\n\n        if use_cache:\n            shuffle(cache)\n\n        for batch_idx, batch in data:\n            step += 1\n            progress_bar.update(1)\n            if f0:\n                (\n                    phone,\n                    phone_lengths,\n                    pitch,\n                    pitchf,\n                    spec,\n                    spec_lengths,\n                    wave,\n                    wave_lengths,\n                    sid,\n                ) = batch\n            else:\n                (\n                    phone,\n                    phone_lengths,\n                    spec,\n                    spec_lengths,\n                    wave,\n                    wave_lengths,\n                    sid,\n                ) = batch\n\n            if not use_cache:\n                phone, phone_lengths = (\n                    phone.to(device=device, non_blocking=True),\n                    phone_lengths.to(device=device, non_blocking=True),\n                )\n                if f0:\n                    pitch, pitchf = (\n                        pitch.to(device=device, non_blocking=True),\n                        pitchf.to(device=device, non_blocking=True),\n                    )\n                sid = sid.to(device=device, non_blocking=True)\n                spec, spec_lengths = (\n                    spec.to(device=device, non_blocking=True),\n                    spec_lengths.to(device=device, non_blocking=True),\n                )\n                wave, wave_lengths = (\n                    wave.to(device=device, non_blocking=True),\n                    wave_lengths.to(device=device, non_blocking=True),\n                )\n                if cache_in_gpu:\n                    if f0:\n                        cache.append(\n                            (\n                                batch_idx,\n                                (\n                                    phone,\n                                    phone_lengths,\n                                    pitch,\n                                    pitchf,\n                                    spec,\n                                    spec_lengths,\n                                    wave,\n                                    wave_lengths,\n                                    sid,\n                                ),\n                            )\n                        )\n                    else:\n                        cache.append(\n                            (\n                                batch_idx,\n                                (\n                                    phone,\n                                    phone_lengths,\n                                    spec,\n                                    spec_lengths,\n                                    wave,\n                                    wave_lengths,\n                                    sid,\n                                ),\n                            )\n                        )\n\n            with autocast(enabled=config.train.fp16_run):\n                if augment:\n                    with torch.no_grad():\n                        if type(augment_net_g) == SynthesizerTrnMs256NSFSid:\n                            new_phone, aug_wave = change_speaker(augment_net_g, augment_speaker_info, embedder, embedding_output_layer, phone, phone_lengths, pitch, pitchf, spec_lengths)\n                        else:\n                            new_phone, aug_wave = change_speaker_nono(augment_net_g, embedder, embedding_output_layer, phone, phone_lengths, spec_lengths)\n                        weight = np.power(.5, step / len(train_loader))  # \u5b66\u7fd2\u306e\u521d\u671f\u306f\u305d\u306e\u307e\u307e\u306ephone embedding\u3092\u4f7f\u3046\n                        phone = phone * weight + new_phone * (1. - weight)\n\n                if f0:\n                    (\n                        y_hat,\n                        ids_slice,\n                        x_mask,\n                        z_mask,\n                        (z, z_p, m_p, logs_p, m_q, logs_q),\n                    ) = net_g(\n                        phone, phone_lengths, pitch, pitchf, spec, spec_lengths, sid\n                    )\n                else:\n                    (\n                        y_hat,\n                        ids_slice,\n                        x_mask,\n                        z_mask,\n                        (z, z_p, m_p, logs_p, m_q, logs_q),\n                    ) = net_g(phone, phone_lengths, spec, spec_lengths, sid)\n                mel = spec_to_mel_torch(\n                    spec,\n                    config.data.filter_length,\n                    config.data.n_mel_channels,\n                    config.data.sampling_rate,\n                    config.data.mel_fmin,\n                    config.data.mel_fmax,\n                )\n                y_mel = commons.slice_segments(\n                    mel, ids_slice, config.train.segment_size // config.data.hop_length\n                )\n                with autocast(enabled=False):\n                    y_hat_mel = mel_spectrogram_torch(\n                        y_hat.float().squeeze(1),\n                        config.data.filter_length,\n                        config.data.n_mel_channels,\n                        config.data.sampling_rate,\n                        config.data.hop_length,\n                        config.data.win_length,\n                        config.data.mel_fmin,\n                        config.data.mel_fmax,\n                    )\n                if config.train.fp16_run == True and device != torch.device(\"mps\"):\n                    y_hat_mel = y_hat_mel.half()\n                wave_slice = commons.slice_segments(\n                    wave, ids_slice * config.data.hop_length, config.train.segment_size\n                )  # slice\n\n                # Discriminator\n                y_d_hat_r, y_d_hat_g, _, _ = net_d(wave_slice, y_hat.detach())\n                with autocast(enabled=False):\n                    loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(\n                        y_d_hat_r, y_d_hat_g\n                    )\n            optim_d.zero_grad()\n            scaler.scale(loss_disc).backward()\n            scaler.unscale_(optim_d)\n            grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n            scaler.step(optim_d)\n\n            with autocast(enabled=config.train.fp16_run):\n                # Generator\n                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(wave_slice, y_hat)\n                with autocast(enabled=False):\n                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * config.train.c_mel\n                    loss_kl = (\n                        kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * config.train.c_kl\n                    )\n                    loss_fm = feature_loss(fmap_r, fmap_g)\n                    loss_gen, losses_gen = generator_loss(y_d_hat_g)\n                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_kl\n            optim_g.zero_grad()\n            scaler.scale(loss_gen_all).backward()\n            scaler.unscale_(optim_g)\n            grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n            scaler.step(optim_g)\n            scaler.update()\n\n            if is_main_process:\n                progress_bar.set_postfix(\n                    epoch=epoch,\n                    loss_g=float(loss_gen_all) if loss_gen_all is not None else 0.0,\n                    loss_d=float(loss_disc) if loss_disc is not None else 0.0,\n                    lr=float(lr) if lr is not None else 0.0,\n                    use_cache=use_cache,\n                )\n                if global_step % config.train.log_interval == 0:\n                    lr = optim_g.param_groups[0][\"lr\"]\n                    # Amor For Tensorboard display\n                    if loss_mel > 50:\n                        loss_mel = 50\n                    if loss_kl > 5:\n                        loss_kl = 5\n\n                    scalar_dict = {\n                        \"loss/g/total\": loss_gen_all,\n                        \"loss/d/total\": loss_disc,\n                        \"learning_rate\": lr,\n                        \"grad_norm_d\": grad_norm_d,\n                        \"grad_norm_g\": grad_norm_g,\n                    }\n                    scalar_dict.update(\n                        {\n                            \"loss/g/fm\": loss_fm,\n                            \"loss/g/mel\": loss_mel,\n                            \"loss/g/kl\": loss_kl,\n                        }\n                    )\n\n                    scalar_dict.update(\n                        {\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)}\n                    )\n                    scalar_dict.update(\n                        {\n                            \"loss/d_r/{}\".format(i): v\n                            for i, v in enumerate(losses_disc_r)\n                        }\n                    )\n                    scalar_dict.update(\n                        {\n                            \"loss/d_g/{}\".format(i): v\n                            for i, v in enumerate(losses_disc_g)\n                        }\n                    )\n                    image_dict = {\n                        \"slice/mel_org\": utils.plot_spectrogram_to_numpy(\n                            y_mel[0].data.cpu().numpy()\n                        ),\n                        \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(\n                            y_hat_mel[0].data.cpu().numpy()\n                        ),\n                        \"all/mel\": utils.plot_spectrogram_to_numpy(\n                            mel[0].data.cpu().numpy()\n                        ),\n                    }\n                    utils.summarize(\n                        writer=writer,\n                        global_step=global_step,\n                        images=image_dict,\n                        scalars=scalar_dict,\n                    )\n            global_step += 1\n        if is_main_process and save_every_epoch != 0 and epoch % save_every_epoch == 0:\n            if save_only_last:\n                old_g_path = os.path.join(\n                    state_dir, f\"G_{epoch - save_every_epoch}.pth\"\n                )\n                old_d_path = os.path.join(\n                    state_dir, f\"D_{epoch - save_every_epoch}.pth\"\n                )\n                old_wav_path = os.path.join(\n                    state_dir, f\"wav_sample_{epoch - save_every_epoch}\"\n                )\n                if os.path.exists(old_g_path):\n                    os.remove(old_g_path)\n                if os.path.exists(old_d_path):\n                    os.remove(old_d_path)\n                if os.path.exists(old_wav_path):\n                    shutil.rmtree(old_wav_path)\n\n            if save_wav_with_checkpoint:\n                with autocast(enabled=config.train.fp16_run):\n                    with torch.no_grad():\n                        if f0:\n                            pred_wave = net_g.infer(phone, phone_lengths, pitch, pitchf, sid)[0]\n                        else:\n                            pred_wave = net_g.infer(phone, phone_lengths, sid)[0]\n                os.makedirs(os.path.join(state_dir, f\"wav_sample_{epoch}\"), exist_ok=True)\n                for i in range(pred_wave.shape[0]):\n                    torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_true.wav\"), src=wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n                    torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_pred.wav\"), src=pred_wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n                    if augment:\n                        torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_aug.wav\"), src=aug_wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n\n            utils.save_state(\n                net_g,\n                optim_g,\n                config.train.learning_rate,\n                epoch,\n                os.path.join(state_dir, f\"G_{epoch}.pth\"),\n            )\n            utils.save_state(\n                net_d,\n                optim_d,\n                config.train.learning_rate,\n                epoch,\n                os.path.join(state_dir, f\"D_{epoch}.pth\"),\n            )\n\n            save(\n                net_g,\n                config.version,\n                sample_rate,\n                f0,\n                embedder_name,\n                embedder_out_channels,\n                embedding_output_layer,\n                os.path.join(training_dir, \"checkpoints\", f\"{model_name}-{epoch}.pth\"),\n                epoch,\n                speaker_info\n            )\n\n        scheduler_g.step()\n        scheduler_d.step()\n\n    if is_main_process:\n        print(\"Training is done. The program is closed.\")\n        save(\n            net_g,\n            config.version,\n            sample_rate,\n            f0,\n            embedder_name,\n            embedder_out_channels,\n            embedding_output_layer,\n            os.path.join(out_dir, f\"{model_name}.pth\"),\n            epoch,\n            speaker_info\n        )", ""]}
{"filename": "lib/rvc/config.py", "chunked_list": ["from typing import *\n\nfrom pydantic import BaseModel\n\n\nclass TrainConfigTrain(BaseModel):\n    log_interval: int\n    seed: int\n    epochs: int\n    learning_rate: float\n    betas: List[float]\n    eps: float\n    batch_size: int\n    fp16_run: bool\n    lr_decay: float\n    segment_size: int\n    init_lr_ratio: int\n    warmup_epochs: int\n    c_mel: int\n    c_kl: float", "\n\nclass TrainConfigData(BaseModel):\n    max_wav_value: float\n    sampling_rate: int\n    filter_length: int\n    hop_length: int\n    win_length: int\n    n_mel_channels: int\n    mel_fmin: float\n    mel_fmax: Any", "\n\nclass TrainConfigModel(BaseModel):\n    inter_channels: int\n    hidden_channels: int\n    filter_channels: int\n    n_heads: int\n    n_layers: int\n    kernel_size: int\n    p_dropout: int\n    resblock: str\n    resblock_kernel_sizes: List[int]\n    resblock_dilation_sizes: List[List[int]]\n    upsample_rates: List[int]\n    upsample_initial_channel: int\n    upsample_kernel_sizes: List[int]\n    use_spectral_norm: bool\n    gin_channels: int\n    emb_channels: int\n    spk_embed_dim: int", "\n\nclass TrainConfig(BaseModel):\n    version: Literal[\"v1\", \"v2\"] = \"v2\"\n    train: TrainConfigTrain\n    data: TrainConfigData\n    model: TrainConfigModel\n\n\nclass DatasetMetaItem(BaseModel):\n    gt_wav: str\n    co256: str\n    f0: Optional[str]\n    f0nsf: Optional[str]\n    speaker_id: int", "\nclass DatasetMetaItem(BaseModel):\n    gt_wav: str\n    co256: str\n    f0: Optional[str]\n    f0nsf: Optional[str]\n    speaker_id: int\n\n\nclass DatasetMetadata(BaseModel):\n    files: Dict[str, DatasetMetaItem]", "\nclass DatasetMetadata(BaseModel):\n    files: Dict[str, DatasetMetaItem]\n    # mute: DatasetMetaItem\n"]}
{"filename": "lib/rvc/data_utils.py", "chunked_list": ["import os\nimport traceback\n\nimport numpy as np\nimport torch\nimport torch.utils.data\n\nfrom .config import DatasetMetadata, DatasetMetaItem, TrainConfigData\nfrom .mel_processing import spectrogram_torch\nfrom .utils import load_wav_to_torch", "from .mel_processing import spectrogram_torch\nfrom .utils import load_wav_to_torch\n\n\nclass TextAudioLoader(torch.utils.data.Dataset):\n    \"\"\"\n    1) loads audio, text pairs\n    2) normalizes text and converts them to sequences of integers\n    3) computes spectrograms from audio files.\n    \"\"\"\n\n    def __init__(self, dataset_meta: DatasetMetadata, data: TrainConfigData):\n        self.dataset_meta = dataset_meta\n        self.max_wav_value = data.max_wav_value\n        self.sampling_rate = data.sampling_rate\n        self.filter_length = data.filter_length\n        self.hop_length = data.hop_length\n        self.win_length = data.win_length\n        self.sampling_rate = data.sampling_rate\n        self.min_text_len = getattr(data, \"min_text_len\", 1)\n        self.max_text_len = getattr(data, \"max_text_len\", 5000)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n        lengths = []\n        for key, data in self.dataset_meta.files.items():\n            if (\n                self.min_text_len <= len(data.co256)\n                and len(data.co256) <= self.max_text_len\n            ):\n                lengths.append(os.path.getsize(data.gt_wav) // (2 * self.hop_length))\n            else:\n                del self.dataset_meta.files[key]\n        self.lengths = lengths\n\n    def get_sid(self, sid):\n        sid = torch.LongTensor([int(sid)])\n        return sid\n\n    def get_audio_text_pair(self, data: DatasetMetaItem):\n        # separate filename and text\n        file = data.gt_wav\n        phone = data.co256\n        dv = data.speaker_id\n\n        phone = self.get_labels(phone)\n        spec, wav = self.get_audio(file)\n        dv = self.get_sid(dv)\n\n        len_phone = phone.size()[0]\n        len_spec = spec.size()[-1]\n        if len_phone != len_spec:\n            len_min = min(len_phone, len_spec)\n            len_wav = len_min * self.hop_length\n            spec = spec[:, :len_min]\n            wav = wav[:, :len_wav]\n            phone = phone[:len_min, :]\n        return (spec, wav, phone, dv)\n\n    def get_labels(self, phone):\n        phone = np.load(phone)\n        phone = np.repeat(phone, 2, axis=0)\n        n_num = min(phone.shape[0], 900)  # DistributedBucketSampler\n        phone = phone[:n_num, :]\n        phone = torch.FloatTensor(phone)\n        return phone\n\n    def get_audio(self, filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != self.sampling_rate:\n            raise ValueError(\n                \"{} SR doesn't match target {} SR\".format(\n                    sampling_rate, self.sampling_rate\n                )\n            )\n        # audio_norm = audio / self.max_wav_value\n        audio_norm = audio.unsqueeze(0)\n        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        if os.path.exists(spec_filename):\n            try:\n                spec = torch.load(spec_filename)\n            except:\n                print(spec_filename, traceback.format_exc())\n                spec = spectrogram_torch(\n                    audio_norm,\n                    self.filter_length,\n                    self.sampling_rate,\n                    self.hop_length,\n                    self.win_length,\n                    center=False,\n                )\n                spec = torch.squeeze(spec, 0)\n                torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n        else:\n            spec = spectrogram_torch(\n                audio_norm,\n                self.filter_length,\n                self.sampling_rate,\n                self.hop_length,\n                self.win_length,\n                center=False,\n            )\n            spec = torch.squeeze(spec, 0)\n            torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n        return spec, audio_norm\n\n    def __getitem__(self, index):\n        _, data = list(self.dataset_meta.files.items())[index]\n        return self.get_audio_text_pair(data)\n\n    def __len__(self):\n        return len(self.dataset_meta.files)", "\n\nclass TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):\n    \"\"\"\n    1) loads audio, text pairs\n    2) normalizes text and converts them to sequences of integers\n    3) computes spectrograms from audio files.\n    \"\"\"\n\n    def __init__(self, dataset_meta: DatasetMetadata, data: TrainConfigData):\n        self.dataset_meta = dataset_meta\n        self.max_wav_value = data.max_wav_value\n        self.sampling_rate = data.sampling_rate\n        self.filter_length = data.filter_length\n        self.hop_length = data.hop_length\n        self.win_length = data.win_length\n        self.sampling_rate = data.sampling_rate\n        self.min_text_len = getattr(data, \"min_text_len\", 1)\n        self.max_text_len = getattr(data, \"max_text_len\", 5000)\n        self._filter()\n\n    def _filter(self):\n        \"\"\"\n        Filter text & store spec lengths\n        \"\"\"\n        # Store spectrogram lengths for Bucketing\n        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n        # spec_length = wav_length // hop_length\n        lengths = []\n        for key, data in self.dataset_meta.files.items():\n            if (\n                self.min_text_len <= len(data.co256)\n                and len(data.co256) <= self.max_text_len\n            ):\n                lengths.append(os.path.getsize(data.gt_wav) // (2 * self.hop_length))\n            else:\n                del self.dataset_meta.files[key]\n        self.lengths = lengths\n\n    def get_sid(self, sid):\n        sid = torch.LongTensor([int(sid)])\n        return sid\n\n    def get_audio_text_pair(self, data: DatasetMetaItem):\n        # separate filename and text\n        file = data.gt_wav\n        phone = data.co256\n        pitch = data.f0\n        pitchf = data.f0nsf\n        dv = data.speaker_id\n\n        phone, pitch, pitchf = self.get_labels(phone, pitch, pitchf)\n        spec, wav = self.get_audio(file)\n        dv = self.get_sid(dv)\n\n        len_phone = phone.size()[0]\n        len_spec = spec.size()[-1]\n        # print(123,phone.shape,pitch.shape,spec.shape)\n        if len_phone != len_spec:\n            len_min = min(len_phone, len_spec)\n            # amor\n            len_wav = len_min * self.hop_length\n\n            spec = spec[:, :len_min]\n            wav = wav[:, :len_wav]\n\n            phone = phone[:len_min, :]\n            pitch = pitch[:len_min]\n            pitchf = pitchf[:len_min]\n\n        return (spec, wav, phone, pitch, pitchf, dv)\n\n    def get_labels(self, phone, pitch, pitchf):\n        phone = np.load(phone)\n        phone = np.repeat(phone, 2, axis=0)\n        pitch = np.load(pitch)\n        pitchf = np.load(pitchf)\n        n_num = min(phone.shape[0], 900)  # DistributedBucketSampler\n        # print(234,phone.shape,pitch.shape)\n        phone = phone[:n_num, :]\n        pitch = pitch[:n_num]\n        pitchf = pitchf[:n_num]\n        phone = torch.FloatTensor(phone)\n        pitch = torch.LongTensor(pitch)\n        pitchf = torch.FloatTensor(pitchf)\n        return phone, pitch, pitchf\n\n    def get_audio(self, filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != self.sampling_rate:\n            raise ValueError(\n                \"{} SR doesn't match target {} SR\".format(\n                    sampling_rate, self.sampling_rate\n                )\n            )\n        # audio_norm = audio / self.max_wav_value\n        audio_norm = audio.unsqueeze(0)\n        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n        if os.path.exists(spec_filename):\n            try:\n                spec = torch.load(spec_filename)\n            except:\n                print(spec_filename, traceback.format_exc())\n                spec = spectrogram_torch(\n                    audio_norm,\n                    self.filter_length,\n                    self.sampling_rate,\n                    self.hop_length,\n                    self.win_length,\n                    center=False,\n                )\n                spec = torch.squeeze(spec, 0)\n                torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n        else:\n            spec = spectrogram_torch(\n                audio_norm,\n                self.filter_length,\n                self.sampling_rate,\n                self.hop_length,\n                self.win_length,\n                center=False,\n            )\n            spec = torch.squeeze(spec, 0)\n            torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n        return spec, audio_norm\n\n    def __getitem__(self, index):\n        _, data = list(self.dataset_meta.files.items())[index]\n        return self.get_audio_text_pair(data)\n\n    def __len__(self):\n        return len(self.dataset_meta.files)", "\n\nclass TextAudioCollateMultiNSFsid:\n    \"\"\"Zero-pads model inputs and targets\"\"\"\n\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[0].size(1) for x in batch]), dim=0, descending=True\n        )\n\n        max_spec_len = max([x[0].size(1) for x in batch])\n        max_wave_len = max([x[1].size(1) for x in batch])\n        spec_lengths = torch.LongTensor(len(batch))\n        wave_lengths = torch.LongTensor(len(batch))\n        spec_padded = torch.FloatTensor(len(batch), batch[0][0].size(0), max_spec_len)\n        wave_padded = torch.FloatTensor(len(batch), 1, max_wave_len)\n        spec_padded.zero_()\n        wave_padded.zero_()\n\n        max_phone_len = max([x[2].size(0) for x in batch])\n        phone_lengths = torch.LongTensor(len(batch))\n        phone_padded = torch.FloatTensor(\n            len(batch), max_phone_len, batch[0][2].shape[1]\n        )  # (spec, wav, phone, pitch)\n        pitch_padded = torch.LongTensor(len(batch), max_phone_len)\n        pitchf_padded = torch.FloatTensor(len(batch), max_phone_len)\n        phone_padded.zero_()\n        pitch_padded.zero_()\n        pitchf_padded.zero_()\n        # dv = torch.FloatTensor(len(batch), 256)#gin=256\n        sid = torch.LongTensor(len(batch))\n\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            spec = row[0]\n            spec_padded[i, :, : spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wave = row[1]\n            wave_padded[i, :, : wave.size(1)] = wave\n            wave_lengths[i] = wave.size(1)\n\n            phone = row[2]\n            phone_padded[i, : phone.size(0), :] = phone\n            phone_lengths[i] = phone.size(0)\n\n            pitch = row[3]\n            pitch_padded[i, : pitch.size(0)] = pitch\n            pitchf = row[4]\n            pitchf_padded[i, : pitchf.size(0)] = pitchf\n\n            # dv[i] = row[5]\n            sid[i] = row[5]\n\n        return (\n            phone_padded,\n            phone_lengths,\n            pitch_padded,\n            pitchf_padded,\n            spec_padded,\n            spec_lengths,\n            wave_padded,\n            wave_lengths,\n            # dv\n            sid,\n        )", "\n\nclass TextAudioCollate:\n    \"\"\"Zero-pads model inputs and targets\"\"\"\n\n    def __init__(self, return_ids=False):\n        self.return_ids = return_ids\n\n    def __call__(self, batch):\n        \"\"\"Collate's training batch from normalized text and aduio\n        PARAMS\n        ------\n        batch: [text_normalized, spec_normalized, wav_normalized]\n        \"\"\"\n        # Right zero-pad all one-hot text sequences to max input length\n        _, ids_sorted_decreasing = torch.sort(\n            torch.LongTensor([x[0].size(1) for x in batch]), dim=0, descending=True\n        )\n\n        max_spec_len = max([x[0].size(1) for x in batch])\n        max_wave_len = max([x[1].size(1) for x in batch])\n        spec_lengths = torch.LongTensor(len(batch))\n        wave_lengths = torch.LongTensor(len(batch))\n        spec_padded = torch.FloatTensor(len(batch), batch[0][0].size(0), max_spec_len)\n        wave_padded = torch.FloatTensor(len(batch), 1, max_wave_len)\n        spec_padded.zero_()\n        wave_padded.zero_()\n\n        max_phone_len = max([x[2].size(0) for x in batch])\n        phone_lengths = torch.LongTensor(len(batch))\n        phone_padded = torch.FloatTensor(\n            len(batch), max_phone_len, batch[0][2].shape[1]\n        )\n        phone_padded.zero_()\n        sid = torch.LongTensor(len(batch))\n\n        for i in range(len(ids_sorted_decreasing)):\n            row = batch[ids_sorted_decreasing[i]]\n\n            spec = row[0]\n            spec_padded[i, :, : spec.size(1)] = spec\n            spec_lengths[i] = spec.size(1)\n\n            wave = row[1]\n            wave_padded[i, :, : wave.size(1)] = wave\n            wave_lengths[i] = wave.size(1)\n\n            phone = row[2]\n            phone_padded[i, : phone.size(0), :] = phone\n            phone_lengths[i] = phone.size(0)\n\n            sid[i] = row[3]\n\n        return (\n            phone_padded,\n            phone_lengths,\n            spec_padded,\n            spec_lengths,\n            wave_padded,\n            wave_lengths,\n            sid,\n        )", "\n\nclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n    \"\"\"\n    Maintain similar input lengths in a batch.\n    Length groups are specified by boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n\n    It removes samples which are not included in the boundaries.\n    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        batch_size,\n        boundaries,\n        num_replicas=None,\n        rank=None,\n        shuffle=True,\n    ):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n        self.lengths = dataset.lengths\n        self.batch_size = batch_size\n        self.boundaries = boundaries\n\n        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n        self.total_size = sum(self.num_samples_per_bucket)\n        self.num_samples = self.total_size // self.num_replicas\n\n    def _create_buckets(self):\n        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n        for i in range(len(self.lengths)):\n            length = self.lengths[i]\n            idx_bucket = self._bisect(length)\n            if idx_bucket != -1:\n                buckets[idx_bucket].append(i)\n\n        for i in range(len(buckets) - 1, -1, -1):  #\n            if len(buckets[i]) == 0:\n                buckets.pop(i)\n                self.boundaries.pop(i + 1)\n\n        num_samples_per_bucket = []\n        for i in range(len(buckets)):\n            len_bucket = len(buckets[i])\n            total_batch_size = self.num_replicas * self.batch_size\n            rem = (\n                total_batch_size - (len_bucket % total_batch_size)\n            ) % total_batch_size\n            num_samples_per_bucket.append(len_bucket + rem)\n        return buckets, num_samples_per_bucket\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        if self.shuffle:\n            for bucket in self.buckets:\n                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n        else:\n            for bucket in self.buckets:\n                indices.append(list(range(len(bucket))))\n\n        batches = []\n        for i in range(len(self.buckets)):\n            bucket = self.buckets[i]\n            len_bucket = len(bucket)\n            ids_bucket = indices[i]\n            num_samples_bucket = self.num_samples_per_bucket[i]\n\n            # add extra samples to make it evenly divisible\n            rem = num_samples_bucket - len_bucket\n            ids_bucket = (\n                ids_bucket\n                + ids_bucket * (rem // len_bucket)\n                + ids_bucket[: (rem % len_bucket)]\n            )\n\n            # subsample\n            ids_bucket = ids_bucket[self.rank :: self.num_replicas]\n\n            # batching\n            for j in range(len(ids_bucket) // self.batch_size):\n                batch = [\n                    bucket[idx]\n                    for idx in ids_bucket[\n                        j * self.batch_size : (j + 1) * self.batch_size\n                    ]\n                ]\n                batches.append(batch)\n\n        if self.shuffle:\n            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n            batches = [batches[i] for i in batch_ids]\n        self.batches = batches\n\n        assert len(self.batches) * self.batch_size == self.num_samples\n        return iter(self.batches)\n\n    def _bisect(self, x, lo=0, hi=None):\n        if hi is None:\n            hi = len(self.boundaries) - 1\n\n        if hi > lo:\n            mid = (hi + lo) // 2\n            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\n                return mid\n            elif x <= self.boundaries[mid]:\n                return self._bisect(x, lo, mid)\n            else:\n                return self._bisect(x, mid + 1, hi)\n        else:\n            return -1\n\n    def __len__(self):\n        return self.num_samples // self.batch_size", ""]}
{"filename": "lib/rvc/models.py", "chunked_list": ["import math\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d, Conv2d, ConvTranspose1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\n\nfrom . import attentions, commons, modules", "\nfrom . import attentions, commons, modules\nfrom .commons import get_padding, init_weights\n\n\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        out_channels: int,\n        hidden_channels: int,\n        filter_channels: int,\n        emb_channels: int,\n        n_heads: int,\n        n_layers: int,\n        kernel_size: int,\n        p_dropout: int,\n        f0: bool = True,\n    ):\n        super().__init__()\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.emb_channels = emb_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.emb_phone = nn.Linear(emb_channels, hidden_channels)\n        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n        if f0 == True:\n            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n        self.encoder = attentions.Encoder(\n            hidden_channels, filter_channels, n_heads, n_layers, kernel_size, p_dropout\n        )\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, phone, pitch, lengths):\n        if pitch == None:\n            x = self.emb_phone(phone)\n        else:\n            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n        x = self.lrelu(x)\n        x = torch.transpose(x, 1, -1)  # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n            x.dtype\n        )\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return m, logs, x_mask", "\n\nclass ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(\n                modules.ResidualCouplingLayer(\n                    channels,\n                    hidden_channels,\n                    kernel_size,\n                    dilation_rate,\n                    n_layers,\n                    gin_channels=gin_channels,\n                    mean_only=True,\n                )\n            )\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n\n    def remove_weight_norm(self):\n        for i in range(self.n_flows):\n            self.flows[i * 2].remove_weight_norm()", "\n\nclass PosteriorEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.gin_channels = gin_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = modules.WN(\n            hidden_channels,\n            kernel_size,\n            dilation_rate,\n            n_layers,\n            gin_channels=gin_channels,\n        )\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(\n            x.dtype\n        )\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n        return z, m, logs, x_mask\n\n    def remove_weight_norm(self):\n        self.enc.remove_weight_norm()", "\n\nclass Generator(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        gin_channels=0,\n    ):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(\n            initial_channel, upsample_initial_channel, 7, 1, padding=3\n        )\n        resblock = modules.ResBlock1 if resblock == \"1\" else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for j, (k, d) in enumerate(\n                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n            ):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n            x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()", "\n\nclass SineGen(torch.nn.Module):\n    \"\"\"Definition of sine generator\n    SineGen(samp_rate, harmonic_num = 0,\n            sine_amp = 0.1, noise_std = 0.003,\n            voiced_threshold = 0,\n            flag_for_pulse=False)\n    samp_rate: sampling rate in Hz\n    harmonic_num: number of harmonic overtones (default 0)\n    sine_amp: amplitude of sine-wavefrom (default 0.1)\n    noise_std: std of Gaussian noise (default 0.003)\n    voiced_thoreshold: F0 threshold for U/V classification (default 0)\n    flag_for_pulse: this SinGen is used inside PulseGen (default False)\n    Note: when flag_for_pulse is True, the first time step of a voiced\n        segment is always sin(np.pi) or cos(0)\n    \"\"\"\n\n    def __init__(\n        self,\n        samp_rate,\n        harmonic_num=0,\n        sine_amp=0.1,\n        noise_std=0.003,\n        voiced_threshold=0,\n        flag_for_pulse=False,\n    ):\n        super(SineGen, self).__init__()\n        self.sine_amp = sine_amp\n        self.noise_std = noise_std\n        self.harmonic_num = harmonic_num\n        self.dim = self.harmonic_num + 1\n        self.sampling_rate = samp_rate\n        self.voiced_threshold = voiced_threshold\n\n    def _f02uv(self, f0):\n        # generate uv signal\n        uv = torch.ones_like(f0)\n        uv = uv * (f0 > self.voiced_threshold)\n        return uv\n\n    def forward(self, f0, upp):\n        \"\"\"sine_tensor, uv = forward(f0)\n        input F0: tensor(batchsize=1, length, dim=1)\n                  f0 for unvoiced steps should be 0\n        output sine_tensor: tensor(batchsize=1, length, dim)\n        output uv: tensor(batchsize=1, length, 1)\n        \"\"\"\n        with torch.no_grad():\n            f0 = f0[:, None].transpose(1, 2)\n            f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim, device=f0.device)\n            # fundamental component\n            f0_buf[:, :, 0] = f0[:, :, 0]\n            for idx in np.arange(self.harmonic_num):\n                f0_buf[:, :, idx + 1] = f0_buf[:, :, 0] * (\n                    idx + 2\n                )  # idx + 2: the (idx+1)-th overtone, (idx+2)-th harmonic\n            rad_values = (f0_buf / self.sampling_rate) % 1  ###%1\u610f\u5473\u7740n_har\u7684\u4e58\u79ef\u65e0\u6cd5\u540e\u5904\u7406\u4f18\u5316\n            rand_ini = torch.rand(\n                f0_buf.shape[0], f0_buf.shape[2], device=f0_buf.device\n            )\n            rand_ini[:, 0] = 0\n            rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini\n            tmp_over_one = torch.cumsum(rad_values, 1)  # % 1  #####%1\u610f\u5473\u7740\u540e\u9762\u7684cumsum\u65e0\u6cd5\u518d\u4f18\u5316\n            tmp_over_one *= upp\n            tmp_over_one = F.interpolate(\n                tmp_over_one.transpose(2, 1),\n                scale_factor=upp,\n                mode=\"linear\",\n                align_corners=True,\n            ).transpose(2, 1)\n            rad_values = F.interpolate(\n                rad_values.transpose(2, 1), scale_factor=upp, mode=\"nearest\"\n            ).transpose(\n                2, 1\n            )  #######\n            tmp_over_one %= 1\n            tmp_over_one_idx = (tmp_over_one[:, 1:, :] - tmp_over_one[:, :-1, :]) < 0\n            cumsum_shift = torch.zeros_like(rad_values)\n            cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0\n            sine_waves = torch.sin(\n                torch.cumsum(rad_values + cumsum_shift, dim=1) * 2 * np.pi\n            )\n            sine_waves = sine_waves * self.sine_amp\n            uv = self._f02uv(f0)\n            uv = F.interpolate(\n                uv.transpose(2, 1), scale_factor=upp, mode=\"nearest\"\n            ).transpose(2, 1)\n            noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3\n            noise = noise_amp * torch.randn_like(sine_waves)\n            sine_waves = sine_waves * uv + noise\n        return sine_waves, uv, noise", "\n\nclass SourceModuleHnNSF(torch.nn.Module):\n    \"\"\"SourceModule for hn-nsf\n    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n                 add_noise_std=0.003, voiced_threshod=0)\n    sampling_rate: sampling_rate in Hz\n    harmonic_num: number of harmonic above F0 (default: 0)\n    sine_amp: amplitude of sine source signal (default: 0.1)\n    add_noise_std: std of additive Gaussian noise (default: 0.003)\n        note that amplitude of noise in unvoiced is decided\n        by sine_amp\n    voiced_threshold: threhold to set U/V given F0 (default: 0)\n    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n    F0_sampled (batchsize, length, 1)\n    Sine_source (batchsize, length, 1)\n    noise_source (batchsize, length 1)\n    uv (batchsize, length, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        sampling_rate,\n        harmonic_num=0,\n        sine_amp=0.1,\n        add_noise_std=0.003,\n        voiced_threshod=0,\n        is_half=True,\n    ):\n        super(SourceModuleHnNSF, self).__init__()\n\n        self.sine_amp = sine_amp\n        self.noise_std = add_noise_std\n        self.is_half = is_half\n        # to produce sine waveforms\n        self.l_sin_gen = SineGen(\n            sampling_rate, harmonic_num, sine_amp, add_noise_std, voiced_threshod\n        )\n\n        # to merge source harmonics into a single excitation\n        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)\n        self.l_tanh = torch.nn.Tanh()\n\n    def forward(self, x, upp=None):\n        sine_wavs, uv, _ = self.l_sin_gen(x, upp)\n        if self.is_half == True:\n            sine_wavs = sine_wavs.half()\n        sine_merge = self.l_tanh(self.l_linear(sine_wavs))\n        return sine_merge, None, None  # noise, uv", "\n\nclass GeneratorNSF(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        gin_channels,\n        sr,\n        is_half=False,\n    ):\n        super(GeneratorNSF, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n\n        self.f0_upsamp = torch.nn.Upsample(scale_factor=np.prod(upsample_rates))\n        self.m_source = SourceModuleHnNSF(\n            sampling_rate=sr, harmonic_num=0, is_half=is_half\n        )\n        self.noise_convs = nn.ModuleList()\n        self.conv_pre = Conv1d(\n            initial_channel, upsample_initial_channel, 7, 1, padding=3\n        )\n        resblock = modules.ResBlock1 if resblock == \"1\" else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            c_cur = upsample_initial_channel // (2 ** (i + 1))\n            self.ups.append(\n                weight_norm(\n                    ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n            if i + 1 < len(upsample_rates):\n                stride_f0 = np.prod(upsample_rates[i + 1 :])\n                self.noise_convs.append(\n                    Conv1d(\n                        1,\n                        c_cur,\n                        kernel_size=stride_f0 * 2,\n                        stride=stride_f0,\n                        padding=stride_f0 // 2,\n                    )\n                )\n            else:\n                self.noise_convs.append(Conv1d(1, c_cur, kernel_size=1))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for j, (k, d) in enumerate(\n                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n            ):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n        self.upp = np.prod(upsample_rates)\n\n    def forward(self, x, f0, g=None):\n        har_source, noi_source, uv = self.m_source(f0, self.upp)\n        har_source = har_source.transpose(1, 2)\n        x = self.conv_pre(x)\n        if g is not None:\n            x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            x_source = self.noise_convs[i](har_source)\n            x = x + x_source\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()", "\n\nsr2sr = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}\n\n\nclass SynthesizerTrnMs256NSFSid(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        spk_embed_dim,\n        gin_channels,\n        emb_channels,\n        sr,\n        **kwargs\n    ):\n        super().__init__()\n        if type(sr) == type(\"strr\"):\n            sr = sr2sr[sr]\n        self.spec_channels = spec_channels\n        self.inter_channels = inter_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.resblock = resblock\n        self.resblock_kernel_sizes = resblock_kernel_sizes\n        self.resblock_dilation_sizes = resblock_dilation_sizes\n        self.upsample_rates = upsample_rates\n        self.upsample_initial_channel = upsample_initial_channel\n        self.upsample_kernel_sizes = upsample_kernel_sizes\n        self.segment_size = segment_size\n        self.gin_channels = gin_channels\n        self.emb_channels = emb_channels\n        self.sr = sr\n        # self.hop_length = hop_length#\n        self.spk_embed_dim = spk_embed_dim\n        self.enc_p = TextEncoder(\n            inter_channels,\n            hidden_channels,\n            filter_channels,\n            emb_channels,\n            n_heads,\n            n_layers,\n            kernel_size,\n            p_dropout,\n        )\n        self.dec = GeneratorNSF(\n            inter_channels,\n            resblock,\n            resblock_kernel_sizes,\n            resblock_dilation_sizes,\n            upsample_rates,\n            upsample_initial_channel,\n            upsample_kernel_sizes,\n            gin_channels=gin_channels,\n            sr=sr,\n            is_half=kwargs[\"is_half\"],\n        )\n        self.enc_q = PosteriorEncoder(\n            spec_channels,\n            inter_channels,\n            hidden_channels,\n            5,\n            1,\n            16,\n            gin_channels=gin_channels,\n        )\n        self.flow = ResidualCouplingBlock(\n            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n        )\n        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n        print(\n            \"gin_channels:\",\n            gin_channels,\n            \"self.spk_embed_dim:\",\n            self.spk_embed_dim,\n            \"emb_channels:\",\n            emb_channels,\n        )\n\n    def remove_weight_norm(self):\n        self.dec.remove_weight_norm()\n        self.flow.remove_weight_norm()\n        self.enc_q.remove_weight_norm()\n\n    def forward(\n        self, phone, phone_lengths, pitch, pitchf, y, y_lengths, ds\n    ):  # \u8fd9\u91ccds\u662fid\uff0c[bs,1]\n        # print(1,pitch.shape)#[bs,t]\n        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1\u662ft\uff0c\u5e7f\u64ad\u7684\n        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n        z_p = self.flow(z, y_mask, g=g)\n        z_slice, ids_slice = commons.rand_slice_segments(\n            z, y_lengths, self.segment_size\n        )\n        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n        # print(-2,pitchf.shape,z_slice.shape)\n        o = self.dec(z_slice, pitchf, g=g)\n        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n    def infer(self, phone, phone_lengths, pitch, nsff0, sid, max_len=None):\n        g = self.emb_g(sid).unsqueeze(-1)\n        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n        z = self.flow(z_p, x_mask, g=g, reverse=True)\n        o = self.dec((z * x_mask)[:, :, :max_len], nsff0, g=g)\n        return o, x_mask, (z, z_p, m_p, logs_p)", "\nclass SynthesizerTrnMs256NSFSid(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        spk_embed_dim,\n        gin_channels,\n        emb_channels,\n        sr,\n        **kwargs\n    ):\n        super().__init__()\n        if type(sr) == type(\"strr\"):\n            sr = sr2sr[sr]\n        self.spec_channels = spec_channels\n        self.inter_channels = inter_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.resblock = resblock\n        self.resblock_kernel_sizes = resblock_kernel_sizes\n        self.resblock_dilation_sizes = resblock_dilation_sizes\n        self.upsample_rates = upsample_rates\n        self.upsample_initial_channel = upsample_initial_channel\n        self.upsample_kernel_sizes = upsample_kernel_sizes\n        self.segment_size = segment_size\n        self.gin_channels = gin_channels\n        self.emb_channels = emb_channels\n        self.sr = sr\n        # self.hop_length = hop_length#\n        self.spk_embed_dim = spk_embed_dim\n        self.enc_p = TextEncoder(\n            inter_channels,\n            hidden_channels,\n            filter_channels,\n            emb_channels,\n            n_heads,\n            n_layers,\n            kernel_size,\n            p_dropout,\n        )\n        self.dec = GeneratorNSF(\n            inter_channels,\n            resblock,\n            resblock_kernel_sizes,\n            resblock_dilation_sizes,\n            upsample_rates,\n            upsample_initial_channel,\n            upsample_kernel_sizes,\n            gin_channels=gin_channels,\n            sr=sr,\n            is_half=kwargs[\"is_half\"],\n        )\n        self.enc_q = PosteriorEncoder(\n            spec_channels,\n            inter_channels,\n            hidden_channels,\n            5,\n            1,\n            16,\n            gin_channels=gin_channels,\n        )\n        self.flow = ResidualCouplingBlock(\n            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n        )\n        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n        print(\n            \"gin_channels:\",\n            gin_channels,\n            \"self.spk_embed_dim:\",\n            self.spk_embed_dim,\n            \"emb_channels:\",\n            emb_channels,\n        )\n\n    def remove_weight_norm(self):\n        self.dec.remove_weight_norm()\n        self.flow.remove_weight_norm()\n        self.enc_q.remove_weight_norm()\n\n    def forward(\n        self, phone, phone_lengths, pitch, pitchf, y, y_lengths, ds\n    ):  # \u8fd9\u91ccds\u662fid\uff0c[bs,1]\n        # print(1,pitch.shape)#[bs,t]\n        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1\u662ft\uff0c\u5e7f\u64ad\u7684\n        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n        z_p = self.flow(z, y_mask, g=g)\n        z_slice, ids_slice = commons.rand_slice_segments(\n            z, y_lengths, self.segment_size\n        )\n        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n        # print(-2,pitchf.shape,z_slice.shape)\n        o = self.dec(z_slice, pitchf, g=g)\n        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n    def infer(self, phone, phone_lengths, pitch, nsff0, sid, max_len=None):\n        g = self.emb_g(sid).unsqueeze(-1)\n        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n        z = self.flow(z_p, x_mask, g=g, reverse=True)\n        o = self.dec((z * x_mask)[:, :, :max_len], nsff0, g=g)\n        return o, x_mask, (z, z_p, m_p, logs_p)", "\n\nclass SynthesizerTrnMs256NSFSidNono(nn.Module):\n    def __init__(\n        self,\n        spec_channels,\n        segment_size,\n        inter_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,\n        spk_embed_dim,\n        gin_channels,\n        emb_channels,\n        sr=None,\n        **kwargs\n    ):\n        super().__init__()\n        self.spec_channels = spec_channels\n        self.inter_channels = inter_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.resblock = resblock\n        self.resblock_kernel_sizes = resblock_kernel_sizes\n        self.resblock_dilation_sizes = resblock_dilation_sizes\n        self.upsample_rates = upsample_rates\n        self.upsample_initial_channel = upsample_initial_channel\n        self.upsample_kernel_sizes = upsample_kernel_sizes\n        self.segment_size = segment_size\n        self.gin_channels = gin_channels\n        self.emb_channels = emb_channels\n        self.sr = sr\n        # self.hop_length = hop_length#\n        self.spk_embed_dim = spk_embed_dim\n        self.enc_p = TextEncoder(\n            inter_channels,\n            hidden_channels,\n            filter_channels,\n            emb_channels,\n            n_heads,\n            n_layers,\n            kernel_size,\n            p_dropout,\n            f0=False,\n        )\n        self.dec = Generator(\n            inter_channels,\n            resblock,\n            resblock_kernel_sizes,\n            resblock_dilation_sizes,\n            upsample_rates,\n            upsample_initial_channel,\n            upsample_kernel_sizes,\n            gin_channels=gin_channels,\n        )\n        self.enc_q = PosteriorEncoder(\n            spec_channels,\n            inter_channels,\n            hidden_channels,\n            5,\n            1,\n            16,\n            gin_channels=gin_channels,\n        )\n        self.flow = ResidualCouplingBlock(\n            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n        )\n        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n        print(\n            \"gin_channels:\",\n            gin_channels,\n            \"self.spk_embed_dim:\",\n            self.spk_embed_dim,\n            \"emb_channels:\",\n            emb_channels,\n        )\n\n    def remove_weight_norm(self):\n        self.dec.remove_weight_norm()\n        self.flow.remove_weight_norm()\n        self.enc_q.remove_weight_norm()\n\n    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # \u8fd9\u91ccds\u662fid\uff0c[bs,1]\n        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1\u662ft\uff0c\u5e7f\u64ad\u7684\n        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n        z_p = self.flow(z, y_mask, g=g)\n        z_slice, ids_slice = commons.rand_slice_segments(\n            z, y_lengths, self.segment_size\n        )\n        o = self.dec(z_slice, g=g)\n        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\n    def infer(self, phone, phone_lengths, sid, max_len=None):\n        g = self.emb_g(sid).unsqueeze(-1)\n        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n        z = self.flow(z_p, x_mask, g=g, reverse=True)\n        o = self.dec((z * x_mask)[:, :, :max_len], g=g)\n        return o, x_mask, (z, z_p, m_p, logs_p)", "\n\nclass DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n            ]\n        )\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap", "\n\nclass DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(\n                    Conv2d(\n                        1,\n                        32,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(get_padding(kernel_size, 1), 0),\n                    )\n                ),\n                norm_f(\n                    Conv2d(\n                        32,\n                        128,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(get_padding(kernel_size, 1), 0),\n                    )\n                ),\n                norm_f(\n                    Conv2d(\n                        128,\n                        512,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(get_padding(kernel_size, 1), 0),\n                    )\n                ),\n                norm_f(\n                    Conv2d(\n                        512,\n                        1024,\n                        (kernel_size, 1),\n                        (stride, 1),\n                        padding=(get_padding(kernel_size, 1), 0),\n                    )\n                ),\n                norm_f(\n                    Conv2d(\n                        1024,\n                        1024,\n                        (kernel_size, 1),\n                        1,\n                        padding=(get_padding(kernel_size, 1), 0),\n                    )\n                ),\n            ]\n        )\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0:  # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap", "\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False, periods=[2, 3, 5, 7, 11, 17]):\n        super(MultiPeriodDiscriminator, self).__init__()\n\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [\n            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n        ]\n        self.discriminators = nn.ModuleList(discs)\n\n    def forward(self, y, y_hat):\n        y_d_rs = []  #\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            # for j in range(len(fmap_r)):\n            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n            y_d_rs.append(y_d_r)\n            y_d_gs.append(y_d_g)\n            fmap_rs.append(fmap_r)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs", ""]}
{"filename": "lib/rvc/pipeline.py", "chunked_list": ["import os\nimport traceback\nfrom typing import *\n\nimport faiss\nimport numpy as np\nimport pyworld\nimport scipy.signal as signal\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport torchcrepe\nfrom torch import Tensor\n# from faiss.swigfaiss_avx2 import IndexIVFFlat # cause crash on windows' faiss-cpu installed from pip\nfrom fairseq.models.hubert import HubertModel\n\nfrom .models import SynthesizerTrnMs256NSFSid\n\n\nclass VocalConvertPipeline(object):\n    def __init__(self, tgt_sr: int, device: Union[str, torch.device], is_half: bool):\n        if isinstance(device, str):\n            device = torch.device(device)\n        if device.type == \"cuda\":\n            vram = torch.cuda.get_device_properties(device).total_memory / 1024**3\n        else:\n            vram = None\n\n        if vram is not None and vram <= 4:\n            self.x_pad = 1\n            self.x_query = 5\n            self.x_center = 30\n            self.x_max = 32\n        elif vram is not None and vram <= 5:\n            self.x_pad = 1\n            self.x_query = 6\n            self.x_center = 38\n            self.x_max = 41\n        else:\n            self.x_pad = 3\n            self.x_query = 10\n            self.x_center = 60\n            self.x_max = 65\n\n        self.sr = 16000  # hubert input sample rate\n        self.window = 160  # hubert input window\n        self.t_pad = self.sr * self.x_pad  # padding time for each utterance\n        self.t_pad_tgt = tgt_sr * self.x_pad\n        self.t_pad2 = self.t_pad * 2\n        self.t_query = self.sr * self.x_query  # query time before and after query point\n        self.t_center = self.sr * self.x_center  # query cut point position\n        self.t_max = self.sr * self.x_max  # max time for no query\n        self.device = device\n        self.is_half = is_half\n\n    def get_optimal_torch_device(self, index: int = 0) -> torch.device:\n        # Get cuda device\n        if torch.cuda.is_available():\n            return torch.device(f\"cuda:{index % torch.cuda.device_count()}\") # Very fast\n        elif torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        # Insert an else here to grab \"xla\" devices if available. TO DO later. Requires the torch_xla.core.xla_model library\n        # Else wise return the \"cpu\" as a torch device, \n        return torch.device(\"cpu\")\n\n    def get_f0_crepe_computation(\n            self, \n            x, \n            f0_min,\n            f0_max,\n            p_len,\n            hop_length=64, # 512 before. Hop length changes the speed that the voice jumps to a different dramatic pitch. Lower hop lengths means more pitch accuracy but longer inference time.\n            model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n    ):\n        x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n        x /= np.quantile(np.abs(x), 0.999)\n        torch_device = self.get_optimal_torch_device()\n        audio = torch.from_numpy(x).to(torch_device, copy=True)\n        audio = torch.unsqueeze(audio, dim=0)\n        if audio.ndim == 2 and audio.shape[0] > 1:\n            audio = torch.mean(audio, dim=0, keepdim=True).detach()\n        audio = audio.detach()\n        print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n        pitch: Tensor = torchcrepe.predict(\n            audio,\n            self.sr,\n            hop_length,\n            f0_min,\n            f0_max,\n            model,\n            batch_size=hop_length * 2,\n            device=torch_device,\n            pad=True\n        )\n        p_len = p_len or x.shape[0] // hop_length\n        # Resize the pitch for final f0\n        source = np.array(pitch.squeeze(0).cpu().float().numpy())\n        source[source < 0.001] = np.nan\n        target = np.interp(\n            np.arange(0, len(source) * p_len, len(source)) / p_len,\n            np.arange(0, len(source)),\n            source\n        )\n        f0 = np.nan_to_num(target)\n        return f0 # Resized f0\n\n    def get_f0_official_crepe_computation(\n            self,\n            x,\n            f0_min,\n            f0_max,\n            model=\"full\",\n    ):\n        # Pick a batch size that doesn't cause memory errors on your gpu\n        batch_size = 512\n        # Compute pitch using first gpu\n        audio = torch.tensor(np.copy(x))[None].float()\n        f0, pd = torchcrepe.predict(\n            audio,\n            self.sr,\n            self.window,\n            f0_min,\n            f0_max,\n            model,\n            batch_size=batch_size,\n            device=self.device,\n            return_periodicity=True,\n        )\n        pd = torchcrepe.filter.median(pd, 3)\n        f0 = torchcrepe.filter.mean(f0, 3)\n        f0[pd < 0.1] = 0\n        f0 = f0[0].cpu().numpy()\n        return f0\n\n    def get_f0(\n        self,\n        x: np.ndarray,\n        p_len: int,\n        f0_up_key: int,\n        f0_method: str,\n        inp_f0: np.ndarray = None,\n    ):\n        f0_min = 50\n        f0_max = 1100\n        f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n        f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\n        if f0_method == \"harvest\":\n            f0, t = pyworld.harvest(\n                x.astype(np.double),\n                fs=self.sr,\n                f0_ceil=f0_max,\n                f0_floor=f0_min,\n                frame_period=10,\n            )\n            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n            f0 = signal.medfilt(f0, 3)\n        elif f0_method == \"dio\":\n            f0, t = pyworld.dio(\n                x.astype(np.double),\n                fs=self.sr,\n                f0_ceil=f0_max,\n                f0_floor=f0_min,\n                frame_period=10,\n            )\n            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n            f0 = signal.medfilt(f0, 3)\n        elif f0_method == \"mangio-crepe\":\n            f0 = self.get_f0_crepe_computation(x, f0_min, f0_max, p_len, 160, \"full\")\n        elif f0_method == \"crepe\":\n            f0 = self.get_f0_official_crepe_computation(x, f0_min, f0_max, \"full\")\n\n        f0 *= pow(2, f0_up_key / 12)\n        tf0 = self.sr // self.window  # f0 points per second\n        if inp_f0 is not None:\n            delta_t = np.round(\n                (inp_f0[:, 0].max() - inp_f0[:, 0].min()) * tf0 + 1\n            ).astype(\"int16\")\n            replace_f0 = np.interp(\n                list(range(delta_t)), inp_f0[:, 0] * 100, inp_f0[:, 1]\n            )\n            shape = f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)].shape[0]\n            f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)] = replace_f0[\n                :shape\n            ]\n\n        f0bak = f0.copy()\n        f0_mel = 1127 * np.log(1 + f0 / 700)\n        f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n            f0_mel_max - f0_mel_min\n        ) + 1\n        f0_mel[f0_mel <= 1] = 1\n        f0_mel[f0_mel > 255] = 255\n        f0_coarse = np.rint(f0_mel).astype(np.int)\n        return f0_coarse, f0bak  # 1-0\n\n    def _convert(\n        self,\n        model: HubertModel,\n        embedding_output_layer: int,\n        net_g: SynthesizerTrnMs256NSFSid,\n        sid: int,\n        audio: np.ndarray,\n        pitch: np.ndarray,\n        pitchf: np.ndarray,\n        index: faiss.IndexIVFFlat,\n        big_npy: np.ndarray,\n        index_rate: float,\n    ):\n        feats = torch.from_numpy(audio)\n        if self.is_half:\n            feats = feats.half()\n        else:\n            feats = feats.float()\n        if feats.dim() == 2:  # double channels\n            feats = feats.mean(-1)\n        assert feats.dim() == 1, feats.dim()\n        feats = feats.view(1, -1)\n        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n\n        half_support = (\n            self.device.type == \"cuda\"\n            and torch.cuda.get_device_capability(self.device)[0] >= 5.3\n        )\n        is_feats_dim_768 = net_g.emb_channels == 768\n\n        if isinstance(model, tuple):\n            feats = model[0](\n                feats.squeeze(0).squeeze(0).to(self.device),\n                return_tensors=\"pt\",\n                sampling_rate=16000,\n            )\n            if self.is_half:\n                feats = feats.input_values.to(self.device).half()\n            else:\n                feats = feats.input_values.to(self.device)\n            with torch.no_grad():\n                if is_feats_dim_768:\n                    feats = model[1](feats).last_hidden_state\n                else:\n                    feats = model[1](feats).extract_features\n        else:\n            inputs = {\n                \"source\": feats.half().to(self.device)\n                if half_support\n                else feats.to(self.device),\n                \"padding_mask\": padding_mask.to(self.device),\n                \"output_layer\": embedding_output_layer,\n            }\n\n            if not half_support:\n                model = model.float()\n                inputs[\"source\"] = inputs[\"source\"].float()\n\n            with torch.no_grad():\n                logits = model.extract_features(**inputs)\n                if is_feats_dim_768:\n                    feats = logits[0]\n                else:\n                    feats = model.final_proj(logits[0])\n\n        if (\n            isinstance(index, type(None)) == False\n            and isinstance(big_npy, type(None)) == False\n            and index_rate != 0\n        ):\n            npy = feats[0].cpu().numpy()\n            if self.is_half:\n                npy = npy.astype(\"float32\")\n\n            score, ix = index.search(npy, k=8)\n            weight = np.square(1 / score)\n            weight /= weight.sum(axis=1, keepdims=True)\n            npy = np.sum(big_npy[ix] * np.expand_dims(weight, axis=2), axis=1)\n\n            if self.is_half:\n                npy = npy.astype(\"float16\")\n            feats = (\n                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n                + (1 - index_rate) * feats\n            )\n\n        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n\n        p_len = audio.shape[0] // self.window\n        if feats.shape[1] < p_len:\n            p_len = feats.shape[1]\n            if pitch != None and pitchf != None:\n                pitch = pitch[:, :p_len]\n                pitchf = pitchf[:, :p_len]\n        p_len = torch.tensor([p_len], device=self.device).long()\n        with torch.no_grad():\n            if pitch != None and pitchf != None:\n                audio1 = (\n                    (net_g.infer(feats, p_len, pitch, pitchf, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n            else:\n                audio1 = (\n                    (net_g.infer(feats, p_len, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n        del feats, p_len, padding_mask\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio1\n\n    def __call__(\n        self,\n        model: HubertModel,\n        embedding_output_layer: int,\n        net_g: SynthesizerTrnMs256NSFSid,\n        sid: int,\n        audio: np.ndarray,\n        transpose: int,\n        f0_method: str,\n        file_index: str,\n        index_rate: float,\n        if_f0: bool,\n        f0_file: str = None,\n    ):\n        if file_index != \"\" and os.path.exists(file_index) and index_rate != 0:\n            try:\n                index = faiss.read_index(file_index)\n                # big_npy = np.load(file_big_npy)\n                big_npy = index.reconstruct_n(0, index.ntotal)\n            except:\n                traceback.print_exc()\n                index = big_npy = None\n        else:\n            index = big_npy = None\n\n        bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n        audio = signal.filtfilt(bh, ah, audio)\n\n        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\")\n        opt_ts = []\n        if audio_pad.shape[0] > self.t_max:\n            audio_sum = np.zeros_like(audio)\n            for i in range(self.window):\n                audio_sum += audio_pad[i : i - self.window]\n            for t in range(self.t_center, audio.shape[0], self.t_center):\n                opt_ts.append(\n                    t\n                    - self.t_query\n                    + np.where(\n                        np.abs(audio_sum[t - self.t_query : t + self.t_query])\n                        == np.abs(audio_sum[t - self.t_query : t + self.t_query]).min()\n                    )[0][0]\n                )\n\n        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\")\n        p_len = audio_pad.shape[0] // self.window\n        inp_f0 = None\n        if hasattr(f0_file, \"name\"):\n            try:\n                with open(f0_file.name, \"r\") as f:\n                    lines = f.read().strip(\"\\n\").split(\"\\n\")\n                inp_f0 = []\n                for line in lines:\n                    inp_f0.append([float(i) for i in line.split(\",\")])\n                inp_f0 = np.array(inp_f0, dtype=\"float32\")\n            except:\n                traceback.print_exc()\n        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n        pitch, pitchf = None, None\n        if if_f0 == 1:\n            pitch, pitchf = self.get_f0(audio_pad, p_len, transpose, f0_method, inp_f0)\n            pitch = pitch[:p_len]\n            pitchf = pitchf[:p_len]\n            if self.device.type == \"mps\":\n                pitchf = pitchf.astype(np.float32)\n            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n\n        audio_opt = []\n\n        s = 0\n        t = None\n\n        for t in opt_ts:\n            t = t // self.window * self.window\n            if if_f0 == 1:\n                audio_opt.append(\n                    self._convert(\n                        model,\n                        embedding_output_layer,\n                        net_g,\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n                        index,\n                        big_npy,\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            else:\n                audio_opt.append(\n                    self._convert(\n                        model,\n                        embedding_output_layer,\n                        net_g,\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        None,\n                        None,\n                        index,\n                        big_npy,\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            s = t\n        if if_f0 == 1:\n            audio_opt.append(\n                self._convert(\n                    model,\n                    embedding_output_layer,\n                    net_g,\n                    sid,\n                    audio_pad[t:],\n                    pitch[:, t // self.window :] if t is not None else pitch,\n                    pitchf[:, t // self.window :] if t is not None else pitchf,\n                    index,\n                    big_npy,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        else:\n            audio_opt.append(\n                self._convert(\n                    model,\n                    embedding_output_layer,\n                    net_g,\n                    sid,\n                    audio_pad[t:],\n                    None,\n                    None,\n                    index,\n                    big_npy,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        audio_opt = np.concatenate(audio_opt)\n        del pitch, pitchf, sid\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio_opt", "\n\nclass VocalConvertPipeline(object):\n    def __init__(self, tgt_sr: int, device: Union[str, torch.device], is_half: bool):\n        if isinstance(device, str):\n            device = torch.device(device)\n        if device.type == \"cuda\":\n            vram = torch.cuda.get_device_properties(device).total_memory / 1024**3\n        else:\n            vram = None\n\n        if vram is not None and vram <= 4:\n            self.x_pad = 1\n            self.x_query = 5\n            self.x_center = 30\n            self.x_max = 32\n        elif vram is not None and vram <= 5:\n            self.x_pad = 1\n            self.x_query = 6\n            self.x_center = 38\n            self.x_max = 41\n        else:\n            self.x_pad = 3\n            self.x_query = 10\n            self.x_center = 60\n            self.x_max = 65\n\n        self.sr = 16000  # hubert input sample rate\n        self.window = 160  # hubert input window\n        self.t_pad = self.sr * self.x_pad  # padding time for each utterance\n        self.t_pad_tgt = tgt_sr * self.x_pad\n        self.t_pad2 = self.t_pad * 2\n        self.t_query = self.sr * self.x_query  # query time before and after query point\n        self.t_center = self.sr * self.x_center  # query cut point position\n        self.t_max = self.sr * self.x_max  # max time for no query\n        self.device = device\n        self.is_half = is_half\n\n    def get_optimal_torch_device(self, index: int = 0) -> torch.device:\n        # Get cuda device\n        if torch.cuda.is_available():\n            return torch.device(f\"cuda:{index % torch.cuda.device_count()}\") # Very fast\n        elif torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        # Insert an else here to grab \"xla\" devices if available. TO DO later. Requires the torch_xla.core.xla_model library\n        # Else wise return the \"cpu\" as a torch device, \n        return torch.device(\"cpu\")\n\n    def get_f0_crepe_computation(\n            self, \n            x, \n            f0_min,\n            f0_max,\n            p_len,\n            hop_length=64, # 512 before. Hop length changes the speed that the voice jumps to a different dramatic pitch. Lower hop lengths means more pitch accuracy but longer inference time.\n            model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n    ):\n        x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n        x /= np.quantile(np.abs(x), 0.999)\n        torch_device = self.get_optimal_torch_device()\n        audio = torch.from_numpy(x).to(torch_device, copy=True)\n        audio = torch.unsqueeze(audio, dim=0)\n        if audio.ndim == 2 and audio.shape[0] > 1:\n            audio = torch.mean(audio, dim=0, keepdim=True).detach()\n        audio = audio.detach()\n        print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n        pitch: Tensor = torchcrepe.predict(\n            audio,\n            self.sr,\n            hop_length,\n            f0_min,\n            f0_max,\n            model,\n            batch_size=hop_length * 2,\n            device=torch_device,\n            pad=True\n        )\n        p_len = p_len or x.shape[0] // hop_length\n        # Resize the pitch for final f0\n        source = np.array(pitch.squeeze(0).cpu().float().numpy())\n        source[source < 0.001] = np.nan\n        target = np.interp(\n            np.arange(0, len(source) * p_len, len(source)) / p_len,\n            np.arange(0, len(source)),\n            source\n        )\n        f0 = np.nan_to_num(target)\n        return f0 # Resized f0\n\n    def get_f0_official_crepe_computation(\n            self,\n            x,\n            f0_min,\n            f0_max,\n            model=\"full\",\n    ):\n        # Pick a batch size that doesn't cause memory errors on your gpu\n        batch_size = 512\n        # Compute pitch using first gpu\n        audio = torch.tensor(np.copy(x))[None].float()\n        f0, pd = torchcrepe.predict(\n            audio,\n            self.sr,\n            self.window,\n            f0_min,\n            f0_max,\n            model,\n            batch_size=batch_size,\n            device=self.device,\n            return_periodicity=True,\n        )\n        pd = torchcrepe.filter.median(pd, 3)\n        f0 = torchcrepe.filter.mean(f0, 3)\n        f0[pd < 0.1] = 0\n        f0 = f0[0].cpu().numpy()\n        return f0\n\n    def get_f0(\n        self,\n        x: np.ndarray,\n        p_len: int,\n        f0_up_key: int,\n        f0_method: str,\n        inp_f0: np.ndarray = None,\n    ):\n        f0_min = 50\n        f0_max = 1100\n        f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n        f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\n        if f0_method == \"harvest\":\n            f0, t = pyworld.harvest(\n                x.astype(np.double),\n                fs=self.sr,\n                f0_ceil=f0_max,\n                f0_floor=f0_min,\n                frame_period=10,\n            )\n            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n            f0 = signal.medfilt(f0, 3)\n        elif f0_method == \"dio\":\n            f0, t = pyworld.dio(\n                x.astype(np.double),\n                fs=self.sr,\n                f0_ceil=f0_max,\n                f0_floor=f0_min,\n                frame_period=10,\n            )\n            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n            f0 = signal.medfilt(f0, 3)\n        elif f0_method == \"mangio-crepe\":\n            f0 = self.get_f0_crepe_computation(x, f0_min, f0_max, p_len, 160, \"full\")\n        elif f0_method == \"crepe\":\n            f0 = self.get_f0_official_crepe_computation(x, f0_min, f0_max, \"full\")\n\n        f0 *= pow(2, f0_up_key / 12)\n        tf0 = self.sr // self.window  # f0 points per second\n        if inp_f0 is not None:\n            delta_t = np.round(\n                (inp_f0[:, 0].max() - inp_f0[:, 0].min()) * tf0 + 1\n            ).astype(\"int16\")\n            replace_f0 = np.interp(\n                list(range(delta_t)), inp_f0[:, 0] * 100, inp_f0[:, 1]\n            )\n            shape = f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)].shape[0]\n            f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)] = replace_f0[\n                :shape\n            ]\n\n        f0bak = f0.copy()\n        f0_mel = 1127 * np.log(1 + f0 / 700)\n        f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n            f0_mel_max - f0_mel_min\n        ) + 1\n        f0_mel[f0_mel <= 1] = 1\n        f0_mel[f0_mel > 255] = 255\n        f0_coarse = np.rint(f0_mel).astype(np.int)\n        return f0_coarse, f0bak  # 1-0\n\n    def _convert(\n        self,\n        model: HubertModel,\n        embedding_output_layer: int,\n        net_g: SynthesizerTrnMs256NSFSid,\n        sid: int,\n        audio: np.ndarray,\n        pitch: np.ndarray,\n        pitchf: np.ndarray,\n        index: faiss.IndexIVFFlat,\n        big_npy: np.ndarray,\n        index_rate: float,\n    ):\n        feats = torch.from_numpy(audio)\n        if self.is_half:\n            feats = feats.half()\n        else:\n            feats = feats.float()\n        if feats.dim() == 2:  # double channels\n            feats = feats.mean(-1)\n        assert feats.dim() == 1, feats.dim()\n        feats = feats.view(1, -1)\n        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n\n        half_support = (\n            self.device.type == \"cuda\"\n            and torch.cuda.get_device_capability(self.device)[0] >= 5.3\n        )\n        is_feats_dim_768 = net_g.emb_channels == 768\n\n        if isinstance(model, tuple):\n            feats = model[0](\n                feats.squeeze(0).squeeze(0).to(self.device),\n                return_tensors=\"pt\",\n                sampling_rate=16000,\n            )\n            if self.is_half:\n                feats = feats.input_values.to(self.device).half()\n            else:\n                feats = feats.input_values.to(self.device)\n            with torch.no_grad():\n                if is_feats_dim_768:\n                    feats = model[1](feats).last_hidden_state\n                else:\n                    feats = model[1](feats).extract_features\n        else:\n            inputs = {\n                \"source\": feats.half().to(self.device)\n                if half_support\n                else feats.to(self.device),\n                \"padding_mask\": padding_mask.to(self.device),\n                \"output_layer\": embedding_output_layer,\n            }\n\n            if not half_support:\n                model = model.float()\n                inputs[\"source\"] = inputs[\"source\"].float()\n\n            with torch.no_grad():\n                logits = model.extract_features(**inputs)\n                if is_feats_dim_768:\n                    feats = logits[0]\n                else:\n                    feats = model.final_proj(logits[0])\n\n        if (\n            isinstance(index, type(None)) == False\n            and isinstance(big_npy, type(None)) == False\n            and index_rate != 0\n        ):\n            npy = feats[0].cpu().numpy()\n            if self.is_half:\n                npy = npy.astype(\"float32\")\n\n            score, ix = index.search(npy, k=8)\n            weight = np.square(1 / score)\n            weight /= weight.sum(axis=1, keepdims=True)\n            npy = np.sum(big_npy[ix] * np.expand_dims(weight, axis=2), axis=1)\n\n            if self.is_half:\n                npy = npy.astype(\"float16\")\n            feats = (\n                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n                + (1 - index_rate) * feats\n            )\n\n        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n\n        p_len = audio.shape[0] // self.window\n        if feats.shape[1] < p_len:\n            p_len = feats.shape[1]\n            if pitch != None and pitchf != None:\n                pitch = pitch[:, :p_len]\n                pitchf = pitchf[:, :p_len]\n        p_len = torch.tensor([p_len], device=self.device).long()\n        with torch.no_grad():\n            if pitch != None and pitchf != None:\n                audio1 = (\n                    (net_g.infer(feats, p_len, pitch, pitchf, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n            else:\n                audio1 = (\n                    (net_g.infer(feats, p_len, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n        del feats, p_len, padding_mask\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio1\n\n    def __call__(\n        self,\n        model: HubertModel,\n        embedding_output_layer: int,\n        net_g: SynthesizerTrnMs256NSFSid,\n        sid: int,\n        audio: np.ndarray,\n        transpose: int,\n        f0_method: str,\n        file_index: str,\n        index_rate: float,\n        if_f0: bool,\n        f0_file: str = None,\n    ):\n        if file_index != \"\" and os.path.exists(file_index) and index_rate != 0:\n            try:\n                index = faiss.read_index(file_index)\n                # big_npy = np.load(file_big_npy)\n                big_npy = index.reconstruct_n(0, index.ntotal)\n            except:\n                traceback.print_exc()\n                index = big_npy = None\n        else:\n            index = big_npy = None\n\n        bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n        audio = signal.filtfilt(bh, ah, audio)\n\n        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\")\n        opt_ts = []\n        if audio_pad.shape[0] > self.t_max:\n            audio_sum = np.zeros_like(audio)\n            for i in range(self.window):\n                audio_sum += audio_pad[i : i - self.window]\n            for t in range(self.t_center, audio.shape[0], self.t_center):\n                opt_ts.append(\n                    t\n                    - self.t_query\n                    + np.where(\n                        np.abs(audio_sum[t - self.t_query : t + self.t_query])\n                        == np.abs(audio_sum[t - self.t_query : t + self.t_query]).min()\n                    )[0][0]\n                )\n\n        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\")\n        p_len = audio_pad.shape[0] // self.window\n        inp_f0 = None\n        if hasattr(f0_file, \"name\"):\n            try:\n                with open(f0_file.name, \"r\") as f:\n                    lines = f.read().strip(\"\\n\").split(\"\\n\")\n                inp_f0 = []\n                for line in lines:\n                    inp_f0.append([float(i) for i in line.split(\",\")])\n                inp_f0 = np.array(inp_f0, dtype=\"float32\")\n            except:\n                traceback.print_exc()\n        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n        pitch, pitchf = None, None\n        if if_f0 == 1:\n            pitch, pitchf = self.get_f0(audio_pad, p_len, transpose, f0_method, inp_f0)\n            pitch = pitch[:p_len]\n            pitchf = pitchf[:p_len]\n            if self.device.type == \"mps\":\n                pitchf = pitchf.astype(np.float32)\n            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n\n        audio_opt = []\n\n        s = 0\n        t = None\n\n        for t in opt_ts:\n            t = t // self.window * self.window\n            if if_f0 == 1:\n                audio_opt.append(\n                    self._convert(\n                        model,\n                        embedding_output_layer,\n                        net_g,\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n                        index,\n                        big_npy,\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            else:\n                audio_opt.append(\n                    self._convert(\n                        model,\n                        embedding_output_layer,\n                        net_g,\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        None,\n                        None,\n                        index,\n                        big_npy,\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            s = t\n        if if_f0 == 1:\n            audio_opt.append(\n                self._convert(\n                    model,\n                    embedding_output_layer,\n                    net_g,\n                    sid,\n                    audio_pad[t:],\n                    pitch[:, t // self.window :] if t is not None else pitch,\n                    pitchf[:, t // self.window :] if t is not None else pitchf,\n                    index,\n                    big_npy,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        else:\n            audio_opt.append(\n                self._convert(\n                    model,\n                    embedding_output_layer,\n                    net_g,\n                    sid,\n                    audio_pad[t:],\n                    None,\n                    None,\n                    index,\n                    big_npy,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        audio_opt = np.concatenate(audio_opt)\n        del pitch, pitchf, sid\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio_opt", ""]}
{"filename": "lib/rvc/mel_processing.py", "chunked_list": ["import torch\nimport torch.utils.data\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)", "\n\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\n", "\n\ndef spectral_normalize_torch(magnitudes):\n    return dynamic_range_compression_torch(magnitudes)\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    return dynamic_range_decompression_torch(magnitudes)\n\n", "\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.07:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.07:\n        print(\"max value is \", torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n            dtype=y.dtype, device=y.device\n        )\n\n    y = torch.nn.functional.pad(\n        y.unsqueeze(1),\n        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n        mode=\"reflect\",\n    )\n    y = y.squeeze(1)\n\n    # mps does not support torch.stft.\n    if y.device.type == \"mps\":\n        i = y.cpu()\n        win = hann_window[wnsize_dtype_device].cpu()\n    else:\n        i = y\n        win = hann_window[wnsize_dtype_device]\n    spec = torch.stft(\n        i,\n        n_fft,\n        hop_length=hop_size,\n        win_length=win_size,\n        window=win,\n        center=center,\n        pad_mode=\"reflect\",\n        normalized=False,\n        onesided=True,\n        return_complex=False,\n    ).to(device=y.device)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec", "\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n            dtype=spec.dtype, device=spec.device\n        )\n    melspec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    melspec = spectral_normalize_torch(melspec)\n    return melspec", "\n\ndef mel_spectrogram_torch(\n    y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False\n):\n    \"\"\"Convert waveform into Mel-frequency Log-amplitude spectrogram.\n\n    Args:\n        y       :: (B, T)           - Waveforms\n    Returns:\n        melspec :: (B, Freq, Frame) - Mel-frequency Log-amplitude spectrogram\n    \"\"\"\n    # Linear-frequency Linear-amplitude spectrogram :: (B, T) -> (B, Freq, Frame)\n    spec = spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center)\n\n    # Mel-frequency Log-amplitude spectrogram :: (B, Freq, Frame) -> (B, Freq=num_mels, Frame)\n    melspec = spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax)\n\n    return melspec", ""]}
{"filename": "lib/rvc/commons.py", "chunked_list": ["import math\n\nimport torch\nfrom torch.nn import functional as F\n\n\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)", "\n\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\n\n\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape", "\n\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += (\n        0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n    )\n    return kl\n", "\n\ndef rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))\n\n\ndef rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g", "def rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\n\n\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret", "\n\ndef slice_segments2(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, idx_str:idx_end]\n    return ret\n", "\n\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size + 1\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str", "\n\ndef get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)\n    num_timescales = channels // 2\n    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (\n        num_timescales - 1\n    )\n    inv_timescales = min_timescale * torch.exp(\n        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n    )\n    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n    signal = F.pad(signal, [0, 0, 0, channels % 2])\n    signal = signal.view(1, channels, length)\n    return signal", "\n\ndef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return x + signal.to(dtype=x.dtype, device=x.device)\n\n\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)", "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n\n\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n", "\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts", "\n\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\n\n\ndef shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x", "def shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\n\n\ndef sequence_mask(length, max_length=None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)", "\n\ndef generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"\n    b, _, t_y, t_x = mask.shape\n    cum_duration = torch.cumsum(duration, -1)\n\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n    path = path.view(b, t_x, t_y)\n    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n    path = path.unsqueeze(1).transpose(2, 3) * mask\n    return path", "\n\ndef clip_grad_value_(parameters, clip_value, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    if clip_value is not None:\n        clip_value = float(clip_value)\n\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item() ** norm_type\n        if clip_value is not None:\n            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n    total_norm = total_norm ** (1.0 / norm_type)\n    return total_norm", ""]}
{"filename": "lib/rvc/utils.py", "chunked_list": ["import glob\nimport logging\nimport os\nimport shutil\nimport socket\nimport sys\n\nimport ffmpeg\nimport matplotlib\nimport matplotlib.pylab as plt", "import matplotlib\nimport matplotlib.pylab as plt\nimport numpy as np\nimport torch\nfrom scipy.io.wavfile import read\nfrom torch.nn import functional as F\n\nfrom modules.shared import ROOT_DIR\n\nfrom .config import TrainConfig", "\nfrom .config import TrainConfig\n\nmatplotlib.use(\"Agg\")\nlogging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging\n\n\ndef load_audio(file: str, sr):\n    try:\n        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n        file = (\n            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )  # Prevent small white copy path head and tail with spaces and \" and return\n        out, _ = (\n            ffmpeg.input(file, threads=0)\n            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load audio: {e}\")\n\n    return np.frombuffer(out, np.float32).flatten()", "\n\ndef load_audio(file: str, sr):\n    try:\n        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n        file = (\n            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )  # Prevent small white copy path head and tail with spaces and \" and return\n        out, _ = (\n            ffmpeg.input(file, threads=0)\n            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load audio: {e}\")\n\n    return np.frombuffer(out, np.float32).flatten()", "\n\ndef find_empty_port():\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((\"\", 0))\n    s.listen(1)\n    port = s.getsockname()[1]\n    s.close()\n    return port\n", "\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None, load_opt=1):\n    assert os.path.isfile(checkpoint_path)\n    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n\n    saved_state_dict = checkpoint_dict[\"model\"]\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    new_state_dict = {}\n    for k, v in state_dict.items():  # \u6a21\u578b\u9700\u8981\u7684shape\n        try:\n            new_state_dict[k] = saved_state_dict[k]\n            if saved_state_dict[k].shape != state_dict[k].shape:\n                print(\n                    f\"shape-{k}-mismatch|need-{state_dict[k].shape}|get-{saved_state_dict[k].shape}\"\n                )\n                if saved_state_dict[k].dim() == 2:  # NOTE: check is this ok?\n                    # for embedded input 256 <==> 768\n                    # this achieves we can continue training from original's pretrained checkpoints when using embedder that 768-th dim output etc.\n                    if saved_state_dict[k].dtype == torch.half:\n                        new_state_dict[k] = (\n                            F.interpolate(\n                                saved_state_dict[k].float().unsqueeze(0).unsqueeze(0),\n                                size=state_dict[k].shape,\n                                mode=\"bilinear\",\n                            )\n                            .half()\n                            .squeeze(0)\n                            .squeeze(0)\n                        )\n                    else:\n                        new_state_dict[k] = (\n                            F.interpolate(\n                                saved_state_dict[k].unsqueeze(0).unsqueeze(0),\n                                size=state_dict[k].shape,\n                                mode=\"bilinear\",\n                            )\n                            .squeeze(0)\n                            .squeeze(0)\n                        )\n                    print(\n                        \"interpolated new_state_dict\",\n                        k,\n                        \"from\",\n                        saved_state_dict[k].shape,\n                        \"to\",\n                        new_state_dict[k].shape,\n                    )\n                else:\n                    raise KeyError\n        except Exception as e:\n            # print(traceback.format_exc())\n            print(f\"{k} is not in the checkpoint\")\n            print(\"error: %s\" % e)\n            new_state_dict[k] = v  # \u6a21\u578b\u81ea\u5e26\u7684\u968f\u673a\u503c\n    if hasattr(model, \"module\"):\n        model.module.load_state_dict(new_state_dict, strict=False)\n    else:\n        model.load_state_dict(new_state_dict, strict=False)\n    print(\"Loaded model weights\")\n\n    epoch = checkpoint_dict[\"epoch\"]\n    learning_rate = checkpoint_dict[\"learning_rate\"]\n    if optimizer is not None and load_opt == 1:\n        optimizer.load_state_dict(checkpoint_dict[\"optimizer\"])\n    print(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, epoch))\n    return model, optimizer, learning_rate, epoch", "\n\ndef save_state(model, optimizer, learning_rate, epoch, checkpoint_path):\n    print(\n        \"Saving model and optimizer state at epoch {} to {}\".format(\n            epoch, checkpoint_path\n        )\n    )\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    torch.save(\n        {\n            \"model\": state_dict,\n            \"epoch\": epoch,\n            \"optimizer\": optimizer.state_dict(),\n            \"learning_rate\": learning_rate,\n        },\n        checkpoint_path,\n    )", "\n\ndef summarize(\n    writer,\n    global_step,\n    scalars={},\n    histograms={},\n    images={},\n    audios={},\n    audio_sampling_rate=22050,\n):\n    for k, v in scalars.items():\n        writer.add_scalar(k, v, global_step)\n    for k, v in histograms.items():\n        writer.add_histogram(k, v, global_step)\n    for k, v in images.items():\n        writer.add_image(k, v, global_step, dataformats=\"HWC\")\n    for k, v in audios.items():\n        writer.add_audio(k, v, global_step, audio_sampling_rate)", "\n\ndef latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n    filelist = glob.glob(os.path.join(dir_path, regex))\n    if len(filelist) == 0:\n        return None\n    filelist.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n    filepath = filelist[-1]\n    return filepath\n", "\n\ndef plot_spectrogram_to_numpy(spectrogram):\n    fig, ax = plt.subplots(figsize=(10, 2))\n    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n    plt.colorbar(im, ax=ax)\n    plt.xlabel(\"Frames\")\n    plt.ylabel(\"Channels\")\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close()\n    return data", "\n\ndef plot_alignment_to_numpy(alignment, info=None):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    im = ax.imshow(\n        alignment.transpose(), aspect=\"auto\", origin=\"lower\", interpolation=\"none\"\n    )\n    fig.colorbar(im, ax=ax)\n    xlabel = \"Decoder timestep\"\n    if info is not None:\n        xlabel += \"\\n\\n\" + info\n    plt.xlabel(xlabel)\n    plt.ylabel(\"Encoder timestep\")\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.close()\n    return data", "\n\ndef load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\n\ndef load_config(training_dir: str, sample_rate: int, emb_channels: int):\n    if emb_channels == 256:\n        config_path = os.path.join(ROOT_DIR, \"configs\", f\"{sample_rate}.json\")\n    else:\n        config_path = os.path.join(\n            ROOT_DIR, \"configs\", f\"{sample_rate}-{emb_channels}.json\"\n        )\n    config_save_path = os.path.join(training_dir, \"config.json\")\n\n    shutil.copyfile(config_path, config_save_path)\n\n    return TrainConfig.parse_file(config_save_path)", ""]}
{"filename": "lib/rvc/checkpoints.py", "chunked_list": ["import os\nfrom collections import OrderedDict\nfrom typing import *\n\nimport torch\n\n\ndef write_config(state_dict: Dict[str, Any], cfg: Dict[str, Any]):\n    state_dict[\"config\"] = []\n    for key, x in cfg.items():\n        state_dict[\"config\"].append(x)\n    state_dict[\"params\"] = cfg", "\n\ndef create_trained_model(\n    weights: Dict[str, Any],\n    version: Literal[\"v1\", \"v2\"],\n    sr: str,\n    f0: bool,\n    emb_name: str,\n    emb_ch: int,\n    emb_output_layer: int,\n    epoch: int,\n    speaker_info: Optional[dict[str, int]]\n):\n    state_dict = OrderedDict()\n    state_dict[\"weight\"] = {}\n    for key in weights.keys():\n        if \"enc_q\" in key:\n            continue\n        state_dict[\"weight\"][key] = weights[key].half()\n    if sr == \"40k\":\n        write_config(\n            state_dict,\n            {\n                \"spec_channels\": 1025,\n                \"segment_size\": 32,\n                \"inter_channels\": 192,\n                \"hidden_channels\": 192,\n                \"filter_channels\": 768,\n                \"n_heads\": 2,\n                \"n_layers\": 6,\n                \"kernel_size\": 3,\n                \"p_dropout\": 0,\n                \"resblock\": \"1\",\n                \"resblock_kernel_sizes\": [3, 7, 11],\n                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n                \"upsample_rates\": [10, 10, 2, 2],\n                \"upsample_initial_channel\": 512,\n                \"upsample_kernel_sizes\": [16, 16, 4, 4],\n                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n                \"gin_channels\": 256,\n                \"emb_channels\": emb_ch,\n                \"sr\": 40000,\n            },\n        )\n    elif sr == \"48k\":\n        write_config(\n            state_dict,\n            {\n                \"spec_channels\": 1025,\n                \"segment_size\": 32,\n                \"inter_channels\": 192,\n                \"hidden_channels\": 192,\n                \"filter_channels\": 768,\n                \"n_heads\": 2,\n                \"n_layers\": 6,\n                \"kernel_size\": 3,\n                \"p_dropout\": 0,\n                \"resblock\": \"1\",\n                \"resblock_kernel_sizes\": [3, 7, 11],\n                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n                \"upsample_rates\": [10, 6, 2, 2, 2],\n                \"upsample_initial_channel\": 512,\n                \"upsample_kernel_sizes\": [16, 16, 4, 4, 4],\n                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n                \"gin_channels\": 256,\n                \"emb_channels\": emb_ch,\n                \"sr\": 48000,\n            },\n        )\n    elif sr == \"32k\":\n        write_config(\n            state_dict,\n            {\n                \"spec_channels\": 513,\n                \"segment_size\": 32,\n                \"inter_channels\": 192,\n                \"hidden_channels\": 192,\n                \"filter_channels\": 768,\n                \"n_heads\": 2,\n                \"n_layers\": 6,\n                \"kernel_size\": 3,\n                \"p_dropout\": 0,\n                \"resblock\": \"1\",\n                \"resblock_kernel_sizes\": [3, 7, 11],\n                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n                \"upsample_rates\": [10, 4, 2, 2, 2],\n                \"upsample_initial_channel\": 512,\n                \"upsample_kernel_sizes\": [16, 16, 4, 4, 4],\n                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n                \"gin_channels\": 256,\n                \"emb_channels\": emb_ch,\n                \"sr\": 32000,\n            },\n        )\n    state_dict[\"version\"] = version\n    state_dict[\"info\"] = f\"{epoch}epoch\"\n    state_dict[\"sr\"] = sr\n    state_dict[\"f0\"] = 1 if f0 else 0\n    state_dict[\"embedder_name\"] = emb_name\n    state_dict[\"embedder_output_layer\"] = emb_output_layer\n    if not speaker_info is None:\n        state_dict[\"speaker_info\"] = {str(v): str(k) for k, v in speaker_info.items()}\n    return state_dict", "\n\ndef save(\n    model,\n    version: Literal[\"v1\", \"v2\"],\n    sr: str,\n    f0: bool,\n    emb_name: str,\n    emb_ch: int,\n    emb_output_layer: int,\n    filepath: str,\n    epoch: int,\n    speaker_info: Optional[dict[str, int]]\n):\n    if hasattr(model, \"module\"):\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n\n    print(f\"save: emb_name: {emb_name} {emb_ch}\")\n\n    state_dict = create_trained_model(\n        state_dict,\n        version,\n        sr,\n        f0,\n        emb_name,\n        emb_ch,\n        emb_output_layer,\n        epoch,\n        speaker_info\n    )\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n    torch.save(state_dict, filepath)", ""]}
{"filename": "lib/rvc/modules.py", "chunked_list": ["import math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv1d\nfrom torch.nn import functional as F\nfrom torch.nn.utils import remove_weight_norm, weight_norm\n\nfrom . import commons\nfrom .commons import get_padding, init_weights", "from . import commons\nfrom .commons import get_padding, init_weights\nfrom .transforms import piecewise_rational_quadratic_transform\n\nLRELU_SLOPE = 0.1\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n        return x.transpose(1, -1)", "\n\nclass ConvReluNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels,\n        out_channels,\n        kernel_size,\n        n_layers,\n        p_dropout,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n        assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\n        self.conv_layers = nn.ModuleList()\n        self.norm_layers = nn.ModuleList()\n        self.conv_layers.append(\n            nn.Conv1d(\n                in_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n            )\n        )\n        self.norm_layers.append(LayerNorm(hidden_channels))\n        self.relu_drop = nn.Sequential(nn.ReLU(), nn.Dropout(p_dropout))\n        for _ in range(n_layers - 1):\n            self.conv_layers.append(\n                nn.Conv1d(\n                    hidden_channels,\n                    hidden_channels,\n                    kernel_size,\n                    padding=kernel_size // 2,\n                )\n            )\n            self.norm_layers.append(LayerNorm(hidden_channels))\n        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask):\n        x_org = x\n        for i in range(self.n_layers):\n            x = self.conv_layers[i](x * x_mask)\n            x = self.norm_layers[i](x)\n            x = self.relu_drop(x)\n        x = x_org + self.proj(x)\n        return x * x_mask", "\n\nclass DDSConv(nn.Module):\n    \"\"\"\n    Dialted and Depth-Separable Convolution\n    \"\"\"\n\n    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n        super().__init__()\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n\n        self.drop = nn.Dropout(p_dropout)\n        self.convs_sep = nn.ModuleList()\n        self.convs_1x1 = nn.ModuleList()\n        self.norms_1 = nn.ModuleList()\n        self.norms_2 = nn.ModuleList()\n        for i in range(n_layers):\n            dilation = kernel_size**i\n            padding = (kernel_size * dilation - dilation) // 2\n            self.convs_sep.append(\n                nn.Conv1d(\n                    channels,\n                    channels,\n                    kernel_size,\n                    groups=channels,\n                    dilation=dilation,\n                    padding=padding,\n                )\n            )\n            self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n            self.norms_1.append(LayerNorm(channels))\n            self.norms_2.append(LayerNorm(channels))\n\n    def forward(self, x, x_mask, g=None):\n        if g is not None:\n            x = x + g\n        for i in range(self.n_layers):\n            y = self.convs_sep[i](x * x_mask)\n            y = self.norms_1[i](y)\n            y = F.gelu(y)\n            y = self.convs_1x1[i](y)\n            y = self.norms_2[i](y)\n            y = F.gelu(y)\n            y = self.drop(y)\n            x = x + y\n        return x * x_mask", "\n\nclass WN(torch.nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0,\n        p_dropout=0,\n    ):\n        super(WN, self).__init__()\n        assert kernel_size % 2 == 1\n        self.hidden_channels = hidden_channels\n        self.kernel_size = (kernel_size,)\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.gin_channels = gin_channels\n        self.p_dropout = p_dropout\n\n        self.in_layers = torch.nn.ModuleList()\n        self.res_skip_layers = torch.nn.ModuleList()\n        self.drop = nn.Dropout(p_dropout)\n\n        if gin_channels != 0:\n            cond_layer = torch.nn.Conv1d(\n                gin_channels, 2 * hidden_channels * n_layers, 1\n            )\n            self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name=\"weight\")\n\n        for i in range(n_layers):\n            dilation = dilation_rate**i\n            padding = int((kernel_size * dilation - dilation) / 2)\n            in_layer = torch.nn.Conv1d(\n                hidden_channels,\n                2 * hidden_channels,\n                kernel_size,\n                dilation=dilation,\n                padding=padding,\n            )\n            in_layer = torch.nn.utils.weight_norm(in_layer, name=\"weight\")\n            self.in_layers.append(in_layer)\n\n            # last one is not necessary\n            if i < n_layers - 1:\n                res_skip_channels = 2 * hidden_channels\n            else:\n                res_skip_channels = hidden_channels\n\n            res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name=\"weight\")\n            self.res_skip_layers.append(res_skip_layer)\n\n    def forward(self, x, x_mask, g=None, **kwargs):\n        output = torch.zeros_like(x)\n        n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\n        if g is not None:\n            g = self.cond_layer(g)\n\n        for i in range(self.n_layers):\n            x_in = self.in_layers[i](x)\n            if g is not None:\n                cond_offset = i * 2 * self.hidden_channels\n                g_l = g[:, cond_offset : cond_offset + 2 * self.hidden_channels, :]\n            else:\n                g_l = torch.zeros_like(x_in)\n\n            acts = commons.fused_add_tanh_sigmoid_multiply(x_in, g_l, n_channels_tensor)\n            acts = self.drop(acts)\n\n            res_skip_acts = self.res_skip_layers[i](acts)\n            if i < self.n_layers - 1:\n                res_acts = res_skip_acts[:, : self.hidden_channels, :]\n                x = (x + res_acts) * x_mask\n                output = output + res_skip_acts[:, self.hidden_channels :, :]\n            else:\n                output = output + res_skip_acts\n        return output * x_mask\n\n    def remove_weight_norm(self):\n        if self.gin_channels != 0:\n            torch.nn.utils.remove_weight_norm(self.cond_layer)\n        for l in self.in_layers:\n            torch.nn.utils.remove_weight_norm(l)\n        for l in self.res_skip_layers:\n            torch.nn.utils.remove_weight_norm(l)", "\n\nclass ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[2],\n                        padding=get_padding(kernel_size, dilation[2]),\n                    )\n                ),\n            ]\n        )\n        self.convs1.apply(init_weights)\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                ),\n            ]\n        )\n        self.convs2.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c2(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)", "\n\nclass ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[0],\n                        padding=get_padding(kernel_size, dilation[0]),\n                    )\n                ),\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[1],\n                        padding=get_padding(kernel_size, dilation[1]),\n                    )\n                ),\n            ]\n        )\n        self.convs.apply(init_weights)\n\n    def forward(self, x, x_mask=None):\n        for c in self.convs:\n            xt = F.leaky_relu(x, LRELU_SLOPE)\n            if x_mask is not None:\n                xt = xt * x_mask\n            xt = c(xt)\n            x = xt + x\n        if x_mask is not None:\n            x = x * x_mask\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)", "\n\nclass Log(nn.Module):\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n            logdet = torch.sum(-y, [1, 2])\n            return y, logdet\n        else:\n            x = torch.exp(x) * x_mask\n            return x", "\n\nclass Flip(nn.Module):\n    def forward(self, x, *args, reverse=False, **kwargs):\n        x = torch.flip(x, [1])\n        if not reverse:\n            logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n            return x, logdet\n        else:\n            return x", "\n\nclass ElementwiseAffine(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.m = nn.Parameter(torch.zeros(channels, 1))\n        self.logs = nn.Parameter(torch.zeros(channels, 1))\n\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = self.m + torch.exp(self.logs) * x\n            y = y * x_mask\n            logdet = torch.sum(self.logs * x_mask, [1, 2])\n            return y, logdet\n        else:\n            x = (x - self.m) * torch.exp(-self.logs) * x_mask\n            return x", "\n\nclass ResidualCouplingLayer(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        p_dropout=0,\n        gin_channels=0,\n        mean_only=False,\n    ):\n        assert channels % 2 == 0, \"channels should be divisible by 2\"\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.half_channels = channels // 2\n        self.mean_only = mean_only\n\n        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n        self.enc = WN(\n            hidden_channels,\n            kernel_size,\n            dilation_rate,\n            n_layers,\n            p_dropout=p_dropout,\n            gin_channels=gin_channels,\n        )\n        self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n        self.post.weight.data.zero_()\n        self.post.bias.data.zero_()\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n        h = self.pre(x0) * x_mask\n        h = self.enc(h, x_mask, g=g)\n        stats = self.post(h) * x_mask\n        if not self.mean_only:\n            m, logs = torch.split(stats, [self.half_channels] * 2, 1)\n        else:\n            m = stats\n            logs = torch.zeros_like(m)\n\n        if not reverse:\n            x1 = m + x1 * torch.exp(logs) * x_mask\n            x = torch.cat([x0, x1], 1)\n            logdet = torch.sum(logs, [1, 2])\n            return x, logdet\n        else:\n            x1 = (x1 - m) * torch.exp(-logs) * x_mask\n            x = torch.cat([x0, x1], 1)\n            return x\n\n    def remove_weight_norm(self):\n        self.enc.remove_weight_norm()", "\n\nclass ConvFlow(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        filter_channels,\n        kernel_size,\n        n_layers,\n        num_bins=10,\n        tail_bound=5.0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.num_bins = num_bins\n        self.tail_bound = tail_bound\n        self.half_channels = in_channels // 2\n\n        self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n        self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.0)\n        self.proj = nn.Conv1d(\n            filter_channels, self.half_channels * (num_bins * 3 - 1), 1\n        )\n        self.proj.weight.data.zero_()\n        self.proj.bias.data.zero_()\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n        h = self.pre(x0)\n        h = self.convs(h, x_mask, g=g)\n        h = self.proj(h) * x_mask\n\n        b, c, t = x0.shape\n        h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n\n        unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n        unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n            self.filter_channels\n        )\n        unnormalized_derivatives = h[..., 2 * self.num_bins :]\n\n        x1, logabsdet = piecewise_rational_quadratic_transform(\n            x1,\n            unnormalized_widths,\n            unnormalized_heights,\n            unnormalized_derivatives,\n            inverse=reverse,\n            tails=\"linear\",\n            tail_bound=self.tail_bound,\n        )\n\n        x = torch.cat([x0, x1], 1) * x_mask\n        logdet = torch.sum(logabsdet * x_mask, [1, 2])\n        if not reverse:\n            return x, logdet\n        else:\n            return x", ""]}
{"filename": "lib/rvc/preprocessing/extract_f0.py", "chunked_list": ["import os\nimport traceback\nfrom concurrent.futures import ProcessPoolExecutor\nfrom typing import *\nimport multiprocessing as mp\n\nimport numpy as np\nimport pyworld\nimport torch\nimport torchcrepe", "import torch\nimport torchcrepe\nfrom torch import Tensor\nfrom tqdm import tqdm\n\nfrom lib.rvc.utils import load_audio\n\ndef get_optimal_torch_device(index: int = 0) -> torch.device:\n    # Get cuda device\n    if torch.cuda.is_available():\n        return torch.device(f\"cuda:{index % torch.cuda.device_count()}\") # Very fast\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    # Insert an else here to grab \"xla\" devices if available. TO DO later. Requires the torch_xla.core.xla_model library\n    # Else wise return the \"cpu\" as a torch device, \n    return torch.device(\"cpu\")", "\ndef get_f0_official_crepe_computation(\n        x,\n        sr,\n        f0_min,\n        f0_max,\n        model=\"full\",\n):\n    batch_size = 512\n    torch_device = get_optimal_torch_device()\n    audio = torch.tensor(np.copy(x))[None].float()\n    f0, pd = torchcrepe.predict(\n        audio,\n        sr,\n        160,\n        f0_min,\n        f0_max,\n        model,\n        batch_size=batch_size,\n        device=torch_device,\n        return_periodicity=True,\n    )\n    pd = torchcrepe.filter.median(pd, 3)\n    f0 = torchcrepe.filter.mean(f0, 3)\n    f0[pd < 0.1] = 0\n    f0 = f0[0].cpu().numpy()\n    f0 = f0[1:] # Get rid of extra first frame\n    return f0", "\ndef get_f0_crepe_computation(\n        x, \n        sr,\n        f0_min,\n        f0_max,\n        hop_length=160, # 512 before. Hop length changes the speed that the voice jumps to a different dramatic pitch. Lower hop lengths means more pitch accuracy but longer inference time.\n        model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n):\n    x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n    x /= np.quantile(np.abs(x), 0.999)\n    torch_device = get_optimal_torch_device()\n    audio = torch.from_numpy(x).to(torch_device, copy=True)\n    audio = torch.unsqueeze(audio, dim=0)\n    if audio.ndim == 2 and audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True).detach()\n    audio = audio.detach()\n    print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n    pitch: Tensor = torchcrepe.predict(\n        audio,\n        sr,\n        hop_length,\n        f0_min,\n        f0_max,\n        model,\n        batch_size=hop_length * 2,\n        device=torch_device,\n        pad=True\n    )\n    p_len = x.shape[0] // hop_length\n    # Resize the pitch for final f0\n    source = np.array(pitch.squeeze(0).cpu().float().numpy())\n    source[source < 0.001] = np.nan\n    target = np.interp(\n        np.arange(0, len(source) * p_len, len(source)) / p_len,\n        np.arange(0, len(source)),\n        source\n    )\n    f0 = np.nan_to_num(target)\n    f0 = f0[1:] # Get rid of extra first frame\n    return f0 # Resized f0", "\n\ndef compute_f0(\n    path: str,\n    f0_method: str,\n    fs: int,\n    hop: int,\n    f0_max: float,\n    f0_min: float,\n):\n    x = load_audio(path, fs)\n    if f0_method == \"harvest\":\n        f0, t = pyworld.harvest(\n            x.astype(np.double),\n            fs=fs,\n            f0_ceil=f0_max,\n            f0_floor=f0_min,\n            frame_period=1000 * hop / fs,\n        )\n        f0 = pyworld.stonemask(x.astype(np.double), f0, t, fs)\n    elif f0_method == \"dio\":\n        f0, t = pyworld.dio(\n            x.astype(np.double),\n            fs=fs,\n            f0_ceil=f0_max,\n            f0_floor=f0_min,\n            frame_period=1000 * hop / fs,\n        )\n        f0 = pyworld.stonemask(x.astype(np.double), f0, t, fs)\n    elif f0_method == \"mangio-crepe\":\n        f0 = get_f0_crepe_computation(x, fs, f0_min, f0_max, 160, \"full\")\n    elif f0_method == \"crepe\":\n        f0 = get_f0_official_crepe_computation(x.astype(np.double), fs, f0_min, f0_max, \"full\")\n    return f0", "\n\ndef coarse_f0(f0, f0_bin, f0_mel_min, f0_mel_max):\n    f0_mel = 1127 * np.log(1 + f0 / 700)\n    f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * (f0_bin - 2) / (\n        f0_mel_max - f0_mel_min\n    ) + 1\n\n    # use 0 or 1\n    f0_mel[f0_mel <= 1] = 1\n    f0_mel[f0_mel > f0_bin - 1] = f0_bin - 1\n    f0_coarse = np.rint(f0_mel).astype(np.int)\n    assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (\n        f0_coarse.max(),\n        f0_coarse.min(),\n    )\n    return f0_coarse", "\n\ndef processor(paths, f0_method, samplerate=16000, hop_size=160, process_id=0):\n    fs = samplerate\n    hop = hop_size\n\n    f0_bin = 256\n    f0_max = 1100.0\n    f0_min = 50.0\n    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n    if len(paths) != 0:\n        for idx, (inp_path, opt_path1, opt_path2) in enumerate(\n            tqdm(paths, position=1 + process_id)\n        ):\n            try:\n                if (\n                    os.path.exists(opt_path1 + \".npy\") == True\n                    and os.path.exists(opt_path2 + \".npy\") == True\n                ):\n                    continue\n                featur_pit = compute_f0(inp_path, f0_method, fs, hop, f0_max, f0_min)\n                np.save(\n                    opt_path2,\n                    featur_pit,\n                    allow_pickle=False,\n                )  # nsf\n                coarse_pit = coarse_f0(featur_pit, f0_bin, f0_mel_min, f0_mel_max)\n                np.save(\n                    opt_path1,\n                    coarse_pit,\n                    allow_pickle=False,\n                )  # ori\n            except:\n                print(f\"f0 failed {idx}: {inp_path} {traceback.format_exc()}\")", "\n\ndef run(training_dir: str, num_processes: int, f0_method: str):\n    paths = []\n    dataset_dir = os.path.join(training_dir, \"1_16k_wavs\")\n    opt_dir_f0 = os.path.join(training_dir, \"2a_f0\")\n    opt_dir_f0_nsf = os.path.join(training_dir, \"2b_f0nsf\")\n\n    if os.path.exists(opt_dir_f0) and os.path.exists(opt_dir_f0_nsf):\n        return\n\n    os.makedirs(opt_dir_f0, exist_ok=True)\n    os.makedirs(opt_dir_f0_nsf, exist_ok=True)\n\n    names = []\n\n    for pathname in sorted(list(os.listdir(dataset_dir))):\n        if os.path.isdir(os.path.join(dataset_dir, pathname)):\n            for f in sorted(list(os.listdir(os.path.join(dataset_dir, pathname)))):\n                if \"spec\" in f:\n                    continue\n                names.append(os.path.join(pathname, f))\n        else:\n            names.append(pathname)\n\n    for name in names:  # dataset_dir/{05d}/file.ext\n        filepath = os.path.join(dataset_dir, name)\n        if \"spec\" in filepath:\n            continue\n        opt_filepath_f0 = os.path.join(opt_dir_f0, name)\n        opt_filepath_f0_nsf = os.path.join(opt_dir_f0_nsf, name)\n        paths.append([filepath, opt_filepath_f0, opt_filepath_f0_nsf])\n\n    for dir in set([(os.path.dirname(p[1]), os.path.dirname(p[2])) for p in paths]):\n        os.makedirs(dir[0], exist_ok=True)\n        os.makedirs(dir[1], exist_ok=True)\n\n    with ProcessPoolExecutor(mp_context=mp.get_context(\"spawn\")) as executer:\n        for i in range(num_processes):\n            executer.submit(processor, paths[i::num_processes], f0_method, process_id=i)\n\n    processor(paths, f0_method)", ""]}
{"filename": "lib/rvc/preprocessing/extract_feature.py", "chunked_list": ["import multiprocessing as mp\nimport os\nimport traceback\nfrom concurrent.futures import ProcessPoolExecutor\nfrom typing import *\n\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nfrom fairseq import checkpoint_utils\nfrom tqdm import tqdm\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nMODELS_DIR = os.path.join(ROOT_DIR, \"models\")\nEMBEDDINGS_LIST = {\n    \"hubert-base-japanese\": (\n        \"rinna_hubert_base_jp.pt\",", "    \"hubert-base-japanese\": (\n        \"rinna_hubert_base_jp.pt\",\n        \"hubert-base-japanese\",\n        \"local\",\n    ),\n    \"contentvec\": (\"checkpoint_best_legacy_500.pt\", \"contentvec\", \"local\"),\n}\n\ndef get_embedder(embedder_name):\n    if embedder_name in EMBEDDINGS_LIST:\n        return EMBEDDINGS_LIST[embedder_name]\n    return None", "def get_embedder(embedder_name):\n    if embedder_name in EMBEDDINGS_LIST:\n        return EMBEDDINGS_LIST[embedder_name]\n    return None\n\n\ndef load_embedder(embedder_path: str, device):\n    try:\n        models, cfg, _ = checkpoint_utils.load_model_ensemble_and_task(\n            [embedder_path],\n            suffix=\"\",\n        )\n        embedder_model = models[0]\n        embedder_model = embedder_model.to(device)\n        if device != \"cpu\":\n            embedder_model = embedder_model.half()\n        else:\n            embedder_model = embedder_model.float()\n        embedder_model.eval()\n    except Exception as e:\n        print(f\"Error: {e} {embedder_path}\")\n        traceback.print_exc()\n\n    return embedder_model, cfg", "\n\n# wave must be 16k, hop_size=320\ndef readwave(wav_path, normalize=False):\n    wav, sr = sf.read(wav_path)\n    assert sr == 16000\n    feats = torch.from_numpy(wav).float()\n    if feats.dim() == 2:  # double channels\n        feats = feats.mean(-1)\n    assert feats.dim() == 1, feats.dim()\n    if normalize:\n        with torch.no_grad():\n            feats = F.layer_norm(feats, feats.shape)\n    feats = feats.view(1, -1)\n    return feats", "\n\ndef processor(\n    todo: List[str],\n    device: torch.device,\n    embedder_path: str,\n    embedder_load_from: str,\n    embedding_channel: bool,\n    embedding_output_layer: int,\n    wav_dir: str,\n    out_dir: str,\n    process_id: int,\n):\n    half_support = (\n        device.type == \"cuda\" and torch.cuda.get_device_capability(device)[0] >= 5.3\n    )\n    is_feats_dim_768 = embedding_channel == 768\n\n    if embedder_load_from == \"local\" and not os.path.exists(embedder_path):\n        return f\"Embedder not found: {embedder_path}\"\n\n    model, cfg = load_embedder(embedder_path, device)\n\n    for file in tqdm(todo, position=1 + process_id):\n        try:\n            if file.endswith(\".wav\"):\n                wav_filepath = os.path.join(wav_dir, file)\n                out_filepath = os.path.join(out_dir, file.replace(\"wav\", \"npy\"))\n\n                if os.path.exists(out_filepath):\n                    continue\n\n                os.makedirs(os.path.dirname(out_filepath), exist_ok=True)\n\n                is_normalize = False if cfg is None else cfg.task.normalize\n                feats = readwave(wav_filepath, normalize=is_normalize)\n                padding_mask = torch.BoolTensor(feats.shape).fill_(False)\n                if isinstance(model, tuple):\n                    feats = model[0](\n                        feats.squeeze(0).squeeze(0).to(device),\n                        return_tensors=\"pt\",\n                        sampling_rate=16000,\n                    )\n                    if half_support:\n                        feats = feats.input_values.to(device).half()\n                    else:\n                        feats = feats.input_values.to(device).float()\n\n                    with torch.no_grad():\n                        if half_support:\n                            if is_feats_dim_768:\n                                feats = model[1](feats).last_hidden_state\n                            else:\n                                feats = model[1](feats).extract_features\n                        else:\n                            if is_feats_dim_768:\n                                feats = model[1].float()(feats).last_hidden_state\n                            else:\n                                feats = model[1].float()(feats).extract_features\n                else:\n                    inputs = {\n                        \"source\": feats.half().to(device)\n                        if half_support\n                        else feats.to(device),\n                        \"padding_mask\": padding_mask.to(device),\n                        \"output_layer\": embedding_output_layer,\n                    }\n\n                    # \u306a\u3093\u304b\u307e\u3060\u3053\u306e\u6642\u70b9\u3067float16\u306a\u306e\u3067\u6539\u3081\u3066\u5909\u63db\n                    if not half_support:\n                        model = model.float()\n                        inputs[\"source\"] = inputs[\"source\"].float()\n\n                    with torch.no_grad():\n                        logits = model.extract_features(**inputs)\n                        if is_feats_dim_768:\n                            feats = logits[0]\n                        else:\n                            feats = model.final_proj(logits[0])\n\n                feats = feats.squeeze(0).float().cpu().numpy()\n                if np.isnan(feats).sum() == 0:\n                    np.save(out_filepath, feats, allow_pickle=False)\n                else:\n                    print(f\"{file} contains nan\")\n        except Exception as e:\n            print(f\"Error: {e} {file}\")\n            traceback.print_exc()", "\n\ndef run(\n    training_dir: str,\n    embedder_path: str,\n    embedder_load_from: str,\n    embedding_channel: int,\n    embedding_output_layer: int,\n    gpu_ids: List[int],\n    device: Optional[Union[torch.device, str]] = None,\n):\n    wav_dir = os.path.join(training_dir, \"1_16k_wavs\")\n    out_dir = os.path.join(training_dir, \"3_feature256\")\n\n    num_gpus = len(gpu_ids)\n\n    for gpu_id in gpu_ids:\n        if num_gpus < gpu_id + 1:\n            print(f\"GPU {gpu_id} is not available\")\n            return\n\n    if os.path.exists(out_dir):\n        return\n\n    os.makedirs(out_dir, exist_ok=True)\n\n    todo = [\n        os.path.join(dir, f)\n        for dir in sorted(list(os.listdir(wav_dir)))\n        if os.path.isdir(os.path.join(wav_dir, dir))\n        for f in sorted(list(os.listdir(os.path.join(wav_dir, dir))))\n    ]\n\n    if device is not None:\n        if type(device) == str:\n            device = torch.device(device)\n        if device.type == \"mps\":\n            device = torch.device(\n                \"cpu\"\n            )  # Mac(MPS) crashes when multiprocess, so change to CPU.\n        processor(\n            todo,\n            device,\n            embedder_path,\n            embedder_load_from,\n            embedding_channel,\n            embedding_output_layer,\n            wav_dir,\n            out_dir,\n            process_id=0,\n        )\n    else:\n        with ProcessPoolExecutor(mp_context=mp.get_context(\"spawn\")) as executor:\n            for i, id in enumerate(gpu_ids):\n                executor.submit(\n                    processor,\n                    todo[i::num_gpus],\n                    torch.device(f\"cuda:{id}\"),\n                    embedder_path,\n                    embedder_load_from,\n                    embedding_channel,\n                    embedding_output_layer,\n                    wav_dir,\n                    out_dir,\n                    process_id=i,\n                )", ""]}
{"filename": "lib/rvc/preprocessing/split.py", "chunked_list": ["import operator\nimport os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom typing import *\n\nimport librosa\nimport numpy as np\nimport scipy.signal as signal\nfrom scipy.io import wavfile\nfrom tqdm import tqdm", "from scipy.io import wavfile\nfrom tqdm import tqdm\n\nfrom lib.rvc.utils import load_audio\n\nfrom .slicer import Slicer\n\n\ndef norm_write(\n    tmp_audio: np.ndarray,\n    idx0: int,\n    idx1: int,\n    speaker_id: int,\n    outdir: str,\n    outdir_16k: str,\n    sampling_rate: int,\n    max: float,\n    alpha: float,\n    is_normalize: bool,\n):\n    if is_normalize:\n        tmp_audio = (tmp_audio / np.abs(tmp_audio).max() * (max * alpha)) + (\n            1 - alpha\n        ) * tmp_audio\n    else:\n        # clip level to max (cause sometimes when floating point decoding)\n        audio_min = np.min(tmp_audio)\n        if audio_min < -max:\n            tmp_audio = tmp_audio / -audio_min * max\n        audio_max = np.max(tmp_audio)\n        if audio_max > max:\n            tmp_audio = tmp_audio / audio_max * max\n\n    wavfile.write(\n        os.path.join(outdir, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n        sampling_rate,\n        tmp_audio.astype(np.float32),\n    )\n\n    tmp_audio = librosa.resample(\n        tmp_audio, orig_sr=sampling_rate, target_sr=16000, res_type=\"soxr_vhq\"\n    )\n    wavfile.write(\n        os.path.join(outdir_16k, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n        16000,\n        tmp_audio.astype(np.float32),\n    )", "def norm_write(\n    tmp_audio: np.ndarray,\n    idx0: int,\n    idx1: int,\n    speaker_id: int,\n    outdir: str,\n    outdir_16k: str,\n    sampling_rate: int,\n    max: float,\n    alpha: float,\n    is_normalize: bool,\n):\n    if is_normalize:\n        tmp_audio = (tmp_audio / np.abs(tmp_audio).max() * (max * alpha)) + (\n            1 - alpha\n        ) * tmp_audio\n    else:\n        # clip level to max (cause sometimes when floating point decoding)\n        audio_min = np.min(tmp_audio)\n        if audio_min < -max:\n            tmp_audio = tmp_audio / -audio_min * max\n        audio_max = np.max(tmp_audio)\n        if audio_max > max:\n            tmp_audio = tmp_audio / audio_max * max\n\n    wavfile.write(\n        os.path.join(outdir, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n        sampling_rate,\n        tmp_audio.astype(np.float32),\n    )\n\n    tmp_audio = librosa.resample(\n        tmp_audio, orig_sr=sampling_rate, target_sr=16000, res_type=\"soxr_vhq\"\n    )\n    wavfile.write(\n        os.path.join(outdir_16k, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n        16000,\n        tmp_audio.astype(np.float32),\n    )", "\n\ndef write_mute(\n    mute_wave_filename: str,\n    speaker_id: int,\n    outdir: str,\n    outdir_16k: str,\n    sampling_rate: int,\n):\n    tmp_audio = load_audio(mute_wave_filename, sampling_rate)\n    wavfile.write(\n        os.path.join(outdir, f\"{speaker_id:05}\", \"mute.wav\"),\n        sampling_rate,\n        tmp_audio.astype(np.float32),\n    )\n    tmp_audio = librosa.resample(\n        tmp_audio, orig_sr=sampling_rate, target_sr=16000, res_type=\"soxr_vhq\"\n    )\n    wavfile.write(\n        os.path.join(outdir_16k, f\"{speaker_id:05}\", \"mute.wav\"),\n        16000,\n        tmp_audio.astype(np.float32),\n    )", "\n\ndef pipeline(\n    slicer: Slicer,\n    datasets: List[Tuple[str, int]],  # List[(path, speaker_id)]\n    outdir: str,\n    outdir_16k: str,\n    sampling_rate: int,\n    is_normalize: bool,\n    process_id: int = 0,\n):\n    per = 3.7\n    overlap = 0.3\n    tail = per + overlap\n    max = 0.95\n    alpha = 0.8\n\n    bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=sampling_rate)\n\n    for index, (wave_filename, speaker_id) in tqdm(datasets, position=1 + process_id):\n        audio = load_audio(wave_filename, sampling_rate)\n        audio = signal.lfilter(bh, ah, audio)\n\n        idx1 = 0\n        for audio in slicer.slice(audio):\n            i = 0\n            while 1:\n                start = int(sampling_rate * (per - overlap) * i)\n                i += 1\n                if len(audio[start:]) > tail * sampling_rate:\n                    tmp_audio = audio[start : start + int(per * sampling_rate)]\n                    norm_write(\n                        tmp_audio,\n                        index,\n                        idx1,\n                        speaker_id,\n                        outdir,\n                        outdir_16k,\n                        sampling_rate,\n                        max,\n                        alpha,\n                        is_normalize,\n                    )\n                    idx1 += 1\n                else:\n                    tmp_audio = audio[start:]\n                    break\n            norm_write(\n                tmp_audio,\n                index,\n                idx1,\n                speaker_id,\n                outdir,\n                outdir_16k,\n                sampling_rate,\n                max,\n                alpha,\n                is_normalize,\n            )\n            idx1 += 1", "\n\ndef preprocess_audio(\n    datasets: List[Tuple[str, int]],  # List[(path, speaker_id)]\n    sampling_rate: int,\n    num_processes: int,\n    training_dir: str,\n    is_normalize: bool,\n    mute_wav_path: str,\n):\n    waves_dir = os.path.join(training_dir, \"0_gt_wavs\")\n    waves16k_dir = os.path.join(training_dir, \"1_16k_wavs\")\n    if os.path.exists(waves_dir) and os.path.exists(waves16k_dir):\n        return\n\n    for speaker_id in set([spk for _, spk in datasets]):\n        os.makedirs(os.path.join(waves_dir, f\"{speaker_id:05}\"), exist_ok=True)\n        os.makedirs(os.path.join(waves16k_dir, f\"{speaker_id:05}\"), exist_ok=True)\n\n    all = [(i, x) for i, x in enumerate(sorted(datasets, key=operator.itemgetter(0)))]\n\n    # n of datasets per process\n    process_all_nums = [len(all) // num_processes] * num_processes\n    # add residual datasets\n    for i in range(len(all) % num_processes):\n        process_all_nums[i] += 1\n\n    assert len(all) == sum(process_all_nums), print(\n        f\"len(all): {len(all)}, sum(process_all_nums): {sum(process_all_nums)}\"\n    )\n\n    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n        all_index = 0\n        for i in range(num_processes):\n            data = all[all_index : all_index + process_all_nums[i]]\n            slicer = Slicer(\n                sr=sampling_rate,\n                threshold=-42,\n                min_length=1500,\n                min_interval=400,\n                hop_size=15,\n                max_sil_kept=500,\n            )\n            executor.submit(\n                pipeline,\n                slicer,\n                data,\n                waves_dir,\n                waves16k_dir,\n                sampling_rate,\n                is_normalize,\n                process_id=i,\n            )\n            all_index += process_all_nums[i]\n\n    for speaker_id in set([spk for _, spk in datasets]):\n        write_mute(mute_wav_path, speaker_id, waves_dir, waves16k_dir, sampling_rate)", ""]}
{"filename": "lib/rvc/preprocessing/slicer.py", "chunked_list": ["import numpy as np\n\n\n# This function is obtained from librosa.\ndef get_rms(\n    y,\n    frame_length=2048,\n    hop_length=512,\n    pad_mode=\"constant\",\n):\n    padding = (int(frame_length // 2), int(frame_length // 2))\n    y = np.pad(y, padding, mode=pad_mode)\n\n    axis = -1\n    # put our new within-frame axis at the end for now\n    out_strides = y.strides + tuple([y.strides[axis]])\n    # Reduce the shape on the framing axis\n    x_shape_trimmed = list(y.shape)\n    x_shape_trimmed[axis] -= frame_length - 1\n    out_shape = tuple(x_shape_trimmed) + tuple([frame_length])\n    xw = np.lib.stride_tricks.as_strided(y, shape=out_shape, strides=out_strides)\n    if axis < 0:\n        target_axis = axis - 1\n    else:\n        target_axis = axis + 1\n    xw = np.moveaxis(xw, -1, target_axis)\n    # Downsample along the target axis\n    slices = [slice(None)] * xw.ndim\n    slices[axis] = slice(0, None, hop_length)\n    x = xw[tuple(slices)]\n\n    # Calculate power\n    power = np.mean(np.abs(x) ** 2, axis=-2, keepdims=True)\n\n    return np.sqrt(power)", "\n\nclass Slicer:\n    def __init__(\n        self,\n        sr: int,\n        threshold: float = -40.0,\n        min_length: int = 5000,\n        min_interval: int = 300,\n        hop_size: int = 20,\n        max_sil_kept: int = 5000,\n    ):\n        if not min_length >= min_interval >= hop_size:\n            raise ValueError(\n                \"The following condition must be satisfied: min_length >= min_interval >= hop_size\"\n            )\n        if not max_sil_kept >= hop_size:\n            raise ValueError(\n                \"The following condition must be satisfied: max_sil_kept >= hop_size\"\n            )\n        min_interval = sr * min_interval / 1000\n        self.threshold = 10 ** (threshold / 20.0)\n        self.hop_size = round(sr * hop_size / 1000)\n        self.win_size = min(round(min_interval), 4 * self.hop_size)\n        self.min_length = round(sr * min_length / 1000 / self.hop_size)\n        self.min_interval = round(min_interval / self.hop_size)\n        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)\n\n    def _apply_slice(self, waveform, begin, end):\n        if len(waveform.shape) > 1:\n            return waveform[\n                :, begin * self.hop_size : min(waveform.shape[1], end * self.hop_size)\n            ]\n        else:\n            return waveform[\n                begin * self.hop_size : min(waveform.shape[0], end * self.hop_size)\n            ]\n\n    # @timeit\n    def slice(self, waveform):\n        if len(waveform.shape) > 1:\n            samples = waveform.mean(axis=0)\n        else:\n            samples = waveform\n        if samples.shape[0] <= self.min_length:\n            return [waveform]\n        rms_list = get_rms(\n            y=samples, frame_length=self.win_size, hop_length=self.hop_size\n        ).squeeze(0)\n        sil_tags = []\n        silence_start = None\n        clip_start = 0\n        for i, rms in enumerate(rms_list):\n            # Keep looping while frame is silent.\n            if rms < self.threshold:\n                # Record start of silent frames.\n                if silence_start is None:\n                    silence_start = i\n                continue\n            # Keep looping while frame is not silent and silence start has not been recorded.\n            if silence_start is None:\n                continue\n            # Clear recorded silence start if interval is not enough or clip is too short\n            is_leading_silence = silence_start == 0 and i > self.max_sil_kept\n            need_slice_middle = (\n                i - silence_start >= self.min_interval\n                and i - clip_start >= self.min_length\n            )\n            if not is_leading_silence and not need_slice_middle:\n                silence_start = None\n                continue\n            # Need slicing. Record the range of silent frames to be removed.\n            if i - silence_start <= self.max_sil_kept:\n                pos = rms_list[silence_start : i + 1].argmin() + silence_start\n                if silence_start == 0:\n                    sil_tags.append((0, pos))\n                else:\n                    sil_tags.append((pos, pos))\n                clip_start = pos\n            elif i - silence_start <= self.max_sil_kept * 2:\n                pos = rms_list[\n                    i - self.max_sil_kept : silence_start + self.max_sil_kept + 1\n                ].argmin()\n                pos += i - self.max_sil_kept\n                pos_l = (\n                    rms_list[\n                        silence_start : silence_start + self.max_sil_kept + 1\n                    ].argmin()\n                    + silence_start\n                )\n                pos_r = (\n                    rms_list[i - self.max_sil_kept : i + 1].argmin()\n                    + i\n                    - self.max_sil_kept\n                )\n                if silence_start == 0:\n                    sil_tags.append((0, pos_r))\n                    clip_start = pos_r\n                else:\n                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))\n                    clip_start = max(pos_r, pos)\n            else:\n                pos_l = (\n                    rms_list[\n                        silence_start : silence_start + self.max_sil_kept + 1\n                    ].argmin()\n                    + silence_start\n                )\n                pos_r = (\n                    rms_list[i - self.max_sil_kept : i + 1].argmin()\n                    + i\n                    - self.max_sil_kept\n                )\n                if silence_start == 0:\n                    sil_tags.append((0, pos_r))\n                else:\n                    sil_tags.append((pos_l, pos_r))\n                clip_start = pos_r\n            silence_start = None\n        # Deal with trailing silence.\n        total_frames = rms_list.shape[0]\n        if (\n            silence_start is not None\n            and total_frames - silence_start >= self.min_interval\n        ):\n            silence_end = min(total_frames, silence_start + self.max_sil_kept)\n            pos = rms_list[silence_start : silence_end + 1].argmin() + silence_start\n            sil_tags.append((pos, total_frames + 1))\n        # Apply and return slices.\n        if len(sil_tags) == 0:\n            return [waveform]\n        else:\n            chunks = []\n            if sil_tags[0][0] > 0:\n                chunks.append(self._apply_slice(waveform, 0, sil_tags[0][0]))\n            for i in range(len(sil_tags) - 1):\n                chunks.append(\n                    self._apply_slice(waveform, sil_tags[i][1], sil_tags[i + 1][0])\n                )\n            if sil_tags[-1][1] < total_frames:\n                chunks.append(\n                    self._apply_slice(waveform, sil_tags[-1][1], total_frames)\n                )\n            return chunks", ""]}
{"filename": "modules/merge.py", "chunked_list": ["from collections import OrderedDict\nfrom typing import *\n\nimport torch\nimport tqdm\n\n\ndef merge(\n    path_a: str,\n    path_b: str,\n    path_c: str,\n    alpha: float,\n    weights: Dict[str, float],\n    method: str,\n):\n    def extract(ckpt: Dict[str, Any]):\n        a = ckpt[\"model\"]\n        opt = OrderedDict()\n        opt[\"weight\"] = {}\n        for key in a.keys():\n            if \"enc_q\" in key:\n                continue\n            opt[\"weight\"][key] = a[key]\n        return opt\n\n    def load_weight(path: str):\n        print(f\"Loading {path}...\")\n        state_dict = torch.load(path, map_location=\"cpu\")\n        if \"model\" in state_dict:\n            weight = extract(state_dict)\n        else:\n            weight = state_dict[\"weight\"]\n        return weight, state_dict\n\n    def get_alpha(key: str):\n        try:\n            filtered = sorted(\n                [x for x in weights.keys() if key.startswith(x)], key=len, reverse=True\n            )\n            if len(filtered) < 1:\n                return alpha\n            return weights[filtered[0]]\n        except:\n            return alpha\n\n    weight_a, state_dict = load_weight(path_a)\n    weight_b, _ = load_weight(path_b)\n    if path_c is not None:\n        weight_c, _ = load_weight(path_c)\n\n    if sorted(list(weight_a.keys())) != sorted(list(weight_b.keys())):\n        raise RuntimeError(\"Failed to merge models.\")\n\n    merged = OrderedDict()\n    merged[\"weight\"] = {}\n\n    def merge_weight(a, b, c, alpha):\n        if method == \"weight_sum\":\n            return (1 - alpha) * a + alpha * b\n        elif method == \"add_diff\":\n            return a + (b - c) * alpha\n\n    for key in tqdm.tqdm(weight_a.keys()):\n        a = get_alpha(key)\n        if path_c is not None:\n            merged[\"weight\"][key] = merge_weight(\n                weight_a[key], weight_b[key], weight_c[key], a\n            )\n        else:\n            merged[\"weight\"][key] = merge_weight(weight_a[key], weight_b[key], None, a)\n    merged[\"config\"] = state_dict[\"config\"]\n    merged[\"params\"] = state_dict[\"params\"] if \"params\" in state_dict else None\n    merged[\"version\"] = state_dict.get(\"version\", \"v1\")\n    merged[\"sr\"] = state_dict[\"sr\"]\n    merged[\"f0\"] = state_dict[\"f0\"]\n    merged[\"info\"] = state_dict[\"info\"]\n    merged[\"embedder_name\"] = (\n        state_dict[\"embedder_name\"] if \"embedder_name\" in state_dict else None\n    )\n    merged[\"embedder_output_layer\"] = state_dict.get(\"embedder_output_layer\", \"12\")\n    return merged", ""]}
{"filename": "modules/cmd_opts.py", "chunked_list": ["import argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--host\", help=\"Host to connect to\", type=str, default=\"127.0.0.1\")\nparser.add_argument(\"--port\", help=\"Port to connect to\", type=int)\nparser.add_argument(\"--share\", help=\"Enable gradio share\", action=\"store_true\")\nparser.add_argument(\n    \"--models-dir\", help=\"Path to models directory\", type=str, default=None\n)", "    \"--models-dir\", help=\"Path to models directory\", type=str, default=None\n)\nparser.add_argument(\n    \"--output-dir\", help=\"Path to output directory\", type=str, default=None\n)\nparser.add_argument(\n    \"--precision\",\n    help=\"Precision to use\",\n    type=str,\n    default=\"fp16\",", "    type=str,\n    default=\"fp16\",\n    choices=[\"fp32\", \"fp16\"],\n)\n\nopts, _ = parser.parse_known_args()\n"]}
{"filename": "modules/separate.py", "chunked_list": ["import os\nfrom typing import *\n\nimport tqdm\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\n\n\ndef separate_audio(\n    input: str,\n    output: str,\n    silence_thresh: int,\n    min_silence_len: int = 1000,\n    keep_silence: int = 100,\n    margin: int = 0,\n    padding: bool = False,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n):\n    if os.path.isfile(input):\n        input = [input]\n    elif os.path.isdir(input):\n        input = [os.path.join(input, f) for f in os.listdir(input)]\n    else:\n        raise ValueError(\"input must be a file or directory\")\n\n    os.makedirs(output, exist_ok=True)\n\n    for file in input:\n        if os.path.splitext(file)[1] == \".mp3\":\n            audio = AudioSegment.from_mp3(file)\n        elif os.path.splitext(file)[1] == \".wav\":\n            audio = AudioSegment.from_wav(file)\n        elif os.path.splitext(file)[1] == \".flac\":\n            audio = AudioSegment.from_file(file, \"flac\")\n        else:\n            raise ValueError(\n                \"Invalid file format. Only MP3 and WAV files are supported.\"\n            )\n\n        chunks = split_on_silence(\n            audio,\n            min_silence_len=min_silence_len,\n            silence_thresh=silence_thresh,\n            keep_silence=keep_silence,\n        )\n\n        output_chunks: List[AudioSegment] = []\n\n        so_short = None\n\n        for chunk in tqdm.tqdm(chunks):\n            if so_short is not None:\n                chunk = so_short + chunk\n                so_short = None\n            if min is None or len(chunk) > min:\n                if max is not None and len(chunk) > max:\n                    sub_chunks = [\n                        chunk[i : i + max + margin]\n                        for i in range(0, len(chunk) - margin, max)\n                    ]\n\n                    if len(sub_chunks[-1]) < min:\n                        if padding and len(sub_chunks) > 2:\n                            output_chunks.extend(sub_chunks[0:-2])\n                            output_chunks.append(sub_chunks[-2] + sub_chunks[-1])\n                        else:\n                            output_chunks.extend(sub_chunks[0:-1])\n                    else:\n                        output_chunks.extend(sub_chunks)\n                else:\n                    output_chunks.append(chunk)\n            else:\n                if so_short is None:\n                    so_short = chunk\n                else:\n                    so_short += chunk\n        basename = os.path.splitext(os.path.basename(file))[0]\n\n        for i, chunk in enumerate(output_chunks):\n            filepath = os.path.join(output, f\"{basename}_{i}.wav\")\n            chunk.export(filepath, format=\"wav\")", "def separate_audio(\n    input: str,\n    output: str,\n    silence_thresh: int,\n    min_silence_len: int = 1000,\n    keep_silence: int = 100,\n    margin: int = 0,\n    padding: bool = False,\n    min: Optional[int] = None,\n    max: Optional[int] = None,\n):\n    if os.path.isfile(input):\n        input = [input]\n    elif os.path.isdir(input):\n        input = [os.path.join(input, f) for f in os.listdir(input)]\n    else:\n        raise ValueError(\"input must be a file or directory\")\n\n    os.makedirs(output, exist_ok=True)\n\n    for file in input:\n        if os.path.splitext(file)[1] == \".mp3\":\n            audio = AudioSegment.from_mp3(file)\n        elif os.path.splitext(file)[1] == \".wav\":\n            audio = AudioSegment.from_wav(file)\n        elif os.path.splitext(file)[1] == \".flac\":\n            audio = AudioSegment.from_file(file, \"flac\")\n        else:\n            raise ValueError(\n                \"Invalid file format. Only MP3 and WAV files are supported.\"\n            )\n\n        chunks = split_on_silence(\n            audio,\n            min_silence_len=min_silence_len,\n            silence_thresh=silence_thresh,\n            keep_silence=keep_silence,\n        )\n\n        output_chunks: List[AudioSegment] = []\n\n        so_short = None\n\n        for chunk in tqdm.tqdm(chunks):\n            if so_short is not None:\n                chunk = so_short + chunk\n                so_short = None\n            if min is None or len(chunk) > min:\n                if max is not None and len(chunk) > max:\n                    sub_chunks = [\n                        chunk[i : i + max + margin]\n                        for i in range(0, len(chunk) - margin, max)\n                    ]\n\n                    if len(sub_chunks[-1]) < min:\n                        if padding and len(sub_chunks) > 2:\n                            output_chunks.extend(sub_chunks[0:-2])\n                            output_chunks.append(sub_chunks[-2] + sub_chunks[-1])\n                        else:\n                            output_chunks.extend(sub_chunks[0:-1])\n                    else:\n                        output_chunks.extend(sub_chunks)\n                else:\n                    output_chunks.append(chunk)\n            else:\n                if so_short is None:\n                    so_short = chunk\n                else:\n                    so_short += chunk\n        basename = os.path.splitext(os.path.basename(file))[0]\n\n        for i, chunk in enumerate(output_chunks):\n            filepath = os.path.join(output, f\"{basename}_{i}.wav\")\n            chunk.export(filepath, format=\"wav\")", ""]}
{"filename": "modules/models.py", "chunked_list": ["import os\nimport re\nfrom typing import *\n\nimport torch\nfrom fairseq import checkpoint_utils\nfrom fairseq.models.hubert.hubert import HubertModel\nfrom pydub import AudioSegment\n\nfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,", "\nfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,\n                            SynthesizerTrnMs256NSFSidNono)\nfrom lib.rvc.pipeline import VocalConvertPipeline\n\nfrom .cmd_opts import opts\nfrom .shared import ROOT_DIR, device, is_half\nfrom .utils import load_audio\n\nAUDIO_OUT_DIR = opts.output_dir or os.path.join(ROOT_DIR, \"outputs\")", "\nAUDIO_OUT_DIR = opts.output_dir or os.path.join(ROOT_DIR, \"outputs\")\n\n\nEMBEDDINGS_LIST = {\n    \"hubert-base-japanese\": (\n        \"rinna_hubert_base_jp.pt\",\n        \"hubert-base-japanese\",\n        \"local\",\n    ),", "        \"local\",\n    ),\n    \"contentvec\": (\"checkpoint_best_legacy_500.pt\", \"contentvec\", \"local\"),\n}\n\n\ndef update_state_dict(state_dict):\n    if \"params\" in state_dict and state_dict[\"params\"] is not None:\n        return\n    keys = [\n        \"spec_channels\",\n        \"segment_size\",\n        \"inter_channels\",\n        \"hidden_channels\",\n        \"filter_channels\",\n        \"n_heads\",\n        \"n_layers\",\n        \"kernel_size\",\n        \"p_dropout\",\n        \"resblock\",\n        \"resblock_kernel_sizes\",\n        \"resblock_dilation_sizes\",\n        \"upsample_rates\",\n        \"upsample_initial_channel\",\n        \"upsample_kernel_sizes\",\n        \"spk_embed_dim\",\n        \"gin_channels\",\n        \"emb_channels\",\n        \"sr\",\n    ]\n    state_dict[\"params\"] = {}\n    n = 0\n    for i, key in enumerate(keys):\n        i = i - n\n        if len(state_dict[\"config\"]) != 19 and key == \"emb_channels\":\n            # backward compat.\n            n += 1\n            continue\n        state_dict[\"params\"][key] = state_dict[\"config\"][i]\n\n    if not \"emb_channels\" in state_dict[\"params\"]:\n        if state_dict.get(\"version\", \"v1\") == \"v1\":\n            state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n            state_dict[\"embedder_output_layer\"] = 9\n        else:\n            state_dict[\"params\"][\"emb_channels\"] = 768  # for backward compat.\n            state_dict[\"embedder_output_layer\"] = 12", "\n\nclass VoiceConvertModel:\n    def __init__(self, model_name: str, state_dict: Dict[str, Any]) -> None:\n        update_state_dict(state_dict)\n        self.model_name = model_name\n        self.state_dict = state_dict\n        self.tgt_sr = state_dict[\"params\"][\"sr\"]\n        f0 = state_dict.get(\"f0\", 1)\n        state_dict[\"params\"][\"spk_embed_dim\"] = state_dict[\"weight\"][\n            \"emb_g.weight\"\n        ].shape[0]\n        if not \"emb_channels\" in state_dict[\"params\"]:\n            state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n\n        if f0 == 1:\n            self.net_g = SynthesizerTrnMs256NSFSid(\n                **state_dict[\"params\"], is_half=is_half\n            )\n        else:\n            self.net_g = SynthesizerTrnMs256NSFSidNono(**state_dict[\"params\"])\n\n        del self.net_g.enc_q\n\n        self.net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n        self.net_g.eval().to(device)\n\n        if is_half:\n            self.net_g = self.net_g.half()\n        else:\n            self.net_g = self.net_g.float()\n\n        self.vc = VocalConvertPipeline(self.tgt_sr, device, is_half)\n        self.n_spk = state_dict[\"params\"][\"spk_embed_dim\"]\n\n    def single(\n        self,\n        sid: int,\n        input_audio: str,\n        embedder_model_name: str,\n        embedding_output_layer: str,\n        f0_up_key: int,\n        f0_file: str,\n        f0_method: str,\n        auto_load_index: bool,\n        faiss_index_file: str,\n        index_rate: float,\n        output_dir: str = AUDIO_OUT_DIR,\n    ):\n        if not input_audio:\n            raise Exception(\"You need to set Source Audio\")\n        f0_up_key = int(f0_up_key)\n        audio = load_audio(input_audio, 16000)\n\n        if embedder_model_name == \"auto\":\n            embedder_model_name = (\n                self.state_dict[\"embedder_name\"]\n                if \"embedder_name\" in self.state_dict\n                else \"hubert_base\"\n            )\n            if embedder_model_name.endswith(\"768\"):\n                embedder_model_name = embedder_model_name[:-3]\n\n        if embedder_model_name == \"hubert_base\":\n            embedder_model_name = \"contentvec\"\n\n        if not embedder_model_name in EMBEDDINGS_LIST.keys():\n            raise Exception(f\"Not supported embedder: {embedder_model_name}\")\n\n        if (\n            embedder_model == None\n            or loaded_embedder_model != EMBEDDINGS_LIST[embedder_model_name][1]\n        ):\n            print(f\"load {embedder_model_name} embedder\")\n            embedder_filename, embedder_name, load_from = get_embedder(\n                embedder_model_name\n            )\n            load_embedder(embedder_filename, embedder_name)\n\n        if embedding_output_layer == \"auto\":\n            embedding_output_layer = (\n                self.state_dict[\"embedding_output_layer\"]\n                if \"embedding_output_layer\" in self.state_dict\n                else 12\n            )\n        else:\n            embedding_output_layer = int(embedding_output_layer)\n\n        f0 = self.state_dict.get(\"f0\", 1)\n\n        if not faiss_index_file and auto_load_index:\n            faiss_index_file = self.get_index_path(sid)\n\n        audio_opt = self.vc(\n            embedder_model,\n            embedding_output_layer,\n            self.net_g,\n            sid,\n            audio,\n            f0_up_key,\n            f0_method,\n            faiss_index_file,\n            index_rate,\n            f0,\n            f0_file=f0_file,\n        )\n\n        audio = AudioSegment(\n            audio_opt,\n            frame_rate=self.tgt_sr,\n            sample_width=2,\n            channels=1,\n        )\n        os.makedirs(output_dir, exist_ok=True)\n        input_audio_splitext = os.path.splitext(os.path.basename(input_audio))[0]\n        model_splitext = os.path.splitext(self.model_name)[0]\n        index = 0\n        existing_files = os.listdir(output_dir)\n        for existing_file in existing_files:\n            result = re.match(r\"\\d+\", existing_file)\n            if result:\n                prefix_num = int(result.group(0))\n                if index < prefix_num:\n                    index = prefix_num\n        audio.export(\n            os.path.join(\n                output_dir, f\"{index+1}-{model_splitext}-{input_audio_splitext}.wav\"\n            ),\n            format=\"wav\",\n        )\n        return audio_opt\n\n    def get_index_path(self, speaker_id: int):\n        basename = os.path.splitext(self.model_name)[0]\n        speaker_index_path = os.path.join(\n            MODELS_DIR,\n            \"checkpoints\",\n            f\"{basename}_index\",\n            f\"{basename}.{speaker_id}.index\",\n        )\n        if os.path.exists(speaker_index_path):\n            return speaker_index_path\n        return os.path.join(MODELS_DIR, \"checkpoints\", f\"{basename}.index\")", "\n\nMODELS_DIR = opts.models_dir or os.path.join(ROOT_DIR, \"models\")\nvc_model: Optional[VoiceConvertModel] = None\nembedder_model: Optional[HubertModel] = None\nloaded_embedder_model = \"\"\n\n\ndef get_models():\n    dir = os.path.join(ROOT_DIR, \"models\", \"checkpoints\")\n    os.makedirs(dir, exist_ok=True)\n    return [\n        file\n        for file in os.listdir(dir)\n        if any([x for x in [\".ckpt\", \".pth\"] if file.endswith(x)])\n    ]", "def get_models():\n    dir = os.path.join(ROOT_DIR, \"models\", \"checkpoints\")\n    os.makedirs(dir, exist_ok=True)\n    return [\n        file\n        for file in os.listdir(dir)\n        if any([x for x in [\".ckpt\", \".pth\"] if file.endswith(x)])\n    ]\n\n\ndef get_embedder(embedder_name):\n    if embedder_name in EMBEDDINGS_LIST:\n        return EMBEDDINGS_LIST[embedder_name]\n    return None", "\n\ndef get_embedder(embedder_name):\n    if embedder_name in EMBEDDINGS_LIST:\n        return EMBEDDINGS_LIST[embedder_name]\n    return None\n\n\ndef load_embedder(emb_file: str, emb_name: str):\n    global embedder_model, loaded_embedder_model\n    emb_file = os.path.join(MODELS_DIR, \"embeddings\", emb_file)\n    models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n        [emb_file],\n        suffix=\"\",\n    )\n    embedder_model = models[0]\n    embedder_model = embedder_model.to(device)\n\n    if is_half:\n        embedder_model = embedder_model.half()\n    else:\n        embedder_model = embedder_model.float()\n    embedder_model.eval()\n\n    loaded_embedder_model = emb_name", "def load_embedder(emb_file: str, emb_name: str):\n    global embedder_model, loaded_embedder_model\n    emb_file = os.path.join(MODELS_DIR, \"embeddings\", emb_file)\n    models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n        [emb_file],\n        suffix=\"\",\n    )\n    embedder_model = models[0]\n    embedder_model = embedder_model.to(device)\n\n    if is_half:\n        embedder_model = embedder_model.half()\n    else:\n        embedder_model = embedder_model.float()\n    embedder_model.eval()\n\n    loaded_embedder_model = emb_name", "\n\ndef get_vc_model(model_name: str):\n    model_path = os.path.join(MODELS_DIR, \"checkpoints\", model_name)\n    weight = torch.load(model_path, map_location=\"cpu\")\n    return VoiceConvertModel(model_name, weight)\n\n\ndef load_model(model_name: str):\n    global vc_model\n    vc_model = get_vc_model(model_name)", "def load_model(model_name: str):\n    global vc_model\n    vc_model = get_vc_model(model_name)\n"]}
{"filename": "modules/ui.py", "chunked_list": ["import importlib\nimport os\nfrom typing import *\n\nimport gradio as gr\nimport gradio.routes\nimport torch\n\nfrom . import models, shared\nfrom .core import preload", "from . import models, shared\nfrom .core import preload\nfrom .shared import ROOT_DIR\n\n\nclass Tab:\n    TABS_DIR = os.path.join(ROOT_DIR, \"modules\", \"tabs\")\n\n    def __init__(self, filepath: str) -> None:\n        self.filepath = filepath\n\n    def sort(self):\n        return 1\n\n    def title(self):\n        return \"\"\n\n    def ui(self, outlet: Callable):\n        pass\n\n    def __call__(self):\n        children_dir = self.filepath[:-3]\n        children = []\n\n        if os.path.isdir(children_dir):\n            for file in os.listdir(children_dir):\n                if not file.endswith(\".py\"):\n                    continue\n                module_name = file[:-3]\n                parent = os.path.relpath(Tab.TABS_DIR, Tab.TABS_DIR).replace(\"/\", \".\")\n\n                if parent.startswith(\".\"):\n                    parent = parent[1:]\n                if parent.endswith(\".\"):\n                    parent = parent[:-1]\n\n                children.append(\n                    importlib.import_module(f\"modules.tabs.{parent}.{module_name}\")\n                )\n\n        children = sorted(children, key=lambda x: x.sort())\n\n        tabs = []\n\n        for child in children:\n            attrs = child.__dict__\n            tab = [x for x in attrs.values() if issubclass(x, Tab)]\n            if len(tab) > 0:\n                tabs.append(tab[0])\n\n        def outlet():\n            with gr.Tabs():\n                for tab in tabs:\n                    with gr.Tab(tab.title()):\n                        tab()\n\n        return self.ui(outlet)", "\n\ndef load_tabs() -> List[Tab]:\n    tabs = []\n    files = os.listdir(os.path.join(ROOT_DIR, \"modules\", \"tabs\"))\n\n    for file in files:\n        if not file.endswith(\".py\"):\n            continue\n        module_name = file[:-3]\n        module = importlib.import_module(f\"modules.tabs.{module_name}\")\n        attrs = module.__dict__\n        TabClass = [\n            x\n            for x in attrs.values()\n            if type(x) == type and issubclass(x, Tab) and not x == Tab\n        ]\n        if len(TabClass) > 0:\n            tabs.append((file, TabClass[0]))\n\n    tabs = sorted([TabClass(file) for file, TabClass in tabs], key=lambda x: x.sort())\n    return tabs", "\n\ndef webpath(fn):\n    if fn.startswith(ROOT_DIR):\n        web_path = os.path.relpath(fn, ROOT_DIR).replace(\"\\\\\", \"/\")\n    else:\n        web_path = os.path.abspath(fn)\n\n    return f\"file={web_path}?{os.path.getmtime(fn)}\"\n", "\n\ndef javascript_html():\n    script_js = os.path.join(ROOT_DIR, \"script.js\")\n    head = f'<script type=\"text/javascript\" src=\"{webpath(script_js)}\"></script>\\n'\n\n    return head\n\n\ndef css_html():\n    return f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{webpath(os.path.join(ROOT_DIR, \"styles.css\"))}\">'", "\ndef css_html():\n    return f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{webpath(os.path.join(ROOT_DIR, \"styles.css\"))}\">'\n\n\ndef create_head():\n    head = \"\"\n    head += css_html()\n    head += javascript_html()\n\n    def template_response(*args, **kwargs):\n        res = shared.gradio_template_response_original(*args, **kwargs)\n        res.body = res.body.replace(b\"</head>\", f\"{head}</head>\".encode(\"utf8\"))\n        res.init_headers()\n        return res\n\n    gradio.routes.templates.TemplateResponse = template_response", "\n\ndef create_ui():\n    preload()\n    block = gr.Blocks()\n\n    with block:\n        with gr.Tabs():\n            tabs = load_tabs()\n            for tab in tabs:\n                with gr.Tab(tab.title()):\n                    tab()\n\n    create_head()\n\n    return block", "\n\ndef create_model_list_ui(speaker_id: bool = True, load: bool = True):\n    speaker_id_info = {\n        \"visible\": False,\n        \"maximum\": 10000,\n    }\n\n    def reload_model(raw=False):\n        model_list = models.get_models()\n        if len(model_list) > 0:\n            models.load_model(model_list[0])\n\n        if models.vc_model is not None:\n            speaker_id_info[\"visible\"] = True\n            speaker_id_info[\"maximum\"] = models.vc_model.n_spk\n\n        return model_list if raw else gr.Dropdown.update(choices=model_list)\n\n    model_list = reload_model(raw=True)\n\n    def load_model(model_name):\n        if load:\n            models.load_model(model_name)\n            speaker_id_info[\"visible\"] = True\n            speaker_id_info[\"maximum\"] = models.vc_model.n_spk\n        else:\n            model = models.get_vc_model(model_name)\n            speaker_id_info[\"visible\"] = True\n            speaker_id_info[\"maximum\"] = model.n_spk\n            del model\n            torch.cuda.empty_cache()\n        return gr.Slider.update(\n            maximum=speaker_id_info[\"maximum\"], visible=speaker_id_info[\"visible\"]\n        )\n\n    with gr.Row(equal_height=False):\n        model = gr.Dropdown(\n            choices=model_list,\n            label=\"Model\",\n            value=model_list[0] if len(model_list) > 0 else None,\n        )\n        speaker_id = gr.Slider(\n            minimum=0,\n            maximum=speaker_id_info[\"maximum\"],\n            step=1,\n            label=\"Speaker ID\",\n            value=0,\n            visible=speaker_id and speaker_id_info[\"visible\"],\n            interactive=True,\n        )\n        reload_model_button = gr.Button(\"\u267b\ufe0f\")\n\n        model.change(load_model, inputs=[model], outputs=[speaker_id])\n        reload_model_button.click(reload_model, outputs=[model])\n\n    return model, speaker_id", "\n\nif not hasattr(shared, \"gradio_template_response_original\"):\n    shared.gradio_template_response_original = gradio.routes.templates.TemplateResponse\n"]}
{"filename": "modules/utils.py", "chunked_list": ["import os\nfrom typing import *\n\nimport ffmpeg\nimport numpy as np\nimport requests\nimport torch\nfrom tqdm import tqdm\n\nfrom lib.rvc.config import TrainConfig", "\nfrom lib.rvc.config import TrainConfig\nfrom modules.shared import ROOT_DIR\n\n\ndef load_audio(file: str, sr):\n    try:\n        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n        file = (\n            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n        )  # Prevent small white copy path head and tail with spaces and \" and return\n        out, _ = (\n            ffmpeg.input(file, threads=0)\n            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load audio: {e}\")\n\n    return np.frombuffer(out, np.float32).flatten()", "\n\ndef get_gpus():\n    num_gpus = torch.cuda.device_count()\n    return [torch.device(f\"cuda:{i}\") for i in range(num_gpus)]\n\n\ndef download_file(url: str, out: str, position: int = 0, show: bool = True):\n    req = requests.get(url, stream=True, allow_redirects=True)\n    content_length = req.headers.get(\"content-length\")\n    if show:\n        progress_bar = tqdm(\n            total=int(content_length) if content_length is not None else None,\n            leave=False,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n            position=position,\n        )\n\n    # with tqdm\n    with open(out, \"wb\") as f:\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:\n                if show:\n                    progress_bar.update(len(chunk))\n                f.write(chunk)", "\n\ndef load_config(\n    version: Literal[\"v1\", \"v2\"],\n    training_dir: str,\n    sample_rate: str,\n    emb_channels: int,\n    fp16: bool,\n):\n    if emb_channels == 256:\n        config_path = os.path.join(ROOT_DIR, \"configs\", f\"{sample_rate}.json\")\n    else:\n        config_path = os.path.join(\n            ROOT_DIR, \"configs\", f\"{sample_rate}-{emb_channels}.json\"\n        )\n\n    config = TrainConfig.parse_file(config_path)\n    config.version = version\n    config.train.fp16_run = fp16\n\n    config_save_path = os.path.join(training_dir, \"config.json\")\n\n    with open(config_save_path, \"w\") as f:\n        f.write(config.json())\n\n    return config", ""]}
{"filename": "modules/core.py", "chunked_list": ["import hashlib\nimport os\nimport shutil\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\n\nfrom modules.models import MODELS_DIR\nfrom modules.shared import ROOT_DIR", "from modules.models import MODELS_DIR\nfrom modules.shared import ROOT_DIR\nfrom modules.utils import download_file\n\n\ndef get_hf_etag(url: str):\n    r = requests.head(url)\n\n    etag = r.headers[\"X-Linked-ETag\"] if \"X-Linked-ETag\" in r.headers else \"\"\n\n    if etag.startswith('\"') and etag.endswith('\"'):\n        etag = etag[1:-1]\n\n    return etag", "\n\ndef calc_sha256(filepath: str):\n    sha256 = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n\ndef download_models():\n    def hash_check(url: str, out: str):\n        if not os.path.exists(out):\n            return False\n        etag = get_hf_etag(url)\n        hash = calc_sha256(out)\n        return etag == hash\n\n    os.makedirs(os.path.join(MODELS_DIR, \"pretrained\", \"v2\"), exist_ok=True)\n\n    tasks = []\n    for template in [\n        \"D{}k\",\n        \"G{}k\",\n        \"f0D{}k\",\n        \"f0G{}k\",\n    ]:\n        basename = template.format(\"40\")\n        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/pretrained/v2/{basename}.pth\"\n        out = os.path.join(MODELS_DIR, \"pretrained\", \"v2\", f\"{basename}.pth\")\n\n        if hash_check(url, out):\n            continue\n\n        tasks.append((url, out))\n\n    for filename in [\n        \"checkpoint_best_legacy_500.pt\",\n    ]:\n        out = os.path.join(MODELS_DIR, \"embeddings\", filename)\n        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\"\n\n        if hash_check(url, out):\n            continue\n\n        tasks.append(\n            (\n                f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\",\n                out,\n            )\n        )\n\n    # japanese-hubert-base (Fairseq)\n    # from official repo\n    # NOTE: change filename?\n    hubert_jp_url = f\"https://huggingface.co/rinna/japanese-hubert-base/resolve/main/fairseq/model.pt\"\n    out = os.path.join(MODELS_DIR, \"embeddings\", \"rinna_hubert_base_jp.pt\")\n    if not hash_check(hubert_jp_url, out):\n        tasks.append(\n            (\n                hubert_jp_url,\n                out,\n            )\n        )\n\n    if len(tasks) < 1:\n        return\n\n    with ThreadPoolExecutor() as pool:\n        pool.map(\n            download_file,\n            *zip(\n                *[(filename, out, i, True) for i, (filename, out) in enumerate(tasks)]\n            ),\n        )", "\n\ndef download_models():\n    def hash_check(url: str, out: str):\n        if not os.path.exists(out):\n            return False\n        etag = get_hf_etag(url)\n        hash = calc_sha256(out)\n        return etag == hash\n\n    os.makedirs(os.path.join(MODELS_DIR, \"pretrained\", \"v2\"), exist_ok=True)\n\n    tasks = []\n    for template in [\n        \"D{}k\",\n        \"G{}k\",\n        \"f0D{}k\",\n        \"f0G{}k\",\n    ]:\n        basename = template.format(\"40\")\n        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/pretrained/v2/{basename}.pth\"\n        out = os.path.join(MODELS_DIR, \"pretrained\", \"v2\", f\"{basename}.pth\")\n\n        if hash_check(url, out):\n            continue\n\n        tasks.append((url, out))\n\n    for filename in [\n        \"checkpoint_best_legacy_500.pt\",\n    ]:\n        out = os.path.join(MODELS_DIR, \"embeddings\", filename)\n        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\"\n\n        if hash_check(url, out):\n            continue\n\n        tasks.append(\n            (\n                f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\",\n                out,\n            )\n        )\n\n    # japanese-hubert-base (Fairseq)\n    # from official repo\n    # NOTE: change filename?\n    hubert_jp_url = f\"https://huggingface.co/rinna/japanese-hubert-base/resolve/main/fairseq/model.pt\"\n    out = os.path.join(MODELS_DIR, \"embeddings\", \"rinna_hubert_base_jp.pt\")\n    if not hash_check(hubert_jp_url, out):\n        tasks.append(\n            (\n                hubert_jp_url,\n                out,\n            )\n        )\n\n    if len(tasks) < 1:\n        return\n\n    with ThreadPoolExecutor() as pool:\n        pool.map(\n            download_file,\n            *zip(\n                *[(filename, out, i, True) for i, (filename, out) in enumerate(tasks)]\n            ),\n        )", "\n\ndef install_ffmpeg():\n    if os.path.exists(os.path.join(ROOT_DIR, \"bin\", \"ffmpeg.exe\")):\n        return\n    tmpdir = os.path.join(ROOT_DIR, \"tmp\")\n    url = (\n        \"https://www.gyan.dev/ffmpeg/builds/packages/ffmpeg-5.1.2-essentials_build.zip\"\n    )\n    out = os.path.join(tmpdir, \"ffmpeg.zip\")\n    os.makedirs(os.path.dirname(out), exist_ok=True)\n    download_file(url, out)\n    shutil.unpack_archive(out, os.path.join(tmpdir, \"ffmpeg\"))\n    shutil.copyfile(\n        os.path.join(\n            tmpdir, \"ffmpeg\", \"ffmpeg-5.1.2-essentials_build\", \"bin\", \"ffmpeg.exe\"\n        ),\n        os.path.join(ROOT_DIR, \"bin\", \"ffmpeg.exe\"),\n    )\n    os.remove(os.path.join(tmpdir, \"ffmpeg.zip\"))\n    shutil.rmtree(os.path.join(tmpdir, \"ffmpeg\"))", "\n\ndef update_modelnames():\n    for sr in [\"32k\", \"40k\", \"48k\"]:\n        files = [\n            f\"f0G{sr}\",\n            f\"f0D{sr}\",\n            f\"G{sr}\",\n            f\"D{sr}\",\n        ]\n        for file in files:\n            filepath = os.path.join(MODELS_DIR, \"pretrained\", f\"{file}.pth\")\n            if os.path.exists(filepath):\n                os.rename(\n                    filepath,\n                    os.path.join(MODELS_DIR, \"pretrained\", f\"{file}256.pth\"),\n                )\n\n    if not os.path.exists(os.path.join(MODELS_DIR, \"embeddings\")):\n        os.makedirs(os.path.join(MODELS_DIR, \"embeddings\"))\n\n    if os.path.exists(os.path.join(MODELS_DIR, \"hubert_base.pt\")):\n        os.rename(\n            os.path.join(MODELS_DIR, \"hubert_base.pt\"),\n            os.path.join(MODELS_DIR, \"embeddings\", \"hubert_base.pt\"),\n        )\n    if os.path.exists(os.path.join(MODELS_DIR, \"checkpoint_best_legacy_500.pt\")):\n        os.rename(\n            os.path.join(MODELS_DIR, \"checkpoint_best_legacy_500.pt\"),\n            os.path.join(MODELS_DIR, \"embeddings\", \"checkpoint_best_legacy_500.pt\"),\n        )", "\n\ndef preload():\n    update_modelnames()\n    download_models()\n    if sys.platform == \"win32\":\n        install_ffmpeg()\n"]}
{"filename": "modules/shared.py", "chunked_list": ["import os\nimport sys\n\nimport torch\n\nfrom modules.cmd_opts import opts\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nMODELS_DIR = os.path.join(ROOT_DIR, \"models\")\n", "MODELS_DIR = os.path.join(ROOT_DIR, \"models\")\n\n\ndef has_mps():\n    if sys.platform != \"darwin\":\n        return False\n    else:\n        if not getattr(torch, \"has_mps\", False):\n            return False\n        try:\n            torch.zeros(1).to(torch.device(\"mps\"))\n            return True\n        except Exception:\n            return False", "\n\nis_half = opts.precision == \"fp16\"\nhalf_support = (\n    torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 5.3\n)\n\nif not half_support:\n    print(\"WARNING: FP16 is not supported on this GPU\")\n    is_half = False", "\ndevice = \"cuda:0\"\n\nif not torch.cuda.is_available():\n    if has_mps():\n        print(\"Using MPS\")\n        device = \"mps\"\n    else:\n        print(\"Using CPU\")\n        device = \"cpu\"", "\ndevice = torch.device(device)\n"]}
{"filename": "modules/server/model.py", "chunked_list": ["import os\nimport re\nfrom typing import *\n\nimport faiss\nimport numpy as np\nimport pyworld\nimport scipy.signal as signal\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport torchaudio\nimport torchcrepe\nfrom fairseq import checkpoint_utils\nfrom fairseq.models.hubert.hubert import HubertModel\nfrom pydub import AudioSegment\nfrom torch import Tensor\n\nfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,", "\nfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,\n                            SynthesizerTrnMs256NSFSidNono)\nfrom lib.rvc.pipeline import VocalConvertPipeline\nfrom modules.cmd_opts import opts\nfrom modules.models import (EMBEDDINGS_LIST, MODELS_DIR, get_embedder,\n                            get_vc_model, update_state_dict)\nfrom modules.shared import ROOT_DIR, device, is_half\n\nMODELS_DIR = opts.models_dir or os.path.join(ROOT_DIR, \"models\")", "\nMODELS_DIR = opts.models_dir or os.path.join(ROOT_DIR, \"models\")\nvc_model: Optional[\"VoiceServerModel\"] = None\nembedder_model: Optional[HubertModel] = None\nloaded_embedder_model = \"\"\n\n\nclass VoiceServerModel:\n    def __init__(self, rvc_model_file: str, faiss_index_file: str) -> None:\n        # setting vram\n        global device, is_half\n        if isinstance(device, str):\n            device = torch.device(device)\n        if device.type == \"cuda\":\n            vram = torch.cuda.get_device_properties(device).total_memory / 1024**3\n        else:\n            vram = None\n        if vram is not None and vram <= 4:\n            self.x_pad = 1\n            self.x_query = 5\n            self.x_center = 30\n            self.x_max = 32\n        elif vram is not None and vram <= 5:\n            self.x_pad = 1\n            self.x_query = 6\n            self.x_center = 38\n            self.x_max = 41\n        else:\n            self.x_pad = 3\n            self.x_query = 10\n            self.x_center = 60\n            self.x_max = 65\n\n        # load_model\n        state_dict = torch.load(rvc_model_file, map_location=\"cpu\")\n        update_state_dict(state_dict)\n        self.state_dict = state_dict\n        self.tgt_sr = state_dict[\"params\"][\"sr\"]\n        self.f0 = state_dict.get(\"f0\", 1)\n        state_dict[\"params\"][\"spk_embed_dim\"] = state_dict[\"weight\"][\n            \"emb_g.weight\"\n        ].shape[0]\n        if not \"emb_channels\" in state_dict[\"params\"]:\n            if state_dict.get(\"version\", \"v1\") == \"v1\":\n                state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n                state_dict[\"embedder_output_layer\"] = 9\n            else:\n                state_dict[\"params\"][\"emb_channels\"] = 768  # for backward compat.\n                state_dict[\"embedder_output_layer\"] = 12\n        if self.f0 == 1:\n            self.net_g = SynthesizerTrnMs256NSFSid(\n                **state_dict[\"params\"], is_half=is_half\n            )\n        else:\n            self.net_g = SynthesizerTrnMs256NSFSidNono(**state_dict[\"params\"])\n        del self.net_g.enc_q\n        self.net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n        self.net_g.eval().to(device)\n        if is_half:\n            self.net_g = self.net_g.half()\n        else:\n            self.net_g = self.net_g.float()\n\n        emb_name = state_dict.get(\"embedder_name\", \"contentvec\")\n        if emb_name == \"hubert_base\":\n            emb_name = \"contentvec\"\n        emb_file = os.path.join(MODELS_DIR, \"embeddings\", EMBEDDINGS_LIST[emb_name][0])\n        models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n            [emb_file],\n            suffix=\"\",\n        )\n        embedder_model = models[0]\n        embedder_model = embedder_model.to(device)\n\n        if is_half:\n            embedder_model = embedder_model.half()\n        else:\n            embedder_model = embedder_model.float()\n        embedder_model.eval()\n        self.embedder_model = embedder_model\n\n        self.embedder_output_layer = state_dict[\"embedder_output_layer\"]\n\n        self.index = None\n        if faiss_index_file != \"\" and os.path.exists(faiss_index_file):\n            self.index = faiss.read_index(faiss_index_file)\n            self.big_npy = self.index.reconstruct_n(0, self.index.ntotal)\n\n        self.n_spk = state_dict[\"params\"][\"spk_embed_dim\"]\n\n        self.sr = 16000  # hubert input sample rate\n        self.window = 160  # hubert input window\n        self.t_pad = self.sr * self.x_pad  # padding time for each utterance\n        self.t_pad_tgt = self.tgt_sr * self.x_pad\n        self.t_pad2 = self.t_pad * 2\n        self.t_query = self.sr * self.x_query  # query time before and after query point\n        self.t_center = self.sr * self.x_center  # query cut point position\n        self.t_max = self.sr * self.x_max  # max time for no query\n        self.device = device\n        self.is_half = is_half\n\n    def __call__(\n        self,\n        audio: np.ndarray,\n        sr: int,\n        sid: int,\n        transpose: int,\n        f0_method: str,\n        index_rate: float,\n    ):\n        # bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n        # audio = signal.filtfilt(bh, ah, audio)\n        if sr != self.sr:\n            audio = torchaudio.functional.resample(torch.from_numpy(audio), sr, self.sr, rolloff=0.99).detach().cpu().numpy()\n        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\" if audio.shape[0] > self.window // 2 else \"constant\")\n\n        opt_ts = []\n        if audio_pad.shape[0] > self.t_max:\n            audio_sum = np.zeros_like(audio)\n            for i in range(self.window):\n                audio_sum += audio_pad[i : i - self.window]\n            for t in range(self.t_center, audio.shape[0], self.t_center):\n                opt_ts.append(\n                    t\n                    - self.t_query\n                    + np.where(\n                        np.abs(audio_sum[t - self.t_query : t + self.t_query])\n                        == np.abs(audio_sum[t - self.t_query : t + self.t_query]).min()\n                    )[0][0]\n                )\n        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\" if audio.shape[0] > self.t_pad else \"constant\")\n        p_len = audio_pad.shape[0] // self.window\n\n        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n        pitch, pitchf = None, None\n        if self.f0 == 1:\n            pitch, pitchf = get_f0(audio_pad, self.sr, p_len, transpose, f0_method)\n            pitch = pitch[:p_len]\n            pitchf = pitchf[:p_len]\n            if self.device.type == \"mps\":\n                pitchf = pitchf.astype(np.float32)\n            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n\n        audio_opt = []\n\n        s = 0\n        t = None\n\n        for t in opt_ts:\n            t = t // self.window * self.window\n            if self.f0 == 1:\n                audio_opt.append(\n                    self._convert(\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            else:\n                audio_opt.append(\n                    self._convert(\n                        sid,\n                        audio_pad[s : t + self.t_pad2 + self.window],\n                        None,\n                        None,\n                        index_rate,\n                    )[self.t_pad_tgt : -self.t_pad_tgt]\n                )\n            s = t\n        if self.f0 == 1:\n            audio_opt.append(\n                self._convert(\n                    sid,\n                    audio_pad[t:],\n                    pitch[:, t // self.window :] if t is not None else pitch,\n                    pitchf[:, t // self.window :] if t is not None else pitchf,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        else:\n            audio_opt.append(\n                self._convert(\n                    sid,\n                    audio_pad[t:],\n                    None,\n                    None,\n                    index_rate,\n                )[self.t_pad_tgt : -self.t_pad_tgt]\n            )\n        audio_opt = np.concatenate(audio_opt)\n        del pitch, pitchf, sid\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio_opt\n\n\n    def _convert(\n        self,\n        sid: int,\n        audio: np.ndarray,\n        pitch: Optional[np.ndarray],\n        pitchf: Optional[np.ndarray],\n        index_rate: float,\n    ):\n        feats = torch.from_numpy(audio)\n        if self.is_half:\n            feats = feats.half()\n        else:\n            feats = feats.float()\n        if feats.dim() == 2:  # double channels\n            feats = feats.mean(-1)\n        assert feats.dim() == 1, feats.dim()\n        feats = feats.view(1, -1)\n        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n\n        half_support = (\n            self.device.type == \"cuda\"\n            and torch.cuda.get_device_capability(self.device)[0] >= 5.3\n        )\n        is_feats_dim_768 = self.net_g.emb_channels == 768\n\n        if isinstance(self.embedder_model, tuple):\n            feats = self.embedder_model[0](\n                feats.squeeze(0).squeeze(0).to(self.device),\n                return_tensors=\"pt\",\n                sampling_rate=16000,\n            )\n            if self.is_half:\n                feats = feats.input_values.to(self.device).half()\n            else:\n                feats = feats.input_values.to(self.device)\n            with torch.no_grad():\n                if is_feats_dim_768:\n                    feats = self.embedder_model[1](feats).last_hidden_state\n                else:\n                    feats = self.embedder_model[1](feats).extract_features\n        else:\n            inputs = {\n                \"source\": feats.half().to(self.device)\n                if half_support\n                else feats.to(self.device),\n                \"padding_mask\": padding_mask.to(self.device),\n                \"output_layer\": self.embedder_output_layer,\n            }\n\n            if not half_support:\n                self.embedder_model = self.embedder_model.float()\n                inputs[\"source\"] = inputs[\"source\"].float()\n\n            with torch.no_grad():\n                logits = self.embedder_model.extract_features(**inputs)\n                if is_feats_dim_768:\n                    feats = logits[0]\n                else:\n                    feats = self.embedder_model.final_proj(logits[0])\n\n        if (\n            isinstance(self.index, type(None)) == False\n            and isinstance(self.big_npy, type(None)) == False\n            and index_rate != 0\n        ):\n            npy = feats[0].cpu().numpy()\n            if self.is_half:\n                npy = npy.astype(\"float32\")\n\n            _, ix = self.index.search(npy, k=1)\n            npy = self.big_npy[ix[:, 0]]\n\n            if self.is_half:\n                npy = npy.astype(\"float16\")\n            feats = (\n                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n                + (1 - index_rate) * feats\n            )\n\n        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n\n        p_len = audio.shape[0] // self.window\n        if feats.shape[1] < p_len:\n            p_len = feats.shape[1]\n            if pitch != None and pitchf != None:\n                pitch = pitch[:, :p_len]\n                pitchf = pitchf[:, :p_len]\n        p_len = torch.tensor([p_len], device=self.device).long()\n        with torch.no_grad():\n            if pitch != None and pitchf != None:\n                audio1 = (\n                    (self.net_g.infer(feats, p_len, pitch, pitchf, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n            else:\n                audio1 = (\n                    (self.net_g.infer(feats, p_len, sid)[0][0, 0] * 32768)\n                    .data.cpu()\n                    .float()\n                    .numpy()\n                    .astype(np.int16)\n                )\n        del feats, p_len, padding_mask\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return audio1", "\n\n# F0 computation\ndef get_f0_crepe_computation(\n        x,\n        sr,\n        f0_min,\n        f0_max,\n        p_len,\n        model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n):\n    hop_length = sr // 100\n    x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n    x /= np.quantile(np.abs(x), 0.999)\n    torch_device = self.get_optimal_torch_device()\n    audio = torch.from_numpy(x).to(torch_device, copy=True)\n    audio = torch.unsqueeze(audio, dim=0)\n    if audio.ndim == 2 and audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True).detach()\n    audio = audio.detach()\n    print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n    pitch: Tensor = torchcrepe.predict(\n        audio,\n        sr,\n        sr // 100,\n        f0_min,\n        f0_max,\n        model,\n        batch_size=hop_length * 2,\n        device=torch_device,\n        pad=True\n    )\n    p_len = p_len or x.shape[0] // hop_length\n    # Resize the pitch for final f0\n    source = np.array(pitch.squeeze(0).cpu().float().numpy())\n    source[source < 0.001] = np.nan\n    target = np.interp(\n        np.arange(0, len(source) * p_len, len(source)) / p_len,\n        np.arange(0, len(source)),\n        source\n    )\n    f0 = np.nan_to_num(target)\n    return f0 # Resized f0", "\ndef get_f0_official_crepe_computation(\n        x,\n        sr,\n        f0_min,\n        f0_max,\n        model=\"full\",\n):\n    # Pick a batch size that doesn't cause memory errors on your gpu\n    batch_size = 512\n    # Compute pitch using first gpu\n    audio = torch.tensor(np.copy(x))[None].float()\n    f0, pd = torchcrepe.predict(\n        audio,\n        sr,\n        sr // 100,\n        f0_min,\n        f0_max,\n        model,\n        batch_size=batch_size,\n        device=device,\n        return_periodicity=True,\n    )\n    pd = torchcrepe.filter.median(pd, 3)\n    f0 = torchcrepe.filter.mean(f0, 3)\n    f0[pd < 0.1] = 0\n    f0 = f0[0].cpu().numpy()\n    return f0", "\ndef get_f0(\n    x: np.ndarray,\n    sr: int,\n    p_len: int,\n    f0_up_key: int,\n    f0_method: str,\n):\n    f0_min = 50\n    f0_max = 1100\n    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\n    if f0_method == \"harvest\":\n        f0, t = pyworld.harvest(\n            x.astype(np.double),\n            fs=sr,\n            f0_ceil=f0_max,\n            f0_floor=f0_min,\n            frame_period=10,\n        )\n        f0 = pyworld.stonemask(x.astype(np.double), f0, t, sr)\n        f0 = signal.medfilt(f0, 3)\n    elif f0_method == \"dio\":\n        f0, t = pyworld.dio(\n            x.astype(np.double),\n            fs=sr,\n            f0_ceil=f0_max,\n            f0_floor=f0_min,\n            frame_period=10,\n        )\n        f0 = pyworld.stonemask(x.astype(np.double), f0, t, sr)\n        f0 = signal.medfilt(f0, 3)\n    elif f0_method == \"mangio-crepe\":\n        f0 = get_f0_crepe_computation(x, sr, f0_min, f0_max, p_len, \"full\")\n    elif f0_method == \"crepe\":\n        f0 = get_f0_official_crepe_computation(x, sr, f0_min, f0_max, \"full\")\n\n    f0 *= pow(2, f0_up_key / 12)\n    f0bak = f0.copy()\n    f0_mel = 1127 * np.log(1 + f0 / 700)\n    f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n        f0_mel_max - f0_mel_min\n    ) + 1\n    f0_mel[f0_mel <= 1] = 1\n    f0_mel[f0_mel > 255] = 255\n    f0_coarse = np.rint(f0_mel).astype(np.int32)\n    return f0_coarse, f0bak  # 1-0"]}
{"filename": "modules/tabs/merge.py", "chunked_list": ["import json\nimport os\nfrom typing import *\n\nimport gradio as gr\nimport torch\n\nfrom modules import models\nfrom modules.merge import merge\nfrom modules.tabs.inference import inference_options_ui", "from modules.merge import merge\nfrom modules.tabs.inference import inference_options_ui\nfrom modules.ui import Tab\n\nMERGE_METHODS = {\n    \"weight_sum\": \"Weight sum:A*(1-alpha)+B*alpha\",\n    \"add_diff\": \"Add difference:A+(B-C)*alpha\",\n}\n\n\nclass Merge(Tab):\n    def title(self):\n        return \"Merge\"\n\n    def sort(self):\n        return 3\n\n    def ui(self, outlet):\n        def merge_ckpt(model_a, model_b, model_c, weight_text, alpha, each_key, method):\n            model_a = model_a if type(model_a) != list and model_a != \"\" else None\n            model_b = model_b if type(model_b) != list and model_b != \"\" else None\n            model_c = model_c if type(model_c) != list and model_c != \"\" else None\n\n            if each_key:\n                weights = json.loads(weight_text)\n            else:\n                weights = {}\n\n            method = [k for k, v in MERGE_METHODS.items() if v == method][0]\n            return merge(\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_a),\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_b),\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_c)\n                if model_c\n                else None,\n                alpha,\n                weights,\n                method,\n            )\n\n        def merge_and_save(\n            model_a, model_b, model_c, alpha, each_key, weight_text, method, out_name\n        ):\n            print(each_key)\n            out_path = os.path.join(models.MODELS_DIR, \"checkpoints\", out_name)\n            if os.path.exists(out_path):\n                return \"Model name already exists.\"\n            merged = merge_ckpt(\n                model_a, model_b, model_c, weight_text, alpha, each_key, method\n            )\n            if not out_name.endswith(\".pth\"):\n                out_name += \".pth\"\n            torch.save(merged, os.path.join(models.MODELS_DIR, \"checkpoints\", out_name))\n            return \"Success\"\n\n        def merge_and_gen(\n            model_a,\n            model_b,\n            model_c,\n            alpha,\n            each_key,\n            weight_text,\n            method,\n            speaker_id,\n            source_audio,\n            embedder_name,\n            embedding_output_layer,\n            transpose,\n            fo_curve_file,\n            pitch_extraction_algo,\n            auto_load_index,\n            faiss_index_file,\n            retrieval_feature_ratio,\n        ):\n            merged = merge_ckpt(\n                model_a, model_b, model_c, weight_text, alpha, each_key, method\n            )\n            model = models.VoiceConvertModel(\"merge\", merged)\n            audio = model.single(\n                speaker_id,\n                source_audio,\n                embedder_name,\n                embedding_output_layer,\n                transpose,\n                fo_curve_file,\n                pitch_extraction_algo,\n                auto_load_index,\n                faiss_index_file,\n                retrieval_feature_ratio,\n            )\n            tgt_sr = model.tgt_sr\n            del merged\n            del model\n            torch.cuda.empty_cache()\n            return \"Success\", (tgt_sr, audio)\n\n        def reload_model():\n            model_list = models.get_models()\n            return (\n                gr.Dropdown.update(choices=model_list),\n                gr.Dropdown.update(choices=model_list),\n                gr.Dropdown.update(choices=model_list),\n            )\n\n        def update_speaker_ids(model):\n            if model == \"\":\n                return gr.Slider.update(\n                    maximum=0,\n                    visible=False,\n                )\n            model = torch.load(\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model),\n                map_location=\"cpu\",\n            )\n            vc_model = models.VoiceConvertModel(\"merge\", model)\n            max = vc_model.n_spk\n            del model\n            del vc_model\n            return gr.Slider.update(\n                maximum=max,\n                visible=True,\n            )\n\n        with gr.Group():\n            with gr.Column():\n                with gr.Row(equal_height=False):\n                    model_a = gr.Dropdown(choices=models.get_models(), label=\"Model A\")\n                    model_b = gr.Dropdown(choices=models.get_models(), label=\"Model B\")\n                    model_c = gr.Dropdown(choices=models.get_models(), label=\"Model C\")\n                    reload_model_button = gr.Button(\"\u267b\ufe0f\")\n                    reload_model_button.click(\n                        reload_model, outputs=[model_a, model_b, model_c]\n                    )\n                with gr.Row(equal_height=False):\n                    method = gr.Radio(\n                        label=\"Merge method\",\n                        choices=list(MERGE_METHODS.values()),\n                        value=\"Weight sum:A*(1-alpha)+B*alpha\",\n                    )\n                    output_name = gr.Textbox(label=\"Output name\")\n                    each_key = gr.Checkbox(label=\"Each key merge\")\n                with gr.Row(equal_height=False):\n                    base_alpha = gr.Slider(\n                        label=\"Base alpha\", minimum=0, maximum=1, value=0.5, step=0.01\n                    )\n\n                default_weights = {}\n                weights = {}\n\n                def create_weight_ui(name: str, *keys_list: List[List[str]]):\n                    with gr.Accordion(label=name, open=False):\n                        with gr.Row(equal_height=False):\n                            for keys in keys_list:\n                                with gr.Column():\n                                    for key in keys:\n                                        default_weights[key] = 0.5\n                                        weights[key] = gr.Slider(\n                                            label=key,\n                                            minimum=0,\n                                            maximum=1,\n                                            step=0.01,\n                                            value=0.5,\n                                        )\n\n                with gr.Box(visible=False) as each_key_ui:\n                    with gr.Column():\n                        create_weight_ui(\n                            \"enc_p\",\n                            [\n                                \"enc_p.encoder.attn_layers.0\",\n                                \"enc_p.encoder.attn_layers.1\",\n                                \"enc_p.encoder.attn_layers.2\",\n                                \"enc_p.encoder.attn_layers.3\",\n                                \"enc_p.encoder.attn_layers.4\",\n                                \"enc_p.encoder.attn_layers.5\",\n                                \"enc_p.encoder.norm_layers_1.0\",\n                                \"enc_p.encoder.norm_layers_1.1\",\n                                \"enc_p.encoder.norm_layers_1.2\",\n                                \"enc_p.encoder.norm_layers_1.3\",\n                                \"enc_p.encoder.norm_layers_1.4\",\n                                \"enc_p.encoder.norm_layers_1.5\",\n                            ],\n                            [\n                                \"enc_p.encoder.ffn_layers.0\",\n                                \"enc_p.encoder.ffn_layers.1\",\n                                \"enc_p.encoder.ffn_layers.2\",\n                                \"enc_p.encoder.ffn_layers.3\",\n                                \"enc_p.encoder.ffn_layers.4\",\n                                \"enc_p.encoder.ffn_layers.5\",\n                                \"enc_p.encoder.norm_layers_2.0\",\n                                \"enc_p.encoder.norm_layers_2.1\",\n                                \"enc_p.encoder.norm_layers_2.2\",\n                                \"enc_p.encoder.norm_layers_2.3\",\n                                \"enc_p.encoder.norm_layers_2.4\",\n                                \"enc_p.encoder.norm_layers_2.5\",\n                            ],\n                            [\n                                \"enc_p.emb_phone\",\n                                \"enc_p.emb_pitch\",\n                            ],\n                        )\n\n                        create_weight_ui(\n                            \"dec\",\n                            [\n                                \"dec.noise_convs.0\",\n                                \"dec.noise_convs.1\",\n                                \"dec.noise_convs.2\",\n                                \"dec.noise_convs.3\",\n                                \"dec.noise_convs.4\",\n                                \"dec.noise_convs.5\",\n                                \"dec.ups.0\",\n                                \"dec.ups.1\",\n                                \"dec.ups.2\",\n                                \"dec.ups.3\",\n                            ],\n                            [\n                                \"dec.resblocks.0\",\n                                \"dec.resblocks.1\",\n                                \"dec.resblocks.2\",\n                                \"dec.resblocks.3\",\n                                \"dec.resblocks.4\",\n                                \"dec.resblocks.5\",\n                                \"dec.resblocks.6\",\n                                \"dec.resblocks.7\",\n                                \"dec.resblocks.8\",\n                                \"dec.resblocks.9\",\n                                \"dec.resblocks.10\",\n                                \"dec.resblocks.11\",\n                            ],\n                            [\n                                \"dec.m_source.l_linear\",\n                                \"dec.conv_pre\",\n                                \"dec.conv_post\",\n                                \"dec.cond\",\n                            ],\n                        )\n\n                        create_weight_ui(\n                            \"flow\",\n                            [\n                                \"flow.flows.0\",\n                                \"flow.flows.1\",\n                                \"flow.flows.2\",\n                                \"flow.flows.3\",\n                                \"flow.flows.4\",\n                                \"flow.flows.5\",\n                                \"flow.flows.6\",\n                                \"emb_g.weight\",\n                            ],\n                        )\n\n                        with gr.Accordion(label=\"JSON\", open=False):\n                            weights_text = gr.TextArea(\n                                value=json.dumps(default_weights),\n                            )\n\n                with gr.Accordion(label=\"Inference options\", open=False):\n                    with gr.Row(equal_height=False):\n                        speaker_id = gr.Slider(\n                            minimum=0,\n                            maximum=2333,\n                            step=1,\n                            label=\"Speaker ID\",\n                            value=0,\n                            visible=True,\n                            interactive=True,\n                        )\n                    (\n                        source_audio,\n                        _,\n                        transpose,\n                        embedder_name,\n                        embedding_output_layer,\n                        pitch_extraction_algo,\n                        auto_load_index,\n                        faiss_index_file,\n                        retrieval_feature_ratio,\n                        fo_curve_file,\n                    ) = inference_options_ui(show_out_dir=False)\n\n                with gr.Row(equal_height=False):\n                    with gr.Column():\n                        status = gr.Textbox(value=\"\", label=\"Status\")\n                        audio_output = gr.Audio(label=\"Output\", interactive=False)\n\n                with gr.Row(equal_height=False):\n                    merge_and_save_button = gr.Button(\n                        \"Merge and save\", variant=\"primary\"\n                    )\n                    merge_and_gen_button = gr.Button(\"Merge and gen\", variant=\"primary\")\n\n                def each_key_on_change(each_key):\n                    return gr.update(visible=each_key)\n\n                each_key.change(\n                    fn=each_key_on_change,\n                    inputs=[each_key],\n                    outputs=[each_key_ui],\n                )\n\n                def update_weights_text(data):\n                    d = {}\n                    for key in weights.keys():\n                        d[key] = data[weights[key]]\n                    return json.dumps(d)\n\n                for w in weights.values():\n                    w.change(\n                        fn=update_weights_text,\n                        inputs={*weights.values()},\n                        outputs=[weights_text],\n                    )\n\n                merge_data = [\n                    model_a,\n                    model_b,\n                    model_c,\n                    base_alpha,\n                    each_key,\n                    weights_text,\n                    method,\n                ]\n\n                inference_opts = [\n                    speaker_id,\n                    source_audio,\n                    embedder_name,\n                    embedding_output_layer,\n                    transpose,\n                    fo_curve_file,\n                    pitch_extraction_algo,\n                    auto_load_index,\n                    faiss_index_file,\n                    retrieval_feature_ratio,\n                ]\n\n                merge_and_save_button.click(\n                    fn=merge_and_save,\n                    inputs=[\n                        *merge_data,\n                        output_name,\n                    ],\n                    outputs=[status],\n                )\n                merge_and_gen_button.click(\n                    fn=merge_and_gen,\n                    inputs=[\n                        *merge_data,\n                        *inference_opts,\n                    ],\n                    outputs=[status, audio_output],\n                )\n\n                model_a.change(\n                    update_speaker_ids, inputs=[model_a], outputs=[speaker_id]\n                )", "\n\nclass Merge(Tab):\n    def title(self):\n        return \"Merge\"\n\n    def sort(self):\n        return 3\n\n    def ui(self, outlet):\n        def merge_ckpt(model_a, model_b, model_c, weight_text, alpha, each_key, method):\n            model_a = model_a if type(model_a) != list and model_a != \"\" else None\n            model_b = model_b if type(model_b) != list and model_b != \"\" else None\n            model_c = model_c if type(model_c) != list and model_c != \"\" else None\n\n            if each_key:\n                weights = json.loads(weight_text)\n            else:\n                weights = {}\n\n            method = [k for k, v in MERGE_METHODS.items() if v == method][0]\n            return merge(\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_a),\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_b),\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model_c)\n                if model_c\n                else None,\n                alpha,\n                weights,\n                method,\n            )\n\n        def merge_and_save(\n            model_a, model_b, model_c, alpha, each_key, weight_text, method, out_name\n        ):\n            print(each_key)\n            out_path = os.path.join(models.MODELS_DIR, \"checkpoints\", out_name)\n            if os.path.exists(out_path):\n                return \"Model name already exists.\"\n            merged = merge_ckpt(\n                model_a, model_b, model_c, weight_text, alpha, each_key, method\n            )\n            if not out_name.endswith(\".pth\"):\n                out_name += \".pth\"\n            torch.save(merged, os.path.join(models.MODELS_DIR, \"checkpoints\", out_name))\n            return \"Success\"\n\n        def merge_and_gen(\n            model_a,\n            model_b,\n            model_c,\n            alpha,\n            each_key,\n            weight_text,\n            method,\n            speaker_id,\n            source_audio,\n            embedder_name,\n            embedding_output_layer,\n            transpose,\n            fo_curve_file,\n            pitch_extraction_algo,\n            auto_load_index,\n            faiss_index_file,\n            retrieval_feature_ratio,\n        ):\n            merged = merge_ckpt(\n                model_a, model_b, model_c, weight_text, alpha, each_key, method\n            )\n            model = models.VoiceConvertModel(\"merge\", merged)\n            audio = model.single(\n                speaker_id,\n                source_audio,\n                embedder_name,\n                embedding_output_layer,\n                transpose,\n                fo_curve_file,\n                pitch_extraction_algo,\n                auto_load_index,\n                faiss_index_file,\n                retrieval_feature_ratio,\n            )\n            tgt_sr = model.tgt_sr\n            del merged\n            del model\n            torch.cuda.empty_cache()\n            return \"Success\", (tgt_sr, audio)\n\n        def reload_model():\n            model_list = models.get_models()\n            return (\n                gr.Dropdown.update(choices=model_list),\n                gr.Dropdown.update(choices=model_list),\n                gr.Dropdown.update(choices=model_list),\n            )\n\n        def update_speaker_ids(model):\n            if model == \"\":\n                return gr.Slider.update(\n                    maximum=0,\n                    visible=False,\n                )\n            model = torch.load(\n                os.path.join(models.MODELS_DIR, \"checkpoints\", model),\n                map_location=\"cpu\",\n            )\n            vc_model = models.VoiceConvertModel(\"merge\", model)\n            max = vc_model.n_spk\n            del model\n            del vc_model\n            return gr.Slider.update(\n                maximum=max,\n                visible=True,\n            )\n\n        with gr.Group():\n            with gr.Column():\n                with gr.Row(equal_height=False):\n                    model_a = gr.Dropdown(choices=models.get_models(), label=\"Model A\")\n                    model_b = gr.Dropdown(choices=models.get_models(), label=\"Model B\")\n                    model_c = gr.Dropdown(choices=models.get_models(), label=\"Model C\")\n                    reload_model_button = gr.Button(\"\u267b\ufe0f\")\n                    reload_model_button.click(\n                        reload_model, outputs=[model_a, model_b, model_c]\n                    )\n                with gr.Row(equal_height=False):\n                    method = gr.Radio(\n                        label=\"Merge method\",\n                        choices=list(MERGE_METHODS.values()),\n                        value=\"Weight sum:A*(1-alpha)+B*alpha\",\n                    )\n                    output_name = gr.Textbox(label=\"Output name\")\n                    each_key = gr.Checkbox(label=\"Each key merge\")\n                with gr.Row(equal_height=False):\n                    base_alpha = gr.Slider(\n                        label=\"Base alpha\", minimum=0, maximum=1, value=0.5, step=0.01\n                    )\n\n                default_weights = {}\n                weights = {}\n\n                def create_weight_ui(name: str, *keys_list: List[List[str]]):\n                    with gr.Accordion(label=name, open=False):\n                        with gr.Row(equal_height=False):\n                            for keys in keys_list:\n                                with gr.Column():\n                                    for key in keys:\n                                        default_weights[key] = 0.5\n                                        weights[key] = gr.Slider(\n                                            label=key,\n                                            minimum=0,\n                                            maximum=1,\n                                            step=0.01,\n                                            value=0.5,\n                                        )\n\n                with gr.Box(visible=False) as each_key_ui:\n                    with gr.Column():\n                        create_weight_ui(\n                            \"enc_p\",\n                            [\n                                \"enc_p.encoder.attn_layers.0\",\n                                \"enc_p.encoder.attn_layers.1\",\n                                \"enc_p.encoder.attn_layers.2\",\n                                \"enc_p.encoder.attn_layers.3\",\n                                \"enc_p.encoder.attn_layers.4\",\n                                \"enc_p.encoder.attn_layers.5\",\n                                \"enc_p.encoder.norm_layers_1.0\",\n                                \"enc_p.encoder.norm_layers_1.1\",\n                                \"enc_p.encoder.norm_layers_1.2\",\n                                \"enc_p.encoder.norm_layers_1.3\",\n                                \"enc_p.encoder.norm_layers_1.4\",\n                                \"enc_p.encoder.norm_layers_1.5\",\n                            ],\n                            [\n                                \"enc_p.encoder.ffn_layers.0\",\n                                \"enc_p.encoder.ffn_layers.1\",\n                                \"enc_p.encoder.ffn_layers.2\",\n                                \"enc_p.encoder.ffn_layers.3\",\n                                \"enc_p.encoder.ffn_layers.4\",\n                                \"enc_p.encoder.ffn_layers.5\",\n                                \"enc_p.encoder.norm_layers_2.0\",\n                                \"enc_p.encoder.norm_layers_2.1\",\n                                \"enc_p.encoder.norm_layers_2.2\",\n                                \"enc_p.encoder.norm_layers_2.3\",\n                                \"enc_p.encoder.norm_layers_2.4\",\n                                \"enc_p.encoder.norm_layers_2.5\",\n                            ],\n                            [\n                                \"enc_p.emb_phone\",\n                                \"enc_p.emb_pitch\",\n                            ],\n                        )\n\n                        create_weight_ui(\n                            \"dec\",\n                            [\n                                \"dec.noise_convs.0\",\n                                \"dec.noise_convs.1\",\n                                \"dec.noise_convs.2\",\n                                \"dec.noise_convs.3\",\n                                \"dec.noise_convs.4\",\n                                \"dec.noise_convs.5\",\n                                \"dec.ups.0\",\n                                \"dec.ups.1\",\n                                \"dec.ups.2\",\n                                \"dec.ups.3\",\n                            ],\n                            [\n                                \"dec.resblocks.0\",\n                                \"dec.resblocks.1\",\n                                \"dec.resblocks.2\",\n                                \"dec.resblocks.3\",\n                                \"dec.resblocks.4\",\n                                \"dec.resblocks.5\",\n                                \"dec.resblocks.6\",\n                                \"dec.resblocks.7\",\n                                \"dec.resblocks.8\",\n                                \"dec.resblocks.9\",\n                                \"dec.resblocks.10\",\n                                \"dec.resblocks.11\",\n                            ],\n                            [\n                                \"dec.m_source.l_linear\",\n                                \"dec.conv_pre\",\n                                \"dec.conv_post\",\n                                \"dec.cond\",\n                            ],\n                        )\n\n                        create_weight_ui(\n                            \"flow\",\n                            [\n                                \"flow.flows.0\",\n                                \"flow.flows.1\",\n                                \"flow.flows.2\",\n                                \"flow.flows.3\",\n                                \"flow.flows.4\",\n                                \"flow.flows.5\",\n                                \"flow.flows.6\",\n                                \"emb_g.weight\",\n                            ],\n                        )\n\n                        with gr.Accordion(label=\"JSON\", open=False):\n                            weights_text = gr.TextArea(\n                                value=json.dumps(default_weights),\n                            )\n\n                with gr.Accordion(label=\"Inference options\", open=False):\n                    with gr.Row(equal_height=False):\n                        speaker_id = gr.Slider(\n                            minimum=0,\n                            maximum=2333,\n                            step=1,\n                            label=\"Speaker ID\",\n                            value=0,\n                            visible=True,\n                            interactive=True,\n                        )\n                    (\n                        source_audio,\n                        _,\n                        transpose,\n                        embedder_name,\n                        embedding_output_layer,\n                        pitch_extraction_algo,\n                        auto_load_index,\n                        faiss_index_file,\n                        retrieval_feature_ratio,\n                        fo_curve_file,\n                    ) = inference_options_ui(show_out_dir=False)\n\n                with gr.Row(equal_height=False):\n                    with gr.Column():\n                        status = gr.Textbox(value=\"\", label=\"Status\")\n                        audio_output = gr.Audio(label=\"Output\", interactive=False)\n\n                with gr.Row(equal_height=False):\n                    merge_and_save_button = gr.Button(\n                        \"Merge and save\", variant=\"primary\"\n                    )\n                    merge_and_gen_button = gr.Button(\"Merge and gen\", variant=\"primary\")\n\n                def each_key_on_change(each_key):\n                    return gr.update(visible=each_key)\n\n                each_key.change(\n                    fn=each_key_on_change,\n                    inputs=[each_key],\n                    outputs=[each_key_ui],\n                )\n\n                def update_weights_text(data):\n                    d = {}\n                    for key in weights.keys():\n                        d[key] = data[weights[key]]\n                    return json.dumps(d)\n\n                for w in weights.values():\n                    w.change(\n                        fn=update_weights_text,\n                        inputs={*weights.values()},\n                        outputs=[weights_text],\n                    )\n\n                merge_data = [\n                    model_a,\n                    model_b,\n                    model_c,\n                    base_alpha,\n                    each_key,\n                    weights_text,\n                    method,\n                ]\n\n                inference_opts = [\n                    speaker_id,\n                    source_audio,\n                    embedder_name,\n                    embedding_output_layer,\n                    transpose,\n                    fo_curve_file,\n                    pitch_extraction_algo,\n                    auto_load_index,\n                    faiss_index_file,\n                    retrieval_feature_ratio,\n                ]\n\n                merge_and_save_button.click(\n                    fn=merge_and_save,\n                    inputs=[\n                        *merge_data,\n                        output_name,\n                    ],\n                    outputs=[status],\n                )\n                merge_and_gen_button.click(\n                    fn=merge_and_gen,\n                    inputs=[\n                        *merge_data,\n                        *inference_opts,\n                    ],\n                    outputs=[status, audio_output],\n                )\n\n                model_a.change(\n                    update_speaker_ids, inputs=[model_a], outputs=[speaker_id]\n                )", ""]}
{"filename": "modules/tabs/inference.py", "chunked_list": ["import glob\nimport os\nimport traceback\n\nimport gradio as gr\n\nfrom modules import models, ui\nfrom modules.ui import Tab\n\n\ndef inference_options_ui(show_out_dir=True):\n    with gr.Row(equal_height=False):\n        with gr.Column():\n            source_audio = gr.Textbox(label=\"Source Audio\")\n            out_dir = gr.Textbox(\n                label=\"Out folder\",\n                visible=show_out_dir,\n                placeholder=models.AUDIO_OUT_DIR,\n            )\n        with gr.Column():\n            transpose = gr.Slider(\n                minimum=-20, maximum=20, value=0, step=1, label=\"Transpose\"\n            )\n            pitch_extraction_algo = gr.Radio(\n                choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n                value=\"crepe\",\n                label=\"Pitch Extraction Algorithm\",\n            )\n            embedding_model = gr.Radio(\n                choices=[\"auto\", *models.EMBEDDINGS_LIST.keys()],\n                value=\"auto\",\n                label=\"Embedder Model\",\n            )\n            embedding_output_layer = gr.Radio(\n                choices=[\"auto\", \"9\", \"12\"],\n                value=\"auto\",\n                label=\"Embedder Output Layer\",\n            )\n        with gr.Column():\n            auto_load_index = gr.Checkbox(value=False, label=\"Auto Load Index\")\n            faiss_index_file = gr.Textbox(value=\"\", label=\"Faiss Index File Path\")\n            retrieval_feature_ratio = gr.Slider(\n                minimum=0,\n                maximum=1,\n                value=1,\n                step=0.01,\n                label=\"Retrieval Feature Ratio\",\n            )\n        with gr.Column():\n            fo_curve_file = gr.File(label=\"F0 Curve File\")\n\n    return (\n        source_audio,\n        out_dir,\n        transpose,\n        embedding_model,\n        embedding_output_layer,\n        pitch_extraction_algo,\n        auto_load_index,\n        faiss_index_file,\n        retrieval_feature_ratio,\n        fo_curve_file,\n    )", "\n\ndef inference_options_ui(show_out_dir=True):\n    with gr.Row(equal_height=False):\n        with gr.Column():\n            source_audio = gr.Textbox(label=\"Source Audio\")\n            out_dir = gr.Textbox(\n                label=\"Out folder\",\n                visible=show_out_dir,\n                placeholder=models.AUDIO_OUT_DIR,\n            )\n        with gr.Column():\n            transpose = gr.Slider(\n                minimum=-20, maximum=20, value=0, step=1, label=\"Transpose\"\n            )\n            pitch_extraction_algo = gr.Radio(\n                choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n                value=\"crepe\",\n                label=\"Pitch Extraction Algorithm\",\n            )\n            embedding_model = gr.Radio(\n                choices=[\"auto\", *models.EMBEDDINGS_LIST.keys()],\n                value=\"auto\",\n                label=\"Embedder Model\",\n            )\n            embedding_output_layer = gr.Radio(\n                choices=[\"auto\", \"9\", \"12\"],\n                value=\"auto\",\n                label=\"Embedder Output Layer\",\n            )\n        with gr.Column():\n            auto_load_index = gr.Checkbox(value=False, label=\"Auto Load Index\")\n            faiss_index_file = gr.Textbox(value=\"\", label=\"Faiss Index File Path\")\n            retrieval_feature_ratio = gr.Slider(\n                minimum=0,\n                maximum=1,\n                value=1,\n                step=0.01,\n                label=\"Retrieval Feature Ratio\",\n            )\n        with gr.Column():\n            fo_curve_file = gr.File(label=\"F0 Curve File\")\n\n    return (\n        source_audio,\n        out_dir,\n        transpose,\n        embedding_model,\n        embedding_output_layer,\n        pitch_extraction_algo,\n        auto_load_index,\n        faiss_index_file,\n        retrieval_feature_ratio,\n        fo_curve_file,\n    )", "\n\nclass Inference(Tab):\n    def title(self):\n        return \"Inference\"\n\n    def sort(self):\n        return 1\n\n    def ui(self, outlet):\n        def infer(\n            sid,\n            input_audio,\n            out_dir,\n            embedder_model,\n            embedding_output_layer,\n            f0_up_key,\n            f0_file,\n            f0_method,\n            auto_load_index,\n            faiss_index_file,\n            index_rate,\n        ):\n            model = models.vc_model\n            try:\n                yield \"Infering...\", None\n                if out_dir == \"\":\n                    out_dir = models.AUDIO_OUT_DIR\n\n                if \"*\" in input_audio:\n                    assert (\n                        out_dir is not None\n                    ), \"Out folder is required for batch processing\"\n                    files = glob.glob(input_audio, recursive=True)\n                elif os.path.isdir(input_audio):\n                    assert (\n                        out_dir is not None\n                    ), \"Out folder is required for batch processing\"\n                    files = glob.glob(\n                        os.path.join(input_audio, \"**\", \"*.wav\"), recursive=True\n                    )\n                else:\n                    files = [input_audio]\n                for file in files:\n                    audio = model.single(\n                        sid,\n                        file,\n                        embedder_model,\n                        embedding_output_layer,\n                        f0_up_key,\n                        f0_file,\n                        f0_method,\n                        auto_load_index,\n                        faiss_index_file,\n                        index_rate,\n                        output_dir=out_dir,\n                    )\n                yield \"Success\", (model.tgt_sr, audio) if len(files) == 1 else None\n            except:\n                yield \"Error: \" + traceback.format_exc(), None\n\n        with gr.Group():\n            with gr.Box():\n                with gr.Column():\n                    _, speaker_id = ui.create_model_list_ui()\n\n                    (\n                        source_audio,\n                        out_dir,\n                        transpose,\n                        embedder_model,\n                        embedding_output_layer,\n                        pitch_extraction_algo,\n                        auto_load_index,\n                        faiss_index_file,\n                        retrieval_feature_ratio,\n                        f0_curve_file,\n                    ) = inference_options_ui()\n\n                    with gr.Row(equal_height=False):\n                        with gr.Column():\n                            status = gr.Textbox(value=\"\", label=\"Status\")\n                            output = gr.Audio(label=\"Output\", interactive=False)\n\n                    with gr.Row():\n                        infer_button = gr.Button(\"Infer\", variant=\"primary\")\n\n        infer_button.click(\n            infer,\n            inputs=[\n                speaker_id,\n                source_audio,\n                out_dir,\n                embedder_model,\n                embedding_output_layer,\n                transpose,\n                f0_curve_file,\n                pitch_extraction_algo,\n                auto_load_index,\n                faiss_index_file,\n                retrieval_feature_ratio,\n            ],\n            outputs=[status, output],\n            queue=True,\n        )", ""]}
{"filename": "modules/tabs/split.py", "chunked_list": ["import gradio as gr\n\nfrom modules.separate import separate_audio\nfrom modules.ui import Tab\n\n\nclass Split(Tab):\n    def title(self):\n        return \"Split Audio\"\n\n    def sort(self):\n        return 5\n\n    def ui(self, outlet):\n        def separate(\n            input_audio,\n            output_dir,\n            silence_thresh,\n            min_silence_len,\n            keep_silence,\n            margin,\n            padding,\n            min,\n            max,\n        ):\n            min = None if min == 0 else min\n            max = None if max == 0 else max\n            separate_audio(\n                input_audio,\n                output_dir,\n                int(silence_thresh),\n                int(min_silence_len),\n                int(keep_silence),\n                int(margin),\n                padding,\n                int(min),\n                int(max),\n            )\n            return \"Success\"\n\n        with gr.Group():\n            with gr.Column():\n                with gr.Row(equal_height=False):\n                    input_audio = gr.Textbox(label=\"Input Audio (File or Directory)\")\n                    output_dir = gr.Textbox(label=\"Output Directory\")\n\n                with gr.Row(equal_height=False):\n                    silence_thresh = gr.Number(value=-40, label=\"Silence Threshold\")\n                    min_silence_len = gr.Number(\n                        value=750, label=\"Minimum Silence Length\"\n                    )\n                    keep_silence = gr.Number(value=750, label=\"Keep Silence\")\n                    margin = gr.Number(value=0, label=\"Margin\")\n                    padding = gr.Checkbox(value=True, label=\"Padding\")\n\n                with gr.Row(equal_height=False):\n                    min = gr.Number(value=1000, label=\"Minimum audio length\")\n                    max = gr.Number(value=5000, label=\"Maximum audio length\")\n\n                with gr.Row(equal_height=False):\n                    status = gr.Textbox(value=\"\", label=\"Status\")\n                with gr.Row(equal_height=False):\n                    separate_button = gr.Button(\"Separate\", variant=\"primary\")\n\n        separate_button.click(\n            separate,\n            inputs=[\n                input_audio,\n                output_dir,\n                silence_thresh,\n                min_silence_len,\n                keep_silence,\n                margin,\n                padding,\n                min,\n                max,\n            ],\n            outputs=[status],\n        )", ""]}
{"filename": "modules/tabs/training.py", "chunked_list": ["import math\nimport os\nimport shutil\nfrom multiprocessing import cpu_count\n\nimport gradio as gr\n\nfrom lib.rvc.preprocessing import extract_f0, extract_feature, split\nfrom lib.rvc.train import create_dataset_meta, glob_dataset, train_index, train_model\nfrom modules import models, utils", "from lib.rvc.train import create_dataset_meta, glob_dataset, train_index, train_model\nfrom modules import models, utils\nfrom modules.shared import MODELS_DIR, device, half_support\nfrom modules.ui import Tab\n\nSR_DICT = {\n    \"32k\": 32000,\n    \"40k\": 40000,\n    \"48k\": 48000,\n}", "    \"48k\": 48000,\n}\n\n\nclass Training(Tab):\n    def title(self):\n        return \"Training\"\n\n    def sort(self):\n        return 2\n\n    def ui(self, outlet):\n        def train_index_only(\n            model_name,\n            target_sr,\n            f0,\n            dataset_glob,\n            recursive,\n            multiple_speakers,\n            speaker_id,\n            gpu_id,\n            num_cpu_process,\n            norm_audio_when_preprocess,\n            pitch_extraction_algo,\n            run_train_index,\n            reduce_index_size,\n            maximum_index_size,\n            embedder_name,\n            embedding_channels,\n            embedding_output_layer,\n            ignore_cache,\n        ):\n            maximum_index_size = int(maximum_index_size)\n            f0 = f0 == \"Yes\"\n            norm_audio_when_preprocess = norm_audio_when_preprocess == \"Yes\"\n            run_train_index = run_train_index == \"Yes\"\n            reduce_index_size = reduce_index_size == \"Yes\"\n            training_dir = os.path.join(MODELS_DIR, \"training\", \"models\", model_name)\n            gpu_ids = [int(x.strip()) for x in gpu_id.split(\",\")] if gpu_id else []\n            yield f\"Training directory: {training_dir}\"\n\n            if os.path.exists(training_dir) and ignore_cache:\n                shutil.rmtree(training_dir)\n\n            os.makedirs(training_dir, exist_ok=True)\n\n            datasets = glob_dataset(\n                dataset_glob,\n                speaker_id,\n                multiple_speakers=multiple_speakers,\n                recursive=recursive,\n            )\n\n            if len(datasets) == 0:\n                raise Exception(\"No audio files found\")\n\n            yield \"Preprocessing...\"\n            split.preprocess_audio(\n                datasets,\n                SR_DICT[target_sr],\n                num_cpu_process,\n                training_dir,\n                norm_audio_when_preprocess,\n                os.path.join(\n                    MODELS_DIR,\n                    \"training\",\n                    \"mute\",\n                    \"0_gt_wavs\",\n                    f\"mute{target_sr}.wav\",\n                ),\n            )\n\n            if f0:\n                yield \"Extracting f0...\"\n                extract_f0.run(training_dir, num_cpu_process, pitch_extraction_algo)\n\n            yield \"Extracting features...\"\n\n            embedder_filepath, _, embedder_load_from = models.get_embedder(\n                embedder_name\n            )\n\n            if embedder_load_from == \"local\":\n                embedder_filepath = os.path.join(\n                    MODELS_DIR, \"embeddings\", embedder_filepath\n                )\n\n            extract_feature.run(\n                training_dir,\n                embedder_filepath,\n                embedder_load_from,\n                int(embedding_channels),\n                int(embedding_output_layer),\n                gpu_ids,\n            )\n\n            out_dir = os.path.join(MODELS_DIR, \"checkpoints\")\n\n            yield \"Training index...\"\n            if run_train_index:\n                if not reduce_index_size:\n                    maximum_index_size = None\n                train_index(\n                    training_dir,\n                    model_name,\n                    out_dir,\n                    int(embedding_channels),\n                    num_cpu_process,\n                    maximum_index_size,\n                )\n\n            yield \"Training complete\"\n\n        def train_all(\n            model_name,\n            version,\n            sampling_rate_str,\n            f0,\n            dataset_glob,\n            recursive,\n            multiple_speakers,\n            speaker_id,\n            gpu_id,\n            num_cpu_process,\n            norm_audio_when_preprocess,\n            pitch_extraction_algo,\n            batch_size,\n            augment,\n            augment_from_pretrain,\n            augment_path,\n            speaker_info_path,\n            cache_batch,\n            num_epochs,\n            save_every_epoch,\n            save_wav_with_checkpoint,\n            fp16,\n            pre_trained_bottom_model_g,\n            pre_trained_bottom_model_d,\n            run_train_index,\n            reduce_index_size,\n            maximum_index_size,\n            embedder_name,\n            embedding_channels,\n            embedding_output_layer,\n            ignore_cache,\n        ):\n            batch_size = int(batch_size)\n            num_epochs = int(num_epochs)\n            maximum_index_size = int(maximum_index_size)\n            f0 = f0 == \"Yes\"\n            norm_audio_when_preprocess = norm_audio_when_preprocess == \"Yes\"\n            run_train_index = run_train_index == \"Yes\"\n            reduce_index_size = reduce_index_size == \"Yes\"\n            training_dir = os.path.join(MODELS_DIR, \"training\", \"models\", model_name)\n            gpu_ids = [int(x.strip()) for x in gpu_id.split(\",\")] if gpu_id else []\n\n            if os.path.exists(training_dir) and ignore_cache:\n                shutil.rmtree(training_dir)\n\n            os.makedirs(training_dir, exist_ok=True)\n\n            yield f\"Training directory: {training_dir}\"\n\n            datasets = glob_dataset(\n                dataset_glob,\n                speaker_id,\n                multiple_speakers=multiple_speakers,\n                recursive=recursive,\n                training_dir=training_dir,\n            )\n\n            if len(datasets) == 0:\n                raise Exception(\"No audio files found\")\n\n            yield \"Preprocessing...\"\n            split.preprocess_audio(\n                datasets,\n                SR_DICT[sampling_rate_str],\n                num_cpu_process,\n                training_dir,\n                norm_audio_when_preprocess,\n                os.path.join(\n                    MODELS_DIR,\n                    \"training\",\n                    \"mute\",\n                    \"0_gt_wavs\",\n                    f\"mute{sampling_rate_str}.wav\",\n                ),\n            )\n\n            if f0:\n                yield \"Extracting f0...\"\n                extract_f0.run(training_dir, num_cpu_process, pitch_extraction_algo)\n\n            yield \"Extracting features...\"\n\n            embedder_filepath, _, embedder_load_from = models.get_embedder(\n                embedder_name\n            )\n\n            if embedder_load_from == \"local\":\n                embedder_filepath = os.path.join(\n                    MODELS_DIR, \"embeddings\", embedder_filepath\n                )\n\n            extract_feature.run(\n                training_dir,\n                embedder_filepath,\n                embedder_load_from,\n                int(embedding_channels),\n                int(embedding_output_layer),\n                gpu_ids,\n                None if len(gpu_ids) > 1 else device,\n            )\n\n            create_dataset_meta(training_dir, f0)\n\n            yield \"Training model...\"\n\n            print(f\"train_all: emb_name: {embedder_name}\")\n\n            config = utils.load_config(\n                version, training_dir, sampling_rate_str, embedding_channels, fp16\n            )\n            out_dir = os.path.join(MODELS_DIR, \"checkpoints\")\n\n            if not augment_from_pretrain:\n                augment_path = None\n                speaker_info_path = None\n\n            train_model(\n                gpu_ids,\n                config,\n                training_dir,\n                model_name,\n                out_dir,\n                sampling_rate_str,\n                f0,\n                batch_size,\n                augment,\n                augment_path,\n                speaker_info_path,\n                cache_batch,\n                num_epochs,\n                save_every_epoch,\n                save_wav_with_checkpoint,\n                pre_trained_bottom_model_g,\n                pre_trained_bottom_model_d,\n                embedder_name,\n                int(embedding_output_layer),\n                False,\n                None if len(gpu_ids) > 1 else device,\n            )\n\n            yield \"Training index...\"\n            if run_train_index:\n                if not reduce_index_size:\n                    maximum_index_size = None\n                train_index(\n                    training_dir,\n                    model_name,\n                    out_dir,\n                    int(embedding_channels),\n                    num_cpu_process,\n                    maximum_index_size,\n                )\n\n            yield \"Training completed\"\n\n        with gr.Group():\n            with gr.Box():\n                with gr.Column():\n                    with gr.Row():\n                        with gr.Column():\n                            model_name = gr.Textbox(label=\"Model Name\")\n                            ignore_cache = gr.Checkbox(label=\"Ignore cache\")\n                        with gr.Column():\n                            dataset_glob = gr.Textbox(\n                                label=\"Dataset glob\", placeholder=\"data/**/*.wav\"\n                            )\n                            recursive = gr.Checkbox(label=\"Recursive\", value=True)\n                            multiple_speakers = gr.Checkbox(\n                                label=\"Multiple speakers\", value=False\n                            )\n                            speaker_id = gr.Slider(\n                                maximum=4,\n                                minimum=0,\n                                value=0,\n                                step=1,\n                                label=\"Speaker ID\",\n                            )\n\n                    with gr.Row(equal_height=False):\n                        version = gr.Radio(\n                            choices=[\"v1\", \"v2\"],\n                            value=\"v2\",\n                            label=\"Model version\",\n                        )\n                        target_sr = gr.Radio(\n                            choices=[\"32k\", \"40k\", \"48k\"],\n                            value=\"40k\",\n                            label=\"Target sampling rate\",\n                        )\n                        f0 = gr.Radio(\n                            choices=[\"Yes\", \"No\"],\n                            value=\"Yes\",\n                            label=\"f0 Model\",\n                        )\n                    with gr.Row(equal_height=False):\n                        embedding_name = gr.Radio(\n                            choices=list(models.EMBEDDINGS_LIST.keys()),\n                            value=\"contentvec\",\n                            label=\"Using phone embedder\",\n                        )\n                        embedding_channels = gr.Radio(\n                            choices=[\"256\", \"768\"],\n                            value=\"768\",\n                            label=\"Embedding channels\",\n                        )\n                        embedding_output_layer = gr.Radio(\n                            choices=[\"9\", \"12\"],\n                            value=\"12\",\n                            label=\"Embedding output layer\",\n                        )\n                    with gr.Row(equal_height=False):\n                        gpu_id = gr.Textbox(\n                            label=\"GPU ID\",\n                            value=\", \".join([f\"{x.index}\" for x in utils.get_gpus()]),\n                        )\n                        num_cpu_process = gr.Slider(\n                            minimum=0,\n                            maximum=cpu_count(),\n                            step=1,\n                            value=math.ceil(cpu_count() / 2),\n                            label=\"Number of CPU processes\",\n                        )\n                        norm_audio_when_preprocess = gr.Radio(\n                            choices=[\"Yes\", \"No\"],\n                            value=\"Yes\",\n                            label=\"Normalize audio volume when preprocess\",\n                        )\n                        pitch_extraction_algo = gr.Radio(\n                            choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n                            value=\"crepe\",\n                            label=\"Pitch extraction algorithm\",\n                        )\n                    with gr.Row(equal_height=False):\n                        batch_size = gr.Number(value=4, label=\"Batch size\")\n                        num_epochs = gr.Number(\n                            value=30,\n                            label=\"Number of epochs\",\n                        )\n                        save_every_epoch = gr.Slider(\n                            minimum=0,\n                            maximum=100,\n                            value=10,\n                            step=1,\n                            label=\"Save every epoch\",\n                        )\n                        save_wav_with_checkpoint = gr.Checkbox(\n                            label=\"save_wav_with_checkpoint\", value=False\n                        )\n                        cache_batch = gr.Checkbox(label=\"Cache batch\", value=True)\n                        fp16 = gr.Checkbox(\n                            label=\"FP16\", value=half_support, disabled=not half_support\n                        )\n                    with gr.Row(equal_height=False):\n                        augment = gr.Checkbox(label=\"Augment\", value=False)\n                        augment_from_pretrain = gr.Checkbox(\n                            label=\"Augment From Pretrain\", value=False\n                        )\n                        augment_path = gr.Textbox(\n                            label=\"Pre trained generator path (pth)\",\n                            value=\"file is not prepared\",\n                        )\n                        speaker_info_path = gr.Textbox(\n                            label=\"speaker info path (npy)\",\n                            value=\"file is not prepared\",\n                        )\n                    with gr.Row(equal_height=False):\n                        pre_trained_generator = gr.Textbox(\n                            label=\"Pre trained generator path\",\n                            value=os.path.join(\n                                MODELS_DIR, \"pretrained\", \"v2\", \"f0G40k.pth\"\n                            ),\n                        )\n                        pre_trained_discriminator = gr.Textbox(\n                            label=\"Pre trained discriminator path\",\n                            value=os.path.join(\n                                MODELS_DIR, \"pretrained\", \"v2\", \"f0D40k.pth\"\n                            ),\n                        )\n                    with gr.Row(equal_height=False):\n                        run_train_index = gr.Radio(\n                            choices=[\"Yes\", \"No\"],\n                            value=\"Yes\",\n                            label=\"Train Index\",\n                        )\n                        reduce_index_size = gr.Radio(\n                            choices=[\"Yes\", \"No\"],\n                            value=\"No\",\n                            label=\"Reduce index size with kmeans\",\n                        )\n                        maximum_index_size = gr.Number(\n                            value=10000, label=\"maximum index size\"\n                        )\n\n                    with gr.Row(equal_height=False):\n                        status = gr.Textbox(value=\"\", label=\"Status\")\n                    with gr.Row(equal_height=False):\n                        train_index_button = gr.Button(\"Train Index\", variant=\"primary\")\n                        train_all_button = gr.Button(\"Train\", variant=\"primary\")\n\n        train_index_button.click(\n            train_index_only,\n            inputs=[\n                model_name,\n                target_sr,\n                f0,\n                dataset_glob,\n                recursive,\n                multiple_speakers,\n                speaker_id,\n                gpu_id,\n                num_cpu_process,\n                norm_audio_when_preprocess,\n                pitch_extraction_algo,\n                run_train_index,\n                reduce_index_size,\n                maximum_index_size,\n                embedding_name,\n                embedding_channels,\n                embedding_output_layer,\n                ignore_cache,\n            ],\n            outputs=[status],\n        )\n\n        train_all_button.click(\n            train_all,\n            inputs=[\n                model_name,\n                version,\n                target_sr,\n                f0,\n                dataset_glob,\n                recursive,\n                multiple_speakers,\n                speaker_id,\n                gpu_id,\n                num_cpu_process,\n                norm_audio_when_preprocess,\n                pitch_extraction_algo,\n                batch_size,\n                augment,\n                augment_from_pretrain,\n                augment_path,\n                speaker_info_path,\n                cache_batch,\n                num_epochs,\n                save_every_epoch,\n                save_wav_with_checkpoint,\n                fp16,\n                pre_trained_generator,\n                pre_trained_discriminator,\n                run_train_index,\n                reduce_index_size,\n                maximum_index_size,\n                embedding_name,\n                embedding_channels,\n                embedding_output_layer,\n                ignore_cache,\n            ],\n            outputs=[status],\n        )", ""]}
{"filename": "modules/tabs/server.py", "chunked_list": ["import io\nimport json\n\nimport gradio as gr\nimport requests\nimport soundfile as sf\nimport torch.multiprocessing as multiprocessing\nfrom scipy.io.wavfile import write\n\nfrom modules.ui import Tab", "\nfrom modules.ui import Tab\nfrom server import app\n\nproc = None\n\ndef server_options_ui(show_out_dir=True):\n    with gr.Row().style(equal_height=False):\n        with gr.Row():\n            host = gr.Textbox(value=\"127.0.0.1\", label=\"host\")\n            port = gr.Textbox(value=\"5001\", label=\"port\")\n    with gr.Row().style(equal_height=False):\n        with gr.Row():\n            rvc_model_file = gr.Textbox(value=\"\", label=\"RVC model file path\")\n            faiss_index_file = gr.Textbox(value=\"\", label=\"Faiss index file path\")\n    with gr.Row().style(equal_height=False):\n        with gr.Row():\n            input_voice_file = gr.Textbox(value=\"\", label=\"input voice file path\")\n            speaker_id = gr.Number(\n                value=0,\n                label=\"speaker_id\",\n            )\n            transpose = gr.Slider(\n                minimum=-20, maximum=20, value=0, step=1, label=\"transpose\"\n            )\n            pitch_extraction_algo = gr.Radio(\n                choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n                value=\"crepe\",\n                label=\"pitch_extraction_algo\",\n            )\n            retrieval_feature_ratio = gr.Slider(\n                minimum=0,\n                maximum=1,\n                value=1,\n                step=0.01,\n                label=\"retrieval_feature_ratio\",\n            )\n    return (\n        host,\n        port,\n        rvc_model_file,\n        faiss_index_file,\n        input_voice_file,\n        speaker_id,\n        transpose,\n        pitch_extraction_algo,\n        retrieval_feature_ratio,\n    )", "\ndef run(**kwargs):\n    app.run(**kwargs)\n\nclass Server(Tab):\n    def title(self):\n        return \"Server(experimental)\"\n\n    def sort(self):\n        return 6\n\n    def ui(self, outlet):\n        def start(host, port):\n            if multiprocessing.get_start_method() == 'fork':\n                multiprocessing.set_start_method('spawn', force=True)\n            proc = multiprocessing.Process(target = run, kwargs = {'host': host, 'port': port})\n            proc.start()\n            yield \"start server\"\n\n        def upload(host, port, rvc_model_file, faiss_index_file):\n            file_names = {\"rvc_model_file\": rvc_model_file, \"faiss_index_file\": faiss_index_file}\n            res = requests.post(f\"http://{host}:{port}/upload_model\", json=file_names)\n            yield res.text\n\n        def convert(host, port, input_voice_file, speaker_id, transpose, pitch_extraction_algo, retrieval_feature_ratio):\n            params = {\n                \"speaker_id\": speaker_id,\n                \"transpose\": transpose,\n                \"pitch_extraction_algo\": pitch_extraction_algo,\n                \"retrieval_feature_ratio\": retrieval_feature_ratio\n            }\n\n            audio, sr = sf.read(input_voice_file)\n            audio_buffer = io.BytesIO()\n            write(audio_buffer, rate=sr, data=audio)\n            json_buffer = io.BytesIO(json.dumps(params).encode('utf-8'))\n            files = {\n                \"input_wav\": audio_buffer,\n                \"params\": json_buffer\n            }\n            res = requests.post(f\"http://{host}:{port}/convert_sound\", files=files)\n            audio, sr = sf.read(io.BytesIO(res.content))\n            yield \"convert succeed\", (sr, audio)\n\n        with gr.Group():\n            with gr.Box():\n                with gr.Column():\n                    (\n                        host,\n                        port,\n                        rvc_model_file,\n                        faiss_index_file,\n                        input_voice_file,\n                        speaker_id,\n                        transpose,\n                        pitch_extraction_algo,\n                        retrieval_feature_ratio,\n                    ) = server_options_ui()\n\n                    with gr.Row().style(equal_height=False):\n                        with gr.Column():\n                            status = gr.Textbox(value=\"\", label=\"Status\")\n                            output = gr.Audio(label=\"Output\", interactive=False)\n\n                    with gr.Row():\n                        start_button = gr.Button(\"Start server\", variant=\"primary\")\n                        upload_button = gr.Button(\"Upload Model\")\n                        convert_button = gr.Button(\"Convert Voice\")\n\n        start_button.click(\n            start,\n            inputs=[\n                host,\n                port\n            ],\n            outputs=[status],\n            queue=True,\n        )\n        upload_button.click(\n            upload,\n            inputs=[\n                host,\n                port,\n                rvc_model_file,\n                faiss_index_file\n            ],\n            outputs=[status],\n            queue=True,\n        )\n        convert_button.click(\n            convert,\n            inputs=[\n                host,\n                port,\n                input_voice_file,\n                speaker_id,\n                transpose,\n                pitch_extraction_algo,\n                retrieval_feature_ratio\n            ],\n            outputs=[status, output],\n            queue=True,\n        )", ""]}
