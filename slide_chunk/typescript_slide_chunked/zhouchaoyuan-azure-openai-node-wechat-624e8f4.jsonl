{"filename": "tsup.config.ts", "chunked_list": ["import { defineConfig } from 'tsup'\n\nexport default defineConfig({\n  entry: ['src/index.ts'],\n  outDir: 'build',\n  target: 'es2020',\n  format: ['esm'],\n  splitting: false,\n  sourcemap: true,\n  minify: false,", "  sourcemap: true,\n  minify: false,\n  shims: true,\n  dts: false,\n})\n"]}
{"filename": "src/const.ts", "chunked_list": ["/// ACCESS_TOKEN START\n(globalThis as any).accessToken = ''\n\n/// API ADDR START\nexport const wechatApiPath = '/wechat/api'\nexport const promptPath = '/prompt-process'\nexport const chatPath = '/chat-process'\n/// API ADDR END\n\n/// GENERAL START", "\n/// GENERAL START\nexport const pastMessagesIncluded = 6\nexport const chatDeploymentName = 'IdeaCreation'\nexport const promptDeploymentName = 'TextIdeaCreation'\nexport const chatModel = 'gpt-35-turbo'\nexport const promptModel = 'text-davinci-003'\n/// GENERAL START\n", ""]}
{"filename": "src/router.ts", "chunked_list": ["import type { FastifyInstance } from 'fastify'\nimport verify from './wechat/verify'\nimport wechatAcceptor from './wechat/acceptor'\nimport openAIAcceptor from './azureopenai/acceptor'\n\nfunction setUpRouter(server: FastifyInstance) {\n  // verify wechat request\n  verify(server)\n  // accept wechat request\n  wechatAcceptor(server)\n\n  // Azure openai sample api receive the request\n  openAIAcceptor(server)\n}\n\nexport default setUpRouter\n"]}
{"filename": "src/index.ts", "chunked_list": ["/* eslint-disable no-console */\nimport Fastify from 'fastify'\nimport * as dotenv from 'dotenv'\nimport { isNotEmptyString } from './utils/is'\nimport parser from './utils/parser'\nimport router from './router'\n\ndotenv.config()\n\nif (!isNotEmptyString(process.env.OPENAI_API_KEY))\n  throw new Error('Missing OPENAI_API_KEY environment variable')\n\nconst server = Fastify({\n  logger: true,\n})\n\nconst port = 80\n", "\nif (!isNotEmptyString(process.env.OPENAI_API_KEY))\n  throw new Error('Missing OPENAI_API_KEY environment variable')\n\nconst server = Fastify({\n  logger: true,\n})\n\nconst port = 80\n\nfunction main() {\n  parser(server)\n  router(server)\n  // Run the server!\n  server.listen({ port, host: '0.0.0.0' }, (err, address) => {", "function main() {\n  parser(server)\n  router(server)\n  // Run the server!\n  server.listen({ port, host: '0.0.0.0' }, (err, address) => {\n    if (err) {\n      server.log.error(err)\n      process.exit(1)\n    }\n    // Server is now listening on ${address}\n    console.log('Server listening at', port)\n  })\n}\n\nmain()\n"]}
{"filename": "src/wechat/acceptor.ts", "chunked_list": ["/* eslint-disable no-console */\nimport crypto from 'crypto'\nimport type { FastifyInstance, FastifyReply } from 'fastify'\nimport { isUUID } from 'src/utils/is'\nimport type { ChatCompletionRequestMessage } from 'azure-openai'\nimport { FixedQueue } from 'src/utils/FixedQueue'\nimport { pastMessagesIncluded, wechatApiPath } from '../const'\nimport { generateTextResponse } from '../utils/responser'\nimport { GetAzureOpenAIChatAnswerAsync } from '../azureopenai/chatgpt'\n", "import { GetAzureOpenAIChatAnswerAsync } from '../azureopenai/chatgpt'\n\nconst cacheMap = new Map()\nconst checkTimes = new Map()\nconst chatHistory = new Map() // cache in memory, should be cleared.\nconst queryCacheMap = new Map()\n\nasync function handleMsg(xml: any, reply: FastifyReply) {\n  if (xml.MsgType === 'text' || xml.MsgType === 'voice') {\n    handleTextMsg(xml, reply)\n  }", "  if (xml.MsgType === 'text' || xml.MsgType === 'voice') {\n    handleTextMsg(xml, reply)\n  }\n  else if (xml.MsgType === 'event') {\n    if (xml.Event === 'subscribe' || xml.Event === 'SCAN')\n      reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '\u611f\u8c22\u60a8\u7684\u652f\u6301\uff0c\u6211\u53ef\u4ee5\u4f5c\u4e3a\u4f60\u7684\u667a\u80fd\u52a9\u624b\uff0c\u6211\u975e\u5e38\u667a\u80fd\uff0c\u867d\u7136\u6211\u53ea\u6709\u9c7c\u7684\u8bb0\u5fc6\uff0c\u4f46\u662f\u4f60\u7684\u95ee\u9898\u6211\u4e00\u5b9a\u62fc\u5c3d\u5168\u529b\uff01\uff01\u8c22\u8c22\u3002'))\n    else\n      reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '\u975e\u5e38\u9057\u61be\uff08so sad\uff09'))\n  }\n  else { reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '\u53ea\u652f\u6301\u6587\u5b57\u6d88\u606f\uff08only support text message\uff09')) }\n}\n\n// <xml>\n//   <ToUserName><![CDATA[toUser]]></ToUserName>\n//   <FromUserName><![CDATA[fromUser]]></FromUserName>\n//   <CreateTime>1348831860</CreateTime>\n//   <MsgType><![CDATA[text]]></MsgType>\n//   <Content><![CDATA[this is a test]]></Content>\n//   <MsgId>1234567890123456</MsgId>\n//   <MsgDataId>xxxx</MsgDataId>\n//   <Idx>xxxx</Idx>\n// </xml>", "async function handleTextMsg(xml: any, reply: FastifyReply) {\n  const msg = (xml?.Content?.toString() ?? xml?.Recognition?.toString()) as string\n  if (msg === undefined || msg.trim().length === 0) {\n    reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '\u548b\u56de\u4e8b\uff0c\u4e0d\u80fd\u53d1\u9001\u7a7a\u5185\u5bb9\u54e6\uff08empty text\uff09'))\n  }\n  else {\n    if (isUUID(msg)) {\n      if (queryCacheMap.has(msg)) {\n        const requestKey = queryCacheMap.get(msg)\n        if (cacheMap.has(requestKey)) {\n          const resp = cacheMap.get(requestKey)", "        if (cacheMap.has(requestKey)) {\n          const resp = cacheMap.get(requestKey)\n          if (resp != null) {\n            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n            return\n          }\n        }\n      }\n      else {\n        reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName,\n          '\u60a8\u8f93\u5165\u7684\u6807\u8bc6\u53ef\u80fd\u5df2\u7ecf\u8fc7\u671f\u6216\u8005\u8f93\u5165\u8fc7\u5feb\u8fd8\u6ca1\u6709\u5904\u7406\u5f97\u5230\u7ed3\u679c'))\n        return\n      }\n    }\n    const uuidKey = crypto.randomUUID()\n    const requestKey = `${xml.FromUserName}_${xml.CreateTime}`", "    if (!cacheMap.has(requestKey)) {\n      const fq = getChatHistoryForUser(xml.FromUserName.toString())\n      fq.enqueue({ role: 'user', content: msg })\n      GetAzureOpenAIChatAnswerAsync(fq.toArray(), xml.FromUserName.toString())\n        .then((response) => {\n          if (!reply.sent) // return the result directly within 5s\n            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, response))\n          cacheMap.set(requestKey, response)\n          fq.enqueue({ role: 'assistant', content: response })\n        }).catch((reason) => {\n          if (!reply.sent) // return the result directly within 5s\n            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, reason))\n          cacheMap.set(requestKey, reason)\n        })\n      cacheMap.set(requestKey, null)\n      checkTimes.set(requestKey, 1)\n    }\n    else {\n      const resp = cacheMap.get(requestKey)", "        }).catch((reason) => {\n          if (!reply.sent) // return the result directly within 5s\n            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, reason))\n          cacheMap.set(requestKey, reason)\n        })\n      cacheMap.set(requestKey, null)\n      checkTimes.set(requestKey, 1)\n    }\n    else {\n      const resp = cacheMap.get(requestKey)\n      if (resp != null) {\n        reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n      }\n      else {\n        const checkTime = checkTimes.get(requestKey) as number", "      if (resp != null) {\n        reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n      }\n      else {\n        const checkTime = checkTimes.get(requestKey) as number\n        if (checkTime === 2) { // this is the third retry from wechat, we need to response the result within 5s\n          const resp = cacheMap.get(requestKey)\n          if (resp != null) {\n            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n          }\n          else {\n            // wait another 3s to get response, so total timeout value is 5 + 5 + 3 = 13s\n            setTimeout(() => {\n              const resp = cacheMap.get(requestKey)", "              if (resp != null) {\n                reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n              }\n              else {\n                queryCacheMap.set(uuidKey, requestKey)\n                reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName,\n                  `\u54ce\u54df\uff0c\u8d85\u65f6\u5566\uff01\u8bf7\u8010\u5fc3\u7b49\u5f85 30 \u79d2\u540e\u8f93\u5165\u4e0b\u9762\u7684\u6807\u8bc6\u83b7\u53d6 chatgpt \u7684\u56de\u590d\uff1a\\n\n                  <a href=\"weixin://bizmsgmenu?msgmenucontent=${uuidKey}&msgmenuid=1\">${uuidKey}</a>`))\n              }\n            }, 3 * 1000)\n          }\n        }\n        else {\n          checkTimes.set(requestKey, checkTimes.get(requestKey) as number + 1)\n          console.log(checkTimes.get(requestKey))\n        }\n      }\n    }\n    // cache 3 mins, this can be optimized.\n    setTimeout(() => {\n      console.log('delete cache')\n      cacheMap.delete(requestKey)\n      checkTimes.delete(requestKey)\n      queryCacheMap.delete(uuidKey)\n    }, 180 * 1000)\n  }\n}\n", "function getChatHistoryForUser(requestKey: string) {\n  if (!chatHistory.has(requestKey))\n    chatHistory.set(requestKey, new FixedQueue<ChatCompletionRequestMessage>(pastMessagesIncluded))\n  return chatHistory.get(requestKey)\n}\n\nfunction route(server: FastifyInstance) {\n  server.post(wechatApiPath, (request, reply) => {\n    const xml = request.body as any\n    handleMsg(xml.xml, reply)\n  })\n}\n\nexport default route\n"]}
{"filename": "src/wechat/verify.ts", "chunked_list": ["import type { FastifyInstance, FastifyRequest } from 'fastify'\nimport { wechatApiPath } from '../const'\nimport { toSha1 } from '../utils/encryptor'\nimport type { VerifyQuery } from './types'\n\nexport function checkIsWechatRequest(request: FastifyRequest) {\n  const q = request.query as VerifyQuery\n  try {\n    const arr: string[] = [process.env.WECHAT_TOKEN, q.timestamp.toString(), q.nonce.toString()]\n    arr.sort()\n    const sha1 = toSha1(arr.join(''))", "    if (sha1 === q.signature) {\n      // Success.\n      return true\n    }\n    else {\n      return false\n    }\n  }\n  catch (error) {\n    console.error(error)\n    return false\n  }\n}\n", "  catch (error) {\n    console.error(error)\n    return false\n  }\n}\n\nfunction route(server: FastifyInstance) {\n  // setup hook\n  server.addHook('onRequest', (request, reply, done) => {\n    // TODO: Do we need to add this hook for all wechat request?\n    if (request?.url?.startsWith(wechatApiPath)) {", "    if (request?.url?.startsWith(wechatApiPath)) {\n      if (checkIsWechatRequest(request)) {\n        // Continue.\n        done()\n      }\n      else {\n        reply.send('who are you?')\n      }\n    }\n    else {\n      done()\n    }\n  })\n\n  server.get(wechatApiPath, (request: FastifyRequest, reply) => {\n    const q = request.query as VerifyQuery", "    if (checkIsWechatRequest(request)) {\n      // Success.\n      reply.send(q.echostr)\n    }\n    else {\n      reply.send('who are you?')\n    }\n  })\n}\n\nexport default route\n"]}
{"filename": "src/wechat/types.ts", "chunked_list": ["export interface VerifyQuery {\n  signature: string\n  timestamp: number\n  nonce: number\n  echostr: string\n  openid: string\n}\n"]}
{"filename": "src/utils/FixedQueue.ts", "chunked_list": ["export class FixedQueue<T> {\n  private queue: T[]\n  private maxLength: number\n\n  constructor(maxLength: number) {\n    this.queue = []\n    this.maxLength = maxLength\n  }\n\n  enqueue(item: T): void {\n    if (this.queue.length >= this.maxLength)\n      this.dequeue()\n\n    this.queue.push(item)\n  }\n\n  dequeue(): T | undefined {\n    return this.queue.shift()\n  }\n\n  peek(): T | undefined {\n    return this.queue[0]\n  }\n\n  clear(): void {\n    this.queue = []\n  }\n\n  get length(): number {\n    return this.queue.length\n  }\n\n  toArray(): T[] {\n    return Array.from(this.queue)\n  }\n}\n", "    if (this.queue.length >= this.maxLength)\n      this.dequeue()\n\n    this.queue.push(item)\n  }\n\n  dequeue(): T | undefined {\n    return this.queue.shift()\n  }\n\n  peek(): T | undefined {\n    return this.queue[0]\n  }\n\n  clear(): void {\n    this.queue = []\n  }\n\n  get length(): number {\n    return this.queue.length\n  }\n\n  toArray(): T[] {\n    return Array.from(this.queue)\n  }\n}\n"]}
{"filename": "src/utils/parser.ts", "chunked_list": ["import { XMLParser } from 'fast-xml-parser'\nimport type { FastifyInstance } from 'fastify'\n\nconst xmlParser = new XMLParser()\n\nexport default function configureParser(server: FastifyInstance) {\n  server.addContentTypeParser(['text/xml', 'application/xml'], (\n    request, payload, done,\n  ) => {\n    const chunks: Uint8Array[] = []\n    payload.on('data', (chunk) => {\n      chunks.push(chunk)\n    })\n    payload.on('end', () => {", "      try {\n        const xml = Buffer.concat(chunks).toString()\n        const result = xmlParser.parse(xml)\n        done(null, result)\n      }\n      catch (err) {\n        done(err as Error)\n      }\n    })\n  })\n}\n"]}
{"filename": "src/utils/responser.ts", "chunked_list": ["import { XMLBuilder } from 'fast-xml-parser'\n\nexport function generateTextResponse(toUser: string, fromUser: string, text: string) {\n  const xmlBuilder = new XMLBuilder({})\n  // <xml>\n  //   <ToUserName><![CDATA[toUser]]></ToUserName>\n  //   <FromUserName><![CDATA[fromUser]]></FromUserName>\n  //   <CreateTime>12345678</CreateTime>\n  //   <MsgType><![CDATA[text]]></MsgType>\n  //   <Content><![CDATA[\u4f60\u597d]]></Content>\n  // </xml>\n  return xmlBuilder.build({\n    xml: {\n      ToUserName: toUser,\n      FromUserName: fromUser,\n      CreateTime: Date.now(),\n      MsgType: 'text',\n      Content: text,\n    },\n  })\n}\n"]}
{"filename": "src/utils/encryptor.ts", "chunked_list": ["import Sha1 from 'sha1'\n\nexport function toSha1(str: string) {\n  return Sha1(str)\n}\n"]}
{"filename": "src/utils/is.ts", "chunked_list": ["export function isNumber<T extends number>(value: T | unknown): value is number {\n  return Object.prototype.toString.call(value) === '[object Number]'\n}\n\nexport function isString<T extends string>(value: T | unknown): value is string {\n  return Object.prototype.toString.call(value) === '[object String]'\n}\n\nexport function isNotEmptyString(value: any): boolean {\n  return typeof value === 'string' && value.length > 0\n}\n", "export function isNotEmptyString(value: any): boolean {\n  return typeof value === 'string' && value.length > 0\n}\n\nexport function isBoolean<T extends boolean>(value: T | unknown): value is boolean {\n  return Object.prototype.toString.call(value) === '[object Boolean]'\n}\n\nexport function isFunction<T extends (...args: any[]) => any | void | never>(value: T | unknown): value is T {\n  return Object.prototype.toString.call(value) === '[object Function]'\n}\n", "export function isFunction<T extends (...args: any[]) => any | void | never>(value: T | unknown): value is T {\n  return Object.prototype.toString.call(value) === '[object Function]'\n}\n\nexport function isUUID(str: string): boolean {\n  const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i\n  return uuidRegex.test(str)\n}\n"]}
{"filename": "src/azureopenai/acceptor.ts", "chunked_list": ["import type { FastifyInstance, FastifyReply } from 'fastify'\nimport { chatPath, promptPath } from '../const'\nimport { GetAzureOpenAIAnswerAsync, GetAzureOpenAIChatAnswerAsync } from '../azureopenai/chatgpt'\n\nfunction route(server: FastifyInstance) {\n  server.post(promptPath, (request, reply) => {\n    // eslint-disable-next-line no-console\n    console.log('prompt')\n    handlePrompt(request.body, reply)\n  })\n  server.post(chatPath, (request, reply) => {\n    handleChat(request.body, reply)\n  })\n}", "async function handlePrompt(requestBody: any, reply: FastifyReply) {\n  GetAzureOpenAIAnswerAsync(requestBody.prompt, 'prompt restapi').then((response) => {\n    reply.send(response)\n  }).catch((reason) => {\n    reply.send(reason)\n  })\n}\n\nasync function handleChat(requestBody: any, reply: FastifyReply) {\n  GetAzureOpenAIChatAnswerAsync(requestBody.messages, 'chat restapi').then((response) => {\n    reply.send(response)", "async function handleChat(requestBody: any, reply: FastifyReply) {\n  GetAzureOpenAIChatAnswerAsync(requestBody.messages, 'chat restapi').then((response) => {\n    reply.send(response)\n  }).catch((reason) => {\n    reply.send(reason)\n  })\n}\n\nexport default route\n"]}
{"filename": "src/azureopenai/chatgpt.ts", "chunked_list": ["/* eslint-disable no-console */\nimport type { ChatCompletionRequestMessage } from 'azure-openai'\nimport { Configuration, OpenAIApi } from 'azure-openai'\nimport { chatDeploymentName, chatModel, promptDeploymentName, promptModel } from 'src/const'\n\nexport async function GetAzureOpenAIAnswerAsync(content: string, user: string) {\n  console.log(`user ${user} try to get prompt answer for \\\"${content}\\\".`)\n  try {\n    const configuration = new Configuration({\n      azure: {\n        apiKey: process.env.OPENAI_API_KEY,\n        endpoint: process.env.OPENAI_API_BASE_URL,\n        deploymentName: promptDeploymentName,\n      },\n    })\n    const openai = new OpenAIApi(configuration)\n    const completion = await openai.createCompletion({\n      model: promptModel,\n      prompt: content,\n      max_tokens: 800,\n    })\n    console.log('prompt response-content: ', completion.data.choices[0].text?.trim())\n    return completion.data.choices[0].text?.trim()\n  }", "  catch (error) {\n    if (error.response)\n      console.log(error.response.status, error.response.data)\n    else\n      console.log(error.message)\n    return '\u6682\u4e0d\u652f\u6301 (unsupported temporally)'\n  }\n}\n\nexport async function GetAzureOpenAIChatAnswerAsync(messages: Array<ChatCompletionRequestMessage>, user: string) {\n  console.log(`user ${user} try to get chat answer for \\\"${messages[messages.length - 1].content}\\\".`)", "export async function GetAzureOpenAIChatAnswerAsync(messages: Array<ChatCompletionRequestMessage>, user: string) {\n  console.log(`user ${user} try to get chat answer for \\\"${messages[messages.length - 1].content}\\\".`)\n  try {\n    const configuration = new Configuration({\n      azure: {\n        apiKey: process.env.OPENAI_API_KEY,\n        endpoint: process.env.OPENAI_API_BASE_URL,\n        deploymentName: chatDeploymentName,\n      },\n    })\n    const openai = new OpenAIApi(configuration)\n    const completion = await openai.createChatCompletion({\n      model: chatModel,\n      messages,\n      max_tokens: 800,\n    })\n    console.log('chat response-content: ', completion.data.choices[0].message.content?.trim())\n    return completion.data.choices[0].message.content?.trim()\n  }", "  catch (error) {\n    if (error.response)\n      console.log(error.response.status, error.response.data)\n    else\n      console.log(error.message)\n    return '\u6682\u4e0d\u652f\u6301 (unsupported temporally)'\n  }\n}\n"]}
